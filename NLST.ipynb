{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## csv_file process\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df_csv = pd.read_csv('dataset_csv/NLST_offical.csv')\n",
    "dataset_dir = '/home/sci/Disk_data/Datasets/NLST/WSI'\n",
    "index = df_csv.index\n",
    "rows_to_delete = []\n",
    "for name in os.listdir(dataset_dir):\n",
    "    slide_id = os.path.splitext(name)[0]\n",
    "\n",
    "    row_index = df_csv[df_csv['slide_id'] == slide_id].index\n",
    "    rows_to_delete.append(row_index.item())\n",
    "    \n",
    "remaining_index = index.difference(rows_to_delete)\n",
    "df_delete = df_csv.drop(remaining_index).reset_index(drop=True)\n",
    "df_delete\n",
    "# df = df_csv.drop(~rows_to_delete)\n",
    "\n",
    "df_delete.to_csv('dataset_csv/NLST_offical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取 CSV 文件到 DataFrame\n",
    "df = pd.read_csv('dataset_csv/NLST.csv')\n",
    "\n",
    "# 重新设置 \"case_id\" 列的值从1开始递增\n",
    "df['case_id'] = range(1, len(df) + 1)\n",
    "# 将整数列 'case_id' 的数据类型更改为 'object'\n",
    "df['case_id'] = df['case_id'].astype('object')\n",
    "\n",
    "# 如果需要，将修改后的 DataFrame 保存回 CSV 文件\n",
    "df.to_csv('dataset_csv/NLST.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     int64\n",
       "case_id        int64\n",
       "slide_id      object\n",
       "label         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset_csv/NLST.csv')\n",
    "\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "for i in range(5):\n",
    "\n",
    "    # 读取 CSV 文件到 DataFrame\n",
    "    df = pd.read_csv(f'splits/nlst_75/splits_{i}.csv')\n",
    "\n",
    "    # 查看每列的数据类型\n",
    "    data_types = df.dtypes\n",
    "    # 打印每列的数据类型\n",
    "    print(data_types)\n",
    "\n",
    "    # df['train'] = df['train'].astype('object')\n",
    "    # df.to_csv(f'splits/nlst_75/splits_{i}.csv', index=False)\n",
    "    # print(f'{df.dtypes}')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已经删除指定列\n"
     ]
    }
   ],
   "source": [
    "# delete columns\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset_csv/cptac_lung_subtyping.csv')\n",
    "if 'specimen_type' in df.columns:\n",
    "    df.drop('specimen_type', axis=1, inplace=True)\n",
    "\n",
    "df.to_csv('dataset_csv/cptac_lung_subtyping.csv', index=True)\n",
    "\n",
    "print('已经删除指定列')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WSI 属性查看\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import openslide\n",
    "import os\n",
    "import torch\n",
    "\n",
    "slide_path = r'/home/sci/Disk_data/Datasets/TCGA-NSCLC/WSI/TCGA-05-4244-01A-01-BS1.svs'\n",
    "path = r\"/home/sci/Disk_data/Datasets/NLST/WSI\"\n",
    "\n",
    "# # ---->> for single slide\n",
    "# slide = openslide.OpenSlide(slide_path)\n",
    "# slide.dimensions\n",
    "# slide.associated_images\n",
    "# slide.level_dimensions\n",
    "# slide.level_count\n",
    "# slide.properties['aperio.AppMag']\n",
    "# slide.properties['aperio.MPP']\n",
    "\n",
    "# ---->> for slide folder\n",
    "slides = os.listdir(path)\n",
    "dimentions_list = []\n",
    "i = 0 \n",
    "for slide in slides:\n",
    "    slide = os.path.join(path,slide)\n",
    "    slide = openslide.OpenSlide(slide)\n",
    "    # print(slide.level_dimensions)\n",
    "    # print(slide.properties['aperio.AppMag'])\n",
    "    # print(slide.level_count)\n",
    "    # print(slide.properties)\n",
    "    if slide.level_count < 3:\n",
    "        # print(slide.level_downsamples)\n",
    "        # print(slide)\n",
    "        i+=1\n",
    "i\n",
    "    ## slide thumbnails\n",
    "    # slide = slide.get_thumbnail((slide.level_dimensions[-1]))\n",
    "    # slide.save(r'/home/sci/Disk2/tcga_crc/DATA_DIRECTORY/test')\n",
    "    # print('save_success')\n",
    "    # dimentions_list.append(slide.level_dimensions[-1])\n",
    "# dimentions_list=torch.tensor(dimentions_list)\n",
    "# index = torch.topk(dimentions_list, 1, dim=1)\n",
    "# maxMap = torch.index_select(dimentions_list, index=index, dim=0 )\n",
    "# maxMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----> wsi transfer\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import openslide\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "path = r\"/home/sci/Disk_data/Datasets/NLST/WSI\"\n",
    "path_20 = os.path.join(path, '20')\n",
    "path_40 = os.path.join(path, '40')\n",
    "os.makedirs(path_20, exist_ok=True)\n",
    "os.makedirs(path_40, exist_ok=True)\n",
    "# slide_list = glob.glob(os.path.join(path, '*.svs'))\n",
    "\n",
    "# for slide_path in slide_list:\n",
    "#     # slide_path = os.path.join(path, slide)\n",
    "#     slide_obj = openslide.open_slide(slide_path)\n",
    "\n",
    "#     if slide_obj.level_count == 3:\n",
    "#         shutil.move(slide_path, path_20)\n",
    "#     elif slide_obj.level_count == 4:\n",
    "#         shutil.move(slide_path, path_40)\n",
    "\n",
    "# ----> trans back\n",
    "allslide_path = glob.glob(os.path.join(path, '*/*.svs'))\n",
    "for slide_path in allslide_path:\n",
    "    shutil.move(slide_path, path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"coords\": shape (2989, 2), type \"<i4\">\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2989, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "path = r'E:\\Workspace\\Project\\CLAM\\data\\RESULTS_DIRECTORY\\patches\\TCGA-A6-2671-01A-01-BS1.h5'\n",
    "f = h5py.File(path,'r')\n",
    "f.keys()\n",
    "data = f.get('coords')\n",
    "print(data)\n",
    "data.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step_1 get patch\n",
    "\n",
    "source svs 文件地址\\\n",
    "save_dir 结果文件\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python create_patches_fp.py \\\n",
    "--source 'svs_dir' \\\n",
    "--save_dir 'result_dir' \\\n",
    "--patch_size 256 \\\n",
    "--seg \\ # 分割=True\n",
    "--patch \\ # 切分patch=True\n",
    "--stitch \\ # \n",
    "process_list_autogen.csv # 这里记录了每张 image 的处理的参数\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:  /home/sci/Disk_data/Datasets/NLST/WSI/20\n",
      "patch_save_dir:  /home/sci/Disk_data/Datasets/NLST/BLOCKS_level1/patches\n",
      "mask_save_dir:  /home/sci/Disk_data/Datasets/NLST/BLOCKS_level1/masks\n",
      "stitch_save_dir:  /home/sci/Disk_data/Datasets/NLST/BLOCKS_level1/stitches\n",
      "thumbnail_save_dir:  /home/sci/Disk_data/Datasets/NLST/BLOCKS_level1/thumbnails\n",
      "source : /home/sci/Disk_data/Datasets/NLST/WSI/20\n",
      "save_dir : /home/sci/Disk_data/Datasets/NLST/BLOCKS_level1\n",
      "patch_save_dir : /home/sci/Disk_data/Datasets/NLST/BLOCKS_level1/patches\n",
      "mask_save_dir : /home/sci/Disk_data/Datasets/NLST/BLOCKS_level1/masks\n",
      "stitch_save_dir : /home/sci/Disk_data/Datasets/NLST/BLOCKS_level1/stitches\n",
      "thumbnail_save_dir : /home/sci/Disk_data/Datasets/NLST/BLOCKS_level1/thumbnails\n",
      "{'seg_params': {'seg_level': -1, 'sthresh': 8, 'mthresh': 7, 'close': 4, 'use_otsu': False, 'keep_ids': 'none', 'exclude_ids': 'none'}, 'filter_params': {'a_t': 100, 'a_h': 16, 'max_n_holes': 8}, 'patch_params': {'use_padding': True, 'contour_fn': 'four_pt'}, 'vis_params': {'vis_level': -1, 'line_thickness': 250}}\n",
      "\n",
      "\n",
      "progress: 0.00, 0/226\n",
      "processing 10024.svs\n",
      "10024 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.00, 1/226\n",
      "processing 10116.svs\n",
      "10116 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.01, 2/226\n",
      "processing 10118.svs\n",
      "10118 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.01, 3/226\n",
      "processing 10119.svs\n",
      "10119 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.02, 4/226\n",
      "processing 10146.svs\n",
      "10146 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.02, 5/226\n",
      "processing 10177.svs\n",
      "10177 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.03, 6/226\n",
      "processing 10384.svs\n",
      "10384 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.03, 7/226\n",
      "processing 10411.svs\n",
      "10411 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.04, 8/226\n",
      "processing 10502.svs\n",
      "10502 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.04, 9/226\n",
      "processing 10504.svs\n",
      "10504 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.04, 10/226\n",
      "processing 10542.svs\n",
      "10542 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.05, 11/226\n",
      "processing 10899.svs\n",
      "10899 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.05, 12/226\n",
      "processing 10908.svs\n",
      "10908 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.06, 13/226\n",
      "processing 10984.svs\n",
      "10984 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.06, 14/226\n",
      "processing 10986.svs\n",
      "10986 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.07, 15/226\n",
      "processing 11013.svs\n",
      "11013 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.07, 16/226\n",
      "processing 11026.svs\n",
      "11026 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.08, 17/226\n",
      "processing 11104.svs\n",
      "11104 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.08, 18/226\n",
      "processing 11132.svs\n",
      "11132 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.08, 19/226\n",
      "processing 11252.svs\n",
      "11252 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.09, 20/226\n",
      "processing 11288.svs\n",
      "11288 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.09, 21/226\n",
      "processing 11290.svs\n",
      "11290 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.10, 22/226\n",
      "processing 11299.svs\n",
      "11299 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.10, 23/226\n",
      "processing 11367.svs\n",
      "11367 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.11, 24/226\n",
      "processing 11379.svs\n",
      "11379 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.11, 25/226\n",
      "processing 11410.svs\n",
      "11410 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.12, 26/226\n",
      "processing 11786.svs\n",
      "11786 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.12, 27/226\n",
      "processing 9461.svs\n",
      "9461 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.12, 28/226\n",
      "processing 9677.svs\n",
      "9677 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.13, 29/226\n",
      "processing 9936.svs\n",
      "9936 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.13, 30/226\n",
      "processing 9973.svs\n",
      "9973 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.14, 31/226\n",
      "processing 9974.svs\n",
      "9974 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.14, 32/226\n",
      "processing NLSI0000004.svs\n",
      "NLSI0000004 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.15, 33/226\n",
      "processing NLSI0000005.svs\n",
      "NLSI0000005 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.15, 34/226\n",
      "processing NLSI0000006.svs\n",
      "NLSI0000006 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.15, 35/226\n",
      "processing NLSI0000023.svs\n",
      "NLSI0000023 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.16, 36/226\n",
      "processing NLSI0000024.svs\n",
      "NLSI0000024 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.16, 37/226\n",
      "processing NLSI0000025.svs\n",
      "NLSI0000025 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.17, 38/226\n",
      "processing NLSI0000026.svs\n",
      "NLSI0000026 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.17, 39/226\n",
      "processing NLSI0000027.svs\n",
      "NLSI0000027 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.18, 40/226\n",
      "processing NLSI0000029.svs\n",
      "NLSI0000029 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.18, 41/226\n",
      "processing NLSI0000030.svs\n",
      "NLSI0000030 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.19, 42/226\n",
      "processing NLSI0000031.svs\n",
      "NLSI0000031 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.19, 43/226\n",
      "processing NLSI0000032.svs\n",
      "NLSI0000032 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.19, 44/226\n",
      "processing NLSI0000033.svs\n",
      "NLSI0000033 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.20, 45/226\n",
      "processing NLSI0000034.svs\n",
      "NLSI0000034 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.20, 46/226\n",
      "processing NLSI0000035.svs\n",
      "NLSI0000035 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.21, 47/226\n",
      "processing NLSI0000036.svs\n",
      "NLSI0000036 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.21, 48/226\n",
      "processing NLSI0000037.svs\n",
      "NLSI0000037 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.22, 49/226\n",
      "processing NLSI0000041.svs\n",
      "NLSI0000041 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.22, 50/226\n",
      "processing NLSI0000042.svs\n",
      "NLSI0000042 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.23, 51/226\n",
      "processing NLSI0000043.svs\n",
      "NLSI0000043 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.23, 52/226\n",
      "processing NLSI0000044.svs\n",
      "NLSI0000044 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.23, 53/226\n",
      "processing NLSI0000045.svs\n",
      "NLSI0000045 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.24, 54/226\n",
      "processing NLSI0000046.svs\n",
      "NLSI0000046 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.24, 55/226\n",
      "processing NLSI0000047.svs\n",
      "NLSI0000047 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.25, 56/226\n",
      "processing NLSI0000048.svs\n",
      "NLSI0000048 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.25, 57/226\n",
      "processing NLSI0000049.svs\n",
      "NLSI0000049 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.26, 58/226\n",
      "processing NLSI0000050.svs\n",
      "NLSI0000050 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.26, 59/226\n",
      "processing NLSI0000051.svs\n",
      "NLSI0000051 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.27, 60/226\n",
      "processing NLSI0000054.svs\n",
      "NLSI0000054 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.27, 61/226\n",
      "processing NLSI0000055.svs\n",
      "NLSI0000055 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.27, 62/226\n",
      "processing NLSI0000062.svs\n",
      "NLSI0000062 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.28, 63/226\n",
      "processing NLSI0000063.svs\n",
      "NLSI0000063 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.28, 64/226\n",
      "processing NLSI0000064.svs\n",
      "NLSI0000064 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.29, 65/226\n",
      "processing NLSI0000068.svs\n",
      "NLSI0000068 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.29, 66/226\n",
      "processing NLSI0000069.svs\n",
      "NLSI0000069 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.30, 67/226\n",
      "processing NLSI0000070.svs\n",
      "NLSI0000070 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.30, 68/226\n",
      "processing NLSI0000071.svs\n",
      "NLSI0000071 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.31, 69/226\n",
      "processing NLSI0000072.svs\n",
      "NLSI0000072 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.31, 70/226\n",
      "processing NLSI0000073.svs\n",
      "NLSI0000073 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.31, 71/226\n",
      "processing NLSI0000074.svs\n",
      "NLSI0000074 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.32, 72/226\n",
      "processing NLSI0000075.svs\n",
      "NLSI0000075 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.32, 73/226\n",
      "processing NLSI0000076.svs\n",
      "NLSI0000076 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.33, 74/226\n",
      "processing NLSI0000077.svs\n",
      "NLSI0000077 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.33, 75/226\n",
      "processing NLSI0000078.svs\n",
      "NLSI0000078 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.34, 76/226\n",
      "processing NLSI0000079-001.svs\n",
      "NLSI0000079-001 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.34, 77/226\n",
      "processing NLSI0000080.svs\n",
      "NLSI0000080 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.35, 78/226\n",
      "processing NLSI0000081.svs\n",
      "NLSI0000081 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.35, 79/226\n",
      "processing NLSI0000082.svs\n",
      "NLSI0000082 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.35, 80/226\n",
      "processing NLSI0000085-001.svs\n",
      "NLSI0000085-001 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.36, 81/226\n",
      "processing NLSI0000086.svs\n",
      "NLSI0000086 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.36, 82/226\n",
      "processing NLSI0000090.svs\n",
      "NLSI0000090 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.37, 83/226\n",
      "processing NLSI0000091.svs\n",
      "NLSI0000091 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.37, 84/226\n",
      "processing NLSI0000092.svs\n",
      "NLSI0000092 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.38, 85/226\n",
      "processing NLSI0000098.svs\n",
      "NLSI0000098 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.38, 86/226\n",
      "processing NLSI0000099.svs\n",
      "NLSI0000099 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.38, 87/226\n",
      "processing NLSI0000100.svs\n",
      "NLSI0000100 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.39, 88/226\n",
      "processing NLSI0000101.svs\n",
      "NLSI0000101 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.39, 89/226\n",
      "processing NLSI0000104.svs\n",
      "NLSI0000104 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.40, 90/226\n",
      "processing NLSI0000105.svs\n",
      "NLSI0000105 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.40, 91/226\n",
      "processing NLSI0000106.svs\n",
      "NLSI0000106 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.41, 92/226\n",
      "processing NLSI0000107.svs\n",
      "NLSI0000107 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.41, 93/226\n",
      "processing NLSI0000108.svs\n",
      "NLSI0000108 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.42, 94/226\n",
      "processing NLSI0000109.svs\n",
      "NLSI0000109 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.42, 95/226\n",
      "processing NLSI0000110.svs\n",
      "NLSI0000110 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.42, 96/226\n",
      "processing NLSI0000111.svs\n",
      "NLSI0000111 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.43, 97/226\n",
      "processing NLSI0000112.svs\n",
      "NLSI0000112 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.43, 98/226\n",
      "processing NLSI0000113.svs\n",
      "NLSI0000113 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.44, 99/226\n",
      "processing NLSI0000114.svs\n",
      "NLSI0000114 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.44, 100/226\n",
      "processing NLSI0000115.svs\n",
      "NLSI0000115 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.45, 101/226\n",
      "processing NLSI0000116.svs\n",
      "NLSI0000116 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.45, 102/226\n",
      "processing NLSI0000118.svs\n",
      "NLSI0000118 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.46, 103/226\n",
      "processing NLSI0000120.svs\n",
      "NLSI0000120 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.46, 104/226\n",
      "processing NLSI0000121.svs\n",
      "NLSI0000121 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.46, 105/226\n",
      "processing NLSI0000125.svs\n",
      "NLSI0000125 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.47, 106/226\n",
      "processing NLSI0000126.svs\n",
      "NLSI0000126 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.47, 107/226\n",
      "processing NLSI0000128.svs\n",
      "NLSI0000128 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.48, 108/226\n",
      "processing NLSI0000129.svs\n",
      "NLSI0000129 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.48, 109/226\n",
      "processing NLSI0000130.svs\n",
      "NLSI0000130 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.49, 110/226\n",
      "processing NLSI0000131.svs\n",
      "NLSI0000131 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.49, 111/226\n",
      "processing NLSI0000133.svs\n",
      "NLSI0000133 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.50, 112/226\n",
      "processing NLSI0000134.svs\n",
      "NLSI0000134 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.50, 113/226\n",
      "processing NLSI0000135.svs\n",
      "NLSI0000135 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.50, 114/226\n",
      "processing NLSI0000136.svs\n",
      "NLSI0000136 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.51, 115/226\n",
      "processing NLSI0000137.svs\n",
      "NLSI0000137 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.51, 116/226\n",
      "processing NLSI0000142.svs\n",
      "NLSI0000142 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.52, 117/226\n",
      "processing NLSI0000143.svs\n",
      "NLSI0000143 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.52, 118/226\n",
      "processing NLSI0000144.svs\n",
      "NLSI0000144 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.53, 119/226\n",
      "processing NLSI0000145.svs\n",
      "NLSI0000145 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.53, 120/226\n",
      "processing NLSI0000146.svs\n",
      "NLSI0000146 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.54, 121/226\n",
      "processing NLSI0000147.svs\n",
      "NLSI0000147 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.54, 122/226\n",
      "processing NLSI0000148.svs\n",
      "NLSI0000148 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.54, 123/226\n",
      "processing NLSI0000149.svs\n",
      "NLSI0000149 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.55, 124/226\n",
      "processing NLSI0000150.svs\n",
      "NLSI0000150 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.55, 125/226\n",
      "processing NLSI0000151.svs\n",
      "NLSI0000151 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.56, 126/226\n",
      "processing NLSI0000152.svs\n",
      "NLSI0000152 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.56, 127/226\n",
      "processing NLSI0000153.svs\n",
      "NLSI0000153 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.57, 128/226\n",
      "processing NLSI0000154.svs\n",
      "NLSI0000154 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.57, 129/226\n",
      "processing NLSI0000155.svs\n",
      "NLSI0000155 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.58, 130/226\n",
      "processing NLSI0000156.svs\n",
      "NLSI0000156 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.58, 131/226\n",
      "processing NLSI0000159.svs\n",
      "NLSI0000159 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.58, 132/226\n",
      "processing NLSI0000160.svs\n",
      "NLSI0000160 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.59, 133/226\n",
      "processing NLSI0000161.svs\n",
      "NLSI0000161 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.59, 134/226\n",
      "processing NLSI0000162-002.svs\n",
      "NLSI0000162-002 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.60, 135/226\n",
      "processing NLSI0000173.svs\n",
      "NLSI0000173 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.60, 136/226\n",
      "processing NLSI0000174.svs\n",
      "NLSI0000174 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.61, 137/226\n",
      "processing NLSI0000181.svs\n",
      "NLSI0000181 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.61, 138/226\n",
      "processing NLSI0000182.svs\n",
      "NLSI0000182 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.62, 139/226\n",
      "processing NLSI0000183.svs\n",
      "NLSI0000183 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.62, 140/226\n",
      "processing NLSI0000184.svs\n",
      "NLSI0000184 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.62, 141/226\n",
      "processing NLSI0000185.svs\n",
      "NLSI0000185 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.63, 142/226\n",
      "processing NLSI0000186.svs\n",
      "NLSI0000186 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.63, 143/226\n",
      "processing NLSI0000187.svs\n",
      "NLSI0000187 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.64, 144/226\n",
      "processing NLSI0000190.svs\n",
      "NLSI0000190 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.64, 145/226\n",
      "processing NLSI0000191.svs\n",
      "NLSI0000191 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.65, 146/226\n",
      "processing NLSI0000192.svs\n",
      "NLSI0000192 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.65, 147/226\n",
      "processing NLSI0000193.svs\n",
      "NLSI0000193 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.65, 148/226\n",
      "processing NLSI0000194.svs\n",
      "NLSI0000194 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.66, 149/226\n",
      "processing NLSI0000195.svs\n",
      "NLSI0000195 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.66, 150/226\n",
      "processing NLSI0000196.svs\n",
      "NLSI0000196 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.67, 151/226\n",
      "processing NLSI0000197.svs\n",
      "NLSI0000197 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.67, 152/226\n",
      "processing NLSI0000198.svs\n",
      "NLSI0000198 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.68, 153/226\n",
      "processing NLSI0000199.svs\n",
      "NLSI0000199 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.68, 154/226\n",
      "processing NLSI0000203.svs\n",
      "NLSI0000203 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.69, 155/226\n",
      "processing NLSI0000204.svs\n",
      "NLSI0000204 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.69, 156/226\n",
      "processing NLSI0000205.svs\n",
      "NLSI0000205 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.69, 157/226\n",
      "processing NLSI0000206.svs\n",
      "NLSI0000206 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.70, 158/226\n",
      "processing NLSI0000207.svs\n",
      "NLSI0000207 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.70, 159/226\n",
      "processing NLSI0000208.svs\n",
      "NLSI0000208 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.71, 160/226\n",
      "processing NLSI0000209.svs\n",
      "NLSI0000209 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.71, 161/226\n",
      "processing NLSI0000210.svs\n",
      "NLSI0000210 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.72, 162/226\n",
      "processing NLSI0000211.svs\n",
      "NLSI0000211 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.72, 163/226\n",
      "processing NLSI0000212.svs\n",
      "NLSI0000212 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.73, 164/226\n",
      "processing NLSI0000213.svs\n",
      "NLSI0000213 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.73, 165/226\n",
      "processing NLSI0000214.svs\n",
      "NLSI0000214 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.73, 166/226\n",
      "processing NLSI0000215.svs\n",
      "NLSI0000215 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.74, 167/226\n",
      "processing NLSI0000216.svs\n",
      "NLSI0000216 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.74, 168/226\n",
      "processing NLSI0000217.svs\n",
      "NLSI0000217 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.75, 169/226\n",
      "processing NLSI0000223.svs\n",
      "NLSI0000223 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.75, 170/226\n",
      "processing NLSI0000224.svs\n",
      "NLSI0000224 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.76, 171/226\n",
      "processing NLSI0000225.svs\n",
      "NLSI0000225 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.76, 172/226\n",
      "processing NLSI0000226.svs\n",
      "NLSI0000226 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.77, 173/226\n",
      "processing NLSI0000227.svs\n",
      "NLSI0000227 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.77, 174/226\n",
      "processing NLSI0000228.svs\n",
      "NLSI0000228 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.77, 175/226\n",
      "processing NLSI0000234.svs\n",
      "NLSI0000234 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.78, 176/226\n",
      "processing NLSI0000235.svs\n",
      "NLSI0000235 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.78, 177/226\n",
      "processing NLSI0000236.svs\n",
      "NLSI0000236 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.79, 178/226\n",
      "processing NLSI0000240.svs\n",
      "NLSI0000240 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.79, 179/226\n",
      "processing NLSI0000241.svs\n",
      "NLSI0000241 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.80, 180/226\n",
      "processing NLSI0000242.svs\n",
      "NLSI0000242 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.80, 181/226\n",
      "processing NLSI0000246.svs\n",
      "NLSI0000246 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.81, 182/226\n",
      "processing NLSI0000247.svs\n",
      "NLSI0000247 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.81, 183/226\n",
      "processing NLSI0000248.svs\n",
      "NLSI0000248 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.81, 184/226\n",
      "processing NLSI0000250.svs\n",
      "NLSI0000250 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.82, 185/226\n",
      "processing NLSI0000252.svs\n",
      "NLSI0000252 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.82, 186/226\n",
      "processing NLSI0000253.svs\n",
      "NLSI0000253 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.83, 187/226\n",
      "processing NLSI0000254.svs\n",
      "NLSI0000254 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.83, 188/226\n",
      "processing NLSI0000255.svs\n",
      "NLSI0000255 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.84, 189/226\n",
      "processing NLSI0000256.svs\n",
      "NLSI0000256 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.84, 190/226\n",
      "processing NLSI0000257.svs\n",
      "NLSI0000257 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.85, 191/226\n",
      "processing NLSI0000259.svs\n",
      "NLSI0000259 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.85, 192/226\n",
      "processing NLSI0000260.svs\n",
      "NLSI0000260 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.85, 193/226\n",
      "processing NLSI0000261.svs\n",
      "NLSI0000261 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.86, 194/226\n",
      "processing NLSI0000262.svs\n",
      "NLSI0000262 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.86, 195/226\n",
      "processing NLSI0000263.svs\n",
      "NLSI0000263 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.87, 196/226\n",
      "processing NLSI0000264.svs\n",
      "NLSI0000264 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.87, 197/226\n",
      "processing NLSI0000265.svs\n",
      "NLSI0000265 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.88, 198/226\n",
      "processing NLSI0000266.svs\n",
      "NLSI0000266 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.88, 199/226\n",
      "processing NLSI0000267.svs\n",
      "NLSI0000267 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.88, 200/226\n",
      "processing NLSI0000268.svs\n",
      "NLSI0000268 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.89, 201/226\n",
      "processing NLSI0000269.svs\n",
      "NLSI0000269 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.89, 202/226\n",
      "processing NLSI0000270.svs\n",
      "NLSI0000270 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.90, 203/226\n",
      "processing NLSI0000271.svs\n",
      "NLSI0000271 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.90, 204/226\n",
      "processing NLSI0000272.svs\n",
      "NLSI0000272 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.91, 205/226\n",
      "processing NLSI0000273.svs\n",
      "NLSI0000273 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.91, 206/226\n",
      "processing NLSI0000274.svs\n",
      "NLSI0000274 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.92, 207/226\n",
      "processing NLSI0000277.svs\n",
      "NLSI0000277 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.92, 208/226\n",
      "processing NLSI0000278.svs\n",
      "NLSI0000278 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.92, 209/226\n",
      "processing NLSI0000279.svs\n",
      "NLSI0000279 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.93, 210/226\n",
      "processing NLSI0000286.svs\n",
      "NLSI0000286 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.93, 211/226\n",
      "processing NLSI0000287.svs\n",
      "NLSI0000287 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.94, 212/226\n",
      "processing NLSI0000288.svs\n",
      "NLSI0000288 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.94, 213/226\n",
      "processing NLSI0000289.svs\n",
      "NLSI0000289 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.95, 214/226\n",
      "processing NLSI0000290.svs\n",
      "NLSI0000290 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.95, 215/226\n",
      "processing NLSI0000291.svs\n",
      "NLSI0000291 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.96, 216/226\n",
      "processing NLSI0000292.svs\n",
      "NLSI0000292 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.96, 217/226\n",
      "processing NLSI0000293.svs\n",
      "NLSI0000293 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.96, 218/226\n",
      "processing NLSI0000294.svs\n",
      "NLSI0000294 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.97, 219/226\n",
      "processing NLSI0000295.svs\n",
      "NLSI0000295 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.97, 220/226\n",
      "processing NLSI0000296.svs\n",
      "NLSI0000296 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.98, 221/226\n",
      "processing NLSI0000297.svs\n",
      "NLSI0000297 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.98, 222/226\n",
      "processing NLSI0000298.svs\n",
      "NLSI0000298 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.99, 223/226\n",
      "processing NLSI0000524.svs\n",
      "NLSI0000524 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.99, 224/226\n",
      "processing NLSI0000525.svs\n",
      "NLSI0000525 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 1.00, 225/226\n",
      "processing NLSI0000526.svs\n",
      "NLSI0000526 already exist in destination location, skipped\n",
      "average segmentation time in s per slide: 0.0\n",
      "average patching time in s per slide: 0.0\n",
      "average stiching time in s per slide: 0.0\n"
     ]
    }
   ],
   "source": [
    "##改 savedir and patch_level\n",
    "!python create_patches_fp.py --source /home/sci/Disk_data/Datasets/NLST/WSI/20 --save_dir /home/sci/Disk_data/Datasets/NLST/BLOCKS_level1 --patch_level 0 --patch_size 256 --seg --patch --stitch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step_2 get patch features\n",
    "data_h5_dir  输出文件地址\\\n",
    "data_slide_dir svs文件地址\\\n",
    "上一步生成的csv csv_path\\\n",
    "feat_dir 输出文件地址\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CUDA_VISIBLE_DEVICES=0 python extract_features_fp.py \\\n",
    "--data_h5_dir data/RESULTS_DIRECTORY/patches \\\n",
    "--data_slide_dir /media/yuansh/14THHD/CLAM/DataSet/toy_example \\\n",
    "--csv_path /media/yuansh/14THHD/CLAM/Step_2.csv \\\n",
    "--feat_dir /media/yuansh/14THHD/CLAM/FEATURES_DIRECTORY \\\n",
    "--batch_size 512 \\\n",
    "--slide_ext .svs\n",
    "'''\n",
    "!python extract_features_fp.py\\\n",
    "    --data_h5_dir E:\\Workspace\\Project\\CLAM\\data\\RESULTS_DIRECTORY \\\n",
    "    --data_slide_dir F:/Download/TCGA \\\n",
    "    --csv_path data\\RESULTS_DIRECTORY\\Step_2.csv \\\n",
    "    --feat_dir E:\\Workspace\\Project\\CLAM\\data\\FEATURES_DIRECTORY \\\n",
    "    --batch_size 256 \\\n",
    "    --slide_ext .svs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成第2步骤需要的csv文件\n",
    "from utils.csv_gen import *\n",
    "\n",
    "csv_dir = r'/home/sci/Disk_data/Datasets/NLST/BLOCKS_level1/process_list_autogen.csv'\n",
    "# sort_csv = pd.read_csv(csv_dir).sort_values('slide_id')\n",
    "result_dir = r'/home/sci/Disk_data/Datasets/NLST/BLOCKS_level1/step2_get_features.csv'\n",
    "patch_dir = r'/home/sci/Disk_data/Datasets/NLST/BLOCKS_level1/patches'\n",
    "csv_gen_step1(csv_dir,result_dir,patch_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing dataset\n",
      "loading model checkpoint\n",
      "\n",
      "progress: 0/876\n",
      "10015\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [89295 85227]\n",
      "level_dim [89295 85227]\n",
      "name 10015\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10015.h5: total of 127 batches\n",
      "batch 0/127, 0 files processed\n",
      "batch 20/127, 10240 files processed\n",
      "batch 40/127, 20480 files processed\n",
      "batch 60/127, 30720 files processed\n",
      "batch 80/127, 40960 files processed\n",
      "batch 100/127, 51200 files processed\n",
      "batch 120/127, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10015.h5 took 95.90167689323425 s\n",
      "features size:  (64911, 1024)\n",
      "coordinates size:  (64911, 2)\n",
      "\n",
      "progress: 1/876\n",
      "10016\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [86415 85735]\n",
      "level_dim [86415 85735]\n",
      "name 10016\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10016.h5: total of 116 batches\n",
      "batch 0/116, 0 files processed\n",
      "batch 20/116, 10240 files processed\n",
      "batch 40/116, 20480 files processed\n",
      "batch 60/116, 30720 files processed\n",
      "batch 80/116, 40960 files processed\n",
      "batch 100/116, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10016.h5 took 98.73756670951843 s\n",
      "features size:  (59282, 1024)\n",
      "coordinates size:  (59282, 2)\n",
      "\n",
      "progress: 2/876\n",
      "10019\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [84494 61937]\n",
      "level_dim [84494 61937]\n",
      "name 10019\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10019.h5: total of 88 batches\n",
      "batch 0/88, 0 files processed\n",
      "batch 20/88, 10240 files processed\n",
      "batch 40/88, 20480 files processed\n",
      "batch 60/88, 30720 files processed\n",
      "batch 80/88, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10019.h5 took 47.02313470840454 s\n",
      "features size:  (44750, 1024)\n",
      "coordinates size:  (44750, 2)\n",
      "\n",
      "progress: 3/876\n",
      "10021\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96976 76282]\n",
      "level_dim [96976 76282]\n",
      "name 10021\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10021.h5: total of 81 batches\n",
      "batch 0/81, 0 files processed\n",
      "batch 20/81, 10240 files processed\n",
      "batch 40/81, 20480 files processed\n",
      "batch 60/81, 30720 files processed\n",
      "batch 80/81, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10021.h5 took 68.50807642936707 s\n",
      "features size:  (41258, 1024)\n",
      "coordinates size:  (41258, 2)\n",
      "\n",
      "progress: 4/876\n",
      "10024\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [21123 17839]\n",
      "level_dim [21123 17839]\n",
      "name 10024\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10024.h5: total of 5 batches\n",
      "batch 0/5, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10024.h5 took 4.826893329620361 s\n",
      "features size:  (2482, 1024)\n",
      "coordinates size:  (2482, 2)\n",
      "\n",
      "progress: 5/876\n",
      "10029\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [122901  85786]\n",
      "level_dim [122901  85786]\n",
      "name 10029\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10029.h5: total of 187 batches\n",
      "batch 0/187, 0 files processed\n",
      "batch 20/187, 10240 files processed\n",
      "batch 40/187, 20480 files processed\n",
      "batch 60/187, 30720 files processed\n",
      "batch 80/187, 40960 files processed\n",
      "batch 100/187, 51200 files processed\n",
      "batch 120/187, 61440 files processed\n",
      "batch 140/187, 71680 files processed\n",
      "batch 160/187, 81920 files processed\n",
      "batch 180/187, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10029.h5 took 126.94789266586304 s\n",
      "features size:  (95684, 1024)\n",
      "coordinates size:  (95684, 2)\n",
      "\n",
      "progress: 6/876\n",
      "10030\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [109459  85767]\n",
      "level_dim [109459  85767]\n",
      "name 10030\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10030.h5: total of 178 batches\n",
      "batch 0/178, 0 files processed\n",
      "batch 20/178, 10240 files processed\n",
      "batch 40/178, 20480 files processed\n",
      "batch 60/178, 30720 files processed\n",
      "batch 80/178, 40960 files processed\n",
      "batch 100/178, 51200 files processed\n",
      "batch 120/178, 61440 files processed\n",
      "batch 140/178, 71680 files processed\n",
      "batch 160/178, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10030.h5 took 158.25150537490845 s\n",
      "features size:  (90986, 1024)\n",
      "coordinates size:  (90986, 2)\n",
      "\n",
      "progress: 7/876\n",
      "10037\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [106578  83204]\n",
      "level_dim [106578  83204]\n",
      "name 10037\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10037.h5: total of 184 batches\n",
      "batch 0/184, 0 files processed\n",
      "batch 20/184, 10240 files processed\n",
      "batch 40/184, 20480 files processed\n",
      "batch 60/184, 30720 files processed\n",
      "batch 80/184, 40960 files processed\n",
      "batch 100/184, 51200 files processed\n",
      "batch 120/184, 61440 files processed\n",
      "batch 140/184, 71680 files processed\n",
      "batch 160/184, 81920 files processed\n",
      "batch 180/184, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10037.h5 took 105.48523163795471 s\n",
      "features size:  (94044, 1024)\n",
      "coordinates size:  (94044, 2)\n",
      "\n",
      "progress: 8/876\n",
      "10046\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [102737  85501]\n",
      "level_dim [102737  85501]\n",
      "name 10046\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10046.h5: total of 138 batches\n",
      "batch 0/138, 0 files processed\n",
      "batch 20/138, 10240 files processed\n",
      "batch 40/138, 20480 files processed\n",
      "batch 60/138, 30720 files processed\n",
      "batch 80/138, 40960 files processed\n",
      "batch 100/138, 51200 files processed\n",
      "batch 120/138, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10046.h5 took 118.02365779876709 s\n",
      "features size:  (70216, 1024)\n",
      "coordinates size:  (70216, 2)\n",
      "\n",
      "progress: 9/876\n",
      "10110\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [136343  89386]\n",
      "level_dim [136343  89386]\n",
      "name 10110\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10110.h5: total of 258 batches\n",
      "batch 0/258, 0 files processed\n",
      "batch 20/258, 10240 files processed\n",
      "batch 40/258, 20480 files processed\n",
      "batch 60/258, 30720 files processed\n",
      "batch 80/258, 40960 files processed\n",
      "batch 100/258, 51200 files processed\n",
      "batch 120/258, 61440 files processed\n",
      "batch 140/258, 71680 files processed\n",
      "batch 160/258, 81920 files processed\n",
      "batch 180/258, 92160 files processed\n",
      "batch 200/258, 102400 files processed\n",
      "batch 220/258, 112640 files processed\n",
      "batch 240/258, 122880 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10110.h5 took 212.8313970565796 s\n",
      "features size:  (131707, 1024)\n",
      "coordinates size:  (131707, 2)\n",
      "\n",
      "progress: 10/876\n",
      "10111\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [126742  89373]\n",
      "level_dim [126742  89373]\n",
      "name 10111\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10111.h5: total of 213 batches\n",
      "batch 0/213, 0 files processed\n",
      "batch 20/213, 10240 files processed\n",
      "batch 40/213, 20480 files processed\n",
      "batch 60/213, 30720 files processed\n",
      "batch 80/213, 40960 files processed\n",
      "batch 100/213, 51200 files processed\n",
      "batch 120/213, 61440 files processed\n",
      "batch 140/213, 71680 files processed\n",
      "batch 160/213, 81920 files processed\n",
      "batch 180/213, 92160 files processed\n",
      "batch 200/213, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10111.h5 took 173.3807053565979 s\n",
      "features size:  (109007, 1024)\n",
      "coordinates size:  (109007, 2)\n",
      "\n",
      "progress: 11/876\n",
      "10112\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [67211 53980]\n",
      "level_dim [67211 53980]\n",
      "name 10112\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10112.h5: total of 41 batches\n",
      "batch 0/41, 0 files processed\n",
      "batch 20/41, 10240 files processed\n",
      "batch 40/41, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10112.h5 took 25.03554677963257 s\n",
      "features size:  (20784, 1024)\n",
      "coordinates size:  (20784, 2)\n",
      "\n",
      "progress: 12/876\n",
      "10113\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [74893 75740]\n",
      "level_dim [74893 75740]\n",
      "name 10113\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10113.h5: total of 63 batches\n",
      "batch 0/63, 0 files processed\n",
      "batch 20/63, 10240 files processed\n",
      "batch 40/63, 20480 files processed\n",
      "batch 60/63, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10113.h5 took 51.82001209259033 s\n",
      "features size:  (31843, 1024)\n",
      "coordinates size:  (31843, 2)\n",
      "\n",
      "progress: 13/876\n",
      "10114\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [79693 65257]\n",
      "level_dim [79693 65257]\n",
      "name 10114\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10114.h5: total of 48 batches\n",
      "batch 0/48, 0 files processed\n",
      "batch 20/48, 10240 files processed\n",
      "batch 40/48, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10114.h5 took 38.254749059677124 s\n",
      "features size:  (24334, 1024)\n",
      "coordinates size:  (24334, 2)\n",
      "\n",
      "progress: 14/876\n",
      "10115\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96016 89075]\n",
      "level_dim [96016 89075]\n",
      "name 10115\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10115.h5: total of 87 batches\n",
      "batch 0/87, 0 files processed\n",
      "batch 20/87, 10240 files processed\n",
      "batch 40/87, 20480 files processed\n",
      "batch 60/87, 30720 files processed\n",
      "batch 80/87, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10115.h5 took 76.9270281791687 s\n",
      "features size:  (44396, 1024)\n",
      "coordinates size:  (44396, 2)\n",
      "\n",
      "progress: 15/876\n",
      "10116\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [59530 41945]\n",
      "level_dim [59530 41945]\n",
      "name 10116\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10116.h5: total of 45 batches\n",
      "batch 0/45, 0 files processed\n",
      "batch 20/45, 10240 files processed\n",
      "batch 40/45, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10116.h5 took 32.03086972236633 s\n",
      "features size:  (22843, 1024)\n",
      "coordinates size:  (22843, 2)\n",
      "\n",
      "progress: 16/876\n",
      "10117\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [66251 77774]\n",
      "level_dim [66251 77774]\n",
      "name 10117\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10117.h5: total of 56 batches\n",
      "batch 0/56, 0 files processed\n",
      "batch 20/56, 10240 files processed\n",
      "batch 40/56, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10117.h5 took 44.68699264526367 s\n",
      "features size:  (28461, 1024)\n",
      "coordinates size:  (28461, 2)\n",
      "\n",
      "progress: 17/876\n",
      "10118\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [65291 45279]\n",
      "level_dim [65291 45279]\n",
      "name 10118\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10118.h5: total of 23 batches\n",
      "batch 0/23, 0 files processed\n",
      "batch 20/23, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10118.h5 took 16.95414447784424 s\n",
      "features size:  (11295, 1024)\n",
      "coordinates size:  (11295, 2)\n",
      "\n",
      "progress: 18/876\n",
      "10119\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [14402 14248]\n",
      "level_dim [14402 14248]\n",
      "name 10119\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10119.h5: total of 2 batches\n",
      "batch 0/2, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10119.h5 took 3.2369320392608643 s\n",
      "features size:  (986, 1024)\n",
      "coordinates size:  (986, 2)\n",
      "\n",
      "progress: 19/876\n",
      "10120\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [99857 77310]\n",
      "level_dim [99857 77310]\n",
      "name 10120\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10120.h5: total of 90 batches\n",
      "batch 0/90, 0 files processed\n",
      "batch 20/90, 10240 files processed\n",
      "batch 40/90, 20480 files processed\n",
      "batch 60/90, 30720 files processed\n",
      "batch 80/90, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10120.h5 took 70.02227330207825 s\n",
      "features size:  (45968, 1024)\n",
      "coordinates size:  (45968, 2)\n",
      "\n",
      "progress: 20/876\n",
      "10121\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [67211 70356]\n",
      "level_dim [67211 70356]\n",
      "name 10121\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10121.h5: total of 66 batches\n",
      "batch 0/66, 0 files processed\n",
      "batch 20/66, 10240 files processed\n",
      "batch 40/66, 20480 files processed\n",
      "batch 60/66, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10121.h5 took 50.273030281066895 s\n",
      "features size:  (33680, 1024)\n",
      "coordinates size:  (33680, 2)\n",
      "\n",
      "progress: 21/876\n",
      "10122\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [86415 84200]\n",
      "level_dim [86415 84200]\n",
      "name 10122\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10122.h5: total of 137 batches\n",
      "batch 0/137, 0 files processed\n",
      "batch 20/137, 10240 files processed\n",
      "batch 40/137, 20480 files processed\n",
      "batch 60/137, 30720 files processed\n",
      "batch 80/137, 40960 files processed\n",
      "batch 100/137, 51200 files processed\n",
      "batch 120/137, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10122.h5 took 102.23762083053589 s\n",
      "features size:  (69676, 1024)\n",
      "coordinates size:  (69676, 2)\n",
      "\n",
      "progress: 22/876\n",
      "10125\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [86415 80617]\n",
      "level_dim [86415 80617]\n",
      "name 10125\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10125.h5: total of 89 batches\n",
      "batch 0/89, 0 files processed\n",
      "batch 20/89, 10240 files processed\n",
      "batch 40/89, 20480 files processed\n",
      "batch 60/89, 30720 files processed\n",
      "batch 80/89, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10125.h5 took 70.78990483283997 s\n",
      "features size:  (45480, 1024)\n",
      "coordinates size:  (45480, 2)\n",
      "\n",
      "progress: 23/876\n",
      "10126\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [76813 79836]\n",
      "level_dim [76813 79836]\n",
      "name 10126\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10126.h5: total of 94 batches\n",
      "batch 0/94, 0 files processed\n",
      "batch 20/94, 10240 files processed\n",
      "batch 40/94, 20480 files processed\n",
      "batch 60/94, 30720 files processed\n",
      "batch 80/94, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10126.h5 took 55.24679160118103 s\n",
      "features size:  (47662, 1024)\n",
      "coordinates size:  (47662, 2)\n",
      "\n",
      "progress: 24/876\n",
      "10127\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [124821  89371]\n",
      "level_dim [124821  89371]\n",
      "name 10127\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10127.h5: total of 221 batches\n",
      "batch 0/221, 0 files processed\n",
      "batch 20/221, 10240 files processed\n",
      "batch 40/221, 20480 files processed\n",
      "batch 60/221, 30720 files processed\n",
      "batch 80/221, 40960 files processed\n",
      "batch 100/221, 51200 files processed\n",
      "batch 120/221, 61440 files processed\n",
      "batch 140/221, 71680 files processed\n",
      "batch 160/221, 81920 files processed\n",
      "batch 180/221, 92160 files processed\n",
      "batch 200/221, 102400 files processed\n",
      "batch 220/221, 112640 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10127.h5 took 166.5227210521698 s\n",
      "features size:  (112924, 1024)\n",
      "coordinates size:  (112924, 2)\n",
      "\n",
      "progress: 25/876\n",
      "10130\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [109459  89349]\n",
      "level_dim [109459  89349]\n",
      "name 10130\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10130.h5: total of 205 batches\n",
      "batch 0/205, 0 files processed\n",
      "batch 20/205, 10240 files processed\n",
      "batch 40/205, 20480 files processed\n",
      "batch 60/205, 30720 files processed\n",
      "batch 80/205, 40960 files processed\n",
      "batch 100/205, 51200 files processed\n",
      "batch 120/205, 61440 files processed\n",
      "batch 140/205, 71680 files processed\n",
      "batch 160/205, 81920 files processed\n",
      "batch 180/205, 92160 files processed\n",
      "batch 200/205, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10130.h5 took 165.51224994659424 s\n",
      "features size:  (104501, 1024)\n",
      "coordinates size:  (104501, 2)\n",
      "\n",
      "progress: 26/876\n",
      "10131\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [95056 80118]\n",
      "level_dim [95056 80118]\n",
      "name 10131\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10131.h5: total of 87 batches\n",
      "batch 0/87, 0 files processed\n",
      "batch 20/87, 10240 files processed\n",
      "batch 40/87, 20480 files processed\n",
      "batch 60/87, 30720 files processed\n",
      "batch 80/87, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10131.h5 took 69.42690515518188 s\n",
      "features size:  (44502, 1024)\n",
      "coordinates size:  (44502, 2)\n",
      "\n",
      "progress: 27/876\n",
      "10132\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [74893 63202]\n",
      "level_dim [74893 63202]\n",
      "name 10132\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10132.h5: total of 79 batches\n",
      "batch 0/79, 0 files processed\n",
      "batch 20/79, 10240 files processed\n",
      "batch 40/79, 20480 files processed\n",
      "batch 60/79, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10132.h5 took 41.7105278968811 s\n",
      "features size:  (40427, 1024)\n",
      "coordinates size:  (40427, 2)\n",
      "\n",
      "progress: 28/876\n",
      "10133\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [102737  89340]\n",
      "level_dim [102737  89340]\n",
      "name 10133\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10133.h5: total of 163 batches\n",
      "batch 0/163, 0 files processed\n",
      "batch 20/163, 10240 files processed\n",
      "batch 40/163, 20480 files processed\n",
      "batch 60/163, 30720 files processed\n",
      "batch 80/163, 40960 files processed\n",
      "batch 100/163, 51200 files processed\n",
      "batch 120/163, 61440 files processed\n",
      "batch 140/163, 71680 files processed\n",
      "batch 160/163, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10133.h5 took 127.8023853302002 s\n",
      "features size:  (83441, 1024)\n",
      "coordinates size:  (83441, 2)\n",
      "\n",
      "progress: 29/876\n",
      "10134\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [83534 78566]\n",
      "level_dim [83534 78566]\n",
      "name 10134\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10134.h5: total of 112 batches\n",
      "batch 0/112, 0 files processed\n",
      "batch 20/112, 10240 files processed\n",
      "batch 40/112, 20480 files processed\n",
      "batch 60/112, 30720 files processed\n",
      "batch 80/112, 40960 files processed\n",
      "batch 100/112, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10134.h5 took 90.1258294582367 s\n",
      "features size:  (57081, 1024)\n",
      "coordinates size:  (57081, 2)\n",
      "\n",
      "progress: 30/876\n",
      "10135\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [93136 88303]\n",
      "level_dim [93136 88303]\n",
      "name 10135\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10135.h5: total of 110 batches\n",
      "batch 0/110, 0 files processed\n",
      "batch 20/110, 10240 files processed\n",
      "batch 40/110, 20480 files processed\n",
      "batch 60/110, 30720 files processed\n",
      "batch 80/110, 40960 files processed\n",
      "batch 100/110, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10135.h5 took 81.658038854599 s\n",
      "features size:  (56063, 1024)\n",
      "coordinates size:  (56063, 2)\n",
      "\n",
      "progress: 31/876\n",
      "10136\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [56649 64968]\n",
      "level_dim [56649 64968]\n",
      "name 10136\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10136.h5: total of 57 batches\n",
      "batch 0/57, 0 files processed\n",
      "batch 20/57, 10240 files processed\n",
      "batch 40/57, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10136.h5 took 41.5237021446228 s\n",
      "features size:  (29104, 1024)\n",
      "coordinates size:  (29104, 2)\n",
      "\n",
      "progress: 32/876\n",
      "10137\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [125781  84766]\n",
      "level_dim [125781  84766]\n",
      "name 10137\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10137.h5: total of 162 batches\n",
      "batch 0/162, 0 files processed\n",
      "batch 20/162, 10240 files processed\n",
      "batch 40/162, 20480 files processed\n",
      "batch 60/162, 30720 files processed\n",
      "batch 80/162, 40960 files processed\n",
      "batch 100/162, 51200 files processed\n",
      "batch 120/162, 61440 files processed\n",
      "batch 140/162, 71680 files processed\n",
      "batch 160/162, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10137.h5 took 130.97542023658752 s\n",
      "features size:  (82561, 1024)\n",
      "coordinates size:  (82561, 2)\n",
      "\n",
      "progress: 33/876\n",
      "10138\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [117140  89360]\n",
      "level_dim [117140  89360]\n",
      "name 10138\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10138.h5: total of 212 batches\n",
      "batch 0/212, 0 files processed\n",
      "batch 20/212, 10240 files processed\n",
      "batch 40/212, 20480 files processed\n",
      "batch 60/212, 30720 files processed\n",
      "batch 80/212, 40960 files processed\n",
      "batch 100/212, 51200 files processed\n",
      "batch 120/212, 61440 files processed\n",
      "batch 140/212, 71680 files processed\n",
      "batch 160/212, 81920 files processed\n",
      "batch 180/212, 92160 files processed\n",
      "batch 200/212, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10138.h5 took 179.11873745918274 s\n",
      "features size:  (108188, 1024)\n",
      "coordinates size:  (108188, 2)\n",
      "\n",
      "progress: 34/876\n",
      "10139\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [114259  89356]\n",
      "level_dim [114259  89356]\n",
      "name 10139\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10139.h5: total of 181 batches\n",
      "batch 0/181, 0 files processed\n",
      "batch 20/181, 10240 files processed\n",
      "batch 40/181, 20480 files processed\n",
      "batch 60/181, 30720 files processed\n",
      "batch 80/181, 40960 files processed\n",
      "batch 100/181, 51200 files processed\n",
      "batch 120/181, 61440 files processed\n",
      "batch 140/181, 71680 files processed\n",
      "batch 160/181, 81920 files processed\n",
      "batch 180/181, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10139.h5 took 99.63841366767883 s\n",
      "features size:  (92296, 1024)\n",
      "coordinates size:  (92296, 2)\n",
      "\n",
      "progress: 35/876\n",
      "10144\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [77773 43249]\n",
      "level_dim [77773 43249]\n",
      "name 10144\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10144.h5: total of 48 batches\n",
      "batch 0/48, 0 files processed\n",
      "batch 20/48, 10240 files processed\n",
      "batch 40/48, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10144.h5 took 34.55044221878052 s\n",
      "features size:  (24429, 1024)\n",
      "coordinates size:  (24429, 2)\n",
      "\n",
      "progress: 36/876\n",
      "10145\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [91215 62457]\n",
      "level_dim [91215 62457]\n",
      "name 10145\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10145.h5: total of 78 batches\n",
      "batch 0/78, 0 files processed\n",
      "batch 20/78, 10240 files processed\n",
      "batch 40/78, 20480 files processed\n",
      "batch 60/78, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10145.h5 took 60.05455780029297 s\n",
      "features size:  (39609, 1024)\n",
      "coordinates size:  (39609, 2)\n",
      "\n",
      "progress: 37/876\n",
      "10146\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [56649 42452]\n",
      "level_dim [56649 42452]\n",
      "name 10146\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10146.h5: total of 42 batches\n",
      "batch 0/42, 0 files processed\n",
      "batch 20/42, 10240 files processed\n",
      "batch 40/42, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10146.h5 took 35.100799322128296 s\n",
      "features size:  (21059, 1024)\n",
      "coordinates size:  (21059, 2)\n",
      "\n",
      "progress: 38/876\n",
      "10150\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [119060  69405]\n",
      "level_dim [119060  69405]\n",
      "name 10150\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10150.h5: total of 131 batches\n",
      "batch 0/131, 0 files processed\n",
      "batch 20/131, 10240 files processed\n",
      "batch 40/131, 20480 files processed\n",
      "batch 60/131, 30720 files processed\n",
      "batch 80/131, 40960 files processed\n",
      "batch 100/131, 51200 files processed\n",
      "batch 120/131, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10150.h5 took 105.82976961135864 s\n",
      "features size:  (66630, 1024)\n",
      "coordinates size:  (66630, 2)\n",
      "\n",
      "progress: 39/876\n",
      "10152\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [107538  84230]\n",
      "level_dim [107538  84230]\n",
      "name 10152\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10152.h5: total of 145 batches\n",
      "batch 0/145, 0 files processed\n",
      "batch 20/145, 10240 files processed\n",
      "batch 40/145, 20480 files processed\n",
      "batch 60/145, 30720 files processed\n",
      "batch 80/145, 40960 files processed\n",
      "batch 100/145, 51200 files processed\n",
      "batch 120/145, 61440 files processed\n",
      "batch 140/145, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10152.h5 took 81.38354587554932 s\n",
      "features size:  (73833, 1024)\n",
      "coordinates size:  (73833, 2)\n",
      "\n",
      "progress: 40/876\n",
      "10153\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [64331 56536]\n",
      "level_dim [64331 56536]\n",
      "name 10153\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10153.h5: total of 36 batches\n",
      "batch 0/36, 0 files processed\n",
      "batch 20/36, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10153.h5 took 24.510527849197388 s\n",
      "features size:  (18154, 1024)\n",
      "coordinates size:  (18154, 2)\n",
      "\n",
      "progress: 41/876\n",
      "10160\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [88335 78829]\n",
      "level_dim [88335 78829]\n",
      "name 10160\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10160.h5: total of 111 batches\n",
      "batch 0/111, 0 files processed\n",
      "batch 20/111, 10240 files processed\n",
      "batch 40/111, 20480 files processed\n",
      "batch 60/111, 30720 files processed\n",
      "batch 80/111, 40960 files processed\n",
      "batch 100/111, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10160.h5 took 70.57485365867615 s\n",
      "features size:  (56384, 1024)\n",
      "coordinates size:  (56384, 2)\n",
      "\n",
      "progress: 42/876\n",
      "10161\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [117140  89360]\n",
      "level_dim [117140  89360]\n",
      "name 10161\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10161.h5: total of 221 batches\n",
      "batch 0/221, 0 files processed\n",
      "batch 20/221, 10240 files processed\n",
      "batch 40/221, 20480 files processed\n",
      "batch 60/221, 30720 files processed\n",
      "batch 80/221, 40960 files processed\n",
      "batch 100/221, 51200 files processed\n",
      "batch 120/221, 61440 files processed\n",
      "batch 140/221, 71680 files processed\n",
      "batch 160/221, 81920 files processed\n",
      "batch 180/221, 92160 files processed\n",
      "batch 200/221, 102400 files processed\n",
      "batch 220/221, 112640 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10161.h5 took 126.11337542533875 s\n",
      "features size:  (112792, 1024)\n",
      "coordinates size:  (112792, 2)\n",
      "\n",
      "progress: 43/876\n",
      "10162\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [101777  89339]\n",
      "level_dim [101777  89339]\n",
      "name 10162\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10162.h5: total of 189 batches\n",
      "batch 0/189, 0 files processed\n",
      "batch 20/189, 10240 files processed\n",
      "batch 40/189, 20480 files processed\n",
      "batch 60/189, 30720 files processed\n",
      "batch 80/189, 40960 files processed\n",
      "batch 100/189, 51200 files processed\n",
      "batch 120/189, 61440 files processed\n",
      "batch 140/189, 71680 files processed\n",
      "batch 160/189, 81920 files processed\n",
      "batch 180/189, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10162.h5 took 111.5048553943634 s\n",
      "features size:  (96419, 1024)\n",
      "coordinates size:  (96419, 2)\n",
      "\n",
      "progress: 44/876\n",
      "10166\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [144985  89399]\n",
      "level_dim [144985  89399]\n",
      "name 10166\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10166.h5: total of 149 batches\n",
      "batch 0/149, 0 files processed\n",
      "batch 20/149, 10240 files processed\n",
      "batch 40/149, 20480 files processed\n",
      "batch 60/149, 30720 files processed\n",
      "batch 80/149, 40960 files processed\n",
      "batch 100/149, 51200 files processed\n",
      "batch 120/149, 61440 files processed\n",
      "batch 140/149, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10166.h5 took 118.32234930992126 s\n",
      "features size:  (75846, 1024)\n",
      "coordinates size:  (75846, 2)\n",
      "\n",
      "progress: 45/876\n",
      "10168\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [129622  89377]\n",
      "level_dim [129622  89377]\n",
      "name 10168\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10168.h5: total of 167 batches\n",
      "batch 0/167, 0 files processed\n",
      "batch 20/167, 10240 files processed\n",
      "batch 40/167, 20480 files processed\n",
      "batch 60/167, 30720 files processed\n",
      "batch 80/167, 40960 files processed\n",
      "batch 100/167, 51200 files processed\n",
      "batch 120/167, 61440 files processed\n",
      "batch 140/167, 71680 files processed\n",
      "batch 160/167, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10168.h5 took 136.32331490516663 s\n",
      "features size:  (85039, 1024)\n",
      "coordinates size:  (85039, 2)\n",
      "\n",
      "progress: 46/876\n",
      "10177\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [29765 26806]\n",
      "level_dim [29765 26806]\n",
      "name 10177\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10177.h5: total of 4 batches\n",
      "batch 0/4, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10177.h5 took 4.114027738571167 s\n",
      "features size:  (2005, 1024)\n",
      "coordinates size:  (2005, 2)\n",
      "\n",
      "progress: 47/876\n",
      "10183\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [86415 72174]\n",
      "level_dim [86415 72174]\n",
      "name 10183\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10183.h5: total of 52 batches\n",
      "batch 0/52, 0 files processed\n",
      "batch 20/52, 10240 files processed\n",
      "batch 40/52, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10183.h5 took 40.1171293258667 s\n",
      "features size:  (26259, 1024)\n",
      "coordinates size:  (26259, 2)\n",
      "\n",
      "progress: 48/876\n",
      "10184\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [82574 43768]\n",
      "level_dim [82574 43768]\n",
      "name 10184\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10184.h5: total of 46 batches\n",
      "batch 0/46, 0 files processed\n",
      "batch 20/46, 10240 files processed\n",
      "batch 40/46, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10184.h5 took 38.42119359970093 s\n",
      "features size:  (23439, 1024)\n",
      "coordinates size:  (23439, 2)\n",
      "\n",
      "progress: 49/876\n",
      "10352\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [110419  71184]\n",
      "level_dim [110419  71184]\n",
      "name 10352\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10352.h5: total of 129 batches\n",
      "batch 0/129, 0 files processed\n",
      "batch 20/129, 10240 files processed\n",
      "batch 40/129, 20480 files processed\n",
      "batch 60/129, 30720 files processed\n",
      "batch 80/129, 40960 files processed\n",
      "batch 100/129, 51200 files processed\n",
      "batch 120/129, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10352.h5 took 117.96103096008301 s\n",
      "features size:  (65736, 1024)\n",
      "coordinates size:  (65736, 2)\n",
      "\n",
      "progress: 50/876\n",
      "10364\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [127702  84513]\n",
      "level_dim [127702  84513]\n",
      "name 10364\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10364.h5: total of 174 batches\n",
      "batch 0/174, 0 files processed\n",
      "batch 20/174, 10240 files processed\n",
      "batch 40/174, 20480 files processed\n",
      "batch 60/174, 30720 files processed\n",
      "batch 80/174, 40960 files processed\n",
      "batch 100/174, 51200 files processed\n",
      "batch 120/174, 61440 files processed\n",
      "batch 140/174, 71680 files processed\n",
      "batch 160/174, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10364.h5 took 110.24713706970215 s\n",
      "features size:  (88847, 1024)\n",
      "coordinates size:  (88847, 2)\n",
      "\n",
      "progress: 51/876\n",
      "10365\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [112339  81422]\n",
      "level_dim [112339  81422]\n",
      "name 10365\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10365.h5: total of 177 batches\n",
      "batch 0/177, 0 files processed\n",
      "batch 20/177, 10240 files processed\n",
      "batch 40/177, 20480 files processed\n",
      "batch 60/177, 30720 files processed\n",
      "batch 80/177, 40960 files processed\n",
      "batch 100/177, 51200 files processed\n",
      "batch 120/177, 61440 files processed\n",
      "batch 140/177, 71680 files processed\n",
      "batch 160/177, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10365.h5 took 104.78226709365845 s\n",
      "features size:  (90312, 1024)\n",
      "coordinates size:  (90312, 2)\n",
      "\n",
      "progress: 52/876\n",
      "10367\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [101777  86267]\n",
      "level_dim [101777  86267]\n",
      "name 10367\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10367.h5: total of 159 batches\n",
      "batch 0/159, 0 files processed\n",
      "batch 20/159, 10240 files processed\n",
      "batch 40/159, 20480 files processed\n",
      "batch 60/159, 30720 files processed\n",
      "batch 80/159, 40960 files processed\n",
      "batch 100/159, 51200 files processed\n",
      "batch 120/159, 61440 files processed\n",
      "batch 140/159, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10367.h5 took 132.09028029441833 s\n",
      "features size:  (81211, 1024)\n",
      "coordinates size:  (81211, 2)\n",
      "\n",
      "progress: 53/876\n",
      "10368\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [114259  89356]\n",
      "level_dim [114259  89356]\n",
      "name 10368\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10368.h5: total of 166 batches\n",
      "batch 0/166, 0 files processed\n",
      "batch 20/166, 10240 files processed\n",
      "batch 40/166, 20480 files processed\n",
      "batch 60/166, 30720 files processed\n",
      "batch 80/166, 40960 files processed\n",
      "batch 100/166, 51200 files processed\n",
      "batch 120/166, 61440 files processed\n",
      "batch 140/166, 71680 files processed\n",
      "batch 160/166, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10368.h5 took 99.19308066368103 s\n",
      "features size:  (84706, 1024)\n",
      "coordinates size:  (84706, 2)\n",
      "\n",
      "progress: 54/876\n",
      "10370\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [110419  82953]\n",
      "level_dim [110419  82953]\n",
      "name 10370\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10370.h5: total of 140 batches\n",
      "batch 0/140, 0 files processed\n",
      "batch 20/140, 10240 files processed\n",
      "batch 40/140, 20480 files processed\n",
      "batch 60/140, 30720 files processed\n",
      "batch 80/140, 40960 files processed\n",
      "batch 100/140, 51200 files processed\n",
      "batch 120/140, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10370.h5 took 93.1974356174469 s\n",
      "features size:  (71287, 1024)\n",
      "coordinates size:  (71287, 2)\n",
      "\n",
      "progress: 55/876\n",
      "10372\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [84494 84964]\n",
      "level_dim [84494 84964]\n",
      "name 10372\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10372.h5: total of 133 batches\n",
      "batch 0/133, 0 files processed\n",
      "batch 20/133, 10240 files processed\n",
      "batch 40/133, 20480 files processed\n",
      "batch 60/133, 30720 files processed\n",
      "batch 80/133, 40960 files processed\n",
      "batch 100/133, 51200 files processed\n",
      "batch 120/133, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10372.h5 took 73.61580014228821 s\n",
      "features size:  (67928, 1024)\n",
      "coordinates size:  (67928, 2)\n",
      "\n",
      "progress: 56/876\n",
      "10373\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [116180  87312]\n",
      "level_dim [116180  87312]\n",
      "name 10373\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10373.h5: total of 224 batches\n",
      "batch 0/224, 0 files processed\n",
      "batch 20/224, 10240 files processed\n",
      "batch 40/224, 20480 files processed\n",
      "batch 60/224, 30720 files processed\n",
      "batch 80/224, 40960 files processed\n",
      "batch 100/224, 51200 files processed\n",
      "batch 120/224, 61440 files processed\n",
      "batch 140/224, 71680 files processed\n",
      "batch 160/224, 81920 files processed\n",
      "batch 180/224, 92160 files processed\n",
      "batch 200/224, 102400 files processed\n",
      "batch 220/224, 112640 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10373.h5 took 175.98553252220154 s\n",
      "features size:  (114415, 1024)\n",
      "coordinates size:  (114415, 2)\n",
      "\n",
      "progress: 57/876\n",
      "10374\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [133463  75054]\n",
      "level_dim [133463  75054]\n",
      "name 10374\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10374.h5: total of 181 batches\n",
      "batch 0/181, 0 files processed\n",
      "batch 20/181, 10240 files processed\n",
      "batch 40/181, 20480 files processed\n",
      "batch 60/181, 30720 files processed\n",
      "batch 80/181, 40960 files processed\n",
      "batch 100/181, 51200 files processed\n",
      "batch 120/181, 61440 files processed\n",
      "batch 140/181, 71680 files processed\n",
      "batch 160/181, 81920 files processed\n",
      "batch 180/181, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10374.h5 took 136.59949588775635 s\n",
      "features size:  (92620, 1024)\n",
      "coordinates size:  (92620, 2)\n",
      "\n",
      "progress: 58/876\n",
      "10375\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [98897 82682]\n",
      "level_dim [98897 82682]\n",
      "name 10375\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10375.h5: total of 148 batches\n",
      "batch 0/148, 0 files processed\n",
      "batch 20/148, 10240 files processed\n",
      "batch 40/148, 20480 files processed\n",
      "batch 60/148, 30720 files processed\n",
      "batch 80/148, 40960 files processed\n",
      "batch 100/148, 51200 files processed\n",
      "batch 120/148, 61440 files processed\n",
      "batch 140/148, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10375.h5 took 83.61182498931885 s\n",
      "features size:  (75277, 1024)\n",
      "coordinates size:  (75277, 2)\n",
      "\n",
      "progress: 59/876\n",
      "10376\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [103698  74245]\n",
      "level_dim [103698  74245]\n",
      "name 10376\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10376.h5: total of 135 batches\n",
      "batch 0/135, 0 files processed\n",
      "batch 20/135, 10240 files processed\n",
      "batch 40/135, 20480 files processed\n",
      "batch 60/135, 30720 files processed\n",
      "batch 80/135, 40960 files processed\n",
      "batch 100/135, 51200 files processed\n",
      "batch 120/135, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10376.h5 took 111.67528653144836 s\n",
      "features size:  (68802, 1024)\n",
      "coordinates size:  (68802, 2)\n",
      "\n",
      "progress: 60/876\n",
      "10377\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [121941  85273]\n",
      "level_dim [121941  85273]\n",
      "name 10377\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10377.h5: total of 143 batches\n",
      "batch 0/143, 0 files processed\n",
      "batch 20/143, 10240 files processed\n",
      "batch 40/143, 20480 files processed\n",
      "batch 60/143, 30720 files processed\n",
      "batch 80/143, 40960 files processed\n",
      "batch 100/143, 51200 files processed\n",
      "batch 120/143, 61440 files processed\n",
      "batch 140/143, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10377.h5 took 73.82319211959839 s\n",
      "features size:  (73205, 1024)\n",
      "coordinates size:  (73205, 2)\n",
      "\n",
      "progress: 61/876\n",
      "10378\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [89295 73969]\n",
      "level_dim [89295 73969]\n",
      "name 10378\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10378.h5: total of 107 batches\n",
      "batch 0/107, 0 files processed\n",
      "batch 20/107, 10240 files processed\n",
      "batch 40/107, 20480 files processed\n",
      "batch 60/107, 30720 files processed\n",
      "batch 80/107, 40960 files processed\n",
      "batch 100/107, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10378.h5 took 87.86822295188904 s\n",
      "features size:  (54697, 1024)\n",
      "coordinates size:  (54697, 2)\n",
      "\n",
      "progress: 62/876\n",
      "10379\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [104658  82690]\n",
      "level_dim [104658  82690]\n",
      "name 10379\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10379.h5: total of 136 batches\n",
      "batch 0/136, 0 files processed\n",
      "batch 20/136, 10240 files processed\n",
      "batch 40/136, 20480 files processed\n",
      "batch 60/136, 30720 files processed\n",
      "batch 80/136, 40960 files processed\n",
      "batch 100/136, 51200 files processed\n",
      "batch 120/136, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10379.h5 took 117.379075050354 s\n",
      "features size:  (69219, 1024)\n",
      "coordinates size:  (69219, 2)\n",
      "\n",
      "progress: 63/876\n",
      "10380\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [130582  89378]\n",
      "level_dim [130582  89378]\n",
      "name 10380\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10380.h5: total of 170 batches\n",
      "batch 0/170, 0 files processed\n",
      "batch 20/170, 10240 files processed\n",
      "batch 40/170, 20480 files processed\n",
      "batch 60/170, 30720 files processed\n",
      "batch 80/170, 40960 files processed\n",
      "batch 100/170, 51200 files processed\n",
      "batch 120/170, 61440 files processed\n",
      "batch 140/170, 71680 files processed\n",
      "batch 160/170, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10380.h5 took 140.04703617095947 s\n",
      "features size:  (86870, 1024)\n",
      "coordinates size:  (86870, 2)\n",
      "\n",
      "progress: 64/876\n",
      "10381\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [90255 81390]\n",
      "level_dim [90255 81390]\n",
      "name 10381\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10381.h5: total of 125 batches\n",
      "batch 0/125, 0 files processed\n",
      "batch 20/125, 10240 files processed\n",
      "batch 40/125, 20480 files processed\n",
      "batch 60/125, 30720 files processed\n",
      "batch 80/125, 40960 files processed\n",
      "batch 100/125, 51200 files processed\n",
      "batch 120/125, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10381.h5 took 104.76325488090515 s\n",
      "features size:  (63846, 1024)\n",
      "coordinates size:  (63846, 2)\n",
      "\n",
      "progress: 65/876\n",
      "10382\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [120020  80664]\n",
      "level_dim [120020  80664]\n",
      "name 10382\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10382.h5: total of 170 batches\n",
      "batch 0/170, 0 files processed\n",
      "batch 20/170, 10240 files processed\n",
      "batch 40/170, 20480 files processed\n",
      "batch 60/170, 30720 files processed\n",
      "batch 80/170, 40960 files processed\n",
      "batch 100/170, 51200 files processed\n",
      "batch 120/170, 61440 files processed\n",
      "batch 140/170, 71680 files processed\n",
      "batch 160/170, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10382.h5 took 144.88982701301575 s\n",
      "features size:  (86660, 1024)\n",
      "coordinates size:  (86660, 2)\n",
      "\n",
      "progress: 66/876\n",
      "10383\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [107538  89346]\n",
      "level_dim [107538  89346]\n",
      "name 10383\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10383.h5: total of 178 batches\n",
      "batch 0/178, 0 files processed\n",
      "batch 20/178, 10240 files processed\n",
      "batch 40/178, 20480 files processed\n",
      "batch 60/178, 30720 files processed\n",
      "batch 80/178, 40960 files processed\n",
      "batch 100/178, 51200 files processed\n",
      "batch 120/178, 61440 files processed\n",
      "batch 140/178, 71680 files processed\n",
      "batch 160/178, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10383.h5 took 144.23292303085327 s\n",
      "features size:  (90633, 1024)\n",
      "coordinates size:  (90633, 2)\n",
      "\n",
      "progress: 67/876\n",
      "10384\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [54729 43217]\n",
      "level_dim [54729 43217]\n",
      "name 10384\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10384.h5: total of 32 batches\n",
      "batch 0/32, 0 files processed\n",
      "batch 20/32, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10384.h5 took 26.298502445220947 s\n",
      "features size:  (16309, 1024)\n",
      "coordinates size:  (16309, 2)\n",
      "\n",
      "progress: 68/876\n",
      "10385\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [106578  89345]\n",
      "level_dim [106578  89345]\n",
      "name 10385\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10385.h5: total of 249 batches\n",
      "batch 0/249, 0 files processed\n",
      "batch 20/249, 10240 files processed\n",
      "batch 40/249, 20480 files processed\n",
      "batch 60/249, 30720 files processed\n",
      "batch 80/249, 40960 files processed\n",
      "batch 100/249, 51200 files processed\n",
      "batch 120/249, 61440 files processed\n",
      "batch 140/249, 71680 files processed\n",
      "batch 160/249, 81920 files processed\n",
      "batch 180/249, 92160 files processed\n",
      "batch 200/249, 102400 files processed\n",
      "batch 220/249, 112640 files processed\n",
      "batch 240/249, 122880 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10385.h5 took 187.23151898384094 s\n",
      "features size:  (127016, 1024)\n",
      "coordinates size:  (127016, 2)\n",
      "\n",
      "progress: 69/876\n",
      "10386\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [116180  89358]\n",
      "level_dim [116180  89358]\n",
      "name 10386\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10386.h5: total of 263 batches\n",
      "batch 0/263, 0 files processed\n",
      "batch 20/263, 10240 files processed\n",
      "batch 40/263, 20480 files processed\n",
      "batch 60/263, 30720 files processed\n",
      "batch 80/263, 40960 files processed\n",
      "batch 100/263, 51200 files processed\n",
      "batch 120/263, 61440 files processed\n",
      "batch 140/263, 71680 files processed\n",
      "batch 160/263, 81920 files processed\n",
      "batch 180/263, 92160 files processed\n",
      "batch 200/263, 102400 files processed\n",
      "batch 220/263, 112640 files processed\n",
      "batch 240/263, 122880 files processed\n",
      "batch 260/263, 133120 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10386.h5 took 138.38511109352112 s\n",
      "features size:  (134441, 1024)\n",
      "coordinates size:  (134441, 2)\n",
      "\n",
      "progress: 70/876\n",
      "10387\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [106578  89345]\n",
      "level_dim [106578  89345]\n",
      "name 10387\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10387.h5: total of 210 batches\n",
      "batch 0/210, 0 files processed\n",
      "batch 20/210, 10240 files processed\n",
      "batch 40/210, 20480 files processed\n",
      "batch 60/210, 30720 files processed\n",
      "batch 80/210, 40960 files processed\n",
      "batch 100/210, 51200 files processed\n",
      "batch 120/210, 61440 files processed\n",
      "batch 140/210, 71680 files processed\n",
      "batch 160/210, 81920 files processed\n",
      "batch 180/210, 92160 files processed\n",
      "batch 200/210, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10387.h5 took 110.07589054107666 s\n",
      "features size:  (107187, 1024)\n",
      "coordinates size:  (107187, 2)\n",
      "\n",
      "progress: 71/876\n",
      "10388\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [118100  89361]\n",
      "level_dim [118100  89361]\n",
      "name 10388\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10388.h5: total of 198 batches\n",
      "batch 0/198, 0 files processed\n",
      "batch 20/198, 10240 files processed\n",
      "batch 40/198, 20480 files processed\n",
      "batch 60/198, 30720 files processed\n",
      "batch 80/198, 40960 files processed\n",
      "batch 100/198, 51200 files processed\n",
      "batch 120/198, 61440 files processed\n",
      "batch 140/198, 71680 files processed\n",
      "batch 160/198, 81920 files processed\n",
      "batch 180/198, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10388.h5 took 158.05820989608765 s\n",
      "features size:  (100953, 1024)\n",
      "coordinates size:  (100953, 2)\n",
      "\n",
      "progress: 72/876\n",
      "10389\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [123861  70179]\n",
      "level_dim [123861  70179]\n",
      "name 10389\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10389.h5: total of 178 batches\n",
      "batch 0/178, 0 files processed\n",
      "batch 20/178, 10240 files processed\n",
      "batch 40/178, 20480 files processed\n",
      "batch 60/178, 30720 files processed\n",
      "batch 80/178, 40960 files processed\n",
      "batch 100/178, 51200 files processed\n",
      "batch 120/178, 61440 files processed\n",
      "batch 140/178, 71680 files processed\n",
      "batch 160/178, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10389.h5 took 146.7182261943817 s\n",
      "features size:  (91099, 1024)\n",
      "coordinates size:  (91099, 2)\n",
      "\n",
      "progress: 73/876\n",
      "10390\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [87375 77037]\n",
      "level_dim [87375 77037]\n",
      "name 10390\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10390.h5: total of 89 batches\n",
      "batch 0/89, 0 files processed\n",
      "batch 20/89, 10240 files processed\n",
      "batch 40/89, 20480 files processed\n",
      "batch 60/89, 30720 files processed\n",
      "batch 80/89, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10390.h5 took 59.56889224052429 s\n",
      "features size:  (45446, 1024)\n",
      "coordinates size:  (45446, 2)\n",
      "\n",
      "progress: 74/876\n",
      "10391\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [116180  89358]\n",
      "level_dim [116180  89358]\n",
      "name 10391\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10391.h5: total of 182 batches\n",
      "batch 0/182, 0 files processed\n",
      "batch 20/182, 10240 files processed\n",
      "batch 40/182, 20480 files processed\n",
      "batch 60/182, 30720 files processed\n",
      "batch 80/182, 40960 files processed\n",
      "batch 100/182, 51200 files processed\n",
      "batch 120/182, 61440 files processed\n",
      "batch 140/182, 71680 files processed\n",
      "batch 160/182, 81920 files processed\n",
      "batch 180/182, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10391.h5 took 99.93189549446106 s\n",
      "features size:  (93129, 1024)\n",
      "coordinates size:  (93129, 2)\n",
      "\n",
      "progress: 75/876\n",
      "10392\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [89295 73713]\n",
      "level_dim [89295 73713]\n",
      "name 10392\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10392.h5: total of 131 batches\n",
      "batch 0/131, 0 files processed\n",
      "batch 20/131, 10240 files processed\n",
      "batch 40/131, 20480 files processed\n",
      "batch 60/131, 30720 files processed\n",
      "batch 80/131, 40960 files processed\n",
      "batch 100/131, 51200 files processed\n",
      "batch 120/131, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10392.h5 took 111.34361457824707 s\n",
      "features size:  (66818, 1024)\n",
      "coordinates size:  (66818, 2)\n",
      "\n",
      "progress: 76/876\n",
      "10393\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [87375 88039]\n",
      "level_dim [87375 88039]\n",
      "name 10393\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10393.h5: total of 93 batches\n",
      "batch 0/93, 0 files processed\n",
      "batch 20/93, 10240 files processed\n",
      "batch 40/93, 20480 files processed\n",
      "batch 60/93, 30720 files processed\n",
      "batch 80/93, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10393.h5 took 73.33346819877625 s\n",
      "features size:  (47239, 1024)\n",
      "coordinates size:  (47239, 2)\n",
      "\n",
      "progress: 77/876\n",
      "10400\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [98897 63236]\n",
      "level_dim [98897 63236]\n",
      "name 10400\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10400.h5: total of 57 batches\n",
      "batch 0/57, 0 files processed\n",
      "batch 20/57, 10240 files processed\n",
      "batch 40/57, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10400.h5 took 40.94292449951172 s\n",
      "features size:  (28919, 1024)\n",
      "coordinates size:  (28919, 2)\n",
      "\n",
      "progress: 78/876\n",
      "10401\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [108498  81928]\n",
      "level_dim [108498  81928]\n",
      "name 10401\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10401.h5: total of 139 batches\n",
      "batch 0/139, 0 files processed\n",
      "batch 20/139, 10240 files processed\n",
      "batch 40/139, 20480 files processed\n",
      "batch 60/139, 30720 files processed\n",
      "batch 80/139, 40960 files processed\n",
      "batch 100/139, 51200 files processed\n",
      "batch 120/139, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10401.h5 took 100.7962281703949 s\n",
      "features size:  (70941, 1024)\n",
      "coordinates size:  (70941, 2)\n",
      "\n",
      "progress: 79/876\n",
      "10402\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [95056 73465]\n",
      "level_dim [95056 73465]\n",
      "name 10402\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10402.h5: total of 113 batches\n",
      "batch 0/113, 0 files processed\n",
      "batch 20/113, 10240 files processed\n",
      "batch 40/113, 20480 files processed\n",
      "batch 60/113, 30720 files processed\n",
      "batch 80/113, 40960 files processed\n",
      "batch 100/113, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10402.h5 took 97.47008538246155 s\n",
      "features size:  (57810, 1024)\n",
      "coordinates size:  (57810, 2)\n",
      "\n",
      "progress: 80/876\n",
      "10403\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96016 62721]\n",
      "level_dim [96016 62721]\n",
      "name 10403\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10403.h5: total of 93 batches\n",
      "batch 0/93, 0 files processed\n",
      "batch 20/93, 10240 files processed\n",
      "batch 40/93, 20480 files processed\n",
      "batch 60/93, 30720 files processed\n",
      "batch 80/93, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10403.h5 took 79.87362217903137 s\n",
      "features size:  (47575, 1024)\n",
      "coordinates size:  (47575, 2)\n",
      "\n",
      "progress: 81/876\n",
      "10404\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [106578  78854]\n",
      "level_dim [106578  78854]\n",
      "name 10404\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10404.h5: total of 164 batches\n",
      "batch 0/164, 0 files processed\n",
      "batch 20/164, 10240 files processed\n",
      "batch 40/164, 20480 files processed\n",
      "batch 60/164, 30720 files processed\n",
      "batch 80/164, 40960 files processed\n",
      "batch 100/164, 51200 files processed\n",
      "batch 120/164, 61440 files processed\n",
      "batch 140/164, 71680 files processed\n",
      "batch 160/164, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10404.h5 took 123.3143699169159 s\n",
      "features size:  (83935, 1024)\n",
      "coordinates size:  (83935, 2)\n",
      "\n",
      "progress: 82/876\n",
      "10408\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [116180  82961]\n",
      "level_dim [116180  82961]\n",
      "name 10408\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10408.h5: total of 162 batches\n",
      "batch 0/162, 0 files processed\n",
      "batch 20/162, 10240 files processed\n",
      "batch 40/162, 20480 files processed\n",
      "batch 60/162, 30720 files processed\n",
      "batch 80/162, 40960 files processed\n",
      "batch 100/162, 51200 files processed\n",
      "batch 120/162, 61440 files processed\n",
      "batch 140/162, 71680 files processed\n",
      "batch 160/162, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10408.h5 took 128.32384252548218 s\n",
      "features size:  (82685, 1024)\n",
      "coordinates size:  (82685, 2)\n",
      "\n",
      "progress: 83/876\n",
      "10409\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [139224  81203]\n",
      "level_dim [139224  81203]\n",
      "name 10409\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10409.h5: total of 189 batches\n",
      "batch 0/189, 0 files processed\n",
      "batch 20/189, 10240 files processed\n",
      "batch 40/189, 20480 files processed\n",
      "batch 60/189, 30720 files processed\n",
      "batch 80/189, 40960 files processed\n",
      "batch 100/189, 51200 files processed\n",
      "batch 120/189, 61440 files processed\n",
      "batch 140/189, 71680 files processed\n",
      "batch 160/189, 81920 files processed\n",
      "batch 180/189, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10409.h5 took 153.02645778656006 s\n",
      "features size:  (96545, 1024)\n",
      "coordinates size:  (96545, 2)\n",
      "\n",
      "progress: 84/876\n",
      "10410\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [91215 89324]\n",
      "level_dim [91215 89324]\n",
      "name 10410\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10410.h5: total of 109 batches\n",
      "batch 0/109, 0 files processed\n",
      "batch 20/109, 10240 files processed\n",
      "batch 40/109, 20480 files processed\n",
      "batch 60/109, 30720 files processed\n",
      "batch 80/109, 40960 files processed\n",
      "batch 100/109, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10410.h5 took 58.574541091918945 s\n",
      "features size:  (55328, 1024)\n",
      "coordinates size:  (55328, 2)\n",
      "\n",
      "progress: 85/876\n",
      "10411\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [58570 46549]\n",
      "level_dim [58570 46549]\n",
      "name 10411\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10411.h5: total of 42 batches\n",
      "batch 0/42, 0 files processed\n",
      "batch 20/42, 10240 files processed\n",
      "batch 40/42, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10411.h5 took 37.391279220581055 s\n",
      "features size:  (21335, 1024)\n",
      "coordinates size:  (21335, 2)\n",
      "\n",
      "progress: 86/876\n",
      "10412\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [107538  75018]\n",
      "level_dim [107538  75018]\n",
      "name 10412\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10412.h5: total of 108 batches\n",
      "batch 0/108, 0 files processed\n",
      "batch 20/108, 10240 files processed\n",
      "batch 40/108, 20480 files processed\n",
      "batch 60/108, 30720 files processed\n",
      "batch 80/108, 40960 files processed\n",
      "batch 100/108, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10412.h5 took 82.14237809181213 s\n",
      "features size:  (54934, 1024)\n",
      "coordinates size:  (54934, 2)\n",
      "\n",
      "progress: 87/876\n",
      "10414\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [126742  89373]\n",
      "level_dim [126742  89373]\n",
      "name 10414\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10414.h5: total of 219 batches\n",
      "batch 0/219, 0 files processed\n",
      "batch 20/219, 10240 files processed\n",
      "batch 40/219, 20480 files processed\n",
      "batch 60/219, 30720 files processed\n",
      "batch 80/219, 40960 files processed\n",
      "batch 100/219, 51200 files processed\n",
      "batch 120/219, 61440 files processed\n",
      "batch 140/219, 71680 files processed\n",
      "batch 160/219, 81920 files processed\n",
      "batch 180/219, 92160 files processed\n",
      "batch 200/219, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10414.h5 took 173.76334285736084 s\n",
      "features size:  (111809, 1024)\n",
      "coordinates size:  (111809, 2)\n",
      "\n",
      "progress: 88/876\n",
      "10415\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [124821  81438]\n",
      "level_dim [124821  81438]\n",
      "name 10415\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10415.h5: total of 171 batches\n",
      "batch 0/171, 0 files processed\n",
      "batch 20/171, 10240 files processed\n",
      "batch 40/171, 20480 files processed\n",
      "batch 60/171, 30720 files processed\n",
      "batch 80/171, 40960 files processed\n",
      "batch 100/171, 51200 files processed\n",
      "batch 120/171, 61440 files processed\n",
      "batch 140/171, 71680 files processed\n",
      "batch 160/171, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10415.h5 took 142.9223871231079 s\n",
      "features size:  (87384, 1024)\n",
      "coordinates size:  (87384, 2)\n",
      "\n",
      "progress: 89/876\n",
      "10416\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [114259  85517]\n",
      "level_dim [114259  85517]\n",
      "name 10416\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10416.h5: total of 171 batches\n",
      "batch 0/171, 0 files processed\n",
      "batch 20/171, 10240 files processed\n",
      "batch 40/171, 20480 files processed\n",
      "batch 60/171, 30720 files processed\n",
      "batch 80/171, 40960 files processed\n",
      "batch 100/171, 51200 files processed\n",
      "batch 120/171, 61440 files processed\n",
      "batch 140/171, 71680 files processed\n",
      "batch 160/171, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10416.h5 took 136.22314047813416 s\n",
      "features size:  (87078, 1024)\n",
      "coordinates size:  (87078, 2)\n",
      "\n",
      "progress: 90/876\n",
      "10417\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [87375 61429]\n",
      "level_dim [87375 61429]\n",
      "name 10417\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10417.h5: total of 92 batches\n",
      "batch 0/92, 0 files processed\n",
      "batch 20/92, 10240 files processed\n",
      "batch 40/92, 20480 files processed\n",
      "batch 60/92, 30720 files processed\n",
      "batch 80/92, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10417.h5 took 76.35665535926819 s\n",
      "features size:  (46717, 1024)\n",
      "coordinates size:  (46717, 2)\n",
      "\n",
      "progress: 91/876\n",
      "10418\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [68171 59612]\n",
      "level_dim [68171 59612]\n",
      "name 10418\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10418.h5: total of 76 batches\n",
      "batch 0/76, 0 files processed\n",
      "batch 20/76, 10240 files processed\n",
      "batch 40/76, 20480 files processed\n",
      "batch 60/76, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10418.h5 took 59.90024971961975 s\n",
      "features size:  (38894, 1024)\n",
      "coordinates size:  (38894, 2)\n",
      "\n",
      "progress: 92/876\n",
      "10419\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [92176 80370]\n",
      "level_dim [92176 80370]\n",
      "name 10419\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10419.h5: total of 126 batches\n",
      "batch 0/126, 0 files processed\n",
      "batch 20/126, 10240 files processed\n",
      "batch 40/126, 20480 files processed\n",
      "batch 60/126, 30720 files processed\n",
      "batch 80/126, 40960 files processed\n",
      "batch 100/126, 51200 files processed\n",
      "batch 120/126, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10419.h5 took 107.01175475120544 s\n",
      "features size:  (64237, 1024)\n",
      "coordinates size:  (64237, 2)\n",
      "\n",
      "progress: 93/876\n",
      "10420\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [94096 65021]\n",
      "level_dim [94096 65021]\n",
      "name 10420\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10420.h5: total of 105 batches\n",
      "batch 0/105, 0 files processed\n",
      "batch 20/105, 10240 files processed\n",
      "batch 40/105, 20480 files processed\n",
      "batch 60/105, 30720 files processed\n",
      "batch 80/105, 40960 files processed\n",
      "batch 100/105, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10420.h5 took 88.76582455635071 s\n",
      "features size:  (53437, 1024)\n",
      "coordinates size:  (53437, 2)\n",
      "\n",
      "progress: 94/876\n",
      "10422\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [93136 79603]\n",
      "level_dim [93136 79603]\n",
      "name 10422\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10422.h5: total of 44 batches\n",
      "batch 0/44, 0 files processed\n",
      "batch 20/44, 10240 files processed\n",
      "batch 40/44, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10422.h5 took 35.42988729476929 s\n",
      "features size:  (22493, 1024)\n",
      "coordinates size:  (22493, 2)\n",
      "\n",
      "progress: 95/876\n",
      "10424\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [82574 74472]\n",
      "level_dim [82574 74472]\n",
      "name 10424\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10424.h5: total of 76 batches\n",
      "batch 0/76, 0 files processed\n",
      "batch 20/76, 10240 files processed\n",
      "batch 40/76, 20480 files processed\n",
      "batch 60/76, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10424.h5 took 67.81093859672546 s\n",
      "features size:  (38486, 1024)\n",
      "coordinates size:  (38486, 2)\n",
      "\n",
      "progress: 96/876\n",
      "10443\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [88335 89320]\n",
      "level_dim [88335 89320]\n",
      "name 10443\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10443.h5: total of 126 batches\n",
      "batch 0/126, 0 files processed\n",
      "batch 20/126, 10240 files processed\n",
      "batch 40/126, 20480 files processed\n",
      "batch 60/126, 30720 files processed\n",
      "batch 80/126, 40960 files processed\n",
      "batch 100/126, 51200 files processed\n",
      "batch 120/126, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10443.h5 took 104.11744666099548 s\n",
      "features size:  (64040, 1024)\n",
      "coordinates size:  (64040, 2)\n",
      "\n",
      "progress: 97/876\n",
      "10458\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [94096 79604]\n",
      "level_dim [94096 79604]\n",
      "name 10458\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10458.h5: total of 139 batches\n",
      "batch 0/139, 0 files processed\n",
      "batch 20/139, 10240 files processed\n",
      "batch 40/139, 20480 files processed\n",
      "batch 60/139, 30720 files processed\n",
      "batch 80/139, 40960 files processed\n",
      "batch 100/139, 51200 files processed\n",
      "batch 120/139, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10458.h5 took 108.76342296600342 s\n",
      "features size:  (71114, 1024)\n",
      "coordinates size:  (71114, 2)\n",
      "\n",
      "progress: 98/876\n",
      "10460\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [114259  89356]\n",
      "level_dim [114259  89356]\n",
      "name 10460\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10460.h5: total of 168 batches\n",
      "batch 0/168, 0 files processed\n",
      "batch 20/168, 10240 files processed\n",
      "batch 40/168, 20480 files processed\n",
      "batch 60/168, 30720 files processed\n",
      "batch 80/168, 40960 files processed\n",
      "batch 100/168, 51200 files processed\n",
      "batch 120/168, 61440 files processed\n",
      "batch 140/168, 71680 files processed\n",
      "batch 160/168, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10460.h5 took 98.56881785392761 s\n",
      "features size:  (85730, 1024)\n",
      "coordinates size:  (85730, 2)\n",
      "\n",
      "progress: 99/876\n",
      "10461\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [115220  89102]\n",
      "level_dim [115220  89102]\n",
      "name 10461\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10461.h5: total of 169 batches\n",
      "batch 0/169, 0 files processed\n",
      "batch 20/169, 10240 files processed\n",
      "batch 40/169, 20480 files processed\n",
      "batch 60/169, 30720 files processed\n",
      "batch 80/169, 40960 files processed\n",
      "batch 100/169, 51200 files processed\n",
      "batch 120/169, 61440 files processed\n",
      "batch 140/169, 71680 files processed\n",
      "batch 160/169, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10461.h5 took 136.21208024024963 s\n",
      "features size:  (86019, 1024)\n",
      "coordinates size:  (86019, 2)\n",
      "\n",
      "progress: 100/876\n",
      "10481\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [88335 74480]\n",
      "level_dim [88335 74480]\n",
      "name 10481\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10481.h5: total of 97 batches\n",
      "batch 0/97, 0 files processed\n",
      "batch 20/97, 10240 files processed\n",
      "batch 40/97, 20480 files processed\n",
      "batch 60/97, 30720 files processed\n",
      "batch 80/97, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10481.h5 took 79.76391196250916 s\n",
      "features size:  (49338, 1024)\n",
      "coordinates size:  (49338, 2)\n",
      "\n",
      "progress: 101/876\n",
      "10482\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [102737  82175]\n",
      "level_dim [102737  82175]\n",
      "name 10482\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10482.h5: total of 106 batches\n",
      "batch 0/106, 0 files processed\n",
      "batch 20/106, 10240 files processed\n",
      "batch 40/106, 20480 files processed\n",
      "batch 60/106, 30720 files processed\n",
      "batch 80/106, 40960 files processed\n",
      "batch 100/106, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10482.h5 took 84.6203453540802 s\n",
      "features size:  (54254, 1024)\n",
      "coordinates size:  (54254, 2)\n",
      "\n",
      "progress: 102/876\n",
      "10483\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [117140  63774]\n",
      "level_dim [117140  63774]\n",
      "name 10483\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10483.h5: total of 120 batches\n",
      "batch 0/120, 0 files processed\n",
      "batch 20/120, 10240 files processed\n",
      "batch 40/120, 20480 files processed\n",
      "batch 60/120, 30720 files processed\n",
      "batch 80/120, 40960 files processed\n",
      "batch 100/120, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10483.h5 took 93.75362372398376 s\n",
      "features size:  (61324, 1024)\n",
      "coordinates size:  (61324, 2)\n",
      "\n",
      "progress: 103/876\n",
      "10484\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [79693 72932]\n",
      "level_dim [79693 72932]\n",
      "name 10484\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10484.h5: total of 92 batches\n",
      "batch 0/92, 0 files processed\n",
      "batch 20/92, 10240 files processed\n",
      "batch 40/92, 20480 files processed\n",
      "batch 60/92, 30720 files processed\n",
      "batch 80/92, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10484.h5 took 76.63731908798218 s\n",
      "features size:  (47086, 1024)\n",
      "coordinates size:  (47086, 2)\n",
      "\n",
      "progress: 104/876\n",
      "10485\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [95056 68092]\n",
      "level_dim [95056 68092]\n",
      "name 10485\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10485.h5: total of 96 batches\n",
      "batch 0/96, 0 files processed\n",
      "batch 20/96, 10240 files processed\n",
      "batch 40/96, 20480 files processed\n",
      "batch 60/96, 30720 files processed\n",
      "batch 80/96, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10485.h5 took 54.204808473587036 s\n",
      "features size:  (48702, 1024)\n",
      "coordinates size:  (48702, 2)\n",
      "\n",
      "progress: 105/876\n",
      "10486\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [76813 74975]\n",
      "level_dim [76813 74975]\n",
      "name 10486\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10486.h5: total of 100 batches\n",
      "batch 0/100, 0 files processed\n",
      "batch 20/100, 10240 files processed\n",
      "batch 40/100, 20480 files processed\n",
      "batch 60/100, 30720 files processed\n",
      "batch 80/100, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10486.h5 took 67.26237940788269 s\n",
      "features size:  (50718, 1024)\n",
      "coordinates size:  (50718, 2)\n",
      "\n",
      "progress: 106/876\n",
      "10487\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [118100  89361]\n",
      "level_dim [118100  89361]\n",
      "name 10487\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10487.h5: total of 171 batches\n",
      "batch 0/171, 0 files processed\n",
      "batch 20/171, 10240 files processed\n",
      "batch 40/171, 20480 files processed\n",
      "batch 60/171, 30720 files processed\n",
      "batch 80/171, 40960 files processed\n",
      "batch 100/171, 51200 files processed\n",
      "batch 120/171, 61440 files processed\n",
      "batch 140/171, 71680 files processed\n",
      "batch 160/171, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10487.h5 took 140.9283127784729 s\n",
      "features size:  (87156, 1024)\n",
      "coordinates size:  (87156, 2)\n",
      "\n",
      "progress: 107/876\n",
      "10488\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [87375 71407]\n",
      "level_dim [87375 71407]\n",
      "name 10488\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10488.h5: total of 70 batches\n",
      "batch 0/70, 0 files processed\n",
      "batch 20/70, 10240 files processed\n",
      "batch 40/70, 20480 files processed\n",
      "batch 60/70, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10488.h5 took 52.43937945365906 s\n",
      "features size:  (35676, 1024)\n",
      "coordinates size:  (35676, 2)\n",
      "\n",
      "progress: 108/876\n",
      "10489\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [86415 84199]\n",
      "level_dim [86415 84199]\n",
      "name 10489\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10489.h5: total of 102 batches\n",
      "batch 0/102, 0 files processed\n",
      "batch 20/102, 10240 files processed\n",
      "batch 40/102, 20480 files processed\n",
      "batch 60/102, 30720 files processed\n",
      "batch 80/102, 40960 files processed\n",
      "batch 100/102, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10489.h5 took 73.85392832756042 s\n",
      "features size:  (51784, 1024)\n",
      "coordinates size:  (51784, 2)\n",
      "\n",
      "progress: 109/876\n",
      "10490\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [76813 80604]\n",
      "level_dim [76813 80604]\n",
      "name 10490\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10490.h5: total of 91 batches\n",
      "batch 0/91, 0 files processed\n",
      "batch 20/91, 10240 files processed\n",
      "batch 40/91, 20480 files processed\n",
      "batch 60/91, 30720 files processed\n",
      "batch 80/91, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10490.h5 took 75.65198349952698 s\n",
      "features size:  (46088, 1024)\n",
      "coordinates size:  (46088, 2)\n",
      "\n",
      "progress: 110/876\n",
      "10491\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [99857 77055]\n",
      "level_dim [99857 77055]\n",
      "name 10491\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10491.h5: total of 114 batches\n",
      "batch 0/114, 0 files processed\n",
      "batch 20/114, 10240 files processed\n",
      "batch 40/114, 20480 files processed\n",
      "batch 60/114, 30720 files processed\n",
      "batch 80/114, 40960 files processed\n",
      "batch 100/114, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10491.h5 took 82.61661076545715 s\n",
      "features size:  (58271, 1024)\n",
      "coordinates size:  (58271, 2)\n",
      "\n",
      "progress: 111/876\n",
      "10492\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [67211 77265]\n",
      "level_dim [67211 77265]\n",
      "name 10492\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10492.h5: total of 71 batches\n",
      "batch 0/71, 0 files processed\n",
      "batch 20/71, 10240 files processed\n",
      "batch 40/71, 20480 files processed\n",
      "batch 60/71, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10492.h5 took 57.7312273979187 s\n",
      "features size:  (36043, 1024)\n",
      "coordinates size:  (36043, 2)\n",
      "\n",
      "progress: 112/876\n",
      "10493\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [104658  81155]\n",
      "level_dim [104658  81155]\n",
      "name 10493\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10493.h5: total of 110 batches\n",
      "batch 0/110, 0 files processed\n",
      "batch 20/110, 10240 files processed\n",
      "batch 40/110, 20480 files processed\n",
      "batch 60/110, 30720 files processed\n",
      "batch 80/110, 40960 files processed\n",
      "batch 100/110, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10493.h5 took 85.56266188621521 s\n",
      "features size:  (55981, 1024)\n",
      "coordinates size:  (55981, 2)\n",
      "\n",
      "progress: 113/876\n",
      "10494\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [68171 49120]\n",
      "level_dim [68171 49120]\n",
      "name 10494\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10494.h5: total of 62 batches\n",
      "batch 0/62, 0 files processed\n",
      "batch 20/62, 10240 files processed\n",
      "batch 40/62, 20480 files processed\n",
      "batch 60/62, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10494.h5 took 57.65751910209656 s\n",
      "features size:  (31498, 1024)\n",
      "coordinates size:  (31498, 2)\n",
      "\n",
      "progress: 114/876\n",
      "10495\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [75853 68577]\n",
      "level_dim [75853 68577]\n",
      "name 10495\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10495.h5: total of 60 batches\n",
      "batch 0/60, 0 files processed\n",
      "batch 20/60, 10240 files processed\n",
      "batch 40/60, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10495.h5 took 45.7066969871521 s\n",
      "features size:  (30246, 1024)\n",
      "coordinates size:  (30246, 2)\n",
      "\n",
      "progress: 115/876\n",
      "10497\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [72012 76760]\n",
      "level_dim [72012 76760]\n",
      "name 10497\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10497.h5: total of 71 batches\n",
      "batch 0/71, 0 files processed\n",
      "batch 20/71, 10240 files processed\n",
      "batch 40/71, 20480 files processed\n",
      "batch 60/71, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10497.h5 took 53.61375665664673 s\n",
      "features size:  (36234, 1024)\n",
      "coordinates size:  (36234, 2)\n",
      "\n",
      "progress: 116/876\n",
      "10498\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [98897 79867]\n",
      "level_dim [98897 79867]\n",
      "name 10498\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10498.h5: total of 118 batches\n",
      "batch 0/118, 0 files processed\n",
      "batch 20/118, 10240 files processed\n",
      "batch 40/118, 20480 files processed\n",
      "batch 60/118, 30720 files processed\n",
      "batch 80/118, 40960 files processed\n",
      "batch 100/118, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10498.h5 took 85.14970231056213 s\n",
      "features size:  (60180, 1024)\n",
      "coordinates size:  (60180, 2)\n",
      "\n",
      "progress: 117/876\n",
      "10499\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [68171 61658]\n",
      "level_dim [68171 61658]\n",
      "name 10499\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10499.h5: total of 55 batches\n",
      "batch 0/55, 0 files processed\n",
      "batch 20/55, 10240 files processed\n",
      "batch 40/55, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10499.h5 took 47.1935715675354 s\n",
      "features size:  (27717, 1024)\n",
      "coordinates size:  (27717, 2)\n",
      "\n",
      "progress: 118/876\n",
      "10500\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [131542  79145]\n",
      "level_dim [131542  79145]\n",
      "name 10500\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10500.h5: total of 130 batches\n",
      "batch 0/130, 0 files processed\n",
      "batch 20/130, 10240 files processed\n",
      "batch 40/130, 20480 files processed\n",
      "batch 60/130, 30720 files processed\n",
      "batch 80/130, 40960 files processed\n",
      "batch 100/130, 51200 files processed\n",
      "batch 120/130, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10500.h5 took 101.50368642807007 s\n",
      "features size:  (66399, 1024)\n",
      "coordinates size:  (66399, 2)\n",
      "\n",
      "progress: 119/876\n",
      "10501\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [107538  77321]\n",
      "level_dim [107538  77321]\n",
      "name 10501\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10501.h5: total of 189 batches\n",
      "batch 0/189, 0 files processed\n",
      "batch 20/189, 10240 files processed\n",
      "batch 40/189, 20480 files processed\n",
      "batch 60/189, 30720 files processed\n",
      "batch 80/189, 40960 files processed\n",
      "batch 100/189, 51200 files processed\n",
      "batch 120/189, 61440 files processed\n",
      "batch 140/189, 71680 files processed\n",
      "batch 160/189, 81920 files processed\n",
      "batch 180/189, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10501.h5 took 118.73933005332947 s\n",
      "features size:  (96295, 1024)\n",
      "coordinates size:  (96295, 2)\n",
      "\n",
      "progress: 120/876\n",
      "10502\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [36486 37818]\n",
      "level_dim [36486 37818]\n",
      "name 10502\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10502.h5: total of 23 batches\n",
      "batch 0/23, 0 files processed\n",
      "batch 20/23, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10502.h5 took 22.66139245033264 s\n",
      "features size:  (11513, 1024)\n",
      "coordinates size:  (11513, 2)\n",
      "\n",
      "progress: 121/876\n",
      "10503\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [94096 89327]\n",
      "level_dim [94096 89327]\n",
      "name 10503\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10503.h5: total of 127 batches\n",
      "batch 0/127, 0 files processed\n",
      "batch 20/127, 10240 files processed\n",
      "batch 40/127, 20480 files processed\n",
      "batch 60/127, 30720 files processed\n",
      "batch 80/127, 40960 files processed\n",
      "batch 100/127, 51200 files processed\n",
      "batch 120/127, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10503.h5 took 70.05657529830933 s\n",
      "features size:  (64541, 1024)\n",
      "coordinates size:  (64541, 2)\n",
      "\n",
      "progress: 122/876\n",
      "10504\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [42247 29895]\n",
      "level_dim [42247 29895]\n",
      "name 10504\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10504.h5: total of 10 batches\n",
      "batch 0/10, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10504.h5 took 8.134323120117188 s\n",
      "features size:  (4949, 1024)\n",
      "coordinates size:  (4949, 2)\n",
      "\n",
      "progress: 123/876\n",
      "10508\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [99857 79101]\n",
      "level_dim [99857 79101]\n",
      "name 10508\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10508.h5: total of 130 batches\n",
      "batch 0/130, 0 files processed\n",
      "batch 20/130, 10240 files processed\n",
      "batch 40/130, 20480 files processed\n",
      "batch 60/130, 30720 files processed\n",
      "batch 80/130, 40960 files processed\n",
      "batch 100/130, 51200 files processed\n",
      "batch 120/130, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10508.h5 took 104.30559086799622 s\n",
      "features size:  (66312, 1024)\n",
      "coordinates size:  (66312, 2)\n",
      "\n",
      "progress: 124/876\n",
      "10509\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [80654 59373]\n",
      "level_dim [80654 59373]\n",
      "name 10509\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10509.h5: total of 72 batches\n",
      "batch 0/72, 0 files processed\n",
      "batch 20/72, 10240 files processed\n",
      "batch 40/72, 20480 files processed\n",
      "batch 60/72, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10509.h5 took 57.500874042510986 s\n",
      "features size:  (36728, 1024)\n",
      "coordinates size:  (36728, 2)\n",
      "\n",
      "progress: 125/876\n",
      "10510\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [76813 49133]\n",
      "level_dim [76813 49133]\n",
      "name 10510\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10510.h5: total of 71 batches\n",
      "batch 0/71, 0 files processed\n",
      "batch 20/71, 10240 files processed\n",
      "batch 40/71, 20480 files processed\n",
      "batch 60/71, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10510.h5 took 56.284181118011475 s\n",
      "features size:  (35888, 1024)\n",
      "coordinates size:  (35888, 2)\n",
      "\n",
      "progress: 126/876\n",
      "10511\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [111379  76303]\n",
      "level_dim [111379  76303]\n",
      "name 10511\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10511.h5: total of 147 batches\n",
      "batch 0/147, 0 files processed\n",
      "batch 20/147, 10240 files processed\n",
      "batch 40/147, 20480 files processed\n",
      "batch 60/147, 30720 files processed\n",
      "batch 80/147, 40960 files processed\n",
      "batch 100/147, 51200 files processed\n",
      "batch 120/147, 61440 files processed\n",
      "batch 140/147, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10511.h5 took 113.65783929824829 s\n",
      "features size:  (75207, 1024)\n",
      "coordinates size:  (75207, 2)\n",
      "\n",
      "progress: 127/876\n",
      "10513\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [91215 69366]\n",
      "level_dim [91215 69366]\n",
      "name 10513\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10513.h5: total of 72 batches\n",
      "batch 0/72, 0 files processed\n",
      "batch 20/72, 10240 files processed\n",
      "batch 40/72, 20480 files processed\n",
      "batch 60/72, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10513.h5 took 52.36928129196167 s\n",
      "features size:  (36791, 1024)\n",
      "coordinates size:  (36791, 2)\n",
      "\n",
      "progress: 128/876\n",
      "10514\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [86415 78059]\n",
      "level_dim [86415 78059]\n",
      "name 10514\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10514.h5: total of 90 batches\n",
      "batch 0/90, 0 files processed\n",
      "batch 20/90, 10240 files processed\n",
      "batch 40/90, 20480 files processed\n",
      "batch 60/90, 30720 files processed\n",
      "batch 80/90, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10514.h5 took 75.85245442390442 s\n",
      "features size:  (45840, 1024)\n",
      "coordinates size:  (45840, 2)\n",
      "\n",
      "progress: 129/876\n",
      "10515\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [89295 77807]\n",
      "level_dim [89295 77807]\n",
      "name 10515\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10515.h5: total of 49 batches\n",
      "batch 0/49, 0 files processed\n",
      "batch 20/49, 10240 files processed\n",
      "batch 40/49, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10515.h5 took 36.778908491134644 s\n",
      "features size:  (25039, 1024)\n",
      "coordinates size:  (25039, 2)\n",
      "\n",
      "progress: 130/876\n",
      "10516\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [112339  84235]\n",
      "level_dim [112339  84235]\n",
      "name 10516\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10516.h5: total of 178 batches\n",
      "batch 0/178, 0 files processed\n",
      "batch 20/178, 10240 files processed\n",
      "batch 40/178, 20480 files processed\n",
      "batch 60/178, 30720 files processed\n",
      "batch 80/178, 40960 files processed\n",
      "batch 100/178, 51200 files processed\n",
      "batch 120/178, 61440 files processed\n",
      "batch 140/178, 71680 files processed\n",
      "batch 160/178, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10516.h5 took 145.91582894325256 s\n",
      "features size:  (90835, 1024)\n",
      "coordinates size:  (90835, 2)\n",
      "\n",
      "progress: 131/876\n",
      "10517\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [117140  83475]\n",
      "level_dim [117140  83475]\n",
      "name 10517\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10517.h5: total of 181 batches\n",
      "batch 0/181, 0 files processed\n",
      "batch 20/181, 10240 files processed\n",
      "batch 40/181, 20480 files processed\n",
      "batch 60/181, 30720 files processed\n",
      "batch 80/181, 40960 files processed\n",
      "batch 100/181, 51200 files processed\n",
      "batch 120/181, 61440 files processed\n",
      "batch 140/181, 71680 files processed\n",
      "batch 160/181, 81920 files processed\n",
      "batch 180/181, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10517.h5 took 106.82570314407349 s\n",
      "features size:  (92551, 1024)\n",
      "coordinates size:  (92551, 2)\n",
      "\n",
      "progress: 132/876\n",
      "10518\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [100817  85499]\n",
      "level_dim [100817  85499]\n",
      "name 10518\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10518.h5: total of 153 batches\n",
      "batch 0/153, 0 files processed\n",
      "batch 20/153, 10240 files processed\n",
      "batch 40/153, 20480 files processed\n",
      "batch 60/153, 30720 files processed\n",
      "batch 80/153, 40960 files processed\n",
      "batch 100/153, 51200 files processed\n",
      "batch 120/153, 61440 files processed\n",
      "batch 140/153, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10518.h5 took 114.29364061355591 s\n",
      "features size:  (77854, 1024)\n",
      "coordinates size:  (77854, 2)\n",
      "\n",
      "progress: 133/876\n",
      "10519\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [107538  89347]\n",
      "level_dim [107538  89347]\n",
      "name 10519\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10519.h5: total of 188 batches\n",
      "batch 0/188, 0 files processed\n",
      "batch 20/188, 10240 files processed\n",
      "batch 40/188, 20480 files processed\n",
      "batch 60/188, 30720 files processed\n",
      "batch 80/188, 40960 files processed\n",
      "batch 100/188, 51200 files processed\n",
      "batch 120/188, 61440 files processed\n",
      "batch 140/188, 71680 files processed\n",
      "batch 160/188, 81920 files processed\n",
      "batch 180/188, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10519.h5 took 143.59012579917908 s\n",
      "features size:  (96109, 1024)\n",
      "coordinates size:  (96109, 2)\n",
      "\n",
      "progress: 134/876\n",
      "10520\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [88335 73456]\n",
      "level_dim [88335 73456]\n",
      "name 10520\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10520.h5: total of 67 batches\n",
      "batch 0/67, 0 files processed\n",
      "batch 20/67, 10240 files processed\n",
      "batch 40/67, 20480 files processed\n",
      "batch 60/67, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10520.h5 took 53.18033409118652 s\n",
      "features size:  (33909, 1024)\n",
      "coordinates size:  (33909, 2)\n",
      "\n",
      "progress: 135/876\n",
      "10521\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [141144  69947]\n",
      "level_dim [141144  69947]\n",
      "name 10521\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10521.h5: total of 155 batches\n",
      "batch 0/155, 0 files processed\n",
      "batch 20/155, 10240 files processed\n",
      "batch 40/155, 20480 files processed\n",
      "batch 60/155, 30720 files processed\n",
      "batch 80/155, 40960 files processed\n",
      "batch 100/155, 51200 files processed\n",
      "batch 120/155, 61440 files processed\n",
      "batch 140/155, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10521.h5 took 120.33674812316895 s\n",
      "features size:  (79146, 1024)\n",
      "coordinates size:  (79146, 2)\n",
      "\n",
      "progress: 136/876\n",
      "10526\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [101777  55821]\n",
      "level_dim [101777  55821]\n",
      "name 10526\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10526.h5: total of 78 batches\n",
      "batch 0/78, 0 files processed\n",
      "batch 20/78, 10240 files processed\n",
      "batch 40/78, 20480 files processed\n",
      "batch 60/78, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10526.h5 took 56.79228115081787 s\n",
      "features size:  (39448, 1024)\n",
      "coordinates size:  (39448, 2)\n",
      "\n",
      "progress: 137/876\n",
      "10530\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [72972 55524]\n",
      "level_dim [72972 55524]\n",
      "name 10530\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10530.h5: total of 43 batches\n",
      "batch 0/43, 0 files processed\n",
      "batch 20/43, 10240 files processed\n",
      "batch 40/43, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10530.h5 took 31.668695211410522 s\n",
      "features size:  (21813, 1024)\n",
      "coordinates size:  (21813, 2)\n",
      "\n",
      "progress: 138/876\n",
      "10542\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [30725 34484]\n",
      "level_dim [30725 34484]\n",
      "name 10542\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10542.h5: total of 15 batches\n",
      "batch 0/15, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10542.h5 took 14.821752309799194 s\n",
      "features size:  (7484, 1024)\n",
      "coordinates size:  (7484, 2)\n",
      "\n",
      "progress: 139/876\n",
      "10617\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [114259  76307]\n",
      "level_dim [114259  76307]\n",
      "name 10617\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10617.h5: total of 122 batches\n",
      "batch 0/122, 0 files processed\n",
      "batch 20/122, 10240 files processed\n",
      "batch 40/122, 20480 files processed\n",
      "batch 60/122, 30720 files processed\n",
      "batch 80/122, 40960 files processed\n",
      "batch 100/122, 51200 files processed\n",
      "batch 120/122, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10617.h5 took 69.16536593437195 s\n",
      "features size:  (62333, 1024)\n",
      "coordinates size:  (62333, 2)\n",
      "\n",
      "progress: 140/876\n",
      "10618\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [73932 85717]\n",
      "level_dim [73932 85717]\n",
      "name 10618\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10618.h5: total of 120 batches\n",
      "batch 0/120, 0 files processed\n",
      "batch 20/120, 10240 files processed\n",
      "batch 40/120, 20480 files processed\n",
      "batch 60/120, 30720 files processed\n",
      "batch 80/120, 40960 files processed\n",
      "batch 100/120, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10618.h5 took 65.98104667663574 s\n",
      "features size:  (61410, 1024)\n",
      "coordinates size:  (61410, 2)\n",
      "\n",
      "progress: 141/876\n",
      "10619\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [67211 67286]\n",
      "level_dim [67211 67286]\n",
      "name 10619\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10619.h5: total of 69 batches\n",
      "batch 0/69, 0 files processed\n",
      "batch 20/69, 10240 files processed\n",
      "batch 40/69, 20480 files processed\n",
      "batch 60/69, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10619.h5 took 55.52948570251465 s\n",
      "features size:  (35090, 1024)\n",
      "coordinates size:  (35090, 2)\n",
      "\n",
      "progress: 142/876\n",
      "10624\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [108498  89348]\n",
      "level_dim [108498  89348]\n",
      "name 10624\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10624.h5: total of 212 batches\n",
      "batch 0/212, 0 files processed\n",
      "batch 20/212, 10240 files processed\n",
      "batch 40/212, 20480 files processed\n",
      "batch 60/212, 30720 files processed\n",
      "batch 80/212, 40960 files processed\n",
      "batch 100/212, 51200 files processed\n",
      "batch 120/212, 61440 files processed\n",
      "batch 140/212, 71680 files processed\n",
      "batch 160/212, 81920 files processed\n",
      "batch 180/212, 92160 files processed\n",
      "batch 200/212, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10624.h5 took 169.82148003578186 s\n",
      "features size:  (108180, 1024)\n",
      "coordinates size:  (108180, 2)\n",
      "\n",
      "progress: 143/876\n",
      "10636\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [58570 49107]\n",
      "level_dim [58570 49107]\n",
      "name 10636\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10636.h5: total of 41 batches\n",
      "batch 0/41, 0 files processed\n",
      "batch 20/41, 10240 files processed\n",
      "batch 40/41, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10636.h5 took 24.54476046562195 s\n",
      "features size:  (20903, 1024)\n",
      "coordinates size:  (20903, 2)\n",
      "\n",
      "progress: 144/876\n",
      "10639\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [91215 82927]\n",
      "level_dim [91215 82927]\n",
      "name 10639\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10639.h5: total of 90 batches\n",
      "batch 0/90, 0 files processed\n",
      "batch 20/90, 10240 files processed\n",
      "batch 40/90, 20480 files processed\n",
      "batch 60/90, 30720 files processed\n",
      "batch 80/90, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10639.h5 took 75.39901328086853 s\n",
      "features size:  (45831, 1024)\n",
      "coordinates size:  (45831, 2)\n",
      "\n",
      "progress: 145/876\n",
      "10640\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [73932 73947]\n",
      "level_dim [73932 73947]\n",
      "name 10640\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10640.h5: total of 72 batches\n",
      "batch 0/72, 0 files processed\n",
      "batch 20/72, 10240 files processed\n",
      "batch 40/72, 20480 files processed\n",
      "batch 60/72, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10640.h5 took 62.80169916152954 s\n",
      "features size:  (36501, 1024)\n",
      "coordinates size:  (36501, 2)\n",
      "\n",
      "progress: 146/876\n",
      "10641\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [93136 59902]\n",
      "level_dim [93136 59902]\n",
      "name 10641\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10641.h5: total of 61 batches\n",
      "batch 0/61, 0 files processed\n",
      "batch 20/61, 10240 files processed\n",
      "batch 40/61, 20480 files processed\n",
      "batch 60/61, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10641.h5 took 47.60137152671814 s\n",
      "features size:  (30976, 1024)\n",
      "coordinates size:  (30976, 2)\n",
      "\n",
      "progress: 147/876\n",
      "10643\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [85454 80616]\n",
      "level_dim [85454 80616]\n",
      "name 10643\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10643.h5: total of 122 batches\n",
      "batch 0/122, 0 files processed\n",
      "batch 20/122, 10240 files processed\n",
      "batch 40/122, 20480 files processed\n",
      "batch 60/122, 30720 files processed\n",
      "batch 80/122, 40960 files processed\n",
      "batch 100/122, 51200 files processed\n",
      "batch 120/122, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10643.h5 took 86.29183840751648 s\n",
      "features size:  (61987, 1024)\n",
      "coordinates size:  (61987, 2)\n",
      "\n",
      "progress: 148/876\n",
      "10653\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [98897 73726]\n",
      "level_dim [98897 73726]\n",
      "name 10653\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10653.h5: total of 78 batches\n",
      "batch 0/78, 0 files processed\n",
      "batch 20/78, 10240 files processed\n",
      "batch 40/78, 20480 files processed\n",
      "batch 60/78, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10653.h5 took 65.36892604827881 s\n",
      "features size:  (39452, 1024)\n",
      "coordinates size:  (39452, 2)\n",
      "\n",
      "progress: 149/876\n",
      "10654\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [114259  70422]\n",
      "level_dim [114259  70422]\n",
      "name 10654\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10654.h5: total of 56 batches\n",
      "batch 0/56, 0 files processed\n",
      "batch 20/56, 10240 files processed\n",
      "batch 40/56, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10654.h5 took 42.960638999938965 s\n",
      "features size:  (28527, 1024)\n",
      "coordinates size:  (28527, 2)\n",
      "\n",
      "progress: 150/876\n",
      "10669\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [97937 76027]\n",
      "level_dim [97937 76027]\n",
      "name 10669\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10669.h5: total of 138 batches\n",
      "batch 0/138, 0 files processed\n",
      "batch 20/138, 10240 files processed\n",
      "batch 40/138, 20480 files processed\n",
      "batch 60/138, 30720 files processed\n",
      "batch 80/138, 40960 files processed\n",
      "batch 100/138, 51200 files processed\n",
      "batch 120/138, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10669.h5 took 111.00261974334717 s\n",
      "features size:  (70263, 1024)\n",
      "coordinates size:  (70263, 2)\n",
      "\n",
      "progress: 151/876\n",
      "10670\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [86415 58869]\n",
      "level_dim [86415 58869]\n",
      "name 10670\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10670.h5: total of 70 batches\n",
      "batch 0/70, 0 files processed\n",
      "batch 20/70, 10240 files processed\n",
      "batch 40/70, 20480 files processed\n",
      "batch 60/70, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10670.h5 took 40.24732708930969 s\n",
      "features size:  (35749, 1024)\n",
      "coordinates size:  (35749, 2)\n",
      "\n",
      "progress: 152/876\n",
      "10672\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [99857 80636]\n",
      "level_dim [99857 80636]\n",
      "name 10672\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10672.h5: total of 143 batches\n",
      "batch 0/143, 0 files processed\n",
      "batch 20/143, 10240 files processed\n",
      "batch 40/143, 20480 files processed\n",
      "batch 60/143, 30720 files processed\n",
      "batch 80/143, 40960 files processed\n",
      "batch 100/143, 51200 files processed\n",
      "batch 120/143, 61440 files processed\n",
      "batch 140/143, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10672.h5 took 133.78493571281433 s\n",
      "features size:  (73090, 1024)\n",
      "coordinates size:  (73090, 2)\n",
      "\n",
      "progress: 153/876\n",
      "10673\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96016 78072]\n",
      "level_dim [96016 78072]\n",
      "name 10673\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10673.h5: total of 143 batches\n",
      "batch 0/143, 0 files processed\n",
      "batch 20/143, 10240 files processed\n",
      "batch 40/143, 20480 files processed\n",
      "batch 60/143, 30720 files processed\n",
      "batch 80/143, 40960 files processed\n",
      "batch 100/143, 51200 files processed\n",
      "batch 120/143, 61440 files processed\n",
      "batch 140/143, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10673.h5 took 103.44410920143127 s\n",
      "features size:  (72914, 1024)\n",
      "coordinates size:  (72914, 2)\n",
      "\n",
      "progress: 154/876\n",
      "10674\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [65291 64725]\n",
      "level_dim [65291 64725]\n",
      "name 10674\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10674.h5: total of 66 batches\n",
      "batch 0/66, 0 files processed\n",
      "batch 20/66, 10240 files processed\n",
      "batch 40/66, 20480 files processed\n",
      "batch 60/66, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10674.h5 took 49.976553201675415 s\n",
      "features size:  (33556, 1024)\n",
      "coordinates size:  (33556, 2)\n",
      "\n",
      "progress: 155/876\n",
      "10675\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [103698  83711]\n",
      "level_dim [103698  83711]\n",
      "name 10675\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10675.h5: total of 192 batches\n",
      "batch 0/192, 0 files processed\n",
      "batch 20/192, 10240 files processed\n",
      "batch 40/192, 20480 files processed\n",
      "batch 60/192, 30720 files processed\n",
      "batch 80/192, 40960 files processed\n",
      "batch 100/192, 51200 files processed\n",
      "batch 120/192, 61440 files processed\n",
      "batch 140/192, 71680 files processed\n",
      "batch 160/192, 81920 files processed\n",
      "batch 180/192, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10675.h5 took 97.40401005744934 s\n",
      "features size:  (97928, 1024)\n",
      "coordinates size:  (97928, 2)\n",
      "\n",
      "progress: 156/876\n",
      "10676\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [43207 57017]\n",
      "level_dim [43207 57017]\n",
      "name 10676\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10676.h5: total of 17 batches\n",
      "batch 0/17, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10676.h5 took 13.021720170974731 s\n",
      "features size:  (8361, 1024)\n",
      "coordinates size:  (8361, 2)\n",
      "\n",
      "progress: 157/876\n",
      "10678\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [95056 82421]\n",
      "level_dim [95056 82421]\n",
      "name 10678\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10678.h5: total of 166 batches\n",
      "batch 0/166, 0 files processed\n",
      "batch 20/166, 10240 files processed\n",
      "batch 40/166, 20480 files processed\n",
      "batch 60/166, 30720 files processed\n",
      "batch 80/166, 40960 files processed\n",
      "batch 100/166, 51200 files processed\n",
      "batch 120/166, 61440 files processed\n",
      "batch 140/166, 71680 files processed\n",
      "batch 160/166, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10678.h5 took 87.73029088973999 s\n",
      "features size:  (84693, 1024)\n",
      "coordinates size:  (84693, 2)\n",
      "\n",
      "progress: 158/876\n",
      "10700\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [87375 73710]\n",
      "level_dim [87375 73710]\n",
      "name 10700\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10700.h5: total of 81 batches\n",
      "batch 0/81, 0 files processed\n",
      "batch 20/81, 10240 files processed\n",
      "batch 40/81, 20480 files processed\n",
      "batch 60/81, 30720 files processed\n",
      "batch 80/81, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10700.h5 took 69.69459581375122 s\n",
      "features size:  (41278, 1024)\n",
      "coordinates size:  (41278, 2)\n",
      "\n",
      "progress: 159/876\n",
      "10701\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [118100  75544]\n",
      "level_dim [118100  75544]\n",
      "name 10701\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10701.h5: total of 156 batches\n",
      "batch 0/156, 0 files processed\n",
      "batch 20/156, 10240 files processed\n",
      "batch 40/156, 20480 files processed\n",
      "batch 60/156, 30720 files processed\n",
      "batch 80/156, 40960 files processed\n",
      "batch 100/156, 51200 files processed\n",
      "batch 120/156, 61440 files processed\n",
      "batch 140/156, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10701.h5 took 123.17617535591125 s\n",
      "features size:  (79776, 1024)\n",
      "coordinates size:  (79776, 2)\n",
      "\n",
      "progress: 160/876\n",
      "10702\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [124821  60201]\n",
      "level_dim [124821  60201]\n",
      "name 10702\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10702.h5: total of 131 batches\n",
      "batch 0/131, 0 files processed\n",
      "batch 20/131, 10240 files processed\n",
      "batch 40/131, 20480 files processed\n",
      "batch 60/131, 30720 files processed\n",
      "batch 80/131, 40960 files processed\n",
      "batch 100/131, 51200 files processed\n",
      "batch 120/131, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10702.h5 took 109.55171465873718 s\n",
      "features size:  (66775, 1024)\n",
      "coordinates size:  (66775, 2)\n",
      "\n",
      "progress: 161/876\n",
      "10837\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [78733 71139]\n",
      "level_dim [78733 71139]\n",
      "name 10837\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10837.h5: total of 95 batches\n",
      "batch 0/95, 0 files processed\n",
      "batch 20/95, 10240 files processed\n",
      "batch 40/95, 20480 files processed\n",
      "batch 60/95, 30720 files processed\n",
      "batch 80/95, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10837.h5 took 74.72612476348877 s\n",
      "features size:  (48286, 1024)\n",
      "coordinates size:  (48286, 2)\n",
      "\n",
      "progress: 162/876\n",
      "10840\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [81614 63979]\n",
      "level_dim [81614 63979]\n",
      "name 10840\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10840.h5: total of 58 batches\n",
      "batch 0/58, 0 files processed\n",
      "batch 20/58, 10240 files processed\n",
      "batch 40/58, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10840.h5 took 46.482532024383545 s\n",
      "features size:  (29685, 1024)\n",
      "coordinates size:  (29685, 2)\n",
      "\n",
      "progress: 163/876\n",
      "10844\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [62410 82887]\n",
      "level_dim [62410 82887]\n",
      "name 10844\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10844.h5: total of 89 batches\n",
      "batch 0/89, 0 files processed\n",
      "batch 20/89, 10240 files processed\n",
      "batch 40/89, 20480 files processed\n",
      "batch 60/89, 30720 files processed\n",
      "batch 80/89, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10844.h5 took 77.9233067035675 s\n",
      "features size:  (45407, 1024)\n",
      "coordinates size:  (45407, 2)\n",
      "\n",
      "progress: 164/876\n",
      "10845\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96976 69886]\n",
      "level_dim [96976 69886]\n",
      "name 10845\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10845.h5: total of 89 batches\n",
      "batch 0/89, 0 files processed\n",
      "batch 20/89, 10240 files processed\n",
      "batch 40/89, 20480 files processed\n",
      "batch 60/89, 30720 files processed\n",
      "batch 80/89, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10845.h5 took 82.59690809249878 s\n",
      "features size:  (45369, 1024)\n",
      "coordinates size:  (45369, 2)\n",
      "\n",
      "progress: 165/876\n",
      "10846\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [119060  83733]\n",
      "level_dim [119060  83733]\n",
      "name 10846\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10846.h5: total of 155 batches\n",
      "batch 0/155, 0 files processed\n",
      "batch 20/155, 10240 files processed\n",
      "batch 40/155, 20480 files processed\n",
      "batch 60/155, 30720 files processed\n",
      "batch 80/155, 40960 files processed\n",
      "batch 100/155, 51200 files processed\n",
      "batch 120/155, 61440 files processed\n",
      "batch 140/155, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10846.h5 took 120.31754994392395 s\n",
      "features size:  (79261, 1024)\n",
      "coordinates size:  (79261, 2)\n",
      "\n",
      "progress: 166/876\n",
      "10847\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96976 83703]\n",
      "level_dim [96976 83703]\n",
      "name 10847\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10847.h5: total of 165 batches\n",
      "batch 0/165, 0 files processed\n",
      "batch 20/165, 10240 files processed\n",
      "batch 40/165, 20480 files processed\n",
      "batch 60/165, 30720 files processed\n",
      "batch 80/165, 40960 files processed\n",
      "batch 100/165, 51200 files processed\n",
      "batch 120/165, 61440 files processed\n",
      "batch 140/165, 71680 files processed\n",
      "batch 160/165, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10847.h5 took 133.09649062156677 s\n",
      "features size:  (84230, 1024)\n",
      "coordinates size:  (84230, 2)\n",
      "\n",
      "progress: 167/876\n",
      "10848\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [90255 78576]\n",
      "level_dim [90255 78576]\n",
      "name 10848\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10848.h5: total of 98 batches\n",
      "batch 0/98, 0 files processed\n",
      "batch 20/98, 10240 files processed\n",
      "batch 40/98, 20480 files processed\n",
      "batch 60/98, 30720 files processed\n",
      "batch 80/98, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10848.h5 took 88.57599210739136 s\n",
      "features size:  (49790, 1024)\n",
      "coordinates size:  (49790, 2)\n",
      "\n",
      "progress: 168/876\n",
      "10850\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96016 80631]\n",
      "level_dim [96016 80631]\n",
      "name 10850\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10850.h5: total of 121 batches\n",
      "batch 0/121, 0 files processed\n",
      "batch 20/121, 10240 files processed\n",
      "batch 40/121, 20480 files processed\n",
      "batch 60/121, 30720 files processed\n",
      "batch 80/121, 40960 files processed\n",
      "batch 100/121, 51200 files processed\n",
      "batch 120/121, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10850.h5 took 68.99642324447632 s\n",
      "features size:  (61951, 1024)\n",
      "coordinates size:  (61951, 2)\n",
      "\n",
      "progress: 169/876\n",
      "10851\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [119060  83734]\n",
      "level_dim [119060  83734]\n",
      "name 10851\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10851.h5: total of 162 batches\n",
      "batch 0/162, 0 files processed\n",
      "batch 20/162, 10240 files processed\n",
      "batch 40/162, 20480 files processed\n",
      "batch 60/162, 30720 files processed\n",
      "batch 80/162, 40960 files processed\n",
      "batch 100/162, 51200 files processed\n",
      "batch 120/162, 61440 files processed\n",
      "batch 140/162, 71680 files processed\n",
      "batch 160/162, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10851.h5 took 119.09739804267883 s\n",
      "features size:  (82479, 1024)\n",
      "coordinates size:  (82479, 2)\n",
      "\n",
      "progress: 170/876\n",
      "10852\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [71052 77781]\n",
      "level_dim [71052 77781]\n",
      "name 10852\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10852.h5: total of 85 batches\n",
      "batch 0/85, 0 files processed\n",
      "batch 20/85, 10240 files processed\n",
      "batch 40/85, 20480 files processed\n",
      "batch 60/85, 30720 files processed\n",
      "batch 80/85, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10852.h5 took 65.13481950759888 s\n",
      "features size:  (43463, 1024)\n",
      "coordinates size:  (43463, 2)\n",
      "\n",
      "progress: 171/876\n",
      "10856\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [93136 74997]\n",
      "level_dim [93136 74997]\n",
      "name 10856\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10856.h5: total of 77 batches\n",
      "batch 0/77, 0 files processed\n",
      "batch 20/77, 10240 files processed\n",
      "batch 40/77, 20480 files processed\n",
      "batch 60/77, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10856.h5 took 64.21453714370728 s\n",
      "features size:  (38968, 1024)\n",
      "coordinates size:  (38968, 2)\n",
      "\n",
      "progress: 172/876\n",
      "10857\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [85454 54263]\n",
      "level_dim [85454 54263]\n",
      "name 10857\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10857.h5: total of 65 batches\n",
      "batch 0/65, 0 files processed\n",
      "batch 20/65, 10240 files processed\n",
      "batch 40/65, 20480 files processed\n",
      "batch 60/65, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10857.h5 took 57.72325253486633 s\n",
      "features size:  (32885, 1024)\n",
      "coordinates size:  (32885, 2)\n",
      "\n",
      "progress: 173/876\n",
      "10858\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [75853 61924]\n",
      "level_dim [75853 61924]\n",
      "name 10858\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10858.h5: total of 58 batches\n",
      "batch 0/58, 0 files processed\n",
      "batch 20/58, 10240 files processed\n",
      "batch 40/58, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10858.h5 took 46.60838341712952 s\n",
      "features size:  (29223, 1024)\n",
      "coordinates size:  (29223, 2)\n",
      "\n",
      "progress: 174/876\n",
      "10859\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [63371 60883]\n",
      "level_dim [63371 60883]\n",
      "name 10859\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10859.h5: total of 59 batches\n",
      "batch 0/59, 0 files processed\n",
      "batch 20/59, 10240 files processed\n",
      "batch 40/59, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10859.h5 took 49.30694055557251 s\n",
      "features size:  (29929, 1024)\n",
      "coordinates size:  (29929, 2)\n",
      "\n",
      "progress: 175/876\n",
      "10860\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [103698  83713]\n",
      "level_dim [103698  83713]\n",
      "name 10860\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10860.h5: total of 188 batches\n",
      "batch 0/188, 0 files processed\n",
      "batch 20/188, 10240 files processed\n",
      "batch 40/188, 20480 files processed\n",
      "batch 60/188, 30720 files processed\n",
      "batch 80/188, 40960 files processed\n",
      "batch 100/188, 51200 files processed\n",
      "batch 120/188, 61440 files processed\n",
      "batch 140/188, 71680 files processed\n",
      "batch 160/188, 81920 files processed\n",
      "batch 180/188, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10860.h5 took 176.4940481185913 s\n",
      "features size:  (95918, 1024)\n",
      "coordinates size:  (95918, 2)\n",
      "\n",
      "progress: 176/876\n",
      "10861\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57610 81089]\n",
      "level_dim [57610 81089]\n",
      "name 10861\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10861.h5: total of 87 batches\n",
      "batch 0/87, 0 files processed\n",
      "batch 20/87, 10240 files processed\n",
      "batch 40/87, 20480 files processed\n",
      "batch 60/87, 30720 files processed\n",
      "batch 80/87, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10861.h5 took 63.570860624313354 s\n",
      "features size:  (44161, 1024)\n",
      "coordinates size:  (44161, 2)\n",
      "\n",
      "progress: 177/876\n",
      "10862\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [54729 56777]\n",
      "level_dim [54729 56777]\n",
      "name 10862\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10862.h5: total of 41 batches\n",
      "batch 0/41, 0 files processed\n",
      "batch 20/41, 10240 files processed\n",
      "batch 40/41, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10862.h5 took 35.07301473617554 s\n",
      "features size:  (20516, 1024)\n",
      "coordinates size:  (20516, 2)\n",
      "\n",
      "progress: 178/876\n",
      "10863\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [86415 75500]\n",
      "level_dim [86415 75500]\n",
      "name 10863\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10863.h5: total of 80 batches\n",
      "batch 0/80, 0 files processed\n",
      "batch 20/80, 10240 files processed\n",
      "batch 40/80, 20480 files processed\n",
      "batch 60/80, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10863.h5 took 57.46468758583069 s\n",
      "features size:  (40650, 1024)\n",
      "coordinates size:  (40650, 2)\n",
      "\n",
      "progress: 179/876\n",
      "10864\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [71052 72664]\n",
      "level_dim [71052 72664]\n",
      "name 10864\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10864.h5: total of 72 batches\n",
      "batch 0/72, 0 files processed\n",
      "batch 20/72, 10240 files processed\n",
      "batch 40/72, 20480 files processed\n",
      "batch 60/72, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10864.h5 took 48.26127648353577 s\n",
      "features size:  (36662, 1024)\n",
      "coordinates size:  (36662, 2)\n",
      "\n",
      "progress: 180/876\n",
      "10872\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [65291 62933]\n",
      "level_dim [65291 62933]\n",
      "name 10872\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10872.h5: total of 74 batches\n",
      "batch 0/74, 0 files processed\n",
      "batch 20/74, 10240 files processed\n",
      "batch 40/74, 20480 files processed\n",
      "batch 60/74, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10872.h5 took 63.352476596832275 s\n",
      "features size:  (37857, 1024)\n",
      "coordinates size:  (37857, 2)\n",
      "\n",
      "progress: 181/876\n",
      "10873\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [76813 73696]\n",
      "level_dim [76813 73696]\n",
      "name 10873\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10873.h5: total of 69 batches\n",
      "batch 0/69, 0 files processed\n",
      "batch 20/69, 10240 files processed\n",
      "batch 40/69, 20480 files processed\n",
      "batch 60/69, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10873.h5 took 53.485355377197266 s\n",
      "features size:  (35024, 1024)\n",
      "coordinates size:  (35024, 2)\n",
      "\n",
      "progress: 182/876\n",
      "10874\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [83534 66541]\n",
      "level_dim [83534 66541]\n",
      "name 10874\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10874.h5: total of 100 batches\n",
      "batch 0/100, 0 files processed\n",
      "batch 20/100, 10240 files processed\n",
      "batch 40/100, 20480 files processed\n",
      "batch 60/100, 30720 files processed\n",
      "batch 80/100, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10874.h5 took 82.02842879295349 s\n",
      "features size:  (50885, 1024)\n",
      "coordinates size:  (50885, 2)\n",
      "\n",
      "progress: 183/876\n",
      "10876\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [124821  83485]\n",
      "level_dim [124821  83485]\n",
      "name 10876\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10876.h5: total of 231 batches\n",
      "batch 0/231, 0 files processed\n",
      "batch 20/231, 10240 files processed\n",
      "batch 40/231, 20480 files processed\n",
      "batch 60/231, 30720 files processed\n",
      "batch 80/231, 40960 files processed\n",
      "batch 100/231, 51200 files processed\n",
      "batch 120/231, 61440 files processed\n",
      "batch 140/231, 71680 files processed\n",
      "batch 160/231, 81920 files processed\n",
      "batch 180/231, 92160 files processed\n",
      "batch 200/231, 102400 files processed\n",
      "batch 220/231, 112640 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10876.h5 took 202.65517807006836 s\n",
      "features size:  (117945, 1024)\n",
      "coordinates size:  (117945, 2)\n",
      "\n",
      "progress: 184/876\n",
      "10877\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [138264  83760]\n",
      "level_dim [138264  83760]\n",
      "name 10877\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10877.h5: total of 143 batches\n",
      "batch 0/143, 0 files processed\n",
      "batch 20/143, 10240 files processed\n",
      "batch 40/143, 20480 files processed\n",
      "batch 60/143, 30720 files processed\n",
      "batch 80/143, 40960 files processed\n",
      "batch 100/143, 51200 files processed\n",
      "batch 120/143, 61440 files processed\n",
      "batch 140/143, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10877.h5 took 112.12793278694153 s\n",
      "features size:  (72924, 1024)\n",
      "coordinates size:  (72924, 2)\n",
      "\n",
      "progress: 185/876\n",
      "10878\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [118100  82197]\n",
      "level_dim [118100  82197]\n",
      "name 10878\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10878.h5: total of 138 batches\n",
      "batch 0/138, 0 files processed\n",
      "batch 20/138, 10240 files processed\n",
      "batch 40/138, 20480 files processed\n",
      "batch 60/138, 30720 files processed\n",
      "batch 80/138, 40960 files processed\n",
      "batch 100/138, 51200 files processed\n",
      "batch 120/138, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10878.h5 took 108.53729557991028 s\n",
      "features size:  (70219, 1024)\n",
      "coordinates size:  (70219, 2)\n",
      "\n",
      "progress: 186/876\n",
      "10879\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [86415 83431]\n",
      "level_dim [86415 83431]\n",
      "name 10879\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10879.h5: total of 133 batches\n",
      "batch 0/133, 0 files processed\n",
      "batch 20/133, 10240 files processed\n",
      "batch 40/133, 20480 files processed\n",
      "batch 60/133, 30720 files processed\n",
      "batch 80/133, 40960 files processed\n",
      "batch 100/133, 51200 files processed\n",
      "batch 120/133, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10879.h5 took 95.4133653640747 s\n",
      "features size:  (67976, 1024)\n",
      "coordinates size:  (67976, 2)\n",
      "\n",
      "progress: 187/876\n",
      "10886\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [103698  73989]\n",
      "level_dim [103698  73989]\n",
      "name 10886\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10886.h5: total of 121 batches\n",
      "batch 0/121, 0 files processed\n",
      "batch 20/121, 10240 files processed\n",
      "batch 40/121, 20480 files processed\n",
      "batch 60/121, 30720 files processed\n",
      "batch 80/121, 40960 files processed\n",
      "batch 100/121, 51200 files processed\n",
      "batch 120/121, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10886.h5 took 101.41264462471008 s\n",
      "features size:  (61672, 1024)\n",
      "coordinates size:  (61672, 2)\n",
      "\n",
      "progress: 188/876\n",
      "10887\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [109459  78347]\n",
      "level_dim [109459  78347]\n",
      "name 10887\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10887.h5: total of 152 batches\n",
      "batch 0/152, 0 files processed\n",
      "batch 20/152, 10240 files processed\n",
      "batch 40/152, 20480 files processed\n",
      "batch 60/152, 30720 files processed\n",
      "batch 80/152, 40960 files processed\n",
      "batch 100/152, 51200 files processed\n",
      "batch 120/152, 61440 files processed\n",
      "batch 140/152, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10887.h5 took 134.5877866744995 s\n",
      "features size:  (77457, 1024)\n",
      "coordinates size:  (77457, 2)\n",
      "\n",
      "progress: 189/876\n",
      "10888\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [138264  79667]\n",
      "level_dim [138264  79667]\n",
      "name 10888\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10888.h5: total of 148 batches\n",
      "batch 0/148, 0 files processed\n",
      "batch 20/148, 10240 files processed\n",
      "batch 40/148, 20480 files processed\n",
      "batch 60/148, 30720 files processed\n",
      "batch 80/148, 40960 files processed\n",
      "batch 100/148, 51200 files processed\n",
      "batch 120/148, 61440 files processed\n",
      "batch 140/148, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10888.h5 took 108.14305448532104 s\n",
      "features size:  (75542, 1024)\n",
      "coordinates size:  (75542, 2)\n",
      "\n",
      "progress: 190/876\n",
      "10889\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [101777  68870]\n",
      "level_dim [101777  68870]\n",
      "name 10889\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10889.h5: total of 126 batches\n",
      "batch 0/126, 0 files processed\n",
      "batch 20/126, 10240 files processed\n",
      "batch 40/126, 20480 files processed\n",
      "batch 60/126, 30720 files processed\n",
      "batch 80/126, 40960 files processed\n",
      "batch 100/126, 51200 files processed\n",
      "batch 120/126, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10889.h5 took 115.1715338230133 s\n",
      "features size:  (64295, 1024)\n",
      "coordinates size:  (64295, 2)\n",
      "\n",
      "progress: 191/876\n",
      "10896\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [143064  83767]\n",
      "level_dim [143064  83767]\n",
      "name 10896\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10896.h5: total of 174 batches\n",
      "batch 0/174, 0 files processed\n",
      "batch 20/174, 10240 files processed\n",
      "batch 40/174, 20480 files processed\n",
      "batch 60/174, 30720 files processed\n",
      "batch 80/174, 40960 files processed\n",
      "batch 100/174, 51200 files processed\n",
      "batch 120/174, 61440 files processed\n",
      "batch 140/174, 71680 files processed\n",
      "batch 160/174, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10896.h5 took 156.7235414981842 s\n",
      "features size:  (88846, 1024)\n",
      "coordinates size:  (88846, 2)\n",
      "\n",
      "progress: 192/876\n",
      "10897\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [104658  83714]\n",
      "level_dim [104658  83714]\n",
      "name 10897\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10897.h5: total of 121 batches\n",
      "batch 0/121, 0 files processed\n",
      "batch 20/121, 10240 files processed\n",
      "batch 40/121, 20480 files processed\n",
      "batch 60/121, 30720 files processed\n",
      "batch 80/121, 40960 files processed\n",
      "batch 100/121, 51200 files processed\n",
      "batch 120/121, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10897.h5 took 97.34868049621582 s\n",
      "features size:  (61849, 1024)\n",
      "coordinates size:  (61849, 2)\n",
      "\n",
      "progress: 193/876\n",
      "10898\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [91215 83694]\n",
      "level_dim [91215 83694]\n",
      "name 10898\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10898.h5: total of 126 batches\n",
      "batch 0/126, 0 files processed\n",
      "batch 20/126, 10240 files processed\n",
      "batch 40/126, 20480 files processed\n",
      "batch 60/126, 30720 files processed\n",
      "batch 80/126, 40960 files processed\n",
      "batch 100/126, 51200 files processed\n",
      "batch 120/126, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10898.h5 took 97.6960711479187 s\n",
      "features size:  (64273, 1024)\n",
      "coordinates size:  (64273, 2)\n",
      "\n",
      "progress: 194/876\n",
      "10899\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [64331 47835]\n",
      "level_dim [64331 47835]\n",
      "name 10899\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10899.h5: total of 42 batches\n",
      "batch 0/42, 0 files processed\n",
      "batch 20/42, 10240 files processed\n",
      "batch 40/42, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10899.h5 took 36.68985104560852 s\n",
      "features size:  (21473, 1024)\n",
      "coordinates size:  (21473, 2)\n",
      "\n",
      "progress: 195/876\n",
      "10900\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [114259  77330]\n",
      "level_dim [114259  77330]\n",
      "name 10900\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10900.h5: total of 103 batches\n",
      "batch 0/103, 0 files processed\n",
      "batch 20/103, 10240 files processed\n",
      "batch 40/103, 20480 files processed\n",
      "batch 60/103, 30720 files processed\n",
      "batch 80/103, 40960 files processed\n",
      "batch 100/103, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10900.h5 took 73.43300127983093 s\n",
      "features size:  (52469, 1024)\n",
      "coordinates size:  (52469, 2)\n",
      "\n",
      "progress: 196/876\n",
      "10901\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [62410 52183]\n",
      "level_dim [62410 52183]\n",
      "name 10901\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10901.h5: total of 28 batches\n",
      "batch 0/28, 0 files processed\n",
      "batch 20/28, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10901.h5 took 22.777071714401245 s\n",
      "features size:  (13917, 1024)\n",
      "coordinates size:  (13917, 2)\n",
      "\n",
      "progress: 197/876\n",
      "10902\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [101777  77568]\n",
      "level_dim [101777  77568]\n",
      "name 10902\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10902.h5: total of 119 batches\n",
      "batch 0/119, 0 files processed\n",
      "batch 20/119, 10240 files processed\n",
      "batch 40/119, 20480 files processed\n",
      "batch 60/119, 30720 files processed\n",
      "batch 80/119, 40960 files processed\n",
      "batch 100/119, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10902.h5 took 95.18562364578247 s\n",
      "features size:  (60721, 1024)\n",
      "coordinates size:  (60721, 2)\n",
      "\n",
      "progress: 198/876\n",
      "10903\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [117140  81684]\n",
      "level_dim [117140  81684]\n",
      "name 10903\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10903.h5: total of 171 batches\n",
      "batch 0/171, 0 files processed\n",
      "batch 20/171, 10240 files processed\n",
      "batch 40/171, 20480 files processed\n",
      "batch 60/171, 30720 files processed\n",
      "batch 80/171, 40960 files processed\n",
      "batch 100/171, 51200 files processed\n",
      "batch 120/171, 61440 files processed\n",
      "batch 140/171, 71680 files processed\n",
      "batch 160/171, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10903.h5 took 151.30373334884644 s\n",
      "features size:  (87471, 1024)\n",
      "coordinates size:  (87471, 2)\n",
      "\n",
      "progress: 199/876\n",
      "10904\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [107538  83461]\n",
      "level_dim [107538  83461]\n",
      "name 10904\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10904.h5: total of 158 batches\n",
      "batch 0/158, 0 files processed\n",
      "batch 20/158, 10240 files processed\n",
      "batch 40/158, 20480 files processed\n",
      "batch 60/158, 30720 files processed\n",
      "batch 80/158, 40960 files processed\n",
      "batch 100/158, 51200 files processed\n",
      "batch 120/158, 61440 files processed\n",
      "batch 140/158, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10904.h5 took 126.01789379119873 s\n",
      "features size:  (80575, 1024)\n",
      "coordinates size:  (80575, 2)\n",
      "\n",
      "progress: 200/876\n",
      "10905\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [110419  82186]\n",
      "level_dim [110419  82186]\n",
      "name 10905\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10905.h5: total of 157 batches\n",
      "batch 0/157, 0 files processed\n",
      "batch 20/157, 10240 files processed\n",
      "batch 40/157, 20480 files processed\n",
      "batch 60/157, 30720 files processed\n",
      "batch 80/157, 40960 files processed\n",
      "batch 100/157, 51200 files processed\n",
      "batch 120/157, 61440 files processed\n",
      "batch 140/157, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10905.h5 took 116.21202349662781 s\n",
      "features size:  (80307, 1024)\n",
      "coordinates size:  (80307, 2)\n",
      "\n",
      "progress: 201/876\n",
      "10908\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [38406 27843]\n",
      "level_dim [38406 27843]\n",
      "name 10908\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10908.h5: total of 15 batches\n",
      "batch 0/15, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10908.h5 took 13.765077590942383 s\n",
      "features size:  (7641, 1024)\n",
      "coordinates size:  (7641, 2)\n",
      "\n",
      "progress: 202/876\n",
      "10915\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [111379  83722]\n",
      "level_dim [111379  83722]\n",
      "name 10915\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10915.h5: total of 210 batches\n",
      "batch 0/210, 0 files processed\n",
      "batch 20/210, 10240 files processed\n",
      "batch 40/210, 20480 files processed\n",
      "batch 60/210, 30720 files processed\n",
      "batch 80/210, 40960 files processed\n",
      "batch 100/210, 51200 files processed\n",
      "batch 120/210, 61440 files processed\n",
      "batch 140/210, 71680 files processed\n",
      "batch 160/210, 81920 files processed\n",
      "batch 180/210, 92160 files processed\n",
      "batch 200/210, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10915.h5 took 138.3705198764801 s\n",
      "features size:  (107156, 1024)\n",
      "coordinates size:  (107156, 2)\n",
      "\n",
      "progress: 203/876\n",
      "10934\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [91215 66807]\n",
      "level_dim [91215 66807]\n",
      "name 10934\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10934.h5: total of 91 batches\n",
      "batch 0/91, 0 files processed\n",
      "batch 20/91, 10240 files processed\n",
      "batch 40/91, 20480 files processed\n",
      "batch 60/91, 30720 files processed\n",
      "batch 80/91, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10934.h5 took 71.91500568389893 s\n",
      "features size:  (46173, 1024)\n",
      "coordinates size:  (46173, 2)\n",
      "\n",
      "progress: 204/876\n",
      "10935\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [98897 83705]\n",
      "level_dim [98897 83705]\n",
      "name 10935\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10935.h5: total of 142 batches\n",
      "batch 0/142, 0 files processed\n",
      "batch 20/142, 10240 files processed\n",
      "batch 40/142, 20480 files processed\n",
      "batch 60/142, 30720 files processed\n",
      "batch 80/142, 40960 files processed\n",
      "batch 100/142, 51200 files processed\n",
      "batch 120/142, 61440 files processed\n",
      "batch 140/142, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10935.h5 took 114.10702586174011 s\n",
      "features size:  (72510, 1024)\n",
      "coordinates size:  (72510, 2)\n",
      "\n",
      "progress: 205/876\n",
      "10936\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [111379  80909]\n",
      "level_dim [111379  80909]\n",
      "name 10936\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10936.h5: total of 147 batches\n",
      "batch 0/147, 0 files processed\n",
      "batch 20/147, 10240 files processed\n",
      "batch 40/147, 20480 files processed\n",
      "batch 60/147, 30720 files processed\n",
      "batch 80/147, 40960 files processed\n",
      "batch 100/147, 51200 files processed\n",
      "batch 120/147, 61440 files processed\n",
      "batch 140/147, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10936.h5 took 121.22655034065247 s\n",
      "features size:  (75255, 1024)\n",
      "coordinates size:  (75255, 2)\n",
      "\n",
      "progress: 206/876\n",
      "10937\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [115220  49186]\n",
      "level_dim [115220  49186]\n",
      "name 10937\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10937.h5: total of 94 batches\n",
      "batch 0/94, 0 files processed\n",
      "batch 20/94, 10240 files processed\n",
      "batch 40/94, 20480 files processed\n",
      "batch 60/94, 30720 files processed\n",
      "batch 80/94, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10937.h5 took 78.77573251724243 s\n",
      "features size:  (47927, 1024)\n",
      "coordinates size:  (47927, 2)\n",
      "\n",
      "progress: 207/876\n",
      "10938\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [103698  70664]\n",
      "level_dim [103698  70664]\n",
      "name 10938\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10938.h5: total of 101 batches\n",
      "batch 0/101, 0 files processed\n",
      "batch 20/101, 10240 files processed\n",
      "batch 40/101, 20480 files processed\n",
      "batch 60/101, 30720 files processed\n",
      "batch 80/101, 40960 files processed\n",
      "batch 100/101, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10938.h5 took 81.21125054359436 s\n",
      "features size:  (51494, 1024)\n",
      "coordinates size:  (51494, 2)\n",
      "\n",
      "progress: 208/876\n",
      "10939\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [110419  83466]\n",
      "level_dim [110419  83466]\n",
      "name 10939\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10939.h5: total of 106 batches\n",
      "batch 0/106, 0 files processed\n",
      "batch 20/106, 10240 files processed\n",
      "batch 40/106, 20480 files processed\n",
      "batch 60/106, 30720 files processed\n",
      "batch 80/106, 40960 files processed\n",
      "batch 100/106, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10939.h5 took 82.5340929031372 s\n",
      "features size:  (54061, 1024)\n",
      "coordinates size:  (54061, 2)\n",
      "\n",
      "progress: 209/876\n",
      "10940\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [93136 79348]\n",
      "level_dim [93136 79348]\n",
      "name 10940\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10940.h5: total of 150 batches\n",
      "batch 0/150, 0 files processed\n",
      "batch 20/150, 10240 files processed\n",
      "batch 40/150, 20480 files processed\n",
      "batch 60/150, 30720 files processed\n",
      "batch 80/150, 40960 files processed\n",
      "batch 100/150, 51200 files processed\n",
      "batch 120/150, 61440 files processed\n",
      "batch 140/150, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10940.h5 took 98.99750256538391 s\n",
      "features size:  (76342, 1024)\n",
      "coordinates size:  (76342, 2)\n",
      "\n",
      "progress: 210/876\n",
      "10982\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [60490 64206]\n",
      "level_dim [60490 64206]\n",
      "name 10982\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10982.h5: total of 79 batches\n",
      "batch 0/79, 0 files processed\n",
      "batch 20/79, 10240 files processed\n",
      "batch 40/79, 20480 files processed\n",
      "batch 60/79, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10982.h5 took 68.13293147087097 s\n",
      "features size:  (40144, 1024)\n",
      "coordinates size:  (40144, 2)\n",
      "\n",
      "progress: 211/876\n",
      "10983\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [62410 67534]\n",
      "level_dim [62410 67534]\n",
      "name 10983\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10983.h5: total of 81 batches\n",
      "batch 0/81, 0 files processed\n",
      "batch 20/81, 10240 files processed\n",
      "batch 40/81, 20480 files processed\n",
      "batch 60/81, 30720 files processed\n",
      "batch 80/81, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10983.h5 took 74.87260937690735 s\n",
      "features size:  (41271, 1024)\n",
      "coordinates size:  (41271, 2)\n",
      "\n",
      "progress: 212/876\n",
      "10984\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [63371 48859]\n",
      "level_dim [63371 48859]\n",
      "name 10984\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10984.h5: total of 49 batches\n",
      "batch 0/49, 0 files processed\n",
      "batch 20/49, 10240 files processed\n",
      "batch 40/49, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10984.h5 took 36.24247980117798 s\n",
      "features size:  (24686, 1024)\n",
      "coordinates size:  (24686, 2)\n",
      "\n",
      "progress: 213/876\n",
      "10985\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [68171 40421]\n",
      "level_dim [68171 40421]\n",
      "name 10985\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10985.h5: total of 45 batches\n",
      "batch 0/45, 0 files processed\n",
      "batch 20/45, 10240 files processed\n",
      "batch 40/45, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10985.h5 took 35.81313514709473 s\n",
      "features size:  (22887, 1024)\n",
      "coordinates size:  (22887, 2)\n",
      "\n",
      "progress: 214/876\n",
      "10986\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [55689 40147]\n",
      "level_dim [55689 40147]\n",
      "name 10986\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10986.h5: total of 30 batches\n",
      "batch 0/30, 0 files processed\n",
      "batch 20/30, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10986.h5 took 28.96649432182312 s\n",
      "features size:  (15141, 1024)\n",
      "coordinates size:  (15141, 2)\n",
      "\n",
      "progress: 215/876\n",
      "10987\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [78733 63976]\n",
      "level_dim [78733 63976]\n",
      "name 10987\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10987.h5: total of 78 batches\n",
      "batch 0/78, 0 files processed\n",
      "batch 20/78, 10240 files processed\n",
      "batch 40/78, 20480 files processed\n",
      "batch 60/78, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10987.h5 took 64.4714343547821 s\n",
      "features size:  (39564, 1024)\n",
      "coordinates size:  (39564, 2)\n",
      "\n",
      "progress: 216/876\n",
      "10988\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [72012 66525]\n",
      "level_dim [72012 66525]\n",
      "name 10988\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10988.h5: total of 83 batches\n",
      "batch 0/83, 0 files processed\n",
      "batch 20/83, 10240 files processed\n",
      "batch 40/83, 20480 files processed\n",
      "batch 60/83, 30720 files processed\n",
      "batch 80/83, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10988.h5 took 48.74210786819458 s\n",
      "features size:  (42015, 1024)\n",
      "coordinates size:  (42015, 2)\n",
      "\n",
      "progress: 217/876\n",
      "10989\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [82574 67051]\n",
      "level_dim [82574 67051]\n",
      "name 10989\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10989.h5: total of 74 batches\n",
      "batch 0/74, 0 files processed\n",
      "batch 20/74, 10240 files processed\n",
      "batch 40/74, 20480 files processed\n",
      "batch 60/74, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10989.h5 took 64.64228796958923 s\n",
      "features size:  (37614, 1024)\n",
      "coordinates size:  (37614, 2)\n",
      "\n",
      "progress: 218/876\n",
      "10990\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [75853 72159]\n",
      "level_dim [75853 72159]\n",
      "name 10990\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10990.h5: total of 96 batches\n",
      "batch 0/96, 0 files processed\n",
      "batch 20/96, 10240 files processed\n",
      "batch 40/96, 20480 files processed\n",
      "batch 60/96, 30720 files processed\n",
      "batch 80/96, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10990.h5 took 79.5461699962616 s\n",
      "features size:  (48944, 1024)\n",
      "coordinates size:  (48944, 2)\n",
      "\n",
      "progress: 219/876\n",
      "10991\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [104658  83714]\n",
      "level_dim [104658  83714]\n",
      "name 10991\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10991.h5: total of 141 batches\n",
      "batch 0/141, 0 files processed\n",
      "batch 20/141, 10240 files processed\n",
      "batch 40/141, 20480 files processed\n",
      "batch 60/141, 30720 files processed\n",
      "batch 80/141, 40960 files processed\n",
      "batch 100/141, 51200 files processed\n",
      "batch 120/141, 61440 files processed\n",
      "batch 140/141, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10991.h5 took 105.97830510139465 s\n",
      "features size:  (71843, 1024)\n",
      "coordinates size:  (71843, 2)\n",
      "\n",
      "progress: 220/876\n",
      "10992\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [97937 83703]\n",
      "level_dim [97937 83703]\n",
      "name 10992\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10992.h5: total of 126 batches\n",
      "batch 0/126, 0 files processed\n",
      "batch 20/126, 10240 files processed\n",
      "batch 40/126, 20480 files processed\n",
      "batch 60/126, 30720 files processed\n",
      "batch 80/126, 40960 files processed\n",
      "batch 100/126, 51200 files processed\n",
      "batch 120/126, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10992.h5 took 65.92118167877197 s\n",
      "features size:  (64334, 1024)\n",
      "coordinates size:  (64334, 2)\n",
      "\n",
      "progress: 221/876\n",
      "10993\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [93136 67834]\n",
      "level_dim [93136 67834]\n",
      "name 10993\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10993.h5: total of 103 batches\n",
      "batch 0/103, 0 files processed\n",
      "batch 20/103, 10240 files processed\n",
      "batch 40/103, 20480 files processed\n",
      "batch 60/103, 30720 files processed\n",
      "batch 80/103, 40960 files processed\n",
      "batch 100/103, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10993.h5 took 75.48675274848938 s\n",
      "features size:  (52728, 1024)\n",
      "coordinates size:  (52728, 2)\n",
      "\n",
      "progress: 222/876\n",
      "10994\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [107538  56852]\n",
      "level_dim [107538  56852]\n",
      "name 10994\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10994.h5: total of 90 batches\n",
      "batch 0/90, 0 files processed\n",
      "batch 20/90, 10240 files processed\n",
      "batch 40/90, 20480 files processed\n",
      "batch 60/90, 30720 files processed\n",
      "batch 80/90, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10994.h5 took 80.73034262657166 s\n",
      "features size:  (45577, 1024)\n",
      "coordinates size:  (45577, 2)\n",
      "\n",
      "progress: 223/876\n",
      "10995\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [95056 78071]\n",
      "level_dim [95056 78071]\n",
      "name 10995\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10995.h5: total of 98 batches\n",
      "batch 0/98, 0 files processed\n",
      "batch 20/98, 10240 files processed\n",
      "batch 40/98, 20480 files processed\n",
      "batch 60/98, 30720 files processed\n",
      "batch 80/98, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10995.h5 took 84.13186287879944 s\n",
      "features size:  (50136, 1024)\n",
      "coordinates size:  (50136, 2)\n",
      "\n",
      "progress: 224/876\n",
      "10996\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [91215 78577]\n",
      "level_dim [91215 78577]\n",
      "name 10996\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/10996.h5: total of 145 batches\n",
      "batch 0/145, 0 files processed\n",
      "batch 20/145, 10240 files processed\n",
      "batch 40/145, 20480 files processed\n",
      "batch 60/145, 30720 files processed\n",
      "batch 80/145, 40960 files processed\n",
      "batch 100/145, 51200 files processed\n",
      "batch 120/145, 61440 files processed\n",
      "batch 140/145, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/10996.h5 took 132.11319375038147 s\n",
      "features size:  (73827, 1024)\n",
      "coordinates size:  (73827, 2)\n",
      "\n",
      "progress: 225/876\n",
      "11001\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [95056 75513]\n",
      "level_dim [95056 75513]\n",
      "name 11001\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11001.h5: total of 101 batches\n",
      "batch 0/101, 0 files processed\n",
      "batch 20/101, 10240 files processed\n",
      "batch 40/101, 20480 files processed\n",
      "batch 60/101, 30720 files processed\n",
      "batch 80/101, 40960 files processed\n",
      "batch 100/101, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11001.h5 took 80.99097156524658 s\n",
      "features size:  (51486, 1024)\n",
      "coordinates size:  (51486, 2)\n",
      "\n",
      "progress: 226/876\n",
      "11002\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [90255 59642]\n",
      "level_dim [90255 59642]\n",
      "name 11002\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11002.h5: total of 92 batches\n",
      "batch 0/92, 0 files processed\n",
      "batch 20/92, 10240 files processed\n",
      "batch 40/92, 20480 files processed\n",
      "batch 60/92, 30720 files processed\n",
      "batch 80/92, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11002.h5 took 69.25026035308838 s\n",
      "features size:  (46748, 1024)\n",
      "coordinates size:  (46748, 2)\n",
      "\n",
      "progress: 227/876\n",
      "11003\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [106578  73226]\n",
      "level_dim [106578  73226]\n",
      "name 11003\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11003.h5: total of 158 batches\n",
      "batch 0/158, 0 files processed\n",
      "batch 20/158, 10240 files processed\n",
      "batch 40/158, 20480 files processed\n",
      "batch 60/158, 30720 files processed\n",
      "batch 80/158, 40960 files processed\n",
      "batch 100/158, 51200 files processed\n",
      "batch 120/158, 61440 files processed\n",
      "batch 140/158, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11003.h5 took 141.88771629333496 s\n",
      "features size:  (80873, 1024)\n",
      "coordinates size:  (80873, 2)\n",
      "\n",
      "progress: 228/876\n",
      "11004\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [126742  80418]\n",
      "level_dim [126742  80418]\n",
      "name 11004\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11004.h5: total of 193 batches\n",
      "batch 0/193, 0 files processed\n",
      "batch 20/193, 10240 files processed\n",
      "batch 40/193, 20480 files processed\n",
      "batch 60/193, 30720 files processed\n",
      "batch 80/193, 40960 files processed\n",
      "batch 100/193, 51200 files processed\n",
      "batch 120/193, 61440 files processed\n",
      "batch 140/193, 71680 files processed\n",
      "batch 160/193, 81920 files processed\n",
      "batch 180/193, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11004.h5 took 162.88042998313904 s\n",
      "features size:  (98396, 1024)\n",
      "coordinates size:  (98396, 2)\n",
      "\n",
      "progress: 229/876\n",
      "11005\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [110419  81675]\n",
      "level_dim [110419  81675]\n",
      "name 11005\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11005.h5: total of 95 batches\n",
      "batch 0/95, 0 files processed\n",
      "batch 20/95, 10240 files processed\n",
      "batch 40/95, 20480 files processed\n",
      "batch 60/95, 30720 files processed\n",
      "batch 80/95, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11005.h5 took 81.04971480369568 s\n",
      "features size:  (48256, 1024)\n",
      "coordinates size:  (48256, 2)\n",
      "\n",
      "progress: 230/876\n",
      "11006\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [99857 78590]\n",
      "level_dim [99857 78590]\n",
      "name 11006\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11006.h5: total of 121 batches\n",
      "batch 0/121, 0 files processed\n",
      "batch 20/121, 10240 files processed\n",
      "batch 40/121, 20480 files processed\n",
      "batch 60/121, 30720 files processed\n",
      "batch 80/121, 40960 files processed\n",
      "batch 100/121, 51200 files processed\n",
      "batch 120/121, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11006.h5 took 83.90622043609619 s\n",
      "features size:  (61567, 1024)\n",
      "coordinates size:  (61567, 2)\n",
      "\n",
      "progress: 231/876\n",
      "11007\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [91215 83694]\n",
      "level_dim [91215 83694]\n",
      "name 11007\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11007.h5: total of 170 batches\n",
      "batch 0/170, 0 files processed\n",
      "batch 20/170, 10240 files processed\n",
      "batch 40/170, 20480 files processed\n",
      "batch 60/170, 30720 files processed\n",
      "batch 80/170, 40960 files processed\n",
      "batch 100/170, 51200 files processed\n",
      "batch 120/170, 61440 files processed\n",
      "batch 140/170, 71680 files processed\n",
      "batch 160/170, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11007.h5 took 125.04012489318848 s\n",
      "features size:  (86591, 1024)\n",
      "coordinates size:  (86591, 2)\n",
      "\n",
      "progress: 232/876\n",
      "11008\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [80654 63979]\n",
      "level_dim [80654 63979]\n",
      "name 11008\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11008.h5: total of 106 batches\n",
      "batch 0/106, 0 files processed\n",
      "batch 20/106, 10240 files processed\n",
      "batch 40/106, 20480 files processed\n",
      "batch 60/106, 30720 files processed\n",
      "batch 80/106, 40960 files processed\n",
      "batch 100/106, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11008.h5 took 80.93913006782532 s\n",
      "features size:  (53797, 1024)\n",
      "coordinates size:  (53797, 2)\n",
      "\n",
      "progress: 233/876\n",
      "11009\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [98897 79355]\n",
      "level_dim [98897 79355]\n",
      "name 11009\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11009.h5: total of 136 batches\n",
      "batch 0/136, 0 files processed\n",
      "batch 20/136, 10240 files processed\n",
      "batch 40/136, 20480 files processed\n",
      "batch 60/136, 30720 files processed\n",
      "batch 80/136, 40960 files processed\n",
      "batch 100/136, 51200 files processed\n",
      "batch 120/136, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11009.h5 took 92.75446629524231 s\n",
      "features size:  (69478, 1024)\n",
      "coordinates size:  (69478, 2)\n",
      "\n",
      "progress: 234/876\n",
      "11010\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [83534 72681]\n",
      "level_dim [83534 72681]\n",
      "name 11010\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11010.h5: total of 96 batches\n",
      "batch 0/96, 0 files processed\n",
      "batch 20/96, 10240 files processed\n",
      "batch 40/96, 20480 files processed\n",
      "batch 60/96, 30720 files processed\n",
      "batch 80/96, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11010.h5 took 83.14783883094788 s\n",
      "features size:  (48735, 1024)\n",
      "coordinates size:  (48735, 2)\n",
      "\n",
      "progress: 235/876\n",
      "11011\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [63371 73933]\n",
      "level_dim [63371 73933]\n",
      "name 11011\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11011.h5: total of 54 batches\n",
      "batch 0/54, 0 files processed\n",
      "batch 20/54, 10240 files processed\n",
      "batch 40/54, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11011.h5 took 46.82620406150818 s\n",
      "features size:  (27586, 1024)\n",
      "coordinates size:  (27586, 2)\n",
      "\n",
      "progress: 236/876\n",
      "11012\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [56649 69830]\n",
      "level_dim [56649 69830]\n",
      "name 11012\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11012.h5: total of 58 batches\n",
      "batch 0/58, 0 files processed\n",
      "batch 20/58, 10240 files processed\n",
      "batch 40/58, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11012.h5 took 38.38032364845276 s\n",
      "features size:  (29258, 1024)\n",
      "coordinates size:  (29258, 2)\n",
      "\n",
      "progress: 237/876\n",
      "11013\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [19203 22698]\n",
      "level_dim [19203 22698]\n",
      "name 11013\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11013.h5: total of 6 batches\n",
      "batch 0/6, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11013.h5 took 6.411319017410278 s\n",
      "features size:  (2779, 1024)\n",
      "coordinates size:  (2779, 2)\n",
      "\n",
      "progress: 238/876\n",
      "11020\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [91215 65528]\n",
      "level_dim [91215 65528]\n",
      "name 11020\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11020.h5: total of 84 batches\n",
      "batch 0/84, 0 files processed\n",
      "batch 20/84, 10240 files processed\n",
      "batch 40/84, 20480 files processed\n",
      "batch 60/84, 30720 files processed\n",
      "batch 80/84, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11020.h5 took 69.59293985366821 s\n",
      "features size:  (42834, 1024)\n",
      "coordinates size:  (42834, 2)\n",
      "\n",
      "progress: 239/876\n",
      "11021\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [102737  72196]\n",
      "level_dim [102737  72196]\n",
      "name 11021\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11021.h5: total of 87 batches\n",
      "batch 0/87, 0 files processed\n",
      "batch 20/87, 10240 files processed\n",
      "batch 40/87, 20480 files processed\n",
      "batch 60/87, 30720 files processed\n",
      "batch 80/87, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11021.h5 took 68.96186995506287 s\n",
      "features size:  (44345, 1024)\n",
      "coordinates size:  (44345, 2)\n",
      "\n",
      "progress: 240/876\n",
      "11022\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [132503  83240]\n",
      "level_dim [132503  83240]\n",
      "name 11022\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11022.h5: total of 151 batches\n",
      "batch 0/151, 0 files processed\n",
      "batch 20/151, 10240 files processed\n",
      "batch 40/151, 20480 files processed\n",
      "batch 60/151, 30720 files processed\n",
      "batch 80/151, 40960 files processed\n",
      "batch 100/151, 51200 files processed\n",
      "batch 120/151, 61440 files processed\n",
      "batch 140/151, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11022.h5 took 111.95635986328125 s\n",
      "features size:  (76935, 1024)\n",
      "coordinates size:  (76935, 2)\n",
      "\n",
      "progress: 241/876\n",
      "11026\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [40327 29380]\n",
      "level_dim [40327 29380]\n",
      "name 11026\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11026.h5: total of 10 batches\n",
      "batch 0/10, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11026.h5 took 8.58857274055481 s\n",
      "features size:  (5002, 1024)\n",
      "coordinates size:  (5002, 2)\n",
      "\n",
      "progress: 242/876\n",
      "11029\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96016 71420]\n",
      "level_dim [96016 71420]\n",
      "name 11029\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11029.h5: total of 115 batches\n",
      "batch 0/115, 0 files processed\n",
      "batch 20/115, 10240 files processed\n",
      "batch 40/115, 20480 files processed\n",
      "batch 60/115, 30720 files processed\n",
      "batch 80/115, 40960 files processed\n",
      "batch 100/115, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11029.h5 took 86.98586988449097 s\n",
      "features size:  (58834, 1024)\n",
      "coordinates size:  (58834, 2)\n",
      "\n",
      "progress: 243/876\n",
      "11030\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [116180  83729]\n",
      "level_dim [116180  83729]\n",
      "name 11030\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11030.h5: total of 151 batches\n",
      "batch 0/151, 0 files processed\n",
      "batch 20/151, 10240 files processed\n",
      "batch 40/151, 20480 files processed\n",
      "batch 60/151, 30720 files processed\n",
      "batch 80/151, 40960 files processed\n",
      "batch 100/151, 51200 files processed\n",
      "batch 120/151, 61440 files processed\n",
      "batch 140/151, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11030.h5 took 108.10521340370178 s\n",
      "features size:  (76908, 1024)\n",
      "coordinates size:  (76908, 2)\n",
      "\n",
      "progress: 244/876\n",
      "11031\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [109459  82697]\n",
      "level_dim [109459  82697]\n",
      "name 11031\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11031.h5: total of 130 batches\n",
      "batch 0/130, 0 files processed\n",
      "batch 20/130, 10240 files processed\n",
      "batch 40/130, 20480 files processed\n",
      "batch 60/130, 30720 files processed\n",
      "batch 80/130, 40960 files processed\n",
      "batch 100/130, 51200 files processed\n",
      "batch 120/130, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11031.h5 took 107.01547694206238 s\n",
      "features size:  (66153, 1024)\n",
      "coordinates size:  (66153, 2)\n",
      "\n",
      "progress: 245/876\n",
      "11032\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [75853 65250]\n",
      "level_dim [75853 65250]\n",
      "name 11032\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11032.h5: total of 69 batches\n",
      "batch 0/69, 0 files processed\n",
      "batch 20/69, 10240 files processed\n",
      "batch 40/69, 20480 files processed\n",
      "batch 60/69, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11032.h5 took 57.18594837188721 s\n",
      "features size:  (34884, 1024)\n",
      "coordinates size:  (34884, 2)\n",
      "\n",
      "progress: 246/876\n",
      "11033\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [84494 60401]\n",
      "level_dim [84494 60401]\n",
      "name 11033\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11033.h5: total of 56 batches\n",
      "batch 0/56, 0 files processed\n",
      "batch 20/56, 10240 files processed\n",
      "batch 40/56, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11033.h5 took 51.52059078216553 s\n",
      "features size:  (28640, 1024)\n",
      "coordinates size:  (28640, 2)\n",
      "\n",
      "progress: 247/876\n",
      "11034\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [81614 73190]\n",
      "level_dim [81614 73190]\n",
      "name 11034\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11034.h5: total of 49 batches\n",
      "batch 0/49, 0 files processed\n",
      "batch 20/49, 10240 files processed\n",
      "batch 40/49, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11034.h5 took 38.25149726867676 s\n",
      "features size:  (24670, 1024)\n",
      "coordinates size:  (24670, 2)\n",
      "\n",
      "progress: 248/876\n",
      "11036\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [94096 77047]\n",
      "level_dim [94096 77047]\n",
      "name 11036\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11036.h5: total of 141 batches\n",
      "batch 0/141, 0 files processed\n",
      "batch 20/141, 10240 files processed\n",
      "batch 40/141, 20480 files processed\n",
      "batch 60/141, 30720 files processed\n",
      "batch 80/141, 40960 files processed\n",
      "batch 100/141, 51200 files processed\n",
      "batch 120/141, 61440 files processed\n",
      "batch 140/141, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11036.h5 took 123.92127084732056 s\n",
      "features size:  (71729, 1024)\n",
      "coordinates size:  (71729, 2)\n",
      "\n",
      "progress: 249/876\n",
      "11037\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [99857 65029]\n",
      "level_dim [99857 65029]\n",
      "name 11037\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11037.h5: total of 107 batches\n",
      "batch 0/107, 0 files processed\n",
      "batch 20/107, 10240 files processed\n",
      "batch 40/107, 20480 files processed\n",
      "batch 60/107, 30720 files processed\n",
      "batch 80/107, 40960 files processed\n",
      "batch 100/107, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11037.h5 took 82.58512568473816 s\n",
      "features size:  (54412, 1024)\n",
      "coordinates size:  (54412, 2)\n",
      "\n",
      "progress: 250/876\n",
      "11038\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [84494 76010]\n",
      "level_dim [84494 76010]\n",
      "name 11038\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11038.h5: total of 108 batches\n",
      "batch 0/108, 0 files processed\n",
      "batch 20/108, 10240 files processed\n",
      "batch 40/108, 20480 files processed\n",
      "batch 60/108, 30720 files processed\n",
      "batch 80/108, 40960 files processed\n",
      "batch 100/108, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11038.h5 took 91.55223345756531 s\n",
      "features size:  (55289, 1024)\n",
      "coordinates size:  (55289, 2)\n",
      "\n",
      "progress: 251/876\n",
      "11042\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [130582  81959]\n",
      "level_dim [130582  81959]\n",
      "name 11042\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11042.h5: total of 187 batches\n",
      "batch 0/187, 0 files processed\n",
      "batch 20/187, 10240 files processed\n",
      "batch 40/187, 20480 files processed\n",
      "batch 60/187, 30720 files processed\n",
      "batch 80/187, 40960 files processed\n",
      "batch 100/187, 51200 files processed\n",
      "batch 120/187, 61440 files processed\n",
      "batch 140/187, 71680 files processed\n",
      "batch 160/187, 81920 files processed\n",
      "batch 180/187, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11042.h5 took 168.42449927330017 s\n",
      "features size:  (95685, 1024)\n",
      "coordinates size:  (95685, 2)\n",
      "\n",
      "progress: 252/876\n",
      "11043\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [120020  80664]\n",
      "level_dim [120020  80664]\n",
      "name 11043\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11043.h5: total of 184 batches\n",
      "batch 0/184, 0 files processed\n",
      "batch 20/184, 10240 files processed\n",
      "batch 40/184, 20480 files processed\n",
      "batch 60/184, 30720 files processed\n",
      "batch 80/184, 40960 files processed\n",
      "batch 100/184, 51200 files processed\n",
      "batch 120/184, 61440 files processed\n",
      "batch 140/184, 71680 files processed\n",
      "batch 160/184, 81920 files processed\n",
      "batch 180/184, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11043.h5 took 150.40255331993103 s\n",
      "features size:  (93799, 1024)\n",
      "coordinates size:  (93799, 2)\n",
      "\n",
      "progress: 253/876\n",
      "11044\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [103698  77572]\n",
      "level_dim [103698  77572]\n",
      "name 11044\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11044.h5: total of 132 batches\n",
      "batch 0/132, 0 files processed\n",
      "batch 20/132, 10240 files processed\n",
      "batch 40/132, 20480 files processed\n",
      "batch 60/132, 30720 files processed\n",
      "batch 80/132, 40960 files processed\n",
      "batch 100/132, 51200 files processed\n",
      "batch 120/132, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11044.h5 took 98.19846844673157 s\n",
      "features size:  (67204, 1024)\n",
      "coordinates size:  (67204, 2)\n",
      "\n",
      "progress: 254/876\n",
      "11053\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [82574 74471]\n",
      "level_dim [82574 74471]\n",
      "name 11053\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11053.h5: total of 108 batches\n",
      "batch 0/108, 0 files processed\n",
      "batch 20/108, 10240 files processed\n",
      "batch 40/108, 20480 files processed\n",
      "batch 60/108, 30720 files processed\n",
      "batch 80/108, 40960 files processed\n",
      "batch 100/108, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11053.h5 took 94.21838068962097 s\n",
      "features size:  (55233, 1024)\n",
      "coordinates size:  (55233, 2)\n",
      "\n",
      "progress: 255/876\n",
      "11101\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [83534 80357]\n",
      "level_dim [83534 80357]\n",
      "name 11101\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11101.h5: total of 69 batches\n",
      "batch 0/69, 0 files processed\n",
      "batch 20/69, 10240 files processed\n",
      "batch 40/69, 20480 files processed\n",
      "batch 60/69, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11101.h5 took 54.812042236328125 s\n",
      "features size:  (35240, 1024)\n",
      "coordinates size:  (35240, 2)\n",
      "\n",
      "progress: 256/876\n",
      "11104\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [24964 21427]\n",
      "level_dim [24964 21427]\n",
      "name 11104\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11104.h5: total of 10 batches\n",
      "batch 0/10, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11104.h5 took 10.333858728408813 s\n",
      "features size:  (4822, 1024)\n",
      "coordinates size:  (4822, 2)\n",
      "\n",
      "progress: 257/876\n",
      "11105\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [83534 83684]\n",
      "level_dim [83534 83684]\n",
      "name 11105\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11105.h5: total of 113 batches\n",
      "batch 0/113, 0 files processed\n",
      "batch 20/113, 10240 files processed\n",
      "batch 40/113, 20480 files processed\n",
      "batch 60/113, 30720 files processed\n",
      "batch 80/113, 40960 files processed\n",
      "batch 100/113, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11105.h5 took 95.29115128517151 s\n",
      "features size:  (57763, 1024)\n",
      "coordinates size:  (57763, 2)\n",
      "\n",
      "progress: 258/876\n",
      "11106\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [68171 78034]\n",
      "level_dim [68171 78034]\n",
      "name 11106\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11106.h5: total of 106 batches\n",
      "batch 0/106, 0 files processed\n",
      "batch 20/106, 10240 files processed\n",
      "batch 40/106, 20480 files processed\n",
      "batch 60/106, 30720 files processed\n",
      "batch 80/106, 40960 files processed\n",
      "batch 100/106, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11106.h5 took 81.22980427742004 s\n",
      "features size:  (53817, 1024)\n",
      "coordinates size:  (53817, 2)\n",
      "\n",
      "progress: 259/876\n",
      "11107\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [100817  72705]\n",
      "level_dim [100817  72705]\n",
      "name 11107\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11107.h5: total of 133 batches\n",
      "batch 0/133, 0 files processed\n",
      "batch 20/133, 10240 files processed\n",
      "batch 40/133, 20480 files processed\n",
      "batch 60/133, 30720 files processed\n",
      "batch 80/133, 40960 files processed\n",
      "batch 100/133, 51200 files processed\n",
      "batch 120/133, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11107.h5 took 117.62639379501343 s\n",
      "features size:  (67730, 1024)\n",
      "coordinates size:  (67730, 2)\n",
      "\n",
      "progress: 260/876\n",
      "11112\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [67211 65239]\n",
      "level_dim [67211 65239]\n",
      "name 11112\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11112.h5: total of 63 batches\n",
      "batch 0/63, 0 files processed\n",
      "batch 20/63, 10240 files processed\n",
      "batch 40/63, 20480 files processed\n",
      "batch 60/63, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11112.h5 took 47.42916536331177 s\n",
      "features size:  (31846, 1024)\n",
      "coordinates size:  (31846, 2)\n",
      "\n",
      "progress: 261/876\n",
      "11113\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [53769 67779]\n",
      "level_dim [53769 67779]\n",
      "name 11113\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11113.h5: total of 32 batches\n",
      "batch 0/32, 0 files processed\n",
      "batch 20/32, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11113.h5 took 25.171696662902832 s\n",
      "features size:  (16293, 1024)\n",
      "coordinates size:  (16293, 2)\n",
      "\n",
      "progress: 262/876\n",
      "11117\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [88335 80109]\n",
      "level_dim [88335 80109]\n",
      "name 11117\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11117.h5: total of 132 batches\n",
      "batch 0/132, 0 files processed\n",
      "batch 20/132, 10240 files processed\n",
      "batch 40/132, 20480 files processed\n",
      "batch 60/132, 30720 files processed\n",
      "batch 80/132, 40960 files processed\n",
      "batch 100/132, 51200 files processed\n",
      "batch 120/132, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11117.h5 took 103.57742881774902 s\n",
      "features size:  (67548, 1024)\n",
      "coordinates size:  (67548, 2)\n",
      "\n",
      "progress: 263/876\n",
      "11118\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [91215 65272]\n",
      "level_dim [91215 65272]\n",
      "name 11118\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11118.h5: total of 87 batches\n",
      "batch 0/87, 0 files processed\n",
      "batch 20/87, 10240 files processed\n",
      "batch 40/87, 20480 files processed\n",
      "batch 60/87, 30720 files processed\n",
      "batch 80/87, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11118.h5 took 77.79755783081055 s\n",
      "features size:  (44449, 1024)\n",
      "coordinates size:  (44449, 2)\n",
      "\n",
      "progress: 264/876\n",
      "11119\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [90255 68854]\n",
      "level_dim [90255 68854]\n",
      "name 11119\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11119.h5: total of 106 batches\n",
      "batch 0/106, 0 files processed\n",
      "batch 20/106, 10240 files processed\n",
      "batch 40/106, 20480 files processed\n",
      "batch 60/106, 30720 files processed\n",
      "batch 80/106, 40960 files processed\n",
      "batch 100/106, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11119.h5 took 92.84451365470886 s\n",
      "features size:  (54109, 1024)\n",
      "coordinates size:  (54109, 2)\n",
      "\n",
      "progress: 265/876\n",
      "11120\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [73932 71389]\n",
      "level_dim [73932 71389]\n",
      "name 11120\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11120.h5: total of 49 batches\n",
      "batch 0/49, 0 files processed\n",
      "batch 20/49, 10240 files processed\n",
      "batch 40/49, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11120.h5 took 40.52768301963806 s\n",
      "features size:  (24599, 1024)\n",
      "coordinates size:  (24599, 2)\n",
      "\n",
      "progress: 266/876\n",
      "11124\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [102737  83711]\n",
      "level_dim [102737  83711]\n",
      "name 11124\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11124.h5: total of 157 batches\n",
      "batch 0/157, 0 files processed\n",
      "batch 20/157, 10240 files processed\n",
      "batch 40/157, 20480 files processed\n",
      "batch 60/157, 30720 files processed\n",
      "batch 80/157, 40960 files processed\n",
      "batch 100/157, 51200 files processed\n",
      "batch 120/157, 61440 files processed\n",
      "batch 140/157, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11124.h5 took 127.75144505500793 s\n",
      "features size:  (80335, 1024)\n",
      "coordinates size:  (80335, 2)\n",
      "\n",
      "progress: 267/876\n",
      "11125\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [105618  81156]\n",
      "level_dim [105618  81156]\n",
      "name 11125\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11125.h5: total of 165 batches\n",
      "batch 0/165, 0 files processed\n",
      "batch 20/165, 10240 files processed\n",
      "batch 40/165, 20480 files processed\n",
      "batch 60/165, 30720 files processed\n",
      "batch 80/165, 40960 files processed\n",
      "batch 100/165, 51200 files processed\n",
      "batch 120/165, 61440 files processed\n",
      "batch 140/165, 71680 files processed\n",
      "batch 160/165, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11125.h5 took 136.62959551811218 s\n",
      "features size:  (84265, 1024)\n",
      "coordinates size:  (84265, 2)\n",
      "\n",
      "progress: 268/876\n",
      "11126\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [86415 77547]\n",
      "level_dim [86415 77547]\n",
      "name 11126\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11126.h5: total of 99 batches\n",
      "batch 0/99, 0 files processed\n",
      "batch 20/99, 10240 files processed\n",
      "batch 40/99, 20480 files processed\n",
      "batch 60/99, 30720 files processed\n",
      "batch 80/99, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11126.h5 took 73.06290769577026 s\n",
      "features size:  (50536, 1024)\n",
      "coordinates size:  (50536, 2)\n",
      "\n",
      "progress: 269/876\n",
      "11129\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [117140  83731]\n",
      "level_dim [117140  83731]\n",
      "name 11129\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11129.h5: total of 174 batches\n",
      "batch 0/174, 0 files processed\n",
      "batch 20/174, 10240 files processed\n",
      "batch 40/174, 20480 files processed\n",
      "batch 60/174, 30720 files processed\n",
      "batch 80/174, 40960 files processed\n",
      "batch 100/174, 51200 files processed\n",
      "batch 120/174, 61440 files processed\n",
      "batch 140/174, 71680 files processed\n",
      "batch 160/174, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11129.h5 took 151.5439488887787 s\n",
      "features size:  (88754, 1024)\n",
      "coordinates size:  (88754, 2)\n",
      "\n",
      "progress: 270/876\n",
      "11130\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [99857 83707]\n",
      "level_dim [99857 83707]\n",
      "name 11130\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11130.h5: total of 132 batches\n",
      "batch 0/132, 0 files processed\n",
      "batch 20/132, 10240 files processed\n",
      "batch 40/132, 20480 files processed\n",
      "batch 60/132, 30720 files processed\n",
      "batch 80/132, 40960 files processed\n",
      "batch 100/132, 51200 files processed\n",
      "batch 120/132, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11130.h5 took 107.82140040397644 s\n",
      "features size:  (67454, 1024)\n",
      "coordinates size:  (67454, 2)\n",
      "\n",
      "progress: 271/876\n",
      "11131\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [115220  83728]\n",
      "level_dim [115220  83728]\n",
      "name 11131\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11131.h5: total of 157 batches\n",
      "batch 0/157, 0 files processed\n",
      "batch 20/157, 10240 files processed\n",
      "batch 40/157, 20480 files processed\n",
      "batch 60/157, 30720 files processed\n",
      "batch 80/157, 40960 files processed\n",
      "batch 100/157, 51200 files processed\n",
      "batch 120/157, 61440 files processed\n",
      "batch 140/157, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11131.h5 took 119.97818422317505 s\n",
      "features size:  (80189, 1024)\n",
      "coordinates size:  (80189, 2)\n",
      "\n",
      "progress: 272/876\n",
      "11132\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [34566 34234]\n",
      "level_dim [34566 34234]\n",
      "name 11132\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11132.h5: total of 23 batches\n",
      "batch 0/23, 0 files processed\n",
      "batch 20/23, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11132.h5 took 22.821062803268433 s\n",
      "features size:  (11555, 1024)\n",
      "coordinates size:  (11555, 2)\n",
      "\n",
      "progress: 273/876\n",
      "11133\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [48008 50883]\n",
      "level_dim [48008 50883]\n",
      "name 11133\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11133.h5: total of 46 batches\n",
      "batch 0/46, 0 files processed\n",
      "batch 20/46, 10240 files processed\n",
      "batch 40/46, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11133.h5 took 40.78225135803223 s\n",
      "features size:  (23328, 1024)\n",
      "coordinates size:  (23328, 2)\n",
      "\n",
      "progress: 274/876\n",
      "11134\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [89295 54780]\n",
      "level_dim [89295 54780]\n",
      "name 11134\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11134.h5: total of 59 batches\n",
      "batch 0/59, 0 files processed\n",
      "batch 20/59, 10240 files processed\n",
      "batch 40/59, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11134.h5 took 47.691991090774536 s\n",
      "features size:  (29900, 1024)\n",
      "coordinates size:  (29900, 2)\n",
      "\n",
      "progress: 275/876\n",
      "11135\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [95056 75256]\n",
      "level_dim [95056 75256]\n",
      "name 11135\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11135.h5: total of 96 batches\n",
      "batch 0/96, 0 files processed\n",
      "batch 20/96, 10240 files processed\n",
      "batch 40/96, 20480 files processed\n",
      "batch 60/96, 30720 files processed\n",
      "batch 80/96, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11135.h5 took 77.4823157787323 s\n",
      "features size:  (48948, 1024)\n",
      "coordinates size:  (48948, 2)\n",
      "\n",
      "progress: 276/876\n",
      "11136\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [107538  56596]\n",
      "level_dim [107538  56596]\n",
      "name 11136\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11136.h5: total of 96 batches\n",
      "batch 0/96, 0 files processed\n",
      "batch 20/96, 10240 files processed\n",
      "batch 40/96, 20480 files processed\n",
      "batch 60/96, 30720 files processed\n",
      "batch 80/96, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11136.h5 took 72.60701823234558 s\n",
      "features size:  (48982, 1024)\n",
      "coordinates size:  (48982, 2)\n",
      "\n",
      "progress: 277/876\n",
      "11182\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [133463  83754]\n",
      "level_dim [133463  83754]\n",
      "name 11182\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11182.h5: total of 223 batches\n",
      "batch 0/223, 0 files processed\n",
      "batch 20/223, 10240 files processed\n",
      "batch 40/223, 20480 files processed\n",
      "batch 60/223, 30720 files processed\n",
      "batch 80/223, 40960 files processed\n",
      "batch 100/223, 51200 files processed\n",
      "batch 120/223, 61440 files processed\n",
      "batch 140/223, 71680 files processed\n",
      "batch 160/223, 81920 files processed\n",
      "batch 180/223, 92160 files processed\n",
      "batch 200/223, 102400 files processed\n",
      "batch 220/223, 112640 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11182.h5 took 156.8608992099762 s\n",
      "features size:  (113732, 1024)\n",
      "coordinates size:  (113732, 2)\n",
      "\n",
      "progress: 278/876\n",
      "11183\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [133463  82474]\n",
      "level_dim [133463  82474]\n",
      "name 11183\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11183.h5: total of 175 batches\n",
      "batch 0/175, 0 files processed\n",
      "batch 20/175, 10240 files processed\n",
      "batch 40/175, 20480 files processed\n",
      "batch 60/175, 30720 files processed\n",
      "batch 80/175, 40960 files processed\n",
      "batch 100/175, 51200 files processed\n",
      "batch 120/175, 61440 files processed\n",
      "batch 140/175, 71680 files processed\n",
      "batch 160/175, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11183.h5 took 130.66173100471497 s\n",
      "features size:  (89504, 1024)\n",
      "coordinates size:  (89504, 2)\n",
      "\n",
      "progress: 279/876\n",
      "11184\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [111379  83723]\n",
      "level_dim [111379  83723]\n",
      "name 11184\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11184.h5: total of 182 batches\n",
      "batch 0/182, 0 files processed\n",
      "batch 20/182, 10240 files processed\n",
      "batch 40/182, 20480 files processed\n",
      "batch 60/182, 30720 files processed\n",
      "batch 80/182, 40960 files processed\n",
      "batch 100/182, 51200 files processed\n",
      "batch 120/182, 61440 files processed\n",
      "batch 140/182, 71680 files processed\n",
      "batch 160/182, 81920 files processed\n",
      "batch 180/182, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11184.h5 took 115.10802173614502 s\n",
      "features size:  (92843, 1024)\n",
      "coordinates size:  (92843, 2)\n",
      "\n",
      "progress: 280/876\n",
      "11185\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [80654 77540]\n",
      "level_dim [80654 77540]\n",
      "name 11185\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11185.h5: total of 108 batches\n",
      "batch 0/108, 0 files processed\n",
      "batch 20/108, 10240 files processed\n",
      "batch 40/108, 20480 files processed\n",
      "batch 60/108, 30720 files processed\n",
      "batch 80/108, 40960 files processed\n",
      "batch 100/108, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11185.h5 took 80.26365995407104 s\n",
      "features size:  (55099, 1024)\n",
      "coordinates size:  (55099, 2)\n",
      "\n",
      "progress: 281/876\n",
      "11186\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [82574 81124]\n",
      "level_dim [82574 81124]\n",
      "name 11186\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11186.h5: total of 113 batches\n",
      "batch 0/113, 0 files processed\n",
      "batch 20/113, 10240 files processed\n",
      "batch 40/113, 20480 files processed\n",
      "batch 60/113, 30720 files processed\n",
      "batch 80/113, 40960 files processed\n",
      "batch 100/113, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11186.h5 took 84.24354529380798 s\n",
      "features size:  (57650, 1024)\n",
      "coordinates size:  (57650, 2)\n",
      "\n",
      "progress: 282/876\n",
      "11187\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [78733 63464]\n",
      "level_dim [78733 63464]\n",
      "name 11187\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11187.h5: total of 73 batches\n",
      "batch 0/73, 0 files processed\n",
      "batch 20/73, 10240 files processed\n",
      "batch 40/73, 20480 files processed\n",
      "batch 60/73, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11187.h5 took 40.019773960113525 s\n",
      "features size:  (37256, 1024)\n",
      "coordinates size:  (37256, 2)\n",
      "\n",
      "progress: 283/876\n",
      "11188\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [123861  83740]\n",
      "level_dim [123861  83740]\n",
      "name 11188\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11188.h5: total of 222 batches\n",
      "batch 0/222, 0 files processed\n",
      "batch 20/222, 10240 files processed\n",
      "batch 40/222, 20480 files processed\n",
      "batch 60/222, 30720 files processed\n",
      "batch 80/222, 40960 files processed\n",
      "batch 100/222, 51200 files processed\n",
      "batch 120/222, 61440 files processed\n",
      "batch 140/222, 71680 files processed\n",
      "batch 160/222, 81920 files processed\n",
      "batch 180/222, 92160 files processed\n",
      "batch 200/222, 102400 files processed\n",
      "batch 220/222, 112640 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11188.h5 took 165.77864861488342 s\n",
      "features size:  (113162, 1024)\n",
      "coordinates size:  (113162, 2)\n",
      "\n",
      "progress: 284/876\n",
      "11189\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96976 82167]\n",
      "level_dim [96976 82167]\n",
      "name 11189\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11189.h5: total of 141 batches\n",
      "batch 0/141, 0 files processed\n",
      "batch 20/141, 10240 files processed\n",
      "batch 40/141, 20480 files processed\n",
      "batch 60/141, 30720 files processed\n",
      "batch 80/141, 40960 files processed\n",
      "batch 100/141, 51200 files processed\n",
      "batch 120/141, 61440 files processed\n",
      "batch 140/141, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11189.h5 took 104.90773463249207 s\n",
      "features size:  (72125, 1024)\n",
      "coordinates size:  (72125, 2)\n",
      "\n",
      "progress: 285/876\n",
      "11190\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [106578  83717]\n",
      "level_dim [106578  83717]\n",
      "name 11190\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11190.h5: total of 147 batches\n",
      "batch 0/147, 0 files processed\n",
      "batch 20/147, 10240 files processed\n",
      "batch 40/147, 20480 files processed\n",
      "batch 60/147, 30720 files processed\n",
      "batch 80/147, 40960 files processed\n",
      "batch 100/147, 51200 files processed\n",
      "batch 120/147, 61440 files processed\n",
      "batch 140/147, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11190.h5 took 111.42598152160645 s\n",
      "features size:  (74948, 1024)\n",
      "coordinates size:  (74948, 2)\n",
      "\n",
      "progress: 286/876\n",
      "11191\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [120981  83736]\n",
      "level_dim [120981  83736]\n",
      "name 11191\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11191.h5: total of 189 batches\n",
      "batch 0/189, 0 files processed\n",
      "batch 20/189, 10240 files processed\n",
      "batch 40/189, 20480 files processed\n",
      "batch 60/189, 30720 files processed\n",
      "batch 80/189, 40960 files processed\n",
      "batch 100/189, 51200 files processed\n",
      "batch 120/189, 61440 files processed\n",
      "batch 140/189, 71680 files processed\n",
      "batch 160/189, 81920 files processed\n",
      "batch 180/189, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11191.h5 took 111.34487295150757 s\n",
      "features size:  (96375, 1024)\n",
      "coordinates size:  (96375, 2)\n",
      "\n",
      "progress: 287/876\n",
      "11192\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [116180  83729]\n",
      "level_dim [116180  83729]\n",
      "name 11192\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11192.h5: total of 139 batches\n",
      "batch 0/139, 0 files processed\n",
      "batch 20/139, 10240 files processed\n",
      "batch 40/139, 20480 files processed\n",
      "batch 60/139, 30720 files processed\n",
      "batch 80/139, 40960 files processed\n",
      "batch 100/139, 51200 files processed\n",
      "batch 120/139, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11192.h5 took 119.05850434303284 s\n",
      "features size:  (70854, 1024)\n",
      "coordinates size:  (70854, 2)\n",
      "\n",
      "progress: 288/876\n",
      "11193\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [77773 48622]\n",
      "level_dim [77773 48622]\n",
      "name 11193\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11193.h5: total of 64 batches\n",
      "batch 0/64, 0 files processed\n",
      "batch 20/64, 10240 files processed\n",
      "batch 40/64, 20480 files processed\n",
      "batch 60/64, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11193.h5 took 56.92301797866821 s\n",
      "features size:  (32420, 1024)\n",
      "coordinates size:  (32420, 2)\n",
      "\n",
      "progress: 289/876\n",
      "11194\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [101777  83709]\n",
      "level_dim [101777  83709]\n",
      "name 11194\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11194.h5: total of 144 batches\n",
      "batch 0/144, 0 files processed\n",
      "batch 20/144, 10240 files processed\n",
      "batch 40/144, 20480 files processed\n",
      "batch 60/144, 30720 files processed\n",
      "batch 80/144, 40960 files processed\n",
      "batch 100/144, 51200 files processed\n",
      "batch 120/144, 61440 files processed\n",
      "batch 140/144, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11194.h5 took 115.24522542953491 s\n",
      "features size:  (73727, 1024)\n",
      "coordinates size:  (73727, 2)\n",
      "\n",
      "progress: 290/876\n",
      "11215\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51849 57286]\n",
      "level_dim [51849 57286]\n",
      "name 11215\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11215.h5: total of 54 batches\n",
      "batch 0/54, 0 files processed\n",
      "batch 20/54, 10240 files processed\n",
      "batch 40/54, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11215.h5 took 31.828076124191284 s\n",
      "features size:  (27561, 1024)\n",
      "coordinates size:  (27561, 2)\n",
      "\n",
      "progress: 291/876\n",
      "11216\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [91215 67575]\n",
      "level_dim [91215 67575]\n",
      "name 11216\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11216.h5: total of 73 batches\n",
      "batch 0/73, 0 files processed\n",
      "batch 20/73, 10240 files processed\n",
      "batch 40/73, 20480 files processed\n",
      "batch 60/73, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11216.h5 took 61.48789691925049 s\n",
      "features size:  (37268, 1024)\n",
      "coordinates size:  (37268, 2)\n",
      "\n",
      "progress: 292/876\n",
      "11217\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [58570 59597]\n",
      "level_dim [58570 59597]\n",
      "name 11217\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11217.h5: total of 37 batches\n",
      "batch 0/37, 0 files processed\n",
      "batch 20/37, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11217.h5 took 30.770875453948975 s\n",
      "features size:  (18456, 1024)\n",
      "coordinates size:  (18456, 2)\n",
      "\n",
      "progress: 293/876\n",
      "11218\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [97937 63747]\n",
      "level_dim [97937 63747]\n",
      "name 11218\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11218.h5: total of 117 batches\n",
      "batch 0/117, 0 files processed\n",
      "batch 20/117, 10240 files processed\n",
      "batch 40/117, 20480 files processed\n",
      "batch 60/117, 30720 files processed\n",
      "batch 80/117, 40960 files processed\n",
      "batch 100/117, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11218.h5 took 66.31355237960815 s\n",
      "features size:  (59640, 1024)\n",
      "coordinates size:  (59640, 2)\n",
      "\n",
      "progress: 294/876\n",
      "11219\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [110419  78092]\n",
      "level_dim [110419  78092]\n",
      "name 11219\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11219.h5: total of 106 batches\n",
      "batch 0/106, 0 files processed\n",
      "batch 20/106, 10240 files processed\n",
      "batch 40/106, 20480 files processed\n",
      "batch 60/106, 30720 files processed\n",
      "batch 80/106, 40960 files processed\n",
      "batch 100/106, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11219.h5 took 79.94589138031006 s\n",
      "features size:  (53900, 1024)\n",
      "coordinates size:  (53900, 2)\n",
      "\n",
      "progress: 295/876\n",
      "11223\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [129622  80167]\n",
      "level_dim [129622  80167]\n",
      "name 11223\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11223.h5: total of 143 batches\n",
      "batch 0/143, 0 files processed\n",
      "batch 20/143, 10240 files processed\n",
      "batch 40/143, 20480 files processed\n",
      "batch 60/143, 30720 files processed\n",
      "batch 80/143, 40960 files processed\n",
      "batch 100/143, 51200 files processed\n",
      "batch 120/143, 61440 files processed\n",
      "batch 140/143, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11223.h5 took 119.1058247089386 s\n",
      "features size:  (73022, 1024)\n",
      "coordinates size:  (73022, 2)\n",
      "\n",
      "progress: 296/876\n",
      "11224\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [102737  83199]\n",
      "level_dim [102737  83199]\n",
      "name 11224\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11224.h5: total of 127 batches\n",
      "batch 0/127, 0 files processed\n",
      "batch 20/127, 10240 files processed\n",
      "batch 40/127, 20480 files processed\n",
      "batch 60/127, 30720 files processed\n",
      "batch 80/127, 40960 files processed\n",
      "batch 100/127, 51200 files processed\n",
      "batch 120/127, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11224.h5 took 74.32343554496765 s\n",
      "features size:  (64566, 1024)\n",
      "coordinates size:  (64566, 2)\n",
      "\n",
      "progress: 297/876\n",
      "11225\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [85454 75243]\n",
      "level_dim [85454 75243]\n",
      "name 11225\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11225.h5: total of 78 batches\n",
      "batch 0/78, 0 files processed\n",
      "batch 20/78, 10240 files processed\n",
      "batch 40/78, 20480 files processed\n",
      "batch 60/78, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11225.h5 took 70.13364338874817 s\n",
      "features size:  (39886, 1024)\n",
      "coordinates size:  (39886, 2)\n",
      "\n",
      "progress: 298/876\n",
      "11226\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [116180  74262]\n",
      "level_dim [116180  74262]\n",
      "name 11226\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11226.h5: total of 170 batches\n",
      "batch 0/170, 0 files processed\n",
      "batch 20/170, 10240 files processed\n",
      "batch 40/170, 20480 files processed\n",
      "batch 60/170, 30720 files processed\n",
      "batch 80/170, 40960 files processed\n",
      "batch 100/170, 51200 files processed\n",
      "batch 120/170, 61440 files processed\n",
      "batch 140/170, 71680 files processed\n",
      "batch 160/170, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11226.h5 took 146.9942877292633 s\n",
      "features size:  (86856, 1024)\n",
      "coordinates size:  (86856, 2)\n",
      "\n",
      "progress: 299/876\n",
      "11227\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [109459  81928]\n",
      "level_dim [109459  81928]\n",
      "name 11227\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11227.h5: total of 149 batches\n",
      "batch 0/149, 0 files processed\n",
      "batch 20/149, 10240 files processed\n",
      "batch 40/149, 20480 files processed\n",
      "batch 60/149, 30720 files processed\n",
      "batch 80/149, 40960 files processed\n",
      "batch 100/149, 51200 files processed\n",
      "batch 120/149, 61440 files processed\n",
      "batch 140/149, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11227.h5 took 126.89303731918335 s\n",
      "features size:  (75820, 1024)\n",
      "coordinates size:  (75820, 2)\n",
      "\n",
      "progress: 300/876\n",
      "11228\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [125781  80161]\n",
      "level_dim [125781  80161]\n",
      "name 11228\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11228.h5: total of 155 batches\n",
      "batch 0/155, 0 files processed\n",
      "batch 20/155, 10240 files processed\n",
      "batch 40/155, 20480 files processed\n",
      "batch 60/155, 30720 files processed\n",
      "batch 80/155, 40960 files processed\n",
      "batch 100/155, 51200 files processed\n",
      "batch 120/155, 61440 files processed\n",
      "batch 140/155, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11228.h5 took 132.481271982193 s\n",
      "features size:  (78932, 1024)\n",
      "coordinates size:  (78932, 2)\n",
      "\n",
      "progress: 301/876\n",
      "11229\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [116180  75542]\n",
      "level_dim [116180  75542]\n",
      "name 11229\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11229.h5: total of 164 batches\n",
      "batch 0/164, 0 files processed\n",
      "batch 20/164, 10240 files processed\n",
      "batch 40/164, 20480 files processed\n",
      "batch 60/164, 30720 files processed\n",
      "batch 80/164, 40960 files processed\n",
      "batch 100/164, 51200 files processed\n",
      "batch 120/164, 61440 files processed\n",
      "batch 140/164, 71680 files processed\n",
      "batch 160/164, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11229.h5 took 134.79496097564697 s\n",
      "features size:  (83846, 1024)\n",
      "coordinates size:  (83846, 2)\n",
      "\n",
      "progress: 302/876\n",
      "11233\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [99857 81915]\n",
      "level_dim [99857 81915]\n",
      "name 11233\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11233.h5: total of 89 batches\n",
      "batch 0/89, 0 files processed\n",
      "batch 20/89, 10240 files processed\n",
      "batch 40/89, 20480 files processed\n",
      "batch 60/89, 30720 files processed\n",
      "batch 80/89, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11233.h5 took 72.22058868408203 s\n",
      "features size:  (45431, 1024)\n",
      "coordinates size:  (45431, 2)\n",
      "\n",
      "progress: 303/876\n",
      "11234\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [110419  76813]\n",
      "level_dim [110419  76813]\n",
      "name 11234\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11234.h5: total of 136 batches\n",
      "batch 0/136, 0 files processed\n",
      "batch 20/136, 10240 files processed\n",
      "batch 40/136, 20480 files processed\n",
      "batch 60/136, 30720 files processed\n",
      "batch 80/136, 40960 files processed\n",
      "batch 100/136, 51200 files processed\n",
      "batch 120/136, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11234.h5 took 110.58078813552856 s\n",
      "features size:  (69179, 1024)\n",
      "coordinates size:  (69179, 2)\n",
      "\n",
      "progress: 304/876\n",
      "11235\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [107538  77833]\n",
      "level_dim [107538  77833]\n",
      "name 11235\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11235.h5: total of 125 batches\n",
      "batch 0/125, 0 files processed\n",
      "batch 20/125, 10240 files processed\n",
      "batch 40/125, 20480 files processed\n",
      "batch 60/125, 30720 files processed\n",
      "batch 80/125, 40960 files processed\n",
      "batch 100/125, 51200 files processed\n",
      "batch 120/125, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11235.h5 took 88.76116609573364 s\n",
      "features size:  (63658, 1024)\n",
      "coordinates size:  (63658, 2)\n",
      "\n",
      "progress: 305/876\n",
      "11236\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [117140  65565]\n",
      "level_dim [117140  65565]\n",
      "name 11236\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11236.h5: total of 163 batches\n",
      "batch 0/163, 0 files processed\n",
      "batch 20/163, 10240 files processed\n",
      "batch 40/163, 20480 files processed\n",
      "batch 60/163, 30720 files processed\n",
      "batch 80/163, 40960 files processed\n",
      "batch 100/163, 51200 files processed\n",
      "batch 120/163, 61440 files processed\n",
      "batch 140/163, 71680 files processed\n",
      "batch 160/163, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11236.h5 took 139.56102561950684 s\n",
      "features size:  (83195, 1024)\n",
      "coordinates size:  (83195, 2)\n",
      "\n",
      "progress: 306/876\n",
      "11237\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96016 83446]\n",
      "level_dim [96016 83446]\n",
      "name 11237\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11237.h5: total of 169 batches\n",
      "batch 0/169, 0 files processed\n",
      "batch 20/169, 10240 files processed\n",
      "batch 40/169, 20480 files processed\n",
      "batch 60/169, 30720 files processed\n",
      "batch 80/169, 40960 files processed\n",
      "batch 100/169, 51200 files processed\n",
      "batch 120/169, 61440 files processed\n",
      "batch 140/169, 71680 files processed\n",
      "batch 160/169, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11237.h5 took 137.64489912986755 s\n",
      "features size:  (86056, 1024)\n",
      "coordinates size:  (86056, 2)\n",
      "\n",
      "progress: 307/876\n",
      "11238\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [136343  83758]\n",
      "level_dim [136343  83758]\n",
      "name 11238\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11238.h5: total of 152 batches\n",
      "batch 0/152, 0 files processed\n",
      "batch 20/152, 10240 files processed\n",
      "batch 40/152, 20480 files processed\n",
      "batch 60/152, 30720 files processed\n",
      "batch 80/152, 40960 files processed\n",
      "batch 100/152, 51200 files processed\n",
      "batch 120/152, 61440 files processed\n",
      "batch 140/152, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11238.h5 took 119.25739002227783 s\n",
      "features size:  (77597, 1024)\n",
      "coordinates size:  (77597, 2)\n",
      "\n",
      "progress: 308/876\n",
      "11239\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [145945  83771]\n",
      "level_dim [145945  83771]\n",
      "name 11239\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11239.h5: total of 162 batches\n",
      "batch 0/162, 0 files processed\n",
      "batch 20/162, 10240 files processed\n",
      "batch 40/162, 20480 files processed\n",
      "batch 60/162, 30720 files processed\n",
      "batch 80/162, 40960 files processed\n",
      "batch 100/162, 51200 files processed\n",
      "batch 120/162, 61440 files processed\n",
      "batch 140/162, 71680 files processed\n",
      "batch 160/162, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11239.h5 took 131.73283410072327 s\n",
      "features size:  (82801, 1024)\n",
      "coordinates size:  (82801, 2)\n",
      "\n",
      "progress: 309/876\n",
      "11240\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [94096 83698]\n",
      "level_dim [94096 83698]\n",
      "name 11240\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11240.h5: total of 171 batches\n",
      "batch 0/171, 0 files processed\n",
      "batch 20/171, 10240 files processed\n",
      "batch 40/171, 20480 files processed\n",
      "batch 60/171, 30720 files processed\n",
      "batch 80/171, 40960 files processed\n",
      "batch 100/171, 51200 files processed\n",
      "batch 120/171, 61440 files processed\n",
      "batch 140/171, 71680 files processed\n",
      "batch 160/171, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11240.h5 took 140.30889534950256 s\n",
      "features size:  (87062, 1024)\n",
      "coordinates size:  (87062, 2)\n",
      "\n",
      "progress: 310/876\n",
      "11241\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [115220  83728]\n",
      "level_dim [115220  83728]\n",
      "name 11241\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11241.h5: total of 195 batches\n",
      "batch 0/195, 0 files processed\n",
      "batch 20/195, 10240 files processed\n",
      "batch 40/195, 20480 files processed\n",
      "batch 60/195, 30720 files processed\n",
      "batch 80/195, 40960 files processed\n",
      "batch 100/195, 51200 files processed\n",
      "batch 120/195, 61440 files processed\n",
      "batch 140/195, 71680 files processed\n",
      "batch 160/195, 81920 files processed\n",
      "batch 180/195, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11241.h5 took 159.32828211784363 s\n",
      "features size:  (99628, 1024)\n",
      "coordinates size:  (99628, 2)\n",
      "\n",
      "progress: 311/876\n",
      "11242\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [60490 78278]\n",
      "level_dim [60490 78278]\n",
      "name 11242\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11242.h5: total of 90 batches\n",
      "batch 0/90, 0 files processed\n",
      "batch 20/90, 10240 files processed\n",
      "batch 40/90, 20480 files processed\n",
      "batch 60/90, 30720 files processed\n",
      "batch 80/90, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11242.h5 took 80.31097912788391 s\n",
      "features size:  (45975, 1024)\n",
      "coordinates size:  (45975, 2)\n",
      "\n",
      "progress: 312/876\n",
      "11243\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [102737  68102]\n",
      "level_dim [102737  68102]\n",
      "name 11243\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11243.h5: total of 83 batches\n",
      "batch 0/83, 0 files processed\n",
      "batch 20/83, 10240 files processed\n",
      "batch 40/83, 20480 files processed\n",
      "batch 60/83, 30720 files processed\n",
      "batch 80/83, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11243.h5 took 57.848785638809204 s\n",
      "features size:  (42394, 1024)\n",
      "coordinates size:  (42394, 2)\n",
      "\n",
      "progress: 313/876\n",
      "11247\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [99857 79356]\n",
      "level_dim [99857 79356]\n",
      "name 11247\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11247.h5: total of 174 batches\n",
      "batch 0/174, 0 files processed\n",
      "batch 20/174, 10240 files processed\n",
      "batch 40/174, 20480 files processed\n",
      "batch 60/174, 30720 files processed\n",
      "batch 80/174, 40960 files processed\n",
      "batch 100/174, 51200 files processed\n",
      "batch 120/174, 61440 files processed\n",
      "batch 140/174, 71680 files processed\n",
      "batch 160/174, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11247.h5 took 141.92444920539856 s\n",
      "features size:  (88698, 1024)\n",
      "coordinates size:  (88698, 2)\n",
      "\n",
      "progress: 314/876\n",
      "11248\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [107538  77064]\n",
      "level_dim [107538  77064]\n",
      "name 11248\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11248.h5: total of 168 batches\n",
      "batch 0/168, 0 files processed\n",
      "batch 20/168, 10240 files processed\n",
      "batch 40/168, 20480 files processed\n",
      "batch 60/168, 30720 files processed\n",
      "batch 80/168, 40960 files processed\n",
      "batch 100/168, 51200 files processed\n",
      "batch 120/168, 61440 files processed\n",
      "batch 140/168, 71680 files processed\n",
      "batch 160/168, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11248.h5 took 99.11233472824097 s\n",
      "features size:  (85628, 1024)\n",
      "coordinates size:  (85628, 2)\n",
      "\n",
      "progress: 315/876\n",
      "11249\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [88335 63733]\n",
      "level_dim [88335 63733]\n",
      "name 11249\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11249.h5: total of 89 batches\n",
      "batch 0/89, 0 files processed\n",
      "batch 20/89, 10240 files processed\n",
      "batch 40/89, 20480 files processed\n",
      "batch 60/89, 30720 files processed\n",
      "batch 80/89, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11249.h5 took 75.66856122016907 s\n",
      "features size:  (45363, 1024)\n",
      "coordinates size:  (45363, 2)\n",
      "\n",
      "progress: 316/876\n",
      "11250\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [97937 76284]\n",
      "level_dim [97937 76284]\n",
      "name 11250\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11250.h5: total of 135 batches\n",
      "batch 0/135, 0 files processed\n",
      "batch 20/135, 10240 files processed\n",
      "batch 40/135, 20480 files processed\n",
      "batch 60/135, 30720 files processed\n",
      "batch 80/135, 40960 files processed\n",
      "batch 100/135, 51200 files processed\n",
      "batch 120/135, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11250.h5 took 88.76687526702881 s\n",
      "features size:  (69068, 1024)\n",
      "coordinates size:  (69068, 2)\n",
      "\n",
      "progress: 317/876\n",
      "11251\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [100817  83708]\n",
      "level_dim [100817  83708]\n",
      "name 11251\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11251.h5: total of 154 batches\n",
      "batch 0/154, 0 files processed\n",
      "batch 20/154, 10240 files processed\n",
      "batch 40/154, 20480 files processed\n",
      "batch 60/154, 30720 files processed\n",
      "batch 80/154, 40960 files processed\n",
      "batch 100/154, 51200 files processed\n",
      "batch 120/154, 61440 files processed\n",
      "batch 140/154, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11251.h5 took 78.72446799278259 s\n",
      "features size:  (78541, 1024)\n",
      "coordinates size:  (78541, 2)\n",
      "\n",
      "progress: 318/876\n",
      "11252\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [27844 25781]\n",
      "level_dim [27844 25781]\n",
      "name 11252\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11252.h5: total of 12 batches\n",
      "batch 0/12, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11252.h5 took 12.595760345458984 s\n",
      "features size:  (5958, 1024)\n",
      "coordinates size:  (5958, 2)\n",
      "\n",
      "progress: 319/876\n",
      "11256\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [125781  83743]\n",
      "level_dim [125781  83743]\n",
      "name 11256\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11256.h5: total of 203 batches\n",
      "batch 0/203, 0 files processed\n",
      "batch 20/203, 10240 files processed\n",
      "batch 40/203, 20480 files processed\n",
      "batch 60/203, 30720 files processed\n",
      "batch 80/203, 40960 files processed\n",
      "batch 100/203, 51200 files processed\n",
      "batch 120/203, 61440 files processed\n",
      "batch 140/203, 71680 files processed\n",
      "batch 160/203, 81920 files processed\n",
      "batch 180/203, 92160 files processed\n",
      "batch 200/203, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11256.h5 took 163.9852409362793 s\n",
      "features size:  (103789, 1024)\n",
      "coordinates size:  (103789, 2)\n",
      "\n",
      "progress: 320/876\n",
      "11257\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [75853 54249]\n",
      "level_dim [75853 54249]\n",
      "name 11257\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11257.h5: total of 62 batches\n",
      "batch 0/62, 0 files processed\n",
      "batch 20/62, 10240 files processed\n",
      "batch 40/62, 20480 files processed\n",
      "batch 60/62, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11257.h5 took 49.96622157096863 s\n",
      "features size:  (31516, 1024)\n",
      "coordinates size:  (31516, 2)\n",
      "\n",
      "progress: 321/876\n",
      "11258\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [45127 52158]\n",
      "level_dim [45127 52158]\n",
      "name 11258\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11258.h5: total of 18 batches\n",
      "batch 0/18, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11258.h5 took 14.079078197479248 s\n",
      "features size:  (8834, 1024)\n",
      "coordinates size:  (8834, 2)\n",
      "\n",
      "progress: 322/876\n",
      "11259\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [50888 62913]\n",
      "level_dim [50888 62913]\n",
      "name 11259\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11259.h5: total of 57 batches\n",
      "batch 0/57, 0 files processed\n",
      "batch 20/57, 10240 files processed\n",
      "batch 40/57, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11259.h5 took 50.00186586380005 s\n",
      "features size:  (29150, 1024)\n",
      "coordinates size:  (29150, 2)\n",
      "\n",
      "progress: 323/876\n",
      "11260\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [55689 57035]\n",
      "level_dim [55689 57035]\n",
      "name 11260\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11260.h5: total of 41 batches\n",
      "batch 0/41, 0 files processed\n",
      "batch 20/41, 10240 files processed\n",
      "batch 40/41, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11260.h5 took 34.92415189743042 s\n",
      "features size:  (20850, 1024)\n",
      "coordinates size:  (20850, 2)\n",
      "\n",
      "progress: 324/876\n",
      "11261\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [55689 60617]\n",
      "level_dim [55689 60617]\n",
      "name 11261\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11261.h5: total of 60 batches\n",
      "batch 0/60, 0 files processed\n",
      "batch 20/60, 10240 files processed\n",
      "batch 40/60, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11261.h5 took 45.08978056907654 s\n",
      "features size:  (30685, 1024)\n",
      "coordinates size:  (30685, 2)\n",
      "\n",
      "progress: 325/876\n",
      "11262\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [72012 82899]\n",
      "level_dim [72012 82899]\n",
      "name 11262\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11262.h5: total of 96 batches\n",
      "batch 0/96, 0 files processed\n",
      "batch 20/96, 10240 files processed\n",
      "batch 40/96, 20480 files processed\n",
      "batch 60/96, 30720 files processed\n",
      "batch 80/96, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11262.h5 took 80.30709934234619 s\n",
      "features size:  (48802, 1024)\n",
      "coordinates size:  (48802, 2)\n",
      "\n",
      "progress: 326/876\n",
      "11263\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [119060  77338]\n",
      "level_dim [119060  77338]\n",
      "name 11263\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11263.h5: total of 151 batches\n",
      "batch 0/151, 0 files processed\n",
      "batch 20/151, 10240 files processed\n",
      "batch 40/151, 20480 files processed\n",
      "batch 60/151, 30720 files processed\n",
      "batch 80/151, 40960 files processed\n",
      "batch 100/151, 51200 files processed\n",
      "batch 120/151, 61440 files processed\n",
      "batch 140/151, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11263.h5 took 121.83270955085754 s\n",
      "features size:  (77035, 1024)\n",
      "coordinates size:  (77035, 2)\n",
      "\n",
      "progress: 327/876\n",
      "11264\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [85454 67311]\n",
      "level_dim [85454 67311]\n",
      "name 11264\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11264.h5: total of 88 batches\n",
      "batch 0/88, 0 files processed\n",
      "batch 20/88, 10240 files processed\n",
      "batch 40/88, 20480 files processed\n",
      "batch 60/88, 30720 files processed\n",
      "batch 80/88, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11264.h5 took 74.9394121170044 s\n",
      "features size:  (44979, 1024)\n",
      "coordinates size:  (44979, 2)\n",
      "\n",
      "progress: 328/876\n",
      "11265\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [92176 58109]\n",
      "level_dim [92176 58109]\n",
      "name 11265\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11265.h5: total of 88 batches\n",
      "batch 0/88, 0 files processed\n",
      "batch 20/88, 10240 files processed\n",
      "batch 40/88, 20480 files processed\n",
      "batch 60/88, 30720 files processed\n",
      "batch 80/88, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11265.h5 took 72.65959668159485 s\n",
      "features size:  (44813, 1024)\n",
      "coordinates size:  (44813, 2)\n",
      "\n",
      "progress: 329/876\n",
      "11266\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [93136 83697]\n",
      "level_dim [93136 83697]\n",
      "name 11266\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11266.h5: total of 182 batches\n",
      "batch 0/182, 0 files processed\n",
      "batch 20/182, 10240 files processed\n",
      "batch 40/182, 20480 files processed\n",
      "batch 60/182, 30720 files processed\n",
      "batch 80/182, 40960 files processed\n",
      "batch 100/182, 51200 files processed\n",
      "batch 120/182, 61440 files processed\n",
      "batch 140/182, 71680 files processed\n",
      "batch 160/182, 81920 files processed\n",
      "batch 180/182, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11266.h5 took 141.18826413154602 s\n",
      "features size:  (93020, 1024)\n",
      "coordinates size:  (93020, 2)\n",
      "\n",
      "progress: 330/876\n",
      "11269\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [88335 51963]\n",
      "level_dim [88335 51963]\n",
      "name 11269\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11269.h5: total of 69 batches\n",
      "batch 0/69, 0 files processed\n",
      "batch 20/69, 10240 files processed\n",
      "batch 40/69, 20480 files processed\n",
      "batch 60/69, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11269.h5 took 43.11248230934143 s\n",
      "features size:  (35092, 1024)\n",
      "coordinates size:  (35092, 2)\n",
      "\n",
      "progress: 331/876\n",
      "11270\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [102737  64009]\n",
      "level_dim [102737  64009]\n",
      "name 11270\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11270.h5: total of 112 batches\n",
      "batch 0/112, 0 files processed\n",
      "batch 20/112, 10240 files processed\n",
      "batch 40/112, 20480 files processed\n",
      "batch 60/112, 30720 files processed\n",
      "batch 80/112, 40960 files processed\n",
      "batch 100/112, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11270.h5 took 86.36186218261719 s\n",
      "features size:  (56940, 1024)\n",
      "coordinates size:  (56940, 2)\n",
      "\n",
      "progress: 332/876\n",
      "11271\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [76813 60135]\n",
      "level_dim [76813 60135]\n",
      "name 11271\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11271.h5: total of 76 batches\n",
      "batch 0/76, 0 files processed\n",
      "batch 20/76, 10240 files processed\n",
      "batch 40/76, 20480 files processed\n",
      "batch 60/76, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11271.h5 took 61.642409801483154 s\n",
      "features size:  (38617, 1024)\n",
      "coordinates size:  (38617, 2)\n",
      "\n",
      "progress: 333/876\n",
      "11272\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [73932 72667]\n",
      "level_dim [73932 72667]\n",
      "name 11272\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11272.h5: total of 92 batches\n",
      "batch 0/92, 0 files processed\n",
      "batch 20/92, 10240 files processed\n",
      "batch 40/92, 20480 files processed\n",
      "batch 60/92, 30720 files processed\n",
      "batch 80/92, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11272.h5 took 73.48489212989807 s\n",
      "features size:  (46970, 1024)\n",
      "coordinates size:  (46970, 2)\n",
      "\n",
      "progress: 334/876\n",
      "11273\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [63371 61651]\n",
      "level_dim [63371 61651]\n",
      "name 11273\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11273.h5: total of 71 batches\n",
      "batch 0/71, 0 files processed\n",
      "batch 20/71, 10240 files processed\n",
      "batch 40/71, 20480 files processed\n",
      "batch 60/71, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11273.h5 took 59.87090444564819 s\n",
      "features size:  (35965, 1024)\n",
      "coordinates size:  (35965, 2)\n",
      "\n",
      "progress: 335/876\n",
      "11274\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [66251 64725]\n",
      "level_dim [66251 64725]\n",
      "name 11274\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11274.h5: total of 75 batches\n",
      "batch 0/75, 0 files processed\n",
      "batch 20/75, 10240 files processed\n",
      "batch 40/75, 20480 files processed\n",
      "batch 60/75, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11274.h5 took 66.12394261360168 s\n",
      "features size:  (38297, 1024)\n",
      "coordinates size:  (38297, 2)\n",
      "\n",
      "progress: 336/876\n",
      "11275\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [86415 69360]\n",
      "level_dim [86415 69360]\n",
      "name 11275\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11275.h5: total of 90 batches\n",
      "batch 0/90, 0 files processed\n",
      "batch 20/90, 10240 files processed\n",
      "batch 40/90, 20480 files processed\n",
      "batch 60/90, 30720 files processed\n",
      "batch 80/90, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11275.h5 took 78.54793429374695 s\n",
      "features size:  (45622, 1024)\n",
      "coordinates size:  (45622, 2)\n",
      "\n",
      "progress: 337/876\n",
      "11279\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [74893 78042]\n",
      "level_dim [74893 78042]\n",
      "name 11279\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11279.h5: total of 80 batches\n",
      "batch 0/80, 0 files processed\n",
      "batch 20/80, 10240 files processed\n",
      "batch 40/80, 20480 files processed\n",
      "batch 60/80, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11279.h5 took 65.82884621620178 s\n",
      "features size:  (40527, 1024)\n",
      "coordinates size:  (40527, 2)\n",
      "\n",
      "progress: 338/876\n",
      "11280\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [67211 82125]\n",
      "level_dim [67211 82125]\n",
      "name 11280\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11280.h5: total of 85 batches\n",
      "batch 0/85, 0 files processed\n",
      "batch 20/85, 10240 files processed\n",
      "batch 40/85, 20480 files processed\n",
      "batch 60/85, 30720 files processed\n",
      "batch 80/85, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11280.h5 took 64.78714632987976 s\n",
      "features size:  (43378, 1024)\n",
      "coordinates size:  (43378, 2)\n",
      "\n",
      "progress: 339/876\n",
      "11281\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [83534 83683]\n",
      "level_dim [83534 83683]\n",
      "name 11281\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11281.h5: total of 93 batches\n",
      "batch 0/93, 0 files processed\n",
      "batch 20/93, 10240 files processed\n",
      "batch 40/93, 20480 files processed\n",
      "batch 60/93, 30720 files processed\n",
      "batch 80/93, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11281.h5 took 66.97636890411377 s\n",
      "features size:  (47227, 1024)\n",
      "coordinates size:  (47227, 2)\n",
      "\n",
      "progress: 340/876\n",
      "11283\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [106578  80646]\n",
      "level_dim [106578  80646]\n",
      "name 11283\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11283.h5: total of 56 batches\n",
      "batch 0/56, 0 files processed\n",
      "batch 20/56, 10240 files processed\n",
      "batch 40/56, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11283.h5 took 35.71340298652649 s\n",
      "features size:  (28260, 1024)\n",
      "coordinates size:  (28260, 2)\n",
      "\n",
      "progress: 341/876\n",
      "11284\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [76813 73184]\n",
      "level_dim [76813 73184]\n",
      "name 11284\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11284.h5: total of 65 batches\n",
      "batch 0/65, 0 files processed\n",
      "batch 20/65, 10240 files processed\n",
      "batch 40/65, 20480 files processed\n",
      "batch 60/65, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11284.h5 took 52.15650463104248 s\n",
      "features size:  (32962, 1024)\n",
      "coordinates size:  (32962, 2)\n",
      "\n",
      "progress: 342/876\n",
      "11285\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [108498  83719]\n",
      "level_dim [108498  83719]\n",
      "name 11285\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11285.h5: total of 139 batches\n",
      "batch 0/139, 0 files processed\n",
      "batch 20/139, 10240 files processed\n",
      "batch 40/139, 20480 files processed\n",
      "batch 60/139, 30720 files processed\n",
      "batch 80/139, 40960 files processed\n",
      "batch 100/139, 51200 files processed\n",
      "batch 120/139, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11285.h5 took 105.07151651382446 s\n",
      "features size:  (70978, 1024)\n",
      "coordinates size:  (70978, 2)\n",
      "\n",
      "progress: 343/876\n",
      "11286\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [116180  80915]\n",
      "level_dim [116180  80915]\n",
      "name 11286\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11286.h5: total of 187 batches\n",
      "batch 0/187, 0 files processed\n",
      "batch 20/187, 10240 files processed\n",
      "batch 40/187, 20480 files processed\n",
      "batch 60/187, 30720 files processed\n",
      "batch 80/187, 40960 files processed\n",
      "batch 100/187, 51200 files processed\n",
      "batch 120/187, 61440 files processed\n",
      "batch 140/187, 71680 files processed\n",
      "batch 160/187, 81920 files processed\n",
      "batch 180/187, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11286.h5 took 152.87903881072998 s\n",
      "features size:  (95258, 1024)\n",
      "coordinates size:  (95258, 2)\n",
      "\n",
      "progress: 344/876\n",
      "11287\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [114259  79633]\n",
      "level_dim [114259  79633]\n",
      "name 11287\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11287.h5: total of 123 batches\n",
      "batch 0/123, 0 files processed\n",
      "batch 20/123, 10240 files processed\n",
      "batch 40/123, 20480 files processed\n",
      "batch 60/123, 30720 files processed\n",
      "batch 80/123, 40960 files processed\n",
      "batch 100/123, 51200 files processed\n",
      "batch 120/123, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11287.h5 took 89.58346223831177 s\n",
      "features size:  (62742, 1024)\n",
      "coordinates size:  (62742, 2)\n",
      "\n",
      "progress: 345/876\n",
      "11288\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [36486 39609]\n",
      "level_dim [36486 39609]\n",
      "name 11288\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11288.h5: total of 10 batches\n",
      "batch 0/10, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11288.h5 took 9.935954809188843 s\n",
      "features size:  (4985, 1024)\n",
      "coordinates size:  (4985, 2)\n",
      "\n",
      "progress: 346/876\n",
      "11289\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [99857 68867]\n",
      "level_dim [99857 68867]\n",
      "name 11289\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11289.h5: total of 109 batches\n",
      "batch 0/109, 0 files processed\n",
      "batch 20/109, 10240 files processed\n",
      "batch 40/109, 20480 files processed\n",
      "batch 60/109, 30720 files processed\n",
      "batch 80/109, 40960 files processed\n",
      "batch 100/109, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11289.h5 took 91.31330585479736 s\n",
      "features size:  (55732, 1024)\n",
      "coordinates size:  (55732, 2)\n",
      "\n",
      "progress: 347/876\n",
      "11290\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [43207 46271]\n",
      "level_dim [43207 46271]\n",
      "name 11290\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11290.h5: total of 23 batches\n",
      "batch 0/23, 0 files processed\n",
      "batch 20/23, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11290.h5 took 19.79848074913025 s\n",
      "features size:  (11657, 1024)\n",
      "coordinates size:  (11657, 2)\n",
      "\n",
      "progress: 348/876\n",
      "11291\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [72972 59107]\n",
      "level_dim [72972 59107]\n",
      "name 11291\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11291.h5: total of 50 batches\n",
      "batch 0/50, 0 files processed\n",
      "batch 20/50, 10240 files processed\n",
      "batch 40/50, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11291.h5 took 39.713494539260864 s\n",
      "features size:  (25494, 1024)\n",
      "coordinates size:  (25494, 2)\n",
      "\n",
      "progress: 349/876\n",
      "11295\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [97937 77051]\n",
      "level_dim [97937 77051]\n",
      "name 11295\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11295.h5: total of 119 batches\n",
      "batch 0/119, 0 files processed\n",
      "batch 20/119, 10240 files processed\n",
      "batch 40/119, 20480 files processed\n",
      "batch 60/119, 30720 files processed\n",
      "batch 80/119, 40960 files processed\n",
      "batch 100/119, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11295.h5 took 94.09084105491638 s\n",
      "features size:  (60912, 1024)\n",
      "coordinates size:  (60912, 2)\n",
      "\n",
      "progress: 350/876\n",
      "11296\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [95056 67836]\n",
      "level_dim [95056 67836]\n",
      "name 11296\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11296.h5: total of 129 batches\n",
      "batch 0/129, 0 files processed\n",
      "batch 20/129, 10240 files processed\n",
      "batch 40/129, 20480 files processed\n",
      "batch 60/129, 30720 files processed\n",
      "batch 80/129, 40960 files processed\n",
      "batch 100/129, 51200 files processed\n",
      "batch 120/129, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11296.h5 took 102.36287689208984 s\n",
      "features size:  (65592, 1024)\n",
      "coordinates size:  (65592, 2)\n",
      "\n",
      "progress: 351/876\n",
      "11297\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [61450 65999]\n",
      "level_dim [61450 65999]\n",
      "name 11297\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11297.h5: total of 58 batches\n",
      "batch 0/58, 0 files processed\n",
      "batch 20/58, 10240 files processed\n",
      "batch 40/58, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11297.h5 took 40.93014121055603 s\n",
      "features size:  (29302, 1024)\n",
      "coordinates size:  (29302, 2)\n",
      "\n",
      "progress: 352/876\n",
      "11299\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [27844 39341]\n",
      "level_dim [27844 39341]\n",
      "name 11299\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11299.h5: total of 6 batches\n",
      "batch 0/6, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11299.h5 took 6.23553729057312 s\n",
      "features size:  (2983, 1024)\n",
      "coordinates size:  (2983, 2)\n",
      "\n",
      "progress: 353/876\n",
      "11301\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [54729 66501]\n",
      "level_dim [54729 66501]\n",
      "name 11301\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11301.h5: total of 16 batches\n",
      "batch 0/16, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11301.h5 took 11.167130947113037 s\n",
      "features size:  (8032, 1024)\n",
      "coordinates size:  (8032, 2)\n",
      "\n",
      "progress: 354/876\n",
      "11303\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [126742  79651]\n",
      "level_dim [126742  79651]\n",
      "name 11303\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11303.h5: total of 131 batches\n",
      "batch 0/131, 0 files processed\n",
      "batch 20/131, 10240 files processed\n",
      "batch 40/131, 20480 files processed\n",
      "batch 60/131, 30720 files processed\n",
      "batch 80/131, 40960 files processed\n",
      "batch 100/131, 51200 files processed\n",
      "batch 120/131, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11303.h5 took 106.14044904708862 s\n",
      "features size:  (66725, 1024)\n",
      "coordinates size:  (66725, 2)\n",
      "\n",
      "progress: 355/876\n",
      "11304\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [100817  81917]\n",
      "level_dim [100817  81917]\n",
      "name 11304\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11304.h5: total of 135 batches\n",
      "batch 0/135, 0 files processed\n",
      "batch 20/135, 10240 files processed\n",
      "batch 40/135, 20480 files processed\n",
      "batch 60/135, 30720 files processed\n",
      "batch 80/135, 40960 files processed\n",
      "batch 100/135, 51200 files processed\n",
      "batch 120/135, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11304.h5 took 116.75947093963623 s\n",
      "features size:  (68685, 1024)\n",
      "coordinates size:  (68685, 2)\n",
      "\n",
      "progress: 356/876\n",
      "11305\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [102737  75012]\n",
      "level_dim [102737  75012]\n",
      "name 11305\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11305.h5: total of 110 batches\n",
      "batch 0/110, 0 files processed\n",
      "batch 20/110, 10240 files processed\n",
      "batch 40/110, 20480 files processed\n",
      "batch 60/110, 30720 files processed\n",
      "batch 80/110, 40960 files processed\n",
      "batch 100/110, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11305.h5 took 94.09740328788757 s\n",
      "features size:  (56141, 1024)\n",
      "coordinates size:  (56141, 2)\n",
      "\n",
      "progress: 357/876\n",
      "11306\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [84494 66286]\n",
      "level_dim [84494 66286]\n",
      "name 11306\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11306.h5: total of 98 batches\n",
      "batch 0/98, 0 files processed\n",
      "batch 20/98, 10240 files processed\n",
      "batch 40/98, 20480 files processed\n",
      "batch 60/98, 30720 files processed\n",
      "batch 80/98, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11306.h5 took 84.39762210845947 s\n",
      "features size:  (49946, 1024)\n",
      "coordinates size:  (49946, 2)\n",
      "\n",
      "progress: 358/876\n",
      "11307\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [111379  83723]\n",
      "level_dim [111379  83723]\n",
      "name 11307\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11307.h5: total of 128 batches\n",
      "batch 0/128, 0 files processed\n",
      "batch 20/128, 10240 files processed\n",
      "batch 40/128, 20480 files processed\n",
      "batch 60/128, 30720 files processed\n",
      "batch 80/128, 40960 files processed\n",
      "batch 100/128, 51200 files processed\n",
      "batch 120/128, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11307.h5 took 76.04294157028198 s\n",
      "features size:  (65383, 1024)\n",
      "coordinates size:  (65383, 2)\n",
      "\n",
      "progress: 359/876\n",
      "11308\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [92176 80114]\n",
      "level_dim [92176 80114]\n",
      "name 11308\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11308.h5: total of 157 batches\n",
      "batch 0/157, 0 files processed\n",
      "batch 20/157, 10240 files processed\n",
      "batch 40/157, 20480 files processed\n",
      "batch 60/157, 30720 files processed\n",
      "batch 80/157, 40960 files processed\n",
      "batch 100/157, 51200 files processed\n",
      "batch 120/157, 61440 files processed\n",
      "batch 140/157, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11308.h5 took 141.39768409729004 s\n",
      "features size:  (80166, 1024)\n",
      "coordinates size:  (80166, 2)\n",
      "\n",
      "progress: 360/876\n",
      "11309\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [101777  78080]\n",
      "level_dim [101777  78080]\n",
      "name 11309\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11309.h5: total of 121 batches\n",
      "batch 0/121, 0 files processed\n",
      "batch 20/121, 10240 files processed\n",
      "batch 40/121, 20480 files processed\n",
      "batch 60/121, 30720 files processed\n",
      "batch 80/121, 40960 files processed\n",
      "batch 100/121, 51200 files processed\n",
      "batch 120/121, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11309.h5 took 98.25070428848267 s\n",
      "features size:  (61907, 1024)\n",
      "coordinates size:  (61907, 2)\n",
      "\n",
      "progress: 361/876\n",
      "11315\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [116180  53793]\n",
      "level_dim [116180  53793]\n",
      "name 11315\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11315.h5: total of 110 batches\n",
      "batch 0/110, 0 files processed\n",
      "batch 20/110, 10240 files processed\n",
      "batch 40/110, 20480 files processed\n",
      "batch 60/110, 30720 files processed\n",
      "batch 80/110, 40960 files processed\n",
      "batch 100/110, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11315.h5 took 91.78747510910034 s\n",
      "features size:  (55868, 1024)\n",
      "coordinates size:  (55868, 2)\n",
      "\n",
      "progress: 362/876\n",
      "11316\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [104658  83713]\n",
      "level_dim [104658  83713]\n",
      "name 11316\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11316.h5: total of 199 batches\n",
      "batch 0/199, 0 files processed\n",
      "batch 20/199, 10240 files processed\n",
      "batch 40/199, 20480 files processed\n",
      "batch 60/199, 30720 files processed\n",
      "batch 80/199, 40960 files processed\n",
      "batch 100/199, 51200 files processed\n",
      "batch 120/199, 61440 files processed\n",
      "batch 140/199, 71680 files processed\n",
      "batch 160/199, 81920 files processed\n",
      "batch 180/199, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11316.h5 took 164.62416982650757 s\n",
      "features size:  (101707, 1024)\n",
      "coordinates size:  (101707, 2)\n",
      "\n",
      "progress: 363/876\n",
      "11317\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [102737  74243]\n",
      "level_dim [102737  74243]\n",
      "name 11317\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11317.h5: total of 127 batches\n",
      "batch 0/127, 0 files processed\n",
      "batch 20/127, 10240 files processed\n",
      "batch 40/127, 20480 files processed\n",
      "batch 60/127, 30720 files processed\n",
      "batch 80/127, 40960 files processed\n",
      "batch 100/127, 51200 files processed\n",
      "batch 120/127, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11317.h5 took 83.68833875656128 s\n",
      "features size:  (64516, 1024)\n",
      "coordinates size:  (64516, 2)\n",
      "\n",
      "progress: 364/876\n",
      "11318\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [123861  83741]\n",
      "level_dim [123861  83741]\n",
      "name 11318\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11318.h5: total of 196 batches\n",
      "batch 0/196, 0 files processed\n",
      "batch 20/196, 10240 files processed\n",
      "batch 40/196, 20480 files processed\n",
      "batch 60/196, 30720 files processed\n",
      "batch 80/196, 40960 files processed\n",
      "batch 100/196, 51200 files processed\n",
      "batch 120/196, 61440 files processed\n",
      "batch 140/196, 71680 files processed\n",
      "batch 160/196, 81920 files processed\n",
      "batch 180/196, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11318.h5 took 157.2333812713623 s\n",
      "features size:  (100126, 1024)\n",
      "coordinates size:  (100126, 2)\n",
      "\n",
      "progress: 365/876\n",
      "11319\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [101777  81662]\n",
      "level_dim [101777  81662]\n",
      "name 11319\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11319.h5: total of 137 batches\n",
      "batch 0/137, 0 files processed\n",
      "batch 20/137, 10240 files processed\n",
      "batch 40/137, 20480 files processed\n",
      "batch 60/137, 30720 files processed\n",
      "batch 80/137, 40960 files processed\n",
      "batch 100/137, 51200 files processed\n",
      "batch 120/137, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11319.h5 took 78.37519836425781 s\n",
      "features size:  (70031, 1024)\n",
      "coordinates size:  (70031, 2)\n",
      "\n",
      "progress: 366/876\n",
      "11320\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [97937 75517]\n",
      "level_dim [97937 75517]\n",
      "name 11320\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11320.h5: total of 147 batches\n",
      "batch 0/147, 0 files processed\n",
      "batch 20/147, 10240 files processed\n",
      "batch 40/147, 20480 files processed\n",
      "batch 60/147, 30720 files processed\n",
      "batch 80/147, 40960 files processed\n",
      "batch 100/147, 51200 files processed\n",
      "batch 120/147, 61440 files processed\n",
      "batch 140/147, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11320.h5 took 98.52168703079224 s\n",
      "features size:  (74778, 1024)\n",
      "coordinates size:  (74778, 2)\n",
      "\n",
      "progress: 367/876\n",
      "11321\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [102737  77058]\n",
      "level_dim [102737  77058]\n",
      "name 11321\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11321.h5: total of 124 batches\n",
      "batch 0/124, 0 files processed\n",
      "batch 20/124, 10240 files processed\n",
      "batch 40/124, 20480 files processed\n",
      "batch 60/124, 30720 files processed\n",
      "batch 80/124, 40960 files processed\n",
      "batch 100/124, 51200 files processed\n",
      "batch 120/124, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11321.h5 took 89.82133507728577 s\n",
      "features size:  (63261, 1024)\n",
      "coordinates size:  (63261, 2)\n",
      "\n",
      "progress: 368/876\n",
      "11322\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [106578  67852]\n",
      "level_dim [106578  67852]\n",
      "name 11322\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11322.h5: total of 130 batches\n",
      "batch 0/130, 0 files processed\n",
      "batch 20/130, 10240 files processed\n",
      "batch 40/130, 20480 files processed\n",
      "batch 60/130, 30720 files processed\n",
      "batch 80/130, 40960 files processed\n",
      "batch 100/130, 51200 files processed\n",
      "batch 120/130, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11322.h5 took 110.78912234306335 s\n",
      "features size:  (66430, 1024)\n",
      "coordinates size:  (66430, 2)\n",
      "\n",
      "progress: 369/876\n",
      "11331\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [52809 50891]\n",
      "level_dim [52809 50891]\n",
      "name 11331\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11331.h5: total of 50 batches\n",
      "batch 0/50, 0 files processed\n",
      "batch 20/50, 10240 files processed\n",
      "batch 40/50, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11331.h5 took 42.37308645248413 s\n",
      "features size:  (25379, 1024)\n",
      "coordinates size:  (25379, 2)\n",
      "\n",
      "progress: 370/876\n",
      "11332\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [97937 76796]\n",
      "level_dim [97937 76796]\n",
      "name 11332\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11332.h5: total of 114 batches\n",
      "batch 0/114, 0 files processed\n",
      "batch 20/114, 10240 files processed\n",
      "batch 40/114, 20480 files processed\n",
      "batch 60/114, 30720 files processed\n",
      "batch 80/114, 40960 files processed\n",
      "batch 100/114, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11332.h5 took 89.66047239303589 s\n",
      "features size:  (57938, 1024)\n",
      "coordinates size:  (57938, 2)\n",
      "\n",
      "progress: 371/876\n",
      "11333\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [106578  83716]\n",
      "level_dim [106578  83716]\n",
      "name 11333\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11333.h5: total of 173 batches\n",
      "batch 0/173, 0 files processed\n",
      "batch 20/173, 10240 files processed\n",
      "batch 40/173, 20480 files processed\n",
      "batch 60/173, 30720 files processed\n",
      "batch 80/173, 40960 files processed\n",
      "batch 100/173, 51200 files processed\n",
      "batch 120/173, 61440 files processed\n",
      "batch 140/173, 71680 files processed\n",
      "batch 160/173, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11333.h5 took 131.31878447532654 s\n",
      "features size:  (88112, 1024)\n",
      "coordinates size:  (88112, 2)\n",
      "\n",
      "progress: 372/876\n",
      "11334\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [86415 83688]\n",
      "level_dim [86415 83688]\n",
      "name 11334\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11334.h5: total of 117 batches\n",
      "batch 0/117, 0 files processed\n",
      "batch 20/117, 10240 files processed\n",
      "batch 40/117, 20480 files processed\n",
      "batch 60/117, 30720 files processed\n",
      "batch 80/117, 40960 files processed\n",
      "batch 100/117, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11334.h5 took 90.42571353912354 s\n",
      "features size:  (59740, 1024)\n",
      "coordinates size:  (59740, 2)\n",
      "\n",
      "progress: 373/876\n",
      "11335\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96016 82934]\n",
      "level_dim [96016 82934]\n",
      "name 11335\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11335.h5: total of 138 batches\n",
      "batch 0/138, 0 files processed\n",
      "batch 20/138, 10240 files processed\n",
      "batch 40/138, 20480 files processed\n",
      "batch 60/138, 30720 files processed\n",
      "batch 80/138, 40960 files processed\n",
      "batch 100/138, 51200 files processed\n",
      "batch 120/138, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11335.h5 took 106.07713270187378 s\n",
      "features size:  (70269, 1024)\n",
      "coordinates size:  (70269, 2)\n",
      "\n",
      "progress: 374/876\n",
      "11336\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [82574 60399]\n",
      "level_dim [82574 60399]\n",
      "name 11336\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11336.h5: total of 64 batches\n",
      "batch 0/64, 0 files processed\n",
      "batch 20/64, 10240 files processed\n",
      "batch 40/64, 20480 files processed\n",
      "batch 60/64, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11336.h5 took 51.62500810623169 s\n",
      "features size:  (32471, 1024)\n",
      "coordinates size:  (32471, 2)\n",
      "\n",
      "progress: 375/876\n",
      "11337\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [70092 75989]\n",
      "level_dim [70092 75989]\n",
      "name 11337\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11337.h5: total of 46 batches\n",
      "batch 0/46, 0 files processed\n",
      "batch 20/46, 10240 files processed\n",
      "batch 40/46, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11337.h5 took 32.603394508361816 s\n",
      "features size:  (23373, 1024)\n",
      "coordinates size:  (23373, 2)\n",
      "\n",
      "progress: 376/876\n",
      "11338\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [71052 57824]\n",
      "level_dim [71052 57824]\n",
      "name 11338\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11338.h5: total of 55 batches\n",
      "batch 0/55, 0 files processed\n",
      "batch 20/55, 10240 files processed\n",
      "batch 40/55, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11338.h5 took 41.205103635787964 s\n",
      "features size:  (27842, 1024)\n",
      "coordinates size:  (27842, 2)\n",
      "\n",
      "progress: 377/876\n",
      "11339\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [134423  83755]\n",
      "level_dim [134423  83755]\n",
      "name 11339\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11339.h5: total of 173 batches\n",
      "batch 0/173, 0 files processed\n",
      "batch 20/173, 10240 files processed\n",
      "batch 40/173, 20480 files processed\n",
      "batch 60/173, 30720 files processed\n",
      "batch 80/173, 40960 files processed\n",
      "batch 100/173, 51200 files processed\n",
      "batch 120/173, 61440 files processed\n",
      "batch 140/173, 71680 files processed\n",
      "batch 160/173, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11339.h5 took 156.71995186805725 s\n",
      "features size:  (88240, 1024)\n",
      "coordinates size:  (88240, 2)\n",
      "\n",
      "progress: 378/876\n",
      "11340\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [117140  83730]\n",
      "level_dim [117140  83730]\n",
      "name 11340\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11340.h5: total of 171 batches\n",
      "batch 0/171, 0 files processed\n",
      "batch 20/171, 10240 files processed\n",
      "batch 40/171, 20480 files processed\n",
      "batch 60/171, 30720 files processed\n",
      "batch 80/171, 40960 files processed\n",
      "batch 100/171, 51200 files processed\n",
      "batch 120/171, 61440 files processed\n",
      "batch 140/171, 71680 files processed\n",
      "batch 160/171, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11340.h5 took 146.4633469581604 s\n",
      "features size:  (87087, 1024)\n",
      "coordinates size:  (87087, 2)\n",
      "\n",
      "progress: 379/876\n",
      "11341\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [122901  83738]\n",
      "level_dim [122901  83738]\n",
      "name 11341\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11341.h5: total of 211 batches\n",
      "batch 0/211, 0 files processed\n",
      "batch 20/211, 10240 files processed\n",
      "batch 40/211, 20480 files processed\n",
      "batch 60/211, 30720 files processed\n",
      "batch 80/211, 40960 files processed\n",
      "batch 100/211, 51200 files processed\n",
      "batch 120/211, 61440 files processed\n",
      "batch 140/211, 71680 files processed\n",
      "batch 160/211, 81920 files processed\n",
      "batch 180/211, 92160 files processed\n",
      "batch 200/211, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11341.h5 took 154.30386781692505 s\n",
      "features size:  (107663, 1024)\n",
      "coordinates size:  (107663, 2)\n",
      "\n",
      "progress: 380/876\n",
      "11342\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [46088 70070]\n",
      "level_dim [46088 70070]\n",
      "name 11342\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11342.h5: total of 49 batches\n",
      "batch 0/49, 0 files processed\n",
      "batch 20/49, 10240 files processed\n",
      "batch 40/49, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11342.h5 took 30.926087379455566 s\n",
      "features size:  (25062, 1024)\n",
      "coordinates size:  (25062, 2)\n",
      "\n",
      "progress: 381/876\n",
      "11343\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [106578  78343]\n",
      "level_dim [106578  78343]\n",
      "name 11343\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11343.h5: total of 149 batches\n",
      "batch 0/149, 0 files processed\n",
      "batch 20/149, 10240 files processed\n",
      "batch 40/149, 20480 files processed\n",
      "batch 60/149, 30720 files processed\n",
      "batch 80/149, 40960 files processed\n",
      "batch 100/149, 51200 files processed\n",
      "batch 120/149, 61440 files processed\n",
      "batch 140/149, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11343.h5 took 114.81140470504761 s\n",
      "features size:  (75888, 1024)\n",
      "coordinates size:  (75888, 2)\n",
      "\n",
      "progress: 382/876\n",
      "11344\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [120020  83734]\n",
      "level_dim [120020  83734]\n",
      "name 11344\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11344.h5: total of 167 batches\n",
      "batch 0/167, 0 files processed\n",
      "batch 20/167, 10240 files processed\n",
      "batch 40/167, 20480 files processed\n",
      "batch 60/167, 30720 files processed\n",
      "batch 80/167, 40960 files processed\n",
      "batch 100/167, 51200 files processed\n",
      "batch 120/167, 61440 files processed\n",
      "batch 140/167, 71680 files processed\n",
      "batch 160/167, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11344.h5 took 127.0595805644989 s\n",
      "features size:  (85224, 1024)\n",
      "coordinates size:  (85224, 2)\n",
      "\n",
      "progress: 383/876\n",
      "11345\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [109459  83720]\n",
      "level_dim [109459  83720]\n",
      "name 11345\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11345.h5: total of 175 batches\n",
      "batch 0/175, 0 files processed\n",
      "batch 20/175, 10240 files processed\n",
      "batch 40/175, 20480 files processed\n",
      "batch 60/175, 30720 files processed\n",
      "batch 80/175, 40960 files processed\n",
      "batch 100/175, 51200 files processed\n",
      "batch 120/175, 61440 files processed\n",
      "batch 140/175, 71680 files processed\n",
      "batch 160/175, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11345.h5 took 126.15852642059326 s\n",
      "features size:  (89233, 1024)\n",
      "coordinates size:  (89233, 2)\n",
      "\n",
      "progress: 384/876\n",
      "11346\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [132503  83752]\n",
      "level_dim [132503  83752]\n",
      "name 11346\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11346.h5: total of 186 batches\n",
      "batch 0/186, 0 files processed\n",
      "batch 20/186, 10240 files processed\n",
      "batch 40/186, 20480 files processed\n",
      "batch 60/186, 30720 files processed\n",
      "batch 80/186, 40960 files processed\n",
      "batch 100/186, 51200 files processed\n",
      "batch 120/186, 61440 files processed\n",
      "batch 140/186, 71680 files processed\n",
      "batch 160/186, 81920 files processed\n",
      "batch 180/186, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11346.h5 took 141.8108241558075 s\n",
      "features size:  (95151, 1024)\n",
      "coordinates size:  (95151, 2)\n",
      "\n",
      "progress: 385/876\n",
      "11347\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [134423  83755]\n",
      "level_dim [134423  83755]\n",
      "name 11347\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11347.h5: total of 196 batches\n",
      "batch 0/196, 0 files processed\n",
      "batch 20/196, 10240 files processed\n",
      "batch 40/196, 20480 files processed\n",
      "batch 60/196, 30720 files processed\n",
      "batch 80/196, 40960 files processed\n",
      "batch 100/196, 51200 files processed\n",
      "batch 120/196, 61440 files processed\n",
      "batch 140/196, 71680 files processed\n",
      "batch 160/196, 81920 files processed\n",
      "batch 180/196, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11347.h5 took 126.97173190116882 s\n",
      "features size:  (100118, 1024)\n",
      "coordinates size:  (100118, 2)\n",
      "\n",
      "progress: 386/876\n",
      "11348\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [78733 58091]\n",
      "level_dim [78733 58091]\n",
      "name 11348\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11348.h5: total of 44 batches\n",
      "batch 0/44, 0 files processed\n",
      "batch 20/44, 10240 files processed\n",
      "batch 40/44, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11348.h5 took 34.13743758201599 s\n",
      "features size:  (22378, 1024)\n",
      "coordinates size:  (22378, 2)\n",
      "\n",
      "progress: 387/876\n",
      "11349\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [116180  83730]\n",
      "level_dim [116180  83730]\n",
      "name 11349\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11349.h5: total of 156 batches\n",
      "batch 0/156, 0 files processed\n",
      "batch 20/156, 10240 files processed\n",
      "batch 40/156, 20480 files processed\n",
      "batch 60/156, 30720 files processed\n",
      "batch 80/156, 40960 files processed\n",
      "batch 100/156, 51200 files processed\n",
      "batch 120/156, 61440 files processed\n",
      "batch 140/156, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11349.h5 took 139.63892936706543 s\n",
      "features size:  (79740, 1024)\n",
      "coordinates size:  (79740, 2)\n",
      "\n",
      "progress: 388/876\n",
      "11350\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [105618  83459]\n",
      "level_dim [105618  83459]\n",
      "name 11350\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11350.h5: total of 138 batches\n",
      "batch 0/138, 0 files processed\n",
      "batch 20/138, 10240 files processed\n",
      "batch 40/138, 20480 files processed\n",
      "batch 60/138, 30720 files processed\n",
      "batch 80/138, 40960 files processed\n",
      "batch 100/138, 51200 files processed\n",
      "batch 120/138, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11350.h5 took 97.57823777198792 s\n",
      "features size:  (70348, 1024)\n",
      "coordinates size:  (70348, 2)\n",
      "\n",
      "progress: 389/876\n",
      "11351\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [110419  83721]\n",
      "level_dim [110419  83721]\n",
      "name 11351\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11351.h5: total of 181 batches\n",
      "batch 0/181, 0 files processed\n",
      "batch 20/181, 10240 files processed\n",
      "batch 40/181, 20480 files processed\n",
      "batch 60/181, 30720 files processed\n",
      "batch 80/181, 40960 files processed\n",
      "batch 100/181, 51200 files processed\n",
      "batch 120/181, 61440 files processed\n",
      "batch 140/181, 71680 files processed\n",
      "batch 160/181, 81920 files processed\n",
      "batch 180/181, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11351.h5 took 147.08585333824158 s\n",
      "features size:  (92403, 1024)\n",
      "coordinates size:  (92403, 2)\n",
      "\n",
      "progress: 390/876\n",
      "11352\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [103698  81920]\n",
      "level_dim [103698  81920]\n",
      "name 11352\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11352.h5: total of 123 batches\n",
      "batch 0/123, 0 files processed\n",
      "batch 20/123, 10240 files processed\n",
      "batch 40/123, 20480 files processed\n",
      "batch 60/123, 30720 files processed\n",
      "batch 80/123, 40960 files processed\n",
      "batch 100/123, 51200 files processed\n",
      "batch 120/123, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11352.h5 took 103.18100571632385 s\n",
      "features size:  (62685, 1024)\n",
      "coordinates size:  (62685, 2)\n",
      "\n",
      "progress: 391/876\n",
      "11353\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [65291 73423]\n",
      "level_dim [65291 73423]\n",
      "name 11353\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11353.h5: total of 61 batches\n",
      "batch 0/61, 0 files processed\n",
      "batch 20/61, 10240 files processed\n",
      "batch 40/61, 20480 files processed\n",
      "batch 60/61, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11353.h5 took 48.14454007148743 s\n",
      "features size:  (31054, 1024)\n",
      "coordinates size:  (31054, 2)\n",
      "\n",
      "progress: 392/876\n",
      "11354\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [82574 79332]\n",
      "level_dim [82574 79332]\n",
      "name 11354\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11354.h5: total of 105 batches\n",
      "batch 0/105, 0 files processed\n",
      "batch 20/105, 10240 files processed\n",
      "batch 40/105, 20480 files processed\n",
      "batch 60/105, 30720 files processed\n",
      "batch 80/105, 40960 files processed\n",
      "batch 100/105, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11354.h5 took 89.89866805076599 s\n",
      "features size:  (53749, 1024)\n",
      "coordinates size:  (53749, 2)\n",
      "\n",
      "progress: 393/876\n",
      "11355\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [70092 62940]\n",
      "level_dim [70092 62940]\n",
      "name 11355\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11355.h5: total of 67 batches\n",
      "batch 0/67, 0 files processed\n",
      "batch 20/67, 10240 files processed\n",
      "batch 40/67, 20480 files processed\n",
      "batch 60/67, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11355.h5 took 58.35205698013306 s\n",
      "features size:  (34096, 1024)\n",
      "coordinates size:  (34096, 2)\n",
      "\n",
      "progress: 394/876\n",
      "11356\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [86415 79338]\n",
      "level_dim [86415 79338]\n",
      "name 11356\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11356.h5: total of 126 batches\n",
      "batch 0/126, 0 files processed\n",
      "batch 20/126, 10240 files processed\n",
      "batch 40/126, 20480 files processed\n",
      "batch 60/126, 30720 files processed\n",
      "batch 80/126, 40960 files processed\n",
      "batch 100/126, 51200 files processed\n",
      "batch 120/126, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11356.h5 took 101.14283275604248 s\n",
      "features size:  (64199, 1024)\n",
      "coordinates size:  (64199, 2)\n",
      "\n",
      "progress: 395/876\n",
      "11357\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [115220  83729]\n",
      "level_dim [115220  83729]\n",
      "name 11357\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11357.h5: total of 213 batches\n",
      "batch 0/213, 0 files processed\n",
      "batch 20/213, 10240 files processed\n",
      "batch 40/213, 20480 files processed\n",
      "batch 60/213, 30720 files processed\n",
      "batch 80/213, 40960 files processed\n",
      "batch 100/213, 51200 files processed\n",
      "batch 120/213, 61440 files processed\n",
      "batch 140/213, 71680 files processed\n",
      "batch 160/213, 81920 files processed\n",
      "batch 180/213, 92160 files processed\n",
      "batch 200/213, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11357.h5 took 179.31972193717957 s\n",
      "features size:  (109049, 1024)\n",
      "coordinates size:  (109049, 2)\n",
      "\n",
      "progress: 396/876\n",
      "11358\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [95056 76280]\n",
      "level_dim [95056 76280]\n",
      "name 11358\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11358.h5: total of 115 batches\n",
      "batch 0/115, 0 files processed\n",
      "batch 20/115, 10240 files processed\n",
      "batch 40/115, 20480 files processed\n",
      "batch 60/115, 30720 files processed\n",
      "batch 80/115, 40960 files processed\n",
      "batch 100/115, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11358.h5 took 95.41328310966492 s\n",
      "features size:  (58471, 1024)\n",
      "coordinates size:  (58471, 2)\n",
      "\n",
      "progress: 397/876\n",
      "11359\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [106578  77575]\n",
      "level_dim [106578  77575]\n",
      "name 11359\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11359.h5: total of 168 batches\n",
      "batch 0/168, 0 files processed\n",
      "batch 20/168, 10240 files processed\n",
      "batch 40/168, 20480 files processed\n",
      "batch 60/168, 30720 files processed\n",
      "batch 80/168, 40960 files processed\n",
      "batch 100/168, 51200 files processed\n",
      "batch 120/168, 61440 files processed\n",
      "batch 140/168, 71680 files processed\n",
      "batch 160/168, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11359.h5 took 136.04547476768494 s\n",
      "features size:  (85551, 1024)\n",
      "coordinates size:  (85551, 2)\n",
      "\n",
      "progress: 398/876\n",
      "11360\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [101777  79872]\n",
      "level_dim [101777  79872]\n",
      "name 11360\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11360.h5: total of 144 batches\n",
      "batch 0/144, 0 files processed\n",
      "batch 20/144, 10240 files processed\n",
      "batch 40/144, 20480 files processed\n",
      "batch 60/144, 30720 files processed\n",
      "batch 80/144, 40960 files processed\n",
      "batch 100/144, 51200 files processed\n",
      "batch 120/144, 61440 files processed\n",
      "batch 140/144, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11360.h5 took 77.06186747550964 s\n",
      "features size:  (73386, 1024)\n",
      "coordinates size:  (73386, 2)\n",
      "\n",
      "progress: 399/876\n",
      "11361\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [114259  83726]\n",
      "level_dim [114259  83726]\n",
      "name 11361\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11361.h5: total of 183 batches\n",
      "batch 0/183, 0 files processed\n",
      "batch 20/183, 10240 files processed\n",
      "batch 40/183, 20480 files processed\n",
      "batch 60/183, 30720 files processed\n",
      "batch 80/183, 40960 files processed\n",
      "batch 100/183, 51200 files processed\n",
      "batch 120/183, 61440 files processed\n",
      "batch 140/183, 71680 files processed\n",
      "batch 160/183, 81920 files processed\n",
      "batch 180/183, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11361.h5 took 128.4951913356781 s\n",
      "features size:  (93305, 1024)\n",
      "coordinates size:  (93305, 2)\n",
      "\n",
      "progress: 400/876\n",
      "11362\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [106578  83717]\n",
      "level_dim [106578  83717]\n",
      "name 11362\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11362.h5: total of 166 batches\n",
      "batch 0/166, 0 files processed\n",
      "batch 20/166, 10240 files processed\n",
      "batch 40/166, 20480 files processed\n",
      "batch 60/166, 30720 files processed\n",
      "batch 80/166, 40960 files processed\n",
      "batch 100/166, 51200 files processed\n",
      "batch 120/166, 61440 files processed\n",
      "batch 140/166, 71680 files processed\n",
      "batch 160/166, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11362.h5 took 129.29882955551147 s\n",
      "features size:  (84606, 1024)\n",
      "coordinates size:  (84606, 2)\n",
      "\n",
      "progress: 401/876\n",
      "11363\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [102737  75522]\n",
      "level_dim [102737  75522]\n",
      "name 11363\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11363.h5: total of 92 batches\n",
      "batch 0/92, 0 files processed\n",
      "batch 20/92, 10240 files processed\n",
      "batch 40/92, 20480 files processed\n",
      "batch 60/92, 30720 files processed\n",
      "batch 80/92, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11363.h5 took 80.45914840698242 s\n",
      "features size:  (46998, 1024)\n",
      "coordinates size:  (46998, 2)\n",
      "\n",
      "progress: 402/876\n",
      "11364\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [95056 67069]\n",
      "level_dim [95056 67069]\n",
      "name 11364\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11364.h5: total of 119 batches\n",
      "batch 0/119, 0 files processed\n",
      "batch 20/119, 10240 files processed\n",
      "batch 40/119, 20480 files processed\n",
      "batch 60/119, 30720 files processed\n",
      "batch 80/119, 40960 files processed\n",
      "batch 100/119, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11364.h5 took 90.09758472442627 s\n",
      "features size:  (60609, 1024)\n",
      "coordinates size:  (60609, 2)\n",
      "\n",
      "progress: 403/876\n",
      "11365\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [91215 72948]\n",
      "level_dim [91215 72948]\n",
      "name 11365\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11365.h5: total of 145 batches\n",
      "batch 0/145, 0 files processed\n",
      "batch 20/145, 10240 files processed\n",
      "batch 40/145, 20480 files processed\n",
      "batch 60/145, 30720 files processed\n",
      "batch 80/145, 40960 files processed\n",
      "batch 100/145, 51200 files processed\n",
      "batch 120/145, 61440 files processed\n",
      "batch 140/145, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11365.h5 took 78.02744269371033 s\n",
      "features size:  (74065, 1024)\n",
      "coordinates size:  (74065, 2)\n",
      "\n",
      "progress: 404/876\n",
      "11366\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [91215 63737]\n",
      "level_dim [91215 63737]\n",
      "name 11366\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11366.h5: total of 102 batches\n",
      "batch 0/102, 0 files processed\n",
      "batch 20/102, 10240 files processed\n",
      "batch 40/102, 20480 files processed\n",
      "batch 60/102, 30720 files processed\n",
      "batch 80/102, 40960 files processed\n",
      "batch 100/102, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11366.h5 took 51.6355984210968 s\n",
      "features size:  (51765, 1024)\n",
      "coordinates size:  (51765, 2)\n",
      "\n",
      "progress: 405/876\n",
      "11367\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [61450 35550]\n",
      "level_dim [61450 35550]\n",
      "name 11367\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11367.h5: total of 13 batches\n",
      "batch 0/13, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11367.h5 took 9.289748907089233 s\n",
      "features size:  (6491, 1024)\n",
      "coordinates size:  (6491, 2)\n",
      "\n",
      "progress: 406/876\n",
      "11368\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [103698  77571]\n",
      "level_dim [103698  77571]\n",
      "name 11368\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11368.h5: total of 126 batches\n",
      "batch 0/126, 0 files processed\n",
      "batch 20/126, 10240 files processed\n",
      "batch 40/126, 20480 files processed\n",
      "batch 60/126, 30720 files processed\n",
      "batch 80/126, 40960 files processed\n",
      "batch 100/126, 51200 files processed\n",
      "batch 120/126, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11368.h5 took 100.39625310897827 s\n",
      "features size:  (64028, 1024)\n",
      "coordinates size:  (64028, 2)\n",
      "\n",
      "progress: 407/876\n",
      "11369\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [103698  68104]\n",
      "level_dim [103698  68104]\n",
      "name 11369\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11369.h5: total of 111 batches\n",
      "batch 0/111, 0 files processed\n",
      "batch 20/111, 10240 files processed\n",
      "batch 40/111, 20480 files processed\n",
      "batch 60/111, 30720 files processed\n",
      "batch 80/111, 40960 files processed\n",
      "batch 100/111, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11369.h5 took 83.85881161689758 s\n",
      "features size:  (56402, 1024)\n",
      "coordinates size:  (56402, 2)\n",
      "\n",
      "progress: 408/876\n",
      "11379\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [26884 17848]\n",
      "level_dim [26884 17848]\n",
      "name 11379\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11379.h5: total of 6 batches\n",
      "batch 0/6, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11379.h5 took 6.085726022720337 s\n",
      "features size:  (2820, 1024)\n",
      "coordinates size:  (2820, 2)\n",
      "\n",
      "progress: 409/876\n",
      "11380\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [80654 68840]\n",
      "level_dim [80654 68840]\n",
      "name 11380\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11380.h5: total of 15 batches\n",
      "batch 0/15, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11380.h5 took 12.118289709091187 s\n",
      "features size:  (7579, 1024)\n",
      "coordinates size:  (7579, 2)\n",
      "\n",
      "progress: 410/876\n",
      "11381\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [101777  77057]\n",
      "level_dim [101777  77057]\n",
      "name 11381\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11381.h5: total of 82 batches\n",
      "batch 0/82, 0 files processed\n",
      "batch 20/82, 10240 files processed\n",
      "batch 40/82, 20480 files processed\n",
      "batch 60/82, 30720 files processed\n",
      "batch 80/82, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11381.h5 took 56.50504231452942 s\n",
      "features size:  (41656, 1024)\n",
      "coordinates size:  (41656, 2)\n",
      "\n",
      "progress: 411/876\n",
      "11382\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [131542  81449]\n",
      "level_dim [131542  81449]\n",
      "name 11382\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11382.h5: total of 168 batches\n",
      "batch 0/168, 0 files processed\n",
      "batch 20/168, 10240 files processed\n",
      "batch 40/168, 20480 files processed\n",
      "batch 60/168, 30720 files processed\n",
      "batch 80/168, 40960 files processed\n",
      "batch 100/168, 51200 files processed\n",
      "batch 120/168, 61440 files processed\n",
      "batch 140/168, 71680 files processed\n",
      "batch 160/168, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11382.h5 took 138.9249758720398 s\n",
      "features size:  (85556, 1024)\n",
      "coordinates size:  (85556, 2)\n",
      "\n",
      "progress: 412/876\n",
      "11383\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [111379  78094]\n",
      "level_dim [111379  78094]\n",
      "name 11383\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11383.h5: total of 133 batches\n",
      "batch 0/133, 0 files processed\n",
      "batch 20/133, 10240 files processed\n",
      "batch 40/133, 20480 files processed\n",
      "batch 60/133, 30720 files processed\n",
      "batch 80/133, 40960 files processed\n",
      "batch 100/133, 51200 files processed\n",
      "batch 120/133, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11383.h5 took 107.06298899650574 s\n",
      "features size:  (67876, 1024)\n",
      "coordinates size:  (67876, 2)\n",
      "\n",
      "progress: 413/876\n",
      "11384\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [84494 73706]\n",
      "level_dim [84494 73706]\n",
      "name 11384\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11384.h5: total of 98 batches\n",
      "batch 0/98, 0 files processed\n",
      "batch 20/98, 10240 files processed\n",
      "batch 40/98, 20480 files processed\n",
      "batch 60/98, 30720 files processed\n",
      "batch 80/98, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11384.h5 took 80.25852131843567 s\n",
      "features size:  (49885, 1024)\n",
      "coordinates size:  (49885, 2)\n",
      "\n",
      "progress: 414/876\n",
      "11385\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [68171 64984]\n",
      "level_dim [68171 64984]\n",
      "name 11385\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11385.h5: total of 82 batches\n",
      "batch 0/82, 0 files processed\n",
      "batch 20/82, 10240 files processed\n",
      "batch 40/82, 20480 files processed\n",
      "batch 60/82, 30720 files processed\n",
      "batch 80/82, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11385.h5 took 72.56549549102783 s\n",
      "features size:  (41847, 1024)\n",
      "coordinates size:  (41847, 2)\n",
      "\n",
      "progress: 415/876\n",
      "11386\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [122901  83738]\n",
      "level_dim [122901  83738]\n",
      "name 11386\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11386.h5: total of 125 batches\n",
      "batch 0/125, 0 files processed\n",
      "batch 20/125, 10240 files processed\n",
      "batch 40/125, 20480 files processed\n",
      "batch 60/125, 30720 files processed\n",
      "batch 80/125, 40960 files processed\n",
      "batch 100/125, 51200 files processed\n",
      "batch 120/125, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11386.h5 took 92.23379397392273 s\n",
      "features size:  (63953, 1024)\n",
      "coordinates size:  (63953, 2)\n",
      "\n",
      "progress: 416/876\n",
      "11387\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [102737  68615]\n",
      "level_dim [102737  68615]\n",
      "name 11387\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11387.h5: total of 90 batches\n",
      "batch 0/90, 0 files processed\n",
      "batch 20/90, 10240 files processed\n",
      "batch 40/90, 20480 files processed\n",
      "batch 60/90, 30720 files processed\n",
      "batch 80/90, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11387.h5 took 65.66886377334595 s\n",
      "features size:  (45776, 1024)\n",
      "coordinates size:  (45776, 2)\n",
      "\n",
      "progress: 417/876\n",
      "11392\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [88335 70130]\n",
      "level_dim [88335 70130]\n",
      "name 11392\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11392.h5: total of 86 batches\n",
      "batch 0/86, 0 files processed\n",
      "batch 20/86, 10240 files processed\n",
      "batch 40/86, 20480 files processed\n",
      "batch 60/86, 30720 files processed\n",
      "batch 80/86, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11392.h5 took 72.5515353679657 s\n",
      "features size:  (43881, 1024)\n",
      "coordinates size:  (43881, 2)\n",
      "\n",
      "progress: 418/876\n",
      "11393\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [80654 81633]\n",
      "level_dim [80654 81633]\n",
      "name 11393\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11393.h5: total of 93 batches\n",
      "batch 0/93, 0 files processed\n",
      "batch 20/93, 10240 files processed\n",
      "batch 40/93, 20480 files processed\n",
      "batch 60/93, 30720 files processed\n",
      "batch 80/93, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11393.h5 took 79.6145544052124 s\n",
      "features size:  (47310, 1024)\n",
      "coordinates size:  (47310, 2)\n",
      "\n",
      "progress: 419/876\n",
      "11394\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [107538  83718]\n",
      "level_dim [107538  83718]\n",
      "name 11394\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11394.h5: total of 145 batches\n",
      "batch 0/145, 0 files processed\n",
      "batch 20/145, 10240 files processed\n",
      "batch 40/145, 20480 files processed\n",
      "batch 60/145, 30720 files processed\n",
      "batch 80/145, 40960 files processed\n",
      "batch 100/145, 51200 files processed\n",
      "batch 120/145, 61440 files processed\n",
      "batch 140/145, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11394.h5 took 119.61917877197266 s\n",
      "features size:  (74207, 1024)\n",
      "coordinates size:  (74207, 2)\n",
      "\n",
      "progress: 420/876\n",
      "11395\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [132503  83752]\n",
      "level_dim [132503  83752]\n",
      "name 11395\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11395.h5: total of 193 batches\n",
      "batch 0/193, 0 files processed\n",
      "batch 20/193, 10240 files processed\n",
      "batch 40/193, 20480 files processed\n",
      "batch 60/193, 30720 files processed\n",
      "batch 80/193, 40960 files processed\n",
      "batch 100/193, 51200 files processed\n",
      "batch 120/193, 61440 files processed\n",
      "batch 140/193, 71680 files processed\n",
      "batch 160/193, 81920 files processed\n",
      "batch 180/193, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11395.h5 took 168.2105507850647 s\n",
      "features size:  (98739, 1024)\n",
      "coordinates size:  (98739, 2)\n",
      "\n",
      "progress: 421/876\n",
      "11396\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [125781  83743]\n",
      "level_dim [125781  83743]\n",
      "name 11396\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11396.h5: total of 191 batches\n",
      "batch 0/191, 0 files processed\n",
      "batch 20/191, 10240 files processed\n",
      "batch 40/191, 20480 files processed\n",
      "batch 60/191, 30720 files processed\n",
      "batch 80/191, 40960 files processed\n",
      "batch 100/191, 51200 files processed\n",
      "batch 120/191, 61440 files processed\n",
      "batch 140/191, 71680 files processed\n",
      "batch 160/191, 81920 files processed\n",
      "batch 180/191, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11396.h5 took 151.65964269638062 s\n",
      "features size:  (97493, 1024)\n",
      "coordinates size:  (97493, 2)\n",
      "\n",
      "progress: 422/876\n",
      "11397\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [107538  66574]\n",
      "level_dim [107538  66574]\n",
      "name 11397\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11397.h5: total of 99 batches\n",
      "batch 0/99, 0 files processed\n",
      "batch 20/99, 10240 files processed\n",
      "batch 40/99, 20480 files processed\n",
      "batch 60/99, 30720 files processed\n",
      "batch 80/99, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11397.h5 took 69.93427729606628 s\n",
      "features size:  (50567, 1024)\n",
      "coordinates size:  (50567, 2)\n",
      "\n",
      "progress: 423/876\n",
      "11398\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [117140  83730]\n",
      "level_dim [117140  83730]\n",
      "name 11398\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11398.h5: total of 196 batches\n",
      "batch 0/196, 0 files processed\n",
      "batch 20/196, 10240 files processed\n",
      "batch 40/196, 20480 files processed\n",
      "batch 60/196, 30720 files processed\n",
      "batch 80/196, 40960 files processed\n",
      "batch 100/196, 51200 files processed\n",
      "batch 120/196, 61440 files processed\n",
      "batch 140/196, 71680 files processed\n",
      "batch 160/196, 81920 files processed\n",
      "batch 180/196, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11398.h5 took 148.34417939186096 s\n",
      "features size:  (99883, 1024)\n",
      "coordinates size:  (99883, 2)\n",
      "\n",
      "progress: 424/876\n",
      "11399\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [126742  77604]\n",
      "level_dim [126742  77604]\n",
      "name 11399\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11399.h5: total of 175 batches\n",
      "batch 0/175, 0 files processed\n",
      "batch 20/175, 10240 files processed\n",
      "batch 40/175, 20480 files processed\n",
      "batch 60/175, 30720 files processed\n",
      "batch 80/175, 40960 files processed\n",
      "batch 100/175, 51200 files processed\n",
      "batch 120/175, 61440 files processed\n",
      "batch 140/175, 71680 files processed\n",
      "batch 160/175, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11399.h5 took 150.2785096168518 s\n",
      "features size:  (89498, 1024)\n",
      "coordinates size:  (89498, 2)\n",
      "\n",
      "progress: 425/876\n",
      "11400\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [84494 50678]\n",
      "level_dim [84494 50678]\n",
      "name 11400\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11400.h5: total of 55 batches\n",
      "batch 0/55, 0 files processed\n",
      "batch 20/55, 10240 files processed\n",
      "batch 40/55, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11400.h5 took 42.76378893852234 s\n",
      "features size:  (27683, 1024)\n",
      "coordinates size:  (27683, 2)\n",
      "\n",
      "progress: 426/876\n",
      "11401\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [113299  83725]\n",
      "level_dim [113299  83725]\n",
      "name 11401\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11401.h5: total of 192 batches\n",
      "batch 0/192, 0 files processed\n",
      "batch 20/192, 10240 files processed\n",
      "batch 40/192, 20480 files processed\n",
      "batch 60/192, 30720 files processed\n",
      "batch 80/192, 40960 files processed\n",
      "batch 100/192, 51200 files processed\n",
      "batch 120/192, 61440 files processed\n",
      "batch 140/192, 71680 files processed\n",
      "batch 160/192, 81920 files processed\n",
      "batch 180/192, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11401.h5 took 141.4683439731598 s\n",
      "features size:  (98171, 1024)\n",
      "coordinates size:  (98171, 2)\n",
      "\n",
      "progress: 427/876\n",
      "11402\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [113299  83726]\n",
      "level_dim [113299  83726]\n",
      "name 11402\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11402.h5: total of 170 batches\n",
      "batch 0/170, 0 files processed\n",
      "batch 20/170, 10240 files processed\n",
      "batch 40/170, 20480 files processed\n",
      "batch 60/170, 30720 files processed\n",
      "batch 80/170, 40960 files processed\n",
      "batch 100/170, 51200 files processed\n",
      "batch 120/170, 61440 files processed\n",
      "batch 140/170, 71680 files processed\n",
      "batch 160/170, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11402.h5 took 154.92661261558533 s\n",
      "features size:  (86590, 1024)\n",
      "coordinates size:  (86590, 2)\n",
      "\n",
      "progress: 428/876\n",
      "11403\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [104658  83714]\n",
      "level_dim [104658  83714]\n",
      "name 11403\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11403.h5: total of 141 batches\n",
      "batch 0/141, 0 files processed\n",
      "batch 20/141, 10240 files processed\n",
      "batch 40/141, 20480 files processed\n",
      "batch 60/141, 30720 files processed\n",
      "batch 80/141, 40960 files processed\n",
      "batch 100/141, 51200 files processed\n",
      "batch 120/141, 61440 files processed\n",
      "batch 140/141, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11403.h5 took 122.45541882514954 s\n",
      "features size:  (71878, 1024)\n",
      "coordinates size:  (71878, 2)\n",
      "\n",
      "progress: 429/876\n",
      "11404\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [73932 83670]\n",
      "level_dim [73932 83670]\n",
      "name 11404\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11404.h5: total of 107 batches\n",
      "batch 0/107, 0 files processed\n",
      "batch 20/107, 10240 files processed\n",
      "batch 40/107, 20480 files processed\n",
      "batch 60/107, 30720 files processed\n",
      "batch 80/107, 40960 files processed\n",
      "batch 100/107, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11404.h5 took 80.1330897808075 s\n",
      "features size:  (54475, 1024)\n",
      "coordinates size:  (54475, 2)\n",
      "\n",
      "progress: 430/876\n",
      "11405\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96976 77562]\n",
      "level_dim [96976 77562]\n",
      "name 11405\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11405.h5: total of 114 batches\n",
      "batch 0/114, 0 files processed\n",
      "batch 20/114, 10240 files processed\n",
      "batch 40/114, 20480 files processed\n",
      "batch 60/114, 30720 files processed\n",
      "batch 80/114, 40960 files processed\n",
      "batch 100/114, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11405.h5 took 92.61333012580872 s\n",
      "features size:  (58121, 1024)\n",
      "coordinates size:  (58121, 2)\n",
      "\n",
      "progress: 431/876\n",
      "11406\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [105618  83714]\n",
      "level_dim [105618  83714]\n",
      "name 11406\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11406.h5: total of 134 batches\n",
      "batch 0/134, 0 files processed\n",
      "batch 20/134, 10240 files processed\n",
      "batch 40/134, 20480 files processed\n",
      "batch 60/134, 30720 files processed\n",
      "batch 80/134, 40960 files processed\n",
      "batch 100/134, 51200 files processed\n",
      "batch 120/134, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11406.h5 took 109.41484260559082 s\n",
      "features size:  (68243, 1024)\n",
      "coordinates size:  (68243, 2)\n",
      "\n",
      "progress: 432/876\n",
      "11407\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [104658  83713]\n",
      "level_dim [104658  83713]\n",
      "name 11407\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11407.h5: total of 157 batches\n",
      "batch 0/157, 0 files processed\n",
      "batch 20/157, 10240 files processed\n",
      "batch 40/157, 20480 files processed\n",
      "batch 60/157, 30720 files processed\n",
      "batch 80/157, 40960 files processed\n",
      "batch 100/157, 51200 files processed\n",
      "batch 120/157, 61440 files processed\n",
      "batch 140/157, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11407.h5 took 130.33176279067993 s\n",
      "features size:  (80111, 1024)\n",
      "coordinates size:  (80111, 2)\n",
      "\n",
      "progress: 433/876\n",
      "11408\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [102737  68359]\n",
      "level_dim [102737  68359]\n",
      "name 11408\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11408.h5: total of 111 batches\n",
      "batch 0/111, 0 files processed\n",
      "batch 20/111, 10240 files processed\n",
      "batch 40/111, 20480 files processed\n",
      "batch 60/111, 30720 files processed\n",
      "batch 80/111, 40960 files processed\n",
      "batch 100/111, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11408.h5 took 78.36689877510071 s\n",
      "features size:  (56334, 1024)\n",
      "coordinates size:  (56334, 2)\n",
      "\n",
      "progress: 434/876\n",
      "11409\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [76813 76766]\n",
      "level_dim [76813 76766]\n",
      "name 11409\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11409.h5: total of 98 batches\n",
      "batch 0/98, 0 files processed\n",
      "batch 20/98, 10240 files processed\n",
      "batch 40/98, 20480 files processed\n",
      "batch 60/98, 30720 files processed\n",
      "batch 80/98, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11409.h5 took 79.96146273612976 s\n",
      "features size:  (49870, 1024)\n",
      "coordinates size:  (49870, 2)\n",
      "\n",
      "progress: 435/876\n",
      "11410\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [60490 33502]\n",
      "level_dim [60490 33502]\n",
      "name 11410\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11410.h5: total of 23 batches\n",
      "batch 0/23, 0 files processed\n",
      "batch 20/23, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11410.h5 took 19.063766717910767 s\n",
      "features size:  (11773, 1024)\n",
      "coordinates size:  (11773, 2)\n",
      "\n",
      "progress: 436/876\n",
      "11411\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [67211 59865]\n",
      "level_dim [67211 59865]\n",
      "name 11411\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11411.h5: total of 70 batches\n",
      "batch 0/70, 0 files processed\n",
      "batch 20/70, 10240 files processed\n",
      "batch 40/70, 20480 files processed\n",
      "batch 60/70, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11411.h5 took 56.37196230888367 s\n",
      "features size:  (35633, 1024)\n",
      "coordinates size:  (35633, 2)\n",
      "\n",
      "progress: 437/876\n",
      "11412\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [78733 70883]\n",
      "level_dim [78733 70883]\n",
      "name 11412\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11412.h5: total of 80 batches\n",
      "batch 0/80, 0 files processed\n",
      "batch 20/80, 10240 files processed\n",
      "batch 40/80, 20480 files processed\n",
      "batch 60/80, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11412.h5 took 60.036227226257324 s\n",
      "features size:  (40766, 1024)\n",
      "coordinates size:  (40766, 2)\n",
      "\n",
      "progress: 438/876\n",
      "11413\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [60490 56018]\n",
      "level_dim [60490 56018]\n",
      "name 11413\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11413.h5: total of 40 batches\n",
      "batch 0/40, 0 files processed\n",
      "batch 20/40, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11413.h5 took 34.79002070426941 s\n",
      "features size:  (20264, 1024)\n",
      "coordinates size:  (20264, 2)\n",
      "\n",
      "progress: 439/876\n",
      "11417\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [112339  83724]\n",
      "level_dim [112339  83724]\n",
      "name 11417\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11417.h5: total of 168 batches\n",
      "batch 0/168, 0 files processed\n",
      "batch 20/168, 10240 files processed\n",
      "batch 40/168, 20480 files processed\n",
      "batch 60/168, 30720 files processed\n",
      "batch 80/168, 40960 files processed\n",
      "batch 100/168, 51200 files processed\n",
      "batch 120/168, 61440 files processed\n",
      "batch 140/168, 71680 files processed\n",
      "batch 160/168, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11417.h5 took 99.62024474143982 s\n",
      "features size:  (85740, 1024)\n",
      "coordinates size:  (85740, 2)\n",
      "\n",
      "progress: 440/876\n",
      "11418\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [78733 83165]\n",
      "level_dim [78733 83165]\n",
      "name 11418\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11418.h5: total of 104 batches\n",
      "batch 0/104, 0 files processed\n",
      "batch 20/104, 10240 files processed\n",
      "batch 40/104, 20480 files processed\n",
      "batch 60/104, 30720 files processed\n",
      "batch 80/104, 40960 files processed\n",
      "batch 100/104, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11418.h5 took 84.49436640739441 s\n",
      "features size:  (52946, 1024)\n",
      "coordinates size:  (52946, 2)\n",
      "\n",
      "progress: 441/876\n",
      "11419\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [64331 71631]\n",
      "level_dim [64331 71631]\n",
      "name 11419\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11419.h5: total of 73 batches\n",
      "batch 0/73, 0 files processed\n",
      "batch 20/73, 10240 files processed\n",
      "batch 40/73, 20480 files processed\n",
      "batch 60/73, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11419.h5 took 56.54703712463379 s\n",
      "features size:  (36873, 1024)\n",
      "coordinates size:  (36873, 2)\n",
      "\n",
      "progress: 442/876\n",
      "11420\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [102737  70918]\n",
      "level_dim [102737  70918]\n",
      "name 11420\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11420.h5: total of 136 batches\n",
      "batch 0/136, 0 files processed\n",
      "batch 20/136, 10240 files processed\n",
      "batch 40/136, 20480 files processed\n",
      "batch 60/136, 30720 files processed\n",
      "batch 80/136, 40960 files processed\n",
      "batch 100/136, 51200 files processed\n",
      "batch 120/136, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11420.h5 took 118.99926376342773 s\n",
      "features size:  (69483, 1024)\n",
      "coordinates size:  (69483, 2)\n",
      "\n",
      "progress: 443/876\n",
      "11421\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [106578  60944]\n",
      "level_dim [106578  60944]\n",
      "name 11421\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11421.h5: total of 101 batches\n",
      "batch 0/101, 0 files processed\n",
      "batch 20/101, 10240 files processed\n",
      "batch 40/101, 20480 files processed\n",
      "batch 60/101, 30720 files processed\n",
      "batch 80/101, 40960 files processed\n",
      "batch 100/101, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11421.h5 took 86.26351976394653 s\n",
      "features size:  (51250, 1024)\n",
      "coordinates size:  (51250, 2)\n",
      "\n",
      "progress: 444/876\n",
      "11422\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [123861  83740]\n",
      "level_dim [123861  83740]\n",
      "name 11422\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11422.h5: total of 197 batches\n",
      "batch 0/197, 0 files processed\n",
      "batch 20/197, 10240 files processed\n",
      "batch 40/197, 20480 files processed\n",
      "batch 60/197, 30720 files processed\n",
      "batch 80/197, 40960 files processed\n",
      "batch 100/197, 51200 files processed\n",
      "batch 120/197, 61440 files processed\n",
      "batch 140/197, 71680 files processed\n",
      "batch 160/197, 81920 files processed\n",
      "batch 180/197, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11422.h5 took 161.305317401886 s\n",
      "features size:  (100540, 1024)\n",
      "coordinates size:  (100540, 2)\n",
      "\n",
      "progress: 445/876\n",
      "11423\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96016 53510]\n",
      "level_dim [96016 53510]\n",
      "name 11423\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11423.h5: total of 82 batches\n",
      "batch 0/82, 0 files processed\n",
      "batch 20/82, 10240 files processed\n",
      "batch 40/82, 20480 files processed\n",
      "batch 60/82, 30720 files processed\n",
      "batch 80/82, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11423.h5 took 70.08502292633057 s\n",
      "features size:  (41730, 1024)\n",
      "coordinates size:  (41730, 2)\n",
      "\n",
      "progress: 446/876\n",
      "11424\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [72972 81622]\n",
      "level_dim [72972 81622]\n",
      "name 11424\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11424.h5: total of 109 batches\n",
      "batch 0/109, 0 files processed\n",
      "batch 20/109, 10240 files processed\n",
      "batch 40/109, 20480 files processed\n",
      "batch 60/109, 30720 files processed\n",
      "batch 80/109, 40960 files processed\n",
      "batch 100/109, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11424.h5 took 63.61259055137634 s\n",
      "features size:  (55537, 1024)\n",
      "coordinates size:  (55537, 2)\n",
      "\n",
      "progress: 447/876\n",
      "11425\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [99857 58376]\n",
      "level_dim [99857 58376]\n",
      "name 11425\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11425.h5: total of 98 batches\n",
      "batch 0/98, 0 files processed\n",
      "batch 20/98, 10240 files processed\n",
      "batch 40/98, 20480 files processed\n",
      "batch 60/98, 30720 files processed\n",
      "batch 80/98, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11425.h5 took 81.91726016998291 s\n",
      "features size:  (49751, 1024)\n",
      "coordinates size:  (49751, 2)\n",
      "\n",
      "progress: 448/876\n",
      "11426\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [80654 78563]\n",
      "level_dim [80654 78563]\n",
      "name 11426\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11426.h5: total of 108 batches\n",
      "batch 0/108, 0 files processed\n",
      "batch 20/108, 10240 files processed\n",
      "batch 40/108, 20480 files processed\n",
      "batch 60/108, 30720 files processed\n",
      "batch 80/108, 40960 files processed\n",
      "batch 100/108, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11426.h5 took 89.40250301361084 s\n",
      "features size:  (55140, 1024)\n",
      "coordinates size:  (55140, 2)\n",
      "\n",
      "progress: 449/876\n",
      "11427\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [97937 60420]\n",
      "level_dim [97937 60420]\n",
      "name 11427\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11427.h5: total of 97 batches\n",
      "batch 0/97, 0 files processed\n",
      "batch 20/97, 10240 files processed\n",
      "batch 40/97, 20480 files processed\n",
      "batch 60/97, 30720 files processed\n",
      "batch 80/97, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11427.h5 took 86.26995778083801 s\n",
      "features size:  (49330, 1024)\n",
      "coordinates size:  (49330, 2)\n",
      "\n",
      "progress: 450/876\n",
      "11428\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [80654 58861]\n",
      "level_dim [80654 58861]\n",
      "name 11428\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11428.h5: total of 82 batches\n",
      "batch 0/82, 0 files processed\n",
      "batch 20/82, 10240 files processed\n",
      "batch 40/82, 20480 files processed\n",
      "batch 60/82, 30720 files processed\n",
      "batch 80/82, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11428.h5 took 73.95094752311707 s\n",
      "features size:  (41876, 1024)\n",
      "coordinates size:  (41876, 2)\n",
      "\n",
      "progress: 451/876\n",
      "11429\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51849 70078]\n",
      "level_dim [51849 70078]\n",
      "name 11429\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11429.h5: total of 72 batches\n",
      "batch 0/72, 0 files processed\n",
      "batch 20/72, 10240 files processed\n",
      "batch 40/72, 20480 files processed\n",
      "batch 60/72, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11429.h5 took 61.54327845573425 s\n",
      "features size:  (36774, 1024)\n",
      "coordinates size:  (36774, 2)\n",
      "\n",
      "progress: 452/876\n",
      "11430\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [109459  62994]\n",
      "level_dim [109459  62994]\n",
      "name 11430\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11430.h5: total of 149 batches\n",
      "batch 0/149, 0 files processed\n",
      "batch 20/149, 10240 files processed\n",
      "batch 40/149, 20480 files processed\n",
      "batch 60/149, 30720 files processed\n",
      "batch 80/149, 40960 files processed\n",
      "batch 100/149, 51200 files processed\n",
      "batch 120/149, 61440 files processed\n",
      "batch 140/149, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11430.h5 took 124.96554446220398 s\n",
      "features size:  (75816, 1024)\n",
      "coordinates size:  (75816, 2)\n",
      "\n",
      "progress: 453/876\n",
      "11431\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [70092 71639]\n",
      "level_dim [70092 71639]\n",
      "name 11431\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11431.h5: total of 96 batches\n",
      "batch 0/96, 0 files processed\n",
      "batch 20/96, 10240 files processed\n",
      "batch 40/96, 20480 files processed\n",
      "batch 60/96, 30720 files processed\n",
      "batch 80/96, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11431.h5 took 77.70452332496643 s\n",
      "features size:  (48711, 1024)\n",
      "coordinates size:  (48711, 2)\n",
      "\n",
      "progress: 454/876\n",
      "11432\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [74893 83672]\n",
      "level_dim [74893 83672]\n",
      "name 11432\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11432.h5: total of 115 batches\n",
      "batch 0/115, 0 files processed\n",
      "batch 20/115, 10240 files processed\n",
      "batch 40/115, 20480 files processed\n",
      "batch 60/115, 30720 files processed\n",
      "batch 80/115, 40960 files processed\n",
      "batch 100/115, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11432.h5 took 97.12549304962158 s\n",
      "features size:  (58743, 1024)\n",
      "coordinates size:  (58743, 2)\n",
      "\n",
      "progress: 455/876\n",
      "11433\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [86415 83688]\n",
      "level_dim [86415 83688]\n",
      "name 11433\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11433.h5: total of 129 batches\n",
      "batch 0/129, 0 files processed\n",
      "batch 20/129, 10240 files processed\n",
      "batch 40/129, 20480 files processed\n",
      "batch 60/129, 30720 files processed\n",
      "batch 80/129, 40960 files processed\n",
      "batch 100/129, 51200 files processed\n",
      "batch 120/129, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11433.h5 took 74.72990226745605 s\n",
      "features size:  (65780, 1024)\n",
      "coordinates size:  (65780, 2)\n",
      "\n",
      "progress: 456/876\n",
      "11434\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [76813 71905]\n",
      "level_dim [76813 71905]\n",
      "name 11434\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11434.h5: total of 60 batches\n",
      "batch 0/60, 0 files processed\n",
      "batch 20/60, 10240 files processed\n",
      "batch 40/60, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11434.h5 took 43.201228857040405 s\n",
      "features size:  (30600, 1024)\n",
      "coordinates size:  (30600, 2)\n",
      "\n",
      "progress: 457/876\n",
      "11435\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [119060  83733]\n",
      "level_dim [119060  83733]\n",
      "name 11435\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11435.h5: total of 179 batches\n",
      "batch 0/179, 0 files processed\n",
      "batch 20/179, 10240 files processed\n",
      "batch 40/179, 20480 files processed\n",
      "batch 60/179, 30720 files processed\n",
      "batch 80/179, 40960 files processed\n",
      "batch 100/179, 51200 files processed\n",
      "batch 120/179, 61440 files processed\n",
      "batch 140/179, 71680 files processed\n",
      "batch 160/179, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11435.h5 took 142.39885640144348 s\n",
      "features size:  (91558, 1024)\n",
      "coordinates size:  (91558, 2)\n",
      "\n",
      "progress: 458/876\n",
      "11436\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [80654 70887]\n",
      "level_dim [80654 70887]\n",
      "name 11436\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11436.h5: total of 95 batches\n",
      "batch 0/95, 0 files processed\n",
      "batch 20/95, 10240 files processed\n",
      "batch 40/95, 20480 files processed\n",
      "batch 60/95, 30720 files processed\n",
      "batch 80/95, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11436.h5 took 58.04250979423523 s\n",
      "features size:  (48585, 1024)\n",
      "coordinates size:  (48585, 2)\n",
      "\n",
      "progress: 459/876\n",
      "11437\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [104658  47125]\n",
      "level_dim [104658  47125]\n",
      "name 11437\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11437.h5: total of 27 batches\n",
      "batch 0/27, 0 files processed\n",
      "batch 20/27, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11437.h5 took 16.725030422210693 s\n",
      "features size:  (13337, 1024)\n",
      "coordinates size:  (13337, 2)\n",
      "\n",
      "progress: 460/876\n",
      "11439\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [113299  79632]\n",
      "level_dim [113299  79632]\n",
      "name 11439\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11439.h5: total of 198 batches\n",
      "batch 0/198, 0 files processed\n",
      "batch 20/198, 10240 files processed\n",
      "batch 40/198, 20480 files processed\n",
      "batch 60/198, 30720 files processed\n",
      "batch 80/198, 40960 files processed\n",
      "batch 100/198, 51200 files processed\n",
      "batch 120/198, 61440 files processed\n",
      "batch 140/198, 71680 files processed\n",
      "batch 160/198, 81920 files processed\n",
      "batch 180/198, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11439.h5 took 164.30238151550293 s\n",
      "features size:  (101292, 1024)\n",
      "coordinates size:  (101292, 2)\n",
      "\n",
      "progress: 461/876\n",
      "11440\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [105618  83715]\n",
      "level_dim [105618  83715]\n",
      "name 11440\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11440.h5: total of 168 batches\n",
      "batch 0/168, 0 files processed\n",
      "batch 20/168, 10240 files processed\n",
      "batch 40/168, 20480 files processed\n",
      "batch 60/168, 30720 files processed\n",
      "batch 80/168, 40960 files processed\n",
      "batch 100/168, 51200 files processed\n",
      "batch 120/168, 61440 files processed\n",
      "batch 140/168, 71680 files processed\n",
      "batch 160/168, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11440.h5 took 143.1298086643219 s\n",
      "features size:  (85714, 1024)\n",
      "coordinates size:  (85714, 2)\n",
      "\n",
      "progress: 462/876\n",
      "11441\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [138264  83761]\n",
      "level_dim [138264  83761]\n",
      "name 11441\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11441.h5: total of 209 batches\n",
      "batch 0/209, 0 files processed\n",
      "batch 20/209, 10240 files processed\n",
      "batch 40/209, 20480 files processed\n",
      "batch 60/209, 30720 files processed\n",
      "batch 80/209, 40960 files processed\n",
      "batch 100/209, 51200 files processed\n",
      "batch 120/209, 61440 files processed\n",
      "batch 140/209, 71680 files processed\n",
      "batch 160/209, 81920 files processed\n",
      "batch 180/209, 92160 files processed\n",
      "batch 200/209, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11441.h5 took 166.434654712677 s\n",
      "features size:  (106599, 1024)\n",
      "coordinates size:  (106599, 2)\n",
      "\n",
      "progress: 463/876\n",
      "11445\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [88335 83179]\n",
      "level_dim [88335 83179]\n",
      "name 11445\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11445.h5: total of 116 batches\n",
      "batch 0/116, 0 files processed\n",
      "batch 20/116, 10240 files processed\n",
      "batch 40/116, 20480 files processed\n",
      "batch 60/116, 30720 files processed\n",
      "batch 80/116, 40960 files processed\n",
      "batch 100/116, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11445.h5 took 94.92356967926025 s\n",
      "features size:  (59026, 1024)\n",
      "coordinates size:  (59026, 2)\n",
      "\n",
      "progress: 464/876\n",
      "11446\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [88335 63477]\n",
      "level_dim [88335 63477]\n",
      "name 11446\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11446.h5: total of 85 batches\n",
      "batch 0/85, 0 files processed\n",
      "batch 20/85, 10240 files processed\n",
      "batch 40/85, 20480 files processed\n",
      "batch 60/85, 30720 files processed\n",
      "batch 80/85, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11446.h5 took 67.84959840774536 s\n",
      "features size:  (43459, 1024)\n",
      "coordinates size:  (43459, 2)\n",
      "\n",
      "progress: 465/876\n",
      "11447\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [117140  79380]\n",
      "level_dim [117140  79380]\n",
      "name 11447\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11447.h5: total of 132 batches\n",
      "batch 0/132, 0 files processed\n",
      "batch 20/132, 10240 files processed\n",
      "batch 40/132, 20480 files processed\n",
      "batch 60/132, 30720 files processed\n",
      "batch 80/132, 40960 files processed\n",
      "batch 100/132, 51200 files processed\n",
      "batch 120/132, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11447.h5 took 96.47463917732239 s\n",
      "features size:  (67316, 1024)\n",
      "coordinates size:  (67316, 2)\n",
      "\n",
      "progress: 466/876\n",
      "11454\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96016 83701]\n",
      "level_dim [96016 83701]\n",
      "name 11454\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11454.h5: total of 163 batches\n",
      "batch 0/163, 0 files processed\n",
      "batch 20/163, 10240 files processed\n",
      "batch 40/163, 20480 files processed\n",
      "batch 60/163, 30720 files processed\n",
      "batch 80/163, 40960 files processed\n",
      "batch 100/163, 51200 files processed\n",
      "batch 120/163, 61440 files processed\n",
      "batch 140/163, 71680 files processed\n",
      "batch 160/163, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11454.h5 took 142.69016408920288 s\n",
      "features size:  (82945, 1024)\n",
      "coordinates size:  (82945, 2)\n",
      "\n",
      "progress: 467/876\n",
      "11455\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [132503  82218]\n",
      "level_dim [132503  82218]\n",
      "name 11455\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11455.h5: total of 189 batches\n",
      "batch 0/189, 0 files processed\n",
      "batch 20/189, 10240 files processed\n",
      "batch 40/189, 20480 files processed\n",
      "batch 60/189, 30720 files processed\n",
      "batch 80/189, 40960 files processed\n",
      "batch 100/189, 51200 files processed\n",
      "batch 120/189, 61440 files processed\n",
      "batch 140/189, 71680 files processed\n",
      "batch 160/189, 81920 files processed\n",
      "batch 180/189, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11455.h5 took 149.34298634529114 s\n",
      "features size:  (96331, 1024)\n",
      "coordinates size:  (96331, 2)\n",
      "\n",
      "progress: 468/876\n",
      "11456\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [78733 74210]\n",
      "level_dim [78733 74210]\n",
      "name 11456\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11456.h5: total of 77 batches\n",
      "batch 0/77, 0 files processed\n",
      "batch 20/77, 10240 files processed\n",
      "batch 40/77, 20480 files processed\n",
      "batch 60/77, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11456.h5 took 61.84537220001221 s\n",
      "features size:  (38940, 1024)\n",
      "coordinates size:  (38940, 2)\n",
      "\n",
      "progress: 469/876\n",
      "11457\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [97937 83704]\n",
      "level_dim [97937 83704]\n",
      "name 11457\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11457.h5: total of 115 batches\n",
      "batch 0/115, 0 files processed\n",
      "batch 20/115, 10240 files processed\n",
      "batch 40/115, 20480 files processed\n",
      "batch 60/115, 30720 files processed\n",
      "batch 80/115, 40960 files processed\n",
      "batch 100/115, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11457.h5 took 81.63669228553772 s\n",
      "features size:  (58430, 1024)\n",
      "coordinates size:  (58430, 2)\n",
      "\n",
      "progress: 470/876\n",
      "11460\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [112339  83213]\n",
      "level_dim [112339  83213]\n",
      "name 11460\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11460.h5: total of 119 batches\n",
      "batch 0/119, 0 files processed\n",
      "batch 20/119, 10240 files processed\n",
      "batch 40/119, 20480 files processed\n",
      "batch 60/119, 30720 files processed\n",
      "batch 80/119, 40960 files processed\n",
      "batch 100/119, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11460.h5 took 88.90093159675598 s\n",
      "features size:  (60575, 1024)\n",
      "coordinates size:  (60575, 2)\n",
      "\n",
      "progress: 471/876\n",
      "11461\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [107538  68110]\n",
      "level_dim [107538  68110]\n",
      "name 11461\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11461.h5: total of 94 batches\n",
      "batch 0/94, 0 files processed\n",
      "batch 20/94, 10240 files processed\n",
      "batch 40/94, 20480 files processed\n",
      "batch 60/94, 30720 files processed\n",
      "batch 80/94, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11461.h5 took 72.03709626197815 s\n",
      "features size:  (47668, 1024)\n",
      "coordinates size:  (47668, 2)\n",
      "\n",
      "progress: 472/876\n",
      "11462\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [108498  69135]\n",
      "level_dim [108498  69135]\n",
      "name 11462\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11462.h5: total of 99 batches\n",
      "batch 0/99, 0 files processed\n",
      "batch 20/99, 10240 files processed\n",
      "batch 40/99, 20480 files processed\n",
      "batch 60/99, 30720 files processed\n",
      "batch 80/99, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11462.h5 took 84.24218392372131 s\n",
      "features size:  (50404, 1024)\n",
      "coordinates size:  (50404, 2)\n",
      "\n",
      "progress: 473/876\n",
      "11463\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [53769 49356]\n",
      "level_dim [53769 49356]\n",
      "name 11463\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11463.h5: total of 28 batches\n",
      "batch 0/28, 0 files processed\n",
      "batch 20/28, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11463.h5 took 24.316757440567017 s\n",
      "features size:  (14266, 1024)\n",
      "coordinates size:  (14266, 2)\n",
      "\n",
      "progress: 474/876\n",
      "11464\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [48968 49861]\n",
      "level_dim [48968 49861]\n",
      "name 11464\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11464.h5: total of 30 batches\n",
      "batch 0/30, 0 files processed\n",
      "batch 20/30, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11464.h5 took 23.136574506759644 s\n",
      "features size:  (15173, 1024)\n",
      "coordinates size:  (15173, 2)\n",
      "\n",
      "progress: 475/876\n",
      "11465\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [94096 77302]\n",
      "level_dim [94096 77302]\n",
      "name 11465\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11465.h5: total of 108 batches\n",
      "batch 0/108, 0 files processed\n",
      "batch 20/108, 10240 files processed\n",
      "batch 40/108, 20480 files processed\n",
      "batch 60/108, 30720 files processed\n",
      "batch 80/108, 40960 files processed\n",
      "batch 100/108, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11465.h5 took 90.53462862968445 s\n",
      "features size:  (55120, 1024)\n",
      "coordinates size:  (55120, 2)\n",
      "\n",
      "progress: 476/876\n",
      "11466\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [82574 69610]\n",
      "level_dim [82574 69610]\n",
      "name 11466\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11466.h5: total of 95 batches\n",
      "batch 0/95, 0 files processed\n",
      "batch 20/95, 10240 files processed\n",
      "batch 40/95, 20480 files processed\n",
      "batch 60/95, 30720 files processed\n",
      "batch 80/95, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11466.h5 took 80.41897821426392 s\n",
      "features size:  (48603, 1024)\n",
      "coordinates size:  (48603, 2)\n",
      "\n",
      "progress: 477/876\n",
      "11467\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [107538  83717]\n",
      "level_dim [107538  83717]\n",
      "name 11467\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11467.h5: total of 162 batches\n",
      "batch 0/162, 0 files processed\n",
      "batch 20/162, 10240 files processed\n",
      "batch 40/162, 20480 files processed\n",
      "batch 60/162, 30720 files processed\n",
      "batch 80/162, 40960 files processed\n",
      "batch 100/162, 51200 files processed\n",
      "batch 120/162, 61440 files processed\n",
      "batch 140/162, 71680 files processed\n",
      "batch 160/162, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11467.h5 took 120.64264488220215 s\n",
      "features size:  (82442, 1024)\n",
      "coordinates size:  (82442, 2)\n",
      "\n",
      "progress: 478/876\n",
      "11468\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [76813 83675]\n",
      "level_dim [76813 83675]\n",
      "name 11468\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11468.h5: total of 120 batches\n",
      "batch 0/120, 0 files processed\n",
      "batch 20/120, 10240 files processed\n",
      "batch 40/120, 20480 files processed\n",
      "batch 60/120, 30720 files processed\n",
      "batch 80/120, 40960 files processed\n",
      "batch 100/120, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11468.h5 took 64.69008445739746 s\n",
      "features size:  (61331, 1024)\n",
      "coordinates size:  (61331, 2)\n",
      "\n",
      "progress: 479/876\n",
      "11469\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [88335 80108]\n",
      "level_dim [88335 80108]\n",
      "name 11469\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11469.h5: total of 118 batches\n",
      "batch 0/118, 0 files processed\n",
      "batch 20/118, 10240 files processed\n",
      "batch 40/118, 20480 files processed\n",
      "batch 60/118, 30720 files processed\n",
      "batch 80/118, 40960 files processed\n",
      "batch 100/118, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11469.h5 took 89.42062711715698 s\n",
      "features size:  (60188, 1024)\n",
      "coordinates size:  (60188, 2)\n",
      "\n",
      "progress: 480/876\n",
      "11470\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [123861  79903]\n",
      "level_dim [123861  79903]\n",
      "name 11470\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11470.h5: total of 100 batches\n",
      "batch 0/100, 0 files processed\n",
      "batch 20/100, 10240 files processed\n",
      "batch 40/100, 20480 files processed\n",
      "batch 60/100, 30720 files processed\n",
      "batch 80/100, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11470.h5 took 79.38664269447327 s\n",
      "features size:  (51188, 1024)\n",
      "coordinates size:  (51188, 2)\n",
      "\n",
      "progress: 481/876\n",
      "11471\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [103698  78595]\n",
      "level_dim [103698  78595]\n",
      "name 11471\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11471.h5: total of 106 batches\n",
      "batch 0/106, 0 files processed\n",
      "batch 20/106, 10240 files processed\n",
      "batch 40/106, 20480 files processed\n",
      "batch 60/106, 30720 files processed\n",
      "batch 80/106, 40960 files processed\n",
      "batch 100/106, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11471.h5 took 89.80977654457092 s\n",
      "features size:  (53763, 1024)\n",
      "coordinates size:  (53763, 2)\n",
      "\n",
      "progress: 482/876\n",
      "11472\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [134423  70194]\n",
      "level_dim [134423  70194]\n",
      "name 11472\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11472.h5: total of 73 batches\n",
      "batch 0/73, 0 files processed\n",
      "batch 20/73, 10240 files processed\n",
      "batch 40/73, 20480 files processed\n",
      "batch 60/73, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11472.h5 took 55.873502016067505 s\n",
      "features size:  (37218, 1024)\n",
      "coordinates size:  (37218, 2)\n",
      "\n",
      "progress: 483/876\n",
      "11531\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [138264  78643]\n",
      "level_dim [138264  78643]\n",
      "name 11531\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11531.h5: total of 171 batches\n",
      "batch 0/171, 0 files processed\n",
      "batch 20/171, 10240 files processed\n",
      "batch 40/171, 20480 files processed\n",
      "batch 60/171, 30720 files processed\n",
      "batch 80/171, 40960 files processed\n",
      "batch 100/171, 51200 files processed\n",
      "batch 120/171, 61440 files processed\n",
      "batch 140/171, 71680 files processed\n",
      "batch 160/171, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11531.h5 took 127.68541526794434 s\n",
      "features size:  (87531, 1024)\n",
      "coordinates size:  (87531, 2)\n",
      "\n",
      "progress: 484/876\n",
      "11779\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [133463  83753]\n",
      "level_dim [133463  83753]\n",
      "name 11779\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11779.h5: total of 173 batches\n",
      "batch 0/173, 0 files processed\n",
      "batch 20/173, 10240 files processed\n",
      "batch 40/173, 20480 files processed\n",
      "batch 60/173, 30720 files processed\n",
      "batch 80/173, 40960 files processed\n",
      "batch 100/173, 51200 files processed\n",
      "batch 120/173, 61440 files processed\n",
      "batch 140/173, 71680 files processed\n",
      "batch 160/173, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11779.h5 took 143.69764351844788 s\n",
      "features size:  (88176, 1024)\n",
      "coordinates size:  (88176, 2)\n",
      "\n",
      "progress: 485/876\n",
      "11780\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [102737  69894]\n",
      "level_dim [102737  69894]\n",
      "name 11780\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11780.h5: total of 112 batches\n",
      "batch 0/112, 0 files processed\n",
      "batch 20/112, 10240 files processed\n",
      "batch 40/112, 20480 files processed\n",
      "batch 60/112, 30720 files processed\n",
      "batch 80/112, 40960 files processed\n",
      "batch 100/112, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11780.h5 took 82.2028923034668 s\n",
      "features size:  (56985, 1024)\n",
      "coordinates size:  (56985, 2)\n",
      "\n",
      "progress: 486/876\n",
      "11781\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [115220  81681]\n",
      "level_dim [115220  81681]\n",
      "name 11781\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11781.h5: total of 144 batches\n",
      "batch 0/144, 0 files processed\n",
      "batch 20/144, 10240 files processed\n",
      "batch 40/144, 20480 files processed\n",
      "batch 60/144, 30720 files processed\n",
      "batch 80/144, 40960 files processed\n",
      "batch 100/144, 51200 files processed\n",
      "batch 120/144, 61440 files processed\n",
      "batch 140/144, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11781.h5 took 104.15096712112427 s\n",
      "features size:  (73511, 1024)\n",
      "coordinates size:  (73511, 2)\n",
      "\n",
      "progress: 487/876\n",
      "11782\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [121941  83737]\n",
      "level_dim [121941  83737]\n",
      "name 11782\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11782.h5: total of 208 batches\n",
      "batch 0/208, 0 files processed\n",
      "batch 20/208, 10240 files processed\n",
      "batch 40/208, 20480 files processed\n",
      "batch 60/208, 30720 files processed\n",
      "batch 80/208, 40960 files processed\n",
      "batch 100/208, 51200 files processed\n",
      "batch 120/208, 61440 files processed\n",
      "batch 140/208, 71680 files processed\n",
      "batch 160/208, 81920 files processed\n",
      "batch 180/208, 92160 files processed\n",
      "batch 200/208, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11782.h5 took 155.69591426849365 s\n",
      "features size:  (106473, 1024)\n",
      "coordinates size:  (106473, 2)\n",
      "\n",
      "progress: 488/876\n",
      "11785\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [142104  83765]\n",
      "level_dim [142104  83765]\n",
      "name 11785\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11785.h5: total of 208 batches\n",
      "batch 0/208, 0 files processed\n",
      "batch 20/208, 10240 files processed\n",
      "batch 40/208, 20480 files processed\n",
      "batch 60/208, 30720 files processed\n",
      "batch 80/208, 40960 files processed\n",
      "batch 100/208, 51200 files processed\n",
      "batch 120/208, 61440 files processed\n",
      "batch 140/208, 71680 files processed\n",
      "batch 160/208, 81920 files processed\n",
      "batch 180/208, 92160 files processed\n",
      "batch 200/208, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11785.h5 took 136.34720826148987 s\n",
      "features size:  (106034, 1024)\n",
      "coordinates size:  (106034, 2)\n",
      "\n",
      "progress: 489/876\n",
      "11786\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [31685 30647]\n",
      "level_dim [31685 30647]\n",
      "name 11786\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11786.h5: total of 13 batches\n",
      "batch 0/13, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11786.h5 took 11.400312900543213 s\n",
      "features size:  (6534, 1024)\n",
      "coordinates size:  (6534, 2)\n",
      "\n",
      "progress: 490/876\n",
      "11787\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [139224  80947]\n",
      "level_dim [139224  80947]\n",
      "name 11787\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11787.h5: total of 142 batches\n",
      "batch 0/142, 0 files processed\n",
      "batch 20/142, 10240 files processed\n",
      "batch 40/142, 20480 files processed\n",
      "batch 60/142, 30720 files processed\n",
      "batch 80/142, 40960 files processed\n",
      "batch 100/142, 51200 files processed\n",
      "batch 120/142, 61440 files processed\n",
      "batch 140/142, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11787.h5 took 107.99795055389404 s\n",
      "features size:  (72266, 1024)\n",
      "coordinates size:  (72266, 2)\n",
      "\n",
      "progress: 491/876\n",
      "11788\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [120981  83736]\n",
      "level_dim [120981  83736]\n",
      "name 11788\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11788.h5: total of 201 batches\n",
      "batch 0/201, 0 files processed\n",
      "batch 20/201, 10240 files processed\n",
      "batch 40/201, 20480 files processed\n",
      "batch 60/201, 30720 files processed\n",
      "batch 80/201, 40960 files processed\n",
      "batch 100/201, 51200 files processed\n",
      "batch 120/201, 61440 files processed\n",
      "batch 140/201, 71680 files processed\n",
      "batch 160/201, 81920 files processed\n",
      "batch 180/201, 92160 files processed\n",
      "batch 200/201, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11788.h5 took 156.14462447166443 s\n",
      "features size:  (102662, 1024)\n",
      "coordinates size:  (102662, 2)\n",
      "\n",
      "progress: 492/876\n",
      "11789\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [137303  73012]\n",
      "level_dim [137303  73012]\n",
      "name 11789\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11789.h5: total of 145 batches\n",
      "batch 0/145, 0 files processed\n",
      "batch 20/145, 10240 files processed\n",
      "batch 40/145, 20480 files processed\n",
      "batch 60/145, 30720 files processed\n",
      "batch 80/145, 40960 files processed\n",
      "batch 100/145, 51200 files processed\n",
      "batch 120/145, 61440 files processed\n",
      "batch 140/145, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11789.h5 took 114.47546935081482 s\n",
      "features size:  (73889, 1024)\n",
      "coordinates size:  (73889, 2)\n",
      "\n",
      "progress: 493/876\n",
      "11790\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [84494 71915]\n",
      "level_dim [84494 71915]\n",
      "name 11790\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11790.h5: total of 125 batches\n",
      "batch 0/125, 0 files processed\n",
      "batch 20/125, 10240 files processed\n",
      "batch 40/125, 20480 files processed\n",
      "batch 60/125, 30720 files processed\n",
      "batch 80/125, 40960 files processed\n",
      "batch 100/125, 51200 files processed\n",
      "batch 120/125, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11790.h5 took 102.39613914489746 s\n",
      "features size:  (63850, 1024)\n",
      "coordinates size:  (63850, 2)\n",
      "\n",
      "progress: 494/876\n",
      "11791\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96016 75001]\n",
      "level_dim [96016 75001]\n",
      "name 11791\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11791.h5: total of 150 batches\n",
      "batch 0/150, 0 files processed\n",
      "batch 20/150, 10240 files processed\n",
      "batch 40/150, 20480 files processed\n",
      "batch 60/150, 30720 files processed\n",
      "batch 80/150, 40960 files processed\n",
      "batch 100/150, 51200 files processed\n",
      "batch 120/150, 61440 files processed\n",
      "batch 140/150, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11791.h5 took 125.23669981956482 s\n",
      "features size:  (76770, 1024)\n",
      "coordinates size:  (76770, 2)\n",
      "\n",
      "progress: 495/876\n",
      "11792\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [100817  80894]\n",
      "level_dim [100817  80894]\n",
      "name 11792\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11792.h5: total of 112 batches\n",
      "batch 0/112, 0 files processed\n",
      "batch 20/112, 10240 files processed\n",
      "batch 40/112, 20480 files processed\n",
      "batch 60/112, 30720 files processed\n",
      "batch 80/112, 40960 files processed\n",
      "batch 100/112, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11792.h5 took 70.54260897636414 s\n",
      "features size:  (56997, 1024)\n",
      "coordinates size:  (56997, 2)\n",
      "\n",
      "progress: 496/876\n",
      "11793\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [120020  68895]\n",
      "level_dim [120020  68895]\n",
      "name 11793\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11793.h5: total of 139 batches\n",
      "batch 0/139, 0 files processed\n",
      "batch 20/139, 10240 files processed\n",
      "batch 40/139, 20480 files processed\n",
      "batch 60/139, 30720 files processed\n",
      "batch 80/139, 40960 files processed\n",
      "batch 100/139, 51200 files processed\n",
      "batch 120/139, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11793.h5 took 119.91755390167236 s\n",
      "features size:  (70766, 1024)\n",
      "coordinates size:  (70766, 2)\n",
      "\n",
      "progress: 497/876\n",
      "11794\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [150746  73800]\n",
      "level_dim [150746  73800]\n",
      "name 11794\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11794.h5: total of 139 batches\n",
      "batch 0/139, 0 files processed\n",
      "batch 20/139, 10240 files processed\n",
      "batch 40/139, 20480 files processed\n",
      "batch 60/139, 30720 files processed\n",
      "batch 80/139, 40960 files processed\n",
      "batch 100/139, 51200 files processed\n",
      "batch 120/139, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11794.h5 took 111.14828181266785 s\n",
      "features size:  (71043, 1024)\n",
      "coordinates size:  (71043, 2)\n",
      "\n",
      "progress: 498/876\n",
      "11795\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [119060  67358]\n",
      "level_dim [119060  67358]\n",
      "name 11795\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11795.h5: total of 102 batches\n",
      "batch 0/102, 0 files processed\n",
      "batch 20/102, 10240 files processed\n",
      "batch 40/102, 20480 files processed\n",
      "batch 60/102, 30720 files processed\n",
      "batch 80/102, 40960 files processed\n",
      "batch 100/102, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11795.h5 took 53.38223385810852 s\n",
      "features size:  (51992, 1024)\n",
      "coordinates size:  (51992, 2)\n",
      "\n",
      "progress: 499/876\n",
      "11796\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [104658  70665]\n",
      "level_dim [104658  70665]\n",
      "name 11796\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/11796.h5: total of 116 batches\n",
      "batch 0/116, 0 files processed\n",
      "batch 20/116, 10240 files processed\n",
      "batch 40/116, 20480 files processed\n",
      "batch 60/116, 30720 files processed\n",
      "batch 80/116, 40960 files processed\n",
      "batch 100/116, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/11796.h5 took 84.95555853843689 s\n",
      "features size:  (59378, 1024)\n",
      "coordinates size:  (59378, 2)\n",
      "\n",
      "progress: 500/876\n",
      "12062\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [115220  89869]\n",
      "level_dim [115220  89869]\n",
      "name 12062\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/12062.h5: total of 208 batches\n",
      "batch 0/208, 0 files processed\n",
      "batch 20/208, 10240 files processed\n",
      "batch 40/208, 20480 files processed\n",
      "batch 60/208, 30720 files processed\n",
      "batch 80/208, 40960 files processed\n",
      "batch 100/208, 51200 files processed\n",
      "batch 120/208, 61440 files processed\n",
      "batch 140/208, 71680 files processed\n",
      "batch 160/208, 81920 files processed\n",
      "batch 180/208, 92160 files processed\n",
      "batch 200/208, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/12062.h5 took 158.4170331954956 s\n",
      "features size:  (106119, 1024)\n",
      "coordinates size:  (106119, 2)\n",
      "\n",
      "progress: 501/876\n",
      "12063\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [108498  78346]\n",
      "level_dim [108498  78346]\n",
      "name 12063\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/12063.h5: total of 114 batches\n",
      "batch 0/114, 0 files processed\n",
      "batch 20/114, 10240 files processed\n",
      "batch 40/114, 20480 files processed\n",
      "batch 60/114, 30720 files processed\n",
      "batch 80/114, 40960 files processed\n",
      "batch 100/114, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/12063.h5 took 75.47240042686462 s\n",
      "features size:  (58019, 1024)\n",
      "coordinates size:  (58019, 2)\n",
      "\n",
      "progress: 502/876\n",
      "9387\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [120981  85527]\n",
      "level_dim [120981  85527]\n",
      "name 9387\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9387.h5: total of 186 batches\n",
      "batch 0/186, 0 files processed\n",
      "batch 20/186, 10240 files processed\n",
      "batch 40/186, 20480 files processed\n",
      "batch 60/186, 30720 files processed\n",
      "batch 80/186, 40960 files processed\n",
      "batch 100/186, 51200 files processed\n",
      "batch 120/186, 61440 files processed\n",
      "batch 140/186, 71680 files processed\n",
      "batch 160/186, 81920 files processed\n",
      "batch 180/186, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9387.h5 took 166.69884252548218 s\n",
      "features size:  (95167, 1024)\n",
      "coordinates size:  (95167, 2)\n",
      "\n",
      "progress: 503/876\n",
      "9388\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [112339  70675]\n",
      "level_dim [112339  70675]\n",
      "name 9388\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9388.h5: total of 70 batches\n",
      "batch 0/70, 0 files processed\n",
      "batch 20/70, 10240 files processed\n",
      "batch 40/70, 20480 files processed\n",
      "batch 60/70, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9388.h5 took 52.9274480342865 s\n",
      "features size:  (35355, 1024)\n",
      "coordinates size:  (35355, 2)\n",
      "\n",
      "progress: 504/876\n",
      "9389\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [68171 39654]\n",
      "level_dim [68171 39654]\n",
      "name 9389\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9389.h5: total of 38 batches\n",
      "batch 0/38, 0 files processed\n",
      "batch 20/38, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9389.h5 took 29.773573875427246 s\n",
      "features size:  (19114, 1024)\n",
      "coordinates size:  (19114, 2)\n",
      "\n",
      "progress: 505/876\n",
      "9390\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [120020  85782]\n",
      "level_dim [120020  85782]\n",
      "name 9390\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9390.h5: total of 137 batches\n",
      "batch 0/137, 0 files processed\n",
      "batch 20/137, 10240 files processed\n",
      "batch 40/137, 20480 files processed\n",
      "batch 60/137, 30720 files processed\n",
      "batch 80/137, 40960 files processed\n",
      "batch 100/137, 51200 files processed\n",
      "batch 120/137, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9390.h5 took 113.63192415237427 s\n",
      "features size:  (69878, 1024)\n",
      "coordinates size:  (69878, 2)\n",
      "\n",
      "progress: 506/876\n",
      "9392\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96976 72189]\n",
      "level_dim [96976 72189]\n",
      "name 9392\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9392.h5: total of 109 batches\n",
      "batch 0/109, 0 files processed\n",
      "batch 20/109, 10240 files processed\n",
      "batch 40/109, 20480 files processed\n",
      "batch 60/109, 30720 files processed\n",
      "batch 80/109, 40960 files processed\n",
      "batch 100/109, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9392.h5 took 73.88986492156982 s\n",
      "features size:  (55542, 1024)\n",
      "coordinates size:  (55542, 2)\n",
      "\n",
      "progress: 507/876\n",
      "9393\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [116180  79380]\n",
      "level_dim [116180  79380]\n",
      "name 9393\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9393.h5: total of 133 batches\n",
      "batch 0/133, 0 files processed\n",
      "batch 20/133, 10240 files processed\n",
      "batch 40/133, 20480 files processed\n",
      "batch 60/133, 30720 files processed\n",
      "batch 80/133, 40960 files processed\n",
      "batch 100/133, 51200 files processed\n",
      "batch 120/133, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9393.h5 took 113.2250804901123 s\n",
      "features size:  (67668, 1024)\n",
      "coordinates size:  (67668, 2)\n",
      "\n",
      "progress: 508/876\n",
      "9398\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96016 85749]\n",
      "level_dim [96016 85749]\n",
      "name 9398\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9398.h5: total of 173 batches\n",
      "batch 0/173, 0 files processed\n",
      "batch 20/173, 10240 files processed\n",
      "batch 40/173, 20480 files processed\n",
      "batch 60/173, 30720 files processed\n",
      "batch 80/173, 40960 files processed\n",
      "batch 100/173, 51200 files processed\n",
      "batch 120/173, 61440 files processed\n",
      "batch 140/173, 71680 files processed\n",
      "batch 160/173, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9398.h5 took 90.85109639167786 s\n",
      "features size:  (88205, 1024)\n",
      "coordinates size:  (88205, 2)\n",
      "\n",
      "progress: 509/876\n",
      "9399\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [95056 84211]\n",
      "level_dim [95056 84211]\n",
      "name 9399\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9399.h5: total of 123 batches\n",
      "batch 0/123, 0 files processed\n",
      "batch 20/123, 10240 files processed\n",
      "batch 40/123, 20480 files processed\n",
      "batch 60/123, 30720 files processed\n",
      "batch 80/123, 40960 files processed\n",
      "batch 100/123, 51200 files processed\n",
      "batch 120/123, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9399.h5 took 93.36009192466736 s\n",
      "features size:  (62767, 1024)\n",
      "coordinates size:  (62767, 2)\n",
      "\n",
      "progress: 510/876\n",
      "9400\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [74893 72158]\n",
      "level_dim [74893 72158]\n",
      "name 9400\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9400.h5: total of 79 batches\n",
      "batch 0/79, 0 files processed\n",
      "batch 20/79, 10240 files processed\n",
      "batch 40/79, 20480 files processed\n",
      "batch 60/79, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9400.h5 took 64.44726133346558 s\n",
      "features size:  (40284, 1024)\n",
      "coordinates size:  (40284, 2)\n",
      "\n",
      "progress: 511/876\n",
      "9401\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [63371 70863]\n",
      "level_dim [63371 70863]\n",
      "name 9401\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9401.h5: total of 72 batches\n",
      "batch 0/72, 0 files processed\n",
      "batch 20/72, 10240 files processed\n",
      "batch 40/72, 20480 files processed\n",
      "batch 60/72, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9401.h5 took 58.38572859764099 s\n",
      "features size:  (36424, 1024)\n",
      "coordinates size:  (36424, 2)\n",
      "\n",
      "progress: 512/876\n",
      "9402\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96976 79097]\n",
      "level_dim [96976 79097]\n",
      "name 9402\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9402.h5: total of 102 batches\n",
      "batch 0/102, 0 files processed\n",
      "batch 20/102, 10240 files processed\n",
      "batch 40/102, 20480 files processed\n",
      "batch 60/102, 30720 files processed\n",
      "batch 80/102, 40960 files processed\n",
      "batch 100/102, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9402.h5 took 89.9610230922699 s\n",
      "features size:  (51965, 1024)\n",
      "coordinates size:  (51965, 2)\n",
      "\n",
      "progress: 513/876\n",
      "9406\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [100817  76288]\n",
      "level_dim [100817  76288]\n",
      "name 9406\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9406.h5: total of 116 batches\n",
      "batch 0/116, 0 files processed\n",
      "batch 20/116, 10240 files processed\n",
      "batch 40/116, 20480 files processed\n",
      "batch 60/116, 30720 files processed\n",
      "batch 80/116, 40960 files processed\n",
      "batch 100/116, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9406.h5 took 87.01088738441467 s\n",
      "features size:  (58952, 1024)\n",
      "coordinates size:  (58952, 2)\n",
      "\n",
      "progress: 514/876\n",
      "9407\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [90255 77041]\n",
      "level_dim [90255 77041]\n",
      "name 9407\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9407.h5: total of 116 batches\n",
      "batch 0/116, 0 files processed\n",
      "batch 20/116, 10240 files processed\n",
      "batch 40/116, 20480 files processed\n",
      "batch 60/116, 30720 files processed\n",
      "batch 80/116, 40960 files processed\n",
      "batch 100/116, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9407.h5 took 102.47880482673645 s\n",
      "features size:  (58935, 1024)\n",
      "coordinates size:  (58935, 2)\n",
      "\n",
      "progress: 515/876\n",
      "9408\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [95056 68604]\n",
      "level_dim [95056 68604]\n",
      "name 9408\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9408.h5: total of 105 batches\n",
      "batch 0/105, 0 files processed\n",
      "batch 20/105, 10240 files processed\n",
      "batch 40/105, 20480 files processed\n",
      "batch 60/105, 30720 files processed\n",
      "batch 80/105, 40960 files processed\n",
      "batch 100/105, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9408.h5 took 90.7452301979065 s\n",
      "features size:  (53385, 1024)\n",
      "coordinates size:  (53385, 2)\n",
      "\n",
      "progress: 516/876\n",
      "9459\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [99857 81148]\n",
      "level_dim [99857 81148]\n",
      "name 9459\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9459.h5: total of 154 batches\n",
      "batch 0/154, 0 files processed\n",
      "batch 20/154, 10240 files processed\n",
      "batch 40/154, 20480 files processed\n",
      "batch 60/154, 30720 files processed\n",
      "batch 80/154, 40960 files processed\n",
      "batch 100/154, 51200 files processed\n",
      "batch 120/154, 61440 files processed\n",
      "batch 140/154, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9459.h5 took 130.62915563583374 s\n",
      "features size:  (78668, 1024)\n",
      "coordinates size:  (78668, 2)\n",
      "\n",
      "progress: 517/876\n",
      "9460\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [101777  74755]\n",
      "level_dim [101777  74755]\n",
      "name 9460\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9460.h5: total of 139 batches\n",
      "batch 0/139, 0 files processed\n",
      "batch 20/139, 10240 files processed\n",
      "batch 40/139, 20480 files processed\n",
      "batch 60/139, 30720 files processed\n",
      "batch 80/139, 40960 files processed\n",
      "batch 100/139, 51200 files processed\n",
      "batch 120/139, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9460.h5 took 125.50872945785522 s\n",
      "features size:  (70991, 1024)\n",
      "coordinates size:  (70991, 2)\n",
      "\n",
      "progress: 518/876\n",
      "9461\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [58570 30429]\n",
      "level_dim [58570 30429]\n",
      "name 9461\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9461.h5: total of 26 batches\n",
      "batch 0/26, 0 files processed\n",
      "batch 20/26, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9461.h5 took 21.0977942943573 s\n",
      "features size:  (12861, 1024)\n",
      "coordinates size:  (12861, 2)\n",
      "\n",
      "progress: 519/876\n",
      "9462\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [72972 60129]\n",
      "level_dim [72972 60129]\n",
      "name 9462\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9462.h5: total of 61 batches\n",
      "batch 0/61, 0 files processed\n",
      "batch 20/61, 10240 files processed\n",
      "batch 40/61, 20480 files processed\n",
      "batch 60/61, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9462.h5 took 48.025009870529175 s\n",
      "features size:  (30891, 1024)\n",
      "coordinates size:  (30891, 2)\n",
      "\n",
      "progress: 520/876\n",
      "9463\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [103698  71686]\n",
      "level_dim [103698  71686]\n",
      "name 9463\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9463.h5: total of 104 batches\n",
      "batch 0/104, 0 files processed\n",
      "batch 20/104, 10240 files processed\n",
      "batch 40/104, 20480 files processed\n",
      "batch 60/104, 30720 files processed\n",
      "batch 80/104, 40960 files processed\n",
      "batch 100/104, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9463.h5 took 83.11890935897827 s\n",
      "features size:  (52906, 1024)\n",
      "coordinates size:  (52906, 2)\n",
      "\n",
      "progress: 521/876\n",
      "9464\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [91215 76019]\n",
      "level_dim [91215 76019]\n",
      "name 9464\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9464.h5: total of 67 batches\n",
      "batch 0/67, 0 files processed\n",
      "batch 20/67, 10240 files processed\n",
      "batch 40/67, 20480 files processed\n",
      "batch 60/67, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9464.h5 took 49.921250104904175 s\n",
      "features size:  (34118, 1024)\n",
      "coordinates size:  (34118, 2)\n",
      "\n",
      "progress: 522/876\n",
      "9465\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [82574 71656]\n",
      "level_dim [82574 71656]\n",
      "name 9465\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9465.h5: total of 92 batches\n",
      "batch 0/92, 0 files processed\n",
      "batch 20/92, 10240 files processed\n",
      "batch 40/92, 20480 files processed\n",
      "batch 60/92, 30720 files processed\n",
      "batch 80/92, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9465.h5 took 71.2907087802887 s\n",
      "features size:  (46671, 1024)\n",
      "coordinates size:  (46671, 2)\n",
      "\n",
      "progress: 523/876\n",
      "9466\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [84494 69357]\n",
      "level_dim [84494 69357]\n",
      "name 9466\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9466.h5: total of 101 batches\n",
      "batch 0/101, 0 files processed\n",
      "batch 20/101, 10240 files processed\n",
      "batch 40/101, 20480 files processed\n",
      "batch 60/101, 30720 files processed\n",
      "batch 80/101, 40960 files processed\n",
      "batch 100/101, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9466.h5 took 59.15607953071594 s\n",
      "features size:  (51610, 1024)\n",
      "coordinates size:  (51610, 2)\n",
      "\n",
      "progress: 524/876\n",
      "9467\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [84494 42492]\n",
      "level_dim [84494 42492]\n",
      "name 9467\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9467.h5: total of 33 batches\n",
      "batch 0/33, 0 files processed\n",
      "batch 20/33, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9467.h5 took 28.363561391830444 s\n",
      "features size:  (16557, 1024)\n",
      "coordinates size:  (16557, 2)\n",
      "\n",
      "progress: 525/876\n",
      "9468\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96016 62977]\n",
      "level_dim [96016 62977]\n",
      "name 9468\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9468.h5: total of 93 batches\n",
      "batch 0/93, 0 files processed\n",
      "batch 20/93, 10240 files processed\n",
      "batch 40/93, 20480 files processed\n",
      "batch 60/93, 30720 files processed\n",
      "batch 80/93, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9468.h5 took 77.89440846443176 s\n",
      "features size:  (47419, 1024)\n",
      "coordinates size:  (47419, 2)\n",
      "\n",
      "progress: 526/876\n",
      "9469\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [113299  78096]\n",
      "level_dim [113299  78096]\n",
      "name 9469\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9469.h5: total of 110 batches\n",
      "batch 0/110, 0 files processed\n",
      "batch 20/110, 10240 files processed\n",
      "batch 40/110, 20480 files processed\n",
      "batch 60/110, 30720 files processed\n",
      "batch 80/110, 40960 files processed\n",
      "batch 100/110, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9469.h5 took 95.42725729942322 s\n",
      "features size:  (56193, 1024)\n",
      "coordinates size:  (56193, 2)\n",
      "\n",
      "progress: 527/876\n",
      "9470\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [94096 76534]\n",
      "level_dim [94096 76534]\n",
      "name 9470\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9470.h5: total of 85 batches\n",
      "batch 0/85, 0 files processed\n",
      "batch 20/85, 10240 files processed\n",
      "batch 40/85, 20480 files processed\n",
      "batch 60/85, 30720 files processed\n",
      "batch 80/85, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9470.h5 took 62.065704107284546 s\n",
      "features size:  (43095, 1024)\n",
      "coordinates size:  (43095, 2)\n",
      "\n",
      "progress: 528/876\n",
      "9471\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [111379  77837]\n",
      "level_dim [111379  77837]\n",
      "name 9471\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9471.h5: total of 138 batches\n",
      "batch 0/138, 0 files processed\n",
      "batch 20/138, 10240 files processed\n",
      "batch 40/138, 20480 files processed\n",
      "batch 60/138, 30720 files processed\n",
      "batch 80/138, 40960 files processed\n",
      "batch 100/138, 51200 files processed\n",
      "batch 120/138, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9471.h5 took 113.56299591064453 s\n",
      "features size:  (70391, 1024)\n",
      "coordinates size:  (70391, 2)\n",
      "\n",
      "progress: 529/876\n",
      "9472\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [105618  72968]\n",
      "level_dim [105618  72968]\n",
      "name 9472\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9472.h5: total of 123 batches\n",
      "batch 0/123, 0 files processed\n",
      "batch 20/123, 10240 files processed\n",
      "batch 40/123, 20480 files processed\n",
      "batch 60/123, 30720 files processed\n",
      "batch 80/123, 40960 files processed\n",
      "batch 100/123, 51200 files processed\n",
      "batch 120/123, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9472.h5 took 114.18148827552795 s\n",
      "features size:  (62896, 1024)\n",
      "coordinates size:  (62896, 2)\n",
      "\n",
      "progress: 530/876\n",
      "9473\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [103698  81409]\n",
      "level_dim [103698  81409]\n",
      "name 9473\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9473.h5: total of 121 batches\n",
      "batch 0/121, 0 files processed\n",
      "batch 20/121, 10240 files processed\n",
      "batch 40/121, 20480 files processed\n",
      "batch 60/121, 30720 files processed\n",
      "batch 80/121, 40960 files processed\n",
      "batch 100/121, 51200 files processed\n",
      "batch 120/121, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9473.h5 took 94.22457838058472 s\n",
      "features size:  (61895, 1024)\n",
      "coordinates size:  (61895, 2)\n",
      "\n",
      "progress: 531/876\n",
      "9474\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [98897 61190]\n",
      "level_dim [98897 61190]\n",
      "name 9474\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9474.h5: total of 101 batches\n",
      "batch 0/101, 0 files processed\n",
      "batch 20/101, 10240 files processed\n",
      "batch 40/101, 20480 files processed\n",
      "batch 60/101, 30720 files processed\n",
      "batch 80/101, 40960 files processed\n",
      "batch 100/101, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9474.h5 took 84.25448250770569 s\n",
      "features size:  (51651, 1024)\n",
      "coordinates size:  (51651, 2)\n",
      "\n",
      "progress: 532/876\n",
      "9475\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96016 81910]\n",
      "level_dim [96016 81910]\n",
      "name 9475\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9475.h5: total of 146 batches\n",
      "batch 0/146, 0 files processed\n",
      "batch 20/146, 10240 files processed\n",
      "batch 40/146, 20480 files processed\n",
      "batch 60/146, 30720 files processed\n",
      "batch 80/146, 40960 files processed\n",
      "batch 100/146, 51200 files processed\n",
      "batch 120/146, 61440 files processed\n",
      "batch 140/146, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9475.h5 took 119.18191385269165 s\n",
      "features size:  (74420, 1024)\n",
      "coordinates size:  (74420, 2)\n",
      "\n",
      "progress: 533/876\n",
      "9480\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [93136 69369]\n",
      "level_dim [93136 69369]\n",
      "name 9480\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9480.h5: total of 118 batches\n",
      "batch 0/118, 0 files processed\n",
      "batch 20/118, 10240 files processed\n",
      "batch 40/118, 20480 files processed\n",
      "batch 60/118, 30720 files processed\n",
      "batch 80/118, 40960 files processed\n",
      "batch 100/118, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9480.h5 took 89.94763612747192 s\n",
      "features size:  (60289, 1024)\n",
      "coordinates size:  (60289, 2)\n",
      "\n",
      "progress: 534/876\n",
      "9481\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96976 70398]\n",
      "level_dim [96976 70398]\n",
      "name 9481\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9481.h5: total of 121 batches\n",
      "batch 0/121, 0 files processed\n",
      "batch 20/121, 10240 files processed\n",
      "batch 40/121, 20480 files processed\n",
      "batch 60/121, 30720 files processed\n",
      "batch 80/121, 40960 files processed\n",
      "batch 100/121, 51200 files processed\n",
      "batch 120/121, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9481.h5 took 95.65656852722168 s\n",
      "features size:  (61606, 1024)\n",
      "coordinates size:  (61606, 2)\n",
      "\n",
      "progress: 535/876\n",
      "9482\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [103698  75524]\n",
      "level_dim [103698  75524]\n",
      "name 9482\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9482.h5: total of 126 batches\n",
      "batch 0/126, 0 files processed\n",
      "batch 20/126, 10240 files processed\n",
      "batch 40/126, 20480 files processed\n",
      "batch 60/126, 30720 files processed\n",
      "batch 80/126, 40960 files processed\n",
      "batch 100/126, 51200 files processed\n",
      "batch 120/126, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9482.h5 took 102.89711284637451 s\n",
      "features size:  (64265, 1024)\n",
      "coordinates size:  (64265, 2)\n",
      "\n",
      "progress: 536/876\n",
      "9483\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [97937 73469]\n",
      "level_dim [97937 73469]\n",
      "name 9483\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9483.h5: total of 128 batches\n",
      "batch 0/128, 0 files processed\n",
      "batch 20/128, 10240 files processed\n",
      "batch 40/128, 20480 files processed\n",
      "batch 60/128, 30720 files processed\n",
      "batch 80/128, 40960 files processed\n",
      "batch 100/128, 51200 files processed\n",
      "batch 120/128, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9483.h5 took 109.35200500488281 s\n",
      "features size:  (65456, 1024)\n",
      "coordinates size:  (65456, 2)\n",
      "\n",
      "progress: 537/876\n",
      "9484\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [77773 74976]\n",
      "level_dim [77773 74976]\n",
      "name 9484\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9484.h5: total of 97 batches\n",
      "batch 0/97, 0 files processed\n",
      "batch 20/97, 10240 files processed\n",
      "batch 40/97, 20480 files processed\n",
      "batch 60/97, 30720 files processed\n",
      "batch 80/97, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9484.h5 took 67.23437547683716 s\n",
      "features size:  (49625, 1024)\n",
      "coordinates size:  (49625, 2)\n",
      "\n",
      "progress: 538/876\n",
      "9485\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [79693 57580]\n",
      "level_dim [79693 57580]\n",
      "name 9485\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9485.h5: total of 63 batches\n",
      "batch 0/63, 0 files processed\n",
      "batch 20/63, 10240 files processed\n",
      "batch 40/63, 20480 files processed\n",
      "batch 60/63, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9485.h5 took 53.857120990753174 s\n",
      "features size:  (32023, 1024)\n",
      "coordinates size:  (32023, 2)\n",
      "\n",
      "progress: 539/876\n",
      "9486\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [113299  79375]\n",
      "level_dim [113299  79375]\n",
      "name 9486\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9486.h5: total of 107 batches\n",
      "batch 0/107, 0 files processed\n",
      "batch 20/107, 10240 files processed\n",
      "batch 40/107, 20480 files processed\n",
      "batch 60/107, 30720 files processed\n",
      "batch 80/107, 40960 files processed\n",
      "batch 100/107, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9486.h5 took 77.66981101036072 s\n",
      "features size:  (54577, 1024)\n",
      "coordinates size:  (54577, 2)\n",
      "\n",
      "progress: 540/876\n",
      "9487\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [83534 66285]\n",
      "level_dim [83534 66285]\n",
      "name 9487\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9487.h5: total of 74 batches\n",
      "batch 0/74, 0 files processed\n",
      "batch 20/74, 10240 files processed\n",
      "batch 40/74, 20480 files processed\n",
      "batch 60/74, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9487.h5 took 66.03224897384644 s\n",
      "features size:  (37558, 1024)\n",
      "coordinates size:  (37558, 2)\n",
      "\n",
      "progress: 541/876\n",
      "9488\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [110419  68882]\n",
      "level_dim [110419  68882]\n",
      "name 9488\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9488.h5: total of 127 batches\n",
      "batch 0/127, 0 files processed\n",
      "batch 20/127, 10240 files processed\n",
      "batch 40/127, 20480 files processed\n",
      "batch 60/127, 30720 files processed\n",
      "batch 80/127, 40960 files processed\n",
      "batch 100/127, 51200 files processed\n",
      "batch 120/127, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9488.h5 took 97.00398302078247 s\n",
      "features size:  (64661, 1024)\n",
      "coordinates size:  (64661, 2)\n",
      "\n",
      "progress: 542/876\n",
      "9489\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [106578  67085]\n",
      "level_dim [106578  67085]\n",
      "name 9489\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9489.h5: total of 96 batches\n",
      "batch 0/96, 0 files processed\n",
      "batch 20/96, 10240 files processed\n",
      "batch 40/96, 20480 files processed\n",
      "batch 60/96, 30720 files processed\n",
      "batch 80/96, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9489.h5 took 74.00337266921997 s\n",
      "features size:  (48865, 1024)\n",
      "coordinates size:  (48865, 2)\n",
      "\n",
      "progress: 543/876\n",
      "9490\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [93136 74486]\n",
      "level_dim [93136 74486]\n",
      "name 9490\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9490.h5: total of 110 batches\n",
      "batch 0/110, 0 files processed\n",
      "batch 20/110, 10240 files processed\n",
      "batch 40/110, 20480 files processed\n",
      "batch 60/110, 30720 files processed\n",
      "batch 80/110, 40960 files processed\n",
      "batch 100/110, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9490.h5 took 98.23023867607117 s\n",
      "features size:  (56119, 1024)\n",
      "coordinates size:  (56119, 2)\n",
      "\n",
      "progress: 544/876\n",
      "9491\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [65291 67283]\n",
      "level_dim [65291 67283]\n",
      "name 9491\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9491.h5: total of 78 batches\n",
      "batch 0/78, 0 files processed\n",
      "batch 20/78, 10240 files processed\n",
      "batch 40/78, 20480 files processed\n",
      "batch 60/78, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9491.h5 took 71.2795763015747 s\n",
      "features size:  (39747, 1024)\n",
      "coordinates size:  (39747, 2)\n",
      "\n",
      "progress: 545/876\n",
      "9495\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [99857 77054]\n",
      "level_dim [99857 77054]\n",
      "name 9495\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9495.h5: total of 142 batches\n",
      "batch 0/142, 0 files processed\n",
      "batch 20/142, 10240 files processed\n",
      "batch 40/142, 20480 files processed\n",
      "batch 60/142, 30720 files processed\n",
      "batch 80/142, 40960 files processed\n",
      "batch 100/142, 51200 files processed\n",
      "batch 120/142, 61440 files processed\n",
      "batch 140/142, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9495.h5 took 105.75830292701721 s\n",
      "features size:  (72396, 1024)\n",
      "coordinates size:  (72396, 2)\n",
      "\n",
      "progress: 546/876\n",
      "9496\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [103698  85759]\n",
      "level_dim [103698  85759]\n",
      "name 9496\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9496.h5: total of 153 batches\n",
      "batch 0/153, 0 files processed\n",
      "batch 20/153, 10240 files processed\n",
      "batch 40/153, 20480 files processed\n",
      "batch 60/153, 30720 files processed\n",
      "batch 80/153, 40960 files processed\n",
      "batch 100/153, 51200 files processed\n",
      "batch 120/153, 61440 files processed\n",
      "batch 140/153, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9496.h5 took 132.4431676864624 s\n",
      "features size:  (78012, 1024)\n",
      "coordinates size:  (78012, 2)\n",
      "\n",
      "progress: 547/876\n",
      "9497\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [98897 83449]\n",
      "level_dim [98897 83449]\n",
      "name 9497\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9497.h5: total of 159 batches\n",
      "batch 0/159, 0 files processed\n",
      "batch 20/159, 10240 files processed\n",
      "batch 40/159, 20480 files processed\n",
      "batch 60/159, 30720 files processed\n",
      "batch 80/159, 40960 files processed\n",
      "batch 100/159, 51200 files processed\n",
      "batch 120/159, 61440 files processed\n",
      "batch 140/159, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9497.h5 took 143.79198026657104 s\n",
      "features size:  (81096, 1024)\n",
      "coordinates size:  (81096, 2)\n",
      "\n",
      "progress: 548/876\n",
      "9498\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [111379  61462]\n",
      "level_dim [111379  61462]\n",
      "name 9498\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9498.h5: total of 103 batches\n",
      "batch 0/103, 0 files processed\n",
      "batch 20/103, 10240 files processed\n",
      "batch 40/103, 20480 files processed\n",
      "batch 60/103, 30720 files processed\n",
      "batch 80/103, 40960 files processed\n",
      "batch 100/103, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9498.h5 took 80.30356812477112 s\n",
      "features size:  (52293, 1024)\n",
      "coordinates size:  (52293, 2)\n",
      "\n",
      "progress: 549/876\n",
      "9499\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [101777  85757]\n",
      "level_dim [101777  85757]\n",
      "name 9499\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9499.h5: total of 177 batches\n",
      "batch 0/177, 0 files processed\n",
      "batch 20/177, 10240 files processed\n",
      "batch 40/177, 20480 files processed\n",
      "batch 60/177, 30720 files processed\n",
      "batch 80/177, 40960 files processed\n",
      "batch 100/177, 51200 files processed\n",
      "batch 120/177, 61440 files processed\n",
      "batch 140/177, 71680 files processed\n",
      "batch 160/177, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9499.h5 took 159.93427872657776 s\n",
      "features size:  (90142, 1024)\n",
      "coordinates size:  (90142, 2)\n",
      "\n",
      "progress: 550/876\n",
      "9500\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96976 79865]\n",
      "level_dim [96976 79865]\n",
      "name 9500\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9500.h5: total of 157 batches\n",
      "batch 0/157, 0 files processed\n",
      "batch 20/157, 10240 files processed\n",
      "batch 40/157, 20480 files processed\n",
      "batch 60/157, 30720 files processed\n",
      "batch 80/157, 40960 files processed\n",
      "batch 100/157, 51200 files processed\n",
      "batch 120/157, 61440 files processed\n",
      "batch 140/157, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9500.h5 took 146.0287528038025 s\n",
      "features size:  (80006, 1024)\n",
      "coordinates size:  (80006, 2)\n",
      "\n",
      "progress: 551/876\n",
      "9527\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [117140  67612]\n",
      "level_dim [117140  67612]\n",
      "name 9527\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9527.h5: total of 146 batches\n",
      "batch 0/146, 0 files processed\n",
      "batch 20/146, 10240 files processed\n",
      "batch 40/146, 20480 files processed\n",
      "batch 60/146, 30720 files processed\n",
      "batch 80/146, 40960 files processed\n",
      "batch 100/146, 51200 files processed\n",
      "batch 120/146, 61440 files processed\n",
      "batch 140/146, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9527.h5 took 86.14844393730164 s\n",
      "features size:  (74528, 1024)\n",
      "coordinates size:  (74528, 2)\n",
      "\n",
      "progress: 552/876\n",
      "9528\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [97937 85750]\n",
      "level_dim [97937 85750]\n",
      "name 9528\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9528.h5: total of 157 batches\n",
      "batch 0/157, 0 files processed\n",
      "batch 20/157, 10240 files processed\n",
      "batch 40/157, 20480 files processed\n",
      "batch 60/157, 30720 files processed\n",
      "batch 80/157, 40960 files processed\n",
      "batch 100/157, 51200 files processed\n",
      "batch 120/157, 61440 files processed\n",
      "batch 140/157, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9528.h5 took 138.71340441703796 s\n",
      "features size:  (79911, 1024)\n",
      "coordinates size:  (79911, 2)\n",
      "\n",
      "progress: 553/876\n",
      "9529\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [105618  81667]\n",
      "level_dim [105618  81667]\n",
      "name 9529\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9529.h5: total of 148 batches\n",
      "batch 0/148, 0 files processed\n",
      "batch 20/148, 10240 files processed\n",
      "batch 40/148, 20480 files processed\n",
      "batch 60/148, 30720 files processed\n",
      "batch 80/148, 40960 files processed\n",
      "batch 100/148, 51200 files processed\n",
      "batch 120/148, 61440 files processed\n",
      "batch 140/148, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9529.h5 took 132.95615124702454 s\n",
      "features size:  (75702, 1024)\n",
      "coordinates size:  (75702, 2)\n",
      "\n",
      "progress: 554/876\n",
      "9530\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [142104  85813]\n",
      "level_dim [142104  85813]\n",
      "name 9530\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9530.h5: total of 210 batches\n",
      "batch 0/210, 0 files processed\n",
      "batch 20/210, 10240 files processed\n",
      "batch 40/210, 20480 files processed\n",
      "batch 60/210, 30720 files processed\n",
      "batch 80/210, 40960 files processed\n",
      "batch 100/210, 51200 files processed\n",
      "batch 120/210, 61440 files processed\n",
      "batch 140/210, 71680 files processed\n",
      "batch 160/210, 81920 files processed\n",
      "batch 180/210, 92160 files processed\n",
      "batch 200/210, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9530.h5 took 178.64615273475647 s\n",
      "features size:  (107386, 1024)\n",
      "coordinates size:  (107386, 2)\n",
      "\n",
      "progress: 555/876\n",
      "9531\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [108498  85766]\n",
      "level_dim [108498  85766]\n",
      "name 9531\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9531.h5: total of 161 batches\n",
      "batch 0/161, 0 files processed\n",
      "batch 20/161, 10240 files processed\n",
      "batch 40/161, 20480 files processed\n",
      "batch 60/161, 30720 files processed\n",
      "batch 80/161, 40960 files processed\n",
      "batch 100/161, 51200 files processed\n",
      "batch 120/161, 61440 files processed\n",
      "batch 140/161, 71680 files processed\n",
      "batch 160/161, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9531.h5 took 138.65179324150085 s\n",
      "features size:  (82088, 1024)\n",
      "coordinates size:  (82088, 2)\n",
      "\n",
      "progress: 556/876\n",
      "9532\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [143064  85814]\n",
      "level_dim [143064  85814]\n",
      "name 9532\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9532.h5: total of 216 batches\n",
      "batch 0/216, 0 files processed\n",
      "batch 20/216, 10240 files processed\n",
      "batch 40/216, 20480 files processed\n",
      "batch 60/216, 30720 files processed\n",
      "batch 80/216, 40960 files processed\n",
      "batch 100/216, 51200 files processed\n",
      "batch 120/216, 61440 files processed\n",
      "batch 140/216, 71680 files processed\n",
      "batch 160/216, 81920 files processed\n",
      "batch 180/216, 92160 files processed\n",
      "batch 200/216, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9532.h5 took 189.7009675502777 s\n",
      "features size:  (110525, 1024)\n",
      "coordinates size:  (110525, 2)\n",
      "\n",
      "progress: 557/876\n",
      "9533\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [107538  83717]\n",
      "level_dim [107538  83717]\n",
      "name 9533\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9533.h5: total of 174 batches\n",
      "batch 0/174, 0 files processed\n",
      "batch 20/174, 10240 files processed\n",
      "batch 40/174, 20480 files processed\n",
      "batch 60/174, 30720 files processed\n",
      "batch 80/174, 40960 files processed\n",
      "batch 100/174, 51200 files processed\n",
      "batch 120/174, 61440 files processed\n",
      "batch 140/174, 71680 files processed\n",
      "batch 160/174, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9533.h5 took 130.35594058036804 s\n",
      "features size:  (88910, 1024)\n",
      "coordinates size:  (88910, 2)\n",
      "\n",
      "progress: 558/876\n",
      "9534\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [123861  85787]\n",
      "level_dim [123861  85787]\n",
      "name 9534\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9534.h5: total of 157 batches\n",
      "batch 0/157, 0 files processed\n",
      "batch 20/157, 10240 files processed\n",
      "batch 40/157, 20480 files processed\n",
      "batch 60/157, 30720 files processed\n",
      "batch 80/157, 40960 files processed\n",
      "batch 100/157, 51200 files processed\n",
      "batch 120/157, 61440 files processed\n",
      "batch 140/157, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9534.h5 took 126.20296573638916 s\n",
      "features size:  (79892, 1024)\n",
      "coordinates size:  (79892, 2)\n",
      "\n",
      "progress: 559/876\n",
      "9535\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [97937 80634]\n",
      "level_dim [97937 80634]\n",
      "name 9535\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9535.h5: total of 122 batches\n",
      "batch 0/122, 0 files processed\n",
      "batch 20/122, 10240 files processed\n",
      "batch 40/122, 20480 files processed\n",
      "batch 60/122, 30720 files processed\n",
      "batch 80/122, 40960 files processed\n",
      "batch 100/122, 51200 files processed\n",
      "batch 120/122, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9535.h5 took 94.63551449775696 s\n",
      "features size:  (61954, 1024)\n",
      "coordinates size:  (61954, 2)\n",
      "\n",
      "progress: 560/876\n",
      "9539\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [81614 67306]\n",
      "level_dim [81614 67306]\n",
      "name 9539\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9539.h5: total of 76 batches\n",
      "batch 0/76, 0 files processed\n",
      "batch 20/76, 10240 files processed\n",
      "batch 40/76, 20480 files processed\n",
      "batch 60/76, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9539.h5 took 62.45751643180847 s\n",
      "features size:  (38411, 1024)\n",
      "coordinates size:  (38411, 2)\n",
      "\n",
      "progress: 561/876\n",
      "9540\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [85454 60915]\n",
      "level_dim [85454 60915]\n",
      "name 9540\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9540.h5: total of 67 batches\n",
      "batch 0/67, 0 files processed\n",
      "batch 20/67, 10240 files processed\n",
      "batch 40/67, 20480 files processed\n",
      "batch 60/67, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9540.h5 took 54.484219551086426 s\n",
      "features size:  (33979, 1024)\n",
      "coordinates size:  (33979, 2)\n",
      "\n",
      "progress: 562/876\n",
      "9541\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [107538  59155]\n",
      "level_dim [107538  59155]\n",
      "name 9541\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9541.h5: total of 100 batches\n",
      "batch 0/100, 0 files processed\n",
      "batch 20/100, 10240 files processed\n",
      "batch 40/100, 20480 files processed\n",
      "batch 60/100, 30720 files processed\n",
      "batch 80/100, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9541.h5 took 84.75099349021912 s\n",
      "features size:  (51061, 1024)\n",
      "coordinates size:  (51061, 2)\n",
      "\n",
      "progress: 563/876\n",
      "9542\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [58570 58318]\n",
      "level_dim [58570 58318]\n",
      "name 9542\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9542.h5: total of 51 batches\n",
      "batch 0/51, 0 files processed\n",
      "batch 20/51, 10240 files processed\n",
      "batch 40/51, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9542.h5 took 42.50823736190796 s\n",
      "features size:  (26042, 1024)\n",
      "coordinates size:  (26042, 2)\n",
      "\n",
      "progress: 564/876\n",
      "9543\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [124821  74275]\n",
      "level_dim [124821  74275]\n",
      "name 9543\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9543.h5: total of 136 batches\n",
      "batch 0/136, 0 files processed\n",
      "batch 20/136, 10240 files processed\n",
      "batch 40/136, 20480 files processed\n",
      "batch 60/136, 30720 files processed\n",
      "batch 80/136, 40960 files processed\n",
      "batch 100/136, 51200 files processed\n",
      "batch 120/136, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9543.h5 took 112.38572382926941 s\n",
      "features size:  (69308, 1024)\n",
      "coordinates size:  (69308, 2)\n",
      "\n",
      "progress: 565/876\n",
      "9544\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [105618  85761]\n",
      "level_dim [105618  85761]\n",
      "name 9544\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9544.h5: total of 125 batches\n",
      "batch 0/125, 0 files processed\n",
      "batch 20/125, 10240 files processed\n",
      "batch 40/125, 20480 files processed\n",
      "batch 60/125, 30720 files processed\n",
      "batch 80/125, 40960 files processed\n",
      "batch 100/125, 51200 files processed\n",
      "batch 120/125, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9544.h5 took 101.87734866142273 s\n",
      "features size:  (63702, 1024)\n",
      "coordinates size:  (63702, 2)\n",
      "\n",
      "progress: 566/876\n",
      "9546\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [77773 71906]\n",
      "level_dim [77773 71906]\n",
      "name 9546\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9546.h5: total of 87 batches\n",
      "batch 0/87, 0 files processed\n",
      "batch 20/87, 10240 files processed\n",
      "batch 40/87, 20480 files processed\n",
      "batch 60/87, 30720 files processed\n",
      "batch 80/87, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9546.h5 took 71.82440710067749 s\n",
      "features size:  (44288, 1024)\n",
      "coordinates size:  (44288, 2)\n",
      "\n",
      "progress: 567/876\n",
      "9547\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [97937 71678]\n",
      "level_dim [97937 71678]\n",
      "name 9547\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9547.h5: total of 116 batches\n",
      "batch 0/116, 0 files processed\n",
      "batch 20/116, 10240 files processed\n",
      "batch 40/116, 20480 files processed\n",
      "batch 60/116, 30720 files processed\n",
      "batch 80/116, 40960 files processed\n",
      "batch 100/116, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9547.h5 took 89.39505672454834 s\n",
      "features size:  (59063, 1024)\n",
      "coordinates size:  (59063, 2)\n",
      "\n",
      "progress: 568/876\n",
      "9548\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [75853 74461]\n",
      "level_dim [75853 74461]\n",
      "name 9548\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9548.h5: total of 89 batches\n",
      "batch 0/89, 0 files processed\n",
      "batch 20/89, 10240 files processed\n",
      "batch 40/89, 20480 files processed\n",
      "batch 60/89, 30720 files processed\n",
      "batch 80/89, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9548.h5 took 72.90360283851624 s\n",
      "features size:  (45372, 1024)\n",
      "coordinates size:  (45372, 2)\n",
      "\n",
      "progress: 569/876\n",
      "9549\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [102737  85757]\n",
      "level_dim [102737  85757]\n",
      "name 9549\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9549.h5: total of 166 batches\n",
      "batch 0/166, 0 files processed\n",
      "batch 20/166, 10240 files processed\n",
      "batch 40/166, 20480 files processed\n",
      "batch 60/166, 30720 files processed\n",
      "batch 80/166, 40960 files processed\n",
      "batch 100/166, 51200 files processed\n",
      "batch 120/166, 61440 files processed\n",
      "batch 140/166, 71680 files processed\n",
      "batch 160/166, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9549.h5 took 143.65380954742432 s\n",
      "features size:  (84841, 1024)\n",
      "coordinates size:  (84841, 2)\n",
      "\n",
      "progress: 570/876\n",
      "9550\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [110419  80651]\n",
      "level_dim [110419  80651]\n",
      "name 9550\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9550.h5: total of 165 batches\n",
      "batch 0/165, 0 files processed\n",
      "batch 20/165, 10240 files processed\n",
      "batch 40/165, 20480 files processed\n",
      "batch 60/165, 30720 files processed\n",
      "batch 80/165, 40960 files processed\n",
      "batch 100/165, 51200 files processed\n",
      "batch 120/165, 61440 files processed\n",
      "batch 140/165, 71680 files processed\n",
      "batch 160/165, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9550.h5 took 143.56451869010925 s\n",
      "features size:  (84462, 1024)\n",
      "coordinates size:  (84462, 2)\n",
      "\n",
      "progress: 571/876\n",
      "9551\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [83534 63470]\n",
      "level_dim [83534 63470]\n",
      "name 9551\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9551.h5: total of 66 batches\n",
      "batch 0/66, 0 files processed\n",
      "batch 20/66, 10240 files processed\n",
      "batch 40/66, 20480 files processed\n",
      "batch 60/66, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9551.h5 took 50.30119204521179 s\n",
      "features size:  (33329, 1024)\n",
      "coordinates size:  (33329, 2)\n",
      "\n",
      "progress: 572/876\n",
      "9659\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [76813 80604]\n",
      "level_dim [76813 80604]\n",
      "name 9659\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9659.h5: total of 54 batches\n",
      "batch 0/54, 0 files processed\n",
      "batch 20/54, 10240 files processed\n",
      "batch 40/54, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9659.h5 took 39.05685353279114 s\n",
      "features size:  (27562, 1024)\n",
      "coordinates size:  (27562, 2)\n",
      "\n",
      "progress: 573/876\n",
      "9660\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [65291 60631]\n",
      "level_dim [65291 60631]\n",
      "name 9660\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9660.h5: total of 58 batches\n",
      "batch 0/58, 0 files processed\n",
      "batch 20/58, 10240 files processed\n",
      "batch 40/58, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9660.h5 took 46.051334619522095 s\n",
      "features size:  (29692, 1024)\n",
      "coordinates size:  (29692, 2)\n",
      "\n",
      "progress: 574/876\n",
      "9661\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [90255 81902]\n",
      "level_dim [90255 81902]\n",
      "name 9661\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9661.h5: total of 76 batches\n",
      "batch 0/76, 0 files processed\n",
      "batch 20/76, 10240 files processed\n",
      "batch 40/76, 20480 files processed\n",
      "batch 60/76, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9661.h5 took 58.82313346862793 s\n",
      "features size:  (38859, 1024)\n",
      "coordinates size:  (38859, 2)\n",
      "\n",
      "progress: 575/876\n",
      "9664\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [74893 58085]\n",
      "level_dim [74893 58085]\n",
      "name 9664\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9664.h5: total of 51 batches\n",
      "batch 0/51, 0 files processed\n",
      "batch 20/51, 10240 files processed\n",
      "batch 40/51, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9664.h5 took 41.81778025627136 s\n",
      "features size:  (25751, 1024)\n",
      "coordinates size:  (25751, 2)\n",
      "\n",
      "progress: 576/876\n",
      "9665\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [72012 54243]\n",
      "level_dim [72012 54243]\n",
      "name 9665\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9665.h5: total of 50 batches\n",
      "batch 0/50, 0 files processed\n",
      "batch 20/50, 10240 files processed\n",
      "batch 40/50, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9665.h5 took 38.85839653015137 s\n",
      "features size:  (25292, 1024)\n",
      "coordinates size:  (25292, 2)\n",
      "\n",
      "progress: 577/876\n",
      "9669\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [127702  69673]\n",
      "level_dim [127702  69673]\n",
      "name 9669\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9669.h5: total of 57 batches\n",
      "batch 0/57, 0 files processed\n",
      "batch 20/57, 10240 files processed\n",
      "batch 40/57, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9669.h5 took 41.02003359794617 s\n",
      "features size:  (28990, 1024)\n",
      "coordinates size:  (28990, 2)\n",
      "\n",
      "progress: 578/876\n",
      "9670\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [88335 72944]\n",
      "level_dim [88335 72944]\n",
      "name 9670\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9670.h5: total of 105 batches\n",
      "batch 0/105, 0 files processed\n",
      "batch 20/105, 10240 files processed\n",
      "batch 40/105, 20480 files processed\n",
      "batch 60/105, 30720 files processed\n",
      "batch 80/105, 40960 files processed\n",
      "batch 100/105, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9670.h5 took 88.22274470329285 s\n",
      "features size:  (53445, 1024)\n",
      "coordinates size:  (53445, 2)\n",
      "\n",
      "progress: 579/876\n",
      "9671\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [97937 85750]\n",
      "level_dim [97937 85750]\n",
      "name 9671\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9671.h5: total of 161 batches\n",
      "batch 0/161, 0 files processed\n",
      "batch 20/161, 10240 files processed\n",
      "batch 40/161, 20480 files processed\n",
      "batch 60/161, 30720 files processed\n",
      "batch 80/161, 40960 files processed\n",
      "batch 100/161, 51200 files processed\n",
      "batch 120/161, 61440 files processed\n",
      "batch 140/161, 71680 files processed\n",
      "batch 160/161, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9671.h5 took 135.26906895637512 s\n",
      "features size:  (82131, 1024)\n",
      "coordinates size:  (82131, 2)\n",
      "\n",
      "progress: 580/876\n",
      "9674\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [92176 78834]\n",
      "level_dim [92176 78834]\n",
      "name 9674\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9674.h5: total of 98 batches\n",
      "batch 0/98, 0 files processed\n",
      "batch 20/98, 10240 files processed\n",
      "batch 40/98, 20480 files processed\n",
      "batch 60/98, 30720 files processed\n",
      "batch 80/98, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9674.h5 took 83.75937104225159 s\n",
      "features size:  (50086, 1024)\n",
      "coordinates size:  (50086, 2)\n",
      "\n",
      "progress: 581/876\n",
      "9675\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [106578  70155]\n",
      "level_dim [106578  70155]\n",
      "name 9675\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9675.h5: total of 105 batches\n",
      "batch 0/105, 0 files processed\n",
      "batch 20/105, 10240 files processed\n",
      "batch 40/105, 20480 files processed\n",
      "batch 60/105, 30720 files processed\n",
      "batch 80/105, 40960 files processed\n",
      "batch 100/105, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9675.h5 took 80.15047717094421 s\n",
      "features size:  (53686, 1024)\n",
      "coordinates size:  (53686, 2)\n",
      "\n",
      "progress: 582/876\n",
      "9676\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [106578  71946]\n",
      "level_dim [106578  71946]\n",
      "name 9676\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9676.h5: total of 103 batches\n",
      "batch 0/103, 0 files processed\n",
      "batch 20/103, 10240 files processed\n",
      "batch 40/103, 20480 files processed\n",
      "batch 60/103, 30720 files processed\n",
      "batch 80/103, 40960 files processed\n",
      "batch 100/103, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9676.h5 took 91.98420596122742 s\n",
      "features size:  (52631, 1024)\n",
      "coordinates size:  (52631, 2)\n",
      "\n",
      "progress: 583/876\n",
      "9677\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [32645 28858]\n",
      "level_dim [32645 28858]\n",
      "name 9677\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9677.h5: total of 14 batches\n",
      "batch 0/14, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9677.h5 took 11.544133424758911 s\n",
      "features size:  (7085, 1024)\n",
      "coordinates size:  (7085, 2)\n",
      "\n",
      "progress: 584/876\n",
      "9678\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [79693 59116]\n",
      "level_dim [79693 59116]\n",
      "name 9678\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9678.h5: total of 37 batches\n",
      "batch 0/37, 0 files processed\n",
      "batch 20/37, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9678.h5 took 25.13798236846924 s\n",
      "features size:  (18622, 1024)\n",
      "coordinates size:  (18622, 2)\n",
      "\n",
      "progress: 585/876\n",
      "9679\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [119060  69405]\n",
      "level_dim [119060  69405]\n",
      "name 9679\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9679.h5: total of 113 batches\n",
      "batch 0/113, 0 files processed\n",
      "batch 20/113, 10240 files processed\n",
      "batch 40/113, 20480 files processed\n",
      "batch 60/113, 30720 files processed\n",
      "batch 80/113, 40960 files processed\n",
      "batch 100/113, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9679.h5 took 95.71367692947388 s\n",
      "features size:  (57748, 1024)\n",
      "coordinates size:  (57748, 2)\n",
      "\n",
      "progress: 586/876\n",
      "9680\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [112339  67860]\n",
      "level_dim [112339  67860]\n",
      "name 9680\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9680.h5: total of 111 batches\n",
      "batch 0/111, 0 files processed\n",
      "batch 20/111, 10240 files processed\n",
      "batch 40/111, 20480 files processed\n",
      "batch 60/111, 30720 files processed\n",
      "batch 80/111, 40960 files processed\n",
      "batch 100/111, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9680.h5 took 88.38089418411255 s\n",
      "features size:  (56650, 1024)\n",
      "coordinates size:  (56650, 2)\n",
      "\n",
      "progress: 587/876\n",
      "9681\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [125781  69670]\n",
      "level_dim [125781  69670]\n",
      "name 9681\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9681.h5: total of 113 batches\n",
      "batch 0/113, 0 files processed\n",
      "batch 20/113, 10240 files processed\n",
      "batch 40/113, 20480 files processed\n",
      "batch 60/113, 30720 files processed\n",
      "batch 80/113, 40960 files processed\n",
      "batch 100/113, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9681.h5 took 101.18618297576904 s\n",
      "features size:  (57846, 1024)\n",
      "coordinates size:  (57846, 2)\n",
      "\n",
      "progress: 588/876\n",
      "9682\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [120981  75804]\n",
      "level_dim [120981  75804]\n",
      "name 9682\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9682.h5: total of 128 batches\n",
      "batch 0/128, 0 files processed\n",
      "batch 20/128, 10240 files processed\n",
      "batch 40/128, 20480 files processed\n",
      "batch 60/128, 30720 files processed\n",
      "batch 80/128, 40960 files processed\n",
      "batch 100/128, 51200 files processed\n",
      "batch 120/128, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9682.h5 took 116.75442814826965 s\n",
      "features size:  (65282, 1024)\n",
      "coordinates size:  (65282, 2)\n",
      "\n",
      "progress: 589/876\n",
      "9683\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [126742  85792]\n",
      "level_dim [126742  85792]\n",
      "name 9683\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9683.h5: total of 214 batches\n",
      "batch 0/214, 0 files processed\n",
      "batch 20/214, 10240 files processed\n",
      "batch 40/214, 20480 files processed\n",
      "batch 60/214, 30720 files processed\n",
      "batch 80/214, 40960 files processed\n",
      "batch 100/214, 51200 files processed\n",
      "batch 120/214, 61440 files processed\n",
      "batch 140/214, 71680 files processed\n",
      "batch 160/214, 81920 files processed\n",
      "batch 180/214, 92160 files processed\n",
      "batch 200/214, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9683.h5 took 186.12646508216858 s\n",
      "features size:  (109224, 1024)\n",
      "coordinates size:  (109224, 2)\n",
      "\n",
      "progress: 590/876\n",
      "9684\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [122901  85786]\n",
      "level_dim [122901  85786]\n",
      "name 9684\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9684.h5: total of 243 batches\n",
      "batch 0/243, 0 files processed\n",
      "batch 20/243, 10240 files processed\n",
      "batch 40/243, 20480 files processed\n",
      "batch 60/243, 30720 files processed\n",
      "batch 80/243, 40960 files processed\n",
      "batch 100/243, 51200 files processed\n",
      "batch 120/243, 61440 files processed\n",
      "batch 140/243, 71680 files processed\n",
      "batch 160/243, 81920 files processed\n",
      "batch 180/243, 92160 files processed\n",
      "batch 200/243, 102400 files processed\n",
      "batch 220/243, 112640 files processed\n",
      "batch 240/243, 122880 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9684.h5 took 210.92375874519348 s\n",
      "features size:  (124076, 1024)\n",
      "coordinates size:  (124076, 2)\n",
      "\n",
      "progress: 591/876\n",
      "9685\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [133463  72495]\n",
      "level_dim [133463  72495]\n",
      "name 9685\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9685.h5: total of 151 batches\n",
      "batch 0/151, 0 files processed\n",
      "batch 20/151, 10240 files processed\n",
      "batch 40/151, 20480 files processed\n",
      "batch 60/151, 30720 files processed\n",
      "batch 80/151, 40960 files processed\n",
      "batch 100/151, 51200 files processed\n",
      "batch 120/151, 61440 files processed\n",
      "batch 140/151, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9685.h5 took 116.85154986381531 s\n",
      "features size:  (77117, 1024)\n",
      "coordinates size:  (77117, 2)\n",
      "\n",
      "progress: 592/876\n",
      "9686\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96976 81400]\n",
      "level_dim [96976 81400]\n",
      "name 9686\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9686.h5: total of 80 batches\n",
      "batch 0/80, 0 files processed\n",
      "batch 20/80, 10240 files processed\n",
      "batch 40/80, 20480 files processed\n",
      "batch 60/80, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9686.h5 took 60.5268349647522 s\n",
      "features size:  (40708, 1024)\n",
      "coordinates size:  (40708, 2)\n",
      "\n",
      "progress: 593/876\n",
      "9687\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [126742  85791]\n",
      "level_dim [126742  85791]\n",
      "name 9687\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9687.h5: total of 198 batches\n",
      "batch 0/198, 0 files processed\n",
      "batch 20/198, 10240 files processed\n",
      "batch 40/198, 20480 files processed\n",
      "batch 60/198, 30720 files processed\n",
      "batch 80/198, 40960 files processed\n",
      "batch 100/198, 51200 files processed\n",
      "batch 120/198, 61440 files processed\n",
      "batch 140/198, 71680 files processed\n",
      "batch 160/198, 81920 files processed\n",
      "batch 180/198, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9687.h5 took 178.78931331634521 s\n",
      "features size:  (101174, 1024)\n",
      "coordinates size:  (101174, 2)\n",
      "\n",
      "progress: 594/876\n",
      "9688\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [125781  83487]\n",
      "level_dim [125781  83487]\n",
      "name 9688\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9688.h5: total of 171 batches\n",
      "batch 0/171, 0 files processed\n",
      "batch 20/171, 10240 files processed\n",
      "batch 40/171, 20480 files processed\n",
      "batch 60/171, 30720 files processed\n",
      "batch 80/171, 40960 files processed\n",
      "batch 100/171, 51200 files processed\n",
      "batch 120/171, 61440 files processed\n",
      "batch 140/171, 71680 files processed\n",
      "batch 160/171, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9688.h5 took 152.1809844970703 s\n",
      "features size:  (87253, 1024)\n",
      "coordinates size:  (87253, 2)\n",
      "\n",
      "progress: 595/876\n",
      "9697\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [92176 76532]\n",
      "level_dim [92176 76532]\n",
      "name 9697\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9697.h5: total of 95 batches\n",
      "batch 0/95, 0 files processed\n",
      "batch 20/95, 10240 files processed\n",
      "batch 40/95, 20480 files processed\n",
      "batch 60/95, 30720 files processed\n",
      "batch 80/95, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9697.h5 took 54.713839292526245 s\n",
      "features size:  (48399, 1024)\n",
      "coordinates size:  (48399, 2)\n",
      "\n",
      "progress: 596/876\n",
      "9698\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [151706  85826]\n",
      "level_dim [151706  85826]\n",
      "name 9698\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9698.h5: total of 142 batches\n",
      "batch 0/142, 0 files processed\n",
      "batch 20/142, 10240 files processed\n",
      "batch 40/142, 20480 files processed\n",
      "batch 60/142, 30720 files processed\n",
      "batch 80/142, 40960 files processed\n",
      "batch 100/142, 51200 files processed\n",
      "batch 120/142, 61440 files processed\n",
      "batch 140/142, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9698.h5 took 117.36598587036133 s\n",
      "features size:  (72590, 1024)\n",
      "coordinates size:  (72590, 2)\n",
      "\n",
      "progress: 597/876\n",
      "9699\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [129622  85796]\n",
      "level_dim [129622  85796]\n",
      "name 9699\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9699.h5: total of 238 batches\n",
      "batch 0/238, 0 files processed\n",
      "batch 20/238, 10240 files processed\n",
      "batch 40/238, 20480 files processed\n",
      "batch 60/238, 30720 files processed\n",
      "batch 80/238, 40960 files processed\n",
      "batch 100/238, 51200 files processed\n",
      "batch 120/238, 61440 files processed\n",
      "batch 140/238, 71680 files processed\n",
      "batch 160/238, 81920 files processed\n",
      "batch 180/238, 92160 files processed\n",
      "batch 200/238, 102400 files processed\n",
      "batch 220/238, 112640 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9699.h5 took 147.36856293678284 s\n",
      "features size:  (121844, 1024)\n",
      "coordinates size:  (121844, 2)\n",
      "\n",
      "progress: 598/876\n",
      "9700\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [107538  75273]\n",
      "level_dim [107538  75273]\n",
      "name 9700\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9700.h5: total of 128 batches\n",
      "batch 0/128, 0 files processed\n",
      "batch 20/128, 10240 files processed\n",
      "batch 40/128, 20480 files processed\n",
      "batch 60/128, 30720 files processed\n",
      "batch 80/128, 40960 files processed\n",
      "batch 100/128, 51200 files processed\n",
      "batch 120/128, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9700.h5 took 90.51506638526917 s\n",
      "features size:  (65043, 1024)\n",
      "coordinates size:  (65043, 2)\n",
      "\n",
      "progress: 599/876\n",
      "9701\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [84494 81638]\n",
      "level_dim [84494 81638]\n",
      "name 9701\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9701.h5: total of 109 batches\n",
      "batch 0/109, 0 files processed\n",
      "batch 20/109, 10240 files processed\n",
      "batch 40/109, 20480 files processed\n",
      "batch 60/109, 30720 files processed\n",
      "batch 80/109, 40960 files processed\n",
      "batch 100/109, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9701.h5 took 82.45113277435303 s\n",
      "features size:  (55584, 1024)\n",
      "coordinates size:  (55584, 2)\n",
      "\n",
      "progress: 600/876\n",
      "9702\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [91215 69367]\n",
      "level_dim [91215 69367]\n",
      "name 9702\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9702.h5: total of 81 batches\n",
      "batch 0/81, 0 files processed\n",
      "batch 20/81, 10240 files processed\n",
      "batch 40/81, 20480 files processed\n",
      "batch 60/81, 30720 files processed\n",
      "batch 80/81, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9702.h5 took 42.37036609649658 s\n",
      "features size:  (41223, 1024)\n",
      "coordinates size:  (41223, 2)\n",
      "\n",
      "progress: 601/876\n",
      "9703\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [87375 85737]\n",
      "level_dim [87375 85737]\n",
      "name 9703\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9703.h5: total of 127 batches\n",
      "batch 0/127, 0 files processed\n",
      "batch 20/127, 10240 files processed\n",
      "batch 40/127, 20480 files processed\n",
      "batch 60/127, 30720 files processed\n",
      "batch 80/127, 40960 files processed\n",
      "batch 100/127, 51200 files processed\n",
      "batch 120/127, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9703.h5 took 108.31030344963074 s\n",
      "features size:  (64998, 1024)\n",
      "coordinates size:  (64998, 2)\n",
      "\n",
      "progress: 602/876\n",
      "9704\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [125781  85534]\n",
      "level_dim [125781  85534]\n",
      "name 9704\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9704.h5: total of 156 batches\n",
      "batch 0/156, 0 files processed\n",
      "batch 20/156, 10240 files processed\n",
      "batch 40/156, 20480 files processed\n",
      "batch 60/156, 30720 files processed\n",
      "batch 80/156, 40960 files processed\n",
      "batch 100/156, 51200 files processed\n",
      "batch 120/156, 61440 files processed\n",
      "batch 140/156, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9704.h5 took 140.50703477859497 s\n",
      "features size:  (79686, 1024)\n",
      "coordinates size:  (79686, 2)\n",
      "\n",
      "progress: 603/876\n",
      "9705\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [91215 70134]\n",
      "level_dim [91215 70134]\n",
      "name 9705\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9705.h5: total of 71 batches\n",
      "batch 0/71, 0 files processed\n",
      "batch 20/71, 10240 files processed\n",
      "batch 40/71, 20480 files processed\n",
      "batch 60/71, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9705.h5 took 57.367597341537476 s\n",
      "features size:  (35910, 1024)\n",
      "coordinates size:  (35910, 2)\n",
      "\n",
      "progress: 604/876\n",
      "9706\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [110419  82186]\n",
      "level_dim [110419  82186]\n",
      "name 9706\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9706.h5: total of 151 batches\n",
      "batch 0/151, 0 files processed\n",
      "batch 20/151, 10240 files processed\n",
      "batch 40/151, 20480 files processed\n",
      "batch 60/151, 30720 files processed\n",
      "batch 80/151, 40960 files processed\n",
      "batch 100/151, 51200 files processed\n",
      "batch 120/151, 61440 files processed\n",
      "batch 140/151, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9706.h5 took 114.29237604141235 s\n",
      "features size:  (77096, 1024)\n",
      "coordinates size:  (77096, 2)\n",
      "\n",
      "progress: 605/876\n",
      "9707\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [95056 77559]\n",
      "level_dim [95056 77559]\n",
      "name 9707\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9707.h5: total of 124 batches\n",
      "batch 0/124, 0 files processed\n",
      "batch 20/124, 10240 files processed\n",
      "batch 40/124, 20480 files processed\n",
      "batch 60/124, 30720 files processed\n",
      "batch 80/124, 40960 files processed\n",
      "batch 100/124, 51200 files processed\n",
      "batch 120/124, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9707.h5 took 102.08157920837402 s\n",
      "features size:  (63139, 1024)\n",
      "coordinates size:  (63139, 2)\n",
      "\n",
      "progress: 606/876\n",
      "9708\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [108498  85766]\n",
      "level_dim [108498  85766]\n",
      "name 9708\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9708.h5: total of 175 batches\n",
      "batch 0/175, 0 files processed\n",
      "batch 20/175, 10240 files processed\n",
      "batch 40/175, 20480 files processed\n",
      "batch 60/175, 30720 files processed\n",
      "batch 80/175, 40960 files processed\n",
      "batch 100/175, 51200 files processed\n",
      "batch 120/175, 61440 files processed\n",
      "batch 140/175, 71680 files processed\n",
      "batch 160/175, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9708.h5 took 92.94730353355408 s\n",
      "features size:  (89254, 1024)\n",
      "coordinates size:  (89254, 2)\n",
      "\n",
      "progress: 607/876\n",
      "9709\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [113299  85772]\n",
      "level_dim [113299  85772]\n",
      "name 9709\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9709.h5: total of 209 batches\n",
      "batch 0/209, 0 files processed\n",
      "batch 20/209, 10240 files processed\n",
      "batch 40/209, 20480 files processed\n",
      "batch 60/209, 30720 files processed\n",
      "batch 80/209, 40960 files processed\n",
      "batch 100/209, 51200 files processed\n",
      "batch 120/209, 61440 files processed\n",
      "batch 140/209, 71680 files processed\n",
      "batch 160/209, 81920 files processed\n",
      "batch 180/209, 92160 files processed\n",
      "batch 200/209, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9709.h5 took 177.29268717765808 s\n",
      "features size:  (106703, 1024)\n",
      "coordinates size:  (106703, 2)\n",
      "\n",
      "progress: 608/876\n",
      "9710\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [108498  85765]\n",
      "level_dim [108498  85765]\n",
      "name 9710\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9710.h5: total of 212 batches\n",
      "batch 0/212, 0 files processed\n",
      "batch 20/212, 10240 files processed\n",
      "batch 40/212, 20480 files processed\n",
      "batch 60/212, 30720 files processed\n",
      "batch 80/212, 40960 files processed\n",
      "batch 100/212, 51200 files processed\n",
      "batch 120/212, 61440 files processed\n",
      "batch 140/212, 71680 files processed\n",
      "batch 160/212, 81920 files processed\n",
      "batch 180/212, 92160 files processed\n",
      "batch 200/212, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9710.h5 took 169.59752249717712 s\n",
      "features size:  (108475, 1024)\n",
      "coordinates size:  (108475, 2)\n",
      "\n",
      "progress: 609/876\n",
      "9711\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [114259  78354]\n",
      "level_dim [114259  78354]\n",
      "name 9711\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9711.h5: total of 98 batches\n",
      "batch 0/98, 0 files processed\n",
      "batch 20/98, 10240 files processed\n",
      "batch 40/98, 20480 files processed\n",
      "batch 60/98, 30720 files processed\n",
      "batch 80/98, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9711.h5 took 59.31169152259827 s\n",
      "features size:  (49852, 1024)\n",
      "coordinates size:  (49852, 2)\n",
      "\n",
      "progress: 610/876\n",
      "9712\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [77773 65254]\n",
      "level_dim [77773 65254]\n",
      "name 9712\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9712.h5: total of 45 batches\n",
      "batch 0/45, 0 files processed\n",
      "batch 20/45, 10240 files processed\n",
      "batch 40/45, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9712.h5 took 32.31478714942932 s\n",
      "features size:  (22736, 1024)\n",
      "coordinates size:  (22736, 2)\n",
      "\n",
      "progress: 611/876\n",
      "9713\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [117140  79125]\n",
      "level_dim [117140  79125]\n",
      "name 9713\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9713.h5: total of 146 batches\n",
      "batch 0/146, 0 files processed\n",
      "batch 20/146, 10240 files processed\n",
      "batch 40/146, 20480 files processed\n",
      "batch 60/146, 30720 files processed\n",
      "batch 80/146, 40960 files processed\n",
      "batch 100/146, 51200 files processed\n",
      "batch 120/146, 61440 files processed\n",
      "batch 140/146, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9713.h5 took 89.32207584381104 s\n",
      "features size:  (74572, 1024)\n",
      "coordinates size:  (74572, 2)\n",
      "\n",
      "progress: 612/876\n",
      "9714\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [109459  75277]\n",
      "level_dim [109459  75277]\n",
      "name 9714\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9714.h5: total of 155 batches\n",
      "batch 0/155, 0 files processed\n",
      "batch 20/155, 10240 files processed\n",
      "batch 40/155, 20480 files processed\n",
      "batch 60/155, 30720 files processed\n",
      "batch 80/155, 40960 files processed\n",
      "batch 100/155, 51200 files processed\n",
      "batch 120/155, 61440 files processed\n",
      "batch 140/155, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9714.h5 took 103.5391674041748 s\n",
      "features size:  (79169, 1024)\n",
      "coordinates size:  (79169, 2)\n",
      "\n",
      "progress: 613/876\n",
      "9715\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [91215 80112]\n",
      "level_dim [91215 80112]\n",
      "name 9715\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9715.h5: total of 87 batches\n",
      "batch 0/87, 0 files processed\n",
      "batch 20/87, 10240 files processed\n",
      "batch 40/87, 20480 files processed\n",
      "batch 60/87, 30720 files processed\n",
      "batch 80/87, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9715.h5 took 57.74728441238403 s\n",
      "features size:  (44061, 1024)\n",
      "coordinates size:  (44061, 2)\n",
      "\n",
      "progress: 614/876\n",
      "9716\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [35526 69800]\n",
      "level_dim [35526 69800]\n",
      "name 9716\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9716.h5: total of 19 batches\n",
      "batch 0/19, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9716.h5 took 15.877365350723267 s\n",
      "features size:  (9421, 1024)\n",
      "coordinates size:  (9421, 2)\n",
      "\n",
      "progress: 615/876\n",
      "9717\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [99857 68355]\n",
      "level_dim [99857 68355]\n",
      "name 9717\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9717.h5: total of 114 batches\n",
      "batch 0/114, 0 files processed\n",
      "batch 20/114, 10240 files processed\n",
      "batch 40/114, 20480 files processed\n",
      "batch 60/114, 30720 files processed\n",
      "batch 80/114, 40960 files processed\n",
      "batch 100/114, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9717.h5 took 97.7840621471405 s\n",
      "features size:  (57972, 1024)\n",
      "coordinates size:  (57972, 2)\n",
      "\n",
      "progress: 616/876\n",
      "9718\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [65291 73936]\n",
      "level_dim [65291 73936]\n",
      "name 9718\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9718.h5: total of 68 batches\n",
      "batch 0/68, 0 files processed\n",
      "batch 20/68, 10240 files processed\n",
      "batch 40/68, 20480 files processed\n",
      "batch 60/68, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9718.h5 took 63.89647388458252 s\n",
      "features size:  (34338, 1024)\n",
      "coordinates size:  (34338, 2)\n",
      "\n",
      "progress: 617/876\n",
      "9719\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [118100  85779]\n",
      "level_dim [118100  85779]\n",
      "name 9719\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9719.h5: total of 211 batches\n",
      "batch 0/211, 0 files processed\n",
      "batch 20/211, 10240 files processed\n",
      "batch 40/211, 20480 files processed\n",
      "batch 60/211, 30720 files processed\n",
      "batch 80/211, 40960 files processed\n",
      "batch 100/211, 51200 files processed\n",
      "batch 120/211, 61440 files processed\n",
      "batch 140/211, 71680 files processed\n",
      "batch 160/211, 81920 files processed\n",
      "batch 180/211, 92160 files processed\n",
      "batch 200/211, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9719.h5 took 109.72874140739441 s\n",
      "features size:  (107952, 1024)\n",
      "coordinates size:  (107952, 2)\n",
      "\n",
      "progress: 618/876\n",
      "9720\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [75853 73438]\n",
      "level_dim [75853 73438]\n",
      "name 9720\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9720.h5: total of 72 batches\n",
      "batch 0/72, 0 files processed\n",
      "batch 20/72, 10240 files processed\n",
      "batch 40/72, 20480 files processed\n",
      "batch 60/72, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9720.h5 took 59.421122312545776 s\n",
      "features size:  (36628, 1024)\n",
      "coordinates size:  (36628, 2)\n",
      "\n",
      "progress: 619/876\n",
      "9721\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [69132 74452]\n",
      "level_dim [69132 74452]\n",
      "name 9721\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9721.h5: total of 64 batches\n",
      "batch 0/64, 0 files processed\n",
      "batch 20/64, 10240 files processed\n",
      "batch 40/64, 20480 files processed\n",
      "batch 60/64, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9721.h5 took 60.04968070983887 s\n",
      "features size:  (32634, 1024)\n",
      "coordinates size:  (32634, 2)\n",
      "\n",
      "progress: 620/876\n",
      "9722\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [86415 72430]\n",
      "level_dim [86415 72430]\n",
      "name 9722\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9722.h5: total of 105 batches\n",
      "batch 0/105, 0 files processed\n",
      "batch 20/105, 10240 files processed\n",
      "batch 40/105, 20480 files processed\n",
      "batch 60/105, 30720 files processed\n",
      "batch 80/105, 40960 files processed\n",
      "batch 100/105, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9722.h5 took 83.26931929588318 s\n",
      "features size:  (53294, 1024)\n",
      "coordinates size:  (53294, 2)\n",
      "\n",
      "progress: 621/876\n",
      "9723\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [94096 76022]\n",
      "level_dim [94096 76022]\n",
      "name 9723\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9723.h5: total of 79 batches\n",
      "batch 0/79, 0 files processed\n",
      "batch 20/79, 10240 files processed\n",
      "batch 40/79, 20480 files processed\n",
      "batch 60/79, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9723.h5 took 66.26930785179138 s\n",
      "features size:  (40208, 1024)\n",
      "coordinates size:  (40208, 2)\n",
      "\n",
      "progress: 622/876\n",
      "9724\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [61450 67278]\n",
      "level_dim [61450 67278]\n",
      "name 9724\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9724.h5: total of 42 batches\n",
      "batch 0/42, 0 files processed\n",
      "batch 20/42, 10240 files processed\n",
      "batch 40/42, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9724.h5 took 33.12183356285095 s\n",
      "features size:  (21499, 1024)\n",
      "coordinates size:  (21499, 2)\n",
      "\n",
      "progress: 623/876\n",
      "9725\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [67211 72403]\n",
      "level_dim [67211 72403]\n",
      "name 9725\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9725.h5: total of 38 batches\n",
      "batch 0/38, 0 files processed\n",
      "batch 20/38, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9725.h5 took 34.14301562309265 s\n",
      "features size:  (19039, 1024)\n",
      "coordinates size:  (19039, 2)\n",
      "\n",
      "progress: 624/876\n",
      "9726\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [61450 72139]\n",
      "level_dim [61450 72139]\n",
      "name 9726\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9726.h5: total of 51 batches\n",
      "batch 0/51, 0 files processed\n",
      "batch 20/51, 10240 files processed\n",
      "batch 40/51, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9726.h5 took 43.96580624580383 s\n",
      "features size:  (26034, 1024)\n",
      "coordinates size:  (26034, 2)\n",
      "\n",
      "progress: 625/876\n",
      "9733\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [86415 69872]\n",
      "level_dim [86415 69872]\n",
      "name 9733\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9733.h5: total of 48 batches\n",
      "batch 0/48, 0 files processed\n",
      "batch 20/48, 10240 files processed\n",
      "batch 40/48, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9733.h5 took 35.55657649040222 s\n",
      "features size:  (24365, 1024)\n",
      "coordinates size:  (24365, 2)\n",
      "\n",
      "progress: 626/876\n",
      "9734\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [83534 62959]\n",
      "level_dim [83534 62959]\n",
      "name 9734\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9734.h5: total of 69 batches\n",
      "batch 0/69, 0 files processed\n",
      "batch 20/69, 10240 files processed\n",
      "batch 40/69, 20480 files processed\n",
      "batch 60/69, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9734.h5 took 52.67711353302002 s\n",
      "features size:  (35023, 1024)\n",
      "coordinates size:  (35023, 2)\n",
      "\n",
      "progress: 627/876\n",
      "9735\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96016 75257]\n",
      "level_dim [96016 75257]\n",
      "name 9735\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9735.h5: total of 115 batches\n",
      "batch 0/115, 0 files processed\n",
      "batch 20/115, 10240 files processed\n",
      "batch 40/115, 20480 files processed\n",
      "batch 60/115, 30720 files processed\n",
      "batch 80/115, 40960 files processed\n",
      "batch 100/115, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9735.h5 took 109.50240063667297 s\n",
      "features size:  (58628, 1024)\n",
      "coordinates size:  (58628, 2)\n",
      "\n",
      "progress: 628/876\n",
      "9736\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [121941  68898]\n",
      "level_dim [121941  68898]\n",
      "name 9736\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9736.h5: total of 109 batches\n",
      "batch 0/109, 0 files processed\n",
      "batch 20/109, 10240 files processed\n",
      "batch 40/109, 20480 files processed\n",
      "batch 60/109, 30720 files processed\n",
      "batch 80/109, 40960 files processed\n",
      "batch 100/109, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9736.h5 took 77.13011193275452 s\n",
      "features size:  (55340, 1024)\n",
      "coordinates size:  (55340, 2)\n",
      "\n",
      "progress: 629/876\n",
      "9737\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [88335 74224]\n",
      "level_dim [88335 74224]\n",
      "name 9737\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9737.h5: total of 99 batches\n",
      "batch 0/99, 0 files processed\n",
      "batch 20/99, 10240 files processed\n",
      "batch 40/99, 20480 files processed\n",
      "batch 60/99, 30720 files processed\n",
      "batch 80/99, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9737.h5 took 76.446280002594 s\n",
      "features size:  (50454, 1024)\n",
      "coordinates size:  (50454, 2)\n",
      "\n",
      "progress: 630/876\n",
      "9738\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [86415 73965]\n",
      "level_dim [86415 73965]\n",
      "name 9738\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9738.h5: total of 80 batches\n",
      "batch 0/80, 0 files processed\n",
      "batch 20/80, 10240 files processed\n",
      "batch 40/80, 20480 files processed\n",
      "batch 60/80, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9738.h5 took 63.41359066963196 s\n",
      "features size:  (40766, 1024)\n",
      "coordinates size:  (40766, 2)\n",
      "\n",
      "progress: 631/876\n",
      "9739\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [113299  85772]\n",
      "level_dim [113299  85772]\n",
      "name 9739\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9739.h5: total of 183 batches\n",
      "batch 0/183, 0 files processed\n",
      "batch 20/183, 10240 files processed\n",
      "batch 40/183, 20480 files processed\n",
      "batch 60/183, 30720 files processed\n",
      "batch 80/183, 40960 files processed\n",
      "batch 100/183, 51200 files processed\n",
      "batch 120/183, 61440 files processed\n",
      "batch 140/183, 71680 files processed\n",
      "batch 160/183, 81920 files processed\n",
      "batch 180/183, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9739.h5 took 143.15953946113586 s\n",
      "features size:  (93599, 1024)\n",
      "coordinates size:  (93599, 2)\n",
      "\n",
      "progress: 632/876\n",
      "9740\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [112339  85771]\n",
      "level_dim [112339  85771]\n",
      "name 9740\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9740.h5: total of 139 batches\n",
      "batch 0/139, 0 files processed\n",
      "batch 20/139, 10240 files processed\n",
      "batch 40/139, 20480 files processed\n",
      "batch 60/139, 30720 files processed\n",
      "batch 80/139, 40960 files processed\n",
      "batch 100/139, 51200 files processed\n",
      "batch 120/139, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9740.h5 took 117.65936946868896 s\n",
      "features size:  (71159, 1024)\n",
      "coordinates size:  (71159, 2)\n",
      "\n",
      "progress: 633/876\n",
      "9741\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [128662  85794]\n",
      "level_dim [128662  85794]\n",
      "name 9741\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9741.h5: total of 165 batches\n",
      "batch 0/165, 0 files processed\n",
      "batch 20/165, 10240 files processed\n",
      "batch 40/165, 20480 files processed\n",
      "batch 60/165, 30720 files processed\n",
      "batch 80/165, 40960 files processed\n",
      "batch 100/165, 51200 files processed\n",
      "batch 120/165, 61440 files processed\n",
      "batch 140/165, 71680 files processed\n",
      "batch 160/165, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9741.h5 took 131.63096475601196 s\n",
      "features size:  (84151, 1024)\n",
      "coordinates size:  (84151, 2)\n",
      "\n",
      "progress: 634/876\n",
      "9742\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [103698  85759]\n",
      "level_dim [103698  85759]\n",
      "name 9742\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9742.h5: total of 159 batches\n",
      "batch 0/159, 0 files processed\n",
      "batch 20/159, 10240 files processed\n",
      "batch 40/159, 20480 files processed\n",
      "batch 60/159, 30720 files processed\n",
      "batch 80/159, 40960 files processed\n",
      "batch 100/159, 51200 files processed\n",
      "batch 120/159, 61440 files processed\n",
      "batch 140/159, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9742.h5 took 88.14959788322449 s\n",
      "features size:  (81273, 1024)\n",
      "coordinates size:  (81273, 2)\n",
      "\n",
      "progress: 635/876\n",
      "9743\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [103698  84736]\n",
      "level_dim [103698  84736]\n",
      "name 9743\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9743.h5: total of 179 batches\n",
      "batch 0/179, 0 files processed\n",
      "batch 20/179, 10240 files processed\n",
      "batch 40/179, 20480 files processed\n",
      "batch 60/179, 30720 files processed\n",
      "batch 80/179, 40960 files processed\n",
      "batch 100/179, 51200 files processed\n",
      "batch 120/179, 61440 files processed\n",
      "batch 140/179, 71680 files processed\n",
      "batch 160/179, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9743.h5 took 131.83021926879883 s\n",
      "features size:  (91265, 1024)\n",
      "coordinates size:  (91265, 2)\n",
      "\n",
      "progress: 636/876\n",
      "9744\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [128662  77350]\n",
      "level_dim [128662  77350]\n",
      "name 9744\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9744.h5: total of 200 batches\n",
      "batch 0/200, 0 files processed\n",
      "batch 20/200, 10240 files processed\n",
      "batch 40/200, 20480 files processed\n",
      "batch 60/200, 30720 files processed\n",
      "batch 80/200, 40960 files processed\n",
      "batch 100/200, 51200 files processed\n",
      "batch 120/200, 61440 files processed\n",
      "batch 140/200, 71680 files processed\n",
      "batch 160/200, 81920 files processed\n",
      "batch 180/200, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9744.h5 took 120.14610409736633 s\n",
      "features size:  (102173, 1024)\n",
      "coordinates size:  (102173, 2)\n",
      "\n",
      "progress: 637/876\n",
      "9745\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51849 57285]\n",
      "level_dim [51849 57285]\n",
      "name 9745\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9745.h5: total of 22 batches\n",
      "batch 0/22, 0 files processed\n",
      "batch 20/22, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9745.h5 took 15.995869159698486 s\n",
      "features size:  (10808, 1024)\n",
      "coordinates size:  (10808, 2)\n",
      "\n",
      "progress: 638/876\n",
      "9746\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [106578  83205]\n",
      "level_dim [106578  83205]\n",
      "name 9746\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9746.h5: total of 155 batches\n",
      "batch 0/155, 0 files processed\n",
      "batch 20/155, 10240 files processed\n",
      "batch 40/155, 20480 files processed\n",
      "batch 60/155, 30720 files processed\n",
      "batch 80/155, 40960 files processed\n",
      "batch 100/155, 51200 files processed\n",
      "batch 120/155, 61440 files processed\n",
      "batch 140/155, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9746.h5 took 129.62463068962097 s\n",
      "features size:  (78915, 1024)\n",
      "coordinates size:  (78915, 2)\n",
      "\n",
      "progress: 639/876\n",
      "9747\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [147865  85821]\n",
      "level_dim [147865  85821]\n",
      "name 9747\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9747.h5: total of 152 batches\n",
      "batch 0/152, 0 files processed\n",
      "batch 20/152, 10240 files processed\n",
      "batch 40/152, 20480 files processed\n",
      "batch 60/152, 30720 files processed\n",
      "batch 80/152, 40960 files processed\n",
      "batch 100/152, 51200 files processed\n",
      "batch 120/152, 61440 files processed\n",
      "batch 140/152, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9747.h5 took 115.66294765472412 s\n",
      "features size:  (77784, 1024)\n",
      "coordinates size:  (77784, 2)\n",
      "\n",
      "progress: 640/876\n",
      "9755\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [106578  80390]\n",
      "level_dim [106578  80390]\n",
      "name 9755\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9755.h5: total of 166 batches\n",
      "batch 0/166, 0 files processed\n",
      "batch 20/166, 10240 files processed\n",
      "batch 40/166, 20480 files processed\n",
      "batch 60/166, 30720 files processed\n",
      "batch 80/166, 40960 files processed\n",
      "batch 100/166, 51200 files processed\n",
      "batch 120/166, 61440 files processed\n",
      "batch 140/166, 71680 files processed\n",
      "batch 160/166, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9755.h5 took 131.73839950561523 s\n",
      "features size:  (84683, 1024)\n",
      "coordinates size:  (84683, 2)\n",
      "\n",
      "progress: 641/876\n",
      "9756\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [95056 68348]\n",
      "level_dim [95056 68348]\n",
      "name 9756\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9756.h5: total of 107 batches\n",
      "batch 0/107, 0 files processed\n",
      "batch 20/107, 10240 files processed\n",
      "batch 40/107, 20480 files processed\n",
      "batch 60/107, 30720 files processed\n",
      "batch 80/107, 40960 files processed\n",
      "batch 100/107, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9756.h5 took 81.1664674282074 s\n",
      "features size:  (54690, 1024)\n",
      "coordinates size:  (54690, 2)\n",
      "\n",
      "progress: 642/876\n",
      "9757\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [78733 74978]\n",
      "level_dim [78733 74978]\n",
      "name 9757\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9757.h5: total of 66 batches\n",
      "batch 0/66, 0 files processed\n",
      "batch 20/66, 10240 files processed\n",
      "batch 40/66, 20480 files processed\n",
      "batch 60/66, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9757.h5 took 52.53227663040161 s\n",
      "features size:  (33723, 1024)\n",
      "coordinates size:  (33723, 2)\n",
      "\n",
      "progress: 643/876\n",
      "9776\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [100817  83708]\n",
      "level_dim [100817  83708]\n",
      "name 9776\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9776.h5: total of 101 batches\n",
      "batch 0/101, 0 files processed\n",
      "batch 20/101, 10240 files processed\n",
      "batch 40/101, 20480 files processed\n",
      "batch 60/101, 30720 files processed\n",
      "batch 80/101, 40960 files processed\n",
      "batch 100/101, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9776.h5 took 78.29750680923462 s\n",
      "features size:  (51499, 1024)\n",
      "coordinates size:  (51499, 2)\n",
      "\n",
      "progress: 644/876\n",
      "9777\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [108498  85766]\n",
      "level_dim [108498  85766]\n",
      "name 9777\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9777.h5: total of 208 batches\n",
      "batch 0/208, 0 files processed\n",
      "batch 20/208, 10240 files processed\n",
      "batch 40/208, 20480 files processed\n",
      "batch 60/208, 30720 files processed\n",
      "batch 80/208, 40960 files processed\n",
      "batch 100/208, 51200 files processed\n",
      "batch 120/208, 61440 files processed\n",
      "batch 140/208, 71680 files processed\n",
      "batch 160/208, 81920 files processed\n",
      "batch 180/208, 92160 files processed\n",
      "batch 200/208, 102400 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9777.h5 took 188.91939854621887 s\n",
      "features size:  (106340, 1024)\n",
      "coordinates size:  (106340, 2)\n",
      "\n",
      "progress: 645/876\n",
      "9778\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [114259  85773]\n",
      "level_dim [114259  85773]\n",
      "name 9778\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9778.h5: total of 226 batches\n",
      "batch 0/226, 0 files processed\n",
      "batch 20/226, 10240 files processed\n",
      "batch 40/226, 20480 files processed\n",
      "batch 60/226, 30720 files processed\n",
      "batch 80/226, 40960 files processed\n",
      "batch 100/226, 51200 files processed\n",
      "batch 120/226, 61440 files processed\n",
      "batch 140/226, 71680 files processed\n",
      "batch 160/226, 81920 files processed\n",
      "batch 180/226, 92160 files processed\n",
      "batch 200/226, 102400 files processed\n",
      "batch 220/226, 112640 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9778.h5 took 195.1698760986328 s\n",
      "features size:  (115302, 1024)\n",
      "coordinates size:  (115302, 2)\n",
      "\n",
      "progress: 646/876\n",
      "9808\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [116180  85521]\n",
      "level_dim [116180  85521]\n",
      "name 9808\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9808.h5: total of 169 batches\n",
      "batch 0/169, 0 files processed\n",
      "batch 20/169, 10240 files processed\n",
      "batch 40/169, 20480 files processed\n",
      "batch 60/169, 30720 files processed\n",
      "batch 80/169, 40960 files processed\n",
      "batch 100/169, 51200 files processed\n",
      "batch 120/169, 61440 files processed\n",
      "batch 140/169, 71680 files processed\n",
      "batch 160/169, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9808.h5 took 144.76239490509033 s\n",
      "features size:  (86357, 1024)\n",
      "coordinates size:  (86357, 2)\n",
      "\n",
      "progress: 647/876\n",
      "9936\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57610 47059]\n",
      "level_dim [57610 47059]\n",
      "name 9936\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9936.h5: total of 30 batches\n",
      "batch 0/30, 0 files processed\n",
      "batch 20/30, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9936.h5 took 26.14076852798462 s\n",
      "features size:  (15118, 1024)\n",
      "coordinates size:  (15118, 2)\n",
      "\n",
      "progress: 648/876\n",
      "9938\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [116180  83729]\n",
      "level_dim [116180  83729]\n",
      "name 9938\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9938.h5: total of 177 batches\n",
      "batch 0/177, 0 files processed\n",
      "batch 20/177, 10240 files processed\n",
      "batch 40/177, 20480 files processed\n",
      "batch 60/177, 30720 files processed\n",
      "batch 80/177, 40960 files processed\n",
      "batch 100/177, 51200 files processed\n",
      "batch 120/177, 61440 files processed\n",
      "batch 140/177, 71680 files processed\n",
      "batch 160/177, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9938.h5 took 130.87821793556213 s\n",
      "features size:  (90484, 1024)\n",
      "coordinates size:  (90484, 2)\n",
      "\n",
      "progress: 649/876\n",
      "9939\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [101777  60425]\n",
      "level_dim [101777  60425]\n",
      "name 9939\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9939.h5: total of 84 batches\n",
      "batch 0/84, 0 files processed\n",
      "batch 20/84, 10240 files processed\n",
      "batch 40/84, 20480 files processed\n",
      "batch 60/84, 30720 files processed\n",
      "batch 80/84, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9939.h5 took 62.83713102340698 s\n",
      "features size:  (42999, 1024)\n",
      "coordinates size:  (42999, 2)\n",
      "\n",
      "progress: 650/876\n",
      "9948\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96016 67838]\n",
      "level_dim [96016 67838]\n",
      "name 9948\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9948.h5: total of 99 batches\n",
      "batch 0/99, 0 files processed\n",
      "batch 20/99, 10240 files processed\n",
      "batch 40/99, 20480 files processed\n",
      "batch 60/99, 30720 files processed\n",
      "batch 80/99, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9948.h5 took 78.87114810943604 s\n",
      "features size:  (50316, 1024)\n",
      "coordinates size:  (50316, 2)\n",
      "\n",
      "progress: 651/876\n",
      "9949\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [66251 50398]\n",
      "level_dim [66251 50398]\n",
      "name 9949\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9949.h5: total of 49 batches\n",
      "batch 0/49, 0 files processed\n",
      "batch 20/49, 10240 files processed\n",
      "batch 40/49, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9949.h5 took 38.025442361831665 s\n",
      "features size:  (24584, 1024)\n",
      "coordinates size:  (24584, 2)\n",
      "\n",
      "progress: 652/876\n",
      "9953\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [95056 76792]\n",
      "level_dim [95056 76792]\n",
      "name 9953\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9953.h5: total of 111 batches\n",
      "batch 0/111, 0 files processed\n",
      "batch 20/111, 10240 files processed\n",
      "batch 40/111, 20480 files processed\n",
      "batch 60/111, 30720 files processed\n",
      "batch 80/111, 40960 files processed\n",
      "batch 100/111, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9953.h5 took 91.63480997085571 s\n",
      "features size:  (56324, 1024)\n",
      "coordinates size:  (56324, 2)\n",
      "\n",
      "progress: 653/876\n",
      "9954\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [114259  79633]\n",
      "level_dim [114259  79633]\n",
      "name 9954\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9954.h5: total of 166 batches\n",
      "batch 0/166, 0 files processed\n",
      "batch 20/166, 10240 files processed\n",
      "batch 40/166, 20480 files processed\n",
      "batch 60/166, 30720 files processed\n",
      "batch 80/166, 40960 files processed\n",
      "batch 100/166, 51200 files processed\n",
      "batch 120/166, 61440 files processed\n",
      "batch 140/166, 71680 files processed\n",
      "batch 160/166, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9954.h5 took 124.1312780380249 s\n",
      "features size:  (84747, 1024)\n",
      "coordinates size:  (84747, 2)\n",
      "\n",
      "progress: 654/876\n",
      "9955\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [60490 59089]\n",
      "level_dim [60490 59089]\n",
      "name 9955\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9955.h5: total of 48 batches\n",
      "batch 0/48, 0 files processed\n",
      "batch 20/48, 10240 files processed\n",
      "batch 40/48, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9955.h5 took 35.40682816505432 s\n",
      "features size:  (24250, 1024)\n",
      "coordinates size:  (24250, 2)\n",
      "\n",
      "progress: 655/876\n",
      "9961\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [81614 65514]\n",
      "level_dim [81614 65514]\n",
      "name 9961\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9961.h5: total of 82 batches\n",
      "batch 0/82, 0 files processed\n",
      "batch 20/82, 10240 files processed\n",
      "batch 40/82, 20480 files processed\n",
      "batch 60/82, 30720 files processed\n",
      "batch 80/82, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9961.h5 took 63.91998863220215 s\n",
      "features size:  (41793, 1024)\n",
      "coordinates size:  (41793, 2)\n",
      "\n",
      "progress: 656/876\n",
      "9962\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [74893 68063]\n",
      "level_dim [74893 68063]\n",
      "name 9962\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9962.h5: total of 90 batches\n",
      "batch 0/90, 0 files processed\n",
      "batch 20/90, 10240 files processed\n",
      "batch 40/90, 20480 files processed\n",
      "batch 60/90, 30720 files processed\n",
      "batch 80/90, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9962.h5 took 74.65381503105164 s\n",
      "features size:  (45995, 1024)\n",
      "coordinates size:  (45995, 2)\n",
      "\n",
      "progress: 657/876\n",
      "9965\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96976 70653]\n",
      "level_dim [96976 70653]\n",
      "name 9965\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9965.h5: total of 49 batches\n",
      "batch 0/49, 0 files processed\n",
      "batch 20/49, 10240 files processed\n",
      "batch 40/49, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9965.h5 took 29.86839461326599 s\n",
      "features size:  (24963, 1024)\n",
      "coordinates size:  (24963, 2)\n",
      "\n",
      "progress: 658/876\n",
      "9966\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [114259  83215]\n",
      "level_dim [114259  83215]\n",
      "name 9966\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9966.h5: total of 149 batches\n",
      "batch 0/149, 0 files processed\n",
      "batch 20/149, 10240 files processed\n",
      "batch 40/149, 20480 files processed\n",
      "batch 60/149, 30720 files processed\n",
      "batch 80/149, 40960 files processed\n",
      "batch 100/149, 51200 files processed\n",
      "batch 120/149, 61440 files processed\n",
      "batch 140/149, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9966.h5 took 131.78330755233765 s\n",
      "features size:  (75823, 1024)\n",
      "coordinates size:  (75823, 2)\n",
      "\n",
      "progress: 659/876\n",
      "9967\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96016 76538]\n",
      "level_dim [96016 76538]\n",
      "name 9967\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9967.h5: total of 148 batches\n",
      "batch 0/148, 0 files processed\n",
      "batch 20/148, 10240 files processed\n",
      "batch 40/148, 20480 files processed\n",
      "batch 60/148, 30720 files processed\n",
      "batch 80/148, 40960 files processed\n",
      "batch 100/148, 51200 files processed\n",
      "batch 120/148, 61440 files processed\n",
      "batch 140/148, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9967.h5 took 92.60458946228027 s\n",
      "features size:  (75563, 1024)\n",
      "coordinates size:  (75563, 2)\n",
      "\n",
      "progress: 660/876\n",
      "9971\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [122901  69922]\n",
      "level_dim [122901  69922]\n",
      "name 9971\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9971.h5: total of 147 batches\n",
      "batch 0/147, 0 files processed\n",
      "batch 20/147, 10240 files processed\n",
      "batch 40/147, 20480 files processed\n",
      "batch 60/147, 30720 files processed\n",
      "batch 80/147, 40960 files processed\n",
      "batch 100/147, 51200 files processed\n",
      "batch 120/147, 61440 files processed\n",
      "batch 140/147, 71680 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9971.h5 took 111.96314334869385 s\n",
      "features size:  (75050, 1024)\n",
      "coordinates size:  (75050, 2)\n",
      "\n",
      "progress: 661/876\n",
      "9973\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [55689 42195]\n",
      "level_dim [55689 42195]\n",
      "name 9973\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9973.h5: total of 38 batches\n",
      "batch 0/38, 0 files processed\n",
      "batch 20/38, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9973.h5 took 32.490004777908325 s\n",
      "features size:  (19037, 1024)\n",
      "coordinates size:  (19037, 2)\n",
      "\n",
      "progress: 662/876\n",
      "9974\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [32645 30905]\n",
      "level_dim [32645 30905]\n",
      "name 9974\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9974.h5: total of 14 batches\n",
      "batch 0/14, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9974.h5 took 13.90993881225586 s\n",
      "features size:  (6972, 1024)\n",
      "coordinates size:  (6972, 2)\n",
      "\n",
      "progress: 663/876\n",
      "9975\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [85454 70637]\n",
      "level_dim [85454 70637]\n",
      "name 9975\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9975.h5: total of 80 batches\n",
      "batch 0/80, 0 files processed\n",
      "batch 20/80, 10240 files processed\n",
      "batch 40/80, 20480 files processed\n",
      "batch 60/80, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9975.h5 took 54.07040548324585 s\n",
      "features size:  (40886, 1024)\n",
      "coordinates size:  (40886, 2)\n",
      "\n",
      "progress: 664/876\n",
      "9976\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [56649 56525]\n",
      "level_dim [56649 56525]\n",
      "name 9976\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9976.h5: total of 45 batches\n",
      "batch 0/45, 0 files processed\n",
      "batch 20/45, 10240 files processed\n",
      "batch 40/45, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9976.h5 took 41.88465404510498 s\n",
      "features size:  (22878, 1024)\n",
      "coordinates size:  (22878, 2)\n",
      "\n",
      "progress: 665/876\n",
      "9977\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [66251 57050]\n",
      "level_dim [66251 57050]\n",
      "name 9977\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9977.h5: total of 35 batches\n",
      "batch 0/35, 0 files processed\n",
      "batch 20/35, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9977.h5 took 28.768901109695435 s\n",
      "features size:  (17798, 1024)\n",
      "coordinates size:  (17798, 2)\n",
      "\n",
      "progress: 666/876\n",
      "9978\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [121941  39474]\n",
      "level_dim [121941  39474]\n",
      "name 9978\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9978.h5: total of 94 batches\n",
      "batch 0/94, 0 files processed\n",
      "batch 20/94, 10240 files processed\n",
      "batch 40/94, 20480 files processed\n",
      "batch 60/94, 30720 files processed\n",
      "batch 80/94, 40960 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9978.h5 took 76.77687883377075 s\n",
      "features size:  (48062, 1024)\n",
      "coordinates size:  (48062, 2)\n",
      "\n",
      "progress: 667/876\n",
      "9979\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [105618  49685]\n",
      "level_dim [105618  49685]\n",
      "name 9979\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9979.h5: total of 80 batches\n",
      "batch 0/80, 0 files processed\n",
      "batch 20/80, 10240 files processed\n",
      "batch 40/80, 20480 files processed\n",
      "batch 60/80, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9979.h5 took 75.46822428703308 s\n",
      "features size:  (40929, 1024)\n",
      "coordinates size:  (40929, 2)\n",
      "\n",
      "progress: 668/876\n",
      "9980\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [72972 39917]\n",
      "level_dim [72972 39917]\n",
      "name 9980\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9980.h5: total of 29 batches\n",
      "batch 0/29, 0 files processed\n",
      "batch 20/29, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9980.h5 took 22.52674889564514 s\n",
      "features size:  (14768, 1024)\n",
      "coordinates size:  (14768, 2)\n",
      "\n",
      "progress: 669/876\n",
      "9981\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [65291 58839]\n",
      "level_dim [65291 58839]\n",
      "name 9981\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9981.h5: total of 54 batches\n",
      "batch 0/54, 0 files processed\n",
      "batch 20/54, 10240 files processed\n",
      "batch 40/54, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9981.h5 took 47.51830005645752 s\n",
      "features size:  (27182, 1024)\n",
      "coordinates size:  (27182, 2)\n",
      "\n",
      "progress: 670/876\n",
      "9986\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [98897 85240]\n",
      "level_dim [98897 85240]\n",
      "name 9986\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9986.h5: total of 124 batches\n",
      "batch 0/124, 0 files processed\n",
      "batch 20/124, 10240 files processed\n",
      "batch 40/124, 20480 files processed\n",
      "batch 60/124, 30720 files processed\n",
      "batch 80/124, 40960 files processed\n",
      "batch 100/124, 51200 files processed\n",
      "batch 120/124, 61440 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9986.h5 took 92.90917468070984 s\n",
      "features size:  (63115, 1024)\n",
      "coordinates size:  (63115, 2)\n",
      "\n",
      "progress: 671/876\n",
      "9987\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [138264  84783]\n",
      "level_dim [138264  84783]\n",
      "name 9987\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9987.h5: total of 193 batches\n",
      "batch 0/193, 0 files processed\n",
      "batch 20/193, 10240 files processed\n",
      "batch 40/193, 20480 files processed\n",
      "batch 60/193, 30720 files processed\n",
      "batch 80/193, 40960 files processed\n",
      "batch 100/193, 51200 files processed\n",
      "batch 120/193, 61440 files processed\n",
      "batch 140/193, 71680 files processed\n",
      "batch 160/193, 81920 files processed\n",
      "batch 180/193, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9987.h5 took 141.50935816764832 s\n",
      "features size:  (98812, 1024)\n",
      "coordinates size:  (98812, 2)\n",
      "\n",
      "progress: 672/876\n",
      "9988\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [96976 68862]\n",
      "level_dim [96976 68862]\n",
      "name 9988\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9988.h5: total of 109 batches\n",
      "batch 0/109, 0 files processed\n",
      "batch 20/109, 10240 files processed\n",
      "batch 40/109, 20480 files processed\n",
      "batch 60/109, 30720 files processed\n",
      "batch 80/109, 40960 files processed\n",
      "batch 100/109, 51200 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9988.h5 took 88.86342477798462 s\n",
      "features size:  (55468, 1024)\n",
      "coordinates size:  (55468, 2)\n",
      "\n",
      "progress: 673/876\n",
      "9989\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [98897 85752]\n",
      "level_dim [98897 85752]\n",
      "name 9989\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9989.h5: total of 183 batches\n",
      "batch 0/183, 0 files processed\n",
      "batch 20/183, 10240 files processed\n",
      "batch 40/183, 20480 files processed\n",
      "batch 60/183, 30720 files processed\n",
      "batch 80/183, 40960 files processed\n",
      "batch 100/183, 51200 files processed\n",
      "batch 120/183, 61440 files processed\n",
      "batch 140/183, 71680 files processed\n",
      "batch 160/183, 81920 files processed\n",
      "batch 180/183, 92160 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9989.h5 took 100.39337706565857 s\n",
      "features size:  (93522, 1024)\n",
      "coordinates size:  (93522, 2)\n",
      "\n",
      "progress: 674/876\n",
      "9990\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [122901  81948]\n",
      "level_dim [122901  81948]\n",
      "name 9990\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9990.h5: total of 171 batches\n",
      "batch 0/171, 0 files processed\n",
      "batch 20/171, 10240 files processed\n",
      "batch 40/171, 20480 files processed\n",
      "batch 60/171, 30720 files processed\n",
      "batch 80/171, 40960 files processed\n",
      "batch 100/171, 51200 files processed\n",
      "batch 120/171, 61440 files processed\n",
      "batch 140/171, 71680 files processed\n",
      "batch 160/171, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9990.h5 took 128.305983543396 s\n",
      "features size:  (87508, 1024)\n",
      "coordinates size:  (87508, 2)\n",
      "\n",
      "progress: 675/876\n",
      "9993\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [114259  85007]\n",
      "level_dim [114259  85007]\n",
      "name 9993\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/9993.h5: total of 170 batches\n",
      "batch 0/170, 0 files processed\n",
      "batch 20/170, 10240 files processed\n",
      "batch 40/170, 20480 files processed\n",
      "batch 60/170, 30720 files processed\n",
      "batch 80/170, 40960 files processed\n",
      "batch 100/170, 51200 files processed\n",
      "batch 120/170, 61440 files processed\n",
      "batch 140/170, 71680 files processed\n",
      "batch 160/170, 81920 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/9993.h5 took 138.51231288909912 s\n",
      "features size:  (86797, 1024)\n",
      "coordinates size:  (86797, 2)\n",
      "\n",
      "progress: 676/876\n",
      "NLSI0000004\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [37879 29953]\n",
      "level_dim [37879 29953]\n",
      "name NLSI0000004\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000004.h5: total of 9 batches\n",
      "batch 0/9, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000004.h5 took 5.7100348472595215 s\n",
      "features size:  (4402, 1024)\n",
      "coordinates size:  (4402, 2)\n",
      "\n",
      "progress: 677/876\n",
      "NLSI0000005\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 33042]\n",
      "level_dim [51835 33042]\n",
      "name NLSI0000005\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000005.h5: total of 27 batches\n",
      "batch 0/27, 0 files processed\n",
      "batch 20/27, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000005.h5 took 13.134409666061401 s\n",
      "features size:  (13573, 1024)\n",
      "coordinates size:  (13573, 2)\n",
      "\n",
      "progress: 678/876\n",
      "NLSI0000006\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [43860 36652]\n",
      "level_dim [43860 36652]\n",
      "name NLSI0000006\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000006.h5: total of 24 batches\n",
      "batch 0/24, 0 files processed\n",
      "batch 20/24, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000006.h5 took 11.857820510864258 s\n",
      "features size:  (12128, 1024)\n",
      "coordinates size:  (12128, 2)\n",
      "\n",
      "progress: 679/876\n",
      "NLSI0000023\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57816 37862]\n",
      "level_dim [57816 37862]\n",
      "name NLSI0000023\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000023.h5: total of 23 batches\n",
      "batch 0/23, 0 files processed\n",
      "batch 20/23, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000023.h5 took 11.320443630218506 s\n",
      "features size:  (11758, 1024)\n",
      "coordinates size:  (11758, 2)\n",
      "\n",
      "progress: 680/876\n",
      "NLSI0000024\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [49841 33285]\n",
      "level_dim [49841 33285]\n",
      "name NLSI0000024\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000024.h5: total of 21 batches\n",
      "batch 0/21, 0 files processed\n",
      "batch 20/21, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000024.h5 took 10.444895029067993 s\n",
      "features size:  (10590, 1024)\n",
      "coordinates size:  (10590, 2)\n",
      "\n",
      "progress: 681/876\n",
      "NLSI0000025\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [49841 42241]\n",
      "level_dim [49841 42241]\n",
      "name NLSI0000025\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000025.h5: total of 30 batches\n",
      "batch 0/30, 0 files processed\n",
      "batch 20/30, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000025.h5 took 13.821398973464966 s\n",
      "features size:  (15015, 1024)\n",
      "coordinates size:  (15015, 2)\n",
      "\n",
      "progress: 682/876\n",
      "NLSI0000026\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [41867 38912]\n",
      "level_dim [41867 38912]\n",
      "name NLSI0000026\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000026.h5: total of 23 batches\n",
      "batch 0/23, 0 files processed\n",
      "batch 20/23, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000026.h5 took 11.597105026245117 s\n",
      "features size:  (11380, 1024)\n",
      "coordinates size:  (11380, 2)\n",
      "\n",
      "progress: 683/876\n",
      "NLSI0000027\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [41867 39674]\n",
      "level_dim [41867 39674]\n",
      "name NLSI0000027\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000027.h5: total of 22 batches\n",
      "batch 0/22, 0 files processed\n",
      "batch 20/22, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000027.h5 took 11.142409563064575 s\n",
      "features size:  (11040, 1024)\n",
      "coordinates size:  (11040, 2)\n",
      "\n",
      "progress: 684/876\n",
      "NLSI0000029\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [53829 43773]\n",
      "level_dim [53829 43773]\n",
      "name NLSI0000029\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000029.h5: total of 34 batches\n",
      "batch 0/34, 0 files processed\n",
      "batch 20/34, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000029.h5 took 15.878462076187134 s\n",
      "features size:  (17335, 1024)\n",
      "coordinates size:  (17335, 2)\n",
      "\n",
      "progress: 685/876\n",
      "NLSI0000030\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [27911 29764]\n",
      "level_dim [27911 29764]\n",
      "name NLSI0000030\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000030.h5: total of 9 batches\n",
      "batch 0/9, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000030.h5 took 5.671620607376099 s\n",
      "features size:  (4338, 1024)\n",
      "coordinates size:  (4338, 2)\n",
      "\n",
      "progress: 686/876\n",
      "NLSI0000031\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [41867 35631]\n",
      "level_dim [41867 35631]\n",
      "name NLSI0000031\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000031.h5: total of 18 batches\n",
      "batch 0/18, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000031.h5 took 8.836529016494751 s\n",
      "features size:  (8866, 1024)\n",
      "coordinates size:  (8866, 2)\n",
      "\n",
      "progress: 687/876\n",
      "NLSI0000032\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [47848 42597]\n",
      "level_dim [47848 42597]\n",
      "name NLSI0000032\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000032.h5: total of 27 batches\n",
      "batch 0/27, 0 files processed\n",
      "batch 20/27, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000032.h5 took 13.449671268463135 s\n",
      "features size:  (13609, 1024)\n",
      "coordinates size:  (13609, 2)\n",
      "\n",
      "progress: 688/876\n",
      "NLSI0000033\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [49841 39212]\n",
      "level_dim [49841 39212]\n",
      "name NLSI0000033\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000033.h5: total of 31 batches\n",
      "batch 0/31, 0 files processed\n",
      "batch 20/31, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000033.h5 took 14.859271049499512 s\n",
      "features size:  (15687, 1024)\n",
      "coordinates size:  (15687, 2)\n",
      "\n",
      "progress: 689/876\n",
      "NLSI0000034\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 38716]\n",
      "level_dim [51835 38716]\n",
      "name NLSI0000034\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000034.h5: total of 21 batches\n",
      "batch 0/21, 0 files processed\n",
      "batch 20/21, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000034.h5 took 10.25436782836914 s\n",
      "features size:  (10588, 1024)\n",
      "coordinates size:  (10588, 2)\n",
      "\n",
      "progress: 690/876\n",
      "NLSI0000035\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [73766 31602]\n",
      "level_dim [73766 31602]\n",
      "name NLSI0000035\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000035.h5: total of 18 batches\n",
      "batch 0/18, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000035.h5 took 9.050824165344238 s\n",
      "features size:  (9003, 1024)\n",
      "coordinates size:  (9003, 2)\n",
      "\n",
      "progress: 691/876\n",
      "NLSI0000036\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [33892 32189]\n",
      "level_dim [33892 32189]\n",
      "name NLSI0000036\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000036.h5: total of 17 batches\n",
      "batch 0/17, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000036.h5 took 9.337759971618652 s\n",
      "features size:  (8376, 1024)\n",
      "coordinates size:  (8376, 2)\n",
      "\n",
      "progress: 692/876\n",
      "NLSI0000037\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [41867 31476]\n",
      "level_dim [41867 31476]\n",
      "name NLSI0000037\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000037.h5: total of 16 batches\n",
      "batch 0/16, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000037.h5 took 9.006030797958374 s\n",
      "features size:  (7978, 1024)\n",
      "coordinates size:  (7978, 2)\n",
      "\n",
      "progress: 693/876\n",
      "NLSI0000041\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [47848 35795]\n",
      "level_dim [47848 35795]\n",
      "name NLSI0000041\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000041.h5: total of 24 batches\n",
      "batch 0/24, 0 files processed\n",
      "batch 20/24, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000041.h5 took 11.774765729904175 s\n",
      "features size:  (11952, 1024)\n",
      "coordinates size:  (11952, 2)\n",
      "\n",
      "progress: 694/876\n",
      "NLSI0000042\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [31898 25747]\n",
      "level_dim [31898 25747]\n",
      "name NLSI0000042\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000042.h5: total of 9 batches\n",
      "batch 0/9, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000042.h5 took 5.8655805587768555 s\n",
      "features size:  (4285, 1024)\n",
      "coordinates size:  (4285, 2)\n",
      "\n",
      "progress: 695/876\n",
      "NLSI0000043\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [43860 36154]\n",
      "level_dim [43860 36154]\n",
      "name NLSI0000043\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000043.h5: total of 22 batches\n",
      "batch 0/22, 0 files processed\n",
      "batch 20/22, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000043.h5 took 11.129743814468384 s\n",
      "features size:  (10757, 1024)\n",
      "coordinates size:  (10757, 2)\n",
      "\n",
      "progress: 696/876\n",
      "NLSI0000044\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [35886 21356]\n",
      "level_dim [35886 21356]\n",
      "name NLSI0000044\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000044.h5: total of 12 batches\n",
      "batch 0/12, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000044.h5 took 7.165029525756836 s\n",
      "features size:  (5743, 1024)\n",
      "coordinates size:  (5743, 2)\n",
      "\n",
      "progress: 697/876\n",
      "NLSI0000045\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [53829 30904]\n",
      "level_dim [53829 30904]\n",
      "name NLSI0000045\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000045.h5: total of 28 batches\n",
      "batch 0/28, 0 files processed\n",
      "batch 20/28, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000045.h5 took 13.637145042419434 s\n",
      "features size:  (14098, 1024)\n",
      "coordinates size:  (14098, 2)\n",
      "\n",
      "progress: 698/876\n",
      "NLSI0000046\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [61803 44318]\n",
      "level_dim [61803 44318]\n",
      "name NLSI0000046\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000046.h5: total of 51 batches\n",
      "batch 0/51, 0 files processed\n",
      "batch 20/51, 10240 files processed\n",
      "batch 40/51, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000046.h5 took 22.924479007720947 s\n",
      "features size:  (25641, 1024)\n",
      "coordinates size:  (25641, 2)\n",
      "\n",
      "progress: 699/876\n",
      "NLSI0000047\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [47848 27479]\n",
      "level_dim [47848 27479]\n",
      "name NLSI0000047\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000047.h5: total of 17 batches\n",
      "batch 0/17, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000047.h5 took 8.613451480865479 s\n",
      "features size:  (8469, 1024)\n",
      "coordinates size:  (8469, 2)\n",
      "\n",
      "progress: 700/876\n",
      "NLSI0000048\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [59810 46826]\n",
      "level_dim [59810 46826]\n",
      "name NLSI0000048\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000048.h5: total of 53 batches\n",
      "batch 0/53, 0 files processed\n",
      "batch 20/53, 10240 files processed\n",
      "batch 40/53, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000048.h5 took 23.976276397705078 s\n",
      "features size:  (27072, 1024)\n",
      "coordinates size:  (27072, 2)\n",
      "\n",
      "progress: 701/876\n",
      "NLSI0000049\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [61803 37763]\n",
      "level_dim [61803 37763]\n",
      "name NLSI0000049\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000049.h5: total of 40 batches\n",
      "batch 0/40, 0 files processed\n",
      "batch 20/40, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000049.h5 took 17.38081431388855 s\n",
      "features size:  (20256, 1024)\n",
      "coordinates size:  (20256, 2)\n",
      "\n",
      "progress: 702/876\n",
      "NLSI0000050\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57816 39759]\n",
      "level_dim [57816 39759]\n",
      "name NLSI0000050\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000050.h5: total of 41 batches\n",
      "batch 0/41, 0 files processed\n",
      "batch 20/41, 10240 files processed\n",
      "batch 40/41, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000050.h5 took 18.268891096115112 s\n",
      "features size:  (20955, 1024)\n",
      "coordinates size:  (20955, 2)\n",
      "\n",
      "progress: 703/876\n",
      "NLSI0000051\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [41867 36513]\n",
      "level_dim [41867 36513]\n",
      "name NLSI0000051\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000051.h5: total of 4 batches\n",
      "batch 0/4, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000051.h5 took 3.379241943359375 s\n",
      "features size:  (1539, 1024)\n",
      "coordinates size:  (1539, 2)\n",
      "\n",
      "progress: 704/876\n",
      "NLSI0000054\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [43860 30598]\n",
      "level_dim [43860 30598]\n",
      "name NLSI0000054\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000054.h5: total of 13 batches\n",
      "batch 0/13, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000054.h5 took 7.470207691192627 s\n",
      "features size:  (6593, 1024)\n",
      "coordinates size:  (6593, 2)\n",
      "\n",
      "progress: 705/876\n",
      "NLSI0000055\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57816 35472]\n",
      "level_dim [57816 35472]\n",
      "name NLSI0000055\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000055.h5: total of 28 batches\n",
      "batch 0/28, 0 files processed\n",
      "batch 20/28, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000055.h5 took 12.944516897201538 s\n",
      "features size:  (13846, 1024)\n",
      "coordinates size:  (13846, 2)\n",
      "\n",
      "progress: 706/876\n",
      "NLSI0000062\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [49841 44003]\n",
      "level_dim [49841 44003]\n",
      "name NLSI0000062\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000062.h5: total of 43 batches\n",
      "batch 0/43, 0 files processed\n",
      "batch 20/43, 10240 files processed\n",
      "batch 40/43, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000062.h5 took 20.08946418762207 s\n",
      "features size:  (21711, 1024)\n",
      "coordinates size:  (21711, 2)\n",
      "\n",
      "progress: 707/876\n",
      "NLSI0000063\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 46169]\n",
      "level_dim [51835 46169]\n",
      "name NLSI0000063\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000063.h5: total of 42 batches\n",
      "batch 0/42, 0 files processed\n",
      "batch 20/42, 10240 files processed\n",
      "batch 40/42, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000063.h5 took 17.994298219680786 s\n",
      "features size:  (21105, 1024)\n",
      "coordinates size:  (21105, 2)\n",
      "\n",
      "progress: 708/876\n",
      "NLSI0000064\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [53829 33680]\n",
      "level_dim [53829 33680]\n",
      "name NLSI0000064\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000064.h5: total of 28 batches\n",
      "batch 0/28, 0 files processed\n",
      "batch 20/28, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000064.h5 took 13.156043767929077 s\n",
      "features size:  (14306, 1024)\n",
      "coordinates size:  (14306, 2)\n",
      "\n",
      "progress: 709/876\n",
      "NLSI0000068\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [41867 34624]\n",
      "level_dim [41867 34624]\n",
      "name NLSI0000068\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000068.h5: total of 22 batches\n",
      "batch 0/22, 0 files processed\n",
      "batch 20/22, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000068.h5 took 10.492753982543945 s\n",
      "features size:  (11018, 1024)\n",
      "coordinates size:  (11018, 2)\n",
      "\n",
      "progress: 710/876\n",
      "NLSI0000069\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [61803 45749]\n",
      "level_dim [61803 45749]\n",
      "name NLSI0000069\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000069.h5: total of 52 batches\n",
      "batch 0/52, 0 files processed\n",
      "batch 20/52, 10240 files processed\n",
      "batch 40/52, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000069.h5 took 23.371068239212036 s\n",
      "features size:  (26374, 1024)\n",
      "coordinates size:  (26374, 2)\n",
      "\n",
      "progress: 711/876\n",
      "NLSI0000070\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [37879 45833]\n",
      "level_dim [37879 45833]\n",
      "name NLSI0000070\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000070.h5: total of 28 batches\n",
      "batch 0/28, 0 files processed\n",
      "batch 20/28, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000070.h5 took 14.035619258880615 s\n",
      "features size:  (14212, 1024)\n",
      "coordinates size:  (14212, 2)\n",
      "\n",
      "progress: 712/876\n",
      "NLSI0000071\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [37879 35991]\n",
      "level_dim [37879 35991]\n",
      "name NLSI0000071\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000071.h5: total of 25 batches\n",
      "batch 0/25, 0 files processed\n",
      "batch 20/25, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000071.h5 took 12.602469682693481 s\n",
      "features size:  (12654, 1024)\n",
      "coordinates size:  (12654, 2)\n",
      "\n",
      "progress: 713/876\n",
      "NLSI0000072\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [35886 36609]\n",
      "level_dim [35886 36609]\n",
      "name NLSI0000072\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000072.h5: total of 21 batches\n",
      "batch 0/21, 0 files processed\n",
      "batch 20/21, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000072.h5 took 10.503114461898804 s\n",
      "features size:  (10736, 1024)\n",
      "coordinates size:  (10736, 2)\n",
      "\n",
      "progress: 714/876\n",
      "NLSI0000073\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [41867 42692]\n",
      "level_dim [41867 42692]\n",
      "name NLSI0000073\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000073.h5: total of 27 batches\n",
      "batch 0/27, 0 files processed\n",
      "batch 20/27, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000073.h5 took 12.731807231903076 s\n",
      "features size:  (13348, 1024)\n",
      "coordinates size:  (13348, 2)\n",
      "\n",
      "progress: 715/876\n",
      "NLSI0000074\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [47848 31255]\n",
      "level_dim [47848 31255]\n",
      "name NLSI0000074\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000074.h5: total of 21 batches\n",
      "batch 0/21, 0 files processed\n",
      "batch 20/21, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000074.h5 took 10.747377157211304 s\n",
      "features size:  (10568, 1024)\n",
      "coordinates size:  (10568, 2)\n",
      "\n",
      "progress: 716/876\n",
      "NLSI0000075\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [41867 38533]\n",
      "level_dim [41867 38533]\n",
      "name NLSI0000075\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000075.h5: total of 27 batches\n",
      "batch 0/27, 0 files processed\n",
      "batch 20/27, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000075.h5 took 13.178029775619507 s\n",
      "features size:  (13411, 1024)\n",
      "coordinates size:  (13411, 2)\n",
      "\n",
      "progress: 717/876\n",
      "NLSI0000076\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [41867 40931]\n",
      "level_dim [41867 40931]\n",
      "name NLSI0000076\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000076.h5: total of 30 batches\n",
      "batch 0/30, 0 files processed\n",
      "batch 20/30, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000076.h5 took 13.751859188079834 s\n",
      "features size:  (15060, 1024)\n",
      "coordinates size:  (15060, 2)\n",
      "\n",
      "progress: 718/876\n",
      "NLSI0000077\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [55822 38805]\n",
      "level_dim [55822 38805]\n",
      "name NLSI0000077\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000077.h5: total of 36 batches\n",
      "batch 0/36, 0 files processed\n",
      "batch 20/36, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000077.h5 took 16.56086277961731 s\n",
      "features size:  (18251, 1024)\n",
      "coordinates size:  (18251, 2)\n",
      "\n",
      "progress: 719/876\n",
      "NLSI0000078\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [43860 45987]\n",
      "level_dim [43860 45987]\n",
      "name NLSI0000078\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000078.h5: total of 33 batches\n",
      "batch 0/33, 0 files processed\n",
      "batch 20/33, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000078.h5 took 14.895615100860596 s\n",
      "features size:  (16842, 1024)\n",
      "coordinates size:  (16842, 2)\n",
      "\n",
      "progress: 720/876\n",
      "NLSI0000079-001\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [47848 35543]\n",
      "level_dim [47848 35543]\n",
      "name NLSI0000079-001\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000079-001.h5: total of 29 batches\n",
      "batch 0/29, 0 files processed\n",
      "batch 20/29, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000079-001.h5 took 14.339487791061401 s\n",
      "features size:  (14662, 1024)\n",
      "coordinates size:  (14662, 2)\n",
      "\n",
      "progress: 721/876\n",
      "NLSI0000080\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [45854 33013]\n",
      "level_dim [45854 33013]\n",
      "name NLSI0000080\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000080.h5: total of 16 batches\n",
      "batch 0/16, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000080.h5 took 8.416621923446655 s\n",
      "features size:  (8007, 1024)\n",
      "coordinates size:  (8007, 2)\n",
      "\n",
      "progress: 722/876\n",
      "NLSI0000081\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [37879 34987]\n",
      "level_dim [37879 34987]\n",
      "name NLSI0000081\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000081.h5: total of 20 batches\n",
      "batch 0/20, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000081.h5 took 10.211268424987793 s\n",
      "features size:  (10165, 1024)\n",
      "coordinates size:  (10165, 2)\n",
      "\n",
      "progress: 723/876\n",
      "NLSI0000082\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [55822 35588]\n",
      "level_dim [55822 35588]\n",
      "name NLSI0000082\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000082.h5: total of 26 batches\n",
      "batch 0/26, 0 files processed\n",
      "batch 20/26, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000082.h5 took 11.787513494491577 s\n",
      "features size:  (13226, 1024)\n",
      "coordinates size:  (13226, 2)\n",
      "\n",
      "progress: 724/876\n",
      "NLSI0000085-001\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [47848 42223]\n",
      "level_dim [47848 42223]\n",
      "name NLSI0000085-001\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000085-001.h5: total of 27 batches\n",
      "batch 0/27, 0 files processed\n",
      "batch 20/27, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000085-001.h5 took 13.162528038024902 s\n",
      "features size:  (13763, 1024)\n",
      "coordinates size:  (13763, 2)\n",
      "\n",
      "progress: 725/876\n",
      "NLSI0000086\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [61803 41980]\n",
      "level_dim [61803 41980]\n",
      "name NLSI0000086\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000086.h5: total of 44 batches\n",
      "batch 0/44, 0 files processed\n",
      "batch 20/44, 10240 files processed\n",
      "batch 40/44, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000086.h5 took 19.854275226593018 s\n",
      "features size:  (22113, 1024)\n",
      "coordinates size:  (22113, 2)\n",
      "\n",
      "progress: 726/876\n",
      "NLSI0000090\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 39346]\n",
      "level_dim [51835 39346]\n",
      "name NLSI0000090\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000090.h5: total of 30 batches\n",
      "batch 0/30, 0 files processed\n",
      "batch 20/30, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000090.h5 took 14.544578552246094 s\n",
      "features size:  (15193, 1024)\n",
      "coordinates size:  (15193, 2)\n",
      "\n",
      "progress: 727/876\n",
      "NLSI0000091\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [59810 44556]\n",
      "level_dim [59810 44556]\n",
      "name NLSI0000091\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000091.h5: total of 45 batches\n",
      "batch 0/45, 0 files processed\n",
      "batch 20/45, 10240 files processed\n",
      "batch 40/45, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000091.h5 took 21.08052372932434 s\n",
      "features size:  (22594, 1024)\n",
      "coordinates size:  (22594, 2)\n",
      "\n",
      "progress: 728/876\n",
      "NLSI0000092\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [53829 38722]\n",
      "level_dim [53829 38722]\n",
      "name NLSI0000092\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000092.h5: total of 36 batches\n",
      "batch 0/36, 0 files processed\n",
      "batch 20/36, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000092.h5 took 17.283750772476196 s\n",
      "features size:  (18376, 1024)\n",
      "coordinates size:  (18376, 2)\n",
      "\n",
      "progress: 729/876\n",
      "NLSI0000098\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [39873 40799]\n",
      "level_dim [39873 40799]\n",
      "name NLSI0000098\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000098.h5: total of 20 batches\n",
      "batch 0/20, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000098.h5 took 9.914660215377808 s\n",
      "features size:  (9973, 1024)\n",
      "coordinates size:  (9973, 2)\n",
      "\n",
      "progress: 730/876\n",
      "NLSI0000099\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 39347]\n",
      "level_dim [51835 39347]\n",
      "name NLSI0000099\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000099.h5: total of 27 batches\n",
      "batch 0/27, 0 files processed\n",
      "batch 20/27, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000099.h5 took 12.495322227478027 s\n",
      "features size:  (13606, 1024)\n",
      "coordinates size:  (13606, 2)\n",
      "\n",
      "progress: 731/876\n",
      "NLSI0000100\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [19936 24453]\n",
      "level_dim [19936 24453]\n",
      "name NLSI0000100\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000100.h5: total of 7 batches\n",
      "batch 0/7, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000100.h5 took 4.555880784988403 s\n",
      "features size:  (3290, 1024)\n",
      "coordinates size:  (3290, 2)\n",
      "\n",
      "progress: 732/876\n",
      "NLSI0000101\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 41871]\n",
      "level_dim [51835 41871]\n",
      "name NLSI0000101\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000101.h5: total of 33 batches\n",
      "batch 0/33, 0 files processed\n",
      "batch 20/33, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000101.h5 took 14.458012580871582 s\n",
      "features size:  (16582, 1024)\n",
      "coordinates size:  (16582, 2)\n",
      "\n",
      "progress: 733/876\n",
      "NLSI0000104\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 35315]\n",
      "level_dim [51835 35315]\n",
      "name NLSI0000104\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000104.h5: total of 20 batches\n",
      "batch 0/20, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000104.h5 took 10.1046142578125 s\n",
      "features size:  (9945, 1024)\n",
      "coordinates size:  (9945, 2)\n",
      "\n",
      "progress: 734/876\n",
      "NLSI0000105\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [17943 13483]\n",
      "level_dim [17943 13483]\n",
      "name NLSI0000105\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000105.h5: total of 4 batches\n",
      "batch 0/4, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000105.h5 took 3.524618148803711 s\n",
      "features size:  (1865, 1024)\n",
      "coordinates size:  (1865, 2)\n",
      "\n",
      "progress: 735/876\n",
      "NLSI0000106\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [49841 43936]\n",
      "level_dim [49841 43936]\n",
      "name NLSI0000106\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000106.h5: total of 32 batches\n",
      "batch 0/32, 0 files processed\n",
      "batch 20/32, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000106.h5 took 14.40477442741394 s\n",
      "features size:  (15976, 1024)\n",
      "coordinates size:  (15976, 2)\n",
      "\n",
      "progress: 736/876\n",
      "NLSI0000107\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [39873 32099]\n",
      "level_dim [39873 32099]\n",
      "name NLSI0000107\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000107.h5: total of 26 batches\n",
      "batch 0/26, 0 files processed\n",
      "batch 20/26, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000107.h5 took 12.840392589569092 s\n",
      "features size:  (12948, 1024)\n",
      "coordinates size:  (12948, 2)\n",
      "\n",
      "progress: 737/876\n",
      "NLSI0000108\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 42877]\n",
      "level_dim [51835 42877]\n",
      "name NLSI0000108\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000108.h5: total of 46 batches\n",
      "batch 0/46, 0 files processed\n",
      "batch 20/46, 10240 files processed\n",
      "batch 40/46, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000108.h5 took 21.52140164375305 s\n",
      "features size:  (23277, 1024)\n",
      "coordinates size:  (23277, 2)\n",
      "\n",
      "progress: 738/876\n",
      "NLSI0000109\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [59810 38246]\n",
      "level_dim [59810 38246]\n",
      "name NLSI0000109\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000109.h5: total of 44 batches\n",
      "batch 0/44, 0 files processed\n",
      "batch 20/44, 10240 files processed\n",
      "batch 40/44, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000109.h5 took 18.61023449897766 s\n",
      "features size:  (22171, 1024)\n",
      "coordinates size:  (22171, 2)\n",
      "\n",
      "progress: 739/876\n",
      "NLSI0000110\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57816 37608]\n",
      "level_dim [57816 37608]\n",
      "name NLSI0000110\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000110.h5: total of 38 batches\n",
      "batch 0/38, 0 files processed\n",
      "batch 20/38, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000110.h5 took 17.366161108016968 s\n",
      "features size:  (19013, 1024)\n",
      "coordinates size:  (19013, 2)\n",
      "\n",
      "progress: 740/876\n",
      "NLSI0000111\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [53829 39354]\n",
      "level_dim [53829 39354]\n",
      "name NLSI0000111\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000111.h5: total of 33 batches\n",
      "batch 0/33, 0 files processed\n",
      "batch 20/33, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000111.h5 took 15.794671535491943 s\n",
      "features size:  (16585, 1024)\n",
      "coordinates size:  (16585, 2)\n",
      "\n",
      "progress: 741/876\n",
      "NLSI0000112\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [49841 42738]\n",
      "level_dim [49841 42738]\n",
      "name NLSI0000112\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000112.h5: total of 35 batches\n",
      "batch 0/35, 0 files processed\n",
      "batch 20/35, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000112.h5 took 15.376998901367188 s\n",
      "features size:  (17770, 1024)\n",
      "coordinates size:  (17770, 2)\n",
      "\n",
      "progress: 742/876\n",
      "NLSI0000113\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 25730]\n",
      "level_dim [51835 25730]\n",
      "name NLSI0000113\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000113.h5: total of 23 batches\n",
      "batch 0/23, 0 files processed\n",
      "batch 20/23, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000113.h5 took 11.123887777328491 s\n",
      "features size:  (11302, 1024)\n",
      "coordinates size:  (11302, 2)\n",
      "\n",
      "progress: 743/876\n",
      "NLSI0000114\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [41867 28575]\n",
      "level_dim [41867 28575]\n",
      "name NLSI0000114\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000114.h5: total of 15 batches\n",
      "batch 0/15, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000114.h5 took 7.9835193157196045 s\n",
      "features size:  (7493, 1024)\n",
      "coordinates size:  (7493, 2)\n",
      "\n",
      "progress: 744/876\n",
      "NLSI0000115\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [53829 37083]\n",
      "level_dim [53829 37083]\n",
      "name NLSI0000115\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000115.h5: total of 33 batches\n",
      "batch 0/33, 0 files processed\n",
      "batch 20/33, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000115.h5 took 15.538858652114868 s\n",
      "features size:  (16737, 1024)\n",
      "coordinates size:  (16737, 2)\n",
      "\n",
      "progress: 745/876\n",
      "NLSI0000116\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [41867 38037]\n",
      "level_dim [41867 38037]\n",
      "name NLSI0000116\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000116.h5: total of 28 batches\n",
      "batch 0/28, 0 files processed\n",
      "batch 20/28, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000116.h5 took 13.44357442855835 s\n",
      "features size:  (14168, 1024)\n",
      "coordinates size:  (14168, 2)\n",
      "\n",
      "progress: 746/876\n",
      "NLSI0000117\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [65791 40425]\n",
      "level_dim [65791 40425]\n",
      "name NLSI0000117\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000117.h5: total of 42 batches\n",
      "batch 0/42, 0 files processed\n",
      "batch 20/42, 10240 files processed\n",
      "batch 40/42, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000117.h5 took 18.980255842208862 s\n",
      "features size:  (21166, 1024)\n",
      "coordinates size:  (21166, 2)\n",
      "\n",
      "progress: 747/876\n",
      "NLSI0000118\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 39599]\n",
      "level_dim [51835 39599]\n",
      "name NLSI0000118\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000118.h5: total of 25 batches\n",
      "batch 0/25, 0 files processed\n",
      "batch 20/25, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000118.h5 took 12.493175506591797 s\n",
      "features size:  (12656, 1024)\n",
      "coordinates size:  (12656, 2)\n",
      "\n",
      "progress: 748/876\n",
      "NLSI0000119\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [67784 41701]\n",
      "level_dim [67784 41701]\n",
      "name NLSI0000119\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000119.h5: total of 45 batches\n",
      "batch 0/45, 0 files processed\n",
      "batch 20/45, 10240 files processed\n",
      "batch 40/45, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000119.h5 took 20.319400548934937 s\n",
      "features size:  (22570, 1024)\n",
      "coordinates size:  (22570, 2)\n",
      "\n",
      "progress: 749/876\n",
      "NLSI0000120\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [59810 46828]\n",
      "level_dim [59810 46828]\n",
      "name NLSI0000120\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000120.h5: total of 70 batches\n",
      "batch 0/70, 0 files processed\n",
      "batch 20/70, 10240 files processed\n",
      "batch 40/70, 20480 files processed\n",
      "batch 60/70, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000120.h5 took 30.587433576583862 s\n",
      "features size:  (35706, 1024)\n",
      "coordinates size:  (35706, 2)\n",
      "\n",
      "progress: 750/876\n",
      "NLSI0000121\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57816 45176]\n",
      "level_dim [57816 45176]\n",
      "name NLSI0000121\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000121.h5: total of 60 batches\n",
      "batch 0/60, 0 files processed\n",
      "batch 20/60, 10240 files processed\n",
      "batch 40/60, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000121.h5 took 26.135772943496704 s\n",
      "features size:  (30593, 1024)\n",
      "coordinates size:  (30593, 2)\n",
      "\n",
      "progress: 751/876\n",
      "NLSI0000125\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [53829 43139]\n",
      "level_dim [53829 43139]\n",
      "name NLSI0000125\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000125.h5: total of 34 batches\n",
      "batch 0/34, 0 files processed\n",
      "batch 20/34, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000125.h5 took 15.671132802963257 s\n",
      "features size:  (17349, 1024)\n",
      "coordinates size:  (17349, 2)\n",
      "\n",
      "progress: 752/876\n",
      "NLSI0000126\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [49841 40344]\n",
      "level_dim [49841 40344]\n",
      "name NLSI0000126\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000126.h5: total of 42 batches\n",
      "batch 0/42, 0 files processed\n",
      "batch 20/42, 10240 files processed\n",
      "batch 40/42, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000126.h5 took 18.522990226745605 s\n",
      "features size:  (21080, 1024)\n",
      "coordinates size:  (21080, 2)\n",
      "\n",
      "progress: 753/876\n",
      "NLSI0000127\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [65791 41937]\n",
      "level_dim [65791 41937]\n",
      "name NLSI0000127\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000127.h5: total of 30 batches\n",
      "batch 0/30, 0 files processed\n",
      "batch 20/30, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000127.h5 took 14.100357294082642 s\n",
      "features size:  (15201, 1024)\n",
      "coordinates size:  (15201, 2)\n",
      "\n",
      "progress: 754/876\n",
      "NLSI0000128\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 44767]\n",
      "level_dim [51835 44767]\n",
      "name NLSI0000128\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000128.h5: total of 49 batches\n",
      "batch 0/49, 0 files processed\n",
      "batch 20/49, 10240 files processed\n",
      "batch 40/49, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000128.h5 took 22.668514251708984 s\n",
      "features size:  (24868, 1024)\n",
      "coordinates size:  (24868, 2)\n",
      "\n",
      "progress: 755/876\n",
      "NLSI0000129\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [53829 43516]\n",
      "level_dim [53829 43516]\n",
      "name NLSI0000129\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000129.h5: total of 47 batches\n",
      "batch 0/47, 0 files processed\n",
      "batch 20/47, 10240 files processed\n",
      "batch 40/47, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000129.h5 took 21.452871799468994 s\n",
      "features size:  (23777, 1024)\n",
      "coordinates size:  (23777, 2)\n",
      "\n",
      "progress: 756/876\n",
      "NLSI0000130\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57816 41896]\n",
      "level_dim [57816 41896]\n",
      "name NLSI0000130\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000130.h5: total of 38 batches\n",
      "batch 0/38, 0 files processed\n",
      "batch 20/38, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000130.h5 took 18.37629008293152 s\n",
      "features size:  (19307, 1024)\n",
      "coordinates size:  (19307, 2)\n",
      "\n",
      "progress: 757/876\n",
      "NLSI0000131\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [55822 37603]\n",
      "level_dim [55822 37603]\n",
      "name NLSI0000131\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000131.h5: total of 25 batches\n",
      "batch 0/25, 0 files processed\n",
      "batch 20/25, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000131.h5 took 12.013877868652344 s\n",
      "features size:  (12784, 1024)\n",
      "coordinates size:  (12784, 2)\n",
      "\n",
      "progress: 758/876\n",
      "NLSI0000132\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [69778 45498]\n",
      "level_dim [69778 45498]\n",
      "name NLSI0000132\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000132.h5: total of 62 batches\n",
      "batch 0/62, 0 files processed\n",
      "batch 20/62, 10240 files processed\n",
      "batch 40/62, 20480 files processed\n",
      "batch 60/62, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000132.h5 took 26.338033437728882 s\n",
      "features size:  (31661, 1024)\n",
      "coordinates size:  (31661, 2)\n",
      "\n",
      "progress: 759/876\n",
      "NLSI0000133\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [47848 31631]\n",
      "level_dim [47848 31631]\n",
      "name NLSI0000133\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000133.h5: total of 22 batches\n",
      "batch 0/22, 0 files processed\n",
      "batch 20/22, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000133.h5 took 10.978164434432983 s\n",
      "features size:  (10962, 1024)\n",
      "coordinates size:  (10962, 2)\n",
      "\n",
      "progress: 760/876\n",
      "NLSI0000134\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57816 29577]\n",
      "level_dim [57816 29577]\n",
      "name NLSI0000134\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000134.h5: total of 31 batches\n",
      "batch 0/31, 0 files processed\n",
      "batch 20/31, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000134.h5 took 15.242175817489624 s\n",
      "features size:  (15588, 1024)\n",
      "coordinates size:  (15588, 2)\n",
      "\n",
      "progress: 761/876\n",
      "NLSI0000135\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 38460]\n",
      "level_dim [51835 38460]\n",
      "name NLSI0000135\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000135.h5: total of 38 batches\n",
      "batch 0/38, 0 files processed\n",
      "batch 20/38, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000135.h5 took 18.01717782020569 s\n",
      "features size:  (19223, 1024)\n",
      "coordinates size:  (19223, 2)\n",
      "\n",
      "progress: 762/876\n",
      "NLSI0000136\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [55822 36214]\n",
      "level_dim [55822 36214]\n",
      "name NLSI0000136\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000136.h5: total of 37 batches\n",
      "batch 0/37, 0 files processed\n",
      "batch 20/37, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000136.h5 took 16.85968017578125 s\n",
      "features size:  (18807, 1024)\n",
      "coordinates size:  (18807, 2)\n",
      "\n",
      "progress: 763/876\n",
      "NLSI0000137\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [49841 36432]\n",
      "level_dim [49841 36432]\n",
      "name NLSI0000137\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000137.h5: total of 31 batches\n",
      "batch 0/31, 0 files processed\n",
      "batch 20/31, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000137.h5 took 14.933597087860107 s\n",
      "features size:  (15474, 1024)\n",
      "coordinates size:  (15474, 2)\n",
      "\n",
      "progress: 764/876\n",
      "NLSI0000142\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [55822 41514]\n",
      "level_dim [55822 41514]\n",
      "name NLSI0000142\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000142.h5: total of 46 batches\n",
      "batch 0/46, 0 files processed\n",
      "batch 20/46, 10240 files processed\n",
      "batch 40/46, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000142.h5 took 20.708443641662598 s\n",
      "features size:  (23233, 1024)\n",
      "coordinates size:  (23233, 2)\n",
      "\n",
      "progress: 765/876\n",
      "NLSI0000143\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 42750]\n",
      "level_dim [51835 42750]\n",
      "name NLSI0000143\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000143.h5: total of 38 batches\n",
      "batch 0/38, 0 files processed\n",
      "batch 20/38, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000143.h5 took 17.50503897666931 s\n",
      "features size:  (19443, 1024)\n",
      "coordinates size:  (19443, 2)\n",
      "\n",
      "progress: 766/876\n",
      "NLSI0000144\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57816 33821]\n",
      "level_dim [57816 33821]\n",
      "name NLSI0000144\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000144.h5: total of 35 batches\n",
      "batch 0/35, 0 files processed\n",
      "batch 20/35, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000144.h5 took 15.93630838394165 s\n",
      "features size:  (17625, 1024)\n",
      "coordinates size:  (17625, 2)\n",
      "\n",
      "progress: 767/876\n",
      "NLSI0000145\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 32154]\n",
      "level_dim [51835 32154]\n",
      "name NLSI0000145\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000145.h5: total of 24 batches\n",
      "batch 0/24, 0 files processed\n",
      "batch 20/24, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000145.h5 took 11.874671459197998 s\n",
      "features size:  (11968, 1024)\n",
      "coordinates size:  (11968, 2)\n",
      "\n",
      "progress: 768/876\n",
      "NLSI0000146\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [45854 33258]\n",
      "level_dim [45854 33258]\n",
      "name NLSI0000146\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000146.h5: total of 27 batches\n",
      "batch 0/27, 0 files processed\n",
      "batch 20/27, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000146.h5 took 13.171848058700562 s\n",
      "features size:  (13685, 1024)\n",
      "coordinates size:  (13685, 2)\n",
      "\n",
      "progress: 769/876\n",
      "NLSI0000147\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [47848 33159]\n",
      "level_dim [47848 33159]\n",
      "name NLSI0000147\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000147.h5: total of 34 batches\n",
      "batch 0/34, 0 files processed\n",
      "batch 20/34, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000147.h5 took 15.97775673866272 s\n",
      "features size:  (16995, 1024)\n",
      "coordinates size:  (16995, 2)\n",
      "\n",
      "progress: 770/876\n",
      "NLSI0000148\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [55822 30409]\n",
      "level_dim [55822 30409]\n",
      "name NLSI0000148\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000148.h5: total of 34 batches\n",
      "batch 0/34, 0 files processed\n",
      "batch 20/34, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000148.h5 took 15.295137882232666 s\n",
      "features size:  (16936, 1024)\n",
      "coordinates size:  (16936, 2)\n",
      "\n",
      "progress: 771/876\n",
      "NLSI0000149\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [43860 31357]\n",
      "level_dim [43860 31357]\n",
      "name NLSI0000149\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000149.h5: total of 23 batches\n",
      "batch 0/23, 0 files processed\n",
      "batch 20/23, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000149.h5 took 12.025161266326904 s\n",
      "features size:  (11563, 1024)\n",
      "coordinates size:  (11563, 2)\n",
      "\n",
      "progress: 772/876\n",
      "NLSI0000150\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [39873 31341]\n",
      "level_dim [39873 31341]\n",
      "name NLSI0000150\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000150.h5: total of 22 batches\n",
      "batch 0/22, 0 files processed\n",
      "batch 20/22, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000150.h5 took 11.103602170944214 s\n",
      "features size:  (10952, 1024)\n",
      "coordinates size:  (10952, 2)\n",
      "\n",
      "progress: 773/876\n",
      "NLSI0000151\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [49841 33183]\n",
      "level_dim [49841 33183]\n",
      "name NLSI0000151\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000151.h5: total of 28 batches\n",
      "batch 0/28, 0 files processed\n",
      "batch 20/28, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000151.h5 took 13.764718055725098 s\n",
      "features size:  (14291, 1024)\n",
      "coordinates size:  (14291, 2)\n",
      "\n",
      "progress: 774/876\n",
      "NLSI0000152\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [45854 30265]\n",
      "level_dim [45854 30265]\n",
      "name NLSI0000152\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000152.h5: total of 24 batches\n",
      "batch 0/24, 0 files processed\n",
      "batch 20/24, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000152.h5 took 11.596583366394043 s\n",
      "features size:  (11797, 1024)\n",
      "coordinates size:  (11797, 2)\n",
      "\n",
      "progress: 775/876\n",
      "NLSI0000153\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [37879 25815]\n",
      "level_dim [37879 25815]\n",
      "name NLSI0000153\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000153.h5: total of 14 batches\n",
      "batch 0/14, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000153.h5 took 7.6857733726501465 s\n",
      "features size:  (6777, 1024)\n",
      "coordinates size:  (6777, 2)\n",
      "\n",
      "progress: 776/876\n",
      "NLSI0000154\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57816 36128]\n",
      "level_dim [57816 36128]\n",
      "name NLSI0000154\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000154.h5: total of 37 batches\n",
      "batch 0/37, 0 files processed\n",
      "batch 20/37, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000154.h5 took 17.12389349937439 s\n",
      "features size:  (18810, 1024)\n",
      "coordinates size:  (18810, 2)\n",
      "\n",
      "progress: 777/876\n",
      "NLSI0000155\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 30781]\n",
      "level_dim [51835 30781]\n",
      "name NLSI0000155\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000155.h5: total of 25 batches\n",
      "batch 0/25, 0 files processed\n",
      "batch 20/25, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000155.h5 took 12.05092453956604 s\n",
      "features size:  (12673, 1024)\n",
      "coordinates size:  (12673, 2)\n",
      "\n",
      "progress: 778/876\n",
      "NLSI0000156\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [45854 30610]\n",
      "level_dim [45854 30610]\n",
      "name NLSI0000156\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000156.h5: total of 23 batches\n",
      "batch 0/23, 0 files processed\n",
      "batch 20/23, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000156.h5 took 11.060336351394653 s\n",
      "features size:  (11299, 1024)\n",
      "coordinates size:  (11299, 2)\n",
      "\n",
      "progress: 779/876\n",
      "NLSI0000159\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [19936 20880]\n",
      "level_dim [19936 20880]\n",
      "name NLSI0000159\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000159.h5: total of 3 batches\n",
      "batch 0/3, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000159.h5 took 2.898505210876465 s\n",
      "features size:  (1226, 1024)\n",
      "coordinates size:  (1226, 2)\n",
      "\n",
      "progress: 780/876\n",
      "NLSI0000160\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [47848 40457]\n",
      "level_dim [47848 40457]\n",
      "name NLSI0000160\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000160.h5: total of 28 batches\n",
      "batch 0/28, 0 files processed\n",
      "batch 20/28, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000160.h5 took 13.246711254119873 s\n",
      "features size:  (14142, 1024)\n",
      "coordinates size:  (14142, 2)\n",
      "\n",
      "progress: 781/876\n",
      "NLSI0000161\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57816 34582]\n",
      "level_dim [57816 34582]\n",
      "name NLSI0000161\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000161.h5: total of 32 batches\n",
      "batch 0/32, 0 files processed\n",
      "batch 20/32, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000161.h5 took 14.505100965499878 s\n",
      "features size:  (15912, 1024)\n",
      "coordinates size:  (15912, 2)\n",
      "\n",
      "progress: 782/876\n",
      "NLSI0000162-002\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [53829 41369]\n",
      "level_dim [53829 41369]\n",
      "name NLSI0000162-002\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000162-002.h5: total of 35 batches\n",
      "batch 0/35, 0 files processed\n",
      "batch 20/35, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000162-002.h5 took 16.289071798324585 s\n",
      "features size:  (17546, 1024)\n",
      "coordinates size:  (17546, 2)\n",
      "\n",
      "progress: 783/876\n",
      "NLSI0000173\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [35886 27060]\n",
      "level_dim [35886 27060]\n",
      "name NLSI0000173\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000173.h5: total of 13 batches\n",
      "batch 0/13, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000173.h5 took 7.806735992431641 s\n",
      "features size:  (6463, 1024)\n",
      "coordinates size:  (6463, 2)\n",
      "\n",
      "progress: 784/876\n",
      "NLSI0000174\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [31898 33058]\n",
      "level_dim [31898 33058]\n",
      "name NLSI0000174\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000174.h5: total of 10 batches\n",
      "batch 0/10, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000174.h5 took 5.916322469711304 s\n",
      "features size:  (4628, 1024)\n",
      "coordinates size:  (4628, 2)\n",
      "\n",
      "progress: 785/876\n",
      "NLSI0000181\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [39873 23768]\n",
      "level_dim [39873 23768]\n",
      "name NLSI0000181\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000181.h5: total of 19 batches\n",
      "batch 0/19, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000181.h5 took 9.872279167175293 s\n",
      "features size:  (9367, 1024)\n",
      "coordinates size:  (9367, 2)\n",
      "\n",
      "progress: 786/876\n",
      "NLSI0000182\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [47848 38183]\n",
      "level_dim [47848 38183]\n",
      "name NLSI0000182\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000182.h5: total of 35 batches\n",
      "batch 0/35, 0 files processed\n",
      "batch 20/35, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000182.h5 took 15.68801474571228 s\n",
      "features size:  (17711, 1024)\n",
      "coordinates size:  (17711, 2)\n",
      "\n",
      "progress: 787/876\n",
      "NLSI0000183\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [59810 37871]\n",
      "level_dim [59810 37871]\n",
      "name NLSI0000183\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000183.h5: total of 47 batches\n",
      "batch 0/47, 0 files processed\n",
      "batch 20/47, 10240 files processed\n",
      "batch 40/47, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000183.h5 took 22.32462167739868 s\n",
      "features size:  (23860, 1024)\n",
      "coordinates size:  (23860, 2)\n",
      "\n",
      "progress: 788/876\n",
      "NLSI0000184\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [45854 28344]\n",
      "level_dim [45854 28344]\n",
      "name NLSI0000184\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000184.h5: total of 16 batches\n",
      "batch 0/16, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000184.h5 took 8.367795944213867 s\n",
      "features size:  (8088, 1024)\n",
      "coordinates size:  (8088, 2)\n",
      "\n",
      "progress: 789/876\n",
      "NLSI0000185\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [43860 34885]\n",
      "level_dim [43860 34885]\n",
      "name NLSI0000185\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000185.h5: total of 18 batches\n",
      "batch 0/18, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000185.h5 took 9.572481632232666 s\n",
      "features size:  (9127, 1024)\n",
      "coordinates size:  (9127, 2)\n",
      "\n",
      "progress: 790/876\n",
      "NLSI0000186\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [45854 32253]\n",
      "level_dim [45854 32253]\n",
      "name NLSI0000186\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000186.h5: total of 19 batches\n",
      "batch 0/19, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000186.h5 took 9.522751808166504 s\n",
      "features size:  (9449, 1024)\n",
      "coordinates size:  (9449, 2)\n",
      "\n",
      "progress: 791/876\n",
      "NLSI0000187\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 29639]\n",
      "level_dim [51835 29639]\n",
      "name NLSI0000187\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000187.h5: total of 29 batches\n",
      "batch 0/29, 0 files processed\n",
      "batch 20/29, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000187.h5 took 13.468958854675293 s\n",
      "features size:  (14635, 1024)\n",
      "coordinates size:  (14635, 2)\n",
      "\n",
      "progress: 792/876\n",
      "NLSI0000190\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [41867 31223]\n",
      "level_dim [41867 31223]\n",
      "name NLSI0000190\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000190.h5: total of 20 batches\n",
      "batch 0/20, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000190.h5 took 10.095425844192505 s\n",
      "features size:  (9755, 1024)\n",
      "coordinates size:  (9755, 2)\n",
      "\n",
      "progress: 793/876\n",
      "NLSI0000191\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [47848 30622]\n",
      "level_dim [47848 30622]\n",
      "name NLSI0000191\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000191.h5: total of 16 batches\n",
      "batch 0/16, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000191.h5 took 8.328142404556274 s\n",
      "features size:  (8075, 1024)\n",
      "coordinates size:  (8075, 2)\n",
      "\n",
      "progress: 794/876\n",
      "NLSI0000192\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [55822 36083]\n",
      "level_dim [55822 36083]\n",
      "name NLSI0000192\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000192.h5: total of 31 batches\n",
      "batch 0/31, 0 files processed\n",
      "batch 20/31, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000192.h5 took 13.407564640045166 s\n",
      "features size:  (15407, 1024)\n",
      "coordinates size:  (15407, 2)\n",
      "\n",
      "progress: 795/876\n",
      "NLSI0000193\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [53829 37844]\n",
      "level_dim [53829 37844]\n",
      "name NLSI0000193\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000193.h5: total of 33 batches\n",
      "batch 0/33, 0 files processed\n",
      "batch 20/33, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000193.h5 took 15.75559663772583 s\n",
      "features size:  (16427, 1024)\n",
      "coordinates size:  (16427, 2)\n",
      "\n",
      "progress: 796/876\n",
      "NLSI0000194\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [37879 41040]\n",
      "level_dim [37879 41040]\n",
      "name NLSI0000194\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000194.h5: total of 29 batches\n",
      "batch 0/29, 0 files processed\n",
      "batch 20/29, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000194.h5 took 13.778931856155396 s\n",
      "features size:  (14487, 1024)\n",
      "coordinates size:  (14487, 2)\n",
      "\n",
      "progress: 797/876\n",
      "NLSI0000195\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [45854 35026]\n",
      "level_dim [45854 35026]\n",
      "name NLSI0000195\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000195.h5: total of 26 batches\n",
      "batch 0/26, 0 files processed\n",
      "batch 20/26, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000195.h5 took 12.447734832763672 s\n",
      "features size:  (13217, 1024)\n",
      "coordinates size:  (13217, 2)\n",
      "\n",
      "progress: 798/876\n",
      "NLSI0000196\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [49841 30015]\n",
      "level_dim [49841 30015]\n",
      "name NLSI0000196\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000196.h5: total of 21 batches\n",
      "batch 0/21, 0 files processed\n",
      "batch 20/21, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000196.h5 took 10.670218229293823 s\n",
      "features size:  (10532, 1024)\n",
      "coordinates size:  (10532, 2)\n",
      "\n",
      "progress: 799/876\n",
      "NLSI0000197\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57816 37484]\n",
      "level_dim [57816 37484]\n",
      "name NLSI0000197\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000197.h5: total of 39 batches\n",
      "batch 0/39, 0 files processed\n",
      "batch 20/39, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000197.h5 took 18.337206840515137 s\n",
      "features size:  (19535, 1024)\n",
      "coordinates size:  (19535, 2)\n",
      "\n",
      "progress: 800/876\n",
      "NLSI0000198\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [63797 39582]\n",
      "level_dim [63797 39582]\n",
      "name NLSI0000198\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000198.h5: total of 42 batches\n",
      "batch 0/42, 0 files processed\n",
      "batch 20/42, 10240 files processed\n",
      "batch 40/42, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000198.h5 took 19.46674394607544 s\n",
      "features size:  (21069, 1024)\n",
      "coordinates size:  (21069, 2)\n",
      "\n",
      "progress: 801/876\n",
      "NLSI0000199\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 34430]\n",
      "level_dim [51835 34430]\n",
      "name NLSI0000199\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000199.h5: total of 33 batches\n",
      "batch 0/33, 0 files processed\n",
      "batch 20/33, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000199.h5 took 15.402682065963745 s\n",
      "features size:  (16816, 1024)\n",
      "coordinates size:  (16816, 2)\n",
      "\n",
      "progress: 802/876\n",
      "NLSI0000203\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57816 37629]\n",
      "level_dim [57816 37629]\n",
      "name NLSI0000203\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000203.h5: total of 38 batches\n",
      "batch 0/38, 0 files processed\n",
      "batch 20/38, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000203.h5 took 16.988582372665405 s\n",
      "features size:  (19173, 1024)\n",
      "coordinates size:  (19173, 2)\n",
      "\n",
      "progress: 803/876\n",
      "NLSI0000204\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57816 40523]\n",
      "level_dim [57816 40523]\n",
      "name NLSI0000204\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000204.h5: total of 40 batches\n",
      "batch 0/40, 0 files processed\n",
      "batch 20/40, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000204.h5 took 18.555340051651 s\n",
      "features size:  (20435, 1024)\n",
      "coordinates size:  (20435, 2)\n",
      "\n",
      "progress: 804/876\n",
      "NLSI0000205\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [41867 33896]\n",
      "level_dim [41867 33896]\n",
      "name NLSI0000205\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000205.h5: total of 19 batches\n",
      "batch 0/19, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000205.h5 took 9.505301713943481 s\n",
      "features size:  (9643, 1024)\n",
      "coordinates size:  (9643, 2)\n",
      "\n",
      "progress: 805/876\n",
      "NLSI0000206\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57816 36353]\n",
      "level_dim [57816 36353]\n",
      "name NLSI0000206\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000206.h5: total of 39 batches\n",
      "batch 0/39, 0 files processed\n",
      "batch 20/39, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000206.h5 took 18.14862632751465 s\n",
      "features size:  (19581, 1024)\n",
      "coordinates size:  (19581, 2)\n",
      "\n",
      "progress: 806/876\n",
      "NLSI0000207\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [41867 40048]\n",
      "level_dim [41867 40048]\n",
      "name NLSI0000207\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000207.h5: total of 31 batches\n",
      "batch 0/31, 0 files processed\n",
      "batch 20/31, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000207.h5 took 15.083511590957642 s\n",
      "features size:  (15434, 1024)\n",
      "coordinates size:  (15434, 2)\n",
      "\n",
      "progress: 807/876\n",
      "NLSI0000208\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 34675]\n",
      "level_dim [51835 34675]\n",
      "name NLSI0000208\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000208.h5: total of 31 batches\n",
      "batch 0/31, 0 files processed\n",
      "batch 20/31, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000208.h5 took 14.695671558380127 s\n",
      "features size:  (15483, 1024)\n",
      "coordinates size:  (15483, 2)\n",
      "\n",
      "progress: 808/876\n",
      "NLSI0000209\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57816 32438]\n",
      "level_dim [57816 32438]\n",
      "name NLSI0000209\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000209.h5: total of 36 batches\n",
      "batch 0/36, 0 files processed\n",
      "batch 20/36, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000209.h5 took 16.44369626045227 s\n",
      "features size:  (18015, 1024)\n",
      "coordinates size:  (18015, 2)\n",
      "\n",
      "progress: 809/876\n",
      "NLSI0000210\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [53829 37461]\n",
      "level_dim [53829 37461]\n",
      "name NLSI0000210\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000210.h5: total of 40 batches\n",
      "batch 0/40, 0 files processed\n",
      "batch 20/40, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000210.h5 took 18.225656986236572 s\n",
      "features size:  (20149, 1024)\n",
      "coordinates size:  (20149, 2)\n",
      "\n",
      "progress: 810/876\n",
      "NLSI0000211\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [27911 30783]\n",
      "level_dim [27911 30783]\n",
      "name NLSI0000211\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000211.h5: total of 14 batches\n",
      "batch 0/14, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000211.h5 took 7.269330024719238 s\n",
      "features size:  (6697, 1024)\n",
      "coordinates size:  (6697, 2)\n",
      "\n",
      "progress: 811/876\n",
      "NLSI0000212\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [55822 39496]\n",
      "level_dim [55822 39496]\n",
      "name NLSI0000212\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000212.h5: total of 31 batches\n",
      "batch 0/31, 0 files processed\n",
      "batch 20/31, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000212.h5 took 14.466211318969727 s\n",
      "features size:  (15747, 1024)\n",
      "coordinates size:  (15747, 2)\n",
      "\n",
      "progress: 812/876\n",
      "NLSI0000213\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [49841 35495]\n",
      "level_dim [49841 35495]\n",
      "name NLSI0000213\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000213.h5: total of 17 batches\n",
      "batch 0/17, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000213.h5 took 9.283031702041626 s\n",
      "features size:  (8331, 1024)\n",
      "coordinates size:  (8331, 2)\n",
      "\n",
      "progress: 813/876\n",
      "NLSI0000214\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [35886 43416]\n",
      "level_dim [35886 43416]\n",
      "name NLSI0000214\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000214.h5: total of 20 batches\n",
      "batch 0/20, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000214.h5 took 10.47519063949585 s\n",
      "features size:  (10091, 1024)\n",
      "coordinates size:  (10091, 2)\n",
      "\n",
      "progress: 814/876\n",
      "NLSI0000215\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [39873 46590]\n",
      "level_dim [39873 46590]\n",
      "name NLSI0000215\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000215.h5: total of 34 batches\n",
      "batch 0/34, 0 files processed\n",
      "batch 20/34, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000215.h5 took 15.001296043395996 s\n",
      "features size:  (17273, 1024)\n",
      "coordinates size:  (17273, 2)\n",
      "\n",
      "progress: 815/876\n",
      "NLSI0000216\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [55822 45830]\n",
      "level_dim [55822 45830]\n",
      "name NLSI0000216\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000216.h5: total of 36 batches\n",
      "batch 0/36, 0 files processed\n",
      "batch 20/36, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000216.h5 took 17.10866117477417 s\n",
      "features size:  (18060, 1024)\n",
      "coordinates size:  (18060, 2)\n",
      "\n",
      "progress: 816/876\n",
      "NLSI0000217\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [63797 46297]\n",
      "level_dim [63797 46297]\n",
      "name NLSI0000217\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000217.h5: total of 55 batches\n",
      "batch 0/55, 0 files processed\n",
      "batch 20/55, 10240 files processed\n",
      "batch 40/55, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000217.h5 took 22.912247896194458 s\n",
      "features size:  (28144, 1024)\n",
      "coordinates size:  (28144, 2)\n",
      "\n",
      "progress: 817/876\n",
      "NLSI0000218\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [69778 40861]\n",
      "level_dim [69778 40861]\n",
      "name NLSI0000218\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000218.h5: total of 30 batches\n",
      "batch 0/30, 0 files processed\n",
      "batch 20/30, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000218.h5 took 13.690812826156616 s\n",
      "features size:  (15152, 1024)\n",
      "coordinates size:  (15152, 2)\n",
      "\n",
      "progress: 818/876\n",
      "NLSI0000223\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [29905 30037]\n",
      "level_dim [29905 30037]\n",
      "name NLSI0000223\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000223.h5: total of 19 batches\n",
      "batch 0/19, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000223.h5 took 9.597623109817505 s\n",
      "features size:  (9499, 1024)\n",
      "coordinates size:  (9499, 2)\n",
      "\n",
      "progress: 819/876\n",
      "NLSI0000224\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [41867 41935]\n",
      "level_dim [41867 41935]\n",
      "name NLSI0000224\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000224.h5: total of 38 batches\n",
      "batch 0/38, 0 files processed\n",
      "batch 20/38, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000224.h5 took 17.518031358718872 s\n",
      "features size:  (19055, 1024)\n",
      "coordinates size:  (19055, 2)\n",
      "\n",
      "progress: 820/876\n",
      "NLSI0000225\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [45854 32778]\n",
      "level_dim [45854 32778]\n",
      "name NLSI0000225\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000225.h5: total of 28 batches\n",
      "batch 0/28, 0 files processed\n",
      "batch 20/28, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000225.h5 took 13.133743524551392 s\n",
      "features size:  (14194, 1024)\n",
      "coordinates size:  (14194, 2)\n",
      "\n",
      "progress: 821/876\n",
      "NLSI0000226\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [39873 39411]\n",
      "level_dim [39873 39411]\n",
      "name NLSI0000226\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000226.h5: total of 30 batches\n",
      "batch 0/30, 0 files processed\n",
      "batch 20/30, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000226.h5 took 14.666636943817139 s\n",
      "features size:  (15139, 1024)\n",
      "coordinates size:  (15139, 2)\n",
      "\n",
      "progress: 822/876\n",
      "NLSI0000227\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [47848 33901]\n",
      "level_dim [47848 33901]\n",
      "name NLSI0000227\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000227.h5: total of 31 batches\n",
      "batch 0/31, 0 files processed\n",
      "batch 20/31, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000227.h5 took 14.57183313369751 s\n",
      "features size:  (15682, 1024)\n",
      "coordinates size:  (15682, 2)\n",
      "\n",
      "progress: 823/876\n",
      "NLSI0000228\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [35886 25640]\n",
      "level_dim [35886 25640]\n",
      "name NLSI0000228\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000228.h5: total of 18 batches\n",
      "batch 0/18, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000228.h5 took 9.357486486434937 s\n",
      "features size:  (9001, 1024)\n",
      "coordinates size:  (9001, 2)\n",
      "\n",
      "progress: 824/876\n",
      "NLSI0000234\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [59810 45058]\n",
      "level_dim [59810 45058]\n",
      "name NLSI0000234\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000234.h5: total of 48 batches\n",
      "batch 0/48, 0 files processed\n",
      "batch 20/48, 10240 files processed\n",
      "batch 40/48, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000234.h5 took 21.657578468322754 s\n",
      "features size:  (24092, 1024)\n",
      "coordinates size:  (24092, 2)\n",
      "\n",
      "progress: 825/876\n",
      "NLSI0000235\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57816 38374]\n",
      "level_dim [57816 38374]\n",
      "name NLSI0000235\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000235.h5: total of 39 batches\n",
      "batch 0/39, 0 files processed\n",
      "batch 20/39, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000235.h5 took 18.277087211608887 s\n",
      "features size:  (19946, 1024)\n",
      "coordinates size:  (19946, 2)\n",
      "\n",
      "progress: 826/876\n",
      "NLSI0000236\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [55822 31681]\n",
      "level_dim [55822 31681]\n",
      "name NLSI0000236\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000236.h5: total of 33 batches\n",
      "batch 0/33, 0 files processed\n",
      "batch 20/33, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000236.h5 took 14.244848489761353 s\n",
      "features size:  (16675, 1024)\n",
      "coordinates size:  (16675, 2)\n",
      "\n",
      "progress: 827/876\n",
      "NLSI0000240\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 40605]\n",
      "level_dim [51835 40605]\n",
      "name NLSI0000240\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000240.h5: total of 33 batches\n",
      "batch 0/33, 0 files processed\n",
      "batch 20/33, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000240.h5 took 15.436067819595337 s\n",
      "features size:  (16789, 1024)\n",
      "coordinates size:  (16789, 2)\n",
      "\n",
      "progress: 828/876\n",
      "NLSI0000241\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57816 47575]\n",
      "level_dim [57816 47575]\n",
      "name NLSI0000241\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000241.h5: total of 51 batches\n",
      "batch 0/51, 0 files processed\n",
      "batch 20/51, 10240 files processed\n",
      "batch 40/51, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000241.h5 took 20.700411558151245 s\n",
      "features size:  (25933, 1024)\n",
      "coordinates size:  (25933, 2)\n",
      "\n",
      "progress: 829/876\n",
      "NLSI0000242\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 41987]\n",
      "level_dim [51835 41987]\n",
      "name NLSI0000242\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000242.h5: total of 29 batches\n",
      "batch 0/29, 0 files processed\n",
      "batch 20/29, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000242.h5 took 13.139694690704346 s\n",
      "features size:  (14696, 1024)\n",
      "coordinates size:  (14696, 2)\n",
      "\n",
      "progress: 830/876\n",
      "NLSI0000246\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [49841 33780]\n",
      "level_dim [49841 33780]\n",
      "name NLSI0000246\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000246.h5: total of 33 batches\n",
      "batch 0/33, 0 files processed\n",
      "batch 20/33, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000246.h5 took 15.262397527694702 s\n",
      "features size:  (16859, 1024)\n",
      "coordinates size:  (16859, 2)\n",
      "\n",
      "progress: 831/876\n",
      "NLSI0000247\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [43860 37914]\n",
      "level_dim [43860 37914]\n",
      "name NLSI0000247\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000247.h5: total of 31 batches\n",
      "batch 0/31, 0 files processed\n",
      "batch 20/31, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000247.h5 took 15.537636518478394 s\n",
      "features size:  (15806, 1024)\n",
      "coordinates size:  (15806, 2)\n",
      "\n",
      "progress: 832/876\n",
      "NLSI0000248\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [61803 43851]\n",
      "level_dim [61803 43851]\n",
      "name NLSI0000248\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000248.h5: total of 43 batches\n",
      "batch 0/43, 0 files processed\n",
      "batch 20/43, 10240 files processed\n",
      "batch 40/43, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000248.h5 took 19.145201921463013 s\n",
      "features size:  (21574, 1024)\n",
      "coordinates size:  (21574, 2)\n",
      "\n",
      "progress: 833/876\n",
      "NLSI0000250\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [55822 46209]\n",
      "level_dim [55822 46209]\n",
      "name NLSI0000250\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000250.h5: total of 50 batches\n",
      "batch 0/50, 0 files processed\n",
      "batch 20/50, 10240 files processed\n",
      "batch 40/50, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000250.h5 took 20.601123332977295 s\n",
      "features size:  (25403, 1024)\n",
      "coordinates size:  (25403, 2)\n",
      "\n",
      "progress: 834/876\n",
      "NLSI0000251-001\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [67784 46375]\n",
      "level_dim [67784 46375]\n",
      "name NLSI0000251-001\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000251-001.h5: total of 63 batches\n",
      "batch 0/63, 0 files processed\n",
      "batch 20/63, 10240 files processed\n",
      "batch 40/63, 20480 files processed\n",
      "batch 60/63, 30720 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000251-001.h5 took 27.40562915802002 s\n",
      "features size:  (31777, 1024)\n",
      "coordinates size:  (31777, 2)\n",
      "\n",
      "progress: 835/876\n",
      "NLSI0000252\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [55822 40116]\n",
      "level_dim [55822 40116]\n",
      "name NLSI0000252\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000252.h5: total of 48 batches\n",
      "batch 0/48, 0 files processed\n",
      "batch 20/48, 10240 files processed\n",
      "batch 40/48, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000252.h5 took 20.43782138824463 s\n",
      "features size:  (24289, 1024)\n",
      "coordinates size:  (24289, 2)\n",
      "\n",
      "progress: 836/876\n",
      "NLSI0000253\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [61803 46335]\n",
      "level_dim [61803 46335]\n",
      "name NLSI0000253\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000253.h5: total of 53 batches\n",
      "batch 0/53, 0 files processed\n",
      "batch 20/53, 10240 files processed\n",
      "batch 40/53, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000253.h5 took 23.189042568206787 s\n",
      "features size:  (26830, 1024)\n",
      "coordinates size:  (26830, 2)\n",
      "\n",
      "progress: 837/876\n",
      "NLSI0000254\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57816 46943]\n",
      "level_dim [57816 46943]\n",
      "name NLSI0000254\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000254.h5: total of 60 batches\n",
      "batch 0/60, 0 files processed\n",
      "batch 20/60, 10240 files processed\n",
      "batch 40/60, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000254.h5 took 26.466747045516968 s\n",
      "features size:  (30392, 1024)\n",
      "coordinates size:  (30392, 2)\n",
      "\n",
      "progress: 838/876\n",
      "NLSI0000255\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [61803 41158]\n",
      "level_dim [61803 41158]\n",
      "name NLSI0000255\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000255.h5: total of 45 batches\n",
      "batch 0/45, 0 files processed\n",
      "batch 20/45, 10240 files processed\n",
      "batch 40/45, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000255.h5 took 19.288481950759888 s\n",
      "features size:  (22936, 1024)\n",
      "coordinates size:  (22936, 2)\n",
      "\n",
      "progress: 839/876\n",
      "NLSI0000256\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [61803 46785]\n",
      "level_dim [61803 46785]\n",
      "name NLSI0000256\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000256.h5: total of 55 batches\n",
      "batch 0/55, 0 files processed\n",
      "batch 20/55, 10240 files processed\n",
      "batch 40/55, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000256.h5 took 22.282669067382812 s\n",
      "features size:  (27817, 1024)\n",
      "coordinates size:  (27817, 2)\n",
      "\n",
      "progress: 840/876\n",
      "NLSI0000257\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [41867 43457]\n",
      "level_dim [41867 43457]\n",
      "name NLSI0000257\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000257.h5: total of 38 batches\n",
      "batch 0/38, 0 files processed\n",
      "batch 20/38, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000257.h5 took 16.671518564224243 s\n",
      "features size:  (19035, 1024)\n",
      "coordinates size:  (19035, 2)\n",
      "\n",
      "progress: 841/876\n",
      "NLSI0000259\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [59810 25901]\n",
      "level_dim [59810 25901]\n",
      "name NLSI0000259\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000259.h5: total of 28 batches\n",
      "batch 0/28, 0 files processed\n",
      "batch 20/28, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000259.h5 took 13.500463247299194 s\n",
      "features size:  (14281, 1024)\n",
      "coordinates size:  (14281, 2)\n",
      "\n",
      "progress: 842/876\n",
      "NLSI0000260\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [55822 31174]\n",
      "level_dim [55822 31174]\n",
      "name NLSI0000260\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000260.h5: total of 30 batches\n",
      "batch 0/30, 0 files processed\n",
      "batch 20/30, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000260.h5 took 14.412720918655396 s\n",
      "features size:  (14856, 1024)\n",
      "coordinates size:  (14856, 2)\n",
      "\n",
      "progress: 843/876\n",
      "NLSI0000261\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [25917 30130]\n",
      "level_dim [25917 30130]\n",
      "name NLSI0000261\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000261.h5: total of 12 batches\n",
      "batch 0/12, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000261.h5 took 7.031704902648926 s\n",
      "features size:  (5642, 1024)\n",
      "coordinates size:  (5642, 2)\n",
      "\n",
      "progress: 844/876\n",
      "NLSI0000262\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [63797 40549]\n",
      "level_dim [63797 40549]\n",
      "name NLSI0000262\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000262.h5: total of 40 batches\n",
      "batch 0/40, 0 files processed\n",
      "batch 20/40, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000262.h5 took 18.167131423950195 s\n",
      "features size:  (20134, 1024)\n",
      "coordinates size:  (20134, 2)\n",
      "\n",
      "progress: 845/876\n",
      "NLSI0000263\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 37829]\n",
      "level_dim [51835 37829]\n",
      "name NLSI0000263\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000263.h5: total of 31 batches\n",
      "batch 0/31, 0 files processed\n",
      "batch 20/31, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000263.h5 took 14.497933387756348 s\n",
      "features size:  (15706, 1024)\n",
      "coordinates size:  (15706, 2)\n",
      "\n",
      "progress: 846/876\n",
      "NLSI0000264\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [43860 36020]\n",
      "level_dim [43860 36020]\n",
      "name NLSI0000264\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000264.h5: total of 22 batches\n",
      "batch 0/22, 0 files processed\n",
      "batch 20/22, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000264.h5 took 11.336627006530762 s\n",
      "features size:  (11256, 1024)\n",
      "coordinates size:  (11256, 2)\n",
      "\n",
      "progress: 847/876\n",
      "NLSI0000265\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [25917 29879]\n",
      "level_dim [25917 29879]\n",
      "name NLSI0000265\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000265.h5: total of 15 batches\n",
      "batch 0/15, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000265.h5 took 8.447010517120361 s\n",
      "features size:  (7667, 1024)\n",
      "coordinates size:  (7667, 2)\n",
      "\n",
      "progress: 848/876\n",
      "NLSI0000266\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [33892 42906]\n",
      "level_dim [33892 42906]\n",
      "name NLSI0000266\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000266.h5: total of 18 batches\n",
      "batch 0/18, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000266.h5 took 9.406708240509033 s\n",
      "features size:  (9172, 1024)\n",
      "coordinates size:  (9172, 2)\n",
      "\n",
      "progress: 849/876\n",
      "NLSI0000267\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [61803 41670]\n",
      "level_dim [61803 41670]\n",
      "name NLSI0000267\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000267.h5: total of 31 batches\n",
      "batch 0/31, 0 files processed\n",
      "batch 20/31, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000267.h5 took 14.827174663543701 s\n",
      "features size:  (15602, 1024)\n",
      "coordinates size:  (15602, 2)\n",
      "\n",
      "progress: 850/876\n",
      "NLSI0000268\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [49841 42359]\n",
      "level_dim [49841 42359]\n",
      "name NLSI0000268\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000268.h5: total of 30 batches\n",
      "batch 0/30, 0 files processed\n",
      "batch 20/30, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000268.h5 took 15.063041687011719 s\n",
      "features size:  (15122, 1024)\n",
      "coordinates size:  (15122, 2)\n",
      "\n",
      "progress: 851/876\n",
      "NLSI0000269\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [61803 44068]\n",
      "level_dim [61803 44068]\n",
      "name NLSI0000269\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000269.h5: total of 48 batches\n",
      "batch 0/48, 0 files processed\n",
      "batch 20/48, 10240 files processed\n",
      "batch 40/48, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000269.h5 took 20.81033682823181 s\n",
      "features size:  (24280, 1024)\n",
      "coordinates size:  (24280, 2)\n",
      "\n",
      "progress: 852/876\n",
      "NLSI0000270\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [47848 47266]\n",
      "level_dim [47848 47266]\n",
      "name NLSI0000270\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000270.h5: total of 45 batches\n",
      "batch 0/45, 0 files processed\n",
      "batch 20/45, 10240 files processed\n",
      "batch 40/45, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000270.h5 took 18.76504611968994 s\n",
      "features size:  (22937, 1024)\n",
      "coordinates size:  (22937, 2)\n",
      "\n",
      "progress: 853/876\n",
      "NLSI0000271\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [41867 46222]\n",
      "level_dim [41867 46222]\n",
      "name NLSI0000271\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000271.h5: total of 38 batches\n",
      "batch 0/38, 0 files processed\n",
      "batch 20/38, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000271.h5 took 16.203020095825195 s\n",
      "features size:  (19396, 1024)\n",
      "coordinates size:  (19396, 2)\n",
      "\n",
      "progress: 854/876\n",
      "NLSI0000272\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [29905 29140]\n",
      "level_dim [29905 29140]\n",
      "name NLSI0000272\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000272.h5: total of 14 batches\n",
      "batch 0/14, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000272.h5 took 7.5414228439331055 s\n",
      "features size:  (7064, 1024)\n",
      "coordinates size:  (7064, 2)\n",
      "\n",
      "progress: 855/876\n",
      "NLSI0000273\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [53829 41305]\n",
      "level_dim [53829 41305]\n",
      "name NLSI0000273\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000273.h5: total of 35 batches\n",
      "batch 0/35, 0 files processed\n",
      "batch 20/35, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000273.h5 took 16.05844020843506 s\n",
      "features size:  (17628, 1024)\n",
      "coordinates size:  (17628, 2)\n",
      "\n",
      "progress: 856/876\n",
      "NLSI0000274\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [49841 40093]\n",
      "level_dim [49841 40093]\n",
      "name NLSI0000274\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000274.h5: total of 34 batches\n",
      "batch 0/34, 0 files processed\n",
      "batch 20/34, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000274.h5 took 15.549068927764893 s\n",
      "features size:  (16924, 1024)\n",
      "coordinates size:  (16924, 2)\n",
      "\n",
      "progress: 857/876\n",
      "NLSI0000277\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [41867 34124]\n",
      "level_dim [41867 34124]\n",
      "name NLSI0000277\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000277.h5: total of 25 batches\n",
      "batch 0/25, 0 files processed\n",
      "batch 20/25, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000277.h5 took 12.662436246871948 s\n",
      "features size:  (12517, 1024)\n",
      "coordinates size:  (12517, 2)\n",
      "\n",
      "progress: 858/876\n",
      "NLSI0000278\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [61803 37946]\n",
      "level_dim [61803 37946]\n",
      "name NLSI0000278\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000278.h5: total of 33 batches\n",
      "batch 0/33, 0 files processed\n",
      "batch 20/33, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000278.h5 took 14.898338079452515 s\n",
      "features size:  (16538, 1024)\n",
      "coordinates size:  (16538, 2)\n",
      "\n",
      "progress: 859/876\n",
      "NLSI0000279\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [43860 46864]\n",
      "level_dim [43860 46864]\n",
      "name NLSI0000279\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000279.h5: total of 42 batches\n",
      "batch 0/42, 0 files processed\n",
      "batch 20/42, 10240 files processed\n",
      "batch 40/42, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000279.h5 took 18.712679624557495 s\n",
      "features size:  (21399, 1024)\n",
      "coordinates size:  (21399, 2)\n",
      "\n",
      "progress: 860/876\n",
      "NLSI0000286\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 38585]\n",
      "level_dim [51835 38585]\n",
      "name NLSI0000286\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000286.h5: total of 30 batches\n",
      "batch 0/30, 0 files processed\n",
      "batch 20/30, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000286.h5 took 14.268274545669556 s\n",
      "features size:  (15213, 1024)\n",
      "coordinates size:  (15213, 2)\n",
      "\n",
      "progress: 861/876\n",
      "NLSI0000287\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [47848 37175]\n",
      "level_dim [47848 37175]\n",
      "name NLSI0000287\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000287.h5: total of 33 batches\n",
      "batch 0/33, 0 files processed\n",
      "batch 20/33, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000287.h5 took 15.829682350158691 s\n",
      "features size:  (16451, 1024)\n",
      "coordinates size:  (16451, 2)\n",
      "\n",
      "progress: 862/876\n",
      "NLSI0000288\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [59810 42794]\n",
      "level_dim [59810 42794]\n",
      "name NLSI0000288\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000288.h5: total of 37 batches\n",
      "batch 0/37, 0 files processed\n",
      "batch 20/37, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000288.h5 took 17.28442692756653 s\n",
      "features size:  (18681, 1024)\n",
      "coordinates size:  (18681, 2)\n",
      "\n",
      "progress: 863/876\n",
      "NLSI0000289\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [47848 32381]\n",
      "level_dim [47848 32381]\n",
      "name NLSI0000289\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000289.h5: total of 21 batches\n",
      "batch 0/21, 0 files processed\n",
      "batch 20/21, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000289.h5 took 10.496564626693726 s\n",
      "features size:  (10536, 1024)\n",
      "coordinates size:  (10536, 2)\n",
      "\n",
      "progress: 864/876\n",
      "NLSI0000290\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [51835 38086]\n",
      "level_dim [51835 38086]\n",
      "name NLSI0000290\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000290.h5: total of 39 batches\n",
      "batch 0/39, 0 files processed\n",
      "batch 20/39, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000290.h5 took 17.721822261810303 s\n",
      "features size:  (19612, 1024)\n",
      "coordinates size:  (19612, 2)\n",
      "\n",
      "progress: 865/876\n",
      "NLSI0000291\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [53829 42130]\n",
      "level_dim [53829 42130]\n",
      "name NLSI0000291\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000291.h5: total of 20 batches\n",
      "batch 0/20, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000291.h5 took 9.776597023010254 s\n",
      "features size:  (10096, 1024)\n",
      "coordinates size:  (10096, 2)\n",
      "\n",
      "progress: 866/876\n",
      "NLSI0000292\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [63797 46599]\n",
      "level_dim [63797 46599]\n",
      "name NLSI0000292\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000292.h5: total of 44 batches\n",
      "batch 0/44, 0 files processed\n",
      "batch 20/44, 10240 files processed\n",
      "batch 40/44, 20480 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000292.h5 took 19.689719438552856 s\n",
      "features size:  (22264, 1024)\n",
      "coordinates size:  (22264, 2)\n",
      "\n",
      "progress: 867/876\n",
      "NLSI0000293\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [61803 44442]\n",
      "level_dim [61803 44442]\n",
      "name NLSI0000293\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000293.h5: total of 40 batches\n",
      "batch 0/40, 0 files processed\n",
      "batch 20/40, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000293.h5 took 18.169005155563354 s\n",
      "features size:  (20045, 1024)\n",
      "coordinates size:  (20045, 2)\n",
      "\n",
      "progress: 868/876\n",
      "NLSI0000294\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [53829 31034]\n",
      "level_dim [53829 31034]\n",
      "name NLSI0000294\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000294.h5: total of 19 batches\n",
      "batch 0/19, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000294.h5 took 10.31588363647461 s\n",
      "features size:  (9622, 1024)\n",
      "coordinates size:  (9622, 2)\n",
      "\n",
      "progress: 869/876\n",
      "NLSI0000295\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [61803 43935]\n",
      "level_dim [61803 43935]\n",
      "name NLSI0000295\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000295.h5: total of 33 batches\n",
      "batch 0/33, 0 files processed\n",
      "batch 20/33, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000295.h5 took 15.542007207870483 s\n",
      "features size:  (16663, 1024)\n",
      "coordinates size:  (16663, 2)\n",
      "\n",
      "progress: 870/876\n",
      "NLSI0000296\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [21930 29741]\n",
      "level_dim [21930 29741]\n",
      "name NLSI0000296\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000296.h5: total of 8 batches\n",
      "batch 0/8, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000296.h5 took 5.58556056022644 s\n",
      "features size:  (3824, 1024)\n",
      "coordinates size:  (3824, 2)\n",
      "\n",
      "progress: 871/876\n",
      "NLSI0000297\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [41867 39169]\n",
      "level_dim [41867 39169]\n",
      "name NLSI0000297\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000297.h5: total of 29 batches\n",
      "batch 0/29, 0 files processed\n",
      "batch 20/29, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000297.h5 took 14.025723934173584 s\n",
      "features size:  (14564, 1024)\n",
      "coordinates size:  (14564, 2)\n",
      "\n",
      "progress: 872/876\n",
      "NLSI0000298\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [57816 41768]\n",
      "level_dim [57816 41768]\n",
      "name NLSI0000298\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000298.h5: total of 40 batches\n",
      "batch 0/40, 0 files processed\n",
      "batch 20/40, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000298.h5 took 18.126928567886353 s\n",
      "features size:  (20056, 1024)\n",
      "coordinates size:  (20056, 2)\n",
      "\n",
      "progress: 873/876\n",
      "NLSI0000524\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [43860 27589]\n",
      "level_dim [43860 27589]\n",
      "name NLSI0000524\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000524.h5: total of 20 batches\n",
      "batch 0/20, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000524.h5 took 10.163344144821167 s\n",
      "features size:  (9742, 1024)\n",
      "coordinates size:  (9742, 2)\n",
      "\n",
      "progress: 874/876\n",
      "NLSI0000525\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [37879 27305]\n",
      "level_dim [37879 27305]\n",
      "name NLSI0000525\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000525.h5: total of 11 batches\n",
      "batch 0/11, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000525.h5 took 6.774893045425415 s\n",
      "features size:  (5269, 1024)\n",
      "coordinates size:  (5269, 2)\n",
      "\n",
      "progress: 875/876\n",
      "NLSI0000526\n",
      "downsample [1. 1.]\n",
      "downsampled_level_dim [39873 32971]\n",
      "level_dim [39873 32971]\n",
      "name NLSI0000526\n",
      "patch_level 0\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/patches/NLSI0000526.h5: total of 23 batches\n",
      "batch 0/23, 0 files processed\n",
      "batch 20/23, 10240 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/NLST/FEATURES_level0/h5_files/NLSI0000526.h5 took 11.067664384841919 s\n",
      "features size:  (11451, 1024)\n",
      "coordinates size:  (11451, 2)\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python extract_features_fp.py --data_h5_dir /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0 \\\n",
    "--data_slide_dir /home/sci/Disk_data/Datasets/NLST/WSI \\\n",
    "--csv_path /home/sci/Disk_data/Datasets/NLST/BLOCKS_level0/step2_get_features.csv \\\n",
    "--feat_dir /home/sci/Disk2/NLST/FEATURES_level0 --batch_size 512 --slide_ext .svs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step3 Create split\n",
    "注意修改Create_split_seq.py文件中的csv路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.csv_gen import * \n",
    "path = r'/home/sci/Disk_data/TCGA-NSCLC/WSI'\n",
    "# sort_csv = pd.read_csv(csv_dir).sort_values('slide_id')\n",
    "result_dir = r'/home/sci/Disk_data/TCGA-NSCLC/RESULTS_DIRECTORY/step3_get_splits.csv' ## 5 + 20\n",
    "patch_dir = r'/home/sci/Disk_data/TCGA-NSCLC/RESULTS_DIRECTORY/patches'\n",
    "csv_gen_test(path,result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "label column: label\n",
      "label dictionary: {'LUAD': 0, 'LUSC': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 0    552\n",
      "1    324\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 194\n",
      "Slide-LVL; Number of samples registered in class 0: 552\n",
      "Patient-LVL; Number of samples registered in class 1: 117\n",
      "Slide-LVL; Number of samples registered in class 1: 324\n",
      "\n",
      "number of training samples: 700\n",
      "number of samples in cls 0: 444\n",
      "number of samples in cls 1: 256\n",
      "\n",
      "number of val samples: 97\n",
      "number of samples in cls 0: 59\n",
      "number of samples in cls 1: 38\n",
      "\n",
      "number of test samples: 79\n",
      "number of samples in cls 0: 49\n",
      "number of samples in cls 1: 30\n",
      "\n",
      "\n",
      "\n",
      "number of training samples: 702\n",
      "number of samples in cls 0: 439\n",
      "number of samples in cls 1: 263\n",
      "\n",
      "number of val samples: 89\n",
      "number of samples in cls 0: 54\n",
      "number of samples in cls 1: 35\n",
      "\n",
      "number of test samples: 85\n",
      "number of samples in cls 0: 59\n",
      "number of samples in cls 1: 26\n",
      "\n",
      "\n",
      "\n",
      "number of training samples: 703\n",
      "number of samples in cls 0: 445\n",
      "number of samples in cls 1: 258\n",
      "\n",
      "number of val samples: 89\n",
      "number of samples in cls 0: 55\n",
      "number of samples in cls 1: 34\n",
      "\n",
      "number of test samples: 84\n",
      "number of samples in cls 0: 52\n",
      "number of samples in cls 1: 32\n",
      "\n",
      "\n",
      "\n",
      "number of training samples: 707\n",
      "number of samples in cls 0: 447\n",
      "number of samples in cls 1: 260\n",
      "\n",
      "number of val samples: 85\n",
      "number of samples in cls 0: 54\n",
      "number of samples in cls 1: 31\n",
      "\n",
      "number of test samples: 84\n",
      "number of samples in cls 0: 51\n",
      "number of samples in cls 1: 33\n",
      "\n",
      "\n",
      "\n",
      "number of training samples: 705\n",
      "number of samples in cls 0: 444\n",
      "number of samples in cls 1: 261\n",
      "\n",
      "number of val samples: 80\n",
      "number of samples in cls 0: 49\n",
      "number of samples in cls 1: 31\n",
      "\n",
      "number of test samples: 91\n",
      "number of samples in cls 0: 59\n",
      "number of samples in cls 1: 32\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python create_splits_seq.py --task task_2_tumor_subtyping --seed 1 --label_frac 1 --k 5 \\\n",
    "    --csv_path dataset_csv/NLST_offical.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 Train\n",
    "注意修改main.py中的csv路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA_VISIBLE_DEVICES=0 python main.py \\\n",
    "--drop_out \\\n",
    "--early_stopping \\\n",
    "--lr 2e-4 \\\n",
    "--k 10 \\\n",
    "--label_frac 0.75 \\\n",
    "--exp_code task_1_tumor_vs_normal_CLAM_50 --weighted_sample --bag_loss ce --inst_loss svm --task task_1_tumor_vs_normal --model_type clam_sb --log_data \\\n",
    "--data_root_dir /media/yuansh/14THHD/CLAM/FEATURES_DIRECTORY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load Dataset\n",
      "label column: label\n",
      "label dictionary: {'LUAD': 0, 'LUSC': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 0    552\n",
      "1    324\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 194\n",
      "Slide-LVL; Number of samples registered in class 0: 552\n",
      "Patient-LVL; Number of samples registered in class 1: 117\n",
      "Slide-LVL; Number of samples registered in class 1: 324\n",
      "split_dir:  /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/NLST_100\n",
      "################# Settings ###################\n",
      "num_splits:  5\n",
      "k_start:  4\n",
      "k_end:  -1\n",
      "task:  task_2_tumor_subtyping\n",
      "max_epochs:  200\n",
      "results_dir:  ./results\n",
      "lr:  0.0002\n",
      "experiment:  nlst_100_level1_clam_sb_adam_testfold4\n",
      "reg:  1e-05\n",
      "label_frac:  1.0\n",
      "bag_loss:  ce\n",
      "seed:  1\n",
      "model_type:  clam_sb\n",
      "model_size:  small\n",
      "use_drop_out:  True\n",
      "weighted_sample:  True\n",
      "opt:  adam\n",
      "bag_weight:  0.7\n",
      "inst_loss:  svm\n",
      "B:  8\n",
      "split_dir:  /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/NLST_100\n",
      "\n",
      "Training Fold 4!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 705 samples\n",
      "Validating on 80 samples\n",
      "Testing on 91 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Setting tau to 1.0\n",
      "Done!\n",
      "CLAM_SB(\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (instance_loss_fn): SmoothTop1SVM()\n",
      ")\n",
      "Total number of parameters: 790791\n",
      "Total number of trainable parameters: 790791\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6352, label: 0, bag_size: 4283\n",
      "batch 39, loss: 0.8561, label: 1, bag_size: 2493\n",
      "batch 59, loss: 0.8507, label: 1, bag_size: 4285\n",
      "batch 79, loss: 0.7822, label: 1, bag_size: 2253\n",
      "batch 99, loss: 0.7724, label: 0, bag_size: 1684\n",
      "batch 119, loss: 0.4616, label: 1, bag_size: 3081\n",
      "batch 139, loss: 0.5140, label: 1, bag_size: 4761\n",
      "batch 159, loss: 1.3729, label: 0, bag_size: 4084\n",
      "batch 179, loss: 0.4222, label: 1, bag_size: 7085\n",
      "batch 199, loss: 0.5203, label: 1, bag_size: 4285\n",
      "batch 219, loss: 0.5980, label: 1, bag_size: 2938\n",
      "batch 239, loss: 0.7320, label: 0, bag_size: 4282\n",
      "batch 259, loss: 0.5955, label: 0, bag_size: 25403\n",
      "batch 279, loss: 0.6579, label: 1, bag_size: 3834\n",
      "batch 299, loss: 1.0468, label: 0, bag_size: 5074\n",
      "batch 319, loss: 0.9201, label: 0, bag_size: 2682\n",
      "batch 339, loss: 0.9335, label: 0, bag_size: 4922\n",
      "batch 359, loss: 0.6311, label: 1, bag_size: 10736\n",
      "batch 379, loss: 0.6545, label: 0, bag_size: 3007\n",
      "batch 399, loss: 0.6180, label: 1, bag_size: 18015\n",
      "batch 419, loss: 0.6415, label: 1, bag_size: 27072\n",
      "batch 439, loss: 0.6647, label: 1, bag_size: 2524\n",
      "batch 459, loss: 0.7071, label: 1, bag_size: 3533\n",
      "batch 479, loss: 0.8226, label: 0, bag_size: 6724\n",
      "batch 499, loss: 0.8198, label: 1, bag_size: 6697\n",
      "batch 519, loss: 0.4286, label: 0, bag_size: 2918\n",
      "batch 539, loss: 0.4959, label: 0, bag_size: 11758\n",
      "batch 559, loss: 0.6281, label: 0, bag_size: 763\n",
      "batch 579, loss: 0.7700, label: 1, bag_size: 4112\n",
      "batch 599, loss: 0.5477, label: 0, bag_size: 2069\n",
      "batch 619, loss: 0.7824, label: 1, bag_size: 2669\n",
      "batch 639, loss: 0.5591, label: 0, bag_size: 4500\n",
      "batch 659, loss: 0.7418, label: 0, bag_size: 4402\n",
      "batch 679, loss: 0.7534, label: 1, bag_size: 5199\n",
      "batch 699, loss: 0.7770, label: 1, bag_size: 5218\n",
      "Epoch: 0, train_loss: 0.6950, train_error: 0.4638\n",
      "class 0: acc 0.5635838150289018, correct 195/346\n",
      "class 1: acc 0.5097493036211699, correct 183/359\n",
      "\n",
      "Val Set, val_loss: 0.6734, val_error: 0.3750, auc: 0.5609\n",
      "class 0: acc 1.0, correct 49/49\n",
      "class 1: acc 0.03225806451612903, correct 1/31\n",
      "Validation loss decreased (inf --> 0.673404).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6404, label: 0, bag_size: 5125\n",
      "batch 39, loss: 0.6982, label: 1, bag_size: 3402\n",
      "batch 59, loss: 0.6066, label: 0, bag_size: 17546\n",
      "batch 79, loss: 0.4415, label: 0, bag_size: 16789\n",
      "batch 99, loss: 1.1410, label: 1, bag_size: 3374\n",
      "batch 119, loss: 0.5237, label: 0, bag_size: 16842\n",
      "batch 139, loss: 0.8837, label: 1, bag_size: 5152\n",
      "batch 159, loss: 0.9769, label: 1, bag_size: 3777\n",
      "batch 179, loss: 0.9308, label: 1, bag_size: 22171\n",
      "batch 199, loss: 0.6206, label: 1, bag_size: 2189\n",
      "batch 219, loss: 0.6056, label: 1, bag_size: 16451\n",
      "batch 239, loss: 0.5269, label: 1, bag_size: 8007\n",
      "batch 259, loss: 0.7973, label: 0, bag_size: 3917\n",
      "batch 279, loss: 0.5216, label: 0, bag_size: 15976\n",
      "batch 299, loss: 0.5197, label: 0, bag_size: 3420\n",
      "batch 319, loss: 0.5751, label: 0, bag_size: 1095\n",
      "batch 339, loss: 0.5379, label: 1, bag_size: 6421\n",
      "batch 359, loss: 0.9835, label: 0, bag_size: 5100\n",
      "batch 379, loss: 0.8557, label: 0, bag_size: 9003\n",
      "batch 399, loss: 0.9163, label: 1, bag_size: 2824\n",
      "batch 419, loss: 0.6352, label: 0, bag_size: 4084\n",
      "batch 439, loss: 0.8095, label: 0, bag_size: 14168\n",
      "batch 459, loss: 1.1530, label: 0, bag_size: 763\n",
      "batch 479, loss: 0.7173, label: 1, bag_size: 5677\n",
      "batch 499, loss: 0.5074, label: 0, bag_size: 3517\n",
      "batch 519, loss: 0.8661, label: 1, bag_size: 2253\n",
      "batch 539, loss: 0.6621, label: 0, bag_size: 5504\n",
      "batch 559, loss: 0.7360, label: 1, bag_size: 6760\n",
      "batch 579, loss: 0.5852, label: 1, bag_size: 5677\n",
      "batch 599, loss: 0.8280, label: 1, bag_size: 1178\n",
      "batch 619, loss: 0.8577, label: 1, bag_size: 2783\n",
      "batch 639, loss: 0.6539, label: 0, bag_size: 763\n",
      "batch 659, loss: 0.7939, label: 0, bag_size: 2302\n",
      "batch 679, loss: 0.5489, label: 0, bag_size: 4031\n",
      "batch 699, loss: 0.7329, label: 1, bag_size: 19581\n",
      "Epoch: 1, train_loss: 0.6875, train_error: 0.4582\n",
      "class 0: acc 0.6844919786096256, correct 256/374\n",
      "class 1: acc 0.3806646525679758, correct 126/331\n",
      "\n",
      "Val Set, val_loss: 0.6644, val_error: 0.3750, auc: 0.5997\n",
      "class 0: acc 1.0, correct 49/49\n",
      "class 1: acc 0.03225806451612903, correct 1/31\n",
      "Validation loss decreased (0.673404 --> 0.664440).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4907, label: 0, bag_size: 16663\n",
      "batch 39, loss: 0.7992, label: 1, bag_size: 3117\n",
      "batch 59, loss: 0.7260, label: 0, bag_size: 3317\n",
      "batch 79, loss: 0.6383, label: 1, bag_size: 19946\n",
      "batch 99, loss: 0.6137, label: 0, bag_size: 2747\n",
      "batch 119, loss: 0.3863, label: 1, bag_size: 27072\n",
      "batch 139, loss: 0.7659, label: 0, bag_size: 1379\n",
      "batch 159, loss: 0.5500, label: 1, bag_size: 3733\n",
      "batch 179, loss: 0.5160, label: 1, bag_size: 1627\n",
      "batch 199, loss: 0.9154, label: 0, bag_size: 2963\n",
      "batch 219, loss: 0.7532, label: 0, bag_size: 5125\n",
      "batch 239, loss: 0.7432, label: 1, bag_size: 4249\n",
      "batch 259, loss: 0.5627, label: 1, bag_size: 3338\n",
      "batch 279, loss: 0.5580, label: 1, bag_size: 5152\n",
      "batch 299, loss: 0.4970, label: 0, bag_size: 24289\n",
      "batch 319, loss: 0.8156, label: 1, bag_size: 1374\n",
      "batch 339, loss: 0.5075, label: 0, bag_size: 24289\n",
      "batch 359, loss: 0.5258, label: 0, bag_size: 16789\n",
      "batch 379, loss: 0.5580, label: 1, bag_size: 1178\n",
      "batch 399, loss: 0.7167, label: 1, bag_size: 1454\n",
      "batch 419, loss: 0.6745, label: 0, bag_size: 2328\n",
      "batch 439, loss: 1.2268, label: 1, bag_size: 3925\n",
      "batch 459, loss: 0.4624, label: 0, bag_size: 8469\n",
      "batch 479, loss: 0.5199, label: 0, bag_size: 11797\n",
      "batch 499, loss: 0.7451, label: 0, bag_size: 5824\n",
      "batch 519, loss: 0.5432, label: 0, bag_size: 2205\n",
      "batch 539, loss: 0.4322, label: 0, bag_size: 4587\n",
      "batch 559, loss: 0.6996, label: 1, bag_size: 2943\n",
      "batch 579, loss: 0.6175, label: 0, bag_size: 3427\n",
      "batch 599, loss: 0.5441, label: 0, bag_size: 11451\n",
      "batch 619, loss: 0.3551, label: 1, bag_size: 24092\n",
      "batch 639, loss: 0.4730, label: 0, bag_size: 15747\n",
      "batch 659, loss: 0.4336, label: 0, bag_size: 5960\n",
      "batch 679, loss: 0.5244, label: 0, bag_size: 1944\n",
      "batch 699, loss: 0.5979, label: 1, bag_size: 2687\n",
      "Epoch: 2, train_loss: 0.6787, train_error: 0.4284\n",
      "class 0: acc 0.5742857142857143, correct 201/350\n",
      "class 1: acc 0.5690140845070423, correct 202/355\n",
      "\n",
      "Val Set, val_loss: 0.6941, val_error: 0.5125, auc: 0.6248\n",
      "class 0: acc 0.3469387755102041, correct 17/49\n",
      "class 1: acc 0.7096774193548387, correct 22/31\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5712, label: 1, bag_size: 24868\n",
      "batch 39, loss: 0.5635, label: 1, bag_size: 3702\n",
      "batch 59, loss: 0.4187, label: 1, bag_size: 5592\n",
      "batch 79, loss: 0.6139, label: 0, bag_size: 16737\n",
      "batch 99, loss: 0.7530, label: 1, bag_size: 4162\n",
      "batch 119, loss: 0.4689, label: 0, bag_size: 6069\n",
      "batch 139, loss: 1.0560, label: 1, bag_size: 13348\n",
      "batch 159, loss: 0.5971, label: 0, bag_size: 1927\n",
      "batch 179, loss: 0.6596, label: 0, bag_size: 23860\n",
      "batch 199, loss: 0.6291, label: 1, bag_size: 5062\n",
      "batch 219, loss: 0.4132, label: 1, bag_size: 15213\n",
      "batch 239, loss: 0.5108, label: 0, bag_size: 5894\n",
      "batch 259, loss: 0.3014, label: 0, bag_size: 3843\n",
      "batch 279, loss: 0.5886, label: 0, bag_size: 1884\n",
      "batch 299, loss: 0.7302, label: 1, bag_size: 1627\n",
      "batch 319, loss: 0.4488, label: 0, bag_size: 4172\n",
      "batch 339, loss: 0.6920, label: 0, bag_size: 3737\n",
      "batch 359, loss: 0.3706, label: 1, bag_size: 19946\n",
      "batch 379, loss: 0.6322, label: 0, bag_size: 5370\n",
      "batch 399, loss: 0.4984, label: 1, bag_size: 3170\n",
      "batch 419, loss: 0.9503, label: 1, bag_size: 5428\n",
      "batch 439, loss: 0.7481, label: 0, bag_size: 1316\n",
      "batch 459, loss: 0.2886, label: 1, bag_size: 1627\n",
      "batch 479, loss: 0.8526, label: 0, bag_size: 25641\n",
      "batch 499, loss: 1.1950, label: 0, bag_size: 4907\n",
      "batch 519, loss: 0.1823, label: 1, bag_size: 5156\n",
      "batch 539, loss: 0.5441, label: 0, bag_size: 2316\n",
      "batch 559, loss: 0.3700, label: 0, bag_size: 6209\n",
      "batch 579, loss: 0.3871, label: 0, bag_size: 4558\n",
      "batch 599, loss: 0.6710, label: 1, bag_size: 6295\n",
      "batch 619, loss: 0.5742, label: 1, bag_size: 2343\n",
      "batch 639, loss: 0.4456, label: 0, bag_size: 2682\n",
      "batch 659, loss: 0.5819, label: 0, bag_size: 25403\n",
      "batch 679, loss: 1.0386, label: 1, bag_size: 6463\n",
      "batch 699, loss: 0.9803, label: 1, bag_size: 1699\n",
      "Epoch: 3, train_loss: 0.6515, train_error: 0.3773\n",
      "class 0: acc 0.630057803468208, correct 218/346\n",
      "class 1: acc 0.6155988857938719, correct 221/359\n",
      "\n",
      "Val Set, val_loss: 0.6942, val_error: 0.5125, auc: 0.6037\n",
      "class 0: acc 0.42857142857142855, correct 21/49\n",
      "class 1: acc 0.5806451612903226, correct 18/31\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7659, label: 0, bag_size: 7171\n",
      "batch 39, loss: 0.6022, label: 1, bag_size: 10757\n",
      "batch 59, loss: 0.6422, label: 0, bag_size: 2346\n",
      "batch 79, loss: 0.8906, label: 1, bag_size: 15483\n",
      "batch 99, loss: 0.3378, label: 0, bag_size: 5880\n",
      "batch 119, loss: 0.3585, label: 0, bag_size: 1671\n",
      "batch 139, loss: 0.6192, label: 1, bag_size: 2701\n",
      "batch 159, loss: 0.6064, label: 0, bag_size: 2892\n",
      "batch 179, loss: 0.3247, label: 0, bag_size: 5476\n",
      "batch 199, loss: 1.1180, label: 0, bag_size: 5269\n",
      "batch 219, loss: 0.4456, label: 0, bag_size: 6135\n",
      "batch 239, loss: 0.4515, label: 1, bag_size: 5379\n",
      "batch 259, loss: 0.4445, label: 1, bag_size: 6463\n",
      "batch 279, loss: 0.2909, label: 1, bag_size: 6421\n",
      "batch 299, loss: 0.6522, label: 1, bag_size: 1958\n",
      "batch 319, loss: 0.3556, label: 0, bag_size: 4287\n",
      "batch 339, loss: 1.1471, label: 0, bag_size: 5100\n",
      "batch 359, loss: 0.9323, label: 1, bag_size: 4819\n",
      "batch 379, loss: 0.6449, label: 1, bag_size: 16451\n",
      "batch 399, loss: 0.3907, label: 0, bag_size: 3600\n",
      "batch 419, loss: 0.6514, label: 1, bag_size: 2961\n",
      "batch 439, loss: 0.8574, label: 1, bag_size: 6392\n",
      "batch 459, loss: 0.1894, label: 1, bag_size: 11952\n",
      "batch 479, loss: 1.1767, label: 0, bag_size: 5877\n",
      "batch 499, loss: 1.1944, label: 1, bag_size: 1454\n",
      "batch 519, loss: 1.0593, label: 0, bag_size: 5697\n",
      "batch 539, loss: 0.4114, label: 1, bag_size: 5672\n",
      "batch 559, loss: 0.5134, label: 0, bag_size: 2863\n",
      "batch 579, loss: 0.7437, label: 0, bag_size: 4076\n",
      "batch 599, loss: 0.8022, label: 0, bag_size: 4628\n",
      "batch 619, loss: 0.5072, label: 0, bag_size: 9945\n",
      "batch 639, loss: 1.1948, label: 1, bag_size: 3454\n",
      "batch 659, loss: 0.3105, label: 0, bag_size: 13606\n",
      "batch 679, loss: 0.4096, label: 1, bag_size: 2961\n",
      "batch 699, loss: 0.3043, label: 0, bag_size: 5448\n",
      "Epoch: 4, train_loss: 0.6260, train_error: 0.3277\n",
      "class 0: acc 0.6946778711484594, correct 248/357\n",
      "class 1: acc 0.6494252873563219, correct 226/348\n",
      "\n",
      "Val Set, val_loss: 0.6472, val_error: 0.3625, auc: 0.6373\n",
      "class 0: acc 0.7142857142857143, correct 35/49\n",
      "class 1: acc 0.5161290322580645, correct 16/31\n",
      "Validation loss decreased (0.664440 --> 0.647204).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7389, label: 0, bag_size: 2892\n",
      "batch 39, loss: 0.7899, label: 0, bag_size: 1207\n",
      "batch 59, loss: 1.5536, label: 1, bag_size: 3134\n",
      "batch 79, loss: 0.4795, label: 1, bag_size: 3578\n",
      "batch 99, loss: 0.3269, label: 0, bag_size: 4649\n",
      "batch 119, loss: 0.2672, label: 1, bag_size: 15141\n",
      "batch 139, loss: 0.4673, label: 1, bag_size: 3777\n",
      "batch 159, loss: 0.3879, label: 0, bag_size: 5001\n",
      "batch 179, loss: 0.3961, label: 0, bag_size: 2362\n",
      "batch 199, loss: 0.0865, label: 1, bag_size: 5156\n",
      "batch 219, loss: 0.2854, label: 1, bag_size: 12948\n",
      "batch 239, loss: 0.2564, label: 1, bag_size: 2961\n",
      "batch 259, loss: 1.1788, label: 1, bag_size: 6812\n",
      "batch 279, loss: 0.3058, label: 0, bag_size: 4750\n",
      "batch 299, loss: 0.3335, label: 0, bag_size: 14662\n",
      "batch 319, loss: 0.3466, label: 0, bag_size: 13573\n",
      "batch 339, loss: 1.6967, label: 1, bag_size: 1178\n",
      "batch 359, loss: 0.1704, label: 0, bag_size: 1943\n",
      "batch 379, loss: 1.5446, label: 0, bag_size: 15687\n",
      "batch 399, loss: 0.3286, label: 1, bag_size: 4233\n",
      "batch 419, loss: 1.5546, label: 1, bag_size: 1004\n",
      "batch 439, loss: 0.0999, label: 1, bag_size: 6371\n",
      "batch 459, loss: 0.1773, label: 1, bag_size: 7641\n",
      "batch 479, loss: 0.2640, label: 1, bag_size: 21059\n",
      "batch 499, loss: 1.1107, label: 1, bag_size: 3727\n",
      "batch 519, loss: 0.5786, label: 0, bag_size: 4649\n",
      "batch 539, loss: 0.6530, label: 1, bag_size: 11295\n",
      "batch 559, loss: 0.5189, label: 0, bag_size: 4162\n",
      "batch 579, loss: 1.1221, label: 1, bag_size: 3159\n",
      "batch 599, loss: 1.0540, label: 0, bag_size: 16309\n",
      "batch 619, loss: 0.3576, label: 0, bag_size: 2338\n",
      "batch 639, loss: 1.3625, label: 0, bag_size: 6909\n",
      "batch 659, loss: 0.6813, label: 1, bag_size: 2646\n",
      "batch 679, loss: 0.6502, label: 0, bag_size: 12861\n",
      "batch 699, loss: 0.2983, label: 1, bag_size: 24868\n",
      "Epoch: 5, train_loss: 0.5954, train_error: 0.2979\n",
      "class 0: acc 0.76, correct 266/350\n",
      "class 1: acc 0.6450704225352113, correct 229/355\n",
      "\n",
      "Val Set, val_loss: 0.5923, val_error: 0.2500, auc: 0.6741\n",
      "class 0: acc 0.9387755102040817, correct 46/49\n",
      "class 1: acc 0.45161290322580644, correct 14/31\n",
      "Validation loss decreased (0.647204 --> 0.592344).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 1.4484, label: 1, bag_size: 2835\n",
      "batch 39, loss: 0.8656, label: 0, bag_size: 14212\n",
      "batch 59, loss: 0.8405, label: 1, bag_size: 2343\n",
      "batch 79, loss: 0.4311, label: 0, bag_size: 19035\n",
      "batch 99, loss: 0.7045, label: 1, bag_size: 16675\n",
      "batch 119, loss: 0.9857, label: 1, bag_size: 534\n",
      "batch 139, loss: 1.0356, label: 1, bag_size: 1004\n",
      "batch 159, loss: 0.5738, label: 0, bag_size: 2792\n",
      "batch 179, loss: 0.7562, label: 0, bag_size: 5546\n",
      "batch 199, loss: 0.9668, label: 1, bag_size: 3925\n",
      "batch 219, loss: 0.8459, label: 1, bag_size: 2835\n",
      "batch 239, loss: 0.6031, label: 0, bag_size: 2747\n",
      "batch 259, loss: 1.0592, label: 1, bag_size: 1454\n",
      "batch 279, loss: 0.3655, label: 1, bag_size: 11563\n",
      "batch 299, loss: 1.0703, label: 0, bag_size: 2146\n",
      "batch 319, loss: 0.6138, label: 0, bag_size: 11451\n",
      "batch 339, loss: 0.4459, label: 1, bag_size: 10757\n",
      "batch 359, loss: 0.1602, label: 1, bag_size: 4233\n",
      "batch 379, loss: 0.9398, label: 0, bag_size: 9127\n",
      "batch 399, loss: 0.5109, label: 0, bag_size: 4329\n",
      "batch 419, loss: 0.5047, label: 1, bag_size: 15118\n",
      "batch 439, loss: 0.4799, label: 1, bag_size: 20256\n",
      "batch 459, loss: 0.5941, label: 0, bag_size: 4098\n",
      "batch 479, loss: 0.5541, label: 0, bag_size: 3598\n",
      "batch 499, loss: 0.4310, label: 1, bag_size: 2899\n",
      "batch 519, loss: 0.3772, label: 1, bag_size: 4800\n",
      "batch 539, loss: 0.3250, label: 0, bag_size: 3448\n",
      "batch 559, loss: 0.6715, label: 1, bag_size: 19013\n",
      "batch 579, loss: 0.1531, label: 1, bag_size: 12948\n",
      "batch 599, loss: 0.1089, label: 1, bag_size: 4803\n",
      "batch 619, loss: 1.0907, label: 1, bag_size: 4112\n",
      "batch 639, loss: 0.6204, label: 1, bag_size: 6138\n",
      "batch 659, loss: 0.9347, label: 0, bag_size: 5125\n",
      "batch 679, loss: 0.6271, label: 0, bag_size: 2298\n",
      "batch 699, loss: 0.5834, label: 0, bag_size: 4146\n",
      "Epoch: 6, train_loss: 0.6270, train_error: 0.3645\n",
      "class 0: acc 0.7297297297297297, correct 243/333\n",
      "class 1: acc 0.5510752688172043, correct 205/372\n",
      "\n",
      "Val Set, val_loss: 0.6382, val_error: 0.3250, auc: 0.7024\n",
      "class 0: acc 0.7959183673469388, correct 39/49\n",
      "class 1: acc 0.4838709677419355, correct 15/31\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5977, label: 0, bag_size: 3184\n",
      "batch 39, loss: 0.3646, label: 1, bag_size: 11563\n",
      "batch 59, loss: 0.3833, label: 0, bag_size: 9367\n",
      "batch 79, loss: 0.2376, label: 1, bag_size: 3548\n",
      "batch 99, loss: 1.0268, label: 0, bag_size: 14212\n",
      "batch 119, loss: 1.5225, label: 1, bag_size: 6759\n",
      "batch 139, loss: 0.1167, label: 1, bag_size: 4452\n",
      "batch 159, loss: 1.2263, label: 1, bag_size: 5665\n",
      "batch 179, loss: 0.4236, label: 1, bag_size: 4249\n",
      "batch 199, loss: 0.6947, label: 1, bag_size: 4285\n",
      "batch 219, loss: 0.6709, label: 0, bag_size: 3943\n",
      "batch 239, loss: 0.7883, label: 1, bag_size: 5671\n",
      "batch 259, loss: 0.0635, label: 1, bag_size: 24092\n",
      "batch 279, loss: 0.1722, label: 1, bag_size: 4802\n",
      "batch 299, loss: 0.6861, label: 0, bag_size: 1865\n",
      "batch 319, loss: 1.1306, label: 1, bag_size: 6812\n",
      "batch 339, loss: 0.1764, label: 1, bag_size: 4452\n",
      "batch 359, loss: 0.2828, label: 0, bag_size: 2667\n",
      "batch 379, loss: 0.4526, label: 0, bag_size: 5982\n",
      "batch 399, loss: 0.2862, label: 0, bag_size: 5982\n",
      "batch 419, loss: 1.6131, label: 1, bag_size: 5665\n",
      "batch 439, loss: 0.1749, label: 0, bag_size: 1943\n",
      "batch 459, loss: 0.1349, label: 0, bag_size: 1488\n",
      "batch 479, loss: 0.3843, label: 0, bag_size: 10568\n",
      "batch 499, loss: 0.2205, label: 1, bag_size: 2250\n",
      "batch 519, loss: 0.4215, label: 1, bag_size: 1251\n",
      "batch 539, loss: 0.3529, label: 1, bag_size: 2178\n",
      "batch 559, loss: 0.4229, label: 0, bag_size: 16737\n",
      "batch 579, loss: 0.6250, label: 1, bag_size: 3420\n",
      "batch 599, loss: 0.1891, label: 1, bag_size: 3533\n",
      "batch 619, loss: 0.0779, label: 1, bag_size: 3905\n",
      "batch 639, loss: 0.4391, label: 0, bag_size: 3228\n",
      "batch 659, loss: 0.5862, label: 0, bag_size: 3317\n",
      "batch 679, loss: 0.2787, label: 0, bag_size: 5382\n",
      "batch 699, loss: 0.7038, label: 1, bag_size: 6841\n",
      "Epoch: 7, train_loss: 0.5660, train_error: 0.2993\n",
      "class 0: acc 0.7859237536656891, correct 268/341\n",
      "class 1: acc 0.6208791208791209, correct 226/364\n",
      "\n",
      "Val Set, val_loss: 0.6263, val_error: 0.4125, auc: 0.7136\n",
      "class 0: acc 0.6122448979591837, correct 30/49\n",
      "class 1: acc 0.5483870967741935, correct 17/31\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0957, label: 1, bag_size: 19013\n",
      "batch 39, loss: 0.4816, label: 1, bag_size: 16451\n",
      "batch 59, loss: 1.3128, label: 1, bag_size: 10952\n",
      "batch 79, loss: 0.1681, label: 1, bag_size: 4458\n",
      "batch 99, loss: 0.2451, label: 0, bag_size: 3070\n",
      "batch 119, loss: 0.3778, label: 0, bag_size: 3706\n",
      "batch 139, loss: 0.1351, label: 1, bag_size: 5641\n",
      "batch 159, loss: 0.2849, label: 1, bag_size: 2899\n",
      "batch 179, loss: 0.6191, label: 0, bag_size: 3118\n",
      "batch 199, loss: 0.9842, label: 1, bag_size: 6912\n",
      "batch 219, loss: 0.4545, label: 0, bag_size: 3372\n",
      "batch 239, loss: 0.3989, label: 1, bag_size: 3391\n",
      "batch 259, loss: 0.3487, label: 1, bag_size: 4332\n",
      "batch 279, loss: 0.1379, label: 1, bag_size: 5428\n",
      "batch 299, loss: 0.4209, label: 0, bag_size: 7290\n",
      "batch 319, loss: 0.5228, label: 0, bag_size: 28144\n",
      "batch 339, loss: 0.3913, label: 0, bag_size: 2146\n",
      "batch 359, loss: 0.2962, label: 0, bag_size: 7179\n",
      "batch 379, loss: 0.1231, label: 1, bag_size: 6421\n",
      "batch 399, loss: 0.5931, label: 0, bag_size: 4427\n",
      "batch 419, loss: 0.7021, label: 1, bag_size: 1251\n",
      "batch 439, loss: 0.6509, label: 1, bag_size: 3925\n",
      "batch 459, loss: 1.7059, label: 0, bag_size: 4758\n",
      "batch 479, loss: 0.0342, label: 1, bag_size: 3834\n",
      "batch 499, loss: 0.5758, label: 1, bag_size: 2624\n",
      "batch 519, loss: 0.4893, label: 0, bag_size: 3381\n",
      "batch 539, loss: 0.3442, label: 0, bag_size: 3448\n",
      "batch 559, loss: 0.6943, label: 1, bag_size: 2253\n",
      "batch 579, loss: 0.7393, label: 0, bag_size: 2298\n",
      "batch 599, loss: 0.1486, label: 0, bag_size: 4283\n",
      "batch 619, loss: 0.8097, label: 0, bag_size: 3420\n",
      "batch 639, loss: 0.2857, label: 0, bag_size: 3295\n",
      "batch 659, loss: 0.2563, label: 0, bag_size: 4076\n",
      "batch 679, loss: 0.2330, label: 0, bag_size: 1539\n",
      "batch 699, loss: 0.2371, label: 1, bag_size: 2824\n",
      "Epoch: 8, train_loss: 0.5699, train_error: 0.3064\n",
      "class 0: acc 0.7636887608069164, correct 265/347\n",
      "class 1: acc 0.6256983240223464, correct 224/358\n",
      "\n",
      "Val Set, val_loss: 0.5898, val_error: 0.3250, auc: 0.7215\n",
      "class 0: acc 0.7959183673469388, correct 39/49\n",
      "class 1: acc 0.4838709677419355, correct 15/31\n",
      "Validation loss decreased (0.592344 --> 0.589779).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5112, label: 1, bag_size: 8021\n",
      "batch 39, loss: 1.0154, label: 1, bag_size: 3764\n",
      "batch 59, loss: 0.2100, label: 0, bag_size: 7179\n",
      "batch 79, loss: 0.3452, label: 0, bag_size: 2923\n",
      "batch 99, loss: 0.5305, label: 0, bag_size: 4907\n",
      "batch 119, loss: 1.6766, label: 0, bag_size: 6909\n",
      "batch 139, loss: 0.1059, label: 1, bag_size: 6600\n",
      "batch 159, loss: 0.1591, label: 1, bag_size: 3670\n",
      "batch 179, loss: 1.0723, label: 0, bag_size: 1648\n",
      "batch 199, loss: 0.9763, label: 1, bag_size: 2943\n",
      "batch 219, loss: 0.3116, label: 0, bag_size: 2338\n",
      "batch 239, loss: 1.1895, label: 1, bag_size: 1374\n",
      "batch 259, loss: 0.3737, label: 0, bag_size: 5307\n",
      "batch 279, loss: 0.5741, label: 1, bag_size: 5677\n",
      "batch 299, loss: 0.4894, label: 0, bag_size: 4558\n",
      "batch 319, loss: 0.7534, label: 0, bag_size: 3600\n",
      "batch 339, loss: 0.9439, label: 1, bag_size: 3588\n",
      "batch 359, loss: 0.5858, label: 0, bag_size: 3420\n",
      "batch 379, loss: 0.2417, label: 0, bag_size: 5617\n",
      "batch 399, loss: 0.3835, label: 0, bag_size: 4572\n",
      "batch 419, loss: 0.1543, label: 1, bag_size: 9643\n",
      "batch 439, loss: 0.3498, label: 0, bag_size: 1884\n",
      "batch 459, loss: 0.8287, label: 0, bag_size: 5211\n",
      "batch 479, loss: 0.1298, label: 1, bag_size: 3672\n",
      "batch 499, loss: 0.0784, label: 1, bag_size: 3834\n",
      "batch 519, loss: 0.3934, label: 0, bag_size: 9367\n",
      "batch 539, loss: 0.7855, label: 0, bag_size: 2529\n",
      "batch 559, loss: 0.3275, label: 0, bag_size: 4587\n",
      "batch 579, loss: 0.2682, label: 0, bag_size: 938\n",
      "batch 599, loss: 0.7403, label: 0, bag_size: 2851\n",
      "batch 619, loss: 0.3650, label: 0, bag_size: 4329\n",
      "batch 639, loss: 0.4786, label: 0, bag_size: 7862\n",
      "batch 659, loss: 0.6915, label: 0, bag_size: 2611\n",
      "batch 679, loss: 0.2897, label: 0, bag_size: 4283\n",
      "batch 699, loss: 0.2523, label: 0, bag_size: 3070\n",
      "Epoch: 9, train_loss: 0.5677, train_error: 0.3035\n",
      "class 0: acc 0.8102981029810298, correct 299/369\n",
      "class 1: acc 0.5714285714285714, correct 192/336\n",
      "\n",
      "Val Set, val_loss: 0.5445, val_error: 0.2375, auc: 0.7439\n",
      "class 0: acc 0.9591836734693877, correct 47/49\n",
      "class 1: acc 0.45161290322580644, correct 14/31\n",
      "Validation loss decreased (0.589779 --> 0.544457).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2312, label: 1, bag_size: 5068\n",
      "batch 39, loss: 0.4089, label: 0, bag_size: 5824\n",
      "batch 59, loss: 0.5640, label: 1, bag_size: 6841\n",
      "batch 79, loss: 0.3417, label: 1, bag_size: 2207\n",
      "batch 99, loss: 0.2051, label: 1, bag_size: 6410\n",
      "batch 119, loss: 0.4551, label: 0, bag_size: 4628\n",
      "batch 139, loss: 1.0305, label: 0, bag_size: 5697\n",
      "batch 159, loss: 0.1945, label: 0, bag_size: 2238\n",
      "batch 179, loss: 0.3080, label: 1, bag_size: 2824\n",
      "batch 199, loss: 0.4649, label: 1, bag_size: 3990\n",
      "batch 219, loss: 0.2518, label: 0, bag_size: 3843\n",
      "batch 239, loss: 0.3689, label: 0, bag_size: 5536\n",
      "batch 259, loss: 0.5429, label: 0, bag_size: 22594\n",
      "batch 279, loss: 0.7392, label: 1, bag_size: 534\n",
      "batch 299, loss: 0.2848, label: 1, bag_size: 5592\n",
      "batch 319, loss: 0.3250, label: 0, bag_size: 5522\n",
      "batch 339, loss: 0.2860, label: 0, bag_size: 5299\n",
      "batch 359, loss: 0.2415, label: 1, bag_size: 5592\n",
      "batch 379, loss: 1.4973, label: 0, bag_size: 6605\n",
      "batch 399, loss: 0.3762, label: 0, bag_size: 25933\n",
      "batch 419, loss: 1.1245, label: 1, bag_size: 13217\n",
      "batch 439, loss: 0.8973, label: 1, bag_size: 5062\n",
      "batch 459, loss: 0.0870, label: 1, bag_size: 24868\n",
      "batch 479, loss: 0.2531, label: 1, bag_size: 2899\n",
      "batch 499, loss: 1.2341, label: 1, bag_size: 2356\n",
      "batch 519, loss: 0.3375, label: 0, bag_size: 24289\n",
      "batch 539, loss: 0.2292, label: 0, bag_size: 14662\n",
      "batch 559, loss: 0.8131, label: 0, bag_size: 1822\n",
      "batch 579, loss: 0.6595, label: 0, bag_size: 4124\n",
      "batch 599, loss: 1.2630, label: 1, bag_size: 2840\n",
      "batch 619, loss: 0.2103, label: 1, bag_size: 3702\n",
      "batch 639, loss: 0.5200, label: 0, bag_size: 2863\n",
      "batch 659, loss: 1.2736, label: 0, bag_size: 2533\n",
      "batch 679, loss: 0.3173, label: 0, bag_size: 5001\n",
      "batch 699, loss: 0.3043, label: 0, bag_size: 3616\n",
      "Epoch: 10, train_loss: 0.5442, train_error: 0.2865\n",
      "class 0: acc 0.7931034482758621, correct 276/348\n",
      "class 1: acc 0.6358543417366946, correct 227/357\n",
      "\n",
      "Val Set, val_loss: 0.5312, val_error: 0.2375, auc: 0.7828\n",
      "class 0: acc 0.9387755102040817, correct 46/49\n",
      "class 1: acc 0.4838709677419355, correct 15/31\n",
      "Validation loss decreased (0.544457 --> 0.531163).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7579, label: 1, bag_size: 13226\n",
      "batch 39, loss: 0.8666, label: 1, bag_size: 3420\n",
      "batch 59, loss: 0.3849, label: 0, bag_size: 5499\n",
      "batch 79, loss: 1.4852, label: 1, bag_size: 5298\n",
      "batch 99, loss: 0.6072, label: 1, bag_size: 12654\n",
      "batch 119, loss: 0.0545, label: 1, bag_size: 2250\n",
      "batch 139, loss: 0.3116, label: 0, bag_size: 3001\n",
      "batch 159, loss: 0.4457, label: 0, bag_size: 5499\n",
      "batch 179, loss: 0.5996, label: 0, bag_size: 10962\n",
      "batch 199, loss: 0.2669, label: 0, bag_size: 11380\n",
      "batch 219, loss: 1.3107, label: 1, bag_size: 13226\n",
      "batch 239, loss: 0.2788, label: 1, bag_size: 4458\n",
      "batch 259, loss: 0.7334, label: 1, bag_size: 4802\n",
      "batch 279, loss: 0.4934, label: 0, bag_size: 1553\n",
      "batch 299, loss: 0.3732, label: 0, bag_size: 1553\n",
      "batch 319, loss: 1.1581, label: 0, bag_size: 6534\n",
      "batch 339, loss: 0.6654, label: 0, bag_size: 4431\n",
      "batch 359, loss: 0.2011, label: 1, bag_size: 3533\n",
      "batch 379, loss: 1.2611, label: 1, bag_size: 5665\n",
      "batch 399, loss: 0.6641, label: 0, bag_size: 2678\n",
      "batch 419, loss: 0.5972, label: 0, bag_size: 19037\n",
      "batch 439, loss: 0.6554, label: 1, bag_size: 4162\n",
      "batch 459, loss: 0.2702, label: 0, bag_size: 5960\n",
      "batch 479, loss: 0.2292, label: 0, bag_size: 2094\n",
      "batch 499, loss: 0.5231, label: 0, bag_size: 7484\n",
      "batch 519, loss: 0.5202, label: 1, bag_size: 5199\n",
      "batch 539, loss: 0.4517, label: 0, bag_size: 9945\n",
      "batch 559, loss: 0.8686, label: 1, bag_size: 6752\n",
      "batch 579, loss: 0.0231, label: 1, bag_size: 5507\n",
      "batch 599, loss: 0.0720, label: 1, bag_size: 3578\n",
      "batch 619, loss: 0.7356, label: 0, bag_size: 3307\n",
      "batch 639, loss: 0.2391, label: 0, bag_size: 938\n",
      "batch 659, loss: 1.3187, label: 0, bag_size: 3793\n",
      "batch 679, loss: 0.2509, label: 0, bag_size: 2013\n",
      "batch 699, loss: 0.2550, label: 1, bag_size: 10736\n",
      "Epoch: 11, train_loss: 0.5392, train_error: 0.2652\n",
      "class 0: acc 0.8531073446327684, correct 302/354\n",
      "class 1: acc 0.6153846153846154, correct 216/351\n",
      "\n",
      "Val Set, val_loss: 0.5430, val_error: 0.2875, auc: 0.7518\n",
      "class 0: acc 0.7959183673469388, correct 39/49\n",
      "class 1: acc 0.5806451612903226, correct 18/31\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.3028, label: 0, bag_size: 1822\n",
      "batch 39, loss: 0.3710, label: 0, bag_size: 4066\n",
      "batch 59, loss: 0.4077, label: 1, bag_size: 3402\n",
      "batch 79, loss: 0.2166, label: 0, bag_size: 1794\n",
      "batch 99, loss: 0.4160, label: 1, bag_size: 6477\n",
      "batch 119, loss: 0.0983, label: 1, bag_size: 1722\n",
      "batch 139, loss: 1.2427, label: 0, bag_size: 3420\n",
      "batch 159, loss: 0.3131, label: 0, bag_size: 16737\n",
      "batch 179, loss: 0.4546, label: 0, bag_size: 5448\n",
      "batch 199, loss: 0.3613, label: 0, bag_size: 1944\n",
      "batch 219, loss: 0.5597, label: 1, bag_size: 2701\n",
      "batch 239, loss: 0.8792, label: 1, bag_size: 23277\n",
      "batch 259, loss: 0.1930, label: 1, bag_size: 1316\n",
      "batch 279, loss: 0.1402, label: 0, bag_size: 2908\n",
      "batch 299, loss: 0.6032, label: 1, bag_size: 8075\n",
      "batch 319, loss: 0.5640, label: 0, bag_size: 4468\n",
      "batch 339, loss: 0.6288, label: 1, bag_size: 11657\n",
      "batch 359, loss: 0.0565, label: 1, bag_size: 24686\n",
      "batch 379, loss: 0.1846, label: 1, bag_size: 16427\n",
      "batch 399, loss: 1.9643, label: 0, bag_size: 3161\n",
      "batch 419, loss: 0.6386, label: 1, bag_size: 4243\n",
      "batch 439, loss: 0.2950, label: 0, bag_size: 4303\n",
      "batch 459, loss: 0.3352, label: 0, bag_size: 4226\n",
      "batch 479, loss: 2.1608, label: 1, bag_size: 19307\n",
      "batch 499, loss: 0.7252, label: 0, bag_size: 11018\n",
      "batch 519, loss: 1.0935, label: 0, bag_size: 16309\n",
      "batch 539, loss: 0.0771, label: 1, bag_size: 3834\n",
      "batch 559, loss: 0.2579, label: 0, bag_size: 3706\n",
      "batch 579, loss: 0.2237, label: 0, bag_size: 5894\n",
      "batch 599, loss: 0.2200, label: 0, bag_size: 3295\n",
      "batch 619, loss: 0.3482, label: 0, bag_size: 3093\n",
      "batch 639, loss: 0.2456, label: 1, bag_size: 5458\n",
      "batch 659, loss: 0.2245, label: 1, bag_size: 3672\n",
      "batch 679, loss: 0.3120, label: 0, bag_size: 3869\n",
      "batch 699, loss: 0.2476, label: 0, bag_size: 2328\n",
      "Epoch: 12, train_loss: 0.5349, train_error: 0.2411\n",
      "class 0: acc 0.8366197183098592, correct 297/355\n",
      "class 1: acc 0.68, correct 238/350\n",
      "\n",
      "Val Set, val_loss: 0.5315, val_error: 0.2375, auc: 0.7966\n",
      "class 0: acc 0.8571428571428571, correct 42/49\n",
      "class 1: acc 0.6129032258064516, correct 19/31\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1184, label: 1, bag_size: 6600\n",
      "batch 39, loss: 0.1632, label: 0, bag_size: 1943\n",
      "batch 59, loss: 0.2799, label: 1, bag_size: 19581\n",
      "batch 79, loss: 0.1824, label: 0, bag_size: 1034\n",
      "batch 99, loss: 0.3394, label: 1, bag_size: 5068\n",
      "batch 119, loss: 0.6272, label: 1, bag_size: 16427\n",
      "batch 139, loss: 0.1020, label: 1, bag_size: 6861\n",
      "batch 159, loss: 0.3756, label: 0, bag_size: 3184\n",
      "batch 179, loss: 0.1691, label: 1, bag_size: 1700\n",
      "batch 199, loss: 0.3968, label: 0, bag_size: 15747\n",
      "batch 219, loss: 0.1424, label: 1, bag_size: 986\n",
      "batch 239, loss: 0.4851, label: 0, bag_size: 5413\n",
      "batch 259, loss: 0.0699, label: 1, bag_size: 11555\n",
      "batch 279, loss: 0.2249, label: 0, bag_size: 23860\n",
      "batch 299, loss: 0.0269, label: 1, bag_size: 3905\n",
      "batch 319, loss: 0.8444, label: 1, bag_size: 4880\n",
      "batch 339, loss: 0.5218, label: 0, bag_size: 16789\n",
      "batch 359, loss: 0.2535, label: 0, bag_size: 10590\n",
      "batch 379, loss: 1.2617, label: 1, bag_size: 1582\n",
      "batch 399, loss: 0.5119, label: 0, bag_size: 2611\n",
      "batch 419, loss: 0.8886, label: 1, bag_size: 11657\n",
      "batch 439, loss: 0.4865, label: 0, bag_size: 3327\n",
      "batch 459, loss: 0.5312, label: 0, bag_size: 4226\n",
      "batch 479, loss: 0.7058, label: 0, bag_size: 11018\n",
      "batch 499, loss: 1.2050, label: 0, bag_size: 5697\n",
      "batch 519, loss: 0.2007, label: 0, bag_size: 2667\n",
      "batch 539, loss: 0.0385, label: 1, bag_size: 7641\n",
      "batch 559, loss: 0.0472, label: 1, bag_size: 4159\n",
      "batch 579, loss: 0.0131, label: 1, bag_size: 24092\n",
      "batch 599, loss: 0.1856, label: 0, bag_size: 3950\n",
      "batch 619, loss: 0.1510, label: 1, bag_size: 8474\n",
      "batch 639, loss: 0.9639, label: 1, bag_size: 2036\n",
      "batch 659, loss: 0.1644, label: 0, bag_size: 5624\n",
      "batch 679, loss: 2.2854, label: 1, bag_size: 3358\n",
      "batch 699, loss: 0.2395, label: 1, bag_size: 6138\n",
      "Epoch: 13, train_loss: 0.5349, train_error: 0.2624\n",
      "class 0: acc 0.8132183908045977, correct 283/348\n",
      "class 1: acc 0.6638655462184874, correct 237/357\n",
      "\n",
      "Val Set, val_loss: 0.5107, val_error: 0.2625, auc: 0.7749\n",
      "class 0: acc 0.8775510204081632, correct 43/49\n",
      "class 1: acc 0.5161290322580645, correct 16/31\n",
      "Validation loss decreased (0.531163 --> 0.510722).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3585, label: 0, bag_size: 9622\n",
      "batch 39, loss: 0.2477, label: 1, bag_size: 3533\n",
      "batch 59, loss: 1.0344, label: 0, bag_size: 3489\n",
      "batch 79, loss: 0.0657, label: 1, bag_size: 5218\n",
      "batch 99, loss: 0.2884, label: 0, bag_size: 941\n",
      "batch 119, loss: 0.3974, label: 0, bag_size: 4468\n",
      "batch 139, loss: 0.3914, label: 0, bag_size: 3392\n",
      "batch 159, loss: 0.8541, label: 1, bag_size: 641\n",
      "batch 179, loss: 0.2659, label: 0, bag_size: 7027\n",
      "batch 199, loss: 0.4502, label: 0, bag_size: 5824\n",
      "batch 219, loss: 1.4269, label: 1, bag_size: 3962\n",
      "batch 239, loss: 0.7466, label: 0, bag_size: 3553\n",
      "batch 259, loss: 1.1895, label: 1, bag_size: 5638\n",
      "batch 279, loss: 1.4438, label: 1, bag_size: 21399\n",
      "batch 299, loss: 1.1791, label: 1, bag_size: 5065\n",
      "batch 319, loss: 0.4588, label: 0, bag_size: 16816\n",
      "batch 339, loss: 1.5355, label: 1, bag_size: 6759\n",
      "batch 359, loss: 0.2963, label: 0, bag_size: 25403\n",
      "batch 379, loss: 0.2966, label: 0, bag_size: 10568\n",
      "batch 399, loss: 0.1994, label: 0, bag_size: 17273\n",
      "batch 419, loss: 0.2334, label: 1, bag_size: 1961\n",
      "batch 439, loss: 0.2702, label: 0, bag_size: 3001\n",
      "batch 459, loss: 0.2004, label: 0, bag_size: 2586\n",
      "batch 479, loss: 1.0196, label: 0, bag_size: 11299\n",
      "batch 499, loss: 1.3464, label: 1, bag_size: 22171\n",
      "batch 519, loss: 0.8185, label: 1, bag_size: 3588\n",
      "batch 539, loss: 0.3305, label: 0, bag_size: 4907\n",
      "batch 559, loss: 0.1628, label: 0, bag_size: 5624\n",
      "batch 579, loss: 0.1198, label: 0, bag_size: 4500\n",
      "batch 599, loss: 1.1130, label: 1, bag_size: 27072\n",
      "batch 619, loss: 0.4650, label: 1, bag_size: 2779\n",
      "batch 639, loss: 1.0444, label: 1, bag_size: 6392\n",
      "batch 659, loss: 0.0307, label: 1, bag_size: 4069\n",
      "batch 679, loss: 0.6111, label: 1, bag_size: 22171\n",
      "batch 699, loss: 0.4241, label: 1, bag_size: 18015\n",
      "Epoch: 14, train_loss: 0.5209, train_error: 0.2539\n",
      "class 0: acc 0.8419618528610354, correct 309/367\n",
      "class 1: acc 0.6420118343195266, correct 217/338\n",
      "\n",
      "Val Set, val_loss: 0.4809, val_error: 0.2250, auc: 0.8394\n",
      "class 0: acc 0.9591836734693877, correct 47/49\n",
      "class 1: acc 0.4838709677419355, correct 15/31\n",
      "Validation loss decreased (0.510722 --> 0.480942).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0469, label: 1, bag_size: 24092\n",
      "batch 39, loss: 0.4832, label: 1, bag_size: 23277\n",
      "batch 59, loss: 1.3421, label: 1, bag_size: 13217\n",
      "batch 79, loss: 0.4390, label: 1, bag_size: 2890\n",
      "batch 99, loss: 0.4989, label: 1, bag_size: 1178\n",
      "batch 119, loss: 0.5765, label: 0, bag_size: 25933\n",
      "batch 139, loss: 0.7304, label: 1, bag_size: 2877\n",
      "batch 159, loss: 1.2008, label: 0, bag_size: 3161\n",
      "batch 179, loss: 0.9527, label: 0, bag_size: 4801\n",
      "batch 199, loss: 0.3072, label: 0, bag_size: 2792\n",
      "batch 219, loss: 0.2107, label: 0, bag_size: 10962\n",
      "batch 239, loss: 2.5444, label: 0, bag_size: 3161\n",
      "batch 259, loss: 0.4143, label: 0, bag_size: 4431\n",
      "batch 279, loss: 0.2522, label: 0, bag_size: 3090\n",
      "batch 299, loss: 0.9195, label: 1, bag_size: 11657\n",
      "batch 319, loss: 0.0711, label: 1, bag_size: 6600\n",
      "batch 339, loss: 1.0479, label: 0, bag_size: 2480\n",
      "batch 359, loss: 0.6522, label: 1, bag_size: 5677\n",
      "batch 379, loss: 1.4234, label: 1, bag_size: 2626\n",
      "batch 399, loss: 0.5506, label: 1, bag_size: 4737\n",
      "batch 419, loss: 0.3083, label: 0, bag_size: 2480\n",
      "batch 439, loss: 0.4249, label: 0, bag_size: 2482\n",
      "batch 459, loss: 0.9976, label: 1, bag_size: 15407\n",
      "batch 479, loss: 0.0263, label: 1, bag_size: 3905\n",
      "batch 499, loss: 0.3794, label: 1, bag_size: 4795\n",
      "batch 519, loss: 0.8179, label: 0, bag_size: 5642\n",
      "batch 539, loss: 0.6158, label: 0, bag_size: 6301\n",
      "batch 559, loss: 0.5052, label: 1, bag_size: 2905\n",
      "batch 579, loss: 0.1347, label: 0, bag_size: 3450\n",
      "batch 599, loss: 0.3246, label: 0, bag_size: 24289\n",
      "batch 619, loss: 0.2066, label: 0, bag_size: 1034\n",
      "batch 639, loss: 0.1449, label: 0, bag_size: 1892\n",
      "batch 659, loss: 0.1280, label: 1, bag_size: 4436\n",
      "batch 679, loss: 0.4434, label: 1, bag_size: 15141\n",
      "batch 699, loss: 0.1090, label: 1, bag_size: 5577\n",
      "Epoch: 15, train_loss: 0.4872, train_error: 0.2397\n",
      "class 0: acc 0.8571428571428571, correct 306/357\n",
      "class 1: acc 0.6609195402298851, correct 230/348\n",
      "\n",
      "Val Set, val_loss: 0.5171, val_error: 0.2250, auc: 0.8137\n",
      "class 0: acc 0.8571428571428571, correct 42/49\n",
      "class 1: acc 0.6451612903225806, correct 20/31\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0431, label: 1, bag_size: 3184\n",
      "batch 39, loss: 0.0617, label: 0, bag_size: 5624\n",
      "batch 59, loss: 0.6540, label: 0, bag_size: 3317\n",
      "batch 79, loss: 0.5682, label: 0, bag_size: 6593\n",
      "batch 99, loss: 0.0814, label: 0, bag_size: 2622\n",
      "batch 119, loss: 0.3042, label: 0, bag_size: 2006\n",
      "batch 139, loss: 0.0885, label: 1, bag_size: 5887\n",
      "batch 159, loss: 0.8597, label: 0, bag_size: 5967\n",
      "batch 179, loss: 0.4929, label: 0, bag_size: 5527\n",
      "batch 199, loss: 0.2078, label: 0, bag_size: 3290\n",
      "batch 219, loss: 1.3926, label: 0, bag_size: 1226\n",
      "batch 239, loss: 0.5198, label: 1, bag_size: 534\n",
      "batch 259, loss: 0.4530, label: 1, bag_size: 14487\n",
      "batch 279, loss: 0.4862, label: 0, bag_size: 4066\n",
      "batch 299, loss: 0.3059, label: 0, bag_size: 4558\n",
      "batch 319, loss: 0.6864, label: 0, bag_size: 4758\n",
      "batch 339, loss: 0.0921, label: 1, bag_size: 3117\n",
      "batch 359, loss: 0.2412, label: 0, bag_size: 15193\n",
      "batch 379, loss: 0.3176, label: 0, bag_size: 9973\n",
      "batch 399, loss: 0.0747, label: 1, bag_size: 4377\n",
      "batch 419, loss: 1.0154, label: 1, bag_size: 1374\n",
      "batch 439, loss: 0.6208, label: 0, bag_size: 6391\n",
      "batch 459, loss: 0.2366, label: 0, bag_size: 3256\n",
      "batch 479, loss: 0.2089, label: 0, bag_size: 3843\n",
      "batch 499, loss: 0.3722, label: 1, bag_size: 2207\n",
      "batch 519, loss: 0.4102, label: 0, bag_size: 14856\n",
      "batch 539, loss: 0.3160, label: 1, bag_size: 23277\n",
      "batch 559, loss: 0.0352, label: 1, bag_size: 4970\n",
      "batch 579, loss: 0.3148, label: 0, bag_size: 3936\n",
      "batch 599, loss: 0.4592, label: 1, bag_size: 2779\n",
      "batch 619, loss: 0.1840, label: 0, bag_size: 4098\n",
      "batch 639, loss: 0.1154, label: 0, bag_size: 2918\n",
      "batch 659, loss: 1.6898, label: 1, bag_size: 3159\n",
      "batch 679, loss: 0.0822, label: 0, bag_size: 1822\n",
      "batch 699, loss: 0.8913, label: 1, bag_size: 13217\n",
      "Epoch: 16, train_loss: 0.5237, train_error: 0.2809\n",
      "class 0: acc 0.8104956268221575, correct 278/343\n",
      "class 1: acc 0.6325966850828729, correct 229/362\n",
      "\n",
      "Val Set, val_loss: 0.5487, val_error: 0.2500, auc: 0.8150\n",
      "class 0: acc 0.8163265306122449, correct 40/49\n",
      "class 1: acc 0.6451612903225806, correct 20/31\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3396, label: 1, bag_size: 2938\n",
      "batch 39, loss: 0.2383, label: 1, bag_size: 5677\n",
      "batch 59, loss: 0.9673, label: 0, bag_size: 6593\n",
      "batch 79, loss: 1.1381, label: 1, bag_size: 2447\n",
      "batch 99, loss: 1.1655, label: 1, bag_size: 15407\n",
      "batch 119, loss: 0.8666, label: 1, bag_size: 10165\n",
      "batch 139, loss: 0.5194, label: 0, bag_size: 3001\n",
      "batch 159, loss: 0.1795, label: 1, bag_size: 4722\n",
      "batch 179, loss: 0.2446, label: 0, bag_size: 2359\n",
      "batch 199, loss: 0.4486, label: 0, bag_size: 2009\n",
      "batch 219, loss: 2.8251, label: 1, bag_size: 1699\n",
      "batch 239, loss: 0.1451, label: 0, bag_size: 2908\n",
      "batch 259, loss: 0.1401, label: 1, bag_size: 8331\n",
      "batch 279, loss: 0.5132, label: 1, bag_size: 6912\n",
      "batch 299, loss: 0.5521, label: 0, bag_size: 10588\n",
      "batch 319, loss: 0.9948, label: 0, bag_size: 3737\n",
      "batch 339, loss: 0.2075, label: 0, bag_size: 4181\n",
      "batch 359, loss: 0.2660, label: 0, bag_size: 2420\n",
      "batch 379, loss: 0.1330, label: 0, bag_size: 3732\n",
      "batch 399, loss: 0.7829, label: 0, bag_size: 3600\n",
      "batch 419, loss: 0.0265, label: 1, bag_size: 3733\n",
      "batch 439, loss: 0.2007, label: 0, bag_size: 18251\n",
      "batch 459, loss: 1.5723, label: 1, bag_size: 16675\n",
      "batch 479, loss: 0.1412, label: 1, bag_size: 3557\n",
      "batch 499, loss: 0.3130, label: 1, bag_size: 3966\n",
      "batch 519, loss: 0.0701, label: 1, bag_size: 5156\n",
      "batch 539, loss: 0.5367, label: 0, bag_size: 2482\n",
      "batch 559, loss: 0.2263, label: 0, bag_size: 3184\n",
      "batch 579, loss: 0.7769, label: 1, bag_size: 1272\n",
      "batch 599, loss: 0.7465, label: 0, bag_size: 4515\n",
      "batch 619, loss: 0.1848, label: 0, bag_size: 3070\n",
      "batch 639, loss: 0.1594, label: 0, bag_size: 4329\n",
      "batch 659, loss: 1.4404, label: 1, bag_size: 2687\n",
      "batch 679, loss: 0.0106, label: 1, bag_size: 4731\n",
      "batch 699, loss: 0.0524, label: 1, bag_size: 6138\n",
      "Epoch: 17, train_loss: 0.4808, train_error: 0.2454\n",
      "class 0: acc 0.8431372549019608, correct 301/357\n",
      "class 1: acc 0.6637931034482759, correct 231/348\n",
      "\n",
      "Val Set, val_loss: 0.5042, val_error: 0.2250, auc: 0.8387\n",
      "class 0: acc 0.7755102040816326, correct 38/49\n",
      "class 1: acc 0.7741935483870968, correct 24/31\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1364, label: 1, bag_size: 1503\n",
      "batch 39, loss: 0.3553, label: 0, bag_size: 13411\n",
      "batch 59, loss: 0.6174, label: 0, bag_size: 6301\n",
      "batch 79, loss: 0.4232, label: 0, bag_size: 10568\n",
      "batch 99, loss: 0.4389, label: 0, bag_size: 12517\n",
      "batch 119, loss: 0.4918, label: 0, bag_size: 16816\n",
      "batch 139, loss: 0.4520, label: 0, bag_size: 2747\n",
      "batch 159, loss: 1.2182, label: 1, bag_size: 20149\n",
      "batch 179, loss: 0.3358, label: 1, bag_size: 23277\n",
      "batch 199, loss: 2.0769, label: 0, bag_size: 4801\n",
      "batch 219, loss: 1.2170, label: 1, bag_size: 1251\n",
      "batch 239, loss: 1.3838, label: 1, bag_size: 1582\n",
      "batch 259, loss: 0.2970, label: 0, bag_size: 2721\n",
      "batch 279, loss: 1.5412, label: 0, bag_size: 5697\n",
      "batch 299, loss: 0.2328, label: 0, bag_size: 5894\n",
      "batch 319, loss: 0.0361, label: 1, bag_size: 3672\n",
      "batch 339, loss: 0.6439, label: 0, bag_size: 4179\n",
      "batch 359, loss: 0.3021, label: 1, bag_size: 2961\n",
      "batch 379, loss: 0.8367, label: 1, bag_size: 22171\n",
      "batch 399, loss: 0.8056, label: 0, bag_size: 2346\n",
      "batch 419, loss: 1.5509, label: 1, bag_size: 19307\n",
      "batch 439, loss: 1.1091, label: 1, bag_size: 12654\n",
      "batch 459, loss: 0.2553, label: 0, bag_size: 4181\n",
      "batch 479, loss: 1.4536, label: 0, bag_size: 12861\n",
      "batch 499, loss: 0.1674, label: 1, bag_size: 16538\n",
      "batch 519, loss: 0.0451, label: 1, bag_size: 1722\n",
      "batch 539, loss: 0.4788, label: 0, bag_size: 2432\n",
      "batch 559, loss: 0.1484, label: 1, bag_size: 2081\n",
      "batch 579, loss: 0.3440, label: 1, bag_size: 3338\n",
      "batch 599, loss: 0.5766, label: 0, bag_size: 5331\n",
      "batch 619, loss: 0.5037, label: 0, bag_size: 4855\n",
      "batch 639, loss: 0.8002, label: 1, bag_size: 3588\n",
      "batch 659, loss: 0.0094, label: 1, bag_size: 8007\n",
      "batch 679, loss: 0.0897, label: 1, bag_size: 4159\n",
      "batch 699, loss: 0.3928, label: 0, bag_size: 4320\n",
      "Epoch: 18, train_loss: 0.4875, train_error: 0.2426\n",
      "class 0: acc 0.8189910979228486, correct 276/337\n",
      "class 1: acc 0.7010869565217391, correct 258/368\n",
      "\n",
      "Val Set, val_loss: 0.5879, val_error: 0.2375, auc: 0.8249\n",
      "class 0: acc 0.7551020408163265, correct 37/49\n",
      "class 1: acc 0.7741935483870968, correct 24/31\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5008, label: 0, bag_size: 17711\n",
      "batch 39, loss: 0.5056, label: 0, bag_size: 5100\n",
      "batch 59, loss: 0.4547, label: 0, bag_size: 3661\n",
      "batch 79, loss: 0.5861, label: 0, bag_size: 6605\n",
      "batch 99, loss: 0.1685, label: 1, bag_size: 1627\n",
      "batch 119, loss: 0.1643, label: 0, bag_size: 1379\n",
      "batch 139, loss: 0.1610, label: 0, bag_size: 4769\n",
      "batch 159, loss: 0.3249, label: 0, bag_size: 3295\n",
      "batch 179, loss: 0.8736, label: 1, bag_size: 4243\n",
      "batch 199, loss: 0.0825, label: 0, bag_size: 4124\n",
      "batch 219, loss: 0.0778, label: 1, bag_size: 10091\n",
      "batch 239, loss: 0.7514, label: 1, bag_size: 1958\n",
      "batch 259, loss: 1.8506, label: 1, bag_size: 1251\n",
      "batch 279, loss: 0.3944, label: 0, bag_size: 4180\n",
      "batch 299, loss: 0.1100, label: 1, bag_size: 1316\n",
      "batch 319, loss: 0.3282, label: 0, bag_size: 6047\n",
      "batch 339, loss: 0.9927, label: 1, bag_size: 22171\n",
      "batch 359, loss: 0.3446, label: 0, bag_size: 3845\n",
      "batch 379, loss: 0.0856, label: 0, bag_size: 999\n",
      "batch 399, loss: 0.3032, label: 0, bag_size: 3070\n",
      "batch 419, loss: 0.2216, label: 0, bag_size: 2918\n",
      "batch 439, loss: 0.0905, label: 0, bag_size: 3843\n",
      "batch 459, loss: 0.1399, label: 1, bag_size: 4007\n",
      "batch 479, loss: 0.6461, label: 1, bag_size: 3588\n",
      "batch 499, loss: 0.3496, label: 0, bag_size: 17770\n",
      "batch 519, loss: 0.0606, label: 0, bag_size: 2667\n",
      "batch 539, loss: 0.2379, label: 0, bag_size: 3529\n",
      "batch 559, loss: 0.5618, label: 0, bag_size: 5409\n",
      "batch 579, loss: 0.1290, label: 1, bag_size: 986\n",
      "batch 599, loss: 0.7597, label: 1, bag_size: 11773\n",
      "batch 619, loss: 2.8255, label: 0, bag_size: 5877\n",
      "batch 639, loss: 0.6476, label: 0, bag_size: 1901\n",
      "batch 659, loss: 0.2950, label: 0, bag_size: 16737\n",
      "batch 679, loss: 0.2498, label: 0, bag_size: 2394\n",
      "batch 699, loss: 0.1057, label: 0, bag_size: 5617\n",
      "Epoch: 19, train_loss: 0.4842, train_error: 0.2383\n",
      "class 0: acc 0.8461538461538461, correct 308/364\n",
      "class 1: acc 0.6715542521994134, correct 229/341\n",
      "\n",
      "Val Set, val_loss: 0.4748, val_error: 0.2000, auc: 0.8262\n",
      "class 0: acc 0.9387755102040817, correct 46/49\n",
      "class 1: acc 0.5806451612903226, correct 18/31\n",
      "Validation loss decreased (0.480942 --> 0.474830).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5159, label: 1, bag_size: 3287\n",
      "batch 39, loss: 0.1661, label: 0, bag_size: 16663\n",
      "batch 59, loss: 0.2562, label: 1, bag_size: 5887\n",
      "batch 79, loss: 0.0666, label: 1, bag_size: 2701\n",
      "batch 99, loss: 1.3515, label: 1, bag_size: 21399\n",
      "batch 119, loss: 1.7741, label: 1, bag_size: 13348\n",
      "batch 139, loss: 1.6219, label: 1, bag_size: 5379\n",
      "batch 159, loss: 1.5274, label: 1, bag_size: 1699\n",
      "batch 179, loss: 0.4492, label: 0, bag_size: 11040\n",
      "batch 199, loss: 0.0611, label: 1, bag_size: 3733\n",
      "batch 219, loss: 1.1070, label: 0, bag_size: 5477\n",
      "batch 239, loss: 0.5710, label: 1, bag_size: 1525\n",
      "batch 259, loss: 0.1362, label: 0, bag_size: 6255\n",
      "batch 279, loss: 1.0184, label: 1, bag_size: 6477\n",
      "batch 299, loss: 0.1071, label: 1, bag_size: 2669\n",
      "batch 319, loss: 0.1157, label: 0, bag_size: 3137\n",
      "batch 339, loss: 0.0287, label: 1, bag_size: 3374\n",
      "batch 359, loss: 0.3328, label: 0, bag_size: 13573\n",
      "batch 379, loss: 0.6708, label: 0, bag_size: 2244\n",
      "batch 399, loss: 0.1259, label: 1, bag_size: 1937\n",
      "batch 419, loss: 2.2608, label: 0, bag_size: 12656\n",
      "batch 439, loss: 0.1782, label: 0, bag_size: 2013\n",
      "batch 459, loss: 0.2451, label: 1, bag_size: 5817\n",
      "batch 479, loss: 0.2962, label: 0, bag_size: 3463\n",
      "batch 499, loss: 0.8993, label: 1, bag_size: 10952\n",
      "batch 519, loss: 0.2682, label: 0, bag_size: 5074\n",
      "batch 539, loss: 0.3656, label: 1, bag_size: 2779\n",
      "batch 559, loss: 0.6383, label: 0, bag_size: 4628\n",
      "batch 579, loss: 1.4115, label: 0, bag_size: 20045\n",
      "batch 599, loss: 0.1219, label: 1, bag_size: 5071\n",
      "batch 619, loss: 0.1104, label: 0, bag_size: 5617\n",
      "batch 639, loss: 0.1747, label: 0, bag_size: 4500\n",
      "batch 659, loss: 0.0892, label: 1, bag_size: 4332\n",
      "batch 679, loss: 0.1665, label: 1, bag_size: 4452\n",
      "batch 699, loss: 0.1591, label: 0, bag_size: 3917\n",
      "Epoch: 20, train_loss: 0.4608, train_error: 0.2213\n",
      "class 0: acc 0.8296703296703297, correct 302/364\n",
      "class 1: acc 0.7243401759530792, correct 247/341\n",
      "\n",
      "Val Set, val_loss: 0.4722, val_error: 0.2250, auc: 0.8394\n",
      "class 0: acc 0.9183673469387755, correct 45/49\n",
      "class 1: acc 0.5483870967741935, correct 17/31\n",
      "Validation loss decreased (0.474830 --> 0.472157).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7783, label: 1, bag_size: 5677\n",
      "batch 39, loss: 0.1345, label: 1, bag_size: 4800\n",
      "batch 59, loss: 0.1634, label: 0, bag_size: 575\n",
      "batch 79, loss: 0.6102, label: 0, bag_size: 6301\n",
      "batch 99, loss: 0.1994, label: 0, bag_size: 5536\n",
      "batch 119, loss: 0.0787, label: 0, bag_size: 2048\n",
      "batch 139, loss: 1.1763, label: 1, bag_size: 2985\n",
      "batch 159, loss: 0.1037, label: 1, bag_size: 4332\n",
      "batch 179, loss: 0.2698, label: 0, bag_size: 2638\n",
      "batch 199, loss: 1.2974, label: 1, bag_size: 21399\n",
      "batch 219, loss: 0.1417, label: 0, bag_size: 3099\n",
      "batch 239, loss: 0.0614, label: 1, bag_size: 8021\n",
      "batch 259, loss: 0.5711, label: 0, bag_size: 3830\n",
      "batch 279, loss: 0.4247, label: 0, bag_size: 14856\n",
      "batch 299, loss: 0.0101, label: 1, bag_size: 5743\n",
      "batch 319, loss: 0.5524, label: 0, bag_size: 3441\n",
      "batch 339, loss: 0.3851, label: 1, bag_size: 5100\n",
      "batch 359, loss: 0.0875, label: 1, bag_size: 5100\n",
      "batch 379, loss: 1.4267, label: 1, bag_size: 10952\n",
      "batch 399, loss: 0.4408, label: 1, bag_size: 1958\n",
      "batch 419, loss: 0.1657, label: 1, bag_size: 5108\n",
      "batch 439, loss: 0.3700, label: 1, bag_size: 1454\n",
      "batch 459, loss: 0.7189, label: 0, bag_size: 2009\n",
      "batch 479, loss: 0.1274, label: 0, bag_size: 11758\n",
      "batch 499, loss: 0.1996, label: 1, bag_size: 2754\n",
      "batch 519, loss: 0.5447, label: 0, bag_size: 3830\n",
      "batch 539, loss: 0.0572, label: 0, bag_size: 4804\n",
      "batch 559, loss: 0.1944, label: 1, bag_size: 4162\n",
      "batch 579, loss: 0.0574, label: 0, bag_size: 2923\n",
      "batch 599, loss: 0.0241, label: 1, bag_size: 4970\n",
      "batch 619, loss: 0.5289, label: 0, bag_size: 4066\n",
      "batch 639, loss: 0.0986, label: 1, bag_size: 15213\n",
      "batch 659, loss: 1.1849, label: 0, bag_size: 4904\n",
      "batch 679, loss: 0.0904, label: 1, bag_size: 2250\n",
      "batch 699, loss: 1.2665, label: 0, bag_size: 3420\n",
      "Epoch: 21, train_loss: 0.4620, train_error: 0.2142\n",
      "class 0: acc 0.8238636363636364, correct 290/352\n",
      "class 1: acc 0.7478753541076487, correct 264/353\n",
      "\n",
      "Val Set, val_loss: 0.5335, val_error: 0.2750, auc: 0.8321\n",
      "class 0: acc 0.7755102040816326, correct 38/49\n",
      "class 1: acc 0.6451612903225806, correct 20/31\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2280, label: 0, bag_size: 2167\n",
      "batch 39, loss: 0.0356, label: 1, bag_size: 3834\n",
      "batch 59, loss: 0.4058, label: 0, bag_size: 5642\n",
      "batch 79, loss: 0.1959, label: 1, bag_size: 534\n",
      "batch 99, loss: 0.1895, label: 0, bag_size: 18251\n",
      "batch 119, loss: 0.1211, label: 1, bag_size: 5156\n",
      "batch 139, loss: 0.0865, label: 0, bag_size: 5924\n",
      "batch 159, loss: 1.1004, label: 1, bag_size: 6812\n",
      "batch 179, loss: 0.2007, label: 0, bag_size: 17546\n",
      "batch 199, loss: 0.3230, label: 0, bag_size: 12784\n",
      "batch 219, loss: 0.6702, label: 0, bag_size: 11040\n",
      "batch 239, loss: 0.2208, label: 0, bag_size: 1944\n",
      "batch 259, loss: 0.9653, label: 1, bag_size: 5062\n",
      "batch 279, loss: 0.0102, label: 1, bag_size: 4800\n",
      "batch 299, loss: 1.0645, label: 1, bag_size: 1374\n",
      "batch 319, loss: 0.1899, label: 0, bag_size: 3936\n",
      "batch 339, loss: 1.0332, label: 1, bag_size: 3626\n",
      "batch 359, loss: 0.3497, label: 1, bag_size: 641\n",
      "batch 379, loss: 0.3731, label: 1, bag_size: 1272\n",
      "batch 399, loss: 0.1311, label: 0, bag_size: 3521\n",
      "batch 419, loss: 0.1334, label: 1, bag_size: 3702\n",
      "batch 439, loss: 0.4437, label: 0, bag_size: 16859\n",
      "batch 459, loss: 1.0789, label: 1, bag_size: 6190\n",
      "batch 479, loss: 0.3670, label: 1, bag_size: 3990\n",
      "batch 499, loss: 0.1334, label: 0, bag_size: 1794\n",
      "batch 519, loss: 0.9288, label: 1, bag_size: 6319\n",
      "batch 539, loss: 0.3179, label: 0, bag_size: 26830\n",
      "batch 559, loss: 0.5210, label: 1, bag_size: 11773\n",
      "batch 579, loss: 0.4284, label: 1, bag_size: 3391\n",
      "batch 599, loss: 0.6706, label: 0, bag_size: 2420\n",
      "batch 619, loss: 0.0091, label: 1, bag_size: 2820\n",
      "batch 639, loss: 0.4983, label: 1, bag_size: 15141\n",
      "batch 659, loss: 0.8960, label: 0, bag_size: 5523\n",
      "batch 679, loss: 0.0267, label: 1, bag_size: 11773\n",
      "batch 699, loss: 0.3897, label: 0, bag_size: 12128\n",
      "Epoch: 22, train_loss: 0.4560, train_error: 0.2284\n",
      "class 0: acc 0.8226744186046512, correct 283/344\n",
      "class 1: acc 0.7229916897506925, correct 261/361\n",
      "\n",
      "Val Set, val_loss: 0.5112, val_error: 0.2625, auc: 0.8354\n",
      "class 0: acc 0.8367346938775511, correct 41/49\n",
      "class 1: acc 0.5806451612903226, correct 18/31\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.9361, label: 0, bag_size: 19037\n",
      "batch 39, loss: 0.0078, label: 1, bag_size: 2820\n",
      "batch 59, loss: 1.8296, label: 1, bag_size: 4722\n",
      "batch 79, loss: 0.2910, label: 0, bag_size: 2678\n",
      "batch 99, loss: 0.4776, label: 1, bag_size: 2943\n",
      "batch 119, loss: 0.2829, label: 1, bag_size: 2687\n",
      "batch 139, loss: 0.6120, label: 0, bag_size: 27817\n",
      "batch 159, loss: 1.4239, label: 0, bag_size: 4406\n",
      "batch 179, loss: 0.0700, label: 0, bag_size: 3987\n",
      "batch 199, loss: 0.8460, label: 0, bag_size: 12861\n",
      "batch 219, loss: 0.6271, label: 0, bag_size: 19535\n",
      "batch 239, loss: 0.3962, label: 1, bag_size: 1178\n",
      "batch 259, loss: 0.0117, label: 1, bag_size: 5743\n",
      "batch 279, loss: 0.0661, label: 1, bag_size: 6463\n",
      "batch 299, loss: 0.5910, label: 1, bag_size: 2036\n",
      "batch 319, loss: 0.0224, label: 1, bag_size: 19946\n",
      "batch 339, loss: 0.0769, label: 1, bag_size: 19581\n",
      "batch 359, loss: 0.5589, label: 0, bag_size: 13573\n",
      "batch 379, loss: 1.7510, label: 1, bag_size: 1374\n",
      "batch 399, loss: 0.2614, label: 1, bag_size: 3571\n",
      "batch 419, loss: 0.0531, label: 1, bag_size: 5677\n",
      "batch 439, loss: 0.0364, label: 1, bag_size: 2701\n",
      "batch 459, loss: 0.2714, label: 0, bag_size: 1944\n",
      "batch 479, loss: 0.1409, label: 0, bag_size: 5448\n",
      "batch 499, loss: 0.5353, label: 0, bag_size: 5894\n",
      "batch 519, loss: 0.0258, label: 1, bag_size: 6760\n",
      "batch 539, loss: 0.2797, label: 0, bag_size: 1865\n",
      "batch 559, loss: 0.9554, label: 1, bag_size: 14564\n",
      "batch 579, loss: 0.0406, label: 1, bag_size: 2005\n",
      "batch 599, loss: 0.3405, label: 0, bag_size: 27817\n",
      "batch 619, loss: 0.1882, label: 0, bag_size: 3674\n",
      "batch 639, loss: 0.3864, label: 0, bag_size: 15912\n",
      "batch 659, loss: 1.0921, label: 1, bag_size: 1374\n",
      "batch 679, loss: 0.2609, label: 1, bag_size: 534\n",
      "batch 699, loss: 0.9935, label: 0, bag_size: 2609\n",
      "Epoch: 23, train_loss: 0.4452, train_error: 0.2156\n",
      "class 0: acc 0.8255451713395638, correct 265/321\n",
      "class 1: acc 0.75, correct 288/384\n",
      "\n",
      "Val Set, val_loss: 0.5225, val_error: 0.2625, auc: 0.8703\n",
      "class 0: acc 0.6530612244897959, correct 32/49\n",
      "class 1: acc 0.8709677419354839, correct 27/31\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0994, label: 1, bag_size: 2905\n",
      "batch 39, loss: 0.0818, label: 0, bag_size: 2094\n",
      "batch 59, loss: 0.3509, label: 1, bag_size: 3287\n",
      "batch 79, loss: 0.3070, label: 0, bag_size: 2328\n",
      "batch 99, loss: 0.0296, label: 1, bag_size: 4803\n",
      "batch 119, loss: 0.0429, label: 1, bag_size: 4452\n",
      "batch 139, loss: 0.2125, label: 0, bag_size: 15193\n",
      "batch 159, loss: 0.1647, label: 0, bag_size: 2918\n",
      "batch 179, loss: 0.1114, label: 0, bag_size: 16585\n",
      "batch 199, loss: 0.6015, label: 1, bag_size: 6392\n",
      "batch 219, loss: 0.7174, label: 0, bag_size: 25641\n",
      "batch 239, loss: 0.7549, label: 1, bag_size: 1374\n",
      "batch 259, loss: 0.0906, label: 1, bag_size: 18681\n",
      "batch 279, loss: 0.2170, label: 1, bag_size: 2938\n",
      "batch 299, loss: 0.3683, label: 0, bag_size: 10588\n",
      "batch 319, loss: 0.1825, label: 0, bag_size: 1226\n",
      "batch 339, loss: 0.1255, label: 0, bag_size: 11758\n",
      "batch 359, loss: 0.2907, label: 0, bag_size: 3137\n",
      "batch 379, loss: 0.2833, label: 0, bag_size: 11299\n",
      "batch 399, loss: 0.0111, label: 1, bag_size: 3834\n",
      "batch 419, loss: 1.8095, label: 1, bag_size: 2447\n",
      "batch 439, loss: 0.2065, label: 1, bag_size: 5677\n",
      "batch 459, loss: 0.0290, label: 1, bag_size: 6760\n",
      "batch 479, loss: 0.3343, label: 0, bag_size: 11299\n",
      "batch 499, loss: 0.2619, label: 1, bag_size: 4985\n",
      "batch 519, loss: 0.0235, label: 1, bag_size: 5671\n",
      "batch 539, loss: 0.7434, label: 0, bag_size: 3600\n",
      "batch 559, loss: 0.5969, label: 1, bag_size: 3402\n",
      "batch 579, loss: 1.3498, label: 1, bag_size: 1937\n",
      "batch 599, loss: 0.1475, label: 0, bag_size: 1379\n",
      "batch 619, loss: 1.0497, label: 1, bag_size: 15483\n",
      "batch 639, loss: 0.3683, label: 0, bag_size: 4922\n",
      "batch 659, loss: 0.4433, label: 0, bag_size: 5360\n",
      "batch 679, loss: 0.0699, label: 0, bag_size: 5924\n",
      "batch 699, loss: 1.1385, label: 1, bag_size: 23277\n",
      "Epoch: 24, train_loss: 0.4676, train_error: 0.2298\n",
      "class 0: acc 0.8072625698324022, correct 289/358\n",
      "class 1: acc 0.7319884726224783, correct 254/347\n",
      "\n",
      "Val Set, val_loss: 0.4807, val_error: 0.2750, auc: 0.8367\n",
      "class 0: acc 0.8571428571428571, correct 42/49\n",
      "class 1: acc 0.5161290322580645, correct 16/31\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0823, label: 0, bag_size: 4287\n",
      "batch 39, loss: 0.9002, label: 1, bag_size: 11295\n",
      "batch 59, loss: 0.1771, label: 0, bag_size: 25933\n",
      "batch 79, loss: 0.5967, label: 0, bag_size: 4949\n",
      "batch 99, loss: 0.7804, label: 0, bag_size: 2089\n",
      "batch 119, loss: 0.4776, label: 0, bag_size: 6463\n",
      "batch 139, loss: 0.0375, label: 0, bag_size: 4468\n",
      "batch 159, loss: 0.5570, label: 1, bag_size: 3905\n",
      "batch 179, loss: 0.1375, label: 0, bag_size: 6463\n",
      "batch 199, loss: 0.0999, label: 0, bag_size: 4737\n",
      "batch 219, loss: 0.0354, label: 1, bag_size: 16936\n",
      "batch 239, loss: 0.1200, label: 1, bag_size: 8007\n",
      "batch 259, loss: 0.3868, label: 0, bag_size: 13846\n",
      "batch 279, loss: 1.9507, label: 1, bag_size: 3626\n",
      "batch 299, loss: 0.0042, label: 1, bag_size: 7641\n",
      "batch 319, loss: 0.1107, label: 0, bag_size: 8088\n",
      "batch 339, loss: 0.5004, label: 0, bag_size: 6152\n",
      "batch 359, loss: 0.1752, label: 0, bag_size: 2540\n",
      "batch 379, loss: 0.1030, label: 0, bag_size: 3869\n",
      "batch 399, loss: 0.0725, label: 0, bag_size: 3843\n",
      "batch 419, loss: 0.7928, label: 1, bag_size: 2943\n",
      "batch 439, loss: 0.3830, label: 0, bag_size: 3149\n",
      "batch 459, loss: 0.6823, label: 1, bag_size: 3990\n",
      "batch 479, loss: 0.7446, label: 1, bag_size: 3670\n",
      "batch 499, loss: 0.0112, label: 1, bag_size: 4159\n",
      "batch 519, loss: 0.2644, label: 1, bag_size: 3253\n",
      "batch 539, loss: 0.9200, label: 0, bag_size: 2110\n",
      "batch 559, loss: 0.0528, label: 0, bag_size: 5522\n",
      "batch 579, loss: 0.0234, label: 0, bag_size: 15747\n",
      "batch 599, loss: 0.1052, label: 0, bag_size: 3843\n",
      "batch 619, loss: 0.6242, label: 0, bag_size: 5880\n",
      "batch 639, loss: 0.2021, label: 1, bag_size: 1746\n",
      "batch 659, loss: 0.2353, label: 0, bag_size: 3489\n",
      "batch 679, loss: 0.0101, label: 1, bag_size: 6371\n",
      "batch 699, loss: 0.5766, label: 0, bag_size: 5894\n",
      "Epoch: 25, train_loss: 0.4549, train_error: 0.2184\n",
      "class 0: acc 0.8347107438016529, correct 303/363\n",
      "class 1: acc 0.7251461988304093, correct 248/342\n",
      "\n",
      "Val Set, val_loss: 0.4369, val_error: 0.2500, auc: 0.8598\n",
      "class 0: acc 0.8367346938775511, correct 41/49\n",
      "class 1: acc 0.6129032258064516, correct 19/31\n",
      "Validation loss decreased (0.472157 --> 0.436917).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.9126, label: 1, bag_size: 2624\n",
      "batch 39, loss: 0.5939, label: 1, bag_size: 16451\n",
      "batch 59, loss: 0.7969, label: 1, bag_size: 3588\n",
      "batch 79, loss: 0.6569, label: 1, bag_size: 11513\n",
      "batch 99, loss: 1.1546, label: 0, bag_size: 4180\n",
      "batch 119, loss: 0.0980, label: 0, bag_size: 7285\n",
      "batch 139, loss: 0.6422, label: 0, bag_size: 2747\n",
      "batch 159, loss: 0.7107, label: 1, bag_size: 23277\n",
      "batch 179, loss: 0.3027, label: 0, bag_size: 22936\n",
      "batch 199, loss: 0.1963, label: 0, bag_size: 6152\n",
      "batch 219, loss: 1.1803, label: 1, bag_size: 2624\n",
      "batch 239, loss: 1.4198, label: 1, bag_size: 4268\n",
      "batch 259, loss: 0.0791, label: 0, bag_size: 1892\n",
      "batch 279, loss: 0.6505, label: 0, bag_size: 5121\n",
      "batch 299, loss: 0.6291, label: 0, bag_size: 7667\n",
      "batch 319, loss: 0.1056, label: 1, bag_size: 3764\n",
      "batch 339, loss: 0.3926, label: 0, bag_size: 4124\n",
      "batch 359, loss: 0.1761, label: 1, bag_size: 3402\n",
      "batch 379, loss: 0.8193, label: 1, bag_size: 3454\n",
      "batch 399, loss: 0.2097, label: 1, bag_size: 21059\n",
      "batch 419, loss: 0.2426, label: 1, bag_size: 4800\n",
      "batch 439, loss: 0.1182, label: 0, bag_size: 4146\n",
      "batch 459, loss: 0.0606, label: 1, bag_size: 6138\n",
      "batch 479, loss: 0.4768, label: 1, bag_size: 22843\n",
      "batch 499, loss: 0.1182, label: 1, bag_size: 3578\n",
      "batch 519, loss: 0.5218, label: 0, bag_size: 2298\n",
      "batch 539, loss: 0.2853, label: 0, bag_size: 2394\n",
      "batch 559, loss: 0.1135, label: 1, bag_size: 3533\n",
      "batch 579, loss: 0.0256, label: 1, bag_size: 6760\n",
      "batch 599, loss: 1.0084, label: 0, bag_size: 4949\n",
      "batch 619, loss: 1.1536, label: 1, bag_size: 6319\n",
      "batch 639, loss: 0.0102, label: 1, bag_size: 2820\n",
      "batch 659, loss: 0.1723, label: 0, bag_size: 14662\n",
      "batch 679, loss: 0.4599, label: 1, bag_size: 11295\n",
      "batch 699, loss: 0.0464, label: 1, bag_size: 8021\n",
      "Epoch: 26, train_loss: 0.4400, train_error: 0.2128\n",
      "class 0: acc 0.8361581920903954, correct 296/354\n",
      "class 1: acc 0.7378917378917379, correct 259/351\n",
      "\n",
      "Val Set, val_loss: 0.5109, val_error: 0.2500, auc: 0.8479\n",
      "class 0: acc 0.7346938775510204, correct 36/49\n",
      "class 1: acc 0.7741935483870968, correct 24/31\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3069, label: 0, bag_size: 3764\n",
      "batch 39, loss: 0.1402, label: 0, bag_size: 15193\n",
      "batch 59, loss: 0.0901, label: 1, bag_size: 15213\n",
      "batch 79, loss: 0.0568, label: 1, bag_size: 3184\n",
      "batch 99, loss: 0.0108, label: 1, bag_size: 6600\n",
      "batch 119, loss: 0.1564, label: 0, bag_size: 7862\n",
      "batch 139, loss: 0.5180, label: 0, bag_size: 4162\n",
      "batch 159, loss: 0.6311, label: 0, bag_size: 3381\n",
      "batch 179, loss: 0.6754, label: 1, bag_size: 16675\n",
      "batch 199, loss: 0.2865, label: 0, bag_size: 2862\n",
      "batch 219, loss: 0.0990, label: 0, bag_size: 1553\n",
      "batch 239, loss: 1.1605, label: 1, bag_size: 1582\n",
      "batch 259, loss: 0.1865, label: 0, bag_size: 25641\n",
      "batch 279, loss: 0.2269, label: 0, bag_size: 25403\n",
      "batch 299, loss: 0.2570, label: 0, bag_size: 3936\n",
      "batch 319, loss: 0.8228, label: 1, bag_size: 12948\n",
      "batch 339, loss: 0.2097, label: 0, bag_size: 6534\n",
      "batch 359, loss: 0.8913, label: 1, bag_size: 6319\n",
      "batch 379, loss: 0.0776, label: 1, bag_size: 8021\n",
      "batch 399, loss: 0.0796, label: 0, bag_size: 5360\n",
      "batch 419, loss: 0.3650, label: 0, bag_size: 19035\n",
      "batch 439, loss: 0.3235, label: 1, bag_size: 8075\n",
      "batch 459, loss: 0.7668, label: 0, bag_size: 3240\n",
      "batch 479, loss: 3.5181, label: 0, bag_size: 5877\n",
      "batch 499, loss: 0.2342, label: 0, bag_size: 4066\n",
      "batch 519, loss: 0.0873, label: 0, bag_size: 2328\n",
      "batch 539, loss: 0.2388, label: 1, bag_size: 10091\n",
      "batch 559, loss: 0.4432, label: 0, bag_size: 13411\n",
      "batch 579, loss: 0.1216, label: 0, bag_size: 2420\n",
      "batch 599, loss: 0.1072, label: 0, bag_size: 7471\n",
      "batch 619, loss: 0.0245, label: 1, bag_size: 3117\n",
      "batch 639, loss: 0.4332, label: 0, bag_size: 16663\n",
      "batch 659, loss: 0.0302, label: 1, bag_size: 16427\n",
      "batch 679, loss: 0.0040, label: 1, bag_size: 5507\n",
      "batch 699, loss: 0.5945, label: 0, bag_size: 9127\n",
      "Epoch: 27, train_loss: 0.4012, train_error: 0.1773\n",
      "class 0: acc 0.889196675900277, correct 321/361\n",
      "class 1: acc 0.752906976744186, correct 259/344\n",
      "\n",
      "Val Set, val_loss: 0.5381, val_error: 0.2500, auc: 0.8492\n",
      "class 0: acc 0.7142857142857143, correct 35/49\n",
      "class 1: acc 0.8064516129032258, correct 25/31\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0338, label: 1, bag_size: 1503\n",
      "batch 39, loss: 0.3519, label: 0, bag_size: 4332\n",
      "batch 59, loss: 2.4752, label: 0, bag_size: 1826\n",
      "batch 79, loss: 0.0336, label: 0, bag_size: 2013\n",
      "batch 99, loss: 0.0391, label: 1, bag_size: 5672\n",
      "batch 119, loss: 1.1118, label: 1, bag_size: 16675\n",
      "batch 139, loss: 0.0967, label: 0, bag_size: 5120\n",
      "batch 159, loss: 0.0376, label: 1, bag_size: 2961\n",
      "batch 179, loss: 0.5459, label: 0, bag_size: 1095\n",
      "batch 199, loss: 0.0824, label: 1, bag_size: 10091\n",
      "batch 219, loss: 0.0267, label: 1, bag_size: 5068\n",
      "batch 239, loss: 0.0214, label: 1, bag_size: 6235\n",
      "batch 259, loss: 1.3641, label: 0, bag_size: 2762\n",
      "batch 279, loss: 0.3861, label: 1, bag_size: 2783\n",
      "batch 299, loss: 0.5552, label: 0, bag_size: 1095\n",
      "batch 319, loss: 1.5299, label: 1, bag_size: 20149\n",
      "batch 339, loss: 0.0473, label: 1, bag_size: 2524\n",
      "batch 359, loss: 0.0219, label: 1, bag_size: 4452\n",
      "batch 379, loss: 3.1824, label: 0, bag_size: 2054\n",
      "batch 399, loss: 0.7149, label: 0, bag_size: 2316\n",
      "batch 419, loss: 0.7634, label: 1, bag_size: 3588\n",
      "batch 439, loss: 0.5025, label: 0, bag_size: 3773\n",
      "batch 459, loss: 0.1072, label: 0, bag_size: 2622\n",
      "batch 479, loss: 2.1745, label: 1, bag_size: 20955\n",
      "batch 499, loss: 0.1529, label: 1, bag_size: 6500\n",
      "batch 519, loss: 0.3794, label: 0, bag_size: 3674\n",
      "batch 539, loss: 0.0791, label: 1, bag_size: 6138\n",
      "batch 559, loss: 0.4409, label: 0, bag_size: 3118\n",
      "batch 579, loss: 0.5504, label: 1, bag_size: 8021\n",
      "batch 599, loss: 0.0375, label: 0, bag_size: 938\n",
      "batch 619, loss: 1.5952, label: 1, bag_size: 2356\n",
      "batch 639, loss: 0.2622, label: 0, bag_size: 2286\n",
      "batch 659, loss: 1.3961, label: 0, bag_size: 15806\n",
      "batch 679, loss: 0.0360, label: 0, bag_size: 15747\n",
      "batch 699, loss: 0.7936, label: 0, bag_size: 4468\n",
      "Epoch: 28, train_loss: 0.4397, train_error: 0.2142\n",
      "class 0: acc 0.8163265306122449, correct 280/343\n",
      "class 1: acc 0.7569060773480663, correct 274/362\n",
      "\n",
      "Val Set, val_loss: 0.4810, val_error: 0.2375, auc: 0.8618\n",
      "class 0: acc 0.7142857142857143, correct 35/49\n",
      "class 1: acc 0.8387096774193549, correct 26/31\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0080, label: 1, bag_size: 5458\n",
      "batch 39, loss: 0.0981, label: 0, bag_size: 1553\n",
      "batch 59, loss: 0.0223, label: 1, bag_size: 24868\n",
      "batch 79, loss: 0.0202, label: 1, bag_size: 15434\n",
      "batch 99, loss: 0.0105, label: 1, bag_size: 19173\n",
      "batch 119, loss: 0.4407, label: 0, bag_size: 3674\n",
      "batch 139, loss: 0.9422, label: 1, bag_size: 5062\n",
      "batch 159, loss: 0.0039, label: 1, bag_size: 7641\n",
      "batch 179, loss: 0.0351, label: 1, bag_size: 2669\n",
      "batch 199, loss: 0.8858, label: 1, bag_size: 4387\n",
      "batch 219, loss: 1.4120, label: 0, bag_size: 3420\n",
      "batch 239, loss: 0.0188, label: 0, bag_size: 3661\n",
      "batch 259, loss: 0.7096, label: 1, bag_size: 4112\n",
      "batch 279, loss: 0.2320, label: 1, bag_size: 6463\n",
      "batch 299, loss: 1.2229, label: 0, bag_size: 1226\n",
      "batch 319, loss: 0.0893, label: 1, bag_size: 3670\n",
      "batch 339, loss: 0.0894, label: 0, bag_size: 3936\n",
      "batch 359, loss: 0.6986, label: 1, bag_size: 5065\n",
      "batch 379, loss: 0.1940, label: 1, bag_size: 5677\n",
      "batch 399, loss: 0.2309, label: 0, bag_size: 2298\n",
      "batch 419, loss: 0.9641, label: 1, bag_size: 11513\n",
      "batch 439, loss: 0.3641, label: 0, bag_size: 3268\n",
      "batch 459, loss: 0.0706, label: 0, bag_size: 5527\n",
      "batch 479, loss: 0.0743, label: 0, bag_size: 1316\n",
      "batch 499, loss: 0.1029, label: 0, bag_size: 1488\n",
      "batch 519, loss: 0.3181, label: 0, bag_size: 3317\n",
      "batch 539, loss: 0.0806, label: 1, bag_size: 3081\n",
      "batch 559, loss: 0.9792, label: 0, bag_size: 3600\n",
      "batch 579, loss: 0.0121, label: 1, bag_size: 24868\n",
      "batch 599, loss: 0.0525, label: 0, bag_size: 586\n",
      "batch 619, loss: 0.3578, label: 0, bag_size: 3090\n",
      "batch 639, loss: 0.3678, label: 1, bag_size: 2779\n",
      "batch 659, loss: 1.1185, label: 1, bag_size: 6759\n",
      "batch 679, loss: 0.1247, label: 0, bag_size: 3504\n",
      "batch 699, loss: 0.4887, label: 0, bag_size: 4332\n",
      "Epoch: 29, train_loss: 0.3993, train_error: 0.1787\n",
      "class 0: acc 0.8633879781420765, correct 316/366\n",
      "class 1: acc 0.775811209439528, correct 263/339\n",
      "\n",
      "Val Set, val_loss: 0.5088, val_error: 0.3000, auc: 0.8506\n",
      "class 0: acc 0.7346938775510204, correct 36/49\n",
      "class 1: acc 0.6451612903225806, correct 20/31\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1945, label: 0, bag_size: 2146\n",
      "batch 39, loss: 0.7690, label: 1, bag_size: 1808\n",
      "batch 59, loss: 0.6659, label: 0, bag_size: 14142\n",
      "batch 79, loss: 0.2917, label: 1, bag_size: 16538\n",
      "batch 99, loss: 1.9412, label: 1, bag_size: 5341\n",
      "batch 119, loss: 0.0829, label: 0, bag_size: 2398\n",
      "batch 139, loss: 0.3006, label: 1, bag_size: 8021\n",
      "batch 159, loss: 1.1301, label: 1, bag_size: 10165\n",
      "batch 179, loss: 0.1801, label: 0, bag_size: 16737\n",
      "batch 199, loss: 0.0365, label: 0, bag_size: 4468\n",
      "batch 219, loss: 0.6498, label: 1, bag_size: 2939\n",
      "batch 239, loss: 0.1205, label: 0, bag_size: 5269\n",
      "batch 259, loss: 0.0558, label: 0, bag_size: 6724\n",
      "batch 279, loss: 1.3644, label: 0, bag_size: 11302\n",
      "batch 299, loss: 0.0275, label: 1, bag_size: 7085\n",
      "batch 319, loss: 0.1148, label: 0, bag_size: 4708\n",
      "batch 339, loss: 0.3178, label: 0, bag_size: 6605\n",
      "batch 359, loss: 0.2306, label: 0, bag_size: 5413\n",
      "batch 379, loss: 0.1330, label: 1, bag_size: 14487\n",
      "batch 399, loss: 0.4013, label: 0, bag_size: 25933\n",
      "batch 419, loss: 0.0793, label: 1, bag_size: 4803\n",
      "batch 439, loss: 0.1544, label: 0, bag_size: 4228\n",
      "batch 459, loss: 0.3999, label: 1, bag_size: 4737\n",
      "batch 479, loss: 0.3797, label: 1, bag_size: 2939\n",
      "batch 499, loss: 0.0179, label: 1, bag_size: 5071\n",
      "batch 519, loss: 0.4098, label: 1, bag_size: 2783\n",
      "batch 539, loss: 0.5833, label: 0, bag_size: 3420\n",
      "batch 559, loss: 0.1946, label: 0, bag_size: 3090\n",
      "batch 579, loss: 0.0691, label: 0, bag_size: 4804\n",
      "batch 599, loss: 0.1312, label: 0, bag_size: 6524\n",
      "batch 619, loss: 0.8917, label: 1, bag_size: 20256\n",
      "batch 639, loss: 0.1174, label: 0, bag_size: 5080\n",
      "batch 659, loss: 0.3594, label: 1, bag_size: 2178\n",
      "batch 679, loss: 0.0119, label: 1, bag_size: 2824\n",
      "batch 699, loss: 0.1537, label: 0, bag_size: 5476\n",
      "Epoch: 30, train_loss: 0.4093, train_error: 0.1830\n",
      "class 0: acc 0.8555555555555555, correct 308/360\n",
      "class 1: acc 0.7768115942028986, correct 268/345\n",
      "\n",
      "Val Set, val_loss: 0.5022, val_error: 0.3000, auc: 0.8440\n",
      "class 0: acc 0.7551020408163265, correct 37/49\n",
      "class 1: acc 0.6129032258064516, correct 19/31\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0283, label: 1, bag_size: 6463\n",
      "batch 39, loss: 0.5558, label: 0, bag_size: 3857\n",
      "batch 59, loss: 0.1953, label: 1, bag_size: 6697\n",
      "batch 79, loss: 0.7416, label: 1, bag_size: 3170\n",
      "batch 99, loss: 0.0187, label: 1, bag_size: 986\n",
      "batch 119, loss: 0.3701, label: 0, bag_size: 1684\n",
      "batch 139, loss: 0.0450, label: 0, bag_size: 7471\n",
      "batch 159, loss: 0.2502, label: 0, bag_size: 12517\n",
      "batch 179, loss: 0.1645, label: 1, bag_size: 5100\n",
      "batch 199, loss: 0.3255, label: 0, bag_size: 4441\n",
      "batch 219, loss: 0.8117, label: 1, bag_size: 13226\n",
      "batch 239, loss: 0.6962, label: 1, bag_size: 3159\n",
      "batch 259, loss: 0.5558, label: 1, bag_size: 1582\n",
      "batch 279, loss: 0.0605, label: 0, bag_size: 3612\n",
      "batch 299, loss: 0.0412, label: 0, bag_size: 1770\n",
      "batch 319, loss: 0.1010, label: 1, bag_size: 5887\n",
      "batch 339, loss: 0.2094, label: 1, bag_size: 1479\n",
      "batch 359, loss: 0.5101, label: 0, bag_size: 6301\n",
      "batch 379, loss: 0.0959, label: 0, bag_size: 2069\n",
      "batch 399, loss: 0.0252, label: 1, bag_size: 5218\n",
      "batch 419, loss: 0.0718, label: 1, bag_size: 3402\n",
      "batch 439, loss: 1.0725, label: 1, bag_size: 14564\n",
      "batch 459, loss: 0.3271, label: 0, bag_size: 4641\n",
      "batch 479, loss: 0.3091, label: 0, bag_size: 4390\n",
      "batch 499, loss: 0.4909, label: 1, bag_size: 5637\n",
      "batch 519, loss: 0.3410, label: 0, bag_size: 6135\n",
      "batch 539, loss: 0.0095, label: 1, bag_size: 5777\n",
      "batch 559, loss: 0.1668, label: 1, bag_size: 18015\n",
      "batch 579, loss: 0.2247, label: 0, bag_size: 5894\n",
      "batch 599, loss: 0.0845, label: 0, bag_size: 2918\n",
      "batch 619, loss: 0.4541, label: 1, bag_size: 6477\n",
      "batch 639, loss: 0.0691, label: 0, bag_size: 4181\n",
      "batch 659, loss: 0.0073, label: 1, bag_size: 7641\n",
      "batch 679, loss: 0.5891, label: 1, bag_size: 4985\n",
      "batch 699, loss: 0.5208, label: 0, bag_size: 18060\n",
      "Epoch: 31, train_loss: 0.4416, train_error: 0.2099\n",
      "class 0: acc 0.8323529411764706, correct 283/340\n",
      "class 1: acc 0.7506849315068493, correct 274/365\n",
      "\n",
      "Val Set, val_loss: 0.6868, val_error: 0.3000, auc: 0.8255\n",
      "class 0: acc 0.6326530612244898, correct 31/49\n",
      "class 1: acc 0.8064516129032258, correct 25/31\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1960, label: 1, bag_size: 5298\n",
      "batch 39, loss: 0.2431, label: 0, bag_size: 13609\n",
      "batch 59, loss: 0.0809, label: 1, bag_size: 3184\n",
      "batch 79, loss: 0.0842, label: 1, bag_size: 6138\n",
      "batch 99, loss: 1.7158, label: 0, bag_size: 12656\n",
      "batch 119, loss: 0.0494, label: 0, bag_size: 3534\n",
      "batch 139, loss: 0.1689, label: 0, bag_size: 3448\n",
      "batch 159, loss: 0.0981, label: 1, bag_size: 10091\n",
      "batch 179, loss: 0.9275, label: 1, bag_size: 22171\n",
      "batch 199, loss: 0.1002, label: 0, bag_size: 11380\n",
      "batch 219, loss: 0.0084, label: 0, bag_size: 1822\n",
      "batch 239, loss: 0.4123, label: 0, bag_size: 8469\n",
      "batch 259, loss: 0.6068, label: 0, bag_size: 4468\n",
      "batch 279, loss: 0.8140, label: 1, bag_size: 20149\n",
      "batch 299, loss: 0.4346, label: 0, bag_size: 2298\n",
      "batch 319, loss: 0.0731, label: 1, bag_size: 2189\n",
      "batch 339, loss: 2.5656, label: 1, bag_size: 2626\n",
      "batch 359, loss: 0.1981, label: 1, bag_size: 3571\n",
      "batch 379, loss: 1.0004, label: 0, bag_size: 19535\n",
      "batch 399, loss: 0.0069, label: 1, bag_size: 4731\n",
      "batch 419, loss: 0.7400, label: 1, bag_size: 11513\n",
      "batch 439, loss: 0.0045, label: 1, bag_size: 6295\n",
      "batch 459, loss: 0.5879, label: 1, bag_size: 2039\n",
      "batch 479, loss: 0.0094, label: 1, bag_size: 3402\n",
      "batch 499, loss: 0.4182, label: 1, bag_size: 5199\n",
      "batch 519, loss: 0.1079, label: 0, bag_size: 3732\n",
      "batch 539, loss: 0.0167, label: 1, bag_size: 3733\n",
      "batch 559, loss: 0.1060, label: 0, bag_size: 2230\n",
      "batch 579, loss: 0.8312, label: 1, bag_size: 5539\n",
      "batch 599, loss: 0.2883, label: 1, bag_size: 3670\n",
      "batch 619, loss: 0.4250, label: 1, bag_size: 4737\n",
      "batch 639, loss: 0.6312, label: 0, bag_size: 14142\n",
      "batch 659, loss: 1.1138, label: 0, bag_size: 3093\n",
      "batch 679, loss: 0.2460, label: 0, bag_size: 19055\n",
      "batch 699, loss: 0.2742, label: 0, bag_size: 9622\n",
      "Epoch: 32, train_loss: 0.3984, train_error: 0.1816\n",
      "class 0: acc 0.8591549295774648, correct 305/355\n",
      "class 1: acc 0.7771428571428571, correct 272/350\n",
      "\n",
      "Val Set, val_loss: 0.3895, val_error: 0.2000, auc: 0.9039\n",
      "class 0: acc 0.8775510204081632, correct 43/49\n",
      "class 1: acc 0.6774193548387096, correct 21/31\n",
      "Validation loss decreased (0.436917 --> 0.389469).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0435, label: 1, bag_size: 24092\n",
      "batch 39, loss: 0.0754, label: 0, bag_size: 5522\n",
      "batch 59, loss: 0.0444, label: 0, bag_size: 6524\n",
      "batch 79, loss: 0.0644, label: 0, bag_size: 5624\n",
      "batch 99, loss: 0.3548, label: 1, bag_size: 3170\n",
      "batch 119, loss: 0.3662, label: 0, bag_size: 13846\n",
      "batch 139, loss: 0.0195, label: 1, bag_size: 3578\n",
      "batch 159, loss: 0.4079, label: 0, bag_size: 23860\n",
      "batch 179, loss: 0.0216, label: 0, bag_size: 3277\n",
      "batch 199, loss: 0.0454, label: 1, bag_size: 8474\n",
      "batch 219, loss: 0.2485, label: 1, bag_size: 10736\n",
      "batch 239, loss: 1.2286, label: 1, bag_size: 22171\n",
      "batch 259, loss: 0.1780, label: 0, bag_size: 3463\n",
      "batch 279, loss: 0.0507, label: 1, bag_size: 15434\n",
      "batch 299, loss: 0.6336, label: 0, bag_size: 3706\n",
      "batch 319, loss: 0.0791, label: 1, bag_size: 3533\n",
      "batch 339, loss: 0.6805, label: 1, bag_size: 5539\n",
      "batch 359, loss: 0.9710, label: 1, bag_size: 20056\n",
      "batch 379, loss: 0.4869, label: 1, bag_size: 16675\n",
      "batch 399, loss: 0.0099, label: 1, bag_size: 4452\n",
      "batch 419, loss: 1.2873, label: 0, bag_size: 15806\n",
      "batch 439, loss: 0.7655, label: 1, bag_size: 4761\n",
      "batch 459, loss: 0.3387, label: 0, bag_size: 4180\n",
      "batch 479, loss: 0.3433, label: 1, bag_size: 6759\n",
      "batch 499, loss: 0.7320, label: 0, bag_size: 9742\n",
      "batch 519, loss: 1.0629, label: 1, bag_size: 2039\n",
      "batch 539, loss: 0.0253, label: 1, bag_size: 3834\n",
      "batch 559, loss: 0.1635, label: 0, bag_size: 3936\n",
      "batch 579, loss: 1.3566, label: 0, bag_size: 2316\n",
      "batch 599, loss: 1.3497, label: 0, bag_size: 6301\n",
      "batch 619, loss: 0.3836, label: 0, bag_size: 4560\n",
      "batch 639, loss: 0.1296, label: 1, bag_size: 24686\n",
      "batch 659, loss: 0.7030, label: 1, bag_size: 3134\n",
      "batch 679, loss: 2.2770, label: 1, bag_size: 2626\n",
      "batch 699, loss: 1.1243, label: 0, bag_size: 1468\n",
      "Epoch: 33, train_loss: 0.3752, train_error: 0.1745\n",
      "class 0: acc 0.844311377245509, correct 282/334\n",
      "class 1: acc 0.8086253369272237, correct 300/371\n",
      "\n",
      "Val Set, val_loss: 0.3765, val_error: 0.2000, auc: 0.9085\n",
      "class 0: acc 0.8367346938775511, correct 41/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "Validation loss decreased (0.389469 --> 0.376480).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 1.7413, label: 1, bag_size: 2447\n",
      "batch 39, loss: 0.3344, label: 0, bag_size: 12128\n",
      "batch 59, loss: 0.3270, label: 1, bag_size: 1272\n",
      "batch 79, loss: 0.0843, label: 1, bag_size: 4249\n",
      "batch 99, loss: 0.2244, label: 1, bag_size: 3402\n",
      "batch 119, loss: 0.1042, label: 0, bag_size: 7471\n",
      "batch 139, loss: 1.5946, label: 0, bag_size: 4282\n",
      "batch 159, loss: 0.0309, label: 1, bag_size: 11555\n",
      "batch 179, loss: 0.0443, label: 1, bag_size: 4572\n",
      "batch 199, loss: 0.7119, label: 0, bag_size: 19037\n",
      "batch 219, loss: 0.0340, label: 1, bag_size: 4731\n",
      "batch 239, loss: 0.0154, label: 0, bag_size: 2230\n",
      "batch 259, loss: 0.3083, label: 0, bag_size: 4291\n",
      "batch 279, loss: 0.0115, label: 1, bag_size: 4458\n",
      "batch 299, loss: 0.6476, label: 0, bag_size: 3535\n",
      "batch 319, loss: 0.1644, label: 0, bag_size: 5413\n",
      "batch 339, loss: 1.0739, label: 1, bag_size: 3081\n",
      "batch 359, loss: 0.0185, label: 1, bag_size: 7641\n",
      "batch 379, loss: 0.0609, label: 0, bag_size: 3704\n",
      "batch 399, loss: 0.0663, label: 0, bag_size: 3732\n",
      "batch 419, loss: 0.0758, label: 0, bag_size: 4283\n",
      "batch 439, loss: 1.5082, label: 1, bag_size: 13348\n",
      "batch 459, loss: 0.5193, label: 0, bag_size: 4180\n",
      "batch 479, loss: 0.4827, label: 0, bag_size: 4329\n",
      "batch 499, loss: 0.4120, label: 1, bag_size: 2835\n",
      "batch 519, loss: 0.1699, label: 1, bag_size: 1937\n",
      "batch 539, loss: 0.5460, label: 1, bag_size: 1316\n",
      "batch 559, loss: 0.1312, label: 0, bag_size: 26374\n",
      "batch 579, loss: 0.1265, label: 0, bag_size: 27817\n",
      "batch 599, loss: 0.0100, label: 1, bag_size: 2820\n",
      "batch 619, loss: 0.3199, label: 1, bag_size: 2624\n",
      "batch 639, loss: 0.8134, label: 0, bag_size: 2452\n",
      "batch 659, loss: 0.5829, label: 1, bag_size: 15483\n",
      "batch 679, loss: 0.0639, label: 1, bag_size: 2899\n",
      "batch 699, loss: 0.7426, label: 0, bag_size: 4180\n",
      "Epoch: 34, train_loss: 0.4345, train_error: 0.2213\n",
      "class 0: acc 0.7899408284023669, correct 267/338\n",
      "class 1: acc 0.7683923705722071, correct 282/367\n",
      "\n",
      "Val Set, val_loss: 0.4382, val_error: 0.2125, auc: 0.9013\n",
      "class 0: acc 0.7142857142857143, correct 35/49\n",
      "class 1: acc 0.9032258064516129, correct 28/31\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0069, label: 0, bag_size: 7978\n",
      "batch 39, loss: 0.1904, label: 1, bag_size: 3391\n",
      "batch 59, loss: 0.1160, label: 1, bag_size: 2985\n",
      "batch 79, loss: 1.0315, label: 1, bag_size: 3159\n",
      "batch 99, loss: 0.0961, label: 1, bag_size: 3533\n",
      "batch 119, loss: 0.0598, label: 0, bag_size: 2069\n",
      "batch 139, loss: 0.9992, label: 1, bag_size: 6319\n",
      "batch 159, loss: 0.3859, label: 0, bag_size: 4573\n",
      "batch 179, loss: 0.1594, label: 1, bag_size: 15141\n",
      "batch 199, loss: 0.8742, label: 1, bag_size: 3454\n",
      "batch 219, loss: 0.2036, label: 0, bag_size: 5001\n",
      "batch 239, loss: 0.1972, label: 0, bag_size: 17273\n",
      "batch 259, loss: 0.1468, label: 0, bag_size: 3869\n",
      "batch 279, loss: 1.6417, label: 1, bag_size: 6319\n",
      "batch 299, loss: 0.3004, label: 1, bag_size: 4795\n",
      "batch 319, loss: 0.7654, label: 0, bag_size: 5824\n",
      "batch 339, loss: 0.0193, label: 0, bag_size: 938\n",
      "batch 359, loss: 0.4077, label: 0, bag_size: 2359\n",
      "batch 379, loss: 0.0597, label: 0, bag_size: 5982\n",
      "batch 399, loss: 0.0407, label: 0, bag_size: 2069\n",
      "batch 419, loss: 0.0374, label: 0, bag_size: 938\n",
      "batch 439, loss: 0.3160, label: 0, bag_size: 17273\n",
      "batch 459, loss: 0.0985, label: 0, bag_size: 4804\n",
      "batch 479, loss: 0.1777, label: 0, bag_size: 4146\n",
      "batch 499, loss: 0.0935, label: 0, bag_size: 4641\n",
      "batch 519, loss: 0.0813, label: 0, bag_size: 4804\n",
      "batch 539, loss: 0.0152, label: 1, bag_size: 4970\n",
      "batch 559, loss: 3.6463, label: 0, bag_size: 2054\n",
      "batch 579, loss: 0.0269, label: 1, bag_size: 1700\n",
      "batch 599, loss: 0.3467, label: 0, bag_size: 6463\n",
      "batch 619, loss: 0.0194, label: 0, bag_size: 5307\n",
      "batch 639, loss: 1.3465, label: 1, bag_size: 13348\n",
      "batch 659, loss: 0.0645, label: 1, bag_size: 2938\n",
      "batch 679, loss: 0.1135, label: 1, bag_size: 3966\n",
      "batch 699, loss: 0.0152, label: 0, bag_size: 6909\n",
      "Epoch: 35, train_loss: 0.3858, train_error: 0.1830\n",
      "class 0: acc 0.8390804597701149, correct 292/348\n",
      "class 1: acc 0.7955182072829131, correct 284/357\n",
      "\n",
      "Val Set, val_loss: 0.3883, val_error: 0.1875, auc: 0.9177\n",
      "class 0: acc 0.8163265306122449, correct 40/49\n",
      "class 1: acc 0.8064516129032258, correct 25/31\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0324, label: 0, bag_size: 3699\n",
      "batch 39, loss: 0.1822, label: 0, bag_size: 5533\n",
      "batch 59, loss: 0.7904, label: 0, bag_size: 19535\n",
      "batch 79, loss: 0.3989, label: 1, bag_size: 1808\n",
      "batch 99, loss: 0.9861, label: 1, bag_size: 8075\n",
      "batch 119, loss: 0.1744, label: 0, bag_size: 11018\n",
      "batch 139, loss: 0.1931, label: 0, bag_size: 13573\n",
      "batch 159, loss: 1.5321, label: 1, bag_size: 13217\n",
      "batch 179, loss: 0.1418, label: 0, bag_size: 6534\n",
      "batch 199, loss: 0.0853, label: 0, bag_size: 8088\n",
      "batch 219, loss: 0.0377, label: 0, bag_size: 3200\n",
      "batch 239, loss: 0.4449, label: 0, bag_size: 4402\n",
      "batch 259, loss: 0.3904, label: 1, bag_size: 15483\n",
      "batch 279, loss: 0.3965, label: 1, bag_size: 3588\n",
      "batch 299, loss: 0.0028, label: 1, bag_size: 5379\n",
      "batch 319, loss: 0.0279, label: 1, bag_size: 5592\n",
      "batch 339, loss: 0.2575, label: 1, bag_size: 1582\n",
      "batch 359, loss: 0.3341, label: 1, bag_size: 1808\n",
      "batch 379, loss: 1.1714, label: 1, bag_size: 10952\n",
      "batch 399, loss: 0.0091, label: 1, bag_size: 6235\n",
      "batch 419, loss: 0.5619, label: 0, bag_size: 5211\n",
      "batch 439, loss: 0.2733, label: 1, bag_size: 3287\n",
      "batch 459, loss: 0.0462, label: 0, bag_size: 2230\n",
      "batch 479, loss: 0.3262, label: 0, bag_size: 2483\n",
      "batch 499, loss: 0.0996, label: 1, bag_size: 18681\n",
      "batch 519, loss: 0.1123, label: 0, bag_size: 6351\n",
      "batch 539, loss: 0.0066, label: 1, bag_size: 4233\n",
      "batch 559, loss: 0.0359, label: 0, bag_size: 5624\n",
      "batch 579, loss: 0.0139, label: 0, bag_size: 7978\n",
      "batch 599, loss: 0.0600, label: 0, bag_size: 14281\n",
      "batch 619, loss: 1.0606, label: 0, bag_size: 4560\n",
      "batch 639, loss: 0.1324, label: 1, bag_size: 19013\n",
      "batch 659, loss: 0.0533, label: 0, bag_size: 10532\n",
      "batch 679, loss: 1.0221, label: 1, bag_size: 4285\n",
      "batch 699, loss: 0.0093, label: 1, bag_size: 23777\n",
      "Epoch: 36, train_loss: 0.3556, train_error: 0.1872\n",
      "class 0: acc 0.8465909090909091, correct 298/352\n",
      "class 1: acc 0.7790368271954674, correct 275/353\n",
      "\n",
      "Val Set, val_loss: 0.3861, val_error: 0.1875, auc: 0.9006\n",
      "class 0: acc 0.8571428571428571, correct 42/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0813, label: 0, bag_size: 5533\n",
      "batch 39, loss: 0.0082, label: 1, bag_size: 8474\n",
      "batch 59, loss: 0.1068, label: 1, bag_size: 3764\n",
      "batch 79, loss: 3.7414, label: 0, bag_size: 1826\n",
      "batch 99, loss: 0.1077, label: 0, bag_size: 3317\n",
      "batch 119, loss: 0.4113, label: 0, bag_size: 2682\n",
      "batch 139, loss: 0.2646, label: 1, bag_size: 5823\n",
      "batch 159, loss: 0.5384, label: 0, bag_size: 11302\n",
      "batch 179, loss: 0.4608, label: 1, bag_size: 3454\n",
      "batch 199, loss: 0.2868, label: 0, bag_size: 26830\n",
      "batch 219, loss: 1.0312, label: 0, bag_size: 3600\n",
      "batch 239, loss: 0.3260, label: 0, bag_size: 3372\n",
      "batch 259, loss: 0.4832, label: 0, bag_size: 4179\n",
      "batch 279, loss: 0.4281, label: 1, bag_size: 3764\n",
      "batch 299, loss: 0.0482, label: 1, bag_size: 3702\n",
      "batch 319, loss: 1.2347, label: 0, bag_size: 4175\n",
      "batch 339, loss: 0.0535, label: 1, bag_size: 3402\n",
      "batch 359, loss: 0.0367, label: 1, bag_size: 14564\n",
      "batch 379, loss: 1.3650, label: 0, bag_size: 4597\n",
      "batch 399, loss: 0.0233, label: 0, bag_size: 5982\n",
      "batch 419, loss: 0.0652, label: 0, bag_size: 7862\n",
      "batch 439, loss: 0.1152, label: 0, bag_size: 9003\n",
      "batch 459, loss: 0.5298, label: 1, bag_size: 4880\n",
      "batch 479, loss: 0.7441, label: 1, bag_size: 10952\n",
      "batch 499, loss: 0.0080, label: 0, bag_size: 3534\n",
      "batch 519, loss: 2.1974, label: 1, bag_size: 641\n",
      "batch 539, loss: 0.4450, label: 1, bag_size: 1272\n",
      "batch 559, loss: 0.0439, label: 0, bag_size: 4162\n",
      "batch 579, loss: 0.0030, label: 0, bag_size: 1358\n",
      "batch 599, loss: 0.1962, label: 1, bag_size: 1479\n",
      "batch 619, loss: 0.5325, label: 1, bag_size: 6759\n",
      "batch 639, loss: 0.0036, label: 1, bag_size: 5743\n",
      "batch 659, loss: 0.0051, label: 1, bag_size: 986\n",
      "batch 679, loss: 0.0365, label: 0, bag_size: 7285\n",
      "batch 699, loss: 0.5757, label: 0, bag_size: 5476\n",
      "Epoch: 37, train_loss: 0.3650, train_error: 0.1716\n",
      "class 0: acc 0.861671469740634, correct 299/347\n",
      "class 1: acc 0.7960893854748603, correct 285/358\n",
      "\n",
      "Val Set, val_loss: 0.4723, val_error: 0.2375, auc: 0.8835\n",
      "class 0: acc 0.7551020408163265, correct 37/49\n",
      "class 1: acc 0.7741935483870968, correct 24/31\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2702, label: 0, bag_size: 28144\n",
      "batch 39, loss: 0.0595, label: 1, bag_size: 7085\n",
      "batch 59, loss: 0.0515, label: 1, bag_size: 6752\n",
      "batch 79, loss: 3.1900, label: 0, bag_size: 21069\n",
      "batch 99, loss: 0.0115, label: 0, bag_size: 3290\n",
      "batch 119, loss: 0.1607, label: 0, bag_size: 22936\n",
      "batch 139, loss: 0.3663, label: 0, bag_size: 19035\n",
      "batch 159, loss: 0.0096, label: 1, bag_size: 5068\n",
      "batch 179, loss: 0.2805, label: 0, bag_size: 3264\n",
      "batch 199, loss: 0.1608, label: 1, bag_size: 1958\n",
      "batch 219, loss: 1.2554, label: 1, bag_size: 3420\n",
      "batch 239, loss: 0.0063, label: 0, bag_size: 3553\n",
      "batch 259, loss: 0.0207, label: 0, bag_size: 3553\n",
      "batch 279, loss: 0.3848, label: 1, bag_size: 2189\n",
      "batch 299, loss: 0.0067, label: 1, bag_size: 4815\n",
      "batch 319, loss: 0.6616, label: 0, bag_size: 16816\n",
      "batch 339, loss: 0.5446, label: 0, bag_size: 15060\n",
      "batch 359, loss: 0.0553, label: 0, bag_size: 2862\n",
      "batch 379, loss: 0.1248, label: 0, bag_size: 2009\n",
      "batch 399, loss: 0.0021, label: 1, bag_size: 3733\n",
      "batch 419, loss: 0.0102, label: 0, bag_size: 3347\n",
      "batch 439, loss: 0.0234, label: 1, bag_size: 5100\n",
      "batch 459, loss: 0.6949, label: 0, bag_size: 5523\n",
      "batch 479, loss: 0.1385, label: 0, bag_size: 3714\n",
      "batch 499, loss: 1.2695, label: 0, bag_size: 4922\n",
      "batch 519, loss: 0.8012, label: 1, bag_size: 2036\n",
      "batch 539, loss: 0.0201, label: 1, bag_size: 4069\n",
      "batch 559, loss: 0.1292, label: 0, bag_size: 11302\n",
      "batch 579, loss: 0.1109, label: 1, bag_size: 8474\n",
      "batch 599, loss: 0.6238, label: 1, bag_size: 2039\n",
      "batch 619, loss: 0.6387, label: 0, bag_size: 1095\n",
      "batch 639, loss: 0.4821, label: 0, bag_size: 2009\n",
      "batch 659, loss: 0.0624, label: 0, bag_size: 5504\n",
      "batch 679, loss: 0.9407, label: 1, bag_size: 1825\n",
      "batch 699, loss: 0.6646, label: 0, bag_size: 3857\n",
      "Epoch: 38, train_loss: 0.3900, train_error: 0.1773\n",
      "class 0: acc 0.8598382749326146, correct 319/371\n",
      "class 1: acc 0.781437125748503, correct 261/334\n",
      "\n",
      "Val Set, val_loss: 0.4424, val_error: 0.2250, auc: 0.8835\n",
      "class 0: acc 0.7959183673469388, correct 39/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3100, label: 0, bag_size: 10590\n",
      "batch 39, loss: 0.1442, label: 1, bag_size: 1251\n",
      "batch 59, loss: 0.0253, label: 1, bag_size: 3184\n",
      "batch 79, loss: 0.0036, label: 1, bag_size: 2961\n",
      "batch 99, loss: 0.0940, label: 0, bag_size: 2298\n",
      "batch 119, loss: 0.2496, label: 1, bag_size: 1272\n",
      "batch 139, loss: 0.3023, label: 0, bag_size: 10588\n",
      "batch 159, loss: 1.0134, label: 1, bag_size: 4094\n",
      "batch 179, loss: 0.3804, label: 1, bag_size: 12948\n",
      "batch 199, loss: 0.4585, label: 0, bag_size: 7667\n",
      "batch 219, loss: 1.1213, label: 0, bag_size: 6972\n",
      "batch 239, loss: 0.1229, label: 0, bag_size: 2205\n",
      "batch 259, loss: 0.0714, label: 0, bag_size: 1379\n",
      "batch 279, loss: 0.2615, label: 1, bag_size: 3548\n",
      "batch 299, loss: 0.4004, label: 0, bag_size: 26374\n",
      "batch 319, loss: 2.1613, label: 1, bag_size: 6392\n",
      "batch 339, loss: 0.7622, label: 1, bag_size: 3925\n",
      "batch 359, loss: 1.7565, label: 0, bag_size: 3228\n",
      "batch 379, loss: 0.3253, label: 1, bag_size: 4802\n",
      "batch 399, loss: 0.2753, label: 0, bag_size: 5370\n",
      "batch 419, loss: 0.0312, label: 1, bag_size: 2550\n",
      "batch 439, loss: 0.0054, label: 1, bag_size: 19173\n",
      "batch 459, loss: 0.6117, label: 1, bag_size: 1808\n",
      "batch 479, loss: 0.2463, label: 1, bag_size: 8075\n",
      "batch 499, loss: 0.0794, label: 0, bag_size: 2483\n",
      "batch 519, loss: 0.4642, label: 1, bag_size: 2158\n",
      "batch 539, loss: 0.0082, label: 1, bag_size: 3834\n",
      "batch 559, loss: 0.0193, label: 0, bag_size: 5299\n",
      "batch 579, loss: 0.0752, label: 0, bag_size: 4427\n",
      "batch 599, loss: 0.6957, label: 0, bag_size: 14194\n",
      "batch 619, loss: 0.0712, label: 0, bag_size: 1532\n",
      "batch 639, loss: 0.0261, label: 1, bag_size: 16451\n",
      "batch 659, loss: 0.4883, label: 1, bag_size: 23277\n",
      "batch 679, loss: 0.1934, label: 1, bag_size: 3672\n",
      "batch 699, loss: 0.0494, label: 1, bag_size: 2701\n",
      "Epoch: 39, train_loss: 0.3291, train_error: 0.1390\n",
      "class 0: acc 0.8728323699421965, correct 302/346\n",
      "class 1: acc 0.8495821727019499, correct 305/359\n",
      "\n",
      "Val Set, val_loss: 0.5397, val_error: 0.2500, auc: 0.8618\n",
      "class 0: acc 0.7346938775510204, correct 36/49\n",
      "class 1: acc 0.7741935483870968, correct 24/31\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.9840, label: 1, bag_size: 5539\n",
      "batch 39, loss: 0.0279, label: 1, bag_size: 4332\n",
      "batch 59, loss: 0.0065, label: 1, bag_size: 3557\n",
      "batch 79, loss: 0.0324, label: 0, bag_size: 5527\n",
      "batch 99, loss: 0.4655, label: 0, bag_size: 22594\n",
      "batch 119, loss: 0.4774, label: 1, bag_size: 20056\n",
      "batch 139, loss: 0.0369, label: 1, bag_size: 3226\n",
      "batch 159, loss: 0.0923, label: 0, bag_size: 5642\n",
      "batch 179, loss: 0.3020, label: 1, bag_size: 3990\n",
      "batch 199, loss: 0.5439, label: 1, bag_size: 11295\n",
      "batch 219, loss: 0.0068, label: 1, bag_size: 1722\n",
      "batch 239, loss: 0.0078, label: 1, bag_size: 24092\n",
      "batch 259, loss: 0.0039, label: 1, bag_size: 1587\n",
      "batch 279, loss: 0.0404, label: 1, bag_size: 15118\n",
      "batch 299, loss: 0.0107, label: 1, bag_size: 10091\n",
      "batch 319, loss: 0.0599, label: 0, bag_size: 8088\n",
      "batch 339, loss: 0.2069, label: 0, bag_size: 2913\n",
      "batch 359, loss: 0.0630, label: 0, bag_size: 2110\n",
      "batch 379, loss: 0.0067, label: 1, bag_size: 1579\n",
      "batch 399, loss: 0.1157, label: 0, bag_size: 4628\n",
      "batch 419, loss: 0.1598, label: 0, bag_size: 3268\n",
      "batch 439, loss: 0.0348, label: 0, bag_size: 20134\n",
      "batch 459, loss: 0.0024, label: 1, bag_size: 4162\n",
      "batch 479, loss: 0.0892, label: 0, bag_size: 2013\n",
      "batch 499, loss: 0.4223, label: 0, bag_size: 3737\n",
      "batch 519, loss: 0.0740, label: 1, bag_size: 5823\n",
      "batch 539, loss: 0.7257, label: 0, bag_size: 2362\n",
      "batch 559, loss: 0.4713, label: 0, bag_size: 8469\n",
      "batch 579, loss: 0.1242, label: 1, bag_size: 5199\n",
      "batch 599, loss: 0.0077, label: 1, bag_size: 7641\n",
      "batch 619, loss: 0.4161, label: 1, bag_size: 5637\n",
      "batch 639, loss: 0.0438, label: 1, bag_size: 16936\n",
      "batch 659, loss: 0.0449, label: 1, bag_size: 3578\n",
      "batch 679, loss: 0.0354, label: 1, bag_size: 3670\n",
      "batch 699, loss: 0.0937, label: 1, bag_size: 2961\n",
      "Epoch: 40, train_loss: 0.3292, train_error: 0.1475\n",
      "class 0: acc 0.8729281767955801, correct 316/362\n",
      "class 1: acc 0.8309037900874635, correct 285/343\n",
      "\n",
      "Val Set, val_loss: 0.4739, val_error: 0.2375, auc: 0.8683\n",
      "class 0: acc 0.8979591836734694, correct 44/49\n",
      "class 1: acc 0.5483870967741935, correct 17/31\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1242, label: 1, bag_size: 15141\n",
      "batch 39, loss: 1.5302, label: 1, bag_size: 5801\n",
      "batch 59, loss: 0.0466, label: 0, bag_size: 4468\n",
      "batch 79, loss: 0.0478, label: 0, bag_size: 2721\n",
      "batch 99, loss: 0.2304, label: 0, bag_size: 14098\n",
      "batch 119, loss: 0.0273, label: 0, bag_size: 5522\n",
      "batch 139, loss: 0.4303, label: 0, bag_size: 3007\n",
      "batch 159, loss: 0.5742, label: 1, bag_size: 2783\n",
      "batch 179, loss: 0.0063, label: 1, bag_size: 6235\n",
      "batch 199, loss: 0.5096, label: 0, bag_size: 3857\n",
      "batch 219, loss: 0.4504, label: 1, bag_size: 2840\n",
      "batch 239, loss: 3.2444, label: 0, bag_size: 3096\n",
      "batch 259, loss: 0.6851, label: 0, bag_size: 2913\n",
      "batch 279, loss: 0.0219, label: 0, bag_size: 2638\n",
      "batch 299, loss: 0.6551, label: 1, bag_size: 5539\n",
      "batch 319, loss: 0.0175, label: 0, bag_size: 5455\n",
      "batch 339, loss: 0.0547, label: 1, bag_size: 16427\n",
      "batch 359, loss: 0.7245, label: 1, bag_size: 6912\n",
      "batch 379, loss: 0.0042, label: 1, bag_size: 19946\n",
      "batch 399, loss: 1.7095, label: 1, bag_size: 5152\n",
      "batch 419, loss: 0.0763, label: 0, bag_size: 4303\n",
      "batch 439, loss: 0.0090, label: 1, bag_size: 5068\n",
      "batch 459, loss: 0.3621, label: 0, bag_size: 3793\n",
      "batch 479, loss: 0.2505, label: 1, bag_size: 3672\n",
      "batch 499, loss: 0.4109, label: 0, bag_size: 3517\n",
      "batch 519, loss: 0.0685, label: 0, bag_size: 2346\n",
      "batch 539, loss: 1.0147, label: 1, bag_size: 2840\n",
      "batch 559, loss: 0.0118, label: 0, bag_size: 2048\n",
      "batch 579, loss: 0.0784, label: 1, bag_size: 3670\n",
      "batch 599, loss: 0.3655, label: 0, bag_size: 3857\n",
      "batch 619, loss: 0.0292, label: 0, bag_size: 3137\n",
      "batch 639, loss: 1.4920, label: 1, bag_size: 2624\n",
      "batch 659, loss: 0.0188, label: 0, bag_size: 4468\n",
      "batch 679, loss: 0.0914, label: 1, bag_size: 3824\n",
      "batch 699, loss: 0.0241, label: 0, bag_size: 3843\n",
      "Epoch: 41, train_loss: 0.3454, train_error: 0.1489\n",
      "class 0: acc 0.883289124668435, correct 333/377\n",
      "class 1: acc 0.8140243902439024, correct 267/328\n",
      "\n",
      "Val Set, val_loss: 0.4996, val_error: 0.2000, auc: 0.8953\n",
      "class 0: acc 0.7142857142857143, correct 35/49\n",
      "class 1: acc 0.9354838709677419, correct 29/31\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0130, label: 1, bag_size: 10091\n",
      "batch 39, loss: 0.0756, label: 0, bag_size: 13411\n",
      "batch 59, loss: 0.0227, label: 0, bag_size: 4181\n",
      "batch 79, loss: 0.0066, label: 1, bag_size: 2493\n",
      "batch 99, loss: 2.6348, label: 0, bag_size: 7667\n",
      "batch 119, loss: 0.0510, label: 0, bag_size: 3295\n",
      "batch 139, loss: 0.0008, label: 1, bag_size: 5156\n",
      "batch 159, loss: 0.0176, label: 0, bag_size: 3312\n",
      "batch 179, loss: 0.1920, label: 0, bag_size: 2863\n",
      "batch 199, loss: 0.0562, label: 0, bag_size: 3928\n",
      "batch 219, loss: 1.5942, label: 1, bag_size: 1004\n",
      "batch 239, loss: 0.0180, label: 0, bag_size: 2006\n",
      "batch 259, loss: 0.0218, label: 0, bag_size: 3521\n",
      "batch 279, loss: 0.2426, label: 0, bag_size: 1553\n",
      "batch 299, loss: 0.3470, label: 0, bag_size: 9499\n",
      "batch 319, loss: 0.2601, label: 1, bag_size: 1454\n",
      "batch 339, loss: 0.0010, label: 1, bag_size: 3578\n",
      "batch 359, loss: 0.4026, label: 0, bag_size: 3548\n",
      "batch 379, loss: 0.0183, label: 0, bag_size: 3065\n",
      "batch 399, loss: 0.1558, label: 0, bag_size: 4287\n",
      "batch 419, loss: 0.1587, label: 1, bag_size: 10736\n",
      "batch 439, loss: 0.0457, label: 1, bag_size: 1587\n",
      "batch 459, loss: 0.0777, label: 1, bag_size: 15483\n",
      "batch 479, loss: 0.0261, label: 1, bag_size: 4970\n",
      "batch 499, loss: 0.0424, label: 1, bag_size: 1961\n",
      "batch 519, loss: 0.2397, label: 0, bag_size: 2480\n",
      "batch 539, loss: 1.2784, label: 1, bag_size: 2840\n",
      "batch 559, loss: 0.1942, label: 0, bag_size: 5523\n",
      "batch 579, loss: 0.7116, label: 1, bag_size: 4737\n",
      "batch 599, loss: 0.2477, label: 0, bag_size: 3149\n",
      "batch 619, loss: 0.0810, label: 0, bag_size: 3001\n",
      "batch 639, loss: 0.9091, label: 0, bag_size: 16816\n",
      "batch 659, loss: 0.0428, label: 1, bag_size: 3557\n",
      "batch 679, loss: 0.1687, label: 0, bag_size: 16737\n",
      "batch 699, loss: 0.1039, label: 1, bag_size: 4803\n",
      "Epoch: 42, train_loss: 0.3399, train_error: 0.1574\n",
      "class 0: acc 0.8661202185792349, correct 317/366\n",
      "class 1: acc 0.8171091445427728, correct 277/339\n",
      "\n",
      "Val Set, val_loss: 0.5016, val_error: 0.2375, auc: 0.8683\n",
      "class 0: acc 0.8367346938775511, correct 41/49\n",
      "class 1: acc 0.6451612903225806, correct 20/31\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0364, label: 1, bag_size: 8007\n",
      "batch 39, loss: 0.0037, label: 1, bag_size: 24868\n",
      "batch 59, loss: 1.1582, label: 1, bag_size: 2343\n",
      "batch 79, loss: 0.3662, label: 0, bag_size: 16816\n",
      "batch 99, loss: 1.7698, label: 1, bag_size: 1699\n",
      "batch 119, loss: 0.0244, label: 1, bag_size: 5458\n",
      "batch 139, loss: 0.0550, label: 0, bag_size: 4558\n",
      "batch 159, loss: 1.1784, label: 0, bag_size: 3857\n",
      "batch 179, loss: 0.1953, label: 0, bag_size: 3661\n",
      "batch 199, loss: 0.2449, label: 0, bag_size: 2655\n",
      "batch 219, loss: 0.2117, label: 0, bag_size: 11797\n",
      "batch 239, loss: 0.2224, label: 0, bag_size: 4587\n",
      "batch 259, loss: 0.0985, label: 0, bag_size: 11302\n",
      "batch 279, loss: 0.1583, label: 0, bag_size: 13846\n",
      "batch 299, loss: 0.0571, label: 0, bag_size: 17711\n",
      "batch 319, loss: 0.0677, label: 0, bag_size: 3256\n",
      "batch 339, loss: 0.1690, label: 0, bag_size: 13846\n",
      "batch 359, loss: 0.1819, label: 1, bag_size: 4795\n",
      "batch 379, loss: 0.0226, label: 1, bag_size: 11563\n",
      "batch 399, loss: 0.0015, label: 1, bag_size: 7085\n",
      "batch 419, loss: 0.0049, label: 1, bag_size: 2877\n",
      "batch 439, loss: 0.3991, label: 1, bag_size: 4880\n",
      "batch 459, loss: 0.9763, label: 1, bag_size: 3134\n",
      "batch 479, loss: 0.2183, label: 1, bag_size: 2506\n",
      "batch 499, loss: 0.0324, label: 1, bag_size: 3834\n",
      "batch 519, loss: 0.1640, label: 0, bag_size: 3317\n",
      "batch 539, loss: 0.2010, label: 0, bag_size: 4769\n",
      "batch 559, loss: 0.1139, label: 0, bag_size: 14281\n",
      "batch 579, loss: 0.9429, label: 1, bag_size: 23277\n",
      "batch 599, loss: 0.5458, label: 0, bag_size: 6391\n",
      "batch 619, loss: 1.1360, label: 1, bag_size: 1251\n",
      "batch 639, loss: 0.0082, label: 1, bag_size: 7641\n",
      "batch 659, loss: 0.0070, label: 1, bag_size: 2656\n",
      "batch 679, loss: 0.2746, label: 0, bag_size: 15912\n",
      "batch 699, loss: 0.0043, label: 1, bag_size: 9643\n",
      "Epoch: 43, train_loss: 0.3405, train_error: 0.1404\n",
      "class 0: acc 0.869942196531792, correct 301/346\n",
      "class 1: acc 0.8495821727019499, correct 305/359\n",
      "\n",
      "Val Set, val_loss: 0.4444, val_error: 0.2250, auc: 0.8822\n",
      "class 0: acc 0.8367346938775511, correct 41/49\n",
      "class 1: acc 0.6774193548387096, correct 21/31\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1100, label: 1, bag_size: 3548\n",
      "batch 39, loss: 0.1553, label: 0, bag_size: 9127\n",
      "batch 59, loss: 0.3107, label: 1, bag_size: 1479\n",
      "batch 79, loss: 0.0359, label: 0, bag_size: 3917\n",
      "batch 99, loss: 0.1739, label: 1, bag_size: 3548\n",
      "batch 119, loss: 0.0320, label: 0, bag_size: 4804\n",
      "batch 139, loss: 0.8803, label: 0, bag_size: 1901\n",
      "batch 159, loss: 0.0035, label: 0, bag_size: 972\n",
      "batch 179, loss: 0.2686, label: 1, bag_size: 16936\n",
      "batch 199, loss: 1.0869, label: 1, bag_size: 5065\n",
      "batch 219, loss: 0.0084, label: 1, bag_size: 6500\n",
      "batch 239, loss: 0.0055, label: 1, bag_size: 2820\n",
      "batch 259, loss: 0.0452, label: 1, bag_size: 16675\n",
      "batch 279, loss: 0.0377, label: 0, bag_size: 2533\n",
      "batch 299, loss: 1.6329, label: 1, bag_size: 4761\n",
      "batch 319, loss: 0.0322, label: 0, bag_size: 4162\n",
      "batch 339, loss: 0.0011, label: 0, bag_size: 7978\n",
      "batch 359, loss: 0.0025, label: 1, bag_size: 1722\n",
      "batch 379, loss: 0.0214, label: 0, bag_size: 5522\n",
      "batch 399, loss: 0.4804, label: 0, bag_size: 4012\n",
      "batch 419, loss: 0.9204, label: 0, bag_size: 4329\n",
      "batch 439, loss: 0.9365, label: 0, bag_size: 4758\n",
      "batch 459, loss: 0.0098, label: 1, bag_size: 4233\n",
      "batch 479, loss: 0.1772, label: 0, bag_size: 8088\n",
      "batch 499, loss: 0.5985, label: 0, bag_size: 5100\n",
      "batch 519, loss: 0.0152, label: 1, bag_size: 5071\n",
      "batch 539, loss: 0.0234, label: 0, bag_size: 4320\n",
      "batch 559, loss: 0.0030, label: 1, bag_size: 21059\n",
      "batch 579, loss: 0.0355, label: 0, bag_size: 3159\n",
      "batch 599, loss: 0.1537, label: 0, bag_size: 9127\n",
      "batch 619, loss: 0.2159, label: 1, bag_size: 22843\n",
      "batch 639, loss: 0.1204, label: 0, bag_size: 3240\n",
      "batch 659, loss: 0.6983, label: 1, bag_size: 1808\n",
      "batch 679, loss: 0.9357, label: 1, bag_size: 3925\n",
      "batch 699, loss: 0.1314, label: 0, bag_size: 7290\n",
      "Epoch: 44, train_loss: 0.3464, train_error: 0.1489\n",
      "class 0: acc 0.8647887323943662, correct 307/355\n",
      "class 1: acc 0.8371428571428572, correct 293/350\n",
      "\n",
      "Val Set, val_loss: 0.5325, val_error: 0.2750, auc: 0.8848\n",
      "class 0: acc 0.673469387755102, correct 33/49\n",
      "class 1: acc 0.8064516129032258, correct 25/31\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0084, label: 0, bag_size: 575\n",
      "batch 39, loss: 0.1724, label: 1, bag_size: 3670\n",
      "batch 59, loss: 1.3479, label: 1, bag_size: 11773\n",
      "batch 79, loss: 0.4586, label: 0, bag_size: 24289\n",
      "batch 99, loss: 1.4911, label: 0, bag_size: 2482\n",
      "batch 119, loss: 1.7547, label: 1, bag_size: 3962\n",
      "batch 139, loss: 0.1078, label: 0, bag_size: 3307\n",
      "batch 159, loss: 0.0052, label: 1, bag_size: 5507\n",
      "batch 179, loss: 0.0057, label: 0, bag_size: 1892\n",
      "batch 199, loss: 0.2827, label: 1, bag_size: 4159\n",
      "batch 219, loss: 0.1625, label: 1, bag_size: 2558\n",
      "batch 239, loss: 0.0641, label: 1, bag_size: 18681\n",
      "batch 259, loss: 0.2961, label: 1, bag_size: 11513\n",
      "batch 279, loss: 0.1505, label: 0, bag_size: 16582\n",
      "batch 299, loss: 0.1842, label: 1, bag_size: 3824\n",
      "batch 319, loss: 0.0009, label: 1, bag_size: 4731\n",
      "batch 339, loss: 0.0106, label: 1, bag_size: 1316\n",
      "batch 359, loss: 1.5432, label: 0, bag_size: 2762\n",
      "batch 379, loss: 0.3161, label: 0, bag_size: 9742\n",
      "batch 399, loss: 0.2523, label: 0, bag_size: 11040\n",
      "batch 419, loss: 0.2339, label: 1, bag_size: 6759\n",
      "batch 439, loss: 0.0036, label: 0, bag_size: 938\n",
      "batch 459, loss: 0.0168, label: 1, bag_size: 8331\n",
      "batch 479, loss: 0.0215, label: 0, bag_size: 2398\n",
      "batch 499, loss: 0.0675, label: 0, bag_size: 2328\n",
      "batch 519, loss: 0.1030, label: 1, bag_size: 2081\n",
      "batch 539, loss: 1.1471, label: 1, bag_size: 3358\n",
      "batch 559, loss: 0.0301, label: 1, bag_size: 4377\n",
      "batch 579, loss: 0.3286, label: 1, bag_size: 3391\n",
      "batch 599, loss: 0.3642, label: 0, bag_size: 16789\n",
      "batch 619, loss: 0.1211, label: 1, bag_size: 2207\n",
      "batch 639, loss: 0.0022, label: 1, bag_size: 11952\n",
      "batch 659, loss: 0.3839, label: 1, bag_size: 1582\n",
      "batch 679, loss: 0.0952, label: 0, bag_size: 1684\n",
      "batch 699, loss: 0.2187, label: 1, bag_size: 1272\n",
      "Epoch: 45, train_loss: 0.3308, train_error: 0.1560\n",
      "class 0: acc 0.8753462603878116, correct 316/361\n",
      "class 1: acc 0.811046511627907, correct 279/344\n",
      "\n",
      "Val Set, val_loss: 0.4147, val_error: 0.2125, auc: 0.9032\n",
      "class 0: acc 0.7755102040816326, correct 38/49\n",
      "class 1: acc 0.8064516129032258, correct 25/31\n",
      "EarlyStopping counter: 12 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0068, label: 1, bag_size: 4970\n",
      "batch 39, loss: 0.4972, label: 1, bag_size: 5065\n",
      "batch 59, loss: 0.4551, label: 0, bag_size: 4076\n",
      "batch 79, loss: 0.0436, label: 1, bag_size: 6500\n",
      "batch 99, loss: 0.0386, label: 0, bag_size: 5074\n",
      "batch 119, loss: 1.1528, label: 1, bag_size: 5341\n",
      "batch 139, loss: 0.0173, label: 1, bag_size: 3081\n",
      "batch 159, loss: 0.2799, label: 0, bag_size: 2009\n",
      "batch 179, loss: 0.6601, label: 1, bag_size: 5065\n",
      "batch 199, loss: 0.6204, label: 0, bag_size: 4468\n",
      "batch 219, loss: 0.7716, label: 1, bag_size: 3626\n",
      "batch 239, loss: 0.3152, label: 0, bag_size: 4822\n",
      "batch 259, loss: 0.0301, label: 1, bag_size: 5887\n",
      "batch 279, loss: 0.0605, label: 0, bag_size: 26374\n",
      "batch 299, loss: 0.2584, label: 0, bag_size: 12517\n",
      "batch 319, loss: 0.1785, label: 0, bag_size: 5269\n",
      "batch 339, loss: 0.1543, label: 0, bag_size: 15976\n",
      "batch 359, loss: 0.0772, label: 1, bag_size: 4803\n",
      "batch 379, loss: 0.1301, label: 0, bag_size: 4587\n",
      "batch 399, loss: 0.3844, label: 0, bag_size: 2747\n",
      "batch 419, loss: 0.2714, label: 0, bag_size: 15706\n",
      "batch 439, loss: 0.4660, label: 1, bag_size: 23277\n",
      "batch 459, loss: 0.0643, label: 0, bag_size: 13606\n",
      "batch 479, loss: 0.0259, label: 1, bag_size: 5672\n",
      "batch 499, loss: 2.9276, label: 0, bag_size: 5877\n",
      "batch 519, loss: 0.0316, label: 1, bag_size: 3702\n",
      "batch 539, loss: 0.1015, label: 0, bag_size: 13609\n",
      "batch 559, loss: 0.0001, label: 0, bag_size: 2667\n",
      "batch 579, loss: 0.0183, label: 0, bag_size: 9367\n",
      "batch 599, loss: 0.4228, label: 1, bag_size: 12948\n",
      "batch 619, loss: 1.5545, label: 1, bag_size: 3627\n",
      "batch 639, loss: 0.3758, label: 0, bag_size: 6391\n",
      "batch 659, loss: 0.0063, label: 0, bag_size: 4548\n",
      "batch 679, loss: 0.2258, label: 1, bag_size: 14487\n",
      "batch 699, loss: 0.1437, label: 1, bag_size: 4737\n",
      "Epoch: 46, train_loss: 0.3448, train_error: 0.1574\n",
      "class 0: acc 0.8715083798882681, correct 312/358\n",
      "class 1: acc 0.8126801152737753, correct 282/347\n",
      "\n",
      "Val Set, val_loss: 0.5157, val_error: 0.2625, auc: 0.8835\n",
      "class 0: acc 0.6938775510204082, correct 34/49\n",
      "class 1: acc 0.8064516129032258, correct 25/31\n",
      "EarlyStopping counter: 13 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0333, label: 0, bag_size: 3778\n",
      "batch 39, loss: 1.5125, label: 1, bag_size: 2036\n",
      "batch 59, loss: 0.0828, label: 1, bag_size: 5072\n",
      "batch 79, loss: 0.0344, label: 1, bag_size: 5100\n",
      "batch 99, loss: 1.3291, label: 1, bag_size: 10165\n",
      "batch 119, loss: 0.0156, label: 1, bag_size: 2558\n",
      "batch 139, loss: 0.0508, label: 1, bag_size: 12654\n",
      "batch 159, loss: 0.1218, label: 1, bag_size: 3571\n",
      "batch 179, loss: 0.2047, label: 0, bag_size: 16737\n",
      "batch 199, loss: 0.0311, label: 0, bag_size: 4737\n",
      "batch 219, loss: 0.9088, label: 1, bag_size: 2250\n",
      "batch 239, loss: 0.0000, label: 0, bag_size: 2667\n",
      "batch 259, loss: 0.0982, label: 1, bag_size: 1178\n",
      "batch 279, loss: 0.5128, label: 0, bag_size: 18060\n",
      "batch 299, loss: 0.1179, label: 1, bag_size: 19581\n",
      "batch 319, loss: 0.1097, label: 0, bag_size: 14696\n",
      "batch 339, loss: 0.0079, label: 1, bag_size: 1503\n",
      "batch 359, loss: 0.0500, label: 0, bag_size: 2328\n",
      "batch 379, loss: 0.2205, label: 0, bag_size: 13846\n",
      "batch 399, loss: 0.4519, label: 1, bag_size: 1808\n",
      "batch 419, loss: 0.0183, label: 0, bag_size: 3137\n",
      "batch 439, loss: 0.0642, label: 1, bag_size: 6410\n",
      "batch 459, loss: 0.3031, label: 1, bag_size: 2687\n",
      "batch 479, loss: 3.7223, label: 0, bag_size: 1648\n",
      "batch 499, loss: 0.9742, label: 1, bag_size: 6392\n",
      "batch 519, loss: 0.0361, label: 1, bag_size: 5817\n",
      "batch 539, loss: 0.0071, label: 0, bag_size: 2667\n",
      "batch 559, loss: 0.1135, label: 0, bag_size: 5533\n",
      "batch 579, loss: 0.0044, label: 1, bag_size: 6421\n",
      "batch 599, loss: 0.0033, label: 0, bag_size: 2667\n",
      "batch 619, loss: 0.1275, label: 0, bag_size: 2913\n",
      "batch 639, loss: 0.0145, label: 1, bag_size: 3733\n",
      "batch 659, loss: 0.2285, label: 1, bag_size: 2178\n",
      "batch 679, loss: 0.0020, label: 1, bag_size: 3117\n",
      "batch 699, loss: 0.4500, label: 0, bag_size: 19037\n",
      "Epoch: 47, train_loss: 0.3233, train_error: 0.1404\n",
      "class 0: acc 0.8760563380281691, correct 311/355\n",
      "class 1: acc 0.8428571428571429, correct 295/350\n",
      "\n",
      "Val Set, val_loss: 0.4814, val_error: 0.2500, auc: 0.8650\n",
      "class 0: acc 0.7551020408163265, correct 37/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 14 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0072, label: 0, bag_size: 5982\n",
      "batch 39, loss: 0.0364, label: 0, bag_size: 5307\n",
      "batch 59, loss: 2.2250, label: 0, bag_size: 4282\n",
      "batch 79, loss: 1.0968, label: 1, bag_size: 4800\n",
      "batch 99, loss: 0.2310, label: 1, bag_size: 2905\n",
      "batch 119, loss: 0.2474, label: 0, bag_size: 20134\n",
      "batch 139, loss: 0.4623, label: 0, bag_size: 3598\n",
      "batch 159, loss: 1.1676, label: 0, bag_size: 3441\n",
      "batch 179, loss: 0.2070, label: 0, bag_size: 2022\n",
      "batch 199, loss: 0.3420, label: 1, bag_size: 3588\n",
      "batch 219, loss: 0.1915, label: 1, bag_size: 1825\n",
      "batch 239, loss: 0.5341, label: 0, bag_size: 3517\n",
      "batch 259, loss: 0.2392, label: 1, bag_size: 5100\n",
      "batch 279, loss: 0.0054, label: 1, bag_size: 24092\n",
      "batch 299, loss: 0.0551, label: 0, bag_size: 6255\n",
      "batch 319, loss: 0.0176, label: 0, bag_size: 7862\n",
      "batch 339, loss: 0.4629, label: 0, bag_size: 3240\n",
      "batch 359, loss: 0.1949, label: 0, bag_size: 6534\n",
      "batch 379, loss: 0.5948, label: 0, bag_size: 1884\n",
      "batch 399, loss: 0.2147, label: 1, bag_size: 24686\n",
      "batch 419, loss: 0.0224, label: 1, bag_size: 2646\n",
      "batch 439, loss: 0.2919, label: 1, bag_size: 4819\n",
      "batch 459, loss: 0.2899, label: 0, bag_size: 2238\n",
      "batch 479, loss: 0.0007, label: 0, bag_size: 1892\n",
      "batch 499, loss: 0.7487, label: 0, bag_size: 3600\n",
      "batch 519, loss: 0.0015, label: 1, bag_size: 11555\n",
      "batch 539, loss: 0.1063, label: 0, bag_size: 4228\n",
      "batch 559, loss: 0.1060, label: 1, bag_size: 3578\n",
      "batch 579, loss: 0.5543, label: 0, bag_size: 3381\n",
      "batch 599, loss: 0.2563, label: 0, bag_size: 26830\n",
      "batch 619, loss: 1.2988, label: 0, bag_size: 2762\n",
      "batch 639, loss: 0.8022, label: 1, bag_size: 4802\n",
      "batch 659, loss: 0.3841, label: 1, bag_size: 22171\n",
      "batch 679, loss: 0.4667, label: 1, bag_size: 3764\n",
      "batch 699, loss: 0.2327, label: 0, bag_size: 4332\n",
      "Epoch: 48, train_loss: 0.3234, train_error: 0.1433\n",
      "class 0: acc 0.8782608695652174, correct 303/345\n",
      "class 1: acc 0.8361111111111111, correct 301/360\n",
      "\n",
      "Val Set, val_loss: 0.4675, val_error: 0.1750, auc: 0.8874\n",
      "class 0: acc 0.9183673469387755, correct 45/49\n",
      "class 1: acc 0.6774193548387096, correct 21/31\n",
      "EarlyStopping counter: 15 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0264, label: 1, bag_size: 4243\n",
      "batch 39, loss: 0.0096, label: 0, bag_size: 2328\n",
      "batch 59, loss: 0.0080, label: 0, bag_size: 5624\n",
      "batch 79, loss: 1.6612, label: 0, bag_size: 6301\n",
      "batch 99, loss: 0.1244, label: 0, bag_size: 1865\n",
      "batch 119, loss: 0.2997, label: 0, bag_size: 2747\n",
      "batch 139, loss: 0.0089, label: 0, bag_size: 2923\n",
      "batch 159, loss: 0.0024, label: 1, bag_size: 24868\n",
      "batch 179, loss: 0.0113, label: 1, bag_size: 4069\n",
      "batch 199, loss: 0.0124, label: 1, bag_size: 3834\n",
      "batch 219, loss: 0.8015, label: 1, bag_size: 5298\n",
      "batch 239, loss: 0.2661, label: 1, bag_size: 5152\n",
      "batch 259, loss: 1.2767, label: 0, bag_size: 5493\n",
      "batch 279, loss: 0.1200, label: 0, bag_size: 5924\n",
      "batch 299, loss: 0.1254, label: 0, bag_size: 2205\n",
      "batch 319, loss: 0.1373, label: 1, bag_size: 1700\n",
      "batch 339, loss: 0.4655, label: 0, bag_size: 14662\n",
      "batch 359, loss: 0.0440, label: 1, bag_size: 2890\n",
      "batch 379, loss: 0.0098, label: 1, bag_size: 5152\n",
      "batch 399, loss: 0.0326, label: 0, bag_size: 2622\n",
      "batch 419, loss: 0.0181, label: 0, bag_size: 1822\n",
      "batch 439, loss: 0.1428, label: 0, bag_size: 4084\n",
      "batch 459, loss: 0.1672, label: 0, bag_size: 11451\n",
      "batch 479, loss: 0.0405, label: 1, bag_size: 6138\n",
      "batch 499, loss: 0.5682, label: 1, bag_size: 1454\n",
      "batch 519, loss: 0.0905, label: 0, bag_size: 25403\n",
      "batch 539, loss: 0.5999, label: 1, bag_size: 4094\n",
      "batch 559, loss: 0.4718, label: 1, bag_size: 4880\n",
      "batch 579, loss: 0.0980, label: 1, bag_size: 21059\n",
      "batch 599, loss: 0.0411, label: 0, bag_size: 3327\n",
      "batch 619, loss: 0.0039, label: 1, bag_size: 5428\n",
      "batch 639, loss: 0.0797, label: 0, bag_size: 3317\n",
      "batch 659, loss: 0.2029, label: 1, bag_size: 2687\n",
      "batch 679, loss: 0.0127, label: 1, bag_size: 3834\n",
      "batch 699, loss: 0.9285, label: 1, bag_size: 10952\n",
      "Epoch: 49, train_loss: 0.3502, train_error: 0.1504\n",
      "class 0: acc 0.8774373259052924, correct 315/359\n",
      "class 1: acc 0.8208092485549133, correct 284/346\n",
      "\n",
      "Val Set, val_loss: 0.3657, val_error: 0.1750, auc: 0.9098\n",
      "class 0: acc 0.9183673469387755, correct 45/49\n",
      "class 1: acc 0.6774193548387096, correct 21/31\n",
      "Validation loss decreased (0.376480 --> 0.365660).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2043, label: 1, bag_size: 5677\n",
      "batch 39, loss: 0.0083, label: 0, bag_size: 3616\n",
      "batch 59, loss: 0.6043, label: 0, bag_size: 2678\n",
      "batch 79, loss: 1.6245, label: 0, bag_size: 3830\n",
      "batch 99, loss: 1.4428, label: 1, bag_size: 21399\n",
      "batch 119, loss: 1.0019, label: 1, bag_size: 1004\n",
      "batch 139, loss: 0.1834, label: 0, bag_size: 1943\n",
      "batch 159, loss: 0.2305, label: 0, bag_size: 3805\n",
      "batch 179, loss: 0.0167, label: 0, bag_size: 3534\n",
      "batch 199, loss: 0.7320, label: 1, bag_size: 4094\n",
      "batch 219, loss: 0.6236, label: 0, bag_size: 2592\n",
      "batch 239, loss: 0.0007, label: 1, bag_size: 19173\n",
      "batch 259, loss: 0.0943, label: 0, bag_size: 26830\n",
      "batch 279, loss: 0.0418, label: 1, bag_size: 5199\n",
      "batch 299, loss: 0.1068, label: 0, bag_size: 5527\n",
      "batch 319, loss: 0.1043, label: 1, bag_size: 5677\n",
      "batch 339, loss: 0.3782, label: 0, bag_size: 4287\n",
      "batch 359, loss: 0.0232, label: 1, bag_size: 7090\n",
      "batch 379, loss: 0.0369, label: 0, bag_size: 10532\n",
      "batch 399, loss: 0.2635, label: 1, bag_size: 5638\n",
      "batch 419, loss: 0.9486, label: 1, bag_size: 3391\n",
      "batch 439, loss: 0.3255, label: 0, bag_size: 2747\n",
      "batch 459, loss: 0.4089, label: 0, bag_size: 17273\n",
      "batch 479, loss: 0.8201, label: 0, bag_size: 2089\n",
      "batch 499, loss: 0.0461, label: 0, bag_size: 2862\n",
      "batch 519, loss: 1.2156, label: 1, bag_size: 2626\n",
      "batch 539, loss: 0.1711, label: 0, bag_size: 6135\n",
      "batch 559, loss: 0.1298, label: 0, bag_size: 4146\n",
      "batch 579, loss: 0.0153, label: 1, bag_size: 5672\n",
      "batch 599, loss: 0.0097, label: 1, bag_size: 4698\n",
      "batch 619, loss: 3.3595, label: 0, bag_size: 2316\n",
      "batch 639, loss: 0.0097, label: 1, bag_size: 4436\n",
      "batch 659, loss: 0.0002, label: 1, bag_size: 6295\n",
      "batch 679, loss: 0.0044, label: 1, bag_size: 1503\n",
      "batch 699, loss: 0.1658, label: 1, bag_size: 3420\n",
      "Epoch: 50, train_loss: 0.2892, train_error: 0.1333\n",
      "class 0: acc 0.8852941176470588, correct 301/340\n",
      "class 1: acc 0.8493150684931506, correct 310/365\n",
      "\n",
      "Val Set, val_loss: 0.5741, val_error: 0.2625, auc: 0.8552\n",
      "class 0: acc 0.7346938775510204, correct 36/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0061, label: 1, bag_size: 4162\n",
      "batch 39, loss: 2.1410, label: 1, bag_size: 20149\n",
      "batch 59, loss: 0.0781, label: 0, bag_size: 14212\n",
      "batch 79, loss: 0.0368, label: 0, bag_size: 3732\n",
      "batch 99, loss: 0.0371, label: 0, bag_size: 11299\n",
      "batch 119, loss: 0.0633, label: 1, bag_size: 2874\n",
      "batch 139, loss: 0.0754, label: 0, bag_size: 4124\n",
      "batch 159, loss: 0.5782, label: 0, bag_size: 3268\n",
      "batch 179, loss: 0.4914, label: 1, bag_size: 20056\n",
      "batch 199, loss: 0.0519, label: 0, bag_size: 4084\n",
      "batch 219, loss: 0.1660, label: 1, bag_size: 1374\n",
      "batch 239, loss: 1.0406, label: 0, bag_size: 1671\n",
      "batch 259, loss: 0.0138, label: 1, bag_size: 4243\n",
      "batch 279, loss: 0.0463, label: 0, bag_size: 2533\n",
      "batch 299, loss: 0.0472, label: 0, bag_size: 3159\n",
      "batch 319, loss: 0.0004, label: 1, bag_size: 6600\n",
      "batch 339, loss: 0.0050, label: 1, bag_size: 18015\n",
      "batch 359, loss: 0.0132, label: 1, bag_size: 3702\n",
      "batch 379, loss: 0.6020, label: 1, bag_size: 4761\n",
      "batch 399, loss: 0.2089, label: 0, bag_size: 5413\n",
      "batch 419, loss: 0.0470, label: 0, bag_size: 7862\n",
      "batch 439, loss: 0.0191, label: 1, bag_size: 5071\n",
      "batch 459, loss: 0.0342, label: 1, bag_size: 2701\n",
      "batch 479, loss: 0.3747, label: 1, bag_size: 5801\n",
      "batch 499, loss: 0.1681, label: 1, bag_size: 3358\n",
      "batch 519, loss: 0.0161, label: 0, bag_size: 4649\n",
      "batch 539, loss: 0.0069, label: 1, bag_size: 15434\n",
      "batch 559, loss: 0.0534, label: 0, bag_size: 9003\n",
      "batch 579, loss: 0.0129, label: 0, bag_size: 1379\n",
      "batch 599, loss: 0.0030, label: 1, bag_size: 4970\n",
      "batch 619, loss: 0.2462, label: 0, bag_size: 6168\n",
      "batch 639, loss: 0.0147, label: 0, bag_size: 3137\n",
      "batch 659, loss: 0.0451, label: 1, bag_size: 16427\n",
      "batch 679, loss: 0.4340, label: 0, bag_size: 24289\n",
      "batch 699, loss: 0.9170, label: 1, bag_size: 5665\n",
      "Epoch: 51, train_loss: 0.2891, train_error: 0.1390\n",
      "class 0: acc 0.8549382716049383, correct 277/324\n",
      "class 1: acc 0.8661417322834646, correct 330/381\n",
      "\n",
      "Val Set, val_loss: 0.4050, val_error: 0.1750, auc: 0.9065\n",
      "class 0: acc 0.8775510204081632, correct 43/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7743, label: 0, bag_size: 3830\n",
      "batch 39, loss: 0.0945, label: 0, bag_size: 7285\n",
      "batch 59, loss: 0.2658, label: 0, bag_size: 3450\n",
      "batch 79, loss: 0.0055, label: 1, bag_size: 3557\n",
      "batch 99, loss: 0.1657, label: 1, bag_size: 534\n",
      "batch 119, loss: 0.0399, label: 1, bag_size: 6410\n",
      "batch 139, loss: 0.0141, label: 1, bag_size: 21059\n",
      "batch 159, loss: 0.5055, label: 1, bag_size: 5072\n",
      "batch 179, loss: 0.0144, label: 1, bag_size: 4069\n",
      "batch 199, loss: 0.0399, label: 0, bag_size: 17770\n",
      "batch 219, loss: 0.2770, label: 0, bag_size: 941\n",
      "batch 239, loss: 0.0113, label: 0, bag_size: 3001\n",
      "batch 259, loss: 0.0003, label: 1, bag_size: 2874\n",
      "batch 279, loss: 0.0018, label: 1, bag_size: 23777\n",
      "batch 299, loss: 0.1647, label: 0, bag_size: 4406\n",
      "batch 319, loss: 0.0251, label: 1, bag_size: 1627\n",
      "batch 339, loss: 0.1449, label: 0, bag_size: 21335\n",
      "batch 359, loss: 0.0847, label: 0, bag_size: 11299\n",
      "batch 379, loss: 0.1386, label: 0, bag_size: 2452\n",
      "batch 399, loss: 0.8369, label: 0, bag_size: 4468\n",
      "batch 419, loss: 0.0024, label: 0, bag_size: 2586\n",
      "batch 439, loss: 0.8755, label: 1, bag_size: 1004\n",
      "batch 459, loss: 0.4458, label: 0, bag_size: 15060\n",
      "batch 479, loss: 0.0090, label: 0, bag_size: 1822\n",
      "batch 499, loss: 0.0152, label: 0, bag_size: 14696\n",
      "batch 519, loss: 0.0093, label: 0, bag_size: 7862\n",
      "batch 539, loss: 0.0073, label: 1, bag_size: 2005\n",
      "batch 559, loss: 0.0101, label: 1, bag_size: 4233\n",
      "batch 579, loss: 0.0534, label: 0, bag_size: 4750\n",
      "batch 599, loss: 1.0742, label: 0, bag_size: 3830\n",
      "batch 619, loss: 0.0129, label: 0, bag_size: 5021\n",
      "batch 639, loss: 0.8092, label: 0, bag_size: 5967\n",
      "batch 659, loss: 0.0135, label: 0, bag_size: 4181\n",
      "batch 679, loss: 0.7654, label: 1, bag_size: 641\n",
      "batch 699, loss: 0.2760, label: 1, bag_size: 3454\n",
      "Epoch: 52, train_loss: 0.2542, train_error: 0.1135\n",
      "class 0: acc 0.8832335329341318, correct 295/334\n",
      "class 1: acc 0.889487870619946, correct 330/371\n",
      "\n",
      "Val Set, val_loss: 0.5536, val_error: 0.2500, auc: 0.8585\n",
      "class 0: acc 0.7346938775510204, correct 36/49\n",
      "class 1: acc 0.7741935483870968, correct 24/31\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1073, label: 1, bag_size: 2207\n",
      "batch 39, loss: 0.0008, label: 0, bag_size: 3159\n",
      "batch 59, loss: 0.8191, label: 0, bag_size: 14212\n",
      "batch 79, loss: 0.0097, label: 1, bag_size: 3672\n",
      "batch 99, loss: 0.3199, label: 0, bag_size: 5120\n",
      "batch 119, loss: 0.3301, label: 1, bag_size: 2039\n",
      "batch 139, loss: 0.7889, label: 0, bag_size: 6491\n",
      "batch 159, loss: 0.0142, label: 0, bag_size: 4649\n",
      "batch 179, loss: 0.8079, label: 1, bag_size: 5298\n",
      "batch 199, loss: 0.0902, label: 1, bag_size: 6410\n",
      "batch 219, loss: 0.0974, label: 1, bag_size: 3672\n",
      "batch 239, loss: 0.0518, label: 0, bag_size: 4641\n",
      "batch 259, loss: 1.3459, label: 1, bag_size: 22171\n",
      "batch 279, loss: 0.0592, label: 0, bag_size: 3714\n",
      "batch 299, loss: 0.1301, label: 1, bag_size: 3824\n",
      "batch 319, loss: 0.0408, label: 0, bag_size: 1553\n",
      "batch 339, loss: 0.0007, label: 1, bag_size: 19173\n",
      "batch 359, loss: 0.2952, label: 0, bag_size: 6209\n",
      "batch 379, loss: 0.0146, label: 1, bag_size: 7641\n",
      "batch 399, loss: 0.0644, label: 0, bag_size: 14212\n",
      "batch 419, loss: 0.0021, label: 1, bag_size: 1722\n",
      "batch 439, loss: 0.1895, label: 0, bag_size: 4155\n",
      "batch 459, loss: 0.3070, label: 0, bag_size: 21574\n",
      "batch 479, loss: 0.4616, label: 0, bag_size: 4758\n",
      "batch 499, loss: 0.8165, label: 1, bag_size: 2036\n",
      "batch 519, loss: 0.8265, label: 1, bag_size: 4268\n",
      "batch 539, loss: 0.1406, label: 0, bag_size: 2483\n",
      "batch 559, loss: 0.7944, label: 1, bag_size: 5065\n",
      "batch 579, loss: 0.0323, label: 0, bag_size: 19055\n",
      "batch 599, loss: 0.0160, label: 1, bag_size: 3672\n",
      "batch 619, loss: 0.0790, label: 0, bag_size: 5504\n",
      "batch 639, loss: 0.0614, label: 1, bag_size: 7090\n",
      "batch 659, loss: 0.0076, label: 0, bag_size: 5546\n",
      "batch 679, loss: 0.8591, label: 0, bag_size: 4468\n",
      "batch 699, loss: 0.0610, label: 0, bag_size: 4758\n",
      "Epoch: 53, train_loss: 0.2968, train_error: 0.1376\n",
      "class 0: acc 0.8704225352112676, correct 309/355\n",
      "class 1: acc 0.8542857142857143, correct 299/350\n",
      "\n",
      "Val Set, val_loss: 0.4121, val_error: 0.1625, auc: 0.9019\n",
      "class 0: acc 0.9387755102040817, correct 46/49\n",
      "class 1: acc 0.6774193548387096, correct 21/31\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0719, label: 0, bag_size: 25403\n",
      "batch 39, loss: 0.4597, label: 0, bag_size: 3598\n",
      "batch 59, loss: 0.8088, label: 0, bag_size: 3295\n",
      "batch 79, loss: 0.0073, label: 1, bag_size: 2656\n",
      "batch 99, loss: 0.0660, label: 0, bag_size: 3504\n",
      "batch 119, loss: 0.5081, label: 1, bag_size: 1838\n",
      "batch 139, loss: 1.1082, label: 0, bag_size: 2851\n",
      "batch 159, loss: 0.3828, label: 0, bag_size: 4332\n",
      "batch 179, loss: 0.1031, label: 0, bag_size: 22936\n",
      "batch 199, loss: 0.0231, label: 0, bag_size: 6534\n",
      "batch 219, loss: 0.0088, label: 1, bag_size: 4452\n",
      "batch 239, loss: 0.3525, label: 1, bag_size: 534\n",
      "batch 259, loss: 0.3486, label: 0, bag_size: 4329\n",
      "batch 279, loss: 0.1192, label: 0, bag_size: 12656\n",
      "batch 299, loss: 0.0067, label: 1, bag_size: 4803\n",
      "batch 319, loss: 0.0092, label: 0, bag_size: 18251\n",
      "batch 339, loss: 0.0032, label: 0, bag_size: 2892\n",
      "batch 359, loss: 0.0759, label: 0, bag_size: 9003\n",
      "batch 379, loss: 0.0005, label: 1, bag_size: 6760\n",
      "batch 399, loss: 0.2216, label: 1, bag_size: 2178\n",
      "batch 419, loss: 2.8310, label: 1, bag_size: 6392\n",
      "batch 439, loss: 0.3497, label: 0, bag_size: 13411\n",
      "batch 459, loss: 0.5258, label: 1, bag_size: 13217\n",
      "batch 479, loss: 0.4058, label: 0, bag_size: 6047\n",
      "batch 499, loss: 0.1029, label: 0, bag_size: 5269\n",
      "batch 519, loss: 0.0069, label: 1, bag_size: 4698\n",
      "batch 539, loss: 0.0070, label: 1, bag_size: 6861\n",
      "batch 559, loss: 0.0042, label: 1, bag_size: 4159\n",
      "batch 579, loss: 0.0077, label: 1, bag_size: 5777\n",
      "batch 599, loss: 0.0817, label: 0, bag_size: 27817\n",
      "batch 619, loss: 0.2625, label: 1, bag_size: 2447\n",
      "batch 639, loss: 0.0318, label: 1, bag_size: 16936\n",
      "batch 659, loss: 0.0149, label: 1, bag_size: 16936\n",
      "batch 679, loss: 0.0241, label: 0, bag_size: 2428\n",
      "batch 699, loss: 0.2611, label: 1, bag_size: 9755\n",
      "Epoch: 54, train_loss: 0.3219, train_error: 0.1418\n",
      "class 0: acc 0.850828729281768, correct 308/362\n",
      "class 1: acc 0.8658892128279884, correct 297/343\n",
      "\n",
      "Val Set, val_loss: 0.4968, val_error: 0.2500, auc: 0.8723\n",
      "class 0: acc 0.7959183673469388, correct 39/49\n",
      "class 1: acc 0.6774193548387096, correct 21/31\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0967, label: 1, bag_size: 1937\n",
      "batch 39, loss: 0.0177, label: 1, bag_size: 1503\n",
      "batch 59, loss: 0.0235, label: 0, bag_size: 2862\n",
      "batch 79, loss: 1.2592, label: 0, bag_size: 3441\n",
      "batch 99, loss: 0.9530, label: 1, bag_size: 5065\n",
      "batch 119, loss: 0.0070, label: 0, bag_size: 15747\n",
      "batch 139, loss: 0.6993, label: 0, bag_size: 1095\n",
      "batch 159, loss: 0.0477, label: 0, bag_size: 11758\n",
      "batch 179, loss: 0.4463, label: 0, bag_size: 4175\n",
      "batch 199, loss: 0.3681, label: 1, bag_size: 8075\n",
      "batch 219, loss: 0.5119, label: 0, bag_size: 3598\n",
      "batch 239, loss: 0.0290, label: 0, bag_size: 13606\n",
      "batch 259, loss: 0.0453, label: 1, bag_size: 2899\n",
      "batch 279, loss: 3.2919, label: 0, bag_size: 2316\n",
      "batch 299, loss: 0.7581, label: 1, bag_size: 3208\n",
      "batch 319, loss: 0.1227, label: 1, bag_size: 10736\n",
      "batch 339, loss: 0.1052, label: 0, bag_size: 1822\n",
      "batch 359, loss: 0.0084, label: 1, bag_size: 5108\n",
      "batch 379, loss: 4.2065, label: 0, bag_size: 2054\n",
      "batch 399, loss: 0.0109, label: 0, bag_size: 5002\n",
      "batch 419, loss: 0.0098, label: 1, bag_size: 4069\n",
      "batch 439, loss: 1.3526, label: 1, bag_size: 15407\n",
      "batch 459, loss: 1.2653, label: 1, bag_size: 5298\n",
      "batch 479, loss: 0.0031, label: 1, bag_size: 5777\n",
      "batch 499, loss: 0.0406, label: 1, bag_size: 1937\n",
      "batch 519, loss: 0.2495, label: 0, bag_size: 3295\n",
      "batch 539, loss: 0.0484, label: 0, bag_size: 3392\n",
      "batch 559, loss: 0.0096, label: 1, bag_size: 4985\n",
      "batch 579, loss: 0.0212, label: 1, bag_size: 4985\n",
      "batch 599, loss: 0.0080, label: 1, bag_size: 16427\n",
      "batch 619, loss: 0.0023, label: 0, bag_size: 4548\n",
      "batch 639, loss: 0.1266, label: 0, bag_size: 13411\n",
      "batch 659, loss: 0.2777, label: 0, bag_size: 8088\n",
      "batch 679, loss: 0.0097, label: 0, bag_size: 3129\n",
      "batch 699, loss: 0.6834, label: 0, bag_size: 8469\n",
      "Epoch: 55, train_loss: 0.3285, train_error: 0.1574\n",
      "class 0: acc 0.8608695652173913, correct 297/345\n",
      "class 1: acc 0.825, correct 297/360\n",
      "\n",
      "Val Set, val_loss: 0.5061, val_error: 0.2625, auc: 0.8604\n",
      "class 0: acc 0.7755102040816326, correct 38/49\n",
      "class 1: acc 0.6774193548387096, correct 21/31\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4583, label: 0, bag_size: 6301\n",
      "batch 39, loss: 0.0252, label: 0, bag_size: 14212\n",
      "batch 59, loss: 0.0008, label: 0, bag_size: 8376\n",
      "batch 79, loss: 0.3340, label: 1, bag_size: 2939\n",
      "batch 99, loss: 0.2457, label: 0, bag_size: 3517\n",
      "batch 119, loss: 0.7426, label: 1, bag_size: 3020\n",
      "batch 139, loss: 0.3711, label: 1, bag_size: 13226\n",
      "batch 159, loss: 0.0281, label: 0, bag_size: 3574\n",
      "batch 179, loss: 0.0398, label: 0, bag_size: 2286\n",
      "batch 199, loss: 0.0078, label: 0, bag_size: 3704\n",
      "batch 219, loss: 0.0013, label: 0, bag_size: 7978\n",
      "batch 239, loss: 0.0187, label: 1, bag_size: 15213\n",
      "batch 259, loss: 0.2179, label: 1, bag_size: 1825\n",
      "batch 279, loss: 0.0009, label: 1, bag_size: 5743\n",
      "batch 299, loss: 0.4237, label: 1, bag_size: 5298\n",
      "batch 319, loss: 0.1179, label: 1, bag_size: 5100\n",
      "batch 339, loss: 0.4564, label: 0, bag_size: 3448\n",
      "batch 359, loss: 0.4546, label: 0, bag_size: 1671\n",
      "batch 379, loss: 0.0273, label: 0, bag_size: 3427\n",
      "batch 399, loss: 0.0507, label: 0, bag_size: 4390\n",
      "batch 419, loss: 0.2401, label: 0, bag_size: 17711\n",
      "batch 439, loss: 0.0323, label: 1, bag_size: 15483\n",
      "batch 459, loss: 0.2763, label: 0, bag_size: 15747\n",
      "batch 479, loss: 0.5023, label: 1, bag_size: 5298\n",
      "batch 499, loss: 0.0176, label: 1, bag_size: 2754\n",
      "batch 519, loss: 0.4035, label: 0, bag_size: 5100\n",
      "batch 539, loss: 0.0196, label: 1, bag_size: 5199\n",
      "batch 559, loss: 0.5645, label: 1, bag_size: 5637\n",
      "batch 579, loss: 0.7085, label: 1, bag_size: 13217\n",
      "batch 599, loss: 0.0454, label: 0, bag_size: 2359\n",
      "batch 619, loss: 2.0698, label: 1, bag_size: 19307\n",
      "batch 639, loss: 0.0135, label: 0, bag_size: 14291\n",
      "batch 659, loss: 0.0160, label: 0, bag_size: 17711\n",
      "batch 679, loss: 0.0124, label: 0, bag_size: 3118\n",
      "batch 699, loss: 0.3638, label: 1, bag_size: 4737\n",
      "Epoch: 56, train_loss: 0.3019, train_error: 0.1319\n",
      "class 0: acc 0.873900293255132, correct 298/341\n",
      "class 1: acc 0.8626373626373627, correct 314/364\n",
      "\n",
      "Val Set, val_loss: 0.6250, val_error: 0.2625, auc: 0.8637\n",
      "class 0: acc 0.6938775510204082, correct 34/49\n",
      "class 1: acc 0.8064516129032258, correct 25/31\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1715, label: 1, bag_size: 6759\n",
      "batch 39, loss: 0.0112, label: 0, bag_size: 5448\n",
      "batch 59, loss: 0.0164, label: 1, bag_size: 3226\n",
      "batch 79, loss: 1.2691, label: 0, bag_size: 3420\n",
      "batch 99, loss: 0.2262, label: 0, bag_size: 4390\n",
      "batch 119, loss: 0.0063, label: 0, bag_size: 14212\n",
      "batch 139, loss: 0.9089, label: 1, bag_size: 1525\n",
      "batch 159, loss: 0.4164, label: 0, bag_size: 2918\n",
      "batch 179, loss: 0.0585, label: 0, bag_size: 3714\n",
      "batch 199, loss: 0.0220, label: 1, bag_size: 5507\n",
      "batch 219, loss: 0.0188, label: 0, bag_size: 2862\n",
      "batch 239, loss: 0.2116, label: 1, bag_size: 6477\n",
      "batch 259, loss: 0.0018, label: 1, bag_size: 1316\n",
      "batch 279, loss: 0.0015, label: 1, bag_size: 6295\n",
      "batch 299, loss: 1.5490, label: 1, bag_size: 13348\n",
      "batch 319, loss: 0.1030, label: 1, bag_size: 3578\n",
      "batch 339, loss: 0.0787, label: 1, bag_size: 3402\n",
      "batch 359, loss: 0.0674, label: 1, bag_size: 5743\n",
      "batch 379, loss: 0.0344, label: 1, bag_size: 4332\n",
      "batch 399, loss: 0.3369, label: 0, bag_size: 2863\n",
      "batch 419, loss: 0.7955, label: 0, bag_size: 3420\n",
      "batch 439, loss: 0.0554, label: 0, bag_size: 21574\n",
      "batch 459, loss: 0.0490, label: 1, bag_size: 10091\n",
      "batch 479, loss: 0.0916, label: 0, bag_size: 941\n",
      "batch 499, loss: 0.1201, label: 0, bag_size: 3240\n",
      "batch 519, loss: 0.2588, label: 1, bag_size: 4800\n",
      "batch 539, loss: 0.1753, label: 1, bag_size: 4880\n",
      "batch 559, loss: 0.0026, label: 1, bag_size: 19946\n",
      "batch 579, loss: 0.1421, label: 1, bag_size: 5539\n",
      "batch 599, loss: 0.0091, label: 0, bag_size: 1770\n",
      "batch 619, loss: 3.3565, label: 1, bag_size: 2343\n",
      "batch 639, loss: 0.0237, label: 0, bag_size: 5455\n",
      "batch 659, loss: 0.0514, label: 0, bag_size: 1250\n",
      "batch 679, loss: 0.0036, label: 1, bag_size: 10757\n",
      "batch 699, loss: 0.1572, label: 0, bag_size: 6301\n",
      "Epoch: 57, train_loss: 0.2787, train_error: 0.1305\n",
      "class 0: acc 0.8700564971751412, correct 308/354\n",
      "class 1: acc 0.8689458689458689, correct 305/351\n",
      "\n",
      "Val Set, val_loss: 0.3993, val_error: 0.1625, auc: 0.9059\n",
      "class 0: acc 0.9183673469387755, correct 45/49\n",
      "class 1: acc 0.7096774193548387, correct 22/31\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6174, label: 1, bag_size: 15483\n",
      "batch 39, loss: 0.0007, label: 1, bag_size: 5507\n",
      "batch 59, loss: 0.0006, label: 1, bag_size: 11952\n",
      "batch 79, loss: 0.0939, label: 0, bag_size: 4146\n",
      "batch 99, loss: 0.0274, label: 0, bag_size: 11256\n",
      "batch 119, loss: 0.1814, label: 1, bag_size: 5801\n",
      "batch 139, loss: 0.0081, label: 0, bag_size: 3347\n",
      "batch 159, loss: 0.6572, label: 1, bag_size: 10952\n",
      "batch 179, loss: 0.0184, label: 1, bag_size: 4377\n",
      "batch 199, loss: 0.0134, label: 0, bag_size: 3070\n",
      "batch 219, loss: 0.0005, label: 0, bag_size: 3615\n",
      "batch 239, loss: 0.0435, label: 0, bag_size: 4066\n",
      "batch 259, loss: 0.3069, label: 1, bag_size: 6697\n",
      "batch 279, loss: 0.0341, label: 0, bag_size: 2540\n",
      "batch 299, loss: 0.1725, label: 0, bag_size: 18376\n",
      "batch 319, loss: 0.0655, label: 0, bag_size: 2359\n",
      "batch 339, loss: 0.0219, label: 0, bag_size: 5527\n",
      "batch 359, loss: 0.0130, label: 1, bag_size: 4950\n",
      "batch 379, loss: 0.0023, label: 1, bag_size: 4815\n",
      "batch 399, loss: 0.4430, label: 0, bag_size: 3764\n",
      "batch 419, loss: 0.3071, label: 0, bag_size: 4012\n",
      "batch 439, loss: 0.8002, label: 1, bag_size: 4268\n",
      "batch 459, loss: 0.5179, label: 1, bag_size: 641\n",
      "batch 479, loss: 0.0519, label: 0, bag_size: 3805\n",
      "batch 499, loss: 0.0257, label: 1, bag_size: 6138\n",
      "batch 519, loss: 0.0044, label: 1, bag_size: 5887\n",
      "batch 539, loss: 0.0040, label: 1, bag_size: 3905\n",
      "batch 559, loss: 0.0026, label: 1, bag_size: 5507\n",
      "batch 579, loss: 0.3303, label: 0, bag_size: 4390\n",
      "batch 599, loss: 1.2554, label: 0, bag_size: 2851\n",
      "batch 619, loss: 0.4693, label: 1, bag_size: 20149\n",
      "batch 639, loss: 0.8006, label: 0, bag_size: 5074\n",
      "batch 659, loss: 0.1440, label: 0, bag_size: 14212\n",
      "batch 679, loss: 0.5461, label: 1, bag_size: 4737\n",
      "batch 699, loss: 0.0400, label: 1, bag_size: 15213\n",
      "Epoch: 58, train_loss: 0.2965, train_error: 0.1319\n",
      "class 0: acc 0.8656716417910447, correct 290/335\n",
      "class 1: acc 0.8702702702702703, correct 322/370\n",
      "\n",
      "Val Set, val_loss: 0.4474, val_error: 0.2125, auc: 0.8907\n",
      "class 0: acc 0.8163265306122449, correct 40/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4689, label: 1, bag_size: 4094\n",
      "batch 39, loss: 0.0114, label: 1, bag_size: 6600\n",
      "batch 59, loss: 0.0182, label: 0, bag_size: 2205\n",
      "batch 79, loss: 0.0058, label: 1, bag_size: 16936\n",
      "batch 99, loss: 0.3744, label: 1, bag_size: 1251\n",
      "batch 119, loss: 0.6362, label: 1, bag_size: 6190\n",
      "batch 139, loss: 0.9751, label: 1, bag_size: 22171\n",
      "batch 159, loss: 0.6909, label: 1, bag_size: 20256\n",
      "batch 179, loss: 0.0054, label: 1, bag_size: 3159\n",
      "batch 199, loss: 0.8983, label: 1, bag_size: 1479\n",
      "batch 219, loss: 0.0165, label: 1, bag_size: 15213\n",
      "batch 239, loss: 0.6053, label: 0, bag_size: 20045\n",
      "batch 259, loss: 0.0621, label: 1, bag_size: 14564\n",
      "batch 279, loss: 0.0267, label: 0, bag_size: 5021\n",
      "batch 299, loss: 0.0348, label: 0, bag_size: 7285\n",
      "batch 319, loss: 0.3298, label: 1, bag_size: 5298\n",
      "batch 339, loss: 0.0217, label: 0, bag_size: 7862\n",
      "batch 359, loss: 0.0676, label: 0, bag_size: 4181\n",
      "batch 379, loss: 0.1164, label: 1, bag_size: 20256\n",
      "batch 399, loss: 0.3724, label: 1, bag_size: 2447\n",
      "batch 419, loss: 0.3310, label: 0, bag_size: 2238\n",
      "batch 439, loss: 0.0020, label: 0, bag_size: 3099\n",
      "batch 459, loss: 0.0507, label: 1, bag_size: 4162\n",
      "batch 479, loss: 0.0587, label: 0, bag_size: 11797\n",
      "batch 499, loss: 0.0008, label: 1, bag_size: 1579\n",
      "batch 519, loss: 0.0205, label: 1, bag_size: 3777\n",
      "batch 539, loss: 0.0032, label: 0, bag_size: 10532\n",
      "batch 559, loss: 0.0176, label: 0, bag_size: 4737\n",
      "batch 579, loss: 0.0251, label: 1, bag_size: 6759\n",
      "batch 599, loss: 0.0070, label: 1, bag_size: 10091\n",
      "batch 619, loss: 0.0884, label: 1, bag_size: 15483\n",
      "batch 639, loss: 0.4492, label: 0, bag_size: 15976\n",
      "batch 659, loss: 0.0369, label: 0, bag_size: 2298\n",
      "batch 679, loss: 0.0110, label: 1, bag_size: 2493\n",
      "batch 699, loss: 0.0544, label: 1, bag_size: 8474\n",
      "Epoch: 59, train_loss: 0.2755, train_error: 0.1220\n",
      "class 0: acc 0.869942196531792, correct 301/346\n",
      "class 1: acc 0.8857938718662952, correct 318/359\n",
      "\n",
      "Val Set, val_loss: 0.4482, val_error: 0.2250, auc: 0.8841\n",
      "class 0: acc 0.8979591836734694, correct 44/49\n",
      "class 1: acc 0.5806451612903226, correct 18/31\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0018, label: 0, bag_size: 7978\n",
      "batch 39, loss: 1.1992, label: 0, bag_size: 4155\n",
      "batch 59, loss: 0.0121, label: 0, bag_size: 3521\n",
      "batch 79, loss: 0.0107, label: 0, bag_size: 5307\n",
      "batch 99, loss: 0.0261, label: 0, bag_size: 2678\n",
      "batch 119, loss: 0.0049, label: 1, bag_size: 16936\n",
      "batch 139, loss: 0.6654, label: 1, bag_size: 2178\n",
      "batch 159, loss: 0.0093, label: 1, bag_size: 8007\n",
      "batch 179, loss: 0.0003, label: 0, bag_size: 3540\n",
      "batch 199, loss: 0.0506, label: 0, bag_size: 4737\n",
      "batch 219, loss: 0.0022, label: 1, bag_size: 4249\n",
      "batch 239, loss: 0.3366, label: 1, bag_size: 3548\n",
      "batch 259, loss: 0.8548, label: 1, bag_size: 13217\n",
      "batch 279, loss: 0.0498, label: 1, bag_size: 5801\n",
      "batch 299, loss: 0.3646, label: 0, bag_size: 6391\n",
      "batch 319, loss: 0.2069, label: 0, bag_size: 4287\n",
      "batch 339, loss: 0.2500, label: 1, bag_size: 4802\n",
      "batch 359, loss: 0.0009, label: 1, bag_size: 7085\n",
      "batch 379, loss: 0.0225, label: 0, bag_size: 4506\n",
      "batch 399, loss: 0.0150, label: 1, bag_size: 4803\n",
      "batch 419, loss: 0.0809, label: 1, bag_size: 1958\n",
      "batch 439, loss: 0.1750, label: 0, bag_size: 7171\n",
      "batch 459, loss: 0.4198, label: 0, bag_size: 5493\n",
      "batch 479, loss: 0.0676, label: 0, bag_size: 3129\n",
      "batch 499, loss: 0.2036, label: 0, bag_size: 4180\n",
      "batch 519, loss: 0.0029, label: 1, bag_size: 4249\n",
      "batch 539, loss: 0.8063, label: 0, bag_size: 2048\n",
      "batch 559, loss: 0.0505, label: 0, bag_size: 25641\n",
      "batch 579, loss: 0.1750, label: 0, bag_size: 3268\n",
      "batch 599, loss: 0.6292, label: 1, bag_size: 5665\n",
      "batch 619, loss: 0.0217, label: 1, bag_size: 11773\n",
      "batch 639, loss: 1.3973, label: 0, bag_size: 16816\n",
      "batch 659, loss: 0.2304, label: 0, bag_size: 9945\n",
      "batch 679, loss: 0.2486, label: 1, bag_size: 3208\n",
      "batch 699, loss: 0.0134, label: 1, bag_size: 4069\n",
      "Epoch: 60, train_loss: 0.2553, train_error: 0.1106\n",
      "class 0: acc 0.8828828828828829, correct 294/333\n",
      "class 1: acc 0.8951612903225806, correct 333/372\n",
      "\n",
      "Val Set, val_loss: 0.5175, val_error: 0.2625, auc: 0.8795\n",
      "class 0: acc 0.7346938775510204, correct 36/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0097, label: 1, bag_size: 5823\n",
      "batch 39, loss: 0.4280, label: 0, bag_size: 2338\n",
      "batch 59, loss: 0.0260, label: 0, bag_size: 4291\n",
      "batch 79, loss: 0.0022, label: 1, bag_size: 4970\n",
      "batch 99, loss: 0.0017, label: 0, bag_size: 3615\n",
      "batch 119, loss: 0.4919, label: 1, bag_size: 2039\n",
      "batch 139, loss: 0.7319, label: 0, bag_size: 6301\n",
      "batch 159, loss: 0.0424, label: 1, bag_size: 2938\n",
      "batch 179, loss: 0.0195, label: 0, bag_size: 23860\n",
      "batch 199, loss: 0.0143, label: 1, bag_size: 2877\n",
      "batch 219, loss: 0.0006, label: 1, bag_size: 5458\n",
      "batch 239, loss: 0.2891, label: 0, bag_size: 2346\n",
      "batch 259, loss: 0.0026, label: 0, bag_size: 5522\n",
      "batch 279, loss: 0.0537, label: 0, bag_size: 16859\n",
      "batch 299, loss: 0.4043, label: 1, bag_size: 20955\n",
      "batch 319, loss: 0.0384, label: 1, bag_size: 5677\n",
      "batch 339, loss: 0.5590, label: 0, bag_size: 5476\n",
      "batch 359, loss: 0.0060, label: 0, bag_size: 1034\n",
      "batch 379, loss: 0.0028, label: 1, bag_size: 1579\n",
      "batch 399, loss: 0.0893, label: 0, bag_size: 4098\n",
      "batch 419, loss: 0.0029, label: 0, bag_size: 1539\n",
      "batch 439, loss: 0.0270, label: 1, bag_size: 3702\n",
      "batch 459, loss: 0.0825, label: 0, bag_size: 2013\n",
      "batch 479, loss: 0.6508, label: 1, bag_size: 11657\n",
      "batch 499, loss: 0.1501, label: 1, bag_size: 1479\n",
      "batch 519, loss: 3.8718, label: 0, bag_size: 3228\n",
      "batch 539, loss: 0.0189, label: 1, bag_size: 15434\n",
      "batch 559, loss: 0.2393, label: 0, bag_size: 28144\n",
      "batch 579, loss: 0.1725, label: 0, bag_size: 7484\n",
      "batch 599, loss: 0.1527, label: 0, bag_size: 3529\n",
      "batch 619, loss: 0.0065, label: 1, bag_size: 2877\n",
      "batch 639, loss: 0.0021, label: 0, bag_size: 5021\n",
      "batch 659, loss: 0.0115, label: 1, bag_size: 3081\n",
      "batch 679, loss: 0.1634, label: 1, bag_size: 4802\n",
      "batch 699, loss: 0.0000, label: 1, bag_size: 11952\n",
      "Epoch: 61, train_loss: 0.2483, train_error: 0.0965\n",
      "class 0: acc 0.8977272727272727, correct 316/352\n",
      "class 1: acc 0.9093484419263456, correct 321/353\n",
      "\n",
      "Val Set, val_loss: 0.6177, val_error: 0.2250, auc: 0.8598\n",
      "class 0: acc 0.7959183673469388, correct 39/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 12 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0445, label: 0, bag_size: 17546\n",
      "batch 39, loss: 0.1247, label: 1, bag_size: 12948\n",
      "batch 59, loss: 0.0010, label: 0, bag_size: 4548\n",
      "batch 79, loss: 0.1484, label: 1, bag_size: 5665\n",
      "batch 99, loss: 0.6177, label: 1, bag_size: 3402\n",
      "batch 119, loss: 0.0016, label: 1, bag_size: 10757\n",
      "batch 139, loss: 0.2687, label: 0, bag_size: 16737\n",
      "batch 159, loss: 2.0183, label: 1, bag_size: 2356\n",
      "batch 179, loss: 0.2699, label: 1, bag_size: 3134\n",
      "batch 199, loss: 0.0005, label: 1, bag_size: 5507\n",
      "batch 219, loss: 0.0448, label: 1, bag_size: 16538\n",
      "batch 239, loss: 0.0037, label: 1, bag_size: 5677\n",
      "batch 259, loss: 0.0144, label: 1, bag_size: 1587\n",
      "batch 279, loss: 5.6846, label: 1, bag_size: 6841\n",
      "batch 299, loss: 0.0283, label: 1, bag_size: 2267\n",
      "batch 319, loss: 0.3444, label: 1, bag_size: 13226\n",
      "batch 339, loss: 0.0242, label: 1, bag_size: 4007\n",
      "batch 359, loss: 0.4896, label: 1, bag_size: 4795\n",
      "batch 379, loss: 0.0017, label: 1, bag_size: 6760\n",
      "batch 399, loss: 0.0036, label: 0, bag_size: 5455\n",
      "batch 419, loss: 0.0728, label: 0, bag_size: 25641\n",
      "batch 439, loss: 0.1471, label: 0, bag_size: 4320\n",
      "batch 459, loss: 0.2301, label: 0, bag_size: 3950\n",
      "batch 479, loss: 0.0389, label: 1, bag_size: 18681\n",
      "batch 499, loss: 0.0152, label: 1, bag_size: 1937\n",
      "batch 519, loss: 0.0991, label: 1, bag_size: 2081\n",
      "batch 539, loss: 1.1758, label: 1, bag_size: 2036\n",
      "batch 559, loss: 0.0087, label: 0, bag_size: 15687\n",
      "batch 579, loss: 0.0008, label: 1, bag_size: 1746\n",
      "batch 599, loss: 0.3003, label: 0, bag_size: 3448\n",
      "batch 619, loss: 0.0584, label: 0, bag_size: 5001\n",
      "batch 639, loss: 0.3246, label: 1, bag_size: 2899\n",
      "batch 659, loss: 0.0059, label: 1, bag_size: 6500\n",
      "batch 679, loss: 0.2550, label: 0, bag_size: 5967\n",
      "batch 699, loss: 0.0144, label: 1, bag_size: 3777\n",
      "Epoch: 62, train_loss: 0.2571, train_error: 0.1106\n",
      "class 0: acc 0.9002770083102493, correct 325/361\n",
      "class 1: acc 0.877906976744186, correct 302/344\n",
      "\n",
      "Val Set, val_loss: 0.4423, val_error: 0.2375, auc: 0.8808\n",
      "class 0: acc 0.7755102040816326, correct 38/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 13 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3568, label: 1, bag_size: 4802\n",
      "batch 39, loss: 0.0063, label: 1, bag_size: 4803\n",
      "batch 59, loss: 0.2302, label: 0, bag_size: 15060\n",
      "batch 79, loss: 0.0127, label: 1, bag_size: 1316\n",
      "batch 99, loss: 0.0114, label: 1, bag_size: 4243\n",
      "batch 119, loss: 0.7460, label: 0, bag_size: 4360\n",
      "batch 139, loss: 0.0024, label: 1, bag_size: 6500\n",
      "batch 159, loss: 0.5901, label: 1, bag_size: 2036\n",
      "batch 179, loss: 0.0074, label: 1, bag_size: 4007\n",
      "batch 199, loss: 0.2670, label: 1, bag_size: 3990\n",
      "batch 219, loss: 0.0013, label: 0, bag_size: 4548\n",
      "batch 239, loss: 0.0008, label: 0, bag_size: 7978\n",
      "batch 259, loss: 0.1574, label: 1, bag_size: 3287\n",
      "batch 279, loss: 0.0097, label: 1, bag_size: 5071\n",
      "batch 299, loss: 0.0138, label: 0, bag_size: 13606\n",
      "batch 319, loss: 0.0280, label: 1, bag_size: 2874\n",
      "batch 339, loss: 0.3299, label: 1, bag_size: 5068\n",
      "batch 359, loss: 1.0132, label: 0, bag_size: 4560\n",
      "batch 379, loss: 0.0087, label: 1, bag_size: 4950\n",
      "batch 399, loss: 0.5420, label: 1, bag_size: 5341\n",
      "batch 419, loss: 0.0064, label: 0, bag_size: 3895\n",
      "batch 439, loss: 0.0097, label: 0, bag_size: 5533\n",
      "batch 459, loss: 0.1985, label: 1, bag_size: 15118\n",
      "batch 479, loss: 0.0965, label: 0, bag_size: 4500\n",
      "batch 499, loss: 0.1232, label: 1, bag_size: 3226\n",
      "batch 519, loss: 0.0294, label: 0, bag_size: 5021\n",
      "batch 539, loss: 0.1672, label: 1, bag_size: 1272\n",
      "batch 559, loss: 0.0052, label: 1, bag_size: 3670\n",
      "batch 579, loss: 0.7402, label: 1, bag_size: 2783\n",
      "batch 599, loss: 0.0084, label: 1, bag_size: 1627\n",
      "batch 619, loss: 0.1804, label: 1, bag_size: 12948\n",
      "batch 639, loss: 0.1864, label: 0, bag_size: 20045\n",
      "batch 659, loss: 0.0087, label: 0, bag_size: 2586\n",
      "batch 679, loss: 0.0138, label: 1, bag_size: 6138\n",
      "batch 699, loss: 0.0016, label: 1, bag_size: 3117\n",
      "Epoch: 63, train_loss: 0.2630, train_error: 0.1262\n",
      "class 0: acc 0.8760563380281691, correct 311/355\n",
      "class 1: acc 0.8714285714285714, correct 305/350\n",
      "\n",
      "Val Set, val_loss: 0.5118, val_error: 0.2125, auc: 0.8861\n",
      "class 0: acc 0.8571428571428571, correct 42/49\n",
      "class 1: acc 0.6774193548387096, correct 21/31\n",
      "EarlyStopping counter: 14 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0007, label: 0, bag_size: 938\n",
      "batch 39, loss: 0.5249, label: 1, bag_size: 3159\n",
      "batch 59, loss: 0.0020, label: 1, bag_size: 20435\n",
      "batch 79, loss: 0.0137, label: 1, bag_size: 16936\n",
      "batch 99, loss: 0.0449, label: 1, bag_size: 1808\n",
      "batch 119, loss: 0.0164, label: 0, bag_size: 3574\n",
      "batch 139, loss: 0.0068, label: 0, bag_size: 16585\n",
      "batch 159, loss: 0.0947, label: 1, bag_size: 8007\n",
      "batch 179, loss: 1.2214, label: 1, bag_size: 6392\n",
      "batch 199, loss: 0.0288, label: 0, bag_size: 1794\n",
      "batch 219, loss: 0.2212, label: 0, bag_size: 3240\n",
      "batch 239, loss: 0.1753, label: 1, bag_size: 1374\n",
      "batch 259, loss: 0.0135, label: 1, bag_size: 7085\n",
      "batch 279, loss: 0.1590, label: 1, bag_size: 2943\n",
      "batch 299, loss: 0.4466, label: 0, bag_size: 972\n",
      "batch 319, loss: 0.0032, label: 1, bag_size: 5458\n",
      "batch 339, loss: 0.0481, label: 1, bag_size: 5577\n",
      "batch 359, loss: 0.0332, label: 1, bag_size: 5199\n",
      "batch 379, loss: 0.1103, label: 1, bag_size: 11555\n",
      "batch 399, loss: 0.0874, label: 0, bag_size: 10590\n",
      "batch 419, loss: 0.0009, label: 1, bag_size: 3117\n",
      "batch 439, loss: 0.0001, label: 1, bag_size: 23777\n",
      "batch 459, loss: 0.0531, label: 1, bag_size: 5539\n",
      "batch 479, loss: 0.0049, label: 0, bag_size: 6605\n",
      "batch 499, loss: 1.4763, label: 0, bag_size: 11256\n",
      "batch 519, loss: 0.0374, label: 1, bag_size: 1178\n",
      "batch 539, loss: 0.0065, label: 0, bag_size: 5455\n",
      "batch 559, loss: 0.0034, label: 1, bag_size: 2961\n",
      "batch 579, loss: 0.0334, label: 0, bag_size: 5499\n",
      "batch 599, loss: 0.5054, label: 0, bag_size: 3830\n",
      "batch 619, loss: 0.0602, label: 1, bag_size: 3672\n",
      "batch 639, loss: 0.1253, label: 0, bag_size: 1943\n",
      "batch 659, loss: 0.3383, label: 1, bag_size: 3962\n",
      "batch 679, loss: 0.1188, label: 1, bag_size: 3287\n",
      "batch 699, loss: 0.0152, label: 0, bag_size: 1034\n",
      "Epoch: 64, train_loss: 0.2650, train_error: 0.1035\n",
      "class 0: acc 0.9025787965616046, correct 315/349\n",
      "class 1: acc 0.8904494382022472, correct 317/356\n",
      "\n",
      "Val Set, val_loss: 0.5090, val_error: 0.2000, auc: 0.8762\n",
      "class 0: acc 0.8367346938775511, correct 41/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 15 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0452, label: 1, bag_size: 2938\n",
      "batch 39, loss: 0.0044, label: 0, bag_size: 7285\n",
      "batch 59, loss: 0.0081, label: 1, bag_size: 4698\n",
      "batch 79, loss: 0.0216, label: 1, bag_size: 13348\n",
      "batch 99, loss: 0.0470, label: 0, bag_size: 5121\n",
      "batch 119, loss: 0.5394, label: 1, bag_size: 3626\n",
      "batch 139, loss: 0.0043, label: 1, bag_size: 18681\n",
      "batch 159, loss: 0.0007, label: 0, bag_size: 1892\n",
      "batch 179, loss: 0.0411, label: 0, bag_size: 2146\n",
      "batch 199, loss: 0.0039, label: 0, bag_size: 3553\n",
      "batch 219, loss: 0.0222, label: 1, bag_size: 1961\n",
      "batch 239, loss: 0.0025, label: 1, bag_size: 8331\n",
      "batch 259, loss: 0.1472, label: 1, bag_size: 11513\n",
      "batch 279, loss: 0.2425, label: 1, bag_size: 19581\n",
      "batch 299, loss: 0.0066, label: 0, bag_size: 5642\n",
      "batch 319, loss: 0.0045, label: 1, bag_size: 8075\n",
      "batch 339, loss: 1.8933, label: 0, bag_size: 4155\n",
      "batch 359, loss: 0.0044, label: 1, bag_size: 16451\n",
      "batch 379, loss: 0.0677, label: 0, bag_size: 18810\n",
      "batch 399, loss: 2.4958, label: 1, bag_size: 19307\n",
      "batch 419, loss: 0.0206, label: 0, bag_size: 5121\n",
      "batch 439, loss: 0.1045, label: 0, bag_size: 8384\n",
      "batch 459, loss: 0.4482, label: 1, bag_size: 13217\n",
      "batch 479, loss: 1.1798, label: 0, bag_size: 4994\n",
      "batch 499, loss: 0.0058, label: 0, bag_size: 4283\n",
      "batch 519, loss: 0.1686, label: 0, bag_size: 22936\n",
      "batch 539, loss: 0.0072, label: 0, bag_size: 3612\n",
      "batch 559, loss: 0.0052, label: 1, bag_size: 6463\n",
      "batch 579, loss: 0.0133, label: 1, bag_size: 2646\n",
      "batch 599, loss: 0.0032, label: 1, bag_size: 6235\n",
      "batch 619, loss: 0.0048, label: 0, bag_size: 2923\n",
      "batch 639, loss: 0.1023, label: 0, bag_size: 16585\n",
      "batch 659, loss: 0.0634, label: 0, bag_size: 6351\n",
      "batch 679, loss: 0.0009, label: 1, bag_size: 2669\n",
      "batch 699, loss: 0.0342, label: 1, bag_size: 9755\n",
      "Epoch: 65, train_loss: 0.2399, train_error: 0.0894\n",
      "class 0: acc 0.9146005509641874, correct 332/363\n",
      "class 1: acc 0.9064327485380117, correct 310/342\n",
      "\n",
      "Val Set, val_loss: 0.5245, val_error: 0.2125, auc: 0.8795\n",
      "class 0: acc 0.8163265306122449, correct 40/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 16 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0513, label: 1, bag_size: 15213\n",
      "batch 39, loss: 0.0069, label: 0, bag_size: 2428\n",
      "batch 59, loss: 0.1176, label: 0, bag_size: 2913\n",
      "batch 79, loss: 0.1068, label: 0, bag_size: 8384\n",
      "batch 99, loss: 0.0022, label: 1, bag_size: 18015\n",
      "batch 119, loss: 0.5942, label: 1, bag_size: 6477\n",
      "batch 139, loss: 0.0024, label: 1, bag_size: 23777\n",
      "batch 159, loss: 0.0020, label: 1, bag_size: 2890\n",
      "batch 179, loss: 0.8858, label: 1, bag_size: 2036\n",
      "batch 199, loss: 0.2281, label: 0, bag_size: 4628\n",
      "batch 219, loss: 0.4969, label: 1, bag_size: 5065\n",
      "batch 239, loss: 0.0029, label: 1, bag_size: 10091\n",
      "batch 259, loss: 0.7088, label: 1, bag_size: 5065\n",
      "batch 279, loss: 0.0006, label: 0, bag_size: 5522\n",
      "batch 299, loss: 0.0019, label: 0, bag_size: 1034\n",
      "batch 319, loss: 0.0337, label: 1, bag_size: 1503\n",
      "batch 339, loss: 0.1442, label: 0, bag_size: 21069\n",
      "batch 359, loss: 0.5954, label: 1, bag_size: 1699\n",
      "batch 379, loss: 0.0093, label: 1, bag_size: 3702\n",
      "batch 399, loss: 0.7390, label: 0, bag_size: 6152\n",
      "batch 419, loss: 0.0050, label: 1, bag_size: 5199\n",
      "batch 439, loss: 0.0026, label: 1, bag_size: 16936\n",
      "batch 459, loss: 0.0082, label: 0, bag_size: 1553\n",
      "batch 479, loss: 0.0005, label: 1, bag_size: 11952\n",
      "batch 499, loss: 0.0019, label: 1, bag_size: 3733\n",
      "batch 519, loss: 0.6178, label: 0, bag_size: 5487\n",
      "batch 539, loss: 0.0411, label: 1, bag_size: 6410\n",
      "batch 559, loss: 0.3815, label: 0, bag_size: 3381\n",
      "batch 579, loss: 0.1180, label: 0, bag_size: 2655\n",
      "batch 599, loss: 0.0164, label: 0, bag_size: 4885\n",
      "batch 619, loss: 1.9951, label: 1, bag_size: 2840\n",
      "batch 639, loss: 0.3752, label: 0, bag_size: 2592\n",
      "batch 659, loss: 0.0141, label: 0, bag_size: 3768\n",
      "batch 679, loss: 1.2602, label: 0, bag_size: 4180\n",
      "batch 699, loss: 0.3587, label: 0, bag_size: 16663\n",
      "Epoch: 66, train_loss: 0.2395, train_error: 0.0936\n",
      "class 0: acc 0.9114285714285715, correct 319/350\n",
      "class 1: acc 0.9014084507042254, correct 320/355\n",
      "\n",
      "Val Set, val_loss: 0.4377, val_error: 0.1750, auc: 0.8953\n",
      "class 0: acc 0.8775510204081632, correct 43/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 17 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0609, label: 0, bag_size: 9003\n",
      "batch 39, loss: 0.0424, label: 0, bag_size: 14856\n",
      "batch 59, loss: 0.0063, label: 1, bag_size: 16451\n",
      "batch 79, loss: 0.0156, label: 0, bag_size: 5299\n",
      "batch 99, loss: 0.9535, label: 1, bag_size: 5801\n",
      "batch 119, loss: 0.0164, label: 0, bag_size: 2908\n",
      "batch 139, loss: 0.2093, label: 1, bag_size: 1479\n",
      "batch 159, loss: 0.0028, label: 1, bag_size: 5671\n",
      "batch 179, loss: 0.0134, label: 1, bag_size: 5677\n",
      "batch 199, loss: 0.0025, label: 0, bag_size: 2006\n",
      "batch 219, loss: 0.0025, label: 1, bag_size: 5507\n",
      "batch 239, loss: 0.0564, label: 0, bag_size: 4558\n",
      "batch 259, loss: 0.0030, label: 1, bag_size: 6421\n",
      "batch 279, loss: 0.0288, label: 1, bag_size: 3672\n",
      "batch 299, loss: 0.3844, label: 0, bag_size: 3448\n",
      "batch 319, loss: 0.0067, label: 0, bag_size: 3704\n",
      "batch 339, loss: 0.2073, label: 1, bag_size: 2250\n",
      "batch 359, loss: 0.0073, label: 0, bag_size: 3616\n",
      "batch 379, loss: 0.0169, label: 0, bag_size: 4124\n",
      "batch 399, loss: 0.2021, label: 0, bag_size: 4406\n",
      "batch 419, loss: 0.0277, label: 0, bag_size: 2346\n",
      "batch 439, loss: 0.0094, label: 1, bag_size: 2524\n",
      "batch 459, loss: 0.0342, label: 1, bag_size: 16936\n",
      "batch 479, loss: 0.0027, label: 1, bag_size: 4233\n",
      "batch 499, loss: 0.0339, label: 1, bag_size: 1503\n",
      "batch 519, loss: 0.0128, label: 1, bag_size: 5458\n",
      "batch 539, loss: 0.1661, label: 0, bag_size: 13573\n",
      "batch 559, loss: 0.0476, label: 0, bag_size: 2747\n",
      "batch 579, loss: 0.0261, label: 0, bag_size: 13609\n",
      "batch 599, loss: 0.0009, label: 1, bag_size: 5507\n",
      "batch 619, loss: 0.2052, label: 0, bag_size: 18810\n",
      "batch 639, loss: 0.0061, label: 1, bag_size: 7641\n",
      "batch 659, loss: 0.0025, label: 1, bag_size: 2646\n",
      "batch 679, loss: 0.0602, label: 0, bag_size: 3096\n",
      "batch 699, loss: 1.1701, label: 0, bag_size: 14662\n",
      "Epoch: 67, train_loss: 0.2614, train_error: 0.1092\n",
      "class 0: acc 0.9043715846994536, correct 331/366\n",
      "class 1: acc 0.8761061946902655, correct 297/339\n",
      "\n",
      "Val Set, val_loss: 0.5232, val_error: 0.2125, auc: 0.8743\n",
      "class 0: acc 0.8163265306122449, correct 40/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 18 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0172, label: 1, bag_size: 3208\n",
      "batch 39, loss: 0.1274, label: 1, bag_size: 3454\n",
      "batch 59, loss: 0.0261, label: 0, bag_size: 4750\n",
      "batch 79, loss: 0.0029, label: 1, bag_size: 10091\n",
      "batch 99, loss: 0.8316, label: 1, bag_size: 5801\n",
      "batch 119, loss: 0.0015, label: 1, bag_size: 5218\n",
      "batch 139, loss: 0.3773, label: 1, bag_size: 22171\n",
      "batch 159, loss: 0.2326, label: 0, bag_size: 22594\n",
      "batch 179, loss: 0.0275, label: 1, bag_size: 6500\n",
      "batch 199, loss: 0.0002, label: 0, bag_size: 972\n",
      "batch 219, loss: 0.0132, label: 0, bag_size: 4572\n",
      "batch 239, loss: 0.0005, label: 1, bag_size: 4970\n",
      "batch 259, loss: 0.4518, label: 0, bag_size: 2054\n",
      "batch 279, loss: 0.0216, label: 0, bag_size: 2205\n",
      "batch 299, loss: 0.0117, label: 1, bag_size: 2701\n",
      "batch 319, loss: 0.0049, label: 0, bag_size: 1349\n",
      "batch 339, loss: 0.0017, label: 1, bag_size: 3117\n",
      "batch 359, loss: 0.1070, label: 0, bag_size: 9742\n",
      "batch 379, loss: 0.0761, label: 0, bag_size: 3535\n",
      "batch 399, loss: 0.0589, label: 0, bag_size: 13573\n",
      "batch 419, loss: 1.1763, label: 1, bag_size: 21399\n",
      "batch 439, loss: 0.1275, label: 0, bag_size: 13411\n",
      "batch 459, loss: 0.0094, label: 1, bag_size: 14487\n",
      "batch 479, loss: 0.3532, label: 1, bag_size: 3990\n",
      "batch 499, loss: 0.0113, label: 0, bag_size: 15687\n",
      "batch 519, loss: 0.0025, label: 1, bag_size: 3402\n",
      "batch 539, loss: 0.0031, label: 1, bag_size: 16675\n",
      "batch 559, loss: 0.0059, label: 0, bag_size: 3290\n",
      "batch 579, loss: 0.0331, label: 1, bag_size: 24686\n",
      "batch 599, loss: 0.0002, label: 1, bag_size: 4731\n",
      "batch 619, loss: 0.0172, label: 0, bag_size: 6534\n",
      "batch 639, loss: 0.0007, label: 1, bag_size: 16936\n",
      "batch 659, loss: 0.1344, label: 1, bag_size: 3990\n",
      "batch 679, loss: 0.0045, label: 0, bag_size: 1943\n",
      "batch 699, loss: 0.6110, label: 0, bag_size: 6391\n",
      "Epoch: 68, train_loss: 0.2086, train_error: 0.0837\n",
      "class 0: acc 0.9297752808988764, correct 331/356\n",
      "class 1: acc 0.9025787965616046, correct 315/349\n",
      "\n",
      "Val Set, val_loss: 0.5648, val_error: 0.2250, auc: 0.8664\n",
      "class 0: acc 0.8775510204081632, correct 43/49\n",
      "class 1: acc 0.6129032258064516, correct 19/31\n",
      "EarlyStopping counter: 19 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0155, label: 0, bag_size: 9003\n",
      "batch 39, loss: 0.0033, label: 0, bag_size: 5299\n",
      "batch 59, loss: 0.0587, label: 1, bag_size: 5152\n",
      "batch 79, loss: 0.0429, label: 1, bag_size: 8021\n",
      "batch 99, loss: 0.0410, label: 0, bag_size: 10590\n",
      "batch 119, loss: 0.2112, label: 1, bag_size: 5638\n",
      "batch 139, loss: 0.0220, label: 1, bag_size: 6463\n",
      "batch 159, loss: 0.0168, label: 0, bag_size: 2586\n",
      "batch 179, loss: 0.1661, label: 0, bag_size: 1901\n",
      "batch 199, loss: 0.0147, label: 1, bag_size: 2824\n",
      "batch 219, loss: 0.0013, label: 0, bag_size: 1770\n",
      "batch 239, loss: 0.1305, label: 1, bag_size: 15141\n",
      "batch 259, loss: 0.0009, label: 1, bag_size: 5068\n",
      "batch 279, loss: 0.8738, label: 0, bag_size: 3268\n",
      "batch 299, loss: 0.0004, label: 0, bag_size: 7027\n",
      "batch 319, loss: 0.0797, label: 1, bag_size: 6841\n",
      "batch 339, loss: 0.9285, label: 0, bag_size: 3830\n",
      "batch 359, loss: 0.1811, label: 0, bag_size: 4597\n",
      "batch 379, loss: 0.6063, label: 1, bag_size: 22171\n",
      "batch 399, loss: 0.0771, label: 0, bag_size: 2983\n",
      "batch 419, loss: 0.0065, label: 1, bag_size: 7085\n",
      "batch 439, loss: 0.0289, label: 1, bag_size: 12948\n",
      "batch 459, loss: 0.5165, label: 0, bag_size: 3381\n",
      "batch 479, loss: 0.0139, label: 1, bag_size: 2890\n",
      "batch 499, loss: 0.0150, label: 1, bag_size: 6759\n",
      "batch 519, loss: 0.0142, label: 0, bag_size: 3327\n",
      "batch 539, loss: 0.0931, label: 1, bag_size: 4332\n",
      "batch 559, loss: 0.0073, label: 1, bag_size: 2754\n",
      "batch 579, loss: 0.9533, label: 0, bag_size: 3420\n",
      "batch 599, loss: 0.0160, label: 1, bag_size: 2250\n",
      "batch 619, loss: 0.8968, label: 1, bag_size: 2939\n",
      "batch 639, loss: 0.0150, label: 0, bag_size: 13606\n",
      "batch 659, loss: 0.0258, label: 1, bag_size: 1503\n",
      "batch 679, loss: 0.0927, label: 0, bag_size: 27817\n",
      "batch 699, loss: 1.5678, label: 1, bag_size: 2356\n",
      "Epoch: 69, train_loss: 0.2740, train_error: 0.1248\n",
      "class 0: acc 0.8818443804034583, correct 306/347\n",
      "class 1: acc 0.8687150837988827, correct 311/358\n",
      "\n",
      "Val Set, val_loss: 0.4289, val_error: 0.2500, auc: 0.8920\n",
      "class 0: acc 0.7755102040816326, correct 38/49\n",
      "class 1: acc 0.7096774193548387, correct 22/31\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Val error: 0.1750, ROC AUC: 0.9098\n",
      "Test error: 0.2747, ROC AUC: 0.7786\n",
      "class 0: acc 0.8305084745762712, correct 49/59\n",
      "class 1: acc 0.53125, correct 17/32\n",
      "finished!\n",
      "end script\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python main.py --drop_out --early_stopping --lr 2e-4 --k 5 --label_frac 1\\\n",
    "--exp_code nlst_100_level1_clam_sb_adam_testfold4 --weighted_sample --bag_loss ce --inst_loss svm \\\n",
    "--task task_2_tumor_subtyping --model_type clam_sb --log_data --data_root_dir /home/sci/Disk_data/Datasets/NLST/FEATURES_level1 \\\n",
    "--split_dir /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/NLST_100 --subtyping \\\n",
    "--csv_path dataset_csv/NLST_offical.csv --no_inst_cluster --k_start 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransformerMIL实验结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load Dataset\n",
      "label column: label\n",
      "label dictionary: {'LUAD': 0, 'LUSC': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 0    552\n",
      "1    324\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 194\n",
      "Slide-LVL; Number of samples registered in class 0: 552\n",
      "Patient-LVL; Number of samples registered in class 1: 117\n",
      "Slide-LVL; Number of samples registered in class 1: 324\n",
      "split_dir:  /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/NLST_100\n",
      "################# Settings ###################\n",
      "num_splits:  5\n",
      "k_start:  -1\n",
      "k_end:  -1\n",
      "task:  task_2_tumor_subtyping\n",
      "max_epochs:  200\n",
      "results_dir:  ./results\n",
      "lr:  0.0002\n",
      "experiment:  nlst_100_level0_TransformerMIL_adam_FLASH\n",
      "reg:  1e-05\n",
      "label_frac:  1.0\n",
      "bag_loss:  ce\n",
      "seed:  1\n",
      "model_type:  transmil\n",
      "model_size:  small\n",
      "use_drop_out:  True\n",
      "weighted_sample:  True\n",
      "opt:  adam\n",
      "bag_weight:  0.7\n",
      "inst_loss:  svm\n",
      "B:  8\n",
      "split_dir:  /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/NLST_100\n",
      "\n",
      "Training Fold 0!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 700 samples\n",
      "Validating on 97 samples\n",
      "Testing on 79 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Setting tau to 1.0\n",
      "Done!\n",
      "TransformerMIL_SB(\n",
      "  (instance_loss_fn): SmoothTop1SVM()\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (transformer): TransformerEncoder_PerformerAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): SelfAttention(\n",
      "            (fast_attention): FastAttention(\n",
      "              (kernel_fn): ReLU()\n",
      "            )\n",
      "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attention): Attn_Net_Gated(\n",
      "    (attention_a): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_b): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (attention_V2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (attention_U2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (attention_weights2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Total number of parameters: 8406537\n",
      "Total number of trainable parameters: 8406537\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 17.5679, instance_loss: 3.2678, weighted_loss: 13.2779, label: 0, bag_size: 6908\n",
      "batch 39, loss: 5.4091, instance_loss: 0.8271, weighted_loss: 4.0345, label: 1, bag_size: 1808\n",
      "batch 59, loss: 0.6629, instance_loss: 0.9381, weighted_loss: 0.7455, label: 1, bag_size: 5507\n",
      "batch 79, loss: 0.0348, instance_loss: 0.6726, weighted_loss: 0.2262, label: 1, bag_size: 6760\n",
      "batch 99, loss: 1.1213, instance_loss: 0.8040, weighted_loss: 1.0261, label: 1, bag_size: 2301\n",
      "batch 119, loss: 0.9550, instance_loss: 2.0889, weighted_loss: 1.2952, label: 0, bag_size: 1067\n",
      "batch 139, loss: 0.6991, instance_loss: 0.8678, weighted_loss: 0.7497, label: 0, bag_size: 11451\n",
      "batch 159, loss: 0.0451, instance_loss: 0.7071, weighted_loss: 0.2437, label: 0, bag_size: 20134\n",
      "batch 179, loss: 0.3480, instance_loss: 0.6932, weighted_loss: 0.4516, label: 0, bag_size: 17625\n",
      "batch 199, loss: 1.3984, instance_loss: 0.6865, weighted_loss: 1.1849, label: 1, bag_size: 1495\n",
      "batch 219, loss: 0.4974, instance_loss: 1.8223, weighted_loss: 0.8949, label: 1, bag_size: 3670\n",
      "batch 239, loss: 1.0637, instance_loss: 1.2029, weighted_loss: 1.1055, label: 0, bag_size: 24280\n",
      "batch 259, loss: 2.6041, instance_loss: 0.6994, weighted_loss: 2.0327, label: 1, bag_size: 6759\n",
      "batch 279, loss: 1.0707, instance_loss: 1.8805, weighted_loss: 1.3137, label: 0, bag_size: 3295\n",
      "batch 299, loss: 0.5249, instance_loss: 0.6853, weighted_loss: 0.5730, label: 1, bag_size: 19581\n",
      "batch 319, loss: 1.8140, instance_loss: 1.8025, weighted_loss: 1.8106, label: 0, bag_size: 10096\n",
      "batch 339, loss: 1.3075, instance_loss: 0.7898, weighted_loss: 1.1521, label: 1, bag_size: 22843\n",
      "batch 359, loss: 0.9944, instance_loss: 0.8196, weighted_loss: 0.9419, label: 1, bag_size: 5273\n",
      "batch 379, loss: 0.6921, instance_loss: 2.1597, weighted_loss: 1.1324, label: 1, bag_size: 2939\n",
      "batch 399, loss: 1.2334, instance_loss: 2.0348, weighted_loss: 1.4738, label: 1, bag_size: 3777\n",
      "batch 419, loss: 2.1435, instance_loss: 0.8547, weighted_loss: 1.7568, label: 1, bag_size: 4800\n",
      "batch 439, loss: 0.7658, instance_loss: 0.8885, weighted_loss: 0.8026, label: 1, bag_size: 5108\n",
      "batch 459, loss: 0.7495, instance_loss: 0.6816, weighted_loss: 0.7291, label: 1, bag_size: 6752\n",
      "batch 479, loss: 0.1051, instance_loss: 0.9724, weighted_loss: 0.3653, label: 1, bag_size: 3777\n",
      "batch 499, loss: 1.6327, instance_loss: 2.5056, weighted_loss: 1.8946, label: 1, bag_size: 20435\n",
      "batch 519, loss: 0.4332, instance_loss: 0.7943, weighted_loss: 0.5416, label: 1, bag_size: 2558\n",
      "batch 539, loss: 0.9757, instance_loss: 0.8720, weighted_loss: 0.9446, label: 1, bag_size: 2961\n",
      "batch 559, loss: 0.3695, instance_loss: 1.2947, weighted_loss: 0.6471, label: 1, bag_size: 3990\n",
      "batch 579, loss: 2.2901, instance_loss: 0.8408, weighted_loss: 1.8553, label: 1, bag_size: 9643\n",
      "batch 599, loss: 0.5142, instance_loss: 1.5232, weighted_loss: 0.8169, label: 1, bag_size: 6500\n",
      "batch 619, loss: 0.7148, instance_loss: 0.9430, weighted_loss: 0.7832, label: 0, bag_size: 3484\n",
      "batch 639, loss: 0.1237, instance_loss: 1.1465, weighted_loss: 0.4305, label: 0, bag_size: 3006\n",
      "batch 659, loss: 0.1043, instance_loss: 0.8921, weighted_loss: 0.3406, label: 1, bag_size: 19013\n",
      "batch 679, loss: 0.8607, instance_loss: 1.0368, weighted_loss: 0.9135, label: 1, bag_size: 4458\n",
      "batch 699, loss: 0.2088, instance_loss: 1.1515, weighted_loss: 0.4916, label: 1, bag_size: 3374\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.91875: correct 10290/11200\n",
      "class 1 clustering acc 0.08625: correct 483/5600\n",
      "Epoch: 0, train_loss: 1.2893, train_clustering_loss:  1.1837, train_error: 0.5100\n",
      "class 0: acc 0.4811594202898551, correct 166/345\n",
      "class 1: acc 0.49859154929577465, correct 177/355\n",
      "\n",
      "Val Set, val_loss: 1.0817, val_error: 0.6082, auc: 0.5910\n",
      "class 0 clustering acc 1.0: correct 1552/1552\n",
      "class 1 clustering acc 0.0: correct 0/776\n",
      "class 0: acc 0.0, correct 0/59\n",
      "class 1: acc 1.0, correct 38/38\n",
      "Validation loss decreased (inf --> 1.081679).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7354, instance_loss: 0.9869, weighted_loss: 0.8109, label: 1, bag_size: 19581\n",
      "batch 39, loss: 0.6596, instance_loss: 0.7689, weighted_loss: 0.6924, label: 0, bag_size: 15122\n",
      "batch 59, loss: 0.7150, instance_loss: 1.1173, weighted_loss: 0.8357, label: 1, bag_size: 2701\n",
      "batch 79, loss: 3.0580, instance_loss: 1.2377, weighted_loss: 2.5119, label: 1, bag_size: 11513\n",
      "batch 99, loss: 2.1448, instance_loss: 1.3352, weighted_loss: 1.9019, label: 1, bag_size: 3126\n",
      "batch 119, loss: 3.3199, instance_loss: 0.8219, weighted_loss: 2.5705, label: 0, bag_size: 6168\n",
      "batch 139, loss: 0.7449, instance_loss: 0.7830, weighted_loss: 0.7563, label: 0, bag_size: 6724\n",
      "batch 159, loss: 1.6146, instance_loss: 0.8559, weighted_loss: 1.3870, label: 1, bag_size: 3966\n",
      "batch 179, loss: 0.2372, instance_loss: 0.8147, weighted_loss: 0.4105, label: 1, bag_size: 1746\n",
      "batch 199, loss: 1.2282, instance_loss: 1.6492, weighted_loss: 1.3545, label: 1, bag_size: 6759\n",
      "batch 219, loss: 0.8236, instance_loss: 1.1257, weighted_loss: 0.9142, label: 0, bag_size: 1539\n",
      "batch 239, loss: 0.0143, instance_loss: 0.6949, weighted_loss: 0.2185, label: 1, bag_size: 5100\n",
      "batch 259, loss: 4.7158, instance_loss: 2.9672, weighted_loss: 4.1912, label: 1, bag_size: 6759\n",
      "batch 279, loss: 1.6442, instance_loss: 1.0892, weighted_loss: 1.4777, label: 1, bag_size: 3420\n",
      "batch 299, loss: 0.9722, instance_loss: 0.8882, weighted_loss: 0.9470, label: 0, bag_size: 14281\n",
      "batch 319, loss: 0.4659, instance_loss: 0.9299, weighted_loss: 0.6051, label: 0, bag_size: 3616\n",
      "batch 339, loss: 1.6133, instance_loss: 1.5928, weighted_loss: 1.6071, label: 0, bag_size: 21574\n",
      "batch 359, loss: 1.9977, instance_loss: 1.2487, weighted_loss: 1.7730, label: 1, bag_size: 4268\n",
      "batch 379, loss: 0.0221, instance_loss: 0.7359, weighted_loss: 0.2362, label: 0, bag_size: 6076\n",
      "batch 399, loss: 0.8876, instance_loss: 1.4048, weighted_loss: 1.0427, label: 0, bag_size: 3600\n",
      "batch 419, loss: 3.9603, instance_loss: 0.9716, weighted_loss: 3.0637, label: 0, bag_size: 1927\n",
      "batch 439, loss: 0.1683, instance_loss: 1.3963, weighted_loss: 0.5367, label: 1, bag_size: 4112\n",
      "batch 459, loss: 0.9521, instance_loss: 0.7189, weighted_loss: 0.8821, label: 0, bag_size: 1034\n",
      "batch 479, loss: 2.1296, instance_loss: 1.6363, weighted_loss: 1.9816, label: 0, bag_size: 2791\n",
      "batch 499, loss: 3.2757, instance_loss: 0.8966, weighted_loss: 2.5620, label: 0, bag_size: 3845\n",
      "batch 519, loss: 0.2664, instance_loss: 1.4277, weighted_loss: 0.6148, label: 1, bag_size: 6759\n",
      "batch 539, loss: 0.1454, instance_loss: 0.8682, weighted_loss: 0.3622, label: 1, bag_size: 2874\n",
      "batch 559, loss: 0.1077, instance_loss: 1.0254, weighted_loss: 0.3830, label: 1, bag_size: 16538\n",
      "batch 579, loss: 0.7644, instance_loss: 0.8654, weighted_loss: 0.7947, label: 0, bag_size: 3022\n",
      "batch 599, loss: 1.3277, instance_loss: 0.9106, weighted_loss: 1.2026, label: 0, bag_size: 2483\n",
      "batch 619, loss: 0.8687, instance_loss: 0.9262, weighted_loss: 0.8860, label: 0, bag_size: 4179\n",
      "batch 639, loss: 2.8870, instance_loss: 0.8628, weighted_loss: 2.2797, label: 0, bag_size: 4855\n",
      "batch 659, loss: 0.6713, instance_loss: 0.9308, weighted_loss: 0.7492, label: 1, bag_size: 6151\n",
      "batch 679, loss: 1.2991, instance_loss: 0.9030, weighted_loss: 1.1803, label: 0, bag_size: 11299\n",
      "batch 699, loss: 0.3870, instance_loss: 0.7667, weighted_loss: 0.5010, label: 0, bag_size: 3548\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9650892857142858: correct 10809/11200\n",
      "class 1 clustering acc 0.03446428571428571: correct 193/5600\n",
      "Epoch: 1, train_loss: 0.9682, train_clustering_loss:  1.1160, train_error: 0.5000\n",
      "class 0: acc 0.4941860465116279, correct 170/344\n",
      "class 1: acc 0.5056179775280899, correct 180/356\n",
      "\n",
      "Val Set, val_loss: 0.6910, val_error: 0.3918, auc: 0.5633\n",
      "class 0 clustering acc 1.0: correct 1552/1552\n",
      "class 1 clustering acc 0.0: correct 0/776\n",
      "class 0: acc 1.0, correct 59/59\n",
      "class 1: acc 0.0, correct 0/38\n",
      "Validation loss decreased (1.081679 --> 0.690952).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1260, instance_loss: 0.7823, weighted_loss: 0.3229, label: 1, bag_size: 20435\n",
      "batch 39, loss: 2.4020, instance_loss: 1.2177, weighted_loss: 2.0468, label: 0, bag_size: 5624\n",
      "batch 59, loss: 0.9941, instance_loss: 0.8856, weighted_loss: 0.9616, label: 1, bag_size: 1525\n",
      "batch 79, loss: 1.2035, instance_loss: 0.7663, weighted_loss: 1.0724, label: 1, bag_size: 15118\n",
      "batch 99, loss: 0.5829, instance_loss: 0.7882, weighted_loss: 0.6445, label: 1, bag_size: 5156\n",
      "batch 119, loss: 0.6082, instance_loss: 1.6401, weighted_loss: 0.9178, label: 0, bag_size: 3129\n",
      "batch 139, loss: 0.8771, instance_loss: 0.9732, weighted_loss: 0.9059, label: 1, bag_size: 6884\n",
      "batch 159, loss: 0.7390, instance_loss: 0.8454, weighted_loss: 0.7709, label: 0, bag_size: 763\n",
      "batch 179, loss: 0.1364, instance_loss: 1.0033, weighted_loss: 0.3965, label: 1, bag_size: 21711\n",
      "batch 199, loss: 0.3650, instance_loss: 0.7877, weighted_loss: 0.4918, label: 0, bag_size: 2238\n",
      "batch 219, loss: 0.4199, instance_loss: 1.9307, weighted_loss: 0.8731, label: 0, bag_size: 3869\n",
      "batch 239, loss: 0.4616, instance_loss: 1.0616, weighted_loss: 0.6416, label: 0, bag_size: 5334\n",
      "batch 259, loss: 0.3675, instance_loss: 1.1391, weighted_loss: 0.5990, label: 0, bag_size: 20045\n",
      "batch 279, loss: 0.7715, instance_loss: 1.4602, weighted_loss: 0.9781, label: 1, bag_size: 15213\n",
      "batch 299, loss: 1.1968, instance_loss: 1.1715, weighted_loss: 1.1892, label: 1, bag_size: 3962\n",
      "batch 319, loss: 0.3968, instance_loss: 1.3366, weighted_loss: 0.6787, label: 0, bag_size: 4427\n",
      "batch 339, loss: 1.3653, instance_loss: 1.2418, weighted_loss: 1.3282, label: 1, bag_size: 1700\n",
      "batch 359, loss: 0.5499, instance_loss: 0.6849, weighted_loss: 0.5904, label: 1, bag_size: 5671\n",
      "batch 379, loss: 0.2147, instance_loss: 0.7887, weighted_loss: 0.3869, label: 0, bag_size: 4506\n",
      "batch 399, loss: 0.9805, instance_loss: 1.2820, weighted_loss: 1.0709, label: 1, bag_size: 4377\n",
      "batch 419, loss: 0.7023, instance_loss: 0.8868, weighted_loss: 0.7576, label: 1, bag_size: 3990\n",
      "batch 439, loss: 0.2284, instance_loss: 1.1327, weighted_loss: 0.4997, label: 0, bag_size: 2923\n",
      "batch 459, loss: 0.2118, instance_loss: 1.2299, weighted_loss: 0.5172, label: 1, bag_size: 13217\n",
      "batch 479, loss: 0.7608, instance_loss: 1.0092, weighted_loss: 0.8353, label: 1, bag_size: 2890\n",
      "batch 499, loss: 0.5810, instance_loss: 0.9928, weighted_loss: 0.7045, label: 0, bag_size: 5476\n",
      "batch 519, loss: 1.2552, instance_loss: 1.1489, weighted_loss: 1.2233, label: 0, bag_size: 2013\n",
      "batch 539, loss: 1.8898, instance_loss: 0.9938, weighted_loss: 1.6210, label: 0, bag_size: 3778\n",
      "batch 559, loss: 0.2650, instance_loss: 1.2505, weighted_loss: 0.5607, label: 0, bag_size: 2268\n",
      "batch 579, loss: 1.6793, instance_loss: 1.5291, weighted_loss: 1.6343, label: 1, bag_size: 4795\n",
      "batch 599, loss: 1.5632, instance_loss: 1.8618, weighted_loss: 1.6528, label: 1, bag_size: 3847\n",
      "batch 619, loss: 2.2012, instance_loss: 0.9813, weighted_loss: 1.8352, label: 1, bag_size: 14564\n",
      "batch 639, loss: 1.0376, instance_loss: 2.0808, weighted_loss: 1.3506, label: 0, bag_size: 7179\n",
      "batch 659, loss: 1.2999, instance_loss: 0.9862, weighted_loss: 1.2058, label: 0, bag_size: 7179\n",
      "batch 679, loss: 0.3328, instance_loss: 0.6253, weighted_loss: 0.4206, label: 0, bag_size: 3521\n",
      "batch 699, loss: 0.9803, instance_loss: 0.8433, weighted_loss: 0.9392, label: 1, bag_size: 4731\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9774107142857142: correct 10947/11200\n",
      "class 1 clustering acc 0.04196428571428571: correct 235/5600\n",
      "Epoch: 2, train_loss: 0.8399, train_clustering_loss:  1.0394, train_error: 0.4814\n",
      "class 0: acc 0.5325779036827195, correct 188/353\n",
      "class 1: acc 0.5043227665706052, correct 175/347\n",
      "\n",
      "Val Set, val_loss: 0.6684, val_error: 0.3918, auc: 0.5754\n",
      "class 0 clustering acc 1.0: correct 1552/1552\n",
      "class 1 clustering acc 0.0: correct 0/776\n",
      "class 0: acc 1.0, correct 59/59\n",
      "class 1: acc 0.0, correct 0/38\n",
      "Validation loss decreased (0.690952 --> 0.668434).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2693, instance_loss: 1.3189, weighted_loss: 0.5842, label: 0, bag_size: 3727\n",
      "batch 39, loss: 0.7601, instance_loss: 1.4212, weighted_loss: 0.9585, label: 1, bag_size: 21059\n",
      "batch 59, loss: 1.9199, instance_loss: 1.0690, weighted_loss: 1.6646, label: 0, bag_size: 4835\n",
      "batch 79, loss: 0.8510, instance_loss: 1.1149, weighted_loss: 0.9302, label: 0, bag_size: 14856\n",
      "batch 99, loss: 1.3559, instance_loss: 2.0021, weighted_loss: 1.5498, label: 1, bag_size: 1961\n",
      "batch 119, loss: 0.5963, instance_loss: 0.9597, weighted_loss: 0.7053, label: 1, bag_size: 2178\n",
      "batch 139, loss: 0.1756, instance_loss: 1.1949, weighted_loss: 0.4814, label: 0, bag_size: 5527\n",
      "batch 159, loss: 0.7639, instance_loss: 0.6352, weighted_loss: 0.7253, label: 1, bag_size: 5637\n",
      "batch 179, loss: 0.8675, instance_loss: 1.2272, weighted_loss: 0.9754, label: 0, bag_size: 3553\n",
      "batch 199, loss: 1.2050, instance_loss: 2.0501, weighted_loss: 1.4585, label: 1, bag_size: 4087\n",
      "batch 219, loss: 0.3187, instance_loss: 1.1865, weighted_loss: 0.5790, label: 0, bag_size: 4179\n",
      "batch 239, loss: 0.7054, instance_loss: 0.6096, weighted_loss: 0.6767, label: 1, bag_size: 3672\n",
      "batch 259, loss: 1.1060, instance_loss: 2.0170, weighted_loss: 1.3793, label: 1, bag_size: 2126\n",
      "batch 279, loss: 0.8046, instance_loss: 0.7953, weighted_loss: 0.8018, label: 1, bag_size: 11563\n",
      "batch 299, loss: 0.2504, instance_loss: 0.7885, weighted_loss: 0.4118, label: 0, bag_size: 3727\n",
      "batch 319, loss: 1.3485, instance_loss: 1.4483, weighted_loss: 1.3785, label: 1, bag_size: 14564\n",
      "batch 339, loss: 0.3418, instance_loss: 1.1607, weighted_loss: 0.5875, label: 0, bag_size: 1976\n",
      "batch 359, loss: 1.1593, instance_loss: 1.8671, weighted_loss: 1.3716, label: 1, bag_size: 10165\n",
      "batch 379, loss: 0.9027, instance_loss: 0.7236, weighted_loss: 0.8489, label: 1, bag_size: 3626\n",
      "batch 399, loss: 1.1377, instance_loss: 1.2208, weighted_loss: 1.1627, label: 0, bag_size: 4332\n",
      "batch 419, loss: 1.3832, instance_loss: 0.6990, weighted_loss: 1.1779, label: 0, bag_size: 5493\n",
      "batch 439, loss: 1.6706, instance_loss: 1.1365, weighted_loss: 1.5104, label: 0, bag_size: 3521\n",
      "batch 459, loss: 1.0107, instance_loss: 1.2966, weighted_loss: 1.0965, label: 0, bag_size: 5331\n",
      "batch 479, loss: 1.3685, instance_loss: 1.1863, weighted_loss: 1.3139, label: 0, bag_size: 3843\n",
      "batch 499, loss: 0.9263, instance_loss: 1.6646, weighted_loss: 1.1478, label: 0, bag_size: 12784\n",
      "batch 519, loss: 0.1844, instance_loss: 1.2142, weighted_loss: 0.4934, label: 0, bag_size: 18807\n",
      "batch 539, loss: 1.0532, instance_loss: 0.9201, weighted_loss: 1.0133, label: 1, bag_size: 1958\n",
      "batch 559, loss: 0.4335, instance_loss: 0.6770, weighted_loss: 0.5065, label: 0, bag_size: 2006\n",
      "batch 579, loss: 1.2168, instance_loss: 0.9819, weighted_loss: 1.1463, label: 1, bag_size: 5935\n",
      "batch 599, loss: 1.0518, instance_loss: 0.6727, weighted_loss: 0.9381, label: 0, bag_size: 3105\n",
      "batch 619, loss: 1.1587, instance_loss: 0.8077, weighted_loss: 1.0534, label: 1, bag_size: 3626\n",
      "batch 639, loss: 0.3322, instance_loss: 1.0109, weighted_loss: 0.5358, label: 0, bag_size: 4287\n",
      "batch 659, loss: 0.8602, instance_loss: 0.8352, weighted_loss: 0.8527, label: 0, bag_size: 3268\n",
      "batch 679, loss: 0.0900, instance_loss: 1.2042, weighted_loss: 0.4243, label: 1, bag_size: 4510\n",
      "batch 699, loss: 0.8360, instance_loss: 0.8304, weighted_loss: 0.8343, label: 1, bag_size: 2877\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9692857142857143: correct 10856/11200\n",
      "class 1 clustering acc 0.04375: correct 245/5600\n",
      "Epoch: 3, train_loss: 0.8148, train_clustering_loss:  1.0288, train_error: 0.4843\n",
      "class 0: acc 0.5170454545454546, correct 182/352\n",
      "class 1: acc 0.514367816091954, correct 179/348\n",
      "\n",
      "Val Set, val_loss: 0.6629, val_error: 0.3918, auc: 0.6044\n",
      "class 0 clustering acc 1.0: correct 1552/1552\n",
      "class 1 clustering acc 0.0: correct 0/776\n",
      "class 0: acc 1.0, correct 59/59\n",
      "class 1: acc 0.0, correct 0/38\n",
      "Validation loss decreased (0.668434 --> 0.662949).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2292, instance_loss: 0.7770, weighted_loss: 0.3936, label: 1, bag_size: 10165\n",
      "batch 39, loss: 0.7765, instance_loss: 0.7333, weighted_loss: 0.7636, label: 0, bag_size: 7493\n",
      "batch 59, loss: 1.0539, instance_loss: 1.2233, weighted_loss: 1.1047, label: 1, bag_size: 3733\n",
      "batch 79, loss: 1.1570, instance_loss: 0.6705, weighted_loss: 1.0110, label: 1, bag_size: 5695\n",
      "batch 99, loss: 0.7014, instance_loss: 1.2695, weighted_loss: 0.8718, label: 0, bag_size: 1053\n",
      "batch 119, loss: 1.0499, instance_loss: 1.0323, weighted_loss: 1.0446, label: 0, bag_size: 4253\n",
      "batch 139, loss: 0.2198, instance_loss: 1.3316, weighted_loss: 0.5533, label: 1, bag_size: 5637\n",
      "batch 159, loss: 0.7392, instance_loss: 1.1419, weighted_loss: 0.8600, label: 0, bag_size: 5522\n",
      "batch 179, loss: 1.1154, instance_loss: 0.7960, weighted_loss: 1.0196, label: 0, bag_size: 3277\n",
      "batch 199, loss: 1.6073, instance_loss: 0.8322, weighted_loss: 1.3748, label: 1, bag_size: 3374\n",
      "batch 219, loss: 1.0015, instance_loss: 0.7780, weighted_loss: 0.9344, label: 1, bag_size: 1374\n",
      "batch 239, loss: 1.2312, instance_loss: 0.7396, weighted_loss: 1.0837, label: 1, bag_size: 3966\n",
      "batch 259, loss: 0.7490, instance_loss: 1.1401, weighted_loss: 0.8663, label: 0, bag_size: 2678\n",
      "batch 279, loss: 0.7885, instance_loss: 1.2570, weighted_loss: 0.9291, label: 1, bag_size: 15213\n",
      "batch 299, loss: 0.9710, instance_loss: 0.8275, weighted_loss: 0.9279, label: 1, bag_size: 1699\n",
      "batch 319, loss: 0.4846, instance_loss: 0.7363, weighted_loss: 0.5601, label: 0, bag_size: 1671\n",
      "batch 339, loss: 0.3846, instance_loss: 0.6221, weighted_loss: 0.4559, label: 1, bag_size: 11295\n",
      "batch 359, loss: 1.6466, instance_loss: 1.1560, weighted_loss: 1.4995, label: 0, bag_size: 5458\n",
      "batch 379, loss: 0.2933, instance_loss: 0.9909, weighted_loss: 0.5026, label: 0, bag_size: 1207\n",
      "batch 399, loss: 0.5098, instance_loss: 0.8909, weighted_loss: 0.6241, label: 1, bag_size: 21105\n",
      "batch 419, loss: 0.5027, instance_loss: 0.9182, weighted_loss: 0.6273, label: 0, bag_size: 18060\n",
      "batch 439, loss: 0.3218, instance_loss: 0.8552, weighted_loss: 0.4818, label: 1, bag_size: 3777\n",
      "batch 459, loss: 0.4491, instance_loss: 1.5135, weighted_loss: 0.7684, label: 0, bag_size: 3161\n",
      "batch 479, loss: 1.4803, instance_loss: 0.7056, weighted_loss: 1.2479, label: 0, bag_size: 5369\n",
      "batch 499, loss: 2.3374, instance_loss: 1.3828, weighted_loss: 2.0510, label: 0, bag_size: 5624\n",
      "batch 519, loss: 2.2367, instance_loss: 1.0354, weighted_loss: 1.8763, label: 1, bag_size: 3159\n",
      "batch 539, loss: 0.8343, instance_loss: 1.0200, weighted_loss: 0.8900, label: 1, bag_size: 9643\n",
      "batch 559, loss: 0.1695, instance_loss: 0.5810, weighted_loss: 0.2930, label: 1, bag_size: 15213\n",
      "batch 579, loss: 0.2522, instance_loss: 0.6874, weighted_loss: 0.3827, label: 0, bag_size: 28144\n",
      "batch 599, loss: 0.5193, instance_loss: 1.1555, weighted_loss: 0.7101, label: 1, bag_size: 4572\n",
      "batch 619, loss: 0.8971, instance_loss: 1.3408, weighted_loss: 1.0302, label: 0, bag_size: 5924\n",
      "batch 639, loss: 0.4962, instance_loss: 0.9256, weighted_loss: 0.6250, label: 1, bag_size: 4377\n",
      "batch 659, loss: 0.6197, instance_loss: 0.8356, weighted_loss: 0.6845, label: 1, bag_size: 5062\n",
      "batch 679, loss: 0.7707, instance_loss: 1.0839, weighted_loss: 0.8647, label: 1, bag_size: 5108\n",
      "batch 699, loss: 1.5877, instance_loss: 1.1273, weighted_loss: 1.4496, label: 1, bag_size: 1699\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9872321428571429: correct 11057/11200\n",
      "class 1 clustering acc 0.027142857142857142: correct 152/5600\n",
      "Epoch: 4, train_loss: 0.8020, train_clustering_loss:  0.9972, train_error: 0.4971\n",
      "class 0: acc 0.4501510574018127, correct 149/331\n",
      "class 1: acc 0.5501355013550135, correct 203/369\n",
      "\n",
      "Val Set, val_loss: 0.7316, val_error: 0.3918, auc: 0.5812\n",
      "class 0 clustering acc 1.0: correct 1552/1552\n",
      "class 1 clustering acc 0.0: correct 0/776\n",
      "class 0: acc 1.0, correct 59/59\n",
      "class 1: acc 0.0, correct 0/38\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2368, instance_loss: 0.8318, weighted_loss: 0.4153, label: 0, bag_size: 5369\n",
      "batch 39, loss: 0.1719, instance_loss: 0.9713, weighted_loss: 0.4117, label: 1, bag_size: 15407\n",
      "batch 59, loss: 0.3042, instance_loss: 0.9483, weighted_loss: 0.4974, label: 1, bag_size: 11657\n",
      "batch 79, loss: 0.4804, instance_loss: 0.7737, weighted_loss: 0.5684, label: 0, bag_size: 21335\n",
      "batch 99, loss: 0.7669, instance_loss: 0.7926, weighted_loss: 0.7746, label: 1, bag_size: 3402\n",
      "batch 119, loss: 0.7987, instance_loss: 1.0363, weighted_loss: 0.8700, label: 0, bag_size: 14168\n",
      "batch 139, loss: 0.8514, instance_loss: 1.0121, weighted_loss: 0.8996, label: 1, bag_size: 15434\n",
      "batch 159, loss: 1.5828, instance_loss: 0.7660, weighted_loss: 1.3378, label: 1, bag_size: 7641\n",
      "batch 179, loss: 0.3853, instance_loss: 0.7128, weighted_loss: 0.4836, label: 1, bag_size: 2039\n",
      "batch 199, loss: 0.6662, instance_loss: 1.2361, weighted_loss: 0.8372, label: 0, bag_size: 3099\n",
      "batch 219, loss: 1.2150, instance_loss: 1.0595, weighted_loss: 1.1684, label: 0, bag_size: 3450\n",
      "batch 239, loss: 1.0703, instance_loss: 1.3740, weighted_loss: 1.1614, label: 1, bag_size: 16936\n",
      "batch 259, loss: 0.6154, instance_loss: 1.2011, weighted_loss: 0.7911, label: 0, bag_size: 3011\n",
      "batch 279, loss: 0.6806, instance_loss: 1.0101, weighted_loss: 0.7794, label: 1, bag_size: 21473\n",
      "batch 299, loss: 0.2891, instance_loss: 1.1255, weighted_loss: 0.5400, label: 1, bag_size: 5507\n",
      "batch 319, loss: 0.7039, instance_loss: 1.0548, weighted_loss: 0.8092, label: 0, bag_size: 3256\n",
      "batch 339, loss: 1.1493, instance_loss: 0.9537, weighted_loss: 1.0906, label: 1, bag_size: 2493\n",
      "batch 359, loss: 1.0662, instance_loss: 1.4648, weighted_loss: 1.1858, label: 1, bag_size: 5156\n",
      "batch 379, loss: 0.6299, instance_loss: 1.0720, weighted_loss: 0.7625, label: 1, bag_size: 1178\n",
      "batch 399, loss: 0.5299, instance_loss: 1.0035, weighted_loss: 0.6720, label: 1, bag_size: 4069\n",
      "batch 419, loss: 1.6178, instance_loss: 0.8285, weighted_loss: 1.3810, label: 1, bag_size: 3191\n",
      "batch 439, loss: 0.1242, instance_loss: 0.7790, weighted_loss: 0.3206, label: 1, bag_size: 6600\n",
      "batch 459, loss: 2.0848, instance_loss: 1.1355, weighted_loss: 1.8000, label: 0, bag_size: 2069\n",
      "batch 479, loss: 0.7636, instance_loss: 0.9275, weighted_loss: 0.8128, label: 0, bag_size: 4441\n",
      "batch 499, loss: 0.3728, instance_loss: 0.9586, weighted_loss: 0.5486, label: 1, bag_size: 5100\n",
      "batch 519, loss: 1.4935, instance_loss: 0.9205, weighted_loss: 1.3216, label: 1, bag_size: 3405\n",
      "batch 539, loss: 0.8943, instance_loss: 0.7868, weighted_loss: 0.8620, label: 0, bag_size: 4922\n",
      "batch 559, loss: 1.1585, instance_loss: 1.1879, weighted_loss: 1.1673, label: 1, bag_size: 4761\n",
      "batch 579, loss: 0.4863, instance_loss: 0.8309, weighted_loss: 0.5897, label: 1, bag_size: 2126\n",
      "batch 599, loss: 0.4292, instance_loss: 1.0791, weighted_loss: 0.6242, label: 1, bag_size: 3670\n",
      "batch 619, loss: 1.0779, instance_loss: 1.1212, weighted_loss: 1.0909, label: 1, bag_size: 1178\n",
      "batch 639, loss: 0.4646, instance_loss: 0.7816, weighted_loss: 0.5597, label: 1, bag_size: 10952\n",
      "batch 659, loss: 0.2221, instance_loss: 0.7050, weighted_loss: 0.3669, label: 0, bag_size: 2856\n",
      "batch 679, loss: 0.9613, instance_loss: 1.0288, weighted_loss: 0.9815, label: 1, bag_size: 1004\n",
      "batch 699, loss: 0.6458, instance_loss: 1.1842, weighted_loss: 0.8073, label: 1, bag_size: 3518\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.995625: correct 11151/11200\n",
      "class 1 clustering acc 0.008035714285714285: correct 45/5600\n",
      "Epoch: 5, train_loss: 0.7879, train_clustering_loss:  0.9873, train_error: 0.5129\n",
      "class 0: acc 0.5391061452513967, correct 193/358\n",
      "class 1: acc 0.4327485380116959, correct 148/342\n",
      "\n",
      "Val Set, val_loss: 0.7193, val_error: 0.5979, auc: 0.5955\n",
      "class 0 clustering acc 1.0: correct 1552/1552\n",
      "class 1 clustering acc 0.0: correct 0/776\n",
      "class 0: acc 0.01694915254237288, correct 1/59\n",
      "class 1: acc 1.0, correct 38/38\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.9231, instance_loss: 0.8328, weighted_loss: 0.8960, label: 0, bag_size: 1901\n",
      "batch 39, loss: 0.6313, instance_loss: 1.0270, weighted_loss: 0.7500, label: 1, bag_size: 4791\n",
      "batch 59, loss: 1.1197, instance_loss: 1.0887, weighted_loss: 1.1104, label: 0, bag_size: 25641\n",
      "batch 79, loss: 0.4188, instance_loss: 0.4981, weighted_loss: 0.4426, label: 1, bag_size: 4387\n",
      "batch 99, loss: 0.6078, instance_loss: 0.9099, weighted_loss: 0.6984, label: 1, bag_size: 4159\n",
      "batch 119, loss: 0.3442, instance_loss: 1.1352, weighted_loss: 0.5815, label: 0, bag_size: 5924\n",
      "batch 139, loss: 0.4588, instance_loss: 0.9362, weighted_loss: 0.6020, label: 1, bag_size: 20149\n",
      "batch 159, loss: 0.5147, instance_loss: 0.8546, weighted_loss: 0.6167, label: 0, bag_size: 5924\n",
      "batch 179, loss: 1.2083, instance_loss: 0.8575, weighted_loss: 1.1031, label: 1, bag_size: 15434\n",
      "batch 199, loss: 1.7253, instance_loss: 1.3448, weighted_loss: 1.6112, label: 1, bag_size: 3226\n",
      "batch 219, loss: 0.3509, instance_loss: 1.0007, weighted_loss: 0.5459, label: 0, bag_size: 5487\n",
      "batch 239, loss: 0.4288, instance_loss: 1.3727, weighted_loss: 0.7120, label: 1, bag_size: 1552\n",
      "batch 259, loss: 0.8034, instance_loss: 1.0760, weighted_loss: 0.8852, label: 1, bag_size: 4069\n",
      "batch 279, loss: 0.6849, instance_loss: 0.7479, weighted_loss: 0.7038, label: 1, bag_size: 11657\n",
      "batch 299, loss: 0.1440, instance_loss: 0.9635, weighted_loss: 0.3898, label: 0, bag_size: 4769\n",
      "batch 319, loss: 0.8296, instance_loss: 0.8759, weighted_loss: 0.8435, label: 0, bag_size: 3090\n",
      "batch 339, loss: 0.4093, instance_loss: 0.7906, weighted_loss: 0.5237, label: 0, bag_size: 2452\n",
      "batch 359, loss: 0.5160, instance_loss: 0.7738, weighted_loss: 0.5934, label: 0, bag_size: 4079\n",
      "batch 379, loss: 0.3569, instance_loss: 0.6585, weighted_loss: 0.4474, label: 0, bag_size: 3137\n",
      "batch 399, loss: 0.5476, instance_loss: 0.7070, weighted_loss: 0.5954, label: 1, bag_size: 3672\n",
      "batch 419, loss: 0.3869, instance_loss: 0.9019, weighted_loss: 0.5414, label: 1, bag_size: 5823\n",
      "batch 439, loss: 0.6638, instance_loss: 1.0630, weighted_loss: 0.7836, label: 0, bag_size: 3149\n",
      "batch 459, loss: 0.4876, instance_loss: 0.8486, weighted_loss: 0.5959, label: 0, bag_size: 2244\n",
      "batch 479, loss: 1.1598, instance_loss: 0.9938, weighted_loss: 1.1100, label: 0, bag_size: 4855\n",
      "batch 499, loss: 0.7106, instance_loss: 1.0619, weighted_loss: 0.8160, label: 1, bag_size: 11563\n",
      "batch 519, loss: 0.6245, instance_loss: 0.7564, weighted_loss: 0.6640, label: 1, bag_size: 7641\n",
      "batch 539, loss: 1.5541, instance_loss: 1.2843, weighted_loss: 1.4732, label: 0, bag_size: 3240\n",
      "batch 559, loss: 0.7969, instance_loss: 1.2851, weighted_loss: 0.9433, label: 1, bag_size: 4880\n",
      "batch 579, loss: 0.6272, instance_loss: 0.8243, weighted_loss: 0.6863, label: 0, bag_size: 2892\n",
      "batch 599, loss: 1.1439, instance_loss: 1.5933, weighted_loss: 1.2787, label: 0, bag_size: 1864\n",
      "batch 619, loss: 0.5576, instance_loss: 1.0479, weighted_loss: 0.7047, label: 1, bag_size: 3910\n",
      "batch 639, loss: 0.3008, instance_loss: 0.7026, weighted_loss: 0.4214, label: 0, bag_size: 1226\n",
      "batch 659, loss: 1.3058, instance_loss: 1.0845, weighted_loss: 1.2394, label: 0, bag_size: 1638\n",
      "batch 679, loss: 0.8167, instance_loss: 1.4634, weighted_loss: 1.0107, label: 0, bag_size: 2316\n",
      "batch 699, loss: 0.2902, instance_loss: 1.4340, weighted_loss: 0.6334, label: 1, bag_size: 3588\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9934821428571429: correct 11127/11200\n",
      "class 1 clustering acc 0.015892857142857143: correct 89/5600\n",
      "Epoch: 6, train_loss: 0.7557, train_clustering_loss:  0.9706, train_error: 0.4943\n",
      "class 0: acc 0.5071225071225072, correct 178/351\n",
      "class 1: acc 0.504297994269341, correct 176/349\n",
      "\n",
      "Val Set, val_loss: 0.9708, val_error: 0.6082, auc: 0.6012\n",
      "class 0 clustering acc 1.0: correct 1552/1552\n",
      "class 1 clustering acc 0.0: correct 0/776\n",
      "class 0: acc 0.0, correct 0/59\n",
      "class 1: acc 1.0, correct 38/38\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6849, instance_loss: 0.9784, weighted_loss: 0.7729, label: 1, bag_size: 22843\n",
      "batch 39, loss: 0.8515, instance_loss: 1.1888, weighted_loss: 0.9527, label: 1, bag_size: 3518\n",
      "batch 59, loss: 1.1154, instance_loss: 0.6923, weighted_loss: 0.9885, label: 1, bag_size: 5638\n",
      "batch 79, loss: 1.0055, instance_loss: 1.0279, weighted_loss: 1.0122, label: 0, bag_size: 5493\n",
      "batch 99, loss: 0.3941, instance_loss: 0.6053, weighted_loss: 0.4575, label: 1, bag_size: 2877\n",
      "batch 119, loss: 0.5995, instance_loss: 1.1441, weighted_loss: 0.7629, label: 0, bag_size: 5153\n",
      "batch 139, loss: 0.5584, instance_loss: 1.0817, weighted_loss: 0.7154, label: 0, bag_size: 2053\n",
      "batch 159, loss: 1.0289, instance_loss: 0.7267, weighted_loss: 0.9382, label: 0, bag_size: 5487\n",
      "batch 179, loss: 1.4593, instance_loss: 1.0027, weighted_loss: 1.3223, label: 1, bag_size: 3733\n",
      "batch 199, loss: 0.3802, instance_loss: 0.8781, weighted_loss: 0.5296, label: 0, bag_size: 4750\n",
      "batch 219, loss: 1.2233, instance_loss: 1.0599, weighted_loss: 1.1742, label: 0, bag_size: 5211\n",
      "batch 239, loss: 0.6238, instance_loss: 0.8868, weighted_loss: 0.7027, label: 1, bag_size: 2301\n",
      "batch 259, loss: 0.9951, instance_loss: 0.7696, weighted_loss: 0.9275, label: 1, bag_size: 2961\n",
      "batch 279, loss: 0.8742, instance_loss: 1.1802, weighted_loss: 0.9660, label: 1, bag_size: 3893\n",
      "batch 299, loss: 1.1393, instance_loss: 0.8245, weighted_loss: 1.0449, label: 1, bag_size: 2624\n",
      "batch 319, loss: 0.9575, instance_loss: 1.0134, weighted_loss: 0.9742, label: 0, bag_size: 3987\n",
      "batch 339, loss: 1.4088, instance_loss: 2.1711, weighted_loss: 1.6375, label: 1, bag_size: 1582\n",
      "batch 359, loss: 1.3077, instance_loss: 0.9112, weighted_loss: 1.1888, label: 1, bag_size: 6871\n",
      "batch 379, loss: 0.4596, instance_loss: 0.8823, weighted_loss: 0.5864, label: 0, bag_size: 10568\n",
      "batch 399, loss: 0.9122, instance_loss: 1.3109, weighted_loss: 1.0318, label: 1, bag_size: 1495\n",
      "batch 419, loss: 3.3924, instance_loss: 0.7558, weighted_loss: 2.6014, label: 1, bag_size: 5819\n",
      "batch 439, loss: 0.7381, instance_loss: 1.1811, weighted_loss: 0.8710, label: 0, bag_size: 6491\n",
      "batch 459, loss: 0.3811, instance_loss: 1.4332, weighted_loss: 0.6967, label: 1, bag_size: 1525\n",
      "batch 479, loss: 0.7585, instance_loss: 1.4267, weighted_loss: 0.9590, label: 0, bag_size: 1648\n",
      "batch 499, loss: 0.3472, instance_loss: 1.0219, weighted_loss: 0.5496, label: 1, bag_size: 5156\n",
      "batch 519, loss: 0.5014, instance_loss: 1.7898, weighted_loss: 0.8879, label: 1, bag_size: 1587\n",
      "batch 539, loss: 0.9609, instance_loss: 0.8738, weighted_loss: 0.9348, label: 0, bag_size: 2205\n",
      "batch 559, loss: 0.6329, instance_loss: 1.1944, weighted_loss: 0.8014, label: 1, bag_size: 4249\n",
      "batch 579, loss: 1.1219, instance_loss: 0.8724, weighted_loss: 1.0471, label: 1, bag_size: 5801\n",
      "batch 599, loss: 0.5042, instance_loss: 1.1779, weighted_loss: 0.7063, label: 1, bag_size: 20149\n",
      "batch 619, loss: 2.0821, instance_loss: 1.2365, weighted_loss: 1.8284, label: 1, bag_size: 13217\n",
      "batch 639, loss: 1.1035, instance_loss: 1.2831, weighted_loss: 1.1574, label: 1, bag_size: 2293\n",
      "batch 659, loss: 1.0552, instance_loss: 0.9566, weighted_loss: 1.0256, label: 1, bag_size: 5428\n",
      "batch 679, loss: 0.1930, instance_loss: 0.5488, weighted_loss: 0.2997, label: 0, bag_size: 4146\n",
      "batch 699, loss: 0.6242, instance_loss: 0.9568, weighted_loss: 0.7240, label: 1, bag_size: 5062\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9877678571428572: correct 11063/11200\n",
      "class 1 clustering acc 0.02125: correct 119/5600\n",
      "Epoch: 7, train_loss: 0.7688, train_clustering_loss:  0.9973, train_error: 0.4900\n",
      "class 0: acc 0.56, correct 210/375\n",
      "class 1: acc 0.4523076923076923, correct 147/325\n",
      "\n",
      "Val Set, val_loss: 0.7302, val_error: 0.6082, auc: 0.4389\n",
      "class 0 clustering acc 1.0: correct 1552/1552\n",
      "class 1 clustering acc 0.0: correct 0/776\n",
      "class 0: acc 0.0, correct 0/59\n",
      "class 1: acc 1.0, correct 38/38\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6525, instance_loss: 0.6761, weighted_loss: 0.6596, label: 1, bag_size: 6477\n",
      "batch 39, loss: 0.1776, instance_loss: 0.8334, weighted_loss: 0.3743, label: 0, bag_size: 19443\n",
      "batch 59, loss: 0.4196, instance_loss: 1.4586, weighted_loss: 0.7313, label: 1, bag_size: 18681\n",
      "batch 79, loss: 1.0322, instance_loss: 0.8131, weighted_loss: 0.9665, label: 0, bag_size: 4560\n",
      "batch 99, loss: 2.7820, instance_loss: 0.5866, weighted_loss: 2.1234, label: 1, bag_size: 2840\n",
      "batch 119, loss: 0.6860, instance_loss: 0.7902, weighted_loss: 0.7173, label: 1, bag_size: 7641\n",
      "batch 139, loss: 0.4057, instance_loss: 1.1216, weighted_loss: 0.6205, label: 0, bag_size: 1826\n",
      "batch 159, loss: 0.9326, instance_loss: 1.7693, weighted_loss: 1.1836, label: 1, bag_size: 4243\n",
      "batch 179, loss: 0.6694, instance_loss: 0.6477, weighted_loss: 0.6629, label: 1, bag_size: 12654\n",
      "batch 199, loss: 0.3314, instance_loss: 1.3493, weighted_loss: 0.6368, label: 1, bag_size: 2293\n",
      "batch 219, loss: 0.6601, instance_loss: 0.8413, weighted_loss: 0.7144, label: 1, bag_size: 3191\n",
      "batch 239, loss: 0.6814, instance_loss: 1.1348, weighted_loss: 0.8174, label: 0, bag_size: 2918\n",
      "batch 259, loss: 0.9136, instance_loss: 0.8362, weighted_loss: 0.8904, label: 0, bag_size: 5476\n",
      "batch 279, loss: 1.2192, instance_loss: 1.0869, weighted_loss: 1.1795, label: 0, bag_size: 4558\n",
      "batch 299, loss: 0.6101, instance_loss: 0.7627, weighted_loss: 0.6559, label: 0, bag_size: 13411\n",
      "batch 319, loss: 0.2972, instance_loss: 0.5937, weighted_loss: 0.3861, label: 0, bag_size: 4031\n",
      "batch 339, loss: 0.6021, instance_loss: 1.0955, weighted_loss: 0.7501, label: 1, bag_size: 20149\n",
      "batch 359, loss: 0.8617, instance_loss: 0.9353, weighted_loss: 0.8838, label: 1, bag_size: 4069\n",
      "batch 379, loss: 0.6598, instance_loss: 0.8130, weighted_loss: 0.7058, label: 1, bag_size: 3518\n",
      "batch 399, loss: 0.6477, instance_loss: 0.8616, weighted_loss: 0.7119, label: 1, bag_size: 5068\n",
      "batch 419, loss: 0.6068, instance_loss: 0.7666, weighted_loss: 0.6548, label: 1, bag_size: 2407\n",
      "batch 439, loss: 0.4978, instance_loss: 0.7771, weighted_loss: 0.5816, label: 0, bag_size: 2428\n",
      "batch 459, loss: 0.7623, instance_loss: 0.7380, weighted_loss: 0.7550, label: 0, bag_size: 12861\n",
      "batch 479, loss: 0.6850, instance_loss: 0.9550, weighted_loss: 0.7660, label: 0, bag_size: 1669\n",
      "batch 499, loss: 1.7890, instance_loss: 1.6693, weighted_loss: 1.7531, label: 1, bag_size: 1825\n",
      "batch 519, loss: 0.4526, instance_loss: 0.8294, weighted_loss: 0.5656, label: 1, bag_size: 4802\n",
      "batch 539, loss: 1.2091, instance_loss: 1.0063, weighted_loss: 1.1482, label: 1, bag_size: 5273\n",
      "batch 559, loss: 0.2597, instance_loss: 1.0345, weighted_loss: 0.4921, label: 1, bag_size: 2407\n",
      "batch 579, loss: 1.1423, instance_loss: 0.7044, weighted_loss: 1.0110, label: 1, bag_size: 15141\n",
      "batch 599, loss: 0.6266, instance_loss: 0.8017, weighted_loss: 0.6791, label: 1, bag_size: 4537\n",
      "batch 619, loss: 0.1980, instance_loss: 0.5602, weighted_loss: 0.3066, label: 0, bag_size: 5996\n",
      "batch 639, loss: 0.7038, instance_loss: 0.9571, weighted_loss: 0.7798, label: 1, bag_size: 13226\n",
      "batch 659, loss: 0.7693, instance_loss: 1.0424, weighted_loss: 0.8512, label: 0, bag_size: 12673\n",
      "batch 679, loss: 0.8592, instance_loss: 1.0187, weighted_loss: 0.9070, label: 1, bag_size: 22171\n",
      "batch 699, loss: 1.0451, instance_loss: 1.0096, weighted_loss: 1.0345, label: 1, bag_size: 20056\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9920535714285714: correct 11111/11200\n",
      "class 1 clustering acc 0.012142857142857143: correct 68/5600\n",
      "Epoch: 8, train_loss: 0.7444, train_clustering_loss:  0.9748, train_error: 0.4743\n",
      "class 0: acc 0.5494505494505495, correct 200/364\n",
      "class 1: acc 0.5, correct 168/336\n",
      "\n",
      "Val Set, val_loss: 0.6618, val_error: 0.3608, auc: 0.6075\n",
      "class 0 clustering acc 1.0: correct 1552/1552\n",
      "class 1 clustering acc 0.0: correct 0/776\n",
      "class 0: acc 0.8135593220338984, correct 48/59\n",
      "class 1: acc 0.3684210526315789, correct 14/38\n",
      "Validation loss decreased (0.662949 --> 0.661759).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4093, instance_loss: 1.0263, weighted_loss: 0.5944, label: 0, bag_size: 2452\n",
      "batch 39, loss: 0.4169, instance_loss: 1.0945, weighted_loss: 0.6202, label: 1, bag_size: 5071\n",
      "batch 59, loss: 0.8377, instance_loss: 0.9733, weighted_loss: 0.8784, label: 0, bag_size: 3987\n",
      "batch 79, loss: 0.8561, instance_loss: 0.9582, weighted_loss: 0.8867, label: 1, bag_size: 3484\n",
      "batch 99, loss: 0.9210, instance_loss: 1.5295, weighted_loss: 1.1036, label: 0, bag_size: 4076\n",
      "batch 119, loss: 1.1749, instance_loss: 1.2510, weighted_loss: 1.1978, label: 1, bag_size: 20149\n",
      "batch 139, loss: 0.7029, instance_loss: 0.9466, weighted_loss: 0.7761, label: 0, bag_size: 2316\n",
      "batch 159, loss: 1.1354, instance_loss: 0.9839, weighted_loss: 1.0899, label: 1, bag_size: 11295\n",
      "batch 179, loss: 0.4052, instance_loss: 0.7954, weighted_loss: 0.5222, label: 1, bag_size: 11563\n",
      "batch 199, loss: 0.9819, instance_loss: 1.1555, weighted_loss: 1.0340, label: 1, bag_size: 10165\n",
      "batch 219, loss: 1.0400, instance_loss: 0.6484, weighted_loss: 0.9226, label: 0, bag_size: 1553\n",
      "batch 239, loss: 0.3340, instance_loss: 0.7705, weighted_loss: 0.4649, label: 0, bag_size: 1034\n",
      "batch 259, loss: 0.7297, instance_loss: 1.1172, weighted_loss: 0.8460, label: 1, bag_size: 5385\n",
      "batch 279, loss: 1.0493, instance_loss: 1.1226, weighted_loss: 1.0713, label: 1, bag_size: 13217\n",
      "batch 299, loss: 0.7008, instance_loss: 1.0042, weighted_loss: 0.7919, label: 0, bag_size: 5121\n",
      "batch 319, loss: 0.4921, instance_loss: 0.7054, weighted_loss: 0.5561, label: 0, bag_size: 5021\n",
      "batch 339, loss: 0.5603, instance_loss: 0.7947, weighted_loss: 0.6306, label: 0, bag_size: 3843\n",
      "batch 359, loss: 0.4138, instance_loss: 0.7073, weighted_loss: 0.5018, label: 1, bag_size: 11295\n",
      "batch 379, loss: 0.8126, instance_loss: 0.5949, weighted_loss: 0.7473, label: 0, bag_size: 6524\n",
      "batch 399, loss: 0.8601, instance_loss: 1.0864, weighted_loss: 0.9280, label: 1, bag_size: 1178\n",
      "batch 419, loss: 1.1848, instance_loss: 1.1427, weighted_loss: 1.1721, label: 0, bag_size: 4098\n",
      "batch 439, loss: 0.5967, instance_loss: 0.8975, weighted_loss: 0.6869, label: 0, bag_size: 3295\n",
      "batch 459, loss: 1.1692, instance_loss: 0.6544, weighted_loss: 1.0148, label: 0, bag_size: 4587\n",
      "batch 479, loss: 1.1650, instance_loss: 1.0560, weighted_loss: 1.1323, label: 0, bag_size: 4124\n",
      "batch 499, loss: 0.8153, instance_loss: 1.1088, weighted_loss: 0.9034, label: 1, bag_size: 3672\n",
      "batch 519, loss: 0.5198, instance_loss: 1.0789, weighted_loss: 0.6875, label: 1, bag_size: 13226\n",
      "batch 539, loss: 0.6829, instance_loss: 1.0103, weighted_loss: 0.7811, label: 0, bag_size: 3084\n",
      "batch 559, loss: 0.5042, instance_loss: 0.6826, weighted_loss: 0.5577, label: 1, bag_size: 2824\n",
      "batch 579, loss: 0.3201, instance_loss: 0.6237, weighted_loss: 0.4112, label: 0, bag_size: 10536\n",
      "batch 599, loss: 0.8275, instance_loss: 0.8163, weighted_loss: 0.8241, label: 0, bag_size: 5002\n",
      "batch 619, loss: 0.8440, instance_loss: 1.0434, weighted_loss: 0.9039, label: 1, bag_size: 2646\n",
      "batch 639, loss: 1.0856, instance_loss: 1.2470, weighted_loss: 1.1340, label: 1, bag_size: 4069\n",
      "batch 659, loss: 0.4344, instance_loss: 0.8275, weighted_loss: 0.5523, label: 0, bag_size: 15060\n",
      "batch 679, loss: 0.5563, instance_loss: 0.5787, weighted_loss: 0.5630, label: 0, bag_size: 2945\n",
      "batch 699, loss: 0.4885, instance_loss: 0.7971, weighted_loss: 0.5811, label: 0, bag_size: 3732\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.991875: correct 11109/11200\n",
      "class 1 clustering acc 0.030892857142857142: correct 173/5600\n",
      "Epoch: 9, train_loss: 0.7303, train_clustering_loss:  0.9392, train_error: 0.5014\n",
      "class 0: acc 0.5356125356125356, correct 188/351\n",
      "class 1: acc 0.46131805157593125, correct 161/349\n",
      "\n",
      "Val Set, val_loss: 0.6532, val_error: 0.3505, auc: 0.6338\n",
      "class 0 clustering acc 1.0: correct 1552/1552\n",
      "class 1 clustering acc 0.0: correct 0/776\n",
      "class 0: acc 0.9152542372881356, correct 54/59\n",
      "class 1: acc 0.23684210526315788, correct 9/38\n",
      "Validation loss decreased (0.661759 --> 0.653174).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 2.1443, instance_loss: 1.0869, weighted_loss: 1.8271, label: 0, bag_size: 3521\n",
      "batch 39, loss: 0.4138, instance_loss: 0.6474, weighted_loss: 0.4838, label: 0, bag_size: 3504\n",
      "batch 59, loss: 0.6773, instance_loss: 0.9451, weighted_loss: 0.7577, label: 1, bag_size: 5638\n",
      "batch 79, loss: 0.2480, instance_loss: 1.0772, weighted_loss: 0.4968, label: 0, bag_size: 11256\n",
      "batch 99, loss: 0.7981, instance_loss: 0.8483, weighted_loss: 0.8132, label: 0, bag_size: 2316\n",
      "batch 119, loss: 0.4681, instance_loss: 0.7472, weighted_loss: 0.5518, label: 1, bag_size: 6697\n",
      "batch 139, loss: 0.2798, instance_loss: 0.7575, weighted_loss: 0.4231, label: 1, bag_size: 11563\n",
      "batch 159, loss: 0.2137, instance_loss: 0.4386, weighted_loss: 0.2812, label: 1, bag_size: 8007\n",
      "batch 179, loss: 1.2839, instance_loss: 1.0182, weighted_loss: 1.2042, label: 1, bag_size: 6752\n",
      "batch 199, loss: 0.4614, instance_loss: 1.0872, weighted_loss: 0.6491, label: 0, bag_size: 6152\n",
      "batch 219, loss: 1.0015, instance_loss: 0.9219, weighted_loss: 0.9776, label: 0, bag_size: 11302\n",
      "batch 239, loss: 0.7258, instance_loss: 0.8541, weighted_loss: 0.7643, label: 1, bag_size: 24686\n",
      "batch 259, loss: 1.3240, instance_loss: 1.3712, weighted_loss: 1.3382, label: 0, bag_size: 2013\n",
      "batch 279, loss: 0.3258, instance_loss: 0.7009, weighted_loss: 0.4383, label: 0, bag_size: 2013\n",
      "batch 299, loss: 0.5772, instance_loss: 0.7136, weighted_loss: 0.6181, label: 0, bag_size: 5642\n",
      "batch 319, loss: 0.6518, instance_loss: 0.8013, weighted_loss: 0.6967, label: 0, bag_size: 10536\n",
      "batch 339, loss: 0.5270, instance_loss: 1.0242, weighted_loss: 0.6762, label: 0, bag_size: 1892\n",
      "batch 359, loss: 0.6919, instance_loss: 1.1849, weighted_loss: 0.8398, label: 1, bag_size: 2783\n",
      "batch 379, loss: 0.5491, instance_loss: 0.8418, weighted_loss: 0.6369, label: 0, bag_size: 16582\n",
      "batch 399, loss: 0.6318, instance_loss: 0.8157, weighted_loss: 0.6870, label: 0, bag_size: 4620\n",
      "batch 419, loss: 0.5938, instance_loss: 1.0217, weighted_loss: 0.7221, label: 0, bag_size: 3661\n",
      "batch 439, loss: 0.4999, instance_loss: 0.7250, weighted_loss: 0.5674, label: 0, bag_size: 5125\n",
      "batch 459, loss: 0.3657, instance_loss: 0.6613, weighted_loss: 0.4544, label: 1, bag_size: 2005\n",
      "batch 479, loss: 0.2031, instance_loss: 0.7945, weighted_loss: 0.3805, label: 0, bag_size: 9742\n",
      "batch 499, loss: 0.4850, instance_loss: 0.6506, weighted_loss: 0.5347, label: 1, bag_size: 5218\n",
      "batch 519, loss: 0.8094, instance_loss: 1.1738, weighted_loss: 0.9187, label: 0, bag_size: 1669\n",
      "batch 539, loss: 0.4731, instance_loss: 0.7023, weighted_loss: 0.5419, label: 0, bag_size: 3857\n",
      "batch 559, loss: 0.3223, instance_loss: 0.6682, weighted_loss: 0.4260, label: 1, bag_size: 2656\n",
      "batch 579, loss: 0.3149, instance_loss: 0.5723, weighted_loss: 0.3921, label: 0, bag_size: 3895\n",
      "batch 599, loss: 1.8896, instance_loss: 1.4899, weighted_loss: 1.7697, label: 0, bag_size: 3869\n",
      "batch 619, loss: 0.9265, instance_loss: 1.2407, weighted_loss: 1.0208, label: 1, bag_size: 10736\n",
      "batch 639, loss: 1.8464, instance_loss: 1.4171, weighted_loss: 1.7176, label: 0, bag_size: 3636\n",
      "batch 659, loss: 0.5663, instance_loss: 0.9297, weighted_loss: 0.6753, label: 0, bag_size: 6351\n",
      "batch 679, loss: 0.3565, instance_loss: 0.3750, weighted_loss: 0.3621, label: 1, bag_size: 3670\n",
      "batch 699, loss: 0.8502, instance_loss: 0.9694, weighted_loss: 0.8860, label: 1, bag_size: 3990\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9876785714285714: correct 11062/11200\n",
      "class 1 clustering acc 0.03696428571428571: correct 207/5600\n",
      "Epoch: 10, train_loss: 0.7209, train_clustering_loss:  0.9468, train_error: 0.4443\n",
      "class 0: acc 0.5868945868945868, correct 206/351\n",
      "class 1: acc 0.5243553008595988, correct 183/349\n",
      "\n",
      "Val Set, val_loss: 0.6590, val_error: 0.3608, auc: 0.6409\n",
      "class 0 clustering acc 1.0: correct 1552/1552\n",
      "class 1 clustering acc 0.0: correct 0/776\n",
      "class 0: acc 0.9322033898305084, correct 55/59\n",
      "class 1: acc 0.18421052631578946, correct 7/38\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5876, instance_loss: 0.8553, weighted_loss: 0.6679, label: 1, bag_size: 4094\n",
      "batch 39, loss: 0.1624, instance_loss: 0.4028, weighted_loss: 0.2345, label: 1, bag_size: 12654\n",
      "batch 59, loss: 0.5564, instance_loss: 1.0635, weighted_loss: 0.7085, label: 0, bag_size: 22113\n",
      "batch 79, loss: 0.1259, instance_loss: 0.4767, weighted_loss: 0.2311, label: 1, bag_size: 4458\n",
      "batch 99, loss: 0.2519, instance_loss: 0.8961, weighted_loss: 0.4452, label: 1, bag_size: 19173\n",
      "batch 119, loss: 0.6242, instance_loss: 0.6927, weighted_loss: 0.6447, label: 1, bag_size: 5100\n",
      "batch 139, loss: 0.7378, instance_loss: 0.9757, weighted_loss: 0.8092, label: 0, bag_size: 2268\n",
      "batch 159, loss: 0.3498, instance_loss: 0.5917, weighted_loss: 0.4224, label: 0, bag_size: 2069\n",
      "batch 179, loss: 1.5364, instance_loss: 1.7566, weighted_loss: 1.6024, label: 0, bag_size: 12861\n",
      "batch 199, loss: 0.6411, instance_loss: 0.7445, weighted_loss: 0.6722, label: 1, bag_size: 6841\n",
      "batch 219, loss: 0.7303, instance_loss: 0.9685, weighted_loss: 0.8018, label: 1, bag_size: 18681\n",
      "batch 239, loss: 0.5751, instance_loss: 0.8425, weighted_loss: 0.6553, label: 1, bag_size: 3893\n",
      "batch 259, loss: 0.2913, instance_loss: 0.8430, weighted_loss: 0.4568, label: 0, bag_size: 3548\n",
      "batch 279, loss: 0.8853, instance_loss: 1.2902, weighted_loss: 1.0068, label: 1, bag_size: 1582\n",
      "batch 299, loss: 0.3071, instance_loss: 0.8052, weighted_loss: 0.4565, label: 0, bag_size: 3093\n",
      "batch 319, loss: 0.4536, instance_loss: 1.1051, weighted_loss: 0.6490, label: 1, bag_size: 3518\n",
      "batch 339, loss: 0.6243, instance_loss: 0.6534, weighted_loss: 0.6330, label: 1, bag_size: 6861\n",
      "batch 359, loss: 0.9683, instance_loss: 0.9080, weighted_loss: 0.9502, label: 1, bag_size: 6500\n",
      "batch 379, loss: 0.8336, instance_loss: 0.6465, weighted_loss: 0.7775, label: 0, bag_size: 2963\n",
      "batch 399, loss: 1.7304, instance_loss: 1.3429, weighted_loss: 1.6142, label: 0, bag_size: 15706\n",
      "batch 419, loss: 0.5344, instance_loss: 0.6188, weighted_loss: 0.5597, label: 1, bag_size: 2840\n",
      "batch 439, loss: 1.0630, instance_loss: 0.8916, weighted_loss: 1.0116, label: 0, bag_size: 5642\n",
      "batch 459, loss: 0.2570, instance_loss: 0.5790, weighted_loss: 0.3536, label: 1, bag_size: 7085\n",
      "batch 479, loss: 0.6047, instance_loss: 0.9693, weighted_loss: 0.7141, label: 0, bag_size: 19612\n",
      "batch 499, loss: 0.4657, instance_loss: 0.7995, weighted_loss: 0.5658, label: 0, bag_size: 3484\n",
      "batch 519, loss: 0.5334, instance_loss: 0.7315, weighted_loss: 0.5929, label: 0, bag_size: 5824\n",
      "batch 539, loss: 0.1354, instance_loss: 0.1389, weighted_loss: 0.1364, label: 1, bag_size: 21473\n",
      "batch 559, loss: 0.5244, instance_loss: 0.7128, weighted_loss: 0.5809, label: 0, bag_size: 7667\n",
      "batch 579, loss: 0.5816, instance_loss: 0.7419, weighted_loss: 0.6297, label: 0, bag_size: 5824\n",
      "batch 599, loss: 0.7686, instance_loss: 0.8673, weighted_loss: 0.7982, label: 0, bag_size: 5426\n",
      "batch 619, loss: 0.2672, instance_loss: 0.7826, weighted_loss: 0.4218, label: 0, bag_size: 16582\n",
      "batch 639, loss: 0.1745, instance_loss: 0.9096, weighted_loss: 0.3950, label: 1, bag_size: 6500\n",
      "batch 659, loss: 0.9876, instance_loss: 0.8507, weighted_loss: 0.9465, label: 1, bag_size: 5152\n",
      "batch 679, loss: 0.8025, instance_loss: 1.3218, weighted_loss: 0.9583, label: 1, bag_size: 4722\n",
      "batch 699, loss: 0.6835, instance_loss: 0.6157, weighted_loss: 0.6632, label: 1, bag_size: 2407\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9747321428571428: correct 10917/11200\n",
      "class 1 clustering acc 0.07446428571428572: correct 417/5600\n",
      "Epoch: 11, train_loss: 0.6882, train_clustering_loss:  0.9182, train_error: 0.4257\n",
      "class 0: acc 0.5443786982248521, correct 184/338\n",
      "class 1: acc 0.6022099447513812, correct 218/362\n",
      "\n",
      "Val Set, val_loss: 0.6579, val_error: 0.3505, auc: 0.6548\n",
      "class 0 clustering acc 0.9858247422680413: correct 1530/1552\n",
      "class 1 clustering acc 0.041237113402061855: correct 32/776\n",
      "class 0: acc 0.864406779661017, correct 51/59\n",
      "class 1: acc 0.3157894736842105, correct 12/38\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4505, instance_loss: 0.8507, weighted_loss: 0.5706, label: 1, bag_size: 3626\n",
      "batch 39, loss: 0.7339, instance_loss: 0.7812, weighted_loss: 0.7481, label: 1, bag_size: 3226\n",
      "batch 59, loss: 1.1663, instance_loss: 1.0696, weighted_loss: 1.1373, label: 0, bag_size: 4146\n",
      "batch 79, loss: 0.1667, instance_loss: 0.4252, weighted_loss: 0.2442, label: 0, bag_size: 7171\n",
      "batch 99, loss: 1.2346, instance_loss: 1.1780, weighted_loss: 1.2176, label: 0, bag_size: 1826\n",
      "batch 119, loss: 1.5464, instance_loss: 1.7450, weighted_loss: 1.6060, label: 0, bag_size: 1828\n",
      "batch 139, loss: 0.5738, instance_loss: 0.8399, weighted_loss: 0.6536, label: 0, bag_size: 4291\n",
      "batch 159, loss: 0.8732, instance_loss: 0.8319, weighted_loss: 0.8608, label: 0, bag_size: 5448\n",
      "batch 179, loss: 0.5593, instance_loss: 0.6080, weighted_loss: 0.5739, label: 1, bag_size: 21059\n",
      "batch 199, loss: 1.4592, instance_loss: 1.4872, weighted_loss: 1.4676, label: 1, bag_size: 13226\n",
      "batch 219, loss: 0.5277, instance_loss: 0.9056, weighted_loss: 0.6411, label: 0, bag_size: 6255\n",
      "batch 239, loss: 0.5417, instance_loss: 0.9368, weighted_loss: 0.6602, label: 1, bag_size: 5379\n",
      "batch 259, loss: 0.5700, instance_loss: 0.8619, weighted_loss: 0.6576, label: 1, bag_size: 4387\n",
      "batch 279, loss: 0.4282, instance_loss: 0.6686, weighted_loss: 0.5003, label: 0, bag_size: 11451\n",
      "batch 299, loss: 0.5580, instance_loss: 1.1571, weighted_loss: 0.7377, label: 1, bag_size: 4722\n",
      "batch 319, loss: 0.8916, instance_loss: 0.8077, weighted_loss: 0.8665, label: 1, bag_size: 5677\n",
      "batch 339, loss: 0.4336, instance_loss: 0.7751, weighted_loss: 0.5361, label: 0, bag_size: 5967\n",
      "batch 359, loss: 1.1878, instance_loss: 1.2328, weighted_loss: 1.2013, label: 1, bag_size: 2646\n",
      "batch 379, loss: 1.1097, instance_loss: 1.2715, weighted_loss: 1.1582, label: 1, bag_size: 21105\n",
      "batch 399, loss: 0.4341, instance_loss: 0.9010, weighted_loss: 0.5742, label: 0, bag_size: 2359\n",
      "batch 419, loss: 0.3626, instance_loss: 1.2331, weighted_loss: 0.6237, label: 1, bag_size: 16538\n",
      "batch 439, loss: 0.9055, instance_loss: 0.6171, weighted_loss: 0.8190, label: 1, bag_size: 3847\n",
      "batch 459, loss: 0.7479, instance_loss: 0.9720, weighted_loss: 0.8152, label: 0, bag_size: 2482\n",
      "batch 479, loss: 0.5576, instance_loss: 0.9523, weighted_loss: 0.6760, label: 1, bag_size: 1272\n",
      "batch 499, loss: 0.6972, instance_loss: 1.0973, weighted_loss: 0.8172, label: 0, bag_size: 5269\n",
      "batch 519, loss: 0.7290, instance_loss: 0.5968, weighted_loss: 0.6894, label: 0, bag_size: 5458\n",
      "batch 539, loss: 0.4506, instance_loss: 0.5330, weighted_loss: 0.4753, label: 1, bag_size: 2253\n",
      "batch 559, loss: 0.4231, instance_loss: 0.6754, weighted_loss: 0.4988, label: 1, bag_size: 3081\n",
      "batch 579, loss: 0.3293, instance_loss: 0.6194, weighted_loss: 0.4163, label: 1, bag_size: 5071\n",
      "batch 599, loss: 0.7395, instance_loss: 1.1149, weighted_loss: 0.8521, label: 0, bag_size: 4875\n",
      "batch 619, loss: 0.3378, instance_loss: 0.5847, weighted_loss: 0.4118, label: 1, bag_size: 6760\n",
      "batch 639, loss: 0.9421, instance_loss: 0.9627, weighted_loss: 0.9483, label: 0, bag_size: 15912\n",
      "batch 659, loss: 1.8034, instance_loss: 1.3962, weighted_loss: 1.6812, label: 0, bag_size: 4522\n",
      "batch 679, loss: 0.6993, instance_loss: 1.2489, weighted_loss: 0.8642, label: 0, bag_size: 3521\n",
      "batch 699, loss: 0.5745, instance_loss: 0.9335, weighted_loss: 0.6822, label: 1, bag_size: 2624\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9746428571428571: correct 10916/11200\n",
      "class 1 clustering acc 0.08803571428571429: correct 493/5600\n",
      "Epoch: 12, train_loss: 0.6797, train_clustering_loss:  0.9080, train_error: 0.4086\n",
      "class 0: acc 0.5783475783475783, correct 203/351\n",
      "class 1: acc 0.6045845272206304, correct 211/349\n",
      "\n",
      "Val Set, val_loss: 0.6548, val_error: 0.3608, auc: 0.6650\n",
      "class 0 clustering acc 1.0: correct 1552/1552\n",
      "class 1 clustering acc 0.0: correct 0/776\n",
      "class 0: acc 0.6779661016949152, correct 40/59\n",
      "class 1: acc 0.5789473684210527, correct 22/38\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.1187, instance_loss: 1.0132, weighted_loss: 1.0871, label: 1, bag_size: 2447\n",
      "batch 39, loss: 0.3102, instance_loss: 0.6089, weighted_loss: 0.3998, label: 0, bag_size: 5522\n",
      "batch 59, loss: 0.6125, instance_loss: 0.6843, weighted_loss: 0.6340, label: 0, bag_size: 4253\n",
      "batch 79, loss: 0.7524, instance_loss: 0.7732, weighted_loss: 0.7587, label: 1, bag_size: 3143\n",
      "batch 99, loss: 0.8491, instance_loss: 0.9503, weighted_loss: 0.8794, label: 1, bag_size: 3162\n",
      "batch 119, loss: 0.1810, instance_loss: 0.7785, weighted_loss: 0.3603, label: 1, bag_size: 11657\n",
      "batch 139, loss: 0.7486, instance_loss: 1.1209, weighted_loss: 0.8603, label: 1, bag_size: 4880\n",
      "batch 159, loss: 0.8576, instance_loss: 1.1802, weighted_loss: 0.9544, label: 1, bag_size: 14564\n",
      "batch 179, loss: 0.7564, instance_loss: 1.1291, weighted_loss: 0.8682, label: 0, bag_size: 17625\n",
      "batch 199, loss: 0.7475, instance_loss: 0.8811, weighted_loss: 0.7875, label: 1, bag_size: 6410\n",
      "batch 219, loss: 0.3076, instance_loss: 0.6150, weighted_loss: 0.3998, label: 1, bag_size: 4537\n",
      "batch 239, loss: 1.1317, instance_loss: 1.1710, weighted_loss: 1.1435, label: 1, bag_size: 4795\n",
      "batch 259, loss: 0.2273, instance_loss: 0.4490, weighted_loss: 0.2938, label: 1, bag_size: 7184\n",
      "batch 279, loss: 0.3736, instance_loss: 0.7169, weighted_loss: 0.4766, label: 1, bag_size: 2301\n",
      "batch 299, loss: 0.3335, instance_loss: 0.4934, weighted_loss: 0.3814, label: 1, bag_size: 19581\n",
      "batch 319, loss: 0.4361, instance_loss: 0.5955, weighted_loss: 0.4839, label: 0, bag_size: 15015\n",
      "batch 339, loss: 0.7166, instance_loss: 0.9498, weighted_loss: 0.7866, label: 0, bag_size: 3936\n",
      "batch 359, loss: 0.2383, instance_loss: 0.4858, weighted_loss: 0.3125, label: 1, bag_size: 5108\n",
      "batch 379, loss: 0.9133, instance_loss: 1.0458, weighted_loss: 0.9531, label: 1, bag_size: 5939\n",
      "batch 399, loss: 0.5205, instance_loss: 0.6128, weighted_loss: 0.5482, label: 1, bag_size: 6500\n",
      "batch 419, loss: 0.5738, instance_loss: 0.8452, weighted_loss: 0.6552, label: 0, bag_size: 9945\n",
      "batch 439, loss: 0.8483, instance_loss: 1.0142, weighted_loss: 0.8981, label: 1, bag_size: 5665\n",
      "batch 459, loss: 0.2810, instance_loss: 0.4885, weighted_loss: 0.3432, label: 1, bag_size: 5819\n",
      "batch 479, loss: 0.4346, instance_loss: 0.5801, weighted_loss: 0.4783, label: 1, bag_size: 2447\n",
      "batch 499, loss: 0.3955, instance_loss: 0.6463, weighted_loss: 0.4707, label: 0, bag_size: 5996\n",
      "batch 519, loss: 0.4325, instance_loss: 0.3961, weighted_loss: 0.4216, label: 0, bag_size: 1352\n",
      "batch 539, loss: 0.3525, instance_loss: 0.6576, weighted_loss: 0.4441, label: 1, bag_size: 1251\n",
      "batch 559, loss: 0.5226, instance_loss: 0.6846, weighted_loss: 0.5712, label: 1, bag_size: 4087\n",
      "batch 579, loss: 0.5522, instance_loss: 0.8459, weighted_loss: 0.6403, label: 1, bag_size: 1746\n",
      "batch 599, loss: 0.4983, instance_loss: 0.7384, weighted_loss: 0.5703, label: 0, bag_size: 5724\n",
      "batch 619, loss: 0.9606, instance_loss: 1.1785, weighted_loss: 1.0260, label: 1, bag_size: 6463\n",
      "batch 639, loss: 0.5056, instance_loss: 0.7993, weighted_loss: 0.5937, label: 1, bag_size: 2840\n",
      "batch 659, loss: 0.7184, instance_loss: 1.0959, weighted_loss: 0.8317, label: 1, bag_size: 3962\n",
      "batch 679, loss: 0.4907, instance_loss: 0.7905, weighted_loss: 0.5806, label: 0, bag_size: 6593\n",
      "batch 699, loss: 0.4257, instance_loss: 0.6413, weighted_loss: 0.4903, label: 0, bag_size: 4654\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9723214285714286: correct 10890/11200\n",
      "class 1 clustering acc 0.12160714285714286: correct 681/5600\n",
      "Epoch: 13, train_loss: 0.6738, train_clustering_loss:  0.8854, train_error: 0.4086\n",
      "class 0: acc 0.5752212389380531, correct 195/339\n",
      "class 1: acc 0.6066481994459834, correct 219/361\n",
      "\n",
      "Val Set, val_loss: 0.6418, val_error: 0.3402, auc: 0.6699\n",
      "class 0 clustering acc 0.9845360824742269: correct 1528/1552\n",
      "class 1 clustering acc 0.059278350515463915: correct 46/776\n",
      "class 0: acc 0.6779661016949152, correct 40/59\n",
      "class 1: acc 0.631578947368421, correct 24/38\n",
      "Validation loss decreased (0.653174 --> 0.641809).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2934, instance_loss: 0.5747, weighted_loss: 0.3778, label: 0, bag_size: 7493\n",
      "batch 39, loss: 0.5360, instance_loss: 0.7906, weighted_loss: 0.6124, label: 1, bag_size: 13685\n",
      "batch 59, loss: 0.4872, instance_loss: 0.4872, weighted_loss: 0.4872, label: 1, bag_size: 3338\n",
      "batch 79, loss: 0.7070, instance_loss: 0.9947, weighted_loss: 0.7933, label: 1, bag_size: 11563\n",
      "batch 99, loss: 0.3278, instance_loss: 0.7728, weighted_loss: 0.4613, label: 0, bag_size: 4283\n",
      "batch 119, loss: 0.6905, instance_loss: 0.6236, weighted_loss: 0.6704, label: 0, bag_size: 5824\n",
      "batch 139, loss: 0.2454, instance_loss: 0.3274, weighted_loss: 0.2700, label: 1, bag_size: 2820\n",
      "batch 159, loss: 0.7147, instance_loss: 1.1491, weighted_loss: 0.8450, label: 1, bag_size: 3338\n",
      "batch 179, loss: 0.1689, instance_loss: 0.6378, weighted_loss: 0.3095, label: 0, bag_size: 19055\n",
      "batch 199, loss: 0.1718, instance_loss: 0.4203, weighted_loss: 0.2463, label: 1, bag_size: 3208\n",
      "batch 219, loss: 0.5291, instance_loss: 0.6407, weighted_loss: 0.5626, label: 1, bag_size: 4094\n",
      "batch 239, loss: 1.7971, instance_loss: 2.2947, weighted_loss: 1.9464, label: 0, bag_size: 6534\n",
      "batch 259, loss: 1.0229, instance_loss: 1.1905, weighted_loss: 1.0732, label: 1, bag_size: 15213\n",
      "batch 279, loss: 0.8641, instance_loss: 1.1360, weighted_loss: 0.9456, label: 1, bag_size: 5801\n",
      "batch 299, loss: 1.2824, instance_loss: 1.2685, weighted_loss: 1.2782, label: 0, bag_size: 1822\n",
      "batch 319, loss: 1.6804, instance_loss: 2.0745, weighted_loss: 1.7986, label: 1, bag_size: 20149\n",
      "batch 339, loss: 0.6131, instance_loss: 0.8312, weighted_loss: 0.6785, label: 1, bag_size: 2081\n",
      "batch 359, loss: 0.8634, instance_loss: 1.0386, weighted_loss: 0.9159, label: 1, bag_size: 20435\n",
      "batch 379, loss: 2.3991, instance_loss: 2.2066, weighted_loss: 2.3414, label: 1, bag_size: 4880\n",
      "batch 399, loss: 0.2607, instance_loss: 0.2068, weighted_loss: 0.2445, label: 1, bag_size: 4795\n",
      "batch 419, loss: 0.2828, instance_loss: 0.3764, weighted_loss: 0.3109, label: 1, bag_size: 4970\n",
      "batch 439, loss: 0.6363, instance_loss: 0.8855, weighted_loss: 0.7111, label: 0, bag_size: 3327\n",
      "batch 459, loss: 1.0441, instance_loss: 1.2756, weighted_loss: 1.1136, label: 0, bag_size: 4076\n",
      "batch 479, loss: 0.1252, instance_loss: 0.1949, weighted_loss: 0.1461, label: 1, bag_size: 6600\n",
      "batch 499, loss: 0.5706, instance_loss: 0.8773, weighted_loss: 0.6626, label: 0, bag_size: 4750\n",
      "batch 519, loss: 0.3036, instance_loss: 0.3913, weighted_loss: 0.3299, label: 1, bag_size: 3966\n",
      "batch 539, loss: 0.1610, instance_loss: 0.4782, weighted_loss: 0.2562, label: 0, bag_size: 5080\n",
      "batch 559, loss: 0.3049, instance_loss: 0.4449, weighted_loss: 0.3469, label: 1, bag_size: 5777\n",
      "batch 579, loss: 0.7707, instance_loss: 0.9820, weighted_loss: 0.8341, label: 1, bag_size: 3777\n",
      "batch 599, loss: 0.4455, instance_loss: 0.7646, weighted_loss: 0.5412, label: 1, bag_size: 4970\n",
      "batch 619, loss: 0.4716, instance_loss: 0.6872, weighted_loss: 0.5362, label: 1, bag_size: 14564\n",
      "batch 639, loss: 0.5636, instance_loss: 0.8613, weighted_loss: 0.6529, label: 0, bag_size: 5370\n",
      "batch 659, loss: 0.2853, instance_loss: 0.5431, weighted_loss: 0.3626, label: 1, bag_size: 3777\n",
      "batch 679, loss: 0.5867, instance_loss: 0.8170, weighted_loss: 0.6558, label: 0, bag_size: 4402\n",
      "batch 699, loss: 0.4744, instance_loss: 0.6773, weighted_loss: 0.5352, label: 1, bag_size: 6151\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9516071428571429: correct 10658/11200\n",
      "class 1 clustering acc 0.18392857142857144: correct 1030/5600\n",
      "Epoch: 14, train_loss: 0.6559, train_clustering_loss:  0.8727, train_error: 0.3743\n",
      "class 0: acc 0.6160714285714286, correct 207/336\n",
      "class 1: acc 0.6346153846153846, correct 231/364\n",
      "\n",
      "Val Set, val_loss: 0.7227, val_error: 0.5361, auc: 0.6583\n",
      "class 0 clustering acc 0.979381443298969: correct 1520/1552\n",
      "class 1 clustering acc 0.06572164948453608: correct 51/776\n",
      "class 0: acc 0.22033898305084745, correct 13/59\n",
      "class 1: acc 0.8421052631578947, correct 32/38\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6151, instance_loss: 0.9393, weighted_loss: 0.7124, label: 1, bag_size: 6477\n",
      "batch 39, loss: 0.4322, instance_loss: 0.6652, weighted_loss: 0.5021, label: 0, bag_size: 3699\n",
      "batch 59, loss: 0.9483, instance_loss: 1.1002, weighted_loss: 0.9939, label: 1, bag_size: 3191\n",
      "batch 79, loss: 0.9207, instance_loss: 1.2574, weighted_loss: 1.0217, label: 1, bag_size: 6759\n",
      "batch 99, loss: 0.7617, instance_loss: 0.7905, weighted_loss: 0.7704, label: 1, bag_size: 7184\n",
      "batch 119, loss: 0.5610, instance_loss: 0.6397, weighted_loss: 0.5846, label: 0, bag_size: 2586\n",
      "batch 139, loss: 0.4384, instance_loss: 0.6226, weighted_loss: 0.4936, label: 0, bag_size: 15015\n",
      "batch 159, loss: 0.7348, instance_loss: 0.9988, weighted_loss: 0.8140, label: 1, bag_size: 4791\n",
      "batch 179, loss: 1.0001, instance_loss: 1.1726, weighted_loss: 1.0519, label: 0, bag_size: 5527\n",
      "batch 199, loss: 1.7523, instance_loss: 1.8241, weighted_loss: 1.7738, label: 1, bag_size: 12948\n",
      "batch 219, loss: 0.4181, instance_loss: 0.6638, weighted_loss: 0.4918, label: 1, bag_size: 1582\n",
      "batch 239, loss: 0.8277, instance_loss: 0.8756, weighted_loss: 0.8421, label: 0, bag_size: 4427\n",
      "batch 259, loss: 0.2968, instance_loss: 0.7345, weighted_loss: 0.4281, label: 0, bag_size: 3090\n",
      "batch 279, loss: 0.3092, instance_loss: 0.5506, weighted_loss: 0.3816, label: 1, bag_size: 2493\n",
      "batch 299, loss: 0.5003, instance_loss: 0.6679, weighted_loss: 0.5506, label: 1, bag_size: 5152\n",
      "batch 319, loss: 0.2537, instance_loss: 0.6425, weighted_loss: 0.3703, label: 0, bag_size: 24289\n",
      "batch 339, loss: 0.7706, instance_loss: 1.0479, weighted_loss: 0.8538, label: 1, bag_size: 5071\n",
      "batch 359, loss: 0.4519, instance_loss: 0.7127, weighted_loss: 0.5301, label: 1, bag_size: 15118\n",
      "batch 379, loss: 0.7001, instance_loss: 0.6931, weighted_loss: 0.6980, label: 1, bag_size: 14487\n",
      "batch 399, loss: 0.2817, instance_loss: 0.4609, weighted_loss: 0.3355, label: 0, bag_size: 972\n",
      "batch 419, loss: 0.9105, instance_loss: 1.2623, weighted_loss: 1.0160, label: 0, bag_size: 4431\n",
      "batch 439, loss: 0.3834, instance_loss: 0.4246, weighted_loss: 0.3958, label: 1, bag_size: 5218\n",
      "batch 459, loss: 0.1040, instance_loss: 0.2424, weighted_loss: 0.1455, label: 1, bag_size: 5810\n",
      "batch 479, loss: 0.8436, instance_loss: 0.9121, weighted_loss: 0.8641, label: 1, bag_size: 2207\n",
      "batch 499, loss: 1.2259, instance_loss: 1.7773, weighted_loss: 1.3914, label: 1, bag_size: 2506\n",
      "batch 519, loss: 0.2621, instance_loss: 0.4110, weighted_loss: 0.3068, label: 1, bag_size: 2646\n",
      "batch 539, loss: 0.4708, instance_loss: 0.6128, weighted_loss: 0.5134, label: 0, bag_size: 19223\n",
      "batch 559, loss: 0.2487, instance_loss: 0.3359, weighted_loss: 0.2748, label: 0, bag_size: 17335\n",
      "batch 579, loss: 0.6825, instance_loss: 0.8531, weighted_loss: 0.7337, label: 1, bag_size: 13685\n",
      "batch 599, loss: 1.0071, instance_loss: 1.1647, weighted_loss: 1.0544, label: 0, bag_size: 12784\n",
      "batch 619, loss: 0.6792, instance_loss: 0.8451, weighted_loss: 0.7290, label: 1, bag_size: 4007\n",
      "batch 639, loss: 1.7106, instance_loss: 2.1552, weighted_loss: 1.8440, label: 0, bag_size: 3869\n",
      "batch 659, loss: 0.8947, instance_loss: 0.9761, weighted_loss: 0.9191, label: 1, bag_size: 3518\n",
      "batch 679, loss: 0.4848, instance_loss: 0.7634, weighted_loss: 0.5684, label: 0, bag_size: 23233\n",
      "batch 699, loss: 0.4400, instance_loss: 0.5875, weighted_loss: 0.4843, label: 0, bag_size: 6152\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9545535714285714: correct 10691/11200\n",
      "class 1 clustering acc 0.19053571428571428: correct 1067/5600\n",
      "Epoch: 15, train_loss: 0.6439, train_clustering_loss:  0.8693, train_error: 0.3757\n",
      "class 0: acc 0.6573033707865169, correct 234/356\n",
      "class 1: acc 0.5901162790697675, correct 203/344\n",
      "\n",
      "Val Set, val_loss: 0.6285, val_error: 0.3299, auc: 0.6695\n",
      "class 0 clustering acc 0.967139175257732: correct 1501/1552\n",
      "class 1 clustering acc 0.1327319587628866: correct 103/776\n",
      "class 0: acc 0.9491525423728814, correct 56/59\n",
      "class 1: acc 0.23684210526315788, correct 9/38\n",
      "Validation loss decreased (0.641809 --> 0.628549).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4176, instance_loss: 0.6388, weighted_loss: 0.4840, label: 1, bag_size: 2899\n",
      "batch 39, loss: 0.9512, instance_loss: 1.2644, weighted_loss: 1.0452, label: 0, bag_size: 5527\n",
      "batch 59, loss: 0.3250, instance_loss: 0.4767, weighted_loss: 0.3705, label: 0, bag_size: 3066\n",
      "batch 79, loss: 1.3090, instance_loss: 1.4223, weighted_loss: 1.3430, label: 0, bag_size: 3714\n",
      "batch 99, loss: 0.7942, instance_loss: 0.9133, weighted_loss: 0.8299, label: 1, bag_size: 6871\n",
      "batch 119, loss: 0.4254, instance_loss: 0.6543, weighted_loss: 0.4941, label: 0, bag_size: 14098\n",
      "batch 139, loss: 0.6231, instance_loss: 0.6843, weighted_loss: 0.6415, label: 0, bag_size: 2346\n",
      "batch 159, loss: 0.7941, instance_loss: 0.9745, weighted_loss: 0.8482, label: 0, bag_size: 3099\n",
      "batch 179, loss: 0.9457, instance_loss: 1.1605, weighted_loss: 1.0101, label: 1, bag_size: 2039\n",
      "batch 199, loss: 0.2875, instance_loss: 0.4366, weighted_loss: 0.3323, label: 0, bag_size: 1250\n",
      "batch 219, loss: 0.4718, instance_loss: 0.8228, weighted_loss: 0.5771, label: 1, bag_size: 4647\n",
      "batch 239, loss: 0.4523, instance_loss: 0.4676, weighted_loss: 0.4569, label: 1, bag_size: 3588\n",
      "batch 259, loss: 1.0053, instance_loss: 1.1223, weighted_loss: 1.0404, label: 1, bag_size: 6151\n",
      "batch 279, loss: 0.3078, instance_loss: 0.3620, weighted_loss: 0.3241, label: 1, bag_size: 5218\n",
      "batch 299, loss: 0.7639, instance_loss: 1.0086, weighted_loss: 0.8373, label: 1, bag_size: 3702\n",
      "batch 319, loss: 0.4640, instance_loss: 0.6667, weighted_loss: 0.5248, label: 0, bag_size: 2398\n",
      "batch 339, loss: 0.5139, instance_loss: 0.7778, weighted_loss: 0.5931, label: 0, bag_size: 2316\n",
      "batch 359, loss: 0.6441, instance_loss: 0.9138, weighted_loss: 0.7250, label: 1, bag_size: 16427\n",
      "batch 379, loss: 0.5478, instance_loss: 0.6838, weighted_loss: 0.5886, label: 0, bag_size: 14168\n",
      "batch 399, loss: 1.0804, instance_loss: 1.4191, weighted_loss: 1.1820, label: 1, bag_size: 3420\n",
      "batch 419, loss: 0.2629, instance_loss: 0.4486, weighted_loss: 0.3186, label: 0, bag_size: 3159\n",
      "batch 439, loss: 0.7787, instance_loss: 1.2634, weighted_loss: 0.9241, label: 1, bag_size: 534\n",
      "batch 459, loss: 0.8019, instance_loss: 1.1019, weighted_loss: 0.8919, label: 0, bag_size: 3381\n",
      "batch 479, loss: 0.3515, instance_loss: 0.5524, weighted_loss: 0.4118, label: 0, bag_size: 4124\n",
      "batch 499, loss: 0.5498, instance_loss: 0.7262, weighted_loss: 0.6027, label: 1, bag_size: 2293\n",
      "batch 519, loss: 0.3727, instance_loss: 0.5462, weighted_loss: 0.4248, label: 1, bag_size: 2081\n",
      "batch 539, loss: 0.5077, instance_loss: 0.6945, weighted_loss: 0.5637, label: 0, bag_size: 2766\n",
      "batch 559, loss: 0.4728, instance_loss: 0.7780, weighted_loss: 0.5644, label: 0, bag_size: 3022\n",
      "batch 579, loss: 0.6755, instance_loss: 0.8349, weighted_loss: 0.7233, label: 1, bag_size: 3824\n",
      "batch 599, loss: 1.3320, instance_loss: 1.2265, weighted_loss: 1.3003, label: 1, bag_size: 15213\n",
      "batch 619, loss: 0.4691, instance_loss: 0.6693, weighted_loss: 0.5291, label: 1, bag_size: 4803\n",
      "batch 639, loss: 0.5077, instance_loss: 0.7435, weighted_loss: 0.5784, label: 0, bag_size: 2238\n",
      "batch 659, loss: 0.7865, instance_loss: 1.0217, weighted_loss: 0.8570, label: 1, bag_size: 21105\n",
      "batch 679, loss: 0.5386, instance_loss: 0.7538, weighted_loss: 0.6032, label: 1, bag_size: 1838\n",
      "batch 699, loss: 0.6589, instance_loss: 0.8628, weighted_loss: 0.7201, label: 1, bag_size: 19173\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9603571428571429: correct 10756/11200\n",
      "class 1 clustering acc 0.12696428571428572: correct 711/5600\n",
      "Epoch: 16, train_loss: 0.6775, train_clustering_loss:  0.8873, train_error: 0.3829\n",
      "class 0: acc 0.6958904109589041, correct 254/365\n",
      "class 1: acc 0.5313432835820896, correct 178/335\n",
      "\n",
      "Val Set, val_loss: 0.6390, val_error: 0.2887, auc: 0.6757\n",
      "class 0 clustering acc 0.9742268041237113: correct 1512/1552\n",
      "class 1 clustering acc 0.07345360824742268: correct 57/776\n",
      "class 0: acc 0.7288135593220338, correct 43/59\n",
      "class 1: acc 0.6842105263157895, correct 26/38\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2041, instance_loss: 0.2374, weighted_loss: 0.2141, label: 1, bag_size: 9755\n",
      "batch 39, loss: 0.7806, instance_loss: 1.1300, weighted_loss: 0.8855, label: 1, bag_size: 3420\n",
      "batch 59, loss: 0.3958, instance_loss: 0.6348, weighted_loss: 0.4675, label: 0, bag_size: 3420\n",
      "batch 79, loss: 0.3923, instance_loss: 0.5064, weighted_loss: 0.4265, label: 0, bag_size: 2302\n",
      "batch 99, loss: 1.1544, instance_loss: 1.5263, weighted_loss: 1.2660, label: 0, bag_size: 2286\n",
      "batch 119, loss: 0.6363, instance_loss: 1.4145, weighted_loss: 0.8698, label: 0, bag_size: 4641\n",
      "batch 139, loss: 1.3317, instance_loss: 2.1411, weighted_loss: 1.5746, label: 0, bag_size: 3732\n",
      "batch 159, loss: 1.0022, instance_loss: 1.1009, weighted_loss: 1.0318, label: 0, bag_size: 2069\n",
      "batch 179, loss: 0.1304, instance_loss: 0.6391, weighted_loss: 0.2830, label: 1, bag_size: 18681\n",
      "batch 199, loss: 0.8705, instance_loss: 1.0209, weighted_loss: 0.9156, label: 0, bag_size: 5590\n",
      "batch 219, loss: 0.3802, instance_loss: 0.8093, weighted_loss: 0.5089, label: 0, bag_size: 2420\n",
      "batch 239, loss: 0.9582, instance_loss: 1.5603, weighted_loss: 1.1388, label: 1, bag_size: 16427\n",
      "batch 259, loss: 0.6025, instance_loss: 0.8524, weighted_loss: 0.6775, label: 1, bag_size: 5379\n",
      "batch 279, loss: 0.4165, instance_loss: 0.7376, weighted_loss: 0.5128, label: 1, bag_size: 2961\n",
      "batch 299, loss: 0.9265, instance_loss: 1.5840, weighted_loss: 1.1237, label: 0, bag_size: 5002\n",
      "batch 319, loss: 0.7387, instance_loss: 0.8331, weighted_loss: 0.7670, label: 0, bag_size: 4692\n",
      "batch 339, loss: 1.2692, instance_loss: 0.9489, weighted_loss: 1.1731, label: 1, bag_size: 2646\n",
      "batch 359, loss: 0.5193, instance_loss: 0.9387, weighted_loss: 0.6451, label: 1, bag_size: 3893\n",
      "batch 379, loss: 0.8505, instance_loss: 0.8945, weighted_loss: 0.8637, label: 1, bag_size: 1700\n",
      "batch 399, loss: 0.8222, instance_loss: 0.7975, weighted_loss: 0.8148, label: 0, bag_size: 15806\n",
      "batch 419, loss: 0.4044, instance_loss: 0.9929, weighted_loss: 0.5810, label: 0, bag_size: 3007\n",
      "batch 439, loss: 1.0102, instance_loss: 1.5427, weighted_loss: 1.1700, label: 1, bag_size: 641\n",
      "batch 459, loss: 1.0464, instance_loss: 1.0284, weighted_loss: 1.0410, label: 0, bag_size: 1532\n",
      "batch 479, loss: 0.2415, instance_loss: 0.5941, weighted_loss: 0.3473, label: 0, bag_size: 3347\n",
      "batch 499, loss: 0.4971, instance_loss: 1.0651, weighted_loss: 0.6675, label: 1, bag_size: 1272\n",
      "batch 519, loss: 0.5273, instance_loss: 0.8913, weighted_loss: 0.6365, label: 0, bag_size: 3022\n",
      "batch 539, loss: 0.3601, instance_loss: 0.7497, weighted_loss: 0.4769, label: 1, bag_size: 3143\n",
      "batch 559, loss: 0.7416, instance_loss: 1.1690, weighted_loss: 0.8698, label: 0, bag_size: 3277\n",
      "batch 579, loss: 0.6764, instance_loss: 1.0552, weighted_loss: 0.7901, label: 0, bag_size: 6524\n",
      "batch 599, loss: 1.2802, instance_loss: 1.4134, weighted_loss: 1.3202, label: 1, bag_size: 13348\n",
      "batch 619, loss: 0.5488, instance_loss: 0.7089, weighted_loss: 0.5968, label: 0, bag_size: 1652\n",
      "batch 639, loss: 0.8708, instance_loss: 1.1573, weighted_loss: 0.9568, label: 0, bag_size: 3129\n",
      "batch 659, loss: 0.3511, instance_loss: 0.7323, weighted_loss: 0.4655, label: 0, bag_size: 3137\n",
      "batch 679, loss: 0.3881, instance_loss: 0.7860, weighted_loss: 0.5075, label: 1, bag_size: 4795\n",
      "batch 699, loss: 1.1963, instance_loss: 1.2936, weighted_loss: 1.2255, label: 1, bag_size: 3020\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9566964285714286: correct 10715/11200\n",
      "class 1 clustering acc 0.10107142857142858: correct 566/5600\n",
      "Epoch: 17, train_loss: 0.6821, train_clustering_loss:  1.0807, train_error: 0.4214\n",
      "class 0: acc 0.5558739255014327, correct 194/349\n",
      "class 1: acc 0.6011396011396012, correct 211/351\n",
      "\n",
      "Val Set, val_loss: 0.6510, val_error: 0.3918, auc: 0.6601\n",
      "class 0 clustering acc 0.9787371134020618: correct 1519/1552\n",
      "class 1 clustering acc 0.03479381443298969: correct 27/776\n",
      "class 0: acc 1.0, correct 59/59\n",
      "class 1: acc 0.0, correct 0/38\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4540, instance_loss: 0.8344, weighted_loss: 0.5681, label: 1, bag_size: 7184\n",
      "batch 39, loss: 0.8470, instance_loss: 1.3179, weighted_loss: 0.9882, label: 0, bag_size: 23233\n",
      "batch 59, loss: 0.7267, instance_loss: 1.0799, weighted_loss: 0.8326, label: 0, bag_size: 3200\n",
      "batch 79, loss: 0.7533, instance_loss: 0.9932, weighted_loss: 0.8253, label: 1, bag_size: 1374\n",
      "batch 99, loss: 1.5867, instance_loss: 1.8817, weighted_loss: 1.6752, label: 0, bag_size: 2592\n",
      "batch 119, loss: 0.9882, instance_loss: 1.3064, weighted_loss: 1.0837, label: 0, bag_size: 3327\n",
      "batch 139, loss: 0.4048, instance_loss: 0.8101, weighted_loss: 0.5264, label: 0, bag_size: 18060\n",
      "batch 159, loss: 0.4589, instance_loss: 0.5275, weighted_loss: 0.4795, label: 0, bag_size: 16582\n",
      "batch 179, loss: 0.3625, instance_loss: 0.6273, weighted_loss: 0.4420, label: 0, bag_size: 5527\n",
      "batch 199, loss: 1.2240, instance_loss: 1.3513, weighted_loss: 1.2622, label: 0, bag_size: 6058\n",
      "batch 219, loss: 0.5088, instance_loss: 0.6052, weighted_loss: 0.5377, label: 1, bag_size: 3405\n",
      "batch 239, loss: 0.4362, instance_loss: 0.7083, weighted_loss: 0.5178, label: 0, bag_size: 2054\n",
      "batch 259, loss: 1.6691, instance_loss: 1.6921, weighted_loss: 1.6760, label: 1, bag_size: 3020\n",
      "batch 279, loss: 0.6189, instance_loss: 1.0693, weighted_loss: 0.7540, label: 1, bag_size: 6759\n",
      "batch 299, loss: 0.2471, instance_loss: 0.3902, weighted_loss: 0.2900, label: 1, bag_size: 8007\n",
      "batch 319, loss: 0.2987, instance_loss: 0.3575, weighted_loss: 0.3164, label: 1, bag_size: 3733\n",
      "batch 339, loss: 0.4229, instance_loss: 0.7873, weighted_loss: 0.5322, label: 0, bag_size: 17625\n",
      "batch 359, loss: 0.5366, instance_loss: 0.8696, weighted_loss: 0.6365, label: 1, bag_size: 6319\n",
      "batch 379, loss: 1.2016, instance_loss: 1.5979, weighted_loss: 1.3205, label: 0, bag_size: 35706\n",
      "batch 399, loss: 0.5584, instance_loss: 0.8559, weighted_loss: 0.6477, label: 0, bag_size: 4994\n",
      "batch 419, loss: 0.2807, instance_loss: 0.3951, weighted_loss: 0.3151, label: 1, bag_size: 3402\n",
      "batch 439, loss: 0.5583, instance_loss: 0.6274, weighted_loss: 0.5791, label: 1, bag_size: 4537\n",
      "batch 459, loss: 0.2939, instance_loss: 0.4747, weighted_loss: 0.3481, label: 1, bag_size: 5939\n",
      "batch 479, loss: 0.4633, instance_loss: 0.5512, weighted_loss: 0.4897, label: 1, bag_size: 6760\n",
      "batch 499, loss: 1.0466, instance_loss: 1.3692, weighted_loss: 1.1434, label: 1, bag_size: 15407\n",
      "batch 519, loss: 0.7980, instance_loss: 1.0245, weighted_loss: 0.8659, label: 1, bag_size: 5637\n",
      "batch 539, loss: 0.4762, instance_loss: 0.6208, weighted_loss: 0.5196, label: 1, bag_size: 4233\n",
      "batch 559, loss: 0.2699, instance_loss: 0.4714, weighted_loss: 0.3303, label: 1, bag_size: 8007\n",
      "batch 579, loss: 0.6070, instance_loss: 0.7381, weighted_loss: 0.6463, label: 1, bag_size: 3990\n",
      "batch 599, loss: 0.7135, instance_loss: 1.0456, weighted_loss: 0.8131, label: 0, bag_size: 4822\n",
      "batch 619, loss: 1.0457, instance_loss: 1.2473, weighted_loss: 1.1062, label: 1, bag_size: 3990\n",
      "batch 639, loss: 0.3783, instance_loss: 0.4580, weighted_loss: 0.4022, label: 1, bag_size: 2558\n",
      "batch 659, loss: 0.3937, instance_loss: 0.7563, weighted_loss: 0.5025, label: 0, bag_size: 5493\n",
      "batch 679, loss: 0.2048, instance_loss: 0.3720, weighted_loss: 0.2549, label: 0, bag_size: 2682\n",
      "batch 699, loss: 0.5974, instance_loss: 0.7524, weighted_loss: 0.6439, label: 0, bag_size: 1207\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9666964285714286: correct 10827/11200\n",
      "class 1 clustering acc 0.15017857142857144: correct 841/5600\n",
      "Epoch: 18, train_loss: 0.6577, train_clustering_loss:  0.8741, train_error: 0.3929\n",
      "class 0: acc 0.5838150289017341, correct 202/346\n",
      "class 1: acc 0.6299435028248588, correct 223/354\n",
      "\n",
      "Val Set, val_loss: 0.6341, val_error: 0.3196, auc: 0.6632\n",
      "class 0 clustering acc 1.0: correct 1552/1552\n",
      "class 1 clustering acc 0.0: correct 0/776\n",
      "class 0: acc 0.7966101694915254, correct 47/59\n",
      "class 1: acc 0.5, correct 19/38\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5607, instance_loss: 0.6690, weighted_loss: 0.5932, label: 1, bag_size: 15141\n",
      "batch 39, loss: 0.9890, instance_loss: 1.2645, weighted_loss: 1.0716, label: 1, bag_size: 20435\n",
      "batch 59, loss: 0.0953, instance_loss: 0.4679, weighted_loss: 0.2070, label: 1, bag_size: 5428\n",
      "batch 79, loss: 0.7235, instance_loss: 0.9874, weighted_loss: 0.8027, label: 0, bag_size: 4979\n",
      "batch 99, loss: 0.4149, instance_loss: 0.6238, weighted_loss: 0.4775, label: 0, bag_size: 5527\n",
      "batch 119, loss: 1.0206, instance_loss: 1.2564, weighted_loss: 1.0913, label: 1, bag_size: 3191\n",
      "batch 139, loss: 0.2236, instance_loss: 0.5371, weighted_loss: 0.3176, label: 0, bag_size: 2069\n",
      "batch 159, loss: 0.3184, instance_loss: 0.5826, weighted_loss: 0.3976, label: 0, bag_size: 3895\n",
      "batch 179, loss: 0.4369, instance_loss: 0.5638, weighted_loss: 0.4750, label: 1, bag_size: 5385\n",
      "batch 199, loss: 0.6380, instance_loss: 0.8893, weighted_loss: 0.7134, label: 0, bag_size: 3895\n",
      "batch 219, loss: 1.3115, instance_loss: 1.8497, weighted_loss: 1.4730, label: 1, bag_size: 10952\n",
      "batch 239, loss: 0.5958, instance_loss: 0.8128, weighted_loss: 0.6609, label: 1, bag_size: 2961\n",
      "batch 259, loss: 0.7789, instance_loss: 0.8377, weighted_loss: 0.7966, label: 0, bag_size: 6301\n",
      "batch 279, loss: 0.9621, instance_loss: 1.1157, weighted_loss: 1.0082, label: 1, bag_size: 11513\n",
      "batch 299, loss: 1.1937, instance_loss: 1.6198, weighted_loss: 1.3215, label: 1, bag_size: 2835\n",
      "batch 319, loss: 0.6870, instance_loss: 0.9681, weighted_loss: 0.7713, label: 0, bag_size: 6301\n",
      "batch 339, loss: 0.9306, instance_loss: 1.1850, weighted_loss: 1.0069, label: 1, bag_size: 6392\n",
      "batch 359, loss: 0.4590, instance_loss: 0.6487, weighted_loss: 0.5159, label: 1, bag_size: 20435\n",
      "batch 379, loss: 0.3294, instance_loss: 0.6203, weighted_loss: 0.4167, label: 0, bag_size: 10962\n",
      "batch 399, loss: 0.8508, instance_loss: 1.1110, weighted_loss: 0.9289, label: 1, bag_size: 4069\n",
      "batch 419, loss: 1.4396, instance_loss: 1.7211, weighted_loss: 1.5241, label: 1, bag_size: 1808\n",
      "batch 439, loss: 0.7706, instance_loss: 1.0761, weighted_loss: 0.8622, label: 1, bag_size: 14564\n",
      "batch 459, loss: 0.5119, instance_loss: 0.7953, weighted_loss: 0.5969, label: 0, bag_size: 5370\n",
      "batch 479, loss: 0.4578, instance_loss: 0.4644, weighted_loss: 0.4598, label: 0, bag_size: 18060\n",
      "batch 499, loss: 0.7906, instance_loss: 0.9946, weighted_loss: 0.8518, label: 1, bag_size: 4087\n",
      "batch 519, loss: 0.2324, instance_loss: 0.3120, weighted_loss: 0.2563, label: 1, bag_size: 2656\n",
      "batch 539, loss: 0.6379, instance_loss: 0.9343, weighted_loss: 0.7268, label: 1, bag_size: 16451\n",
      "batch 559, loss: 0.5300, instance_loss: 0.6276, weighted_loss: 0.5593, label: 1, bag_size: 2178\n",
      "batch 579, loss: 1.2115, instance_loss: 1.4931, weighted_loss: 1.2960, label: 1, bag_size: 6759\n",
      "batch 599, loss: 0.6282, instance_loss: 0.7737, weighted_loss: 0.6719, label: 0, bag_size: 4180\n",
      "batch 619, loss: 0.5546, instance_loss: 0.7136, weighted_loss: 0.6023, label: 1, bag_size: 4458\n",
      "batch 639, loss: 0.3958, instance_loss: 0.5500, weighted_loss: 0.4420, label: 1, bag_size: 2820\n",
      "batch 659, loss: 0.3715, instance_loss: 0.6024, weighted_loss: 0.4408, label: 0, bag_size: 5080\n",
      "batch 679, loss: 0.7603, instance_loss: 0.9902, weighted_loss: 0.8292, label: 0, bag_size: 3778\n",
      "batch 699, loss: 0.9224, instance_loss: 1.2448, weighted_loss: 1.0191, label: 1, bag_size: 14306\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9561607142857143: correct 10709/11200\n",
      "class 1 clustering acc 0.1975: correct 1106/5600\n",
      "Epoch: 19, train_loss: 0.6527, train_clustering_loss:  0.8591, train_error: 0.3800\n",
      "class 0: acc 0.6685552407932012, correct 236/353\n",
      "class 1: acc 0.5706051873198847, correct 198/347\n",
      "\n",
      "Val Set, val_loss: 0.6725, val_error: 0.3814, auc: 0.6659\n",
      "class 0 clustering acc 1.0: correct 1552/1552\n",
      "class 1 clustering acc 0.0: correct 0/776\n",
      "class 0: acc 0.559322033898305, correct 33/59\n",
      "class 1: acc 0.7105263157894737, correct 27/38\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.3044, instance_loss: 1.6166, weighted_loss: 1.3980, label: 0, bag_size: 17625\n",
      "batch 39, loss: 0.4975, instance_loss: 0.7893, weighted_loss: 0.5851, label: 0, bag_size: 2230\n",
      "batch 59, loss: 1.9675, instance_loss: 2.5445, weighted_loss: 2.1406, label: 0, bag_size: 2022\n",
      "batch 79, loss: 0.4972, instance_loss: 0.6672, weighted_loss: 0.5482, label: 0, bag_size: 3548\n",
      "batch 99, loss: 0.8168, instance_loss: 0.9873, weighted_loss: 0.8680, label: 1, bag_size: 19013\n",
      "batch 119, loss: 0.3943, instance_loss: 0.5588, weighted_loss: 0.4436, label: 0, bag_size: 3843\n",
      "batch 139, loss: 0.4574, instance_loss: 0.7422, weighted_loss: 0.5428, label: 1, bag_size: 2824\n",
      "batch 159, loss: 0.0851, instance_loss: 0.0609, weighted_loss: 0.0779, label: 1, bag_size: 15434\n",
      "batch 179, loss: 1.1173, instance_loss: 1.5218, weighted_loss: 1.2386, label: 0, bag_size: 19055\n",
      "batch 199, loss: 0.3972, instance_loss: 0.4775, weighted_loss: 0.4213, label: 0, bag_size: 14142\n",
      "batch 219, loss: 0.7023, instance_loss: 0.7854, weighted_loss: 0.7272, label: 1, bag_size: 5458\n",
      "batch 239, loss: 0.7563, instance_loss: 1.0472, weighted_loss: 0.8436, label: 1, bag_size: 6477\n",
      "batch 259, loss: 0.9587, instance_loss: 1.0984, weighted_loss: 1.0006, label: 0, bag_size: 1884\n",
      "batch 279, loss: 0.5855, instance_loss: 0.7224, weighted_loss: 0.6266, label: 0, bag_size: 1901\n",
      "batch 299, loss: 0.9617, instance_loss: 1.3909, weighted_loss: 1.0904, label: 0, bag_size: 1828\n",
      "batch 319, loss: 0.2634, instance_loss: 0.3101, weighted_loss: 0.2774, label: 1, bag_size: 534\n",
      "batch 339, loss: 0.7851, instance_loss: 1.0208, weighted_loss: 0.8558, label: 0, bag_size: 11758\n",
      "batch 359, loss: 0.5384, instance_loss: 0.6771, weighted_loss: 0.5800, label: 0, bag_size: 2268\n",
      "batch 379, loss: 0.6520, instance_loss: 0.7066, weighted_loss: 0.6684, label: 0, bag_size: 6255\n",
      "batch 399, loss: 1.1606, instance_loss: 1.3811, weighted_loss: 1.2267, label: 1, bag_size: 5665\n",
      "batch 419, loss: 0.4665, instance_loss: 0.6451, weighted_loss: 0.5201, label: 0, bag_size: 9742\n",
      "batch 439, loss: 0.2635, instance_loss: 0.3538, weighted_loss: 0.2906, label: 1, bag_size: 5570\n",
      "batch 459, loss: 0.2794, instance_loss: 0.3347, weighted_loss: 0.2960, label: 0, bag_size: 11797\n",
      "batch 479, loss: 0.0428, instance_loss: 13.7025, weighted_loss: 4.1407, label: 1, bag_size: 7085\n",
      "batch 499, loss: 1.0517, instance_loss: 1.7873, weighted_loss: 1.2724, label: 1, bag_size: 18681\n",
      "batch 519, loss: 1.2058, instance_loss: 1.3261, weighted_loss: 1.2419, label: 0, bag_size: 2918\n",
      "batch 539, loss: 0.2240, instance_loss: 0.6613, weighted_loss: 0.3552, label: 1, bag_size: 3626\n",
      "batch 559, loss: 0.9936, instance_loss: 0.9903, weighted_loss: 0.9926, label: 0, bag_size: 2856\n",
      "batch 579, loss: 0.3463, instance_loss: 0.6564, weighted_loss: 0.4393, label: 0, bag_size: 14291\n",
      "batch 599, loss: 0.3280, instance_loss: 0.6287, weighted_loss: 0.4182, label: 1, bag_size: 4268\n",
      "batch 619, loss: 0.1351, instance_loss: 0.4856, weighted_loss: 0.2403, label: 1, bag_size: 21473\n",
      "batch 639, loss: 0.5406, instance_loss: 0.7371, weighted_loss: 0.5995, label: 0, bag_size: 5002\n",
      "batch 659, loss: 0.3671, instance_loss: 0.7339, weighted_loss: 0.4771, label: 0, bag_size: 3635\n",
      "batch 679, loss: 1.1265, instance_loss: 1.4235, weighted_loss: 1.2156, label: 1, bag_size: 3159\n",
      "batch 699, loss: 1.0618, instance_loss: 1.1189, weighted_loss: 1.0790, label: 1, bag_size: 3287\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.948125: correct 10619/11200\n",
      "class 1 clustering acc 0.25214285714285717: correct 1412/5600\n",
      "Epoch: 20, train_loss: 0.6219, train_clustering_loss:  0.8563, train_error: 0.3486\n",
      "class 0: acc 0.589171974522293, correct 185/314\n",
      "class 1: acc 0.7020725388601037, correct 271/386\n",
      "\n",
      "Val Set, val_loss: 0.6438, val_error: 0.3814, auc: 0.6949\n",
      "class 0 clustering acc 1.0: correct 1552/1552\n",
      "class 1 clustering acc 0.0: correct 0/776\n",
      "class 0: acc 0.9830508474576272, correct 58/59\n",
      "class 1: acc 0.05263157894736842, correct 2/38\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6716, instance_loss: 1.1077, weighted_loss: 0.8024, label: 0, bag_size: 16842\n",
      "batch 39, loss: 0.5134, instance_loss: 0.9241, weighted_loss: 0.6366, label: 1, bag_size: 1579\n",
      "batch 59, loss: 0.3247, instance_loss: 0.5783, weighted_loss: 0.4008, label: 1, bag_size: 1627\n",
      "batch 79, loss: 1.3325, instance_loss: 1.4290, weighted_loss: 1.3615, label: 1, bag_size: 2524\n",
      "batch 99, loss: 0.9453, instance_loss: 0.9608, weighted_loss: 0.9499, label: 0, bag_size: 2302\n",
      "batch 119, loss: 0.8081, instance_loss: 1.0535, weighted_loss: 0.8817, label: 1, bag_size: 1004\n",
      "batch 139, loss: 0.6160, instance_loss: 0.8282, weighted_loss: 0.6797, label: 1, bag_size: 1495\n",
      "batch 159, loss: 1.1620, instance_loss: 1.4945, weighted_loss: 1.2617, label: 0, bag_size: 5536\n",
      "batch 179, loss: 0.5472, instance_loss: 0.7508, weighted_loss: 0.6083, label: 1, bag_size: 2646\n",
      "batch 199, loss: 0.5953, instance_loss: 0.7476, weighted_loss: 0.6410, label: 1, bag_size: 1495\n",
      "batch 219, loss: 0.4291, instance_loss: 0.6502, weighted_loss: 0.4954, label: 1, bag_size: 6760\n",
      "batch 239, loss: 0.3037, instance_loss: 0.4502, weighted_loss: 0.3477, label: 1, bag_size: 4737\n",
      "batch 259, loss: 0.4305, instance_loss: 0.6533, weighted_loss: 0.4973, label: 0, bag_size: 5924\n",
      "batch 279, loss: 0.0171, instance_loss: 0.4159, weighted_loss: 0.1367, label: 1, bag_size: 986\n",
      "batch 299, loss: 0.1623, instance_loss: 0.4006, weighted_loss: 0.2338, label: 0, bag_size: 5522\n",
      "batch 319, loss: 1.3636, instance_loss: 1.6902, weighted_loss: 1.4616, label: 1, bag_size: 1525\n",
      "batch 339, loss: 0.5555, instance_loss: 0.7908, weighted_loss: 0.6261, label: 0, bag_size: 3714\n",
      "batch 359, loss: 0.6661, instance_loss: 0.7080, weighted_loss: 0.6787, label: 1, bag_size: 5428\n",
      "batch 379, loss: 0.7882, instance_loss: 0.9289, weighted_loss: 0.8304, label: 0, bag_size: 4360\n",
      "batch 399, loss: 0.2613, instance_loss: 0.4720, weighted_loss: 0.3245, label: 0, bag_size: 9001\n",
      "batch 419, loss: 0.5097, instance_loss: 0.4302, weighted_loss: 0.4859, label: 0, bag_size: 972\n",
      "batch 439, loss: 0.4091, instance_loss: 0.4898, weighted_loss: 0.4333, label: 1, bag_size: 4249\n",
      "batch 459, loss: 0.4625, instance_loss: 0.8350, weighted_loss: 0.5743, label: 0, bag_size: 5120\n",
      "batch 479, loss: 1.1687, instance_loss: 1.2704, weighted_loss: 1.1992, label: 0, bag_size: 16309\n",
      "batch 499, loss: 0.6781, instance_loss: 1.0288, weighted_loss: 0.7833, label: 0, bag_size: 3006\n",
      "batch 519, loss: 0.2943, instance_loss: 0.4880, weighted_loss: 0.3524, label: 1, bag_size: 4802\n",
      "batch 539, loss: 1.3332, instance_loss: 1.3691, weighted_loss: 1.3440, label: 1, bag_size: 2301\n",
      "batch 559, loss: 0.3911, instance_loss: 0.5924, weighted_loss: 0.4515, label: 0, bag_size: 2054\n",
      "batch 579, loss: 0.1318, instance_loss: 0.2073, weighted_loss: 0.1544, label: 1, bag_size: 5823\n",
      "batch 599, loss: 0.5617, instance_loss: 0.5203, weighted_loss: 0.5493, label: 1, bag_size: 2646\n",
      "batch 619, loss: 0.6081, instance_loss: 0.6699, weighted_loss: 0.6266, label: 0, bag_size: 19443\n",
      "batch 639, loss: 0.2295, instance_loss: 0.3954, weighted_loss: 0.2792, label: 0, bag_size: 6209\n",
      "batch 659, loss: 0.4984, instance_loss: 0.6842, weighted_loss: 0.5541, label: 1, bag_size: 3143\n",
      "batch 679, loss: 1.3488, instance_loss: 1.5971, weighted_loss: 1.4233, label: 0, bag_size: 4506\n",
      "batch 699, loss: 0.3076, instance_loss: 0.3991, weighted_loss: 0.3350, label: 1, bag_size: 4731\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9495535714285714: correct 10635/11200\n",
      "class 1 clustering acc 0.15446428571428572: correct 865/5600\n",
      "Epoch: 21, train_loss: 0.6582, train_clustering_loss:  0.8937, train_error: 0.3900\n",
      "class 0: acc 0.6005830903790087, correct 206/343\n",
      "class 1: acc 0.6190476190476191, correct 221/357\n",
      "\n",
      "Val Set, val_loss: 0.6777, val_error: 0.4433, auc: 0.7047\n",
      "class 0 clustering acc 0.9896907216494846: correct 1536/1552\n",
      "class 1 clustering acc 0.030927835051546393: correct 24/776\n",
      "class 0: acc 0.423728813559322, correct 25/59\n",
      "class 1: acc 0.7631578947368421, correct 29/38\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7531, instance_loss: 0.8083, weighted_loss: 0.7696, label: 0, bag_size: 5448\n",
      "batch 39, loss: 0.5393, instance_loss: 0.8064, weighted_loss: 0.6194, label: 0, bag_size: 4804\n",
      "batch 59, loss: 0.4237, instance_loss: 0.6208, weighted_loss: 0.4829, label: 1, bag_size: 1272\n",
      "batch 79, loss: 1.3840, instance_loss: 1.4719, weighted_loss: 1.4104, label: 1, bag_size: 13226\n",
      "batch 99, loss: 0.6540, instance_loss: 0.9968, weighted_loss: 0.7568, label: 0, bag_size: 5153\n",
      "batch 119, loss: 0.4514, instance_loss: 0.6636, weighted_loss: 0.5151, label: 0, bag_size: 13763\n",
      "batch 139, loss: 1.0560, instance_loss: 1.3068, weighted_loss: 1.1313, label: 1, bag_size: 15213\n",
      "batch 159, loss: 0.2773, instance_loss: 0.3888, weighted_loss: 0.3108, label: 1, bag_size: 4800\n",
      "batch 179, loss: 0.4819, instance_loss: 0.6207, weighted_loss: 0.5236, label: 0, bag_size: 2682\n",
      "batch 199, loss: 0.5290, instance_loss: 0.6553, weighted_loss: 0.5669, label: 1, bag_size: 2874\n",
      "batch 219, loss: 0.4295, instance_loss: 0.6191, weighted_loss: 0.4864, label: 0, bag_size: 3290\n",
      "batch 239, loss: 0.8540, instance_loss: 0.9456, weighted_loss: 0.8815, label: 1, bag_size: 3191\n",
      "batch 259, loss: 0.9784, instance_loss: 1.2877, weighted_loss: 1.0712, label: 1, bag_size: 4384\n",
      "batch 279, loss: 0.9573, instance_loss: 1.2597, weighted_loss: 1.0480, label: 0, bag_size: 15474\n",
      "batch 299, loss: 0.6579, instance_loss: 0.9884, weighted_loss: 0.7570, label: 0, bag_size: 4360\n",
      "batch 319, loss: 0.7433, instance_loss: 1.0177, weighted_loss: 0.8257, label: 0, bag_size: 2851\n",
      "batch 339, loss: 0.8136, instance_loss: 0.8726, weighted_loss: 0.8313, label: 0, bag_size: 4329\n",
      "batch 359, loss: 0.3787, instance_loss: 0.5993, weighted_loss: 0.4449, label: 0, bag_size: 5477\n",
      "batch 379, loss: 0.5842, instance_loss: 0.8361, weighted_loss: 0.6598, label: 1, bag_size: 2506\n",
      "batch 399, loss: 0.2103, instance_loss: 0.3646, weighted_loss: 0.2566, label: 0, bag_size: 14142\n",
      "batch 419, loss: 1.0976, instance_loss: 1.7265, weighted_loss: 1.2863, label: 1, bag_size: 12948\n",
      "batch 439, loss: 0.4924, instance_loss: 0.5289, weighted_loss: 0.5034, label: 1, bag_size: 3910\n",
      "batch 459, loss: 0.4186, instance_loss: 0.6494, weighted_loss: 0.4878, label: 1, bag_size: 4510\n",
      "batch 479, loss: 0.6093, instance_loss: 0.6764, weighted_loss: 0.6294, label: 1, bag_size: 2407\n",
      "batch 499, loss: 0.1930, instance_loss: 0.2713, weighted_loss: 0.2165, label: 1, bag_size: 2126\n",
      "batch 519, loss: 0.5056, instance_loss: 0.6338, weighted_loss: 0.5440, label: 1, bag_size: 2783\n",
      "batch 539, loss: 0.4988, instance_loss: 0.6134, weighted_loss: 0.5332, label: 0, bag_size: 3936\n",
      "batch 559, loss: 0.7585, instance_loss: 1.1412, weighted_loss: 0.8733, label: 1, bag_size: 6477\n",
      "batch 579, loss: 0.1384, instance_loss: 0.1737, weighted_loss: 0.1490, label: 1, bag_size: 5428\n",
      "batch 599, loss: 0.5548, instance_loss: 0.7343, weighted_loss: 0.6087, label: 0, bag_size: 6593\n",
      "batch 619, loss: 0.9474, instance_loss: 1.0707, weighted_loss: 0.9844, label: 1, bag_size: 5810\n",
      "batch 639, loss: 0.4381, instance_loss: 0.6339, weighted_loss: 0.4968, label: 1, bag_size: 6884\n",
      "batch 659, loss: 1.2471, instance_loss: 1.6688, weighted_loss: 1.3736, label: 0, bag_size: 5458\n",
      "batch 679, loss: 0.3249, instance_loss: 0.5513, weighted_loss: 0.3928, label: 0, bag_size: 2054\n",
      "batch 699, loss: 0.3128, instance_loss: 0.4400, weighted_loss: 0.3510, label: 0, bag_size: 28144\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9578571428571429: correct 10728/11200\n",
      "class 1 clustering acc 0.19642857142857142: correct 1100/5600\n",
      "Epoch: 22, train_loss: 0.6527, train_clustering_loss:  0.8507, train_error: 0.3886\n",
      "class 0: acc 0.5901162790697675, correct 203/344\n",
      "class 1: acc 0.6320224719101124, correct 225/356\n",
      "\n",
      "Val Set, val_loss: 0.6134, val_error: 0.3093, auc: 0.6967\n",
      "class 0 clustering acc 0.9555412371134021: correct 1483/1552\n",
      "class 1 clustering acc 0.23840206185567012: correct 185/776\n",
      "class 0: acc 0.864406779661017, correct 51/59\n",
      "class 1: acc 0.42105263157894735, correct 16/38\n",
      "Validation loss decreased (0.628549 --> 0.613382).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.8092, instance_loss: 1.1210, weighted_loss: 0.9028, label: 1, bag_size: 4698\n",
      "batch 39, loss: 0.3584, instance_loss: 0.4362, weighted_loss: 0.3818, label: 0, bag_size: 5522\n",
      "batch 59, loss: 0.5487, instance_loss: 0.6752, weighted_loss: 0.5866, label: 0, bag_size: 4303\n",
      "batch 79, loss: 0.4811, instance_loss: 0.6559, weighted_loss: 0.5336, label: 0, bag_size: 7667\n",
      "batch 99, loss: 0.1253, instance_loss: 0.1832, weighted_loss: 0.1427, label: 1, bag_size: 4795\n",
      "batch 119, loss: 0.6017, instance_loss: 0.8250, weighted_loss: 0.6687, label: 0, bag_size: 3450\n",
      "batch 139, loss: 0.4300, instance_loss: 0.4761, weighted_loss: 0.4438, label: 0, bag_size: 5369\n",
      "batch 159, loss: 0.4091, instance_loss: 0.4865, weighted_loss: 0.4324, label: 0, bag_size: 2678\n",
      "batch 179, loss: 0.3516, instance_loss: 0.4289, weighted_loss: 0.3748, label: 0, bag_size: 22113\n",
      "batch 199, loss: 0.1224, instance_loss: 0.1938, weighted_loss: 0.1438, label: 0, bag_size: 21574\n",
      "batch 219, loss: 0.7525, instance_loss: 1.1378, weighted_loss: 0.8681, label: 1, bag_size: 24686\n",
      "batch 239, loss: 0.3911, instance_loss: 0.3438, weighted_loss: 0.3769, label: 0, bag_size: 28144\n",
      "batch 259, loss: 0.4852, instance_loss: 0.5386, weighted_loss: 0.5012, label: 0, bag_size: 15193\n",
      "batch 279, loss: 0.5747, instance_loss: 0.7456, weighted_loss: 0.6260, label: 0, bag_size: 6777\n",
      "batch 299, loss: 1.2204, instance_loss: 1.8704, weighted_loss: 1.4154, label: 1, bag_size: 1582\n",
      "batch 319, loss: 0.5953, instance_loss: 0.8822, weighted_loss: 0.6814, label: 0, bag_size: 6351\n",
      "batch 339, loss: 0.8757, instance_loss: 1.1486, weighted_loss: 0.9576, label: 1, bag_size: 11513\n",
      "batch 359, loss: 0.5456, instance_loss: 0.6436, weighted_loss: 0.5750, label: 0, bag_size: 2110\n",
      "batch 379, loss: 0.7629, instance_loss: 0.8763, weighted_loss: 0.7969, label: 0, bag_size: 1826\n",
      "batch 399, loss: 0.1344, instance_loss: 0.1488, weighted_loss: 0.1387, label: 1, bag_size: 4044\n",
      "batch 419, loss: 0.3639, instance_loss: 0.3880, weighted_loss: 0.3711, label: 1, bag_size: 5071\n",
      "batch 439, loss: 1.4233, instance_loss: 1.7039, weighted_loss: 1.5075, label: 0, bag_size: 2762\n",
      "batch 459, loss: 0.2612, instance_loss: 0.2596, weighted_loss: 0.2608, label: 0, bag_size: 17273\n",
      "batch 479, loss: 0.2536, instance_loss: 0.2047, weighted_loss: 0.2389, label: 0, bag_size: 16582\n",
      "batch 499, loss: 0.2619, instance_loss: 0.3266, weighted_loss: 0.2813, label: 1, bag_size: 5777\n",
      "batch 519, loss: 0.8026, instance_loss: 1.1482, weighted_loss: 0.9063, label: 0, bag_size: 10568\n",
      "batch 539, loss: 0.3042, instance_loss: 0.3883, weighted_loss: 0.3294, label: 1, bag_size: 3143\n",
      "batch 559, loss: 0.5066, instance_loss: 0.4601, weighted_loss: 0.4926, label: 1, bag_size: 3143\n",
      "batch 579, loss: 0.5402, instance_loss: 0.6357, weighted_loss: 0.5688, label: 1, bag_size: 13217\n",
      "batch 599, loss: 0.5676, instance_loss: 0.8052, weighted_loss: 0.6389, label: 0, bag_size: 5370\n",
      "batch 619, loss: 0.7429, instance_loss: 0.8555, weighted_loss: 0.7767, label: 0, bag_size: 4548\n",
      "batch 639, loss: 0.1915, instance_loss: 0.1847, weighted_loss: 0.1895, label: 0, bag_size: 16842\n",
      "batch 659, loss: 0.5150, instance_loss: 0.6481, weighted_loss: 0.5549, label: 0, bag_size: 5369\n",
      "batch 679, loss: 0.5374, instance_loss: 0.6534, weighted_loss: 0.5722, label: 0, bag_size: 5370\n",
      "batch 699, loss: 0.4227, instance_loss: 0.5288, weighted_loss: 0.4545, label: 0, bag_size: 2054\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9382142857142857: correct 10508/11200\n",
      "class 1 clustering acc 0.3457142857142857: correct 1936/5600\n",
      "Epoch: 23, train_loss: 0.6147, train_clustering_loss:  0.7829, train_error: 0.3243\n",
      "class 0: acc 0.725, correct 261/360\n",
      "class 1: acc 0.6235294117647059, correct 212/340\n",
      "\n",
      "Val Set, val_loss: 0.6655, val_error: 0.3402, auc: 0.6695\n",
      "class 0 clustering acc 0.9561855670103093: correct 1484/1552\n",
      "class 1 clustering acc 0.10438144329896908: correct 81/776\n",
      "class 0: acc 0.6101694915254238, correct 36/59\n",
      "class 1: acc 0.7368421052631579, correct 28/38\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3577, instance_loss: 0.4770, weighted_loss: 0.3935, label: 1, bag_size: 5379\n",
      "batch 39, loss: 0.5346, instance_loss: 0.5398, weighted_loss: 0.5362, label: 0, bag_size: 16859\n",
      "batch 59, loss: 0.5070, instance_loss: 0.6221, weighted_loss: 0.5415, label: 1, bag_size: 1503\n",
      "batch 79, loss: 0.4115, instance_loss: 0.6671, weighted_loss: 0.4882, label: 1, bag_size: 2624\n",
      "batch 99, loss: 0.9164, instance_loss: 1.2402, weighted_loss: 1.0136, label: 1, bag_size: 5100\n",
      "batch 119, loss: 0.5233, instance_loss: 0.5600, weighted_loss: 0.5343, label: 0, bag_size: 6524\n",
      "batch 139, loss: 0.2105, instance_loss: 0.1963, weighted_loss: 0.2062, label: 1, bag_size: 4249\n",
      "batch 159, loss: 0.4399, instance_loss: 0.5125, weighted_loss: 0.4617, label: 1, bag_size: 4572\n",
      "batch 179, loss: 0.1193, instance_loss: 0.1106, weighted_loss: 0.1167, label: 0, bag_size: 21574\n",
      "batch 199, loss: 0.6517, instance_loss: 0.8904, weighted_loss: 0.7233, label: 0, bag_size: 2268\n",
      "batch 219, loss: 0.7928, instance_loss: 1.0737, weighted_loss: 0.8771, label: 1, bag_size: 2506\n",
      "batch 239, loss: 0.5014, instance_loss: 0.7445, weighted_loss: 0.5743, label: 0, bag_size: 2302\n",
      "batch 259, loss: 0.7317, instance_loss: 1.1035, weighted_loss: 0.8432, label: 1, bag_size: 4268\n",
      "batch 279, loss: 0.5871, instance_loss: 0.7984, weighted_loss: 0.6505, label: 0, bag_size: 4694\n",
      "batch 299, loss: 0.1406, instance_loss: 0.1430, weighted_loss: 0.1413, label: 0, bag_size: 3699\n",
      "batch 319, loss: 0.6191, instance_loss: 0.7008, weighted_loss: 0.6436, label: 0, bag_size: 19443\n",
      "batch 339, loss: 1.4507, instance_loss: 1.8273, weighted_loss: 1.5637, label: 0, bag_size: 3724\n",
      "batch 359, loss: 0.3462, instance_loss: 0.5801, weighted_loss: 0.4164, label: 0, bag_size: 14168\n",
      "batch 379, loss: 0.4261, instance_loss: 0.5391, weighted_loss: 0.4600, label: 0, bag_size: 4086\n",
      "batch 399, loss: 0.5923, instance_loss: 0.7417, weighted_loss: 0.6371, label: 0, bag_size: 3534\n",
      "batch 419, loss: 0.3641, instance_loss: 0.5485, weighted_loss: 0.4194, label: 0, bag_size: 5120\n",
      "batch 439, loss: 1.0514, instance_loss: 1.3230, weighted_loss: 1.1329, label: 1, bag_size: 10952\n",
      "batch 459, loss: 0.2345, instance_loss: 0.2209, weighted_loss: 0.2304, label: 1, bag_size: 2820\n",
      "batch 479, loss: 0.3312, instance_loss: 0.3286, weighted_loss: 0.3304, label: 1, bag_size: 15141\n",
      "batch 499, loss: 0.5473, instance_loss: 0.6715, weighted_loss: 0.5846, label: 1, bag_size: 5385\n",
      "batch 519, loss: 0.3226, instance_loss: 0.3467, weighted_loss: 0.3298, label: 0, bag_size: 4835\n",
      "batch 539, loss: 0.9743, instance_loss: 1.2755, weighted_loss: 1.0647, label: 0, bag_size: 5269\n",
      "batch 559, loss: 0.4195, instance_loss: 0.4819, weighted_loss: 0.4382, label: 1, bag_size: 2961\n",
      "batch 579, loss: 0.9931, instance_loss: 1.2058, weighted_loss: 1.0569, label: 1, bag_size: 11555\n",
      "batch 599, loss: 1.2567, instance_loss: 1.4876, weighted_loss: 1.3260, label: 0, bag_size: 19223\n",
      "batch 619, loss: 1.0335, instance_loss: 1.3856, weighted_loss: 1.1391, label: 0, bag_size: 4226\n",
      "batch 639, loss: 0.6555, instance_loss: 0.8751, weighted_loss: 0.7214, label: 1, bag_size: 2824\n",
      "batch 659, loss: 0.2380, instance_loss: 0.1962, weighted_loss: 0.2254, label: 1, bag_size: 4815\n",
      "batch 679, loss: 0.1525, instance_loss: 0.1081, weighted_loss: 0.1392, label: 0, bag_size: 13573\n",
      "batch 699, loss: 0.3608, instance_loss: 0.6539, weighted_loss: 0.4487, label: 1, bag_size: 16936\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9423214285714285: correct 10554/11200\n",
      "class 1 clustering acc 0.3092857142857143: correct 1732/5600\n",
      "Epoch: 24, train_loss: 0.6237, train_clustering_loss:  0.7940, train_error: 0.3329\n",
      "class 0: acc 0.7150684931506849, correct 261/365\n",
      "class 1: acc 0.6149253731343284, correct 206/335\n",
      "\n",
      "Val Set, val_loss: 0.8363, val_error: 0.5155, auc: 0.6740\n",
      "class 0 clustering acc 0.8846649484536082: correct 1373/1552\n",
      "class 1 clustering acc 0.2036082474226804: correct 158/776\n",
      "class 0: acc 0.2542372881355932, correct 15/59\n",
      "class 1: acc 0.8421052631578947, correct 32/38\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3481, instance_loss: 0.2341, weighted_loss: 0.3139, label: 1, bag_size: 4731\n",
      "batch 39, loss: 0.0720, instance_loss: 0.2543, weighted_loss: 0.1267, label: 1, bag_size: 1479\n",
      "batch 59, loss: 1.3600, instance_loss: 1.8574, weighted_loss: 1.5092, label: 1, bag_size: 2178\n",
      "batch 79, loss: 1.1844, instance_loss: 1.6017, weighted_loss: 1.3096, label: 0, bag_size: 1648\n",
      "batch 99, loss: 0.8961, instance_loss: 1.2784, weighted_loss: 1.0108, label: 1, bag_size: 4819\n",
      "batch 119, loss: 0.2738, instance_loss: 0.3127, weighted_loss: 0.2855, label: 1, bag_size: 1587\n",
      "batch 139, loss: 0.4729, instance_loss: 0.6168, weighted_loss: 0.5161, label: 1, bag_size: 11968\n",
      "batch 159, loss: 0.6311, instance_loss: 0.8852, weighted_loss: 0.7073, label: 1, bag_size: 4233\n",
      "batch 179, loss: 1.0423, instance_loss: 1.0483, weighted_loss: 1.0441, label: 1, bag_size: 1004\n",
      "batch 199, loss: 1.0100, instance_loss: 1.3384, weighted_loss: 1.1085, label: 1, bag_size: 4243\n",
      "batch 219, loss: 2.5601, instance_loss: 2.4557, weighted_loss: 2.5288, label: 1, bag_size: 2783\n",
      "batch 239, loss: 0.3770, instance_loss: 0.8190, weighted_loss: 0.5096, label: 1, bag_size: 3338\n",
      "batch 259, loss: 0.7438, instance_loss: 1.1759, weighted_loss: 0.8735, label: 0, bag_size: 4427\n",
      "batch 279, loss: 0.2098, instance_loss: 0.3653, weighted_loss: 0.2564, label: 1, bag_size: 6760\n",
      "batch 299, loss: 0.3215, instance_loss: 0.3899, weighted_loss: 0.3421, label: 1, bag_size: 5068\n",
      "batch 319, loss: 0.4770, instance_loss: 0.6539, weighted_loss: 0.5301, label: 0, bag_size: 3392\n",
      "batch 339, loss: 0.2920, instance_loss: 0.4223, weighted_loss: 0.3311, label: 1, bag_size: 2407\n",
      "batch 359, loss: 0.7110, instance_loss: 1.0357, weighted_loss: 0.8084, label: 0, bag_size: 4597\n",
      "batch 379, loss: 0.7618, instance_loss: 0.9619, weighted_loss: 0.8218, label: 1, bag_size: 4647\n",
      "batch 399, loss: 1.2436, instance_loss: 1.5545, weighted_loss: 1.3369, label: 1, bag_size: 3338\n",
      "batch 419, loss: 0.6260, instance_loss: 0.6901, weighted_loss: 0.6452, label: 0, bag_size: 17625\n",
      "batch 439, loss: 0.2944, instance_loss: 0.2693, weighted_loss: 0.2869, label: 1, bag_size: 3834\n",
      "batch 459, loss: 1.0345, instance_loss: 1.2728, weighted_loss: 1.1060, label: 1, bag_size: 4094\n",
      "batch 479, loss: 1.4233, instance_loss: 1.7680, weighted_loss: 1.5267, label: 1, bag_size: 2524\n",
      "batch 499, loss: 0.4884, instance_loss: 1.4565, weighted_loss: 0.7789, label: 0, bag_size: 17546\n",
      "batch 519, loss: 0.5178, instance_loss: 0.7437, weighted_loss: 0.5856, label: 1, bag_size: 1479\n",
      "batch 539, loss: 1.0785, instance_loss: 1.0729, weighted_loss: 1.0768, label: 0, bag_size: 1638\n",
      "batch 559, loss: 0.4739, instance_loss: 0.6903, weighted_loss: 0.5388, label: 1, bag_size: 1958\n",
      "batch 579, loss: 0.4884, instance_loss: 0.5657, weighted_loss: 0.5116, label: 0, bag_size: 575\n",
      "batch 599, loss: 0.0342, instance_loss: 0.3287, weighted_loss: 0.1225, label: 0, bag_size: 17546\n",
      "batch 619, loss: 0.8397, instance_loss: 0.7973, weighted_loss: 0.8270, label: 0, bag_size: 4179\n",
      "batch 639, loss: 0.3485, instance_loss: 0.5710, weighted_loss: 0.4152, label: 0, bag_size: 2230\n",
      "batch 659, loss: 0.9328, instance_loss: 1.2916, weighted_loss: 1.0404, label: 1, bag_size: 1004\n",
      "batch 679, loss: 0.2610, instance_loss: 0.3996, weighted_loss: 0.3026, label: 0, bag_size: 16663\n",
      "batch 699, loss: 0.3305, instance_loss: 0.4812, weighted_loss: 0.3757, label: 1, bag_size: 2036\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9361607142857142: correct 10485/11200\n",
      "class 1 clustering acc 0.32357142857142857: correct 1812/5600\n",
      "Epoch: 25, train_loss: 0.6201, train_clustering_loss:  0.8091, train_error: 0.3457\n",
      "class 0: acc 0.6713881019830028, correct 237/353\n",
      "class 1: acc 0.6368876080691642, correct 221/347\n",
      "\n",
      "Val Set, val_loss: 0.7277, val_error: 0.4948, auc: 0.6748\n",
      "class 0 clustering acc 0.8492268041237113: correct 1318/1552\n",
      "class 1 clustering acc 0.27963917525773196: correct 217/776\n",
      "class 0: acc 0.3389830508474576, correct 20/59\n",
      "class 1: acc 0.7631578947368421, correct 29/38\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2266, instance_loss: 0.2501, weighted_loss: 0.2336, label: 1, bag_size: 4537\n",
      "batch 39, loss: 0.0701, instance_loss: 0.0457, weighted_loss: 0.0628, label: 0, bag_size: 14168\n",
      "batch 59, loss: 0.2478, instance_loss: 0.3060, weighted_loss: 0.2652, label: 0, bag_size: 4468\n",
      "batch 79, loss: 0.6661, instance_loss: 0.8388, weighted_loss: 0.7179, label: 0, bag_size: 5370\n",
      "batch 99, loss: 0.5624, instance_loss: 0.8124, weighted_loss: 0.6374, label: 0, bag_size: 586\n",
      "batch 119, loss: 1.1230, instance_loss: 1.3559, weighted_loss: 1.1928, label: 1, bag_size: 3159\n",
      "batch 139, loss: 0.9050, instance_loss: 1.0388, weighted_loss: 0.9451, label: 1, bag_size: 8075\n",
      "batch 159, loss: 0.9021, instance_loss: 1.1249, weighted_loss: 0.9689, label: 0, bag_size: 3196\n",
      "batch 179, loss: 0.7128, instance_loss: 0.8796, weighted_loss: 0.7628, label: 0, bag_size: 5493\n",
      "batch 199, loss: 0.4928, instance_loss: 0.5135, weighted_loss: 0.4990, label: 0, bag_size: 11758\n",
      "batch 219, loss: 0.0159, instance_loss: 0.0416, weighted_loss: 0.0236, label: 1, bag_size: 5218\n",
      "batch 239, loss: 1.3355, instance_loss: 1.7026, weighted_loss: 1.4456, label: 0, bag_size: 6777\n",
      "batch 259, loss: 0.3180, instance_loss: 0.4332, weighted_loss: 0.3525, label: 0, bag_size: 3463\n",
      "batch 279, loss: 0.3456, instance_loss: 0.5535, weighted_loss: 0.4079, label: 0, bag_size: 3312\n",
      "batch 299, loss: 0.5166, instance_loss: 0.5910, weighted_loss: 0.5389, label: 1, bag_size: 13217\n",
      "batch 319, loss: 0.4428, instance_loss: 0.4460, weighted_loss: 0.4437, label: 0, bag_size: 5125\n",
      "batch 339, loss: 0.8320, instance_loss: 1.0176, weighted_loss: 0.8877, label: 1, bag_size: 21059\n",
      "batch 359, loss: 0.0860, instance_loss: 0.1056, weighted_loss: 0.0919, label: 0, bag_size: 2069\n",
      "batch 379, loss: 0.6651, instance_loss: 0.8243, weighted_loss: 0.7128, label: 0, bag_size: 5522\n",
      "batch 399, loss: 0.1137, instance_loss: 0.1231, weighted_loss: 0.1165, label: 0, bag_size: 5924\n",
      "batch 419, loss: 0.3947, instance_loss: 0.4669, weighted_loss: 0.4164, label: 1, bag_size: 22171\n",
      "batch 439, loss: 0.0900, instance_loss: 0.0804, weighted_loss: 0.0871, label: 1, bag_size: 3777\n",
      "batch 459, loss: 1.0138, instance_loss: 1.3676, weighted_loss: 1.1200, label: 1, bag_size: 5677\n",
      "batch 479, loss: 0.6189, instance_loss: 0.8086, weighted_loss: 0.6758, label: 0, bag_size: 11758\n",
      "batch 499, loss: 0.5078, instance_loss: 0.5915, weighted_loss: 0.5329, label: 1, bag_size: 8331\n",
      "batch 519, loss: 0.7225, instance_loss: 1.0040, weighted_loss: 0.8070, label: 0, bag_size: 4855\n",
      "batch 539, loss: 0.0883, instance_loss: 0.1014, weighted_loss: 0.0922, label: 1, bag_size: 4044\n",
      "batch 559, loss: 1.1974, instance_loss: 1.5680, weighted_loss: 1.3086, label: 1, bag_size: 10952\n",
      "batch 579, loss: 1.0116, instance_loss: 1.3272, weighted_loss: 1.1063, label: 1, bag_size: 4761\n",
      "batch 599, loss: 0.5155, instance_loss: 0.6259, weighted_loss: 0.5486, label: 0, bag_size: 5967\n",
      "batch 619, loss: 1.0916, instance_loss: 1.3653, weighted_loss: 1.1737, label: 0, bag_size: 999\n",
      "batch 639, loss: 0.3483, instance_loss: 0.4680, weighted_loss: 0.3842, label: 1, bag_size: 6878\n",
      "batch 659, loss: 0.8318, instance_loss: 1.1397, weighted_loss: 0.9242, label: 1, bag_size: 2207\n",
      "batch 679, loss: 0.8960, instance_loss: 1.2520, weighted_loss: 1.0028, label: 0, bag_size: 5153\n",
      "batch 699, loss: 0.3068, instance_loss: 0.3516, weighted_loss: 0.3202, label: 0, bag_size: 14142\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9475892857142857: correct 10613/11200\n",
      "class 1 clustering acc 0.3182142857142857: correct 1782/5600\n",
      "Epoch: 26, train_loss: 0.6186, train_clustering_loss:  0.7878, train_error: 0.3386\n",
      "class 0: acc 0.6829971181556196, correct 237/347\n",
      "class 1: acc 0.6402266288951841, correct 226/353\n",
      "\n",
      "Val Set, val_loss: 0.6486, val_error: 0.3299, auc: 0.6726\n",
      "class 0 clustering acc 0.9329896907216495: correct 1448/1552\n",
      "class 1 clustering acc 0.17525773195876287: correct 136/776\n",
      "class 0: acc 0.6610169491525424, correct 39/59\n",
      "class 1: acc 0.6842105263157895, correct 26/38\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4882, instance_loss: 0.5696, weighted_loss: 0.5126, label: 1, bag_size: 3208\n",
      "batch 39, loss: 0.2631, instance_loss: 0.2720, weighted_loss: 0.2658, label: 1, bag_size: 5651\n",
      "batch 59, loss: 1.1366, instance_loss: 1.5448, weighted_loss: 1.2591, label: 1, bag_size: 2301\n",
      "batch 79, loss: 0.3002, instance_loss: 0.4966, weighted_loss: 0.3591, label: 0, bag_size: 2302\n",
      "batch 99, loss: 0.6881, instance_loss: 0.9270, weighted_loss: 0.7598, label: 0, bag_size: 4922\n",
      "batch 119, loss: 0.7480, instance_loss: 1.0550, weighted_loss: 0.8401, label: 1, bag_size: 2646\n",
      "batch 139, loss: 0.3820, instance_loss: 0.4088, weighted_loss: 0.3900, label: 0, bag_size: 3928\n",
      "batch 159, loss: 0.7315, instance_loss: 0.9622, weighted_loss: 0.8007, label: 1, bag_size: 2207\n",
      "batch 179, loss: 0.8669, instance_loss: 1.1672, weighted_loss: 0.9570, label: 1, bag_size: 1004\n",
      "batch 199, loss: 0.1504, instance_loss: 0.1356, weighted_loss: 0.1460, label: 0, bag_size: 1379\n",
      "batch 219, loss: 0.3312, instance_loss: 0.4588, weighted_loss: 0.3695, label: 1, bag_size: 4737\n",
      "batch 239, loss: 0.1367, instance_loss: 0.1531, weighted_loss: 0.1416, label: 1, bag_size: 5823\n",
      "batch 259, loss: 0.4925, instance_loss: 0.5687, weighted_loss: 0.5153, label: 1, bag_size: 4853\n",
      "batch 279, loss: 1.1053, instance_loss: 1.4429, weighted_loss: 1.2066, label: 1, bag_size: 641\n",
      "batch 299, loss: 0.3999, instance_loss: 0.4564, weighted_loss: 0.4169, label: 0, bag_size: 5924\n",
      "batch 319, loss: 0.5697, instance_loss: 0.7249, weighted_loss: 0.6163, label: 0, bag_size: 3540\n",
      "batch 339, loss: 0.2175, instance_loss: 0.2297, weighted_loss: 0.2212, label: 1, bag_size: 16427\n",
      "batch 359, loss: 0.4337, instance_loss: 0.5055, weighted_loss: 0.4553, label: 0, bag_size: 14856\n",
      "batch 379, loss: 0.6466, instance_loss: 0.7665, weighted_loss: 0.6826, label: 0, bag_size: 5697\n",
      "batch 399, loss: 0.4484, instance_loss: 0.5353, weighted_loss: 0.4745, label: 0, bag_size: 5338\n",
      "batch 419, loss: 0.3164, instance_loss: 0.4269, weighted_loss: 0.3496, label: 0, bag_size: 11040\n",
      "batch 439, loss: 0.4339, instance_loss: 0.5566, weighted_loss: 0.4707, label: 1, bag_size: 8075\n",
      "batch 459, loss: 0.6015, instance_loss: 0.7908, weighted_loss: 0.6583, label: 1, bag_size: 1552\n",
      "batch 479, loss: 0.5531, instance_loss: 0.6969, weighted_loss: 0.5962, label: 0, bag_size: 3450\n",
      "batch 499, loss: 0.8732, instance_loss: 1.0627, weighted_loss: 0.9301, label: 0, bag_size: 3264\n",
      "batch 519, loss: 0.6298, instance_loss: 0.7949, weighted_loss: 0.6793, label: 0, bag_size: 2244\n",
      "batch 539, loss: 0.8224, instance_loss: 1.0598, weighted_loss: 0.8936, label: 1, bag_size: 1958\n",
      "batch 559, loss: 0.3028, instance_loss: 0.3714, weighted_loss: 0.3234, label: 0, bag_size: 3022\n",
      "batch 579, loss: 0.3444, instance_loss: 0.4240, weighted_loss: 0.3683, label: 1, bag_size: 4069\n",
      "batch 599, loss: 0.8973, instance_loss: 1.0828, weighted_loss: 0.9529, label: 1, bag_size: 5671\n",
      "batch 619, loss: 0.9283, instance_loss: 1.2023, weighted_loss: 1.0105, label: 0, bag_size: 4181\n",
      "batch 639, loss: 1.5003, instance_loss: 2.0535, weighted_loss: 1.6662, label: 1, bag_size: 2506\n",
      "batch 659, loss: 0.3835, instance_loss: 0.3696, weighted_loss: 0.3794, label: 0, bag_size: 2346\n",
      "batch 679, loss: 0.2316, instance_loss: 0.2988, weighted_loss: 0.2518, label: 0, bag_size: 6724\n",
      "batch 699, loss: 0.9609, instance_loss: 1.3147, weighted_loss: 1.0671, label: 0, bag_size: 2146\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9525892857142857: correct 10669/11200\n",
      "class 1 clustering acc 0.27232142857142855: correct 1525/5600\n",
      "Epoch: 27, train_loss: 0.6341, train_clustering_loss:  0.8003, train_error: 0.3657\n",
      "class 0: acc 0.7043010752688172, correct 262/372\n",
      "class 1: acc 0.5548780487804879, correct 182/328\n",
      "\n",
      "Val Set, val_loss: 0.6299, val_error: 0.3299, auc: 0.6824\n",
      "class 0 clustering acc 0.7609536082474226: correct 1181/1552\n",
      "class 1 clustering acc 0.4484536082474227: correct 348/776\n",
      "class 0: acc 0.8983050847457628, correct 53/59\n",
      "class 1: acc 0.3157894736842105, correct 12/38\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4254, instance_loss: 0.5494, weighted_loss: 0.4626, label: 1, bag_size: 13217\n",
      "batch 39, loss: 0.8868, instance_loss: 1.1271, weighted_loss: 0.9589, label: 1, bag_size: 2207\n",
      "batch 59, loss: 0.0117, instance_loss: 0.2487, weighted_loss: 0.0828, label: 1, bag_size: 5695\n",
      "batch 79, loss: 0.2474, instance_loss: 0.2840, weighted_loss: 0.2583, label: 1, bag_size: 8075\n",
      "batch 99, loss: 0.7697, instance_loss: 0.9650, weighted_loss: 0.8283, label: 1, bag_size: 3191\n",
      "batch 119, loss: 0.3475, instance_loss: 0.4521, weighted_loss: 0.3789, label: 1, bag_size: 5677\n",
      "batch 139, loss: 1.0605, instance_loss: 1.4276, weighted_loss: 1.1706, label: 0, bag_size: 5331\n",
      "batch 159, loss: 1.1128, instance_loss: 1.3875, weighted_loss: 1.1953, label: 1, bag_size: 4046\n",
      "batch 179, loss: 0.2040, instance_loss: 0.1884, weighted_loss: 0.1993, label: 1, bag_size: 4044\n",
      "batch 199, loss: 0.2857, instance_loss: 0.3802, weighted_loss: 0.3141, label: 0, bag_size: 3022\n",
      "batch 219, loss: 1.3913, instance_loss: 1.3886, weighted_loss: 1.3905, label: 0, bag_size: 4254\n",
      "batch 239, loss: 0.1586, instance_loss: 0.2233, weighted_loss: 0.1780, label: 0, bag_size: 10536\n",
      "batch 259, loss: 0.7745, instance_loss: 1.0386, weighted_loss: 0.8537, label: 1, bag_size: 5935\n",
      "batch 279, loss: 0.1895, instance_loss: 0.2103, weighted_loss: 0.1957, label: 1, bag_size: 6392\n",
      "batch 299, loss: 0.4852, instance_loss: 0.5382, weighted_loss: 0.5011, label: 0, bag_size: 1226\n",
      "batch 319, loss: 0.5770, instance_loss: 0.7345, weighted_loss: 0.6243, label: 0, bag_size: 3936\n",
      "batch 339, loss: 0.7300, instance_loss: 0.9841, weighted_loss: 0.8062, label: 0, bag_size: 2238\n",
      "batch 359, loss: 0.3872, instance_loss: 0.3927, weighted_loss: 0.3889, label: 1, bag_size: 6884\n",
      "batch 379, loss: 1.4063, instance_loss: 1.7730, weighted_loss: 1.5163, label: 0, bag_size: 2655\n",
      "batch 399, loss: 0.0695, instance_loss: 0.0512, weighted_loss: 0.0640, label: 1, bag_size: 986\n",
      "batch 419, loss: 1.1402, instance_loss: 1.4416, weighted_loss: 1.2306, label: 0, bag_size: 7171\n",
      "batch 439, loss: 0.3405, instance_loss: 0.3420, weighted_loss: 0.3410, label: 1, bag_size: 4069\n",
      "batch 459, loss: 0.0691, instance_loss: 0.0733, weighted_loss: 0.0704, label: 1, bag_size: 2005\n",
      "batch 479, loss: 0.6239, instance_loss: 0.6950, weighted_loss: 0.6453, label: 0, bag_size: 7290\n",
      "batch 499, loss: 2.3744, instance_loss: 2.8817, weighted_loss: 2.5266, label: 1, bag_size: 20149\n",
      "batch 519, loss: 0.3646, instance_loss: 0.4548, weighted_loss: 0.3917, label: 0, bag_size: 2678\n",
      "batch 539, loss: 0.2994, instance_loss: 0.3099, weighted_loss: 0.3026, label: 0, bag_size: 10096\n",
      "batch 559, loss: 0.7410, instance_loss: 0.9656, weighted_loss: 0.8084, label: 0, bag_size: 15193\n",
      "batch 579, loss: 0.5791, instance_loss: 0.6542, weighted_loss: 0.6017, label: 1, bag_size: 3338\n",
      "batch 599, loss: 0.5462, instance_loss: 0.7355, weighted_loss: 0.6030, label: 0, bag_size: 2582\n",
      "batch 619, loss: 0.2438, instance_loss: 0.2549, weighted_loss: 0.2471, label: 1, bag_size: 10091\n",
      "batch 639, loss: 0.7196, instance_loss: 0.8921, weighted_loss: 0.7714, label: 1, bag_size: 4458\n",
      "batch 659, loss: 0.2262, instance_loss: 0.2512, weighted_loss: 0.2337, label: 1, bag_size: 2407\n",
      "batch 679, loss: 0.8917, instance_loss: 1.1696, weighted_loss: 0.9751, label: 0, bag_size: 5426\n",
      "batch 699, loss: 0.3971, instance_loss: 0.5213, weighted_loss: 0.4344, label: 0, bag_size: 10096\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9408035714285714: correct 10537/11200\n",
      "class 1 clustering acc 0.3130357142857143: correct 1753/5600\n",
      "Epoch: 28, train_loss: 0.6215, train_clustering_loss:  0.7834, train_error: 0.3457\n",
      "class 0: acc 0.6828571428571428, correct 239/350\n",
      "class 1: acc 0.6257142857142857, correct 219/350\n",
      "\n",
      "Val Set, val_loss: 0.6670, val_error: 0.3505, auc: 0.6864\n",
      "class 0 clustering acc 0.7603092783505154: correct 1180/1552\n",
      "class 1 clustering acc 0.43556701030927836: correct 338/776\n",
      "class 0: acc 0.6949152542372882, correct 41/59\n",
      "class 1: acc 0.5789473684210527, correct 22/38\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.0469, instance_loss: 1.4677, weighted_loss: 1.1731, label: 1, bag_size: 3358\n",
      "batch 39, loss: 0.3876, instance_loss: 0.4900, weighted_loss: 0.4183, label: 0, bag_size: 4620\n",
      "batch 59, loss: 0.5994, instance_loss: 0.8170, weighted_loss: 0.6647, label: 1, bag_size: 9755\n",
      "batch 79, loss: 1.3421, instance_loss: 1.6745, weighted_loss: 1.4418, label: 1, bag_size: 6697\n",
      "batch 99, loss: 1.1292, instance_loss: 1.5055, weighted_loss: 1.2421, label: 1, bag_size: 1004\n",
      "batch 119, loss: 0.3834, instance_loss: 0.4619, weighted_loss: 0.4069, label: 0, bag_size: 3159\n",
      "batch 139, loss: 1.1149, instance_loss: 1.3678, weighted_loss: 1.1908, label: 1, bag_size: 4880\n",
      "batch 159, loss: 0.2021, instance_loss: 0.2402, weighted_loss: 0.2135, label: 0, bag_size: 9945\n",
      "batch 179, loss: 0.7999, instance_loss: 1.0436, weighted_loss: 0.8730, label: 0, bag_size: 5590\n",
      "batch 199, loss: 0.1274, instance_loss: 0.1490, weighted_loss: 0.1339, label: 0, bag_size: 3699\n",
      "batch 219, loss: 0.1672, instance_loss: 0.1964, weighted_loss: 0.1760, label: 0, bag_size: 1794\n",
      "batch 239, loss: 0.3828, instance_loss: 0.4685, weighted_loss: 0.4085, label: 0, bag_size: 19055\n",
      "batch 259, loss: 0.4887, instance_loss: 0.6891, weighted_loss: 0.5488, label: 0, bag_size: 1826\n",
      "batch 279, loss: 0.2551, instance_loss: 0.4023, weighted_loss: 0.2993, label: 0, bag_size: 3159\n",
      "batch 299, loss: 0.5034, instance_loss: 0.6029, weighted_loss: 0.5332, label: 0, bag_size: 3381\n",
      "batch 319, loss: 0.4233, instance_loss: 0.8263, weighted_loss: 0.5442, label: 1, bag_size: 8075\n",
      "batch 339, loss: 0.0688, instance_loss: 0.0897, weighted_loss: 0.0751, label: 1, bag_size: 5651\n",
      "batch 359, loss: 0.4236, instance_loss: 0.6351, weighted_loss: 0.4870, label: 0, bag_size: 5080\n",
      "batch 379, loss: 0.3597, instance_loss: 0.4331, weighted_loss: 0.3817, label: 0, bag_size: 18810\n",
      "batch 399, loss: 0.6090, instance_loss: 0.6859, weighted_loss: 0.6321, label: 0, bag_size: 4835\n",
      "batch 419, loss: 0.5209, instance_loss: 0.6078, weighted_loss: 0.5470, label: 0, bag_size: 4427\n",
      "batch 439, loss: 0.5434, instance_loss: 0.7813, weighted_loss: 0.6148, label: 1, bag_size: 5385\n",
      "batch 459, loss: 0.1328, instance_loss: 0.2282, weighted_loss: 0.1614, label: 0, bag_size: 7493\n",
      "batch 479, loss: 0.1055, instance_loss: 0.1713, weighted_loss: 0.1252, label: 1, bag_size: 5651\n",
      "batch 499, loss: 1.1914, instance_loss: 1.6021, weighted_loss: 1.3146, label: 0, bag_size: 2244\n",
      "batch 519, loss: 0.5748, instance_loss: 0.7538, weighted_loss: 0.6285, label: 0, bag_size: 3635\n",
      "batch 539, loss: 0.1561, instance_loss: 0.1794, weighted_loss: 0.1631, label: 0, bag_size: 3066\n",
      "batch 559, loss: 0.5394, instance_loss: 0.6250, weighted_loss: 0.5651, label: 0, bag_size: 5005\n",
      "batch 579, loss: 1.5711, instance_loss: 2.0536, weighted_loss: 1.7158, label: 0, bag_size: 4522\n",
      "batch 599, loss: 0.3256, instance_loss: 0.3456, weighted_loss: 0.3316, label: 0, bag_size: 3159\n",
      "batch 619, loss: 0.0612, instance_loss: 0.0400, weighted_loss: 0.0549, label: 1, bag_size: 5651\n",
      "batch 639, loss: 0.4912, instance_loss: 0.7106, weighted_loss: 0.5570, label: 0, bag_size: 1671\n",
      "batch 659, loss: 0.6091, instance_loss: 0.8087, weighted_loss: 0.6690, label: 1, bag_size: 9755\n",
      "batch 679, loss: 0.5329, instance_loss: 0.5531, weighted_loss: 0.5389, label: 0, bag_size: 6076\n",
      "batch 699, loss: 0.1060, instance_loss: 0.1266, weighted_loss: 0.1122, label: 1, bag_size: 5428\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9442857142857143: correct 10576/11200\n",
      "class 1 clustering acc 0.3055357142857143: correct 1711/5600\n",
      "Epoch: 29, train_loss: 0.6156, train_clustering_loss:  0.7838, train_error: 0.3286\n",
      "class 0: acc 0.6845238095238095, correct 230/336\n",
      "class 1: acc 0.6593406593406593, correct 240/364\n",
      "\n",
      "Val Set, val_loss: 0.7359, val_error: 0.4330, auc: 0.6851\n",
      "class 0 clustering acc 0.884020618556701: correct 1372/1552\n",
      "class 1 clustering acc 0.3015463917525773: correct 234/776\n",
      "class 0: acc 0.4406779661016949, correct 26/59\n",
      "class 1: acc 0.7631578947368421, correct 29/38\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4249, instance_loss: 0.5489, weighted_loss: 0.4621, label: 1, bag_size: 19013\n",
      "batch 39, loss: 2.1111, instance_loss: 2.6447, weighted_loss: 2.2711, label: 1, bag_size: 4387\n",
      "batch 59, loss: 0.1012, instance_loss: 0.0968, weighted_loss: 0.0999, label: 0, bag_size: 8866\n",
      "batch 79, loss: 0.2930, instance_loss: 0.4223, weighted_loss: 0.3318, label: 1, bag_size: 4537\n",
      "batch 99, loss: 0.9620, instance_loss: 1.2590, weighted_loss: 1.0511, label: 1, bag_size: 1699\n",
      "batch 119, loss: 0.6032, instance_loss: 0.7522, weighted_loss: 0.6479, label: 0, bag_size: 6909\n",
      "batch 139, loss: 0.2643, instance_loss: 0.3682, weighted_loss: 0.2955, label: 0, bag_size: 17711\n",
      "batch 159, loss: 1.5070, instance_loss: 2.3817, weighted_loss: 1.7694, label: 0, bag_size: 16309\n",
      "batch 179, loss: 0.4896, instance_loss: 0.6483, weighted_loss: 0.5372, label: 0, bag_size: 3704\n",
      "batch 199, loss: 0.6401, instance_loss: 0.9210, weighted_loss: 0.7244, label: 0, bag_size: 2963\n",
      "batch 219, loss: 0.6982, instance_loss: 0.8774, weighted_loss: 0.7520, label: 0, bag_size: 4427\n",
      "batch 239, loss: 0.3959, instance_loss: 0.5168, weighted_loss: 0.4322, label: 1, bag_size: 5671\n",
      "batch 259, loss: 0.4848, instance_loss: 0.5825, weighted_loss: 0.5141, label: 1, bag_size: 16936\n",
      "batch 279, loss: 0.8800, instance_loss: 1.1962, weighted_loss: 0.9749, label: 0, bag_size: 11302\n",
      "batch 299, loss: 0.2218, instance_loss: 0.3163, weighted_loss: 0.2502, label: 1, bag_size: 5428\n",
      "batch 319, loss: 1.0926, instance_loss: 1.2909, weighted_loss: 1.1521, label: 1, bag_size: 1961\n",
      "batch 339, loss: 0.8385, instance_loss: 1.1021, weighted_loss: 0.9176, label: 0, bag_size: 4649\n",
      "batch 359, loss: 0.4867, instance_loss: 0.6287, weighted_loss: 0.5293, label: 0, bag_size: 2006\n",
      "batch 379, loss: 0.8433, instance_loss: 1.0069, weighted_loss: 0.8924, label: 0, bag_size: 4515\n",
      "batch 399, loss: 0.7775, instance_loss: 0.9389, weighted_loss: 0.8259, label: 1, bag_size: 11513\n",
      "batch 419, loss: 0.0038, instance_loss: 0.0037, weighted_loss: 0.0038, label: 1, bag_size: 5218\n",
      "batch 439, loss: 0.2253, instance_loss: 0.2738, weighted_loss: 0.2398, label: 1, bag_size: 6752\n",
      "batch 459, loss: 0.2726, instance_loss: 0.3372, weighted_loss: 0.2920, label: 1, bag_size: 2943\n",
      "batch 479, loss: 0.8143, instance_loss: 1.1878, weighted_loss: 0.9263, label: 1, bag_size: 6477\n",
      "batch 499, loss: 0.2704, instance_loss: 0.2710, weighted_loss: 0.2706, label: 0, bag_size: 3636\n",
      "batch 519, loss: 0.5871, instance_loss: 0.6482, weighted_loss: 0.6054, label: 0, bag_size: 3105\n",
      "batch 539, loss: 0.2109, instance_loss: 0.2249, weighted_loss: 0.2151, label: 0, bag_size: 5924\n",
      "batch 559, loss: 0.9261, instance_loss: 1.0585, weighted_loss: 0.9659, label: 0, bag_size: 1648\n",
      "batch 579, loss: 0.4309, instance_loss: 0.5083, weighted_loss: 0.4541, label: 0, bag_size: 3732\n",
      "batch 599, loss: 1.6802, instance_loss: 2.1983, weighted_loss: 1.8356, label: 1, bag_size: 21711\n",
      "batch 619, loss: 2.6847, instance_loss: 3.0256, weighted_loss: 2.7870, label: 0, bag_size: 21335\n",
      "batch 639, loss: 0.4032, instance_loss: 0.2184, weighted_loss: 0.3477, label: 0, bag_size: 1892\n",
      "batch 659, loss: 0.8448, instance_loss: 1.0610, weighted_loss: 0.9096, label: 1, bag_size: 3338\n",
      "batch 679, loss: 0.4534, instance_loss: 0.6726, weighted_loss: 0.5191, label: 0, bag_size: 999\n",
      "batch 699, loss: 0.4407, instance_loss: 0.8594, weighted_loss: 0.5663, label: 1, bag_size: 3834\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9448214285714286: correct 10582/11200\n",
      "class 1 clustering acc 0.34535714285714286: correct 1934/5600\n",
      "Epoch: 30, train_loss: 0.6183, train_clustering_loss:  0.7812, train_error: 0.3500\n",
      "class 0: acc 0.6695906432748538, correct 229/342\n",
      "class 1: acc 0.6312849162011173, correct 226/358\n",
      "\n",
      "Val Set, val_loss: 0.6642, val_error: 0.3918, auc: 0.6855\n",
      "class 0 clustering acc 1.0: correct 1552/1552\n",
      "class 1 clustering acc 0.0: correct 0/776\n",
      "class 0: acc 0.5423728813559322, correct 32/59\n",
      "class 1: acc 0.7105263157894737, correct 27/38\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5003, instance_loss: 0.6597, weighted_loss: 0.5481, label: 0, bag_size: 1944\n",
      "batch 39, loss: 0.8676, instance_loss: 1.1447, weighted_loss: 0.9507, label: 1, bag_size: 2961\n",
      "batch 59, loss: 0.7030, instance_loss: 0.8586, weighted_loss: 0.7497, label: 0, bag_size: 3714\n",
      "batch 79, loss: 0.3880, instance_loss: 0.5350, weighted_loss: 0.4321, label: 1, bag_size: 5777\n",
      "batch 99, loss: 0.2618, instance_loss: 0.5164, weighted_loss: 0.3382, label: 1, bag_size: 8007\n",
      "batch 119, loss: 1.2138, instance_loss: 1.3884, weighted_loss: 1.2662, label: 1, bag_size: 11968\n",
      "batch 139, loss: 0.2513, instance_loss: 0.2892, weighted_loss: 0.2627, label: 1, bag_size: 2905\n",
      "batch 159, loss: 1.8265, instance_loss: 1.8738, weighted_loss: 1.8407, label: 1, bag_size: 2293\n",
      "batch 179, loss: 0.3490, instance_loss: 0.4254, weighted_loss: 0.3719, label: 0, bag_size: 15912\n",
      "batch 199, loss: 0.5823, instance_loss: 0.7579, weighted_loss: 0.6350, label: 1, bag_size: 4159\n",
      "batch 219, loss: 0.3144, instance_loss: 0.3865, weighted_loss: 0.3361, label: 0, bag_size: 15474\n",
      "batch 239, loss: 0.5001, instance_loss: 0.5152, weighted_loss: 0.5046, label: 0, bag_size: 17711\n",
      "batch 259, loss: 1.2800, instance_loss: 1.6063, weighted_loss: 1.3779, label: 0, bag_size: 2851\n",
      "batch 279, loss: 0.9332, instance_loss: 1.2155, weighted_loss: 1.0178, label: 0, bag_size: 2791\n",
      "batch 299, loss: 0.4793, instance_loss: 0.7204, weighted_loss: 0.5516, label: 1, bag_size: 3226\n",
      "batch 319, loss: 0.3945, instance_loss: 0.4014, weighted_loss: 0.3965, label: 1, bag_size: 2899\n",
      "batch 339, loss: 0.4585, instance_loss: 0.8160, weighted_loss: 0.5658, label: 1, bag_size: 1495\n",
      "batch 359, loss: 0.2951, instance_loss: 0.6685, weighted_loss: 0.4071, label: 1, bag_size: 3402\n",
      "batch 379, loss: 1.1450, instance_loss: 1.2483, weighted_loss: 1.1760, label: 0, bag_size: 3196\n",
      "batch 399, loss: 0.2332, instance_loss: 0.5346, weighted_loss: 0.3236, label: 0, bag_size: 5960\n",
      "batch 419, loss: 0.4005, instance_loss: 0.5204, weighted_loss: 0.4365, label: 1, bag_size: 5156\n",
      "batch 439, loss: 1.1805, instance_loss: 1.5310, weighted_loss: 1.2856, label: 1, bag_size: 2447\n",
      "batch 459, loss: 0.6688, instance_loss: 0.9713, weighted_loss: 0.7595, label: 0, bag_size: 3317\n",
      "batch 479, loss: 0.8569, instance_loss: 1.0926, weighted_loss: 0.9277, label: 1, bag_size: 15434\n",
      "batch 499, loss: 0.4590, instance_loss: 0.8581, weighted_loss: 0.5787, label: 1, bag_size: 5637\n",
      "batch 519, loss: 0.4970, instance_loss: 0.6861, weighted_loss: 0.5537, label: 1, bag_size: 5819\n",
      "batch 539, loss: 0.1697, instance_loss: 0.3343, weighted_loss: 0.2190, label: 1, bag_size: 5823\n",
      "batch 559, loss: 0.5437, instance_loss: 0.6597, weighted_loss: 0.5785, label: 1, bag_size: 6421\n",
      "batch 579, loss: 0.3361, instance_loss: 0.4146, weighted_loss: 0.3597, label: 1, bag_size: 5810\n",
      "batch 599, loss: 1.5501, instance_loss: 1.7586, weighted_loss: 1.6127, label: 1, bag_size: 4384\n",
      "batch 619, loss: 1.0649, instance_loss: 1.3389, weighted_loss: 1.1471, label: 0, bag_size: 4254\n",
      "batch 639, loss: 0.3098, instance_loss: 0.4025, weighted_loss: 0.3376, label: 0, bag_size: 3084\n",
      "batch 659, loss: 0.3916, instance_loss: 0.4669, weighted_loss: 0.4142, label: 0, bag_size: 7179\n",
      "batch 679, loss: 0.2632, instance_loss: 0.2949, weighted_loss: 0.2727, label: 0, bag_size: 3290\n",
      "batch 699, loss: 1.8218, instance_loss: 2.2941, weighted_loss: 1.9635, label: 0, bag_size: 6047\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9485714285714286: correct 10624/11200\n",
      "class 1 clustering acc 0.26142857142857145: correct 1464/5600\n",
      "Epoch: 31, train_loss: 0.6248, train_clustering_loss:  0.8405, train_error: 0.3300\n",
      "class 0: acc 0.6818181818181818, correct 240/352\n",
      "class 1: acc 0.6580459770114943, correct 229/348\n",
      "\n",
      "Val Set, val_loss: 0.7487, val_error: 0.4433, auc: 0.6753\n",
      "class 0 clustering acc 0.916881443298969: correct 1423/1552\n",
      "class 1 clustering acc 0.23195876288659795: correct 180/776\n",
      "class 0: acc 0.4576271186440678, correct 27/59\n",
      "class 1: acc 0.7105263157894737, correct 27/38\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1392, instance_loss: 0.1906, weighted_loss: 0.1546, label: 0, bag_size: 4124\n",
      "batch 39, loss: 0.7518, instance_loss: 0.8123, weighted_loss: 0.7699, label: 0, bag_size: 4654\n",
      "batch 59, loss: 0.5568, instance_loss: 0.6442, weighted_loss: 0.5830, label: 1, bag_size: 7184\n",
      "batch 79, loss: 0.2089, instance_loss: 0.2098, weighted_loss: 0.2091, label: 1, bag_size: 6410\n",
      "batch 99, loss: 0.9857, instance_loss: 1.2714, weighted_loss: 1.0715, label: 0, bag_size: 5409\n",
      "batch 119, loss: 0.7406, instance_loss: 0.8874, weighted_loss: 0.7846, label: 1, bag_size: 6759\n",
      "batch 139, loss: 1.1760, instance_loss: 1.5132, weighted_loss: 1.2772, label: 1, bag_size: 21105\n",
      "batch 159, loss: 0.6817, instance_loss: 0.7579, weighted_loss: 0.7045, label: 0, bag_size: 2152\n",
      "batch 179, loss: 0.9493, instance_loss: 1.0813, weighted_loss: 0.9889, label: 1, bag_size: 6463\n",
      "batch 199, loss: 0.8385, instance_loss: 1.1502, weighted_loss: 0.9320, label: 0, bag_size: 12128\n",
      "batch 219, loss: 0.4852, instance_loss: 0.6210, weighted_loss: 0.5260, label: 0, bag_size: 1892\n",
      "batch 239, loss: 0.2812, instance_loss: 0.3827, weighted_loss: 0.3116, label: 1, bag_size: 1495\n",
      "batch 259, loss: 0.3193, instance_loss: 0.4121, weighted_loss: 0.3472, label: 0, bag_size: 2048\n",
      "batch 279, loss: 0.0824, instance_loss: 0.0951, weighted_loss: 0.0862, label: 0, bag_size: 5527\n",
      "batch 299, loss: 0.6985, instance_loss: 0.7972, weighted_loss: 0.7281, label: 0, bag_size: 10590\n",
      "batch 319, loss: 0.1683, instance_loss: 0.1906, weighted_loss: 0.1750, label: 0, bag_size: 5005\n",
      "batch 339, loss: 1.2782, instance_loss: 1.5927, weighted_loss: 1.3726, label: 0, bag_size: 1822\n",
      "batch 359, loss: 0.1836, instance_loss: 0.1376, weighted_loss: 0.1698, label: 0, bag_size: 14662\n",
      "batch 379, loss: 0.9122, instance_loss: 1.1619, weighted_loss: 0.9871, label: 1, bag_size: 3338\n",
      "batch 399, loss: 0.4810, instance_loss: 0.4780, weighted_loss: 0.4801, label: 0, bag_size: 3895\n",
      "batch 419, loss: 0.9292, instance_loss: 1.1135, weighted_loss: 0.9845, label: 0, bag_size: 3381\n",
      "batch 439, loss: 0.6649, instance_loss: 0.8736, weighted_loss: 0.7275, label: 1, bag_size: 6463\n",
      "batch 459, loss: 0.6468, instance_loss: 0.8775, weighted_loss: 0.7160, label: 1, bag_size: 18015\n",
      "batch 479, loss: 0.2409, instance_loss: 0.2928, weighted_loss: 0.2564, label: 0, bag_size: 9945\n",
      "batch 499, loss: 1.0717, instance_loss: 1.4751, weighted_loss: 1.1927, label: 1, bag_size: 6371\n",
      "batch 519, loss: 0.3600, instance_loss: 0.4874, weighted_loss: 0.3982, label: 1, bag_size: 3420\n",
      "batch 539, loss: 1.2960, instance_loss: 1.7519, weighted_loss: 1.4327, label: 0, bag_size: 2022\n",
      "batch 559, loss: 0.1743, instance_loss: 0.1606, weighted_loss: 0.1702, label: 0, bag_size: 13411\n",
      "batch 579, loss: 0.5366, instance_loss: 0.6667, weighted_loss: 0.5756, label: 1, bag_size: 4510\n",
      "batch 599, loss: 0.2805, instance_loss: 0.3303, weighted_loss: 0.2955, label: 0, bag_size: 5960\n",
      "batch 619, loss: 0.6659, instance_loss: 0.8722, weighted_loss: 0.7278, label: 1, bag_size: 1746\n",
      "batch 639, loss: 0.3640, instance_loss: 0.4277, weighted_loss: 0.3831, label: 0, bag_size: 14856\n",
      "batch 659, loss: 0.3800, instance_loss: 0.4924, weighted_loss: 0.4137, label: 1, bag_size: 3518\n",
      "batch 679, loss: 0.4349, instance_loss: 0.5531, weighted_loss: 0.4704, label: 1, bag_size: 6884\n",
      "batch 699, loss: 1.2122, instance_loss: 1.5665, weighted_loss: 1.3185, label: 0, bag_size: 6491\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9434821428571428: correct 10567/11200\n",
      "class 1 clustering acc 0.29910714285714285: correct 1675/5600\n",
      "Epoch: 32, train_loss: 0.6230, train_clustering_loss:  0.7832, train_error: 0.3471\n",
      "class 0: acc 0.6657223796033994, correct 235/353\n",
      "class 1: acc 0.6397694524495677, correct 222/347\n",
      "\n",
      "Val Set, val_loss: 0.7355, val_error: 0.4536, auc: 0.6784\n",
      "class 0 clustering acc 0.7847938144329897: correct 1218/1552\n",
      "class 1 clustering acc 0.3853092783505155: correct 299/776\n",
      "class 0: acc 0.423728813559322, correct 25/59\n",
      "class 1: acc 0.7368421052631579, correct 28/38\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3947, instance_loss: 0.4198, weighted_loss: 0.4022, label: 1, bag_size: 11968\n",
      "batch 39, loss: 0.8565, instance_loss: 0.9345, weighted_loss: 0.8799, label: 1, bag_size: 5671\n",
      "batch 59, loss: 0.2560, instance_loss: 0.3865, weighted_loss: 0.2951, label: 1, bag_size: 5108\n",
      "batch 79, loss: 0.3047, instance_loss: 0.4002, weighted_loss: 0.3334, label: 0, bag_size: 2110\n",
      "batch 99, loss: 1.0808, instance_loss: 1.2638, weighted_loss: 1.1357, label: 1, bag_size: 4087\n",
      "batch 119, loss: 0.6368, instance_loss: 0.8205, weighted_loss: 0.6919, label: 1, bag_size: 3910\n",
      "batch 139, loss: 0.5638, instance_loss: 0.7463, weighted_loss: 0.6186, label: 0, bag_size: 5536\n",
      "batch 159, loss: 0.9819, instance_loss: 1.2100, weighted_loss: 1.0503, label: 0, bag_size: 2480\n",
      "batch 179, loss: 0.2350, instance_loss: 0.2452, weighted_loss: 0.2381, label: 1, bag_size: 5810\n",
      "batch 199, loss: 0.6130, instance_loss: 0.8715, weighted_loss: 0.6905, label: 1, bag_size: 2506\n",
      "batch 219, loss: 0.6500, instance_loss: 0.8161, weighted_loss: 0.6998, label: 1, bag_size: 5671\n",
      "batch 239, loss: 1.5722, instance_loss: 1.9392, weighted_loss: 1.6823, label: 0, bag_size: 3489\n",
      "batch 259, loss: 0.6717, instance_loss: 0.9824, weighted_loss: 0.7650, label: 0, bag_size: 16859\n",
      "batch 279, loss: 0.3607, instance_loss: 0.4979, weighted_loss: 0.4019, label: 0, bag_size: 1828\n",
      "batch 299, loss: 1.0340, instance_loss: 1.4161, weighted_loss: 1.1486, label: 1, bag_size: 15213\n",
      "batch 319, loss: 0.9455, instance_loss: 1.3120, weighted_loss: 1.0554, label: 0, bag_size: 2638\n",
      "batch 339, loss: 0.9429, instance_loss: 1.2433, weighted_loss: 1.0330, label: 1, bag_size: 14564\n",
      "batch 359, loss: 1.9198, instance_loss: 2.4089, weighted_loss: 2.0665, label: 0, bag_size: 1826\n",
      "batch 379, loss: 0.1422, instance_loss: 0.2963, weighted_loss: 0.1885, label: 1, bag_size: 1627\n",
      "batch 399, loss: 0.7183, instance_loss: 0.8841, weighted_loss: 0.7681, label: 1, bag_size: 1808\n",
      "batch 419, loss: 0.9387, instance_loss: 1.4797, weighted_loss: 1.1010, label: 0, bag_size: 2022\n",
      "batch 439, loss: 0.7789, instance_loss: 1.0207, weighted_loss: 0.8515, label: 0, bag_size: 3084\n",
      "batch 459, loss: 0.4285, instance_loss: 0.4487, weighted_loss: 0.4345, label: 1, bag_size: 3824\n",
      "batch 479, loss: 0.8873, instance_loss: 1.1565, weighted_loss: 0.9681, label: 1, bag_size: 1587\n",
      "batch 499, loss: 0.4713, instance_loss: 0.5765, weighted_loss: 0.5029, label: 0, bag_size: 4332\n",
      "batch 519, loss: 0.3072, instance_loss: 0.4150, weighted_loss: 0.3395, label: 1, bag_size: 5068\n",
      "batch 539, loss: 0.8906, instance_loss: 1.2047, weighted_loss: 0.9848, label: 0, bag_size: 3724\n",
      "batch 559, loss: 0.2463, instance_loss: 0.2661, weighted_loss: 0.2523, label: 1, bag_size: 10091\n",
      "batch 579, loss: 0.7750, instance_loss: 0.9481, weighted_loss: 0.8269, label: 0, bag_size: 4179\n",
      "batch 599, loss: 0.5829, instance_loss: 0.7417, weighted_loss: 0.6306, label: 1, bag_size: 6861\n",
      "batch 619, loss: 0.2565, instance_loss: 0.4116, weighted_loss: 0.3030, label: 0, bag_size: 3011\n",
      "batch 639, loss: 0.8047, instance_loss: 1.0829, weighted_loss: 0.8882, label: 1, bag_size: 3226\n",
      "batch 659, loss: 0.3073, instance_loss: 0.3205, weighted_loss: 0.3113, label: 0, bag_size: 5960\n",
      "batch 679, loss: 0.7036, instance_loss: 0.9021, weighted_loss: 0.7631, label: 1, bag_size: 4243\n",
      "batch 699, loss: 0.9344, instance_loss: 0.9253, weighted_loss: 0.9317, label: 1, bag_size: 22843\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9404464285714286: correct 10533/11200\n",
      "class 1 clustering acc 0.34160714285714283: correct 1913/5600\n",
      "Epoch: 33, train_loss: 0.6109, train_clustering_loss:  0.7668, train_error: 0.3343\n",
      "class 0: acc 0.6715116279069767, correct 231/344\n",
      "class 1: acc 0.6601123595505618, correct 235/356\n",
      "\n",
      "Val Set, val_loss: 0.7346, val_error: 0.3814, auc: 0.6789\n",
      "class 0 clustering acc 0.6346649484536082: correct 985/1552\n",
      "class 1 clustering acc 0.5670103092783505: correct 440/776\n",
      "class 0: acc 0.9830508474576272, correct 58/59\n",
      "class 1: acc 0.05263157894736842, correct 2/38\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7387, instance_loss: 0.8492, weighted_loss: 0.7719, label: 0, bag_size: 4228\n",
      "batch 39, loss: 0.4946, instance_loss: 0.4547, weighted_loss: 0.4826, label: 1, bag_size: 10736\n",
      "batch 59, loss: 0.5366, instance_loss: 0.7670, weighted_loss: 0.6058, label: 1, bag_size: 2783\n",
      "batch 79, loss: 0.6161, instance_loss: 0.7854, weighted_loss: 0.6669, label: 0, bag_size: 3504\n",
      "batch 99, loss: 0.5285, instance_loss: 0.5984, weighted_loss: 0.5495, label: 1, bag_size: 6600\n",
      "batch 119, loss: 0.7930, instance_loss: 0.9552, weighted_loss: 0.8417, label: 1, bag_size: 1587\n",
      "batch 139, loss: 1.0915, instance_loss: 1.3247, weighted_loss: 1.1614, label: 0, bag_size: 1226\n",
      "batch 159, loss: 0.4365, instance_loss: 0.5320, weighted_loss: 0.4652, label: 1, bag_size: 5777\n",
      "batch 179, loss: 0.7146, instance_loss: 0.9272, weighted_loss: 0.7784, label: 1, bag_size: 6371\n",
      "batch 199, loss: 0.4204, instance_loss: 0.6164, weighted_loss: 0.4792, label: 0, bag_size: 4175\n",
      "batch 219, loss: 0.8158, instance_loss: 1.0992, weighted_loss: 0.9008, label: 1, bag_size: 5385\n",
      "batch 239, loss: 0.2433, instance_loss: 0.4067, weighted_loss: 0.2923, label: 1, bag_size: 1272\n",
      "batch 259, loss: 0.1196, instance_loss: 0.1012, weighted_loss: 0.1141, label: 0, bag_size: 3732\n",
      "batch 279, loss: 1.4791, instance_loss: 1.8209, weighted_loss: 1.5817, label: 0, bag_size: 3256\n",
      "batch 299, loss: 0.2487, instance_loss: 0.2648, weighted_loss: 0.2535, label: 0, bag_size: 1250\n",
      "batch 319, loss: 0.7919, instance_loss: 1.0963, weighted_loss: 0.8832, label: 0, bag_size: 11758\n",
      "batch 339, loss: 0.9755, instance_loss: 1.3487, weighted_loss: 1.0875, label: 0, bag_size: 5487\n",
      "batch 359, loss: 1.3363, instance_loss: 1.6655, weighted_loss: 1.4350, label: 0, bag_size: 4079\n",
      "batch 379, loss: 0.2953, instance_loss: 0.2822, weighted_loss: 0.2913, label: 1, bag_size: 3391\n",
      "batch 399, loss: 1.4356, instance_loss: 1.6904, weighted_loss: 1.5121, label: 0, bag_size: 2791\n",
      "batch 419, loss: 0.1277, instance_loss: 0.1288, weighted_loss: 0.1281, label: 0, bag_size: 3699\n",
      "batch 439, loss: 0.3938, instance_loss: 0.5068, weighted_loss: 0.4277, label: 1, bag_size: 2493\n",
      "batch 459, loss: 0.5760, instance_loss: 0.7322, weighted_loss: 0.6229, label: 1, bag_size: 3143\n",
      "batch 479, loss: 0.5372, instance_loss: 0.6683, weighted_loss: 0.5765, label: 1, bag_size: 4722\n",
      "batch 499, loss: 0.9975, instance_loss: 1.1691, weighted_loss: 1.0490, label: 1, bag_size: 2253\n",
      "batch 519, loss: 0.2173, instance_loss: 0.2228, weighted_loss: 0.2189, label: 0, bag_size: 3427\n",
      "batch 539, loss: 1.5015, instance_loss: 1.9127, weighted_loss: 1.6249, label: 1, bag_size: 21711\n",
      "batch 559, loss: 0.8870, instance_loss: 1.1476, weighted_loss: 0.9652, label: 0, bag_size: 5458\n",
      "batch 579, loss: 2.1305, instance_loss: 2.7476, weighted_loss: 2.3156, label: 1, bag_size: 20149\n",
      "batch 599, loss: 0.8883, instance_loss: 1.2319, weighted_loss: 0.9914, label: 1, bag_size: 4510\n",
      "batch 619, loss: 0.2275, instance_loss: 0.2500, weighted_loss: 0.2343, label: 1, bag_size: 3081\n",
      "batch 639, loss: 0.5031, instance_loss: 0.5749, weighted_loss: 0.5247, label: 0, bag_size: 9945\n",
      "batch 659, loss: 2.6135, instance_loss: 3.1669, weighted_loss: 2.7795, label: 0, bag_size: 1885\n",
      "batch 679, loss: 1.8117, instance_loss: 2.1938, weighted_loss: 1.9264, label: 1, bag_size: 4510\n",
      "batch 699, loss: 0.3809, instance_loss: 0.3726, weighted_loss: 0.3784, label: 0, bag_size: 4649\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9440178571428571: correct 10573/11200\n",
      "class 1 clustering acc 0.3530357142857143: correct 1977/5600\n",
      "Epoch: 34, train_loss: 0.6084, train_clustering_loss:  0.7599, train_error: 0.3457\n",
      "class 0: acc 0.6772334293948127, correct 235/347\n",
      "class 1: acc 0.6317280453257791, correct 223/353\n",
      "\n",
      "Val Set, val_loss: 0.6279, val_error: 0.3505, auc: 0.6824\n",
      "class 0 clustering acc 0.9310567010309279: correct 1445/1552\n",
      "class 1 clustering acc 0.4265463917525773: correct 331/776\n",
      "class 0: acc 0.9152542372881356, correct 54/59\n",
      "class 1: acc 0.23684210526315788, correct 9/38\n",
      "EarlyStopping counter: 12 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2629, instance_loss: 0.2401, weighted_loss: 0.2560, label: 0, bag_size: 3290\n",
      "batch 39, loss: 1.8042, instance_loss: 2.2609, weighted_loss: 1.9412, label: 1, bag_size: 14306\n",
      "batch 59, loss: 0.0577, instance_loss: 0.1791, weighted_loss: 0.0941, label: 0, bag_size: 938\n",
      "batch 79, loss: 0.2457, instance_loss: 0.2494, weighted_loss: 0.2469, label: 0, bag_size: 13606\n",
      "batch 99, loss: 0.7199, instance_loss: 0.8969, weighted_loss: 0.7730, label: 1, bag_size: 1495\n",
      "batch 119, loss: 0.4601, instance_loss: 0.5284, weighted_loss: 0.4806, label: 0, bag_size: 2908\n",
      "batch 139, loss: 0.5306, instance_loss: 0.6141, weighted_loss: 0.5556, label: 1, bag_size: 4800\n",
      "batch 159, loss: 0.0929, instance_loss: 0.0881, weighted_loss: 0.0914, label: 1, bag_size: 7641\n",
      "batch 179, loss: 0.3122, instance_loss: 0.3417, weighted_loss: 0.3210, label: 0, bag_size: 4175\n",
      "batch 199, loss: 0.1788, instance_loss: 0.2069, weighted_loss: 0.1872, label: 1, bag_size: 4159\n",
      "batch 219, loss: 0.9993, instance_loss: 1.4797, weighted_loss: 1.1435, label: 0, bag_size: 23860\n",
      "batch 239, loss: 1.1620, instance_loss: 1.3818, weighted_loss: 1.2280, label: 1, bag_size: 6841\n",
      "batch 259, loss: 0.4752, instance_loss: 0.8385, weighted_loss: 0.5842, label: 1, bag_size: 2840\n",
      "batch 279, loss: 1.3829, instance_loss: 1.5295, weighted_loss: 1.4269, label: 1, bag_size: 3893\n",
      "batch 299, loss: 0.3862, instance_loss: 0.5956, weighted_loss: 0.4490, label: 0, bag_size: 3727\n",
      "batch 319, loss: 0.8559, instance_loss: 0.9585, weighted_loss: 0.8867, label: 1, bag_size: 5108\n",
      "batch 339, loss: 1.2903, instance_loss: 1.5262, weighted_loss: 1.3610, label: 0, bag_size: 4441\n",
      "batch 359, loss: 0.6449, instance_loss: 0.8263, weighted_loss: 0.6993, label: 1, bag_size: 2624\n",
      "batch 379, loss: 0.3945, instance_loss: 0.6045, weighted_loss: 0.4575, label: 1, bag_size: 1178\n",
      "batch 399, loss: 0.3894, instance_loss: 0.4135, weighted_loss: 0.3967, label: 0, bag_size: 5477\n",
      "batch 419, loss: 0.6707, instance_loss: 0.8415, weighted_loss: 0.7219, label: 1, bag_size: 3405\n",
      "batch 439, loss: 0.5712, instance_loss: 0.7590, weighted_loss: 0.6275, label: 0, bag_size: 2432\n",
      "batch 459, loss: 0.7127, instance_loss: 1.0006, weighted_loss: 0.7991, label: 0, bag_size: 2167\n",
      "batch 479, loss: 0.8328, instance_loss: 1.1995, weighted_loss: 0.9428, label: 1, bag_size: 3966\n",
      "batch 499, loss: 1.2006, instance_loss: 1.5590, weighted_loss: 1.3082, label: 0, bag_size: 3264\n",
      "batch 519, loss: 0.2405, instance_loss: 0.2480, weighted_loss: 0.2427, label: 0, bag_size: 5370\n",
      "batch 539, loss: 0.3380, instance_loss: 0.3472, weighted_loss: 0.3408, label: 0, bag_size: 2432\n",
      "batch 559, loss: 0.6106, instance_loss: 0.8371, weighted_loss: 0.6785, label: 0, bag_size: 3857\n",
      "batch 579, loss: 0.3034, instance_loss: 0.4149, weighted_loss: 0.3369, label: 0, bag_size: 3277\n",
      "batch 599, loss: 0.5679, instance_loss: 0.7283, weighted_loss: 0.6161, label: 1, bag_size: 4819\n",
      "batch 619, loss: 0.5801, instance_loss: 0.8041, weighted_loss: 0.6473, label: 0, bag_size: 2388\n",
      "batch 639, loss: 0.3671, instance_loss: 0.4693, weighted_loss: 0.3978, label: 1, bag_size: 2961\n",
      "batch 659, loss: 1.0855, instance_loss: 1.2590, weighted_loss: 1.1376, label: 0, bag_size: 4597\n",
      "batch 679, loss: 1.8948, instance_loss: 2.1353, weighted_loss: 1.9669, label: 0, bag_size: 2638\n",
      "batch 699, loss: 0.3031, instance_loss: 0.3613, weighted_loss: 0.3206, label: 1, bag_size: 4731\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9372321428571428: correct 10497/11200\n",
      "class 1 clustering acc 0.3717857142857143: correct 2082/5600\n",
      "Epoch: 35, train_loss: 0.6040, train_clustering_loss:  0.7632, train_error: 0.3271\n",
      "class 0: acc 0.7213114754098361, correct 264/366\n",
      "class 1: acc 0.6197604790419161, correct 207/334\n",
      "\n",
      "Val Set, val_loss: 0.6289, val_error: 0.3402, auc: 0.6820\n",
      "class 0 clustering acc 0.9252577319587629: correct 1436/1552\n",
      "class 1 clustering acc 0.44458762886597936: correct 345/776\n",
      "class 0: acc 0.8813559322033898, correct 52/59\n",
      "class 1: acc 0.3157894736842105, correct 12/38\n",
      "EarlyStopping counter: 13 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2789, instance_loss: 0.2650, weighted_loss: 0.2747, label: 0, bag_size: 3347\n",
      "batch 39, loss: 0.5711, instance_loss: 0.6764, weighted_loss: 0.6027, label: 1, bag_size: 5108\n",
      "batch 59, loss: 0.7348, instance_loss: 0.9541, weighted_loss: 0.8006, label: 0, bag_size: 4769\n",
      "batch 79, loss: 0.5039, instance_loss: 0.5714, weighted_loss: 0.5241, label: 1, bag_size: 11968\n",
      "batch 99, loss: 1.5563, instance_loss: 2.0573, weighted_loss: 1.7066, label: 1, bag_size: 4387\n",
      "batch 119, loss: 0.2379, instance_loss: 0.2690, weighted_loss: 0.2472, label: 0, bag_size: 5120\n",
      "batch 139, loss: 0.5923, instance_loss: 0.7495, weighted_loss: 0.6395, label: 1, bag_size: 3208\n",
      "batch 159, loss: 0.6613, instance_loss: 0.9208, weighted_loss: 0.7392, label: 0, bag_size: 6058\n",
      "batch 179, loss: 0.6469, instance_loss: 0.8543, weighted_loss: 0.7091, label: 1, bag_size: 5379\n",
      "batch 199, loss: 0.3263, instance_loss: 0.4065, weighted_loss: 0.3504, label: 0, bag_size: 14281\n",
      "batch 219, loss: 0.5183, instance_loss: 0.6440, weighted_loss: 0.5560, label: 0, bag_size: 24280\n",
      "batch 239, loss: 0.3804, instance_loss: 0.4313, weighted_loss: 0.3956, label: 0, bag_size: 2152\n",
      "batch 259, loss: 0.2285, instance_loss: 0.2129, weighted_loss: 0.2238, label: 0, bag_size: 4835\n",
      "batch 279, loss: 1.4033, instance_loss: 1.7728, weighted_loss: 1.5141, label: 1, bag_size: 15434\n",
      "batch 299, loss: 0.3674, instance_loss: 0.4765, weighted_loss: 0.4001, label: 0, bag_size: 5522\n",
      "batch 319, loss: 0.3154, instance_loss: 0.3465, weighted_loss: 0.3248, label: 0, bag_size: 11256\n",
      "batch 339, loss: 0.2621, instance_loss: 0.3843, weighted_loss: 0.2988, label: 1, bag_size: 5152\n",
      "batch 359, loss: 0.5793, instance_loss: 0.7379, weighted_loss: 0.6268, label: 0, bag_size: 2205\n",
      "batch 379, loss: 0.2897, instance_loss: 0.4013, weighted_loss: 0.3232, label: 0, bag_size: 3228\n",
      "batch 399, loss: 0.7623, instance_loss: 0.9552, weighted_loss: 0.8202, label: 0, bag_size: 4694\n",
      "batch 419, loss: 0.3693, instance_loss: 0.3777, weighted_loss: 0.3718, label: 0, bag_size: 11451\n",
      "batch 439, loss: 0.2461, instance_loss: 0.2384, weighted_loss: 0.2438, label: 0, bag_size: 5108\n",
      "batch 459, loss: 0.1588, instance_loss: 0.1663, weighted_loss: 0.1610, label: 1, bag_size: 4069\n",
      "batch 479, loss: 0.4992, instance_loss: 0.6575, weighted_loss: 0.5467, label: 0, bag_size: 3090\n",
      "batch 499, loss: 1.1530, instance_loss: 1.4113, weighted_loss: 1.2305, label: 1, bag_size: 6697\n",
      "batch 519, loss: 0.4211, instance_loss: 0.5372, weighted_loss: 0.4560, label: 0, bag_size: 21574\n",
      "batch 539, loss: 0.9364, instance_loss: 1.1537, weighted_loss: 1.0016, label: 1, bag_size: 4853\n",
      "batch 559, loss: 0.4051, instance_loss: 0.4620, weighted_loss: 0.4221, label: 0, bag_size: 6909\n",
      "batch 579, loss: 0.1262, instance_loss: 0.1088, weighted_loss: 0.1210, label: 0, bag_size: 16859\n",
      "batch 599, loss: 0.1636, instance_loss: 0.1634, weighted_loss: 0.1635, label: 1, bag_size: 19013\n",
      "batch 619, loss: 0.8813, instance_loss: 1.1151, weighted_loss: 0.9514, label: 1, bag_size: 5939\n",
      "batch 639, loss: 0.1159, instance_loss: 0.1055, weighted_loss: 0.1128, label: 0, bag_size: 17546\n",
      "batch 659, loss: 1.2084, instance_loss: 1.5248, weighted_loss: 1.3033, label: 1, bag_size: 3925\n",
      "batch 679, loss: 0.5894, instance_loss: 0.8028, weighted_loss: 0.6535, label: 1, bag_size: 21711\n",
      "batch 699, loss: 1.7934, instance_loss: 2.3010, weighted_loss: 1.9457, label: 1, bag_size: 15483\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9417857142857143: correct 10548/11200\n",
      "class 1 clustering acc 0.45982142857142855: correct 2575/5600\n",
      "Epoch: 36, train_loss: 0.5542, train_clustering_loss:  0.6947, train_error: 0.2943\n",
      "class 0: acc 0.7589041095890411, correct 277/365\n",
      "class 1: acc 0.6477611940298508, correct 217/335\n",
      "\n",
      "Val Set, val_loss: 0.6281, val_error: 0.2887, auc: 0.6896\n",
      "class 0 clustering acc 0.8518041237113402: correct 1322/1552\n",
      "class 1 clustering acc 0.422680412371134: correct 328/776\n",
      "class 0: acc 0.7966101694915254, correct 47/59\n",
      "class 1: acc 0.5789473684210527, correct 22/38\n",
      "EarlyStopping counter: 14 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4459, instance_loss: 0.5567, weighted_loss: 0.4791, label: 0, bag_size: 4181\n",
      "batch 39, loss: 0.9506, instance_loss: 1.3024, weighted_loss: 1.0562, label: 0, bag_size: 8088\n",
      "batch 59, loss: 0.2878, instance_loss: 0.3162, weighted_loss: 0.2963, label: 0, bag_size: 1976\n",
      "batch 79, loss: 0.5441, instance_loss: 0.8156, weighted_loss: 0.6256, label: 1, bag_size: 2624\n",
      "batch 99, loss: 0.4201, instance_loss: 0.5422, weighted_loss: 0.4567, label: 0, bag_size: 12673\n",
      "batch 119, loss: 0.2347, instance_loss: 0.2511, weighted_loss: 0.2396, label: 0, bag_size: 14662\n",
      "batch 139, loss: 0.0819, instance_loss: 0.0799, weighted_loss: 0.0813, label: 0, bag_size: 10590\n",
      "batch 159, loss: 0.0557, instance_loss: 0.1764, weighted_loss: 0.0919, label: 1, bag_size: 4233\n",
      "batch 179, loss: 0.7966, instance_loss: 0.9959, weighted_loss: 0.8564, label: 1, bag_size: 2877\n",
      "batch 199, loss: 0.1110, instance_loss: 0.0934, weighted_loss: 0.1057, label: 0, bag_size: 3290\n",
      "batch 219, loss: 0.0746, instance_loss: 0.1179, weighted_loss: 0.0876, label: 1, bag_size: 5819\n",
      "batch 239, loss: 0.1695, instance_loss: 0.3637, weighted_loss: 0.2277, label: 1, bag_size: 3777\n",
      "batch 259, loss: 0.9574, instance_loss: 1.3628, weighted_loss: 1.0790, label: 0, bag_size: 21335\n",
      "batch 279, loss: 0.0431, instance_loss: 0.0393, weighted_loss: 0.0419, label: 1, bag_size: 4377\n",
      "batch 299, loss: 0.9781, instance_loss: 1.0734, weighted_loss: 1.0067, label: 1, bag_size: 1808\n",
      "batch 319, loss: 0.3487, instance_loss: 0.3311, weighted_loss: 0.3434, label: 0, bag_size: 16859\n",
      "batch 339, loss: 0.3353, instance_loss: 0.3892, weighted_loss: 0.3514, label: 0, bag_size: 17546\n",
      "batch 359, loss: 0.3616, instance_loss: 0.5440, weighted_loss: 0.4163, label: 0, bag_size: 13573\n",
      "batch 379, loss: 0.9777, instance_loss: 1.0848, weighted_loss: 1.0099, label: 1, bag_size: 15118\n",
      "batch 399, loss: 0.5781, instance_loss: 0.6742, weighted_loss: 0.6069, label: 0, bag_size: 4468\n",
      "batch 419, loss: 0.6492, instance_loss: 0.7882, weighted_loss: 0.6909, label: 1, bag_size: 21105\n",
      "batch 439, loss: 0.1024, instance_loss: 0.1555, weighted_loss: 0.1183, label: 1, bag_size: 3777\n",
      "batch 459, loss: 1.4089, instance_loss: 1.8118, weighted_loss: 1.5298, label: 0, bag_size: 1638\n",
      "batch 479, loss: 0.6516, instance_loss: 0.8634, weighted_loss: 0.7151, label: 1, bag_size: 2877\n",
      "batch 499, loss: 0.2580, instance_loss: 0.3156, weighted_loss: 0.2753, label: 1, bag_size: 20435\n",
      "batch 519, loss: 0.1235, instance_loss: 0.1010, weighted_loss: 0.1167, label: 0, bag_size: 5536\n",
      "batch 539, loss: 0.2534, instance_loss: 0.2450, weighted_loss: 0.2509, label: 0, bag_size: 15015\n",
      "batch 559, loss: 0.0111, instance_loss: 0.0232, weighted_loss: 0.0147, label: 1, bag_size: 4795\n",
      "batch 579, loss: 0.8284, instance_loss: 1.1590, weighted_loss: 0.9276, label: 1, bag_size: 1251\n",
      "batch 599, loss: 0.2068, instance_loss: 0.1547, weighted_loss: 0.1912, label: 0, bag_size: 15060\n",
      "batch 619, loss: 0.0620, instance_loss: 0.0619, weighted_loss: 0.0620, label: 0, bag_size: 21574\n",
      "batch 639, loss: 0.1780, instance_loss: 0.2113, weighted_loss: 0.1880, label: 0, bag_size: 3521\n",
      "batch 659, loss: 0.6790, instance_loss: 0.7863, weighted_loss: 0.7112, label: 0, bag_size: 2398\n",
      "batch 679, loss: 0.4543, instance_loss: 0.5710, weighted_loss: 0.4893, label: 0, bag_size: 6301\n",
      "batch 699, loss: 1.0515, instance_loss: 1.3595, weighted_loss: 1.1439, label: 0, bag_size: 12861\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9401785714285714: correct 10530/11200\n",
      "class 1 clustering acc 0.4351785714285714: correct 2437/5600\n",
      "Epoch: 37, train_loss: 0.5682, train_clustering_loss:  0.7223, train_error: 0.2771\n",
      "class 0: acc 0.7236467236467237, correct 254/351\n",
      "class 1: acc 0.7220630372492837, correct 252/349\n",
      "\n",
      "Val Set, val_loss: 0.6536, val_error: 0.2887, auc: 0.6913\n",
      "class 0 clustering acc 0.8318298969072165: correct 1291/1552\n",
      "class 1 clustering acc 0.49871134020618557: correct 387/776\n",
      "class 0: acc 0.7627118644067796, correct 45/59\n",
      "class 1: acc 0.631578947368421, correct 24/38\n",
      "EarlyStopping counter: 15 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.9926, instance_loss: 2.3159, weighted_loss: 2.0896, label: 0, bag_size: 2592\n",
      "batch 39, loss: 0.5140, instance_loss: 0.5985, weighted_loss: 0.5394, label: 0, bag_size: 5120\n",
      "batch 59, loss: 0.7059, instance_loss: 0.8675, weighted_loss: 0.7544, label: 0, bag_size: 3521\n",
      "batch 79, loss: 0.2986, instance_loss: 0.3812, weighted_loss: 0.3234, label: 1, bag_size: 6884\n",
      "batch 99, loss: 0.4834, instance_loss: 0.6660, weighted_loss: 0.5382, label: 1, bag_size: 3162\n",
      "batch 119, loss: 0.4497, instance_loss: 0.5991, weighted_loss: 0.4946, label: 1, bag_size: 5379\n",
      "batch 139, loss: 0.4934, instance_loss: 0.6282, weighted_loss: 0.5339, label: 0, bag_size: 6168\n",
      "batch 159, loss: 1.1992, instance_loss: 1.3675, weighted_loss: 1.2497, label: 0, bag_size: 12861\n",
      "batch 179, loss: 0.1692, instance_loss: 0.1837, weighted_loss: 0.1735, label: 0, bag_size: 3427\n",
      "batch 199, loss: 2.2594, instance_loss: 2.6581, weighted_loss: 2.3790, label: 1, bag_size: 3159\n",
      "batch 219, loss: 0.5003, instance_loss: 0.9226, weighted_loss: 0.6270, label: 0, bag_size: 5724\n",
      "batch 239, loss: 0.0050, instance_loss: 0.8136, weighted_loss: 0.2476, label: 0, bag_size: 16582\n",
      "batch 259, loss: 0.5704, instance_loss: 0.7964, weighted_loss: 0.6382, label: 1, bag_size: 1746\n",
      "batch 279, loss: 0.7497, instance_loss: 0.9808, weighted_loss: 0.8190, label: 0, bag_size: 5153\n",
      "batch 299, loss: 0.1521, instance_loss: 0.2283, weighted_loss: 0.1749, label: 1, bag_size: 3143\n",
      "batch 319, loss: 0.7505, instance_loss: 0.9885, weighted_loss: 0.8219, label: 1, bag_size: 1495\n",
      "batch 339, loss: 0.1280, instance_loss: 0.3128, weighted_loss: 0.1835, label: 1, bag_size: 11657\n",
      "batch 359, loss: 0.3004, instance_loss: 0.3853, weighted_loss: 0.3259, label: 1, bag_size: 2899\n",
      "batch 379, loss: 0.2316, instance_loss: 0.2820, weighted_loss: 0.2467, label: 0, bag_size: 4468\n",
      "batch 399, loss: 0.1198, instance_loss: 0.3399, weighted_loss: 0.1859, label: 1, bag_size: 1627\n",
      "batch 419, loss: 0.0339, instance_loss: 0.2488, weighted_loss: 0.0984, label: 0, bag_size: 1553\n",
      "batch 439, loss: 0.2001, instance_loss: 0.3126, weighted_loss: 0.2338, label: 1, bag_size: 4249\n",
      "batch 459, loss: 0.1214, instance_loss: 0.2089, weighted_loss: 0.1476, label: 0, bag_size: 16789\n",
      "batch 479, loss: 0.8700, instance_loss: 0.9687, weighted_loss: 0.8996, label: 1, bag_size: 6371\n",
      "batch 499, loss: 0.2317, instance_loss: 0.2384, weighted_loss: 0.2337, label: 1, bag_size: 3391\n",
      "batch 519, loss: 0.6093, instance_loss: 0.7727, weighted_loss: 0.6583, label: 1, bag_size: 534\n",
      "batch 539, loss: 0.2137, instance_loss: 0.3811, weighted_loss: 0.2639, label: 0, bag_size: 3105\n",
      "batch 559, loss: 0.4006, instance_loss: 0.4993, weighted_loss: 0.4302, label: 0, bag_size: 7667\n",
      "batch 579, loss: 0.5102, instance_loss: 0.5993, weighted_loss: 0.5369, label: 1, bag_size: 18681\n",
      "batch 599, loss: 0.1074, instance_loss: 0.1982, weighted_loss: 0.1346, label: 0, bag_size: 3463\n",
      "batch 619, loss: 0.7337, instance_loss: 0.9921, weighted_loss: 0.8112, label: 0, bag_size: 3600\n",
      "batch 639, loss: 0.6322, instance_loss: 0.8514, weighted_loss: 0.6980, label: 0, bag_size: 6909\n",
      "batch 659, loss: 0.5209, instance_loss: 0.6339, weighted_loss: 0.5548, label: 0, bag_size: 4468\n",
      "batch 679, loss: 0.2412, instance_loss: 0.2894, weighted_loss: 0.2556, label: 1, bag_size: 6752\n",
      "batch 699, loss: 1.0386, instance_loss: 1.2991, weighted_loss: 1.1168, label: 1, bag_size: 5935\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9352678571428571: correct 10475/11200\n",
      "class 1 clustering acc 0.3796428571428571: correct 2126/5600\n",
      "Epoch: 38, train_loss: 0.5895, train_clustering_loss:  0.7798, train_error: 0.3057\n",
      "class 0: acc 0.6936416184971098, correct 240/346\n",
      "class 1: acc 0.6949152542372882, correct 246/354\n",
      "\n",
      "Val Set, val_loss: 0.6589, val_error: 0.2577, auc: 0.6704\n",
      "class 0 clustering acc 0.9574742268041238: correct 1486/1552\n",
      "class 1 clustering acc 0.20747422680412372: correct 161/776\n",
      "class 0: acc 0.7966101694915254, correct 47/59\n",
      "class 1: acc 0.6578947368421053, correct 25/38\n",
      "EarlyStopping counter: 16 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.8237, instance_loss: 1.1729, weighted_loss: 0.9285, label: 1, bag_size: 18015\n",
      "batch 39, loss: 0.3210, instance_loss: 0.4125, weighted_loss: 0.3484, label: 1, bag_size: 5638\n",
      "batch 59, loss: 1.0997, instance_loss: 1.5333, weighted_loss: 1.2298, label: 0, bag_size: 5477\n",
      "batch 79, loss: 0.2095, instance_loss: 0.2130, weighted_loss: 0.2106, label: 0, bag_size: 8469\n",
      "batch 99, loss: 1.6485, instance_loss: 2.1498, weighted_loss: 1.7989, label: 1, bag_size: 5935\n",
      "batch 119, loss: 1.1543, instance_loss: 1.4333, weighted_loss: 1.2380, label: 0, bag_size: 4875\n",
      "batch 139, loss: 1.7352, instance_loss: 1.8167, weighted_loss: 1.7597, label: 1, bag_size: 5072\n",
      "batch 159, loss: 0.1211, instance_loss: 0.1235, weighted_loss: 0.1218, label: 0, bag_size: 3936\n",
      "batch 179, loss: 0.1388, instance_loss: 0.1339, weighted_loss: 0.1374, label: 1, bag_size: 5819\n",
      "batch 199, loss: 0.0657, instance_loss: 0.0472, weighted_loss: 0.0602, label: 1, bag_size: 8007\n",
      "batch 219, loss: 0.4231, instance_loss: 0.5985, weighted_loss: 0.4757, label: 0, bag_size: 2586\n",
      "batch 239, loss: 0.9175, instance_loss: 1.1145, weighted_loss: 0.9766, label: 1, bag_size: 2356\n",
      "batch 259, loss: 1.3724, instance_loss: 1.7927, weighted_loss: 1.4985, label: 0, bag_size: 6058\n",
      "batch 279, loss: 0.8147, instance_loss: 0.9008, weighted_loss: 0.8405, label: 1, bag_size: 3966\n",
      "batch 299, loss: 0.4293, instance_loss: 0.4739, weighted_loss: 0.4427, label: 0, bag_size: 15912\n",
      "batch 319, loss: 0.3218, instance_loss: 0.3709, weighted_loss: 0.3365, label: 1, bag_size: 6861\n",
      "batch 339, loss: 0.4566, instance_loss: 0.6243, weighted_loss: 0.5069, label: 0, bag_size: 2969\n",
      "batch 359, loss: 0.2789, instance_loss: 0.3337, weighted_loss: 0.2953, label: 0, bag_size: 4506\n",
      "batch 379, loss: 0.2180, instance_loss: 0.2190, weighted_loss: 0.2183, label: 0, bag_size: 3534\n",
      "batch 399, loss: 1.8503, instance_loss: 2.3495, weighted_loss: 2.0001, label: 0, bag_size: 2167\n",
      "batch 419, loss: 0.7932, instance_loss: 0.9807, weighted_loss: 0.8494, label: 1, bag_size: 10952\n",
      "batch 439, loss: 1.3690, instance_loss: 1.7926, weighted_loss: 1.4961, label: 0, bag_size: 2483\n",
      "batch 459, loss: 0.3863, instance_loss: 0.3998, weighted_loss: 0.3903, label: 0, bag_size: 5527\n",
      "batch 479, loss: 0.1727, instance_loss: 0.1501, weighted_loss: 0.1659, label: 1, bag_size: 6600\n",
      "batch 499, loss: 0.3965, instance_loss: 0.5127, weighted_loss: 0.4314, label: 1, bag_size: 4737\n",
      "batch 519, loss: 0.6791, instance_loss: 0.8475, weighted_loss: 0.7296, label: 1, bag_size: 5671\n",
      "batch 539, loss: 0.2036, instance_loss: 0.2172, weighted_loss: 0.2077, label: 1, bag_size: 6392\n",
      "batch 559, loss: 0.8880, instance_loss: 1.1840, weighted_loss: 0.9768, label: 0, bag_size: 2963\n",
      "batch 579, loss: 1.8137, instance_loss: 2.2414, weighted_loss: 1.9420, label: 1, bag_size: 3454\n",
      "batch 599, loss: 0.5795, instance_loss: 0.6978, weighted_loss: 0.6150, label: 0, bag_size: 4427\n",
      "batch 619, loss: 0.0026, instance_loss: 0.5645, weighted_loss: 0.1711, label: 1, bag_size: 3126\n",
      "batch 639, loss: 0.6224, instance_loss: 0.7854, weighted_loss: 0.6713, label: 0, bag_size: 3724\n",
      "batch 659, loss: 0.3905, instance_loss: 0.5019, weighted_loss: 0.4239, label: 1, bag_size: 2624\n",
      "batch 679, loss: 0.3360, instance_loss: 0.4692, weighted_loss: 0.3760, label: 1, bag_size: 12654\n",
      "batch 699, loss: 0.9685, instance_loss: 1.2635, weighted_loss: 1.0570, label: 0, bag_size: 4431\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9458035714285714: correct 10593/11200\n",
      "class 1 clustering acc 0.42696428571428574: correct 2391/5600\n",
      "Epoch: 39, train_loss: 0.5753, train_clustering_loss:  0.7201, train_error: 0.3243\n",
      "class 0: acc 0.6801152737752162, correct 236/347\n",
      "class 1: acc 0.6713881019830028, correct 237/353\n",
      "\n",
      "Val Set, val_loss: 0.6945, val_error: 0.3299, auc: 0.6806\n",
      "class 0 clustering acc 0.907860824742268: correct 1409/1552\n",
      "class 1 clustering acc 0.24742268041237114: correct 192/776\n",
      "class 0: acc 0.6440677966101694, correct 38/59\n",
      "class 1: acc 0.7105263157894737, correct 27/38\n",
      "EarlyStopping counter: 17 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4513, instance_loss: 0.4570, weighted_loss: 0.4530, label: 1, bag_size: 13685\n",
      "batch 39, loss: 0.0670, instance_loss: 0.0715, weighted_loss: 0.0684, label: 1, bag_size: 3402\n",
      "batch 59, loss: 0.5326, instance_loss: 0.6753, weighted_loss: 0.5754, label: 1, bag_size: 15118\n",
      "batch 79, loss: 0.2763, instance_loss: 0.3148, weighted_loss: 0.2878, label: 1, bag_size: 2890\n",
      "batch 99, loss: 0.5923, instance_loss: 0.8109, weighted_loss: 0.6578, label: 0, bag_size: 763\n",
      "batch 119, loss: 0.8355, instance_loss: 1.0283, weighted_loss: 0.8933, label: 0, bag_size: 3540\n",
      "batch 139, loss: 0.1898, instance_loss: 0.1827, weighted_loss: 0.1876, label: 1, bag_size: 5152\n",
      "batch 159, loss: 0.6141, instance_loss: 0.7223, weighted_loss: 0.6466, label: 1, bag_size: 2253\n",
      "batch 179, loss: 0.9222, instance_loss: 1.2528, weighted_loss: 1.0214, label: 1, bag_size: 5935\n",
      "batch 199, loss: 0.6302, instance_loss: 0.6432, weighted_loss: 0.6341, label: 1, bag_size: 1374\n",
      "batch 219, loss: 0.0960, instance_loss: 0.1097, weighted_loss: 0.1001, label: 1, bag_size: 18681\n",
      "batch 239, loss: 1.1297, instance_loss: 1.4765, weighted_loss: 1.2337, label: 0, bag_size: 25403\n",
      "batch 259, loss: 0.4353, instance_loss: 0.5554, weighted_loss: 0.4714, label: 1, bag_size: 2524\n",
      "batch 279, loss: 0.1059, instance_loss: 0.0840, weighted_loss: 0.0993, label: 1, bag_size: 3777\n",
      "batch 299, loss: 0.1808, instance_loss: 0.1919, weighted_loss: 0.1841, label: 1, bag_size: 6421\n",
      "batch 319, loss: 0.4557, instance_loss: 0.5705, weighted_loss: 0.4902, label: 1, bag_size: 5062\n",
      "batch 339, loss: 0.2008, instance_loss: 0.1862, weighted_loss: 0.1964, label: 0, bag_size: 2009\n",
      "batch 359, loss: 0.2096, instance_loss: 0.2149, weighted_loss: 0.2112, label: 1, bag_size: 4094\n",
      "batch 379, loss: 0.1786, instance_loss: 0.1888, weighted_loss: 0.1816, label: 0, bag_size: 3535\n",
      "batch 399, loss: 0.0604, instance_loss: 0.0797, weighted_loss: 0.0662, label: 1, bag_size: 4800\n",
      "batch 419, loss: 1.7315, instance_loss: 2.1112, weighted_loss: 1.8454, label: 0, bag_size: 4522\n",
      "batch 439, loss: 0.0456, instance_loss: 0.3998, weighted_loss: 0.1519, label: 0, bag_size: 1379\n",
      "batch 459, loss: 0.2019, instance_loss: 0.2534, weighted_loss: 0.2173, label: 1, bag_size: 24686\n",
      "batch 479, loss: 0.6364, instance_loss: 0.7509, weighted_loss: 0.6707, label: 0, bag_size: 4360\n",
      "batch 499, loss: 0.2145, instance_loss: 0.2472, weighted_loss: 0.2243, label: 1, bag_size: 2356\n",
      "batch 519, loss: 2.1792, instance_loss: 2.8434, weighted_loss: 2.3785, label: 1, bag_size: 3159\n",
      "batch 539, loss: 0.6462, instance_loss: 0.7531, weighted_loss: 0.6783, label: 0, bag_size: 7493\n",
      "batch 559, loss: 0.4204, instance_loss: 0.4410, weighted_loss: 0.4266, label: 0, bag_size: 5960\n",
      "batch 579, loss: 0.2199, instance_loss: 0.2720, weighted_loss: 0.2355, label: 1, bag_size: 3391\n",
      "batch 599, loss: 0.3332, instance_loss: 0.4266, weighted_loss: 0.3612, label: 0, bag_size: 2110\n",
      "batch 619, loss: 0.5635, instance_loss: 0.7732, weighted_loss: 0.6264, label: 1, bag_size: 6371\n",
      "batch 639, loss: 0.2538, instance_loss: 0.3232, weighted_loss: 0.2746, label: 1, bag_size: 21105\n",
      "batch 659, loss: 0.4992, instance_loss: 0.6182, weighted_loss: 0.5349, label: 1, bag_size: 4436\n",
      "batch 679, loss: 0.6437, instance_loss: 0.7670, weighted_loss: 0.6807, label: 0, bag_size: 6605\n",
      "batch 699, loss: 0.5087, instance_loss: 0.4918, weighted_loss: 0.5037, label: 0, bag_size: 18807\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9370535714285714: correct 10495/11200\n",
      "class 1 clustering acc 0.43125: correct 2415/5600\n",
      "Epoch: 40, train_loss: 0.5719, train_clustering_loss:  0.7193, train_error: 0.3014\n",
      "class 0: acc 0.6796407185628742, correct 227/334\n",
      "class 1: acc 0.7158469945355191, correct 262/366\n",
      "\n",
      "Val Set, val_loss: 0.6456, val_error: 0.3093, auc: 0.6878\n",
      "class 0 clustering acc 0.9181701030927835: correct 1425/1552\n",
      "class 1 clustering acc 0.4381443298969072: correct 340/776\n",
      "class 0: acc 0.6949152542372882, correct 41/59\n",
      "class 1: acc 0.6842105263157895, correct 26/38\n",
      "EarlyStopping counter: 18 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.1218, instance_loss: 1.2402, weighted_loss: 1.1574, label: 0, bag_size: 3137\n",
      "batch 39, loss: 0.0097, instance_loss: 0.0956, weighted_loss: 0.0355, label: 0, bag_size: 1034\n",
      "batch 59, loss: 0.0100, instance_loss: 0.0234, weighted_loss: 0.0141, label: 0, bag_size: 14098\n",
      "batch 79, loss: 0.5182, instance_loss: 0.7311, weighted_loss: 0.5821, label: 0, bag_size: 2398\n",
      "batch 99, loss: 0.6926, instance_loss: 0.7046, weighted_loss: 0.6962, label: 1, bag_size: 3020\n",
      "batch 119, loss: 1.1105, instance_loss: 1.6322, weighted_loss: 1.2671, label: 0, bag_size: 15139\n",
      "batch 139, loss: 0.2505, instance_loss: 0.5610, weighted_loss: 0.3436, label: 0, bag_size: 4180\n",
      "batch 159, loss: 0.1008, instance_loss: 0.4087, weighted_loss: 0.1932, label: 1, bag_size: 2820\n",
      "batch 179, loss: 0.2800, instance_loss: 0.2869, weighted_loss: 0.2821, label: 0, bag_size: 3372\n",
      "batch 199, loss: 0.0815, instance_loss: 0.0919, weighted_loss: 0.0846, label: 0, bag_size: 4885\n",
      "batch 219, loss: 0.1288, instance_loss: 0.9144, weighted_loss: 0.3644, label: 0, bag_size: 3096\n",
      "batch 239, loss: 1.0286, instance_loss: 1.1702, weighted_loss: 1.0711, label: 1, bag_size: 3020\n",
      "batch 259, loss: 0.2665, instance_loss: 0.3325, weighted_loss: 0.2863, label: 1, bag_size: 6500\n",
      "batch 279, loss: 0.4828, instance_loss: 0.7391, weighted_loss: 0.5597, label: 0, bag_size: 3196\n",
      "batch 299, loss: 0.6635, instance_loss: 0.7956, weighted_loss: 0.7031, label: 1, bag_size: 5671\n",
      "batch 319, loss: 0.7906, instance_loss: 1.1328, weighted_loss: 0.8932, label: 1, bag_size: 3454\n",
      "batch 339, loss: 0.2954, instance_loss: 0.2503, weighted_loss: 0.2819, label: 1, bag_size: 5071\n",
      "batch 359, loss: 0.2827, instance_loss: 0.2385, weighted_loss: 0.2694, label: 0, bag_size: 14696\n",
      "batch 379, loss: 0.9033, instance_loss: 1.0575, weighted_loss: 0.9495, label: 1, bag_size: 4268\n",
      "batch 399, loss: 0.3149, instance_loss: 0.3655, weighted_loss: 0.3301, label: 0, bag_size: 3022\n",
      "batch 419, loss: 0.2649, instance_loss: 0.3256, weighted_loss: 0.2831, label: 1, bag_size: 16427\n",
      "batch 439, loss: 1.9947, instance_loss: 2.3497, weighted_loss: 2.1012, label: 1, bag_size: 15483\n",
      "batch 459, loss: 0.0483, instance_loss: 0.0857, weighted_loss: 0.0595, label: 0, bag_size: 1034\n",
      "batch 479, loss: 0.2201, instance_loss: 0.2521, weighted_loss: 0.2297, label: 1, bag_size: 2036\n",
      "batch 499, loss: 0.8781, instance_loss: 1.0921, weighted_loss: 0.9423, label: 1, bag_size: 21105\n",
      "batch 519, loss: 0.8987, instance_loss: 1.1913, weighted_loss: 0.9865, label: 1, bag_size: 6151\n",
      "batch 539, loss: 1.0411, instance_loss: 1.3625, weighted_loss: 1.1375, label: 0, bag_size: 3264\n",
      "batch 559, loss: 0.1243, instance_loss: 0.1479, weighted_loss: 0.1314, label: 0, bag_size: 10590\n",
      "batch 579, loss: 0.5865, instance_loss: 0.7465, weighted_loss: 0.6345, label: 0, bag_size: 5120\n",
      "batch 599, loss: 0.6044, instance_loss: 0.7119, weighted_loss: 0.6366, label: 1, bag_size: 5273\n",
      "batch 619, loss: 1.6912, instance_loss: 2.1503, weighted_loss: 1.8290, label: 1, bag_size: 5065\n",
      "batch 639, loss: 0.5996, instance_loss: 0.7251, weighted_loss: 0.6373, label: 1, bag_size: 4510\n",
      "batch 659, loss: 0.4052, instance_loss: 0.4393, weighted_loss: 0.4154, label: 1, bag_size: 3391\n",
      "batch 679, loss: 0.1323, instance_loss: 0.1257, weighted_loss: 0.1303, label: 0, bag_size: 5724\n",
      "batch 699, loss: 0.0711, instance_loss: 0.0758, weighted_loss: 0.0725, label: 0, bag_size: 5960\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9419642857142857: correct 10550/11200\n",
      "class 1 clustering acc 0.37089285714285714: correct 2077/5600\n",
      "Epoch: 41, train_loss: 0.5976, train_clustering_loss:  0.7698, train_error: 0.3043\n",
      "class 0: acc 0.7052341597796143, correct 256/363\n",
      "class 1: acc 0.685459940652819, correct 231/337\n",
      "\n",
      "Val Set, val_loss: 0.6657, val_error: 0.2784, auc: 0.6842\n",
      "class 0 clustering acc 0.759020618556701: correct 1178/1552\n",
      "class 1 clustering acc 0.38402061855670105: correct 298/776\n",
      "class 0: acc 0.8305084745762712, correct 49/59\n",
      "class 1: acc 0.5526315789473685, correct 21/38\n",
      "EarlyStopping counter: 19 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0820, instance_loss: 0.0989, weighted_loss: 0.0871, label: 0, bag_size: 13606\n",
      "batch 39, loss: 0.9124, instance_loss: 1.1912, weighted_loss: 0.9960, label: 0, bag_size: 1207\n",
      "batch 59, loss: 1.6382, instance_loss: 1.9305, weighted_loss: 1.7259, label: 1, bag_size: 11513\n",
      "batch 79, loss: 0.2727, instance_loss: 0.2699, weighted_loss: 0.2718, label: 0, bag_size: 35706\n",
      "batch 99, loss: 0.2440, instance_loss: 0.2281, weighted_loss: 0.2392, label: 0, bag_size: 1539\n",
      "batch 119, loss: 0.9209, instance_loss: 1.0393, weighted_loss: 0.9564, label: 1, bag_size: 3966\n",
      "batch 139, loss: 0.2520, instance_loss: 0.2776, weighted_loss: 0.2597, label: 1, bag_size: 6500\n",
      "batch 159, loss: 0.7975, instance_loss: 1.0388, weighted_loss: 0.8699, label: 1, bag_size: 3893\n",
      "batch 179, loss: 1.4887, instance_loss: 1.9038, weighted_loss: 1.6132, label: 1, bag_size: 15483\n",
      "batch 199, loss: 0.3078, instance_loss: 0.3741, weighted_loss: 0.3277, label: 1, bag_size: 2356\n",
      "batch 219, loss: 0.2331, instance_loss: 0.2642, weighted_loss: 0.2425, label: 1, bag_size: 1479\n",
      "batch 239, loss: 0.2446, instance_loss: 0.2632, weighted_loss: 0.2502, label: 1, bag_size: 11563\n",
      "batch 259, loss: 0.7555, instance_loss: 1.0001, weighted_loss: 0.8289, label: 0, bag_size: 5476\n",
      "batch 279, loss: 0.4418, instance_loss: 0.5750, weighted_loss: 0.4817, label: 1, bag_size: 6759\n",
      "batch 299, loss: 0.3671, instance_loss: 0.3855, weighted_loss: 0.3726, label: 0, bag_size: 7484\n",
      "batch 319, loss: 0.4690, instance_loss: 0.5777, weighted_loss: 0.5017, label: 1, bag_size: 5738\n",
      "batch 339, loss: 0.9000, instance_loss: 1.1471, weighted_loss: 0.9741, label: 1, bag_size: 5638\n",
      "batch 359, loss: 0.3792, instance_loss: 0.3976, weighted_loss: 0.3847, label: 1, bag_size: 5671\n",
      "batch 379, loss: 0.2764, instance_loss: 0.2997, weighted_loss: 0.2834, label: 1, bag_size: 5507\n",
      "batch 399, loss: 0.3469, instance_loss: 0.4179, weighted_loss: 0.3682, label: 0, bag_size: 5448\n",
      "batch 419, loss: 1.0085, instance_loss: 1.3324, weighted_loss: 1.1057, label: 0, bag_size: 2483\n",
      "batch 439, loss: 0.0701, instance_loss: 0.1120, weighted_loss: 0.0827, label: 1, bag_size: 3777\n",
      "batch 459, loss: 0.2058, instance_loss: 0.2446, weighted_loss: 0.2174, label: 0, bag_size: 35706\n",
      "batch 479, loss: 0.3846, instance_loss: 0.4429, weighted_loss: 0.4021, label: 0, bag_size: 9622\n",
      "batch 499, loss: 0.1226, instance_loss: 0.1294, weighted_loss: 0.1246, label: 0, bag_size: 1034\n",
      "batch 519, loss: 0.2106, instance_loss: 0.2042, weighted_loss: 0.2087, label: 0, bag_size: 1619\n",
      "batch 539, loss: 0.2247, instance_loss: 0.2197, weighted_loss: 0.2232, label: 0, bag_size: 16663\n",
      "batch 559, loss: 0.9445, instance_loss: 1.2227, weighted_loss: 1.0280, label: 0, bag_size: 2762\n",
      "batch 579, loss: 0.4366, instance_loss: 0.5506, weighted_loss: 0.4708, label: 1, bag_size: 15118\n",
      "batch 599, loss: 0.5619, instance_loss: 0.6844, weighted_loss: 0.5986, label: 1, bag_size: 1958\n",
      "batch 619, loss: 0.8132, instance_loss: 0.9888, weighted_loss: 0.8659, label: 0, bag_size: 3200\n",
      "batch 639, loss: 0.2691, instance_loss: 0.2854, weighted_loss: 0.2740, label: 0, bag_size: 5724\n",
      "batch 659, loss: 0.0587, instance_loss: 0.0597, weighted_loss: 0.0590, label: 1, bag_size: 3670\n",
      "batch 679, loss: 0.7246, instance_loss: 0.9079, weighted_loss: 0.7796, label: 1, bag_size: 12654\n",
      "batch 699, loss: 0.4728, instance_loss: 0.6089, weighted_loss: 0.5136, label: 0, bag_size: 14856\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9375: correct 10500/11200\n",
      "class 1 clustering acc 0.5003571428571428: correct 2802/5600\n",
      "Epoch: 42, train_loss: 0.5686, train_clustering_loss:  0.7023, train_error: 0.2871\n",
      "class 0: acc 0.7130177514792899, correct 241/338\n",
      "class 1: acc 0.712707182320442, correct 258/362\n",
      "\n",
      "Val Set, val_loss: 0.6716, val_error: 0.3196, auc: 0.6757\n",
      "class 0 clustering acc 0.9020618556701031: correct 1400/1552\n",
      "class 1 clustering acc 0.24871134020618557: correct 193/776\n",
      "class 0: acc 0.6779661016949152, correct 40/59\n",
      "class 1: acc 0.6842105263157895, correct 26/38\n",
      "EarlyStopping counter: 20 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.0498, instance_loss: 1.3879, weighted_loss: 1.1512, label: 0, bag_size: 3149\n",
      "batch 39, loss: 0.7680, instance_loss: 0.9817, weighted_loss: 0.8321, label: 0, bag_size: 4076\n",
      "batch 59, loss: 1.3202, instance_loss: 1.7642, weighted_loss: 1.4534, label: 1, bag_size: 3020\n",
      "batch 79, loss: 0.5351, instance_loss: 0.7258, weighted_loss: 0.5923, label: 1, bag_size: 5273\n",
      "batch 99, loss: 0.0148, instance_loss: 0.0874, weighted_loss: 0.0366, label: 1, bag_size: 5218\n",
      "batch 119, loss: 0.6553, instance_loss: 0.8511, weighted_loss: 0.7140, label: 1, bag_size: 3966\n",
      "batch 139, loss: 0.9162, instance_loss: 1.1737, weighted_loss: 0.9934, label: 0, bag_size: 3504\n",
      "batch 159, loss: 0.2186, instance_loss: 0.3129, weighted_loss: 0.2469, label: 1, bag_size: 3588\n",
      "batch 179, loss: 0.1132, instance_loss: 0.1027, weighted_loss: 0.1101, label: 0, bag_size: 5005\n",
      "batch 199, loss: 0.1861, instance_loss: 0.1771, weighted_loss: 0.1834, label: 0, bag_size: 6724\n",
      "batch 219, loss: 0.8365, instance_loss: 0.9853, weighted_loss: 0.8812, label: 1, bag_size: 6861\n",
      "batch 239, loss: 1.3753, instance_loss: 1.7413, weighted_loss: 1.4851, label: 0, bag_size: 4064\n",
      "batch 259, loss: 1.0437, instance_loss: 1.4651, weighted_loss: 1.1701, label: 0, bag_size: 2167\n",
      "batch 279, loss: 0.2883, instance_loss: 0.2214, weighted_loss: 0.2682, label: 0, bag_size: 24289\n",
      "batch 299, loss: 0.1448, instance_loss: 0.1645, weighted_loss: 0.1507, label: 0, bag_size: 3065\n",
      "batch 319, loss: 2.3532, instance_loss: 2.8365, weighted_loss: 2.4982, label: 0, bag_size: 4522\n",
      "batch 339, loss: 0.0476, instance_loss: 0.0465, weighted_loss: 0.0473, label: 0, bag_size: 2420\n",
      "batch 359, loss: 2.1019, instance_loss: 2.3710, weighted_loss: 2.1826, label: 0, bag_size: 2146\n",
      "batch 379, loss: 0.0990, instance_loss: 0.2300, weighted_loss: 0.1383, label: 0, bag_size: 3105\n",
      "batch 399, loss: 1.9011, instance_loss: 1.9465, weighted_loss: 1.9147, label: 0, bag_size: 4431\n",
      "batch 419, loss: 1.2317, instance_loss: 1.7936, weighted_loss: 1.4003, label: 0, bag_size: 4855\n",
      "batch 439, loss: 0.0173, instance_loss: 0.0498, weighted_loss: 0.0270, label: 1, bag_size: 3287\n",
      "batch 459, loss: 1.1366, instance_loss: 1.3122, weighted_loss: 1.1893, label: 1, bag_size: 6697\n",
      "batch 479, loss: 0.0428, instance_loss: 0.1066, weighted_loss: 0.0620, label: 1, bag_size: 986\n",
      "batch 499, loss: 0.9174, instance_loss: 0.9644, weighted_loss: 0.9315, label: 0, bag_size: 9742\n",
      "batch 519, loss: 0.0027, instance_loss: 0.8255, weighted_loss: 0.2496, label: 1, bag_size: 7085\n",
      "batch 539, loss: 1.2911, instance_loss: 1.5532, weighted_loss: 1.3698, label: 1, bag_size: 3358\n",
      "batch 559, loss: 0.3652, instance_loss: 0.4010, weighted_loss: 0.3759, label: 0, bag_size: 16859\n",
      "batch 579, loss: 0.2985, instance_loss: 0.4236, weighted_loss: 0.3361, label: 1, bag_size: 1579\n",
      "batch 599, loss: 0.9393, instance_loss: 1.2511, weighted_loss: 1.0329, label: 0, bag_size: 3521\n",
      "batch 619, loss: 0.4964, instance_loss: 0.6605, weighted_loss: 0.5457, label: 0, bag_size: 4692\n",
      "batch 639, loss: 0.6529, instance_loss: 0.8621, weighted_loss: 0.7156, label: 0, bag_size: 6168\n",
      "batch 659, loss: 0.2507, instance_loss: 0.2934, weighted_loss: 0.2635, label: 1, bag_size: 4800\n",
      "batch 679, loss: 0.2091, instance_loss: 0.2232, weighted_loss: 0.2133, label: 0, bag_size: 2420\n",
      "batch 699, loss: 0.3642, instance_loss: 0.4439, weighted_loss: 0.3881, label: 1, bag_size: 11968\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9434821428571428: correct 10567/11200\n",
      "class 1 clustering acc 0.41660714285714284: correct 2333/5600\n",
      "Epoch: 43, train_loss: 0.5898, train_clustering_loss:  0.7363, train_error: 0.3143\n",
      "class 0: acc 0.6716867469879518, correct 223/332\n",
      "class 1: acc 0.6983695652173914, correct 257/368\n",
      "\n",
      "Val Set, val_loss: 0.6456, val_error: 0.3093, auc: 0.6757\n",
      "class 0 clustering acc 0.8801546391752577: correct 1366/1552\n",
      "class 1 clustering acc 0.38917525773195877: correct 302/776\n",
      "class 0: acc 0.7457627118644068, correct 44/59\n",
      "class 1: acc 0.6052631578947368, correct 23/38\n",
      "EarlyStopping counter: 21 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7758, instance_loss: 0.9040, weighted_loss: 0.8143, label: 0, bag_size: 10568\n",
      "batch 39, loss: 1.5158, instance_loss: 2.1621, weighted_loss: 1.7097, label: 1, bag_size: 1582\n",
      "batch 59, loss: 0.6448, instance_loss: 0.8391, weighted_loss: 0.7031, label: 1, bag_size: 1178\n",
      "batch 79, loss: 0.1579, instance_loss: 0.2356, weighted_loss: 0.1812, label: 0, bag_size: 2009\n",
      "batch 99, loss: 0.4005, instance_loss: 0.4140, weighted_loss: 0.4045, label: 0, bag_size: 4587\n",
      "batch 119, loss: 0.6441, instance_loss: 0.7539, weighted_loss: 0.6770, label: 0, bag_size: 3793\n",
      "batch 139, loss: 0.6439, instance_loss: 0.8707, weighted_loss: 0.7120, label: 1, bag_size: 3454\n",
      "batch 159, loss: 0.5907, instance_loss: 0.7108, weighted_loss: 0.6267, label: 1, bag_size: 4007\n",
      "batch 179, loss: 0.2515, instance_loss: 0.2565, weighted_loss: 0.2530, label: 0, bag_size: 3161\n",
      "batch 199, loss: 1.3038, instance_loss: 1.7020, weighted_loss: 1.4233, label: 0, bag_size: 3598\n",
      "batch 219, loss: 0.4773, instance_loss: 0.5761, weighted_loss: 0.5069, label: 0, bag_size: 1638\n",
      "batch 239, loss: 0.7912, instance_loss: 0.9812, weighted_loss: 0.8482, label: 1, bag_size: 4647\n",
      "batch 259, loss: 0.5173, instance_loss: 0.6261, weighted_loss: 0.5499, label: 1, bag_size: 6463\n",
      "batch 279, loss: 1.1289, instance_loss: 1.4797, weighted_loss: 1.2342, label: 0, bag_size: 3256\n",
      "batch 299, loss: 0.5992, instance_loss: 0.7860, weighted_loss: 0.6553, label: 1, bag_size: 23277\n",
      "batch 319, loss: 0.1376, instance_loss: 0.1283, weighted_loss: 0.1348, label: 1, bag_size: 6392\n",
      "batch 339, loss: 0.8191, instance_loss: 1.0307, weighted_loss: 0.8826, label: 1, bag_size: 3358\n",
      "batch 359, loss: 0.0373, instance_loss: 0.0410, weighted_loss: 0.0384, label: 0, bag_size: 1379\n",
      "batch 379, loss: 0.3010, instance_loss: 0.3595, weighted_loss: 0.3185, label: 1, bag_size: 19013\n",
      "batch 399, loss: 0.8190, instance_loss: 1.0996, weighted_loss: 0.9032, label: 1, bag_size: 1746\n",
      "batch 419, loss: 0.2311, instance_loss: 0.2096, weighted_loss: 0.2247, label: 1, bag_size: 3420\n",
      "batch 439, loss: 0.1710, instance_loss: 0.1721, weighted_loss: 0.1714, label: 1, bag_size: 4802\n",
      "batch 459, loss: 0.3137, instance_loss: 0.3752, weighted_loss: 0.3321, label: 0, bag_size: 10568\n",
      "batch 479, loss: 0.1557, instance_loss: 0.1416, weighted_loss: 0.1514, label: 1, bag_size: 10091\n",
      "batch 499, loss: 0.5277, instance_loss: 0.6624, weighted_loss: 0.5681, label: 0, bag_size: 2302\n",
      "batch 519, loss: 0.3165, instance_loss: 0.4113, weighted_loss: 0.3449, label: 0, bag_size: 2963\n",
      "batch 539, loss: 1.2262, instance_loss: 1.4633, weighted_loss: 1.2973, label: 1, bag_size: 4880\n",
      "batch 559, loss: 3.0708, instance_loss: 3.6140, weighted_loss: 3.2337, label: 0, bag_size: 4441\n",
      "batch 579, loss: 0.1274, instance_loss: 0.1002, weighted_loss: 0.1192, label: 0, bag_size: 15193\n",
      "batch 599, loss: 0.3681, instance_loss: 0.4771, weighted_loss: 0.4008, label: 1, bag_size: 1525\n",
      "batch 619, loss: 1.1582, instance_loss: 1.4867, weighted_loss: 1.2568, label: 0, bag_size: 19223\n",
      "batch 639, loss: 0.4997, instance_loss: 0.6875, weighted_loss: 0.5561, label: 1, bag_size: 4647\n",
      "batch 659, loss: 0.8397, instance_loss: 1.0686, weighted_loss: 0.9084, label: 1, bag_size: 6871\n",
      "batch 679, loss: 0.1573, instance_loss: 0.2686, weighted_loss: 0.1907, label: 0, bag_size: 7179\n",
      "batch 699, loss: 0.0602, instance_loss: 0.0384, weighted_loss: 0.0537, label: 0, bag_size: 3535\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9382142857142857: correct 10508/11200\n",
      "class 1 clustering acc 0.48660714285714285: correct 2725/5600\n",
      "Epoch: 44, train_loss: 0.5577, train_clustering_loss:  0.6910, train_error: 0.2843\n",
      "class 0: acc 0.7106017191977078, correct 248/349\n",
      "class 1: acc 0.7207977207977208, correct 253/351\n",
      "\n",
      "Val Set, val_loss: 0.6859, val_error: 0.2990, auc: 0.6954\n",
      "class 0 clustering acc 0.6778350515463918: correct 1052/1552\n",
      "class 1 clustering acc 0.5244845360824743: correct 407/776\n",
      "class 0: acc 0.9322033898305084, correct 55/59\n",
      "class 1: acc 0.34210526315789475, correct 13/38\n",
      "EarlyStopping counter: 22 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1107, instance_loss: 0.0928, weighted_loss: 0.1054, label: 1, bag_size: 2558\n",
      "batch 39, loss: 0.0650, instance_loss: 0.0424, weighted_loss: 0.0582, label: 1, bag_size: 2005\n",
      "batch 59, loss: 0.4783, instance_loss: 0.5034, weighted_loss: 0.4858, label: 0, bag_size: 4084\n",
      "batch 79, loss: 1.3455, instance_loss: 1.7567, weighted_loss: 1.4688, label: 1, bag_size: 1838\n",
      "batch 99, loss: 0.3647, instance_loss: 0.4636, weighted_loss: 0.3944, label: 1, bag_size: 4985\n",
      "batch 119, loss: 0.2931, instance_loss: 0.3538, weighted_loss: 0.3113, label: 1, bag_size: 5108\n",
      "batch 139, loss: 0.4667, instance_loss: 0.5578, weighted_loss: 0.4940, label: 0, bag_size: 5536\n",
      "batch 159, loss: 0.6732, instance_loss: 0.8418, weighted_loss: 0.7238, label: 1, bag_size: 2840\n",
      "batch 179, loss: 0.5492, instance_loss: 0.6791, weighted_loss: 0.5882, label: 0, bag_size: 22936\n",
      "batch 199, loss: 0.7594, instance_loss: 0.8665, weighted_loss: 0.7915, label: 1, bag_size: 2890\n",
      "batch 219, loss: 0.1985, instance_loss: 0.2176, weighted_loss: 0.2042, label: 0, bag_size: 5478\n",
      "batch 239, loss: 0.9216, instance_loss: 1.1361, weighted_loss: 0.9859, label: 1, bag_size: 4722\n",
      "batch 259, loss: 0.2516, instance_loss: 0.2828, weighted_loss: 0.2610, label: 1, bag_size: 10736\n",
      "batch 279, loss: 0.0385, instance_loss: 0.0389, weighted_loss: 0.0386, label: 1, bag_size: 3834\n",
      "batch 299, loss: 0.2225, instance_loss: 0.2295, weighted_loss: 0.2246, label: 0, bag_size: 1828\n",
      "batch 319, loss: 0.1914, instance_loss: 0.1567, weighted_loss: 0.1810, label: 0, bag_size: 15706\n",
      "batch 339, loss: 0.0046, instance_loss: 0.0042, weighted_loss: 0.0045, label: 1, bag_size: 5695\n",
      "batch 359, loss: 1.1405, instance_loss: 1.4422, weighted_loss: 1.2310, label: 0, bag_size: 17349\n",
      "batch 379, loss: 0.2346, instance_loss: 0.2615, weighted_loss: 0.2426, label: 0, bag_size: 14194\n",
      "batch 399, loss: 0.1280, instance_loss: 0.1096, weighted_loss: 0.1225, label: 0, bag_size: 4560\n",
      "batch 419, loss: 0.6006, instance_loss: 0.7520, weighted_loss: 0.6460, label: 0, bag_size: 30392\n",
      "batch 439, loss: 0.1984, instance_loss: 0.1562, weighted_loss: 0.1858, label: 1, bag_size: 5677\n",
      "batch 459, loss: 0.0046, instance_loss: 0.0135, weighted_loss: 0.0073, label: 1, bag_size: 5695\n",
      "batch 479, loss: 0.2100, instance_loss: 0.2604, weighted_loss: 0.2251, label: 0, bag_size: 5924\n",
      "batch 499, loss: 0.3108, instance_loss: 0.2669, weighted_loss: 0.2976, label: 0, bag_size: 6777\n",
      "batch 519, loss: 0.3304, instance_loss: 0.2983, weighted_loss: 0.3208, label: 1, bag_size: 4094\n",
      "batch 539, loss: 0.1573, instance_loss: 0.1489, weighted_loss: 0.1548, label: 0, bag_size: 6724\n",
      "batch 559, loss: 1.1542, instance_loss: 1.3231, weighted_loss: 1.2048, label: 1, bag_size: 4112\n",
      "batch 579, loss: 0.5400, instance_loss: 0.6518, weighted_loss: 0.5736, label: 1, bag_size: 3966\n",
      "batch 599, loss: 0.0953, instance_loss: 0.0827, weighted_loss: 0.0915, label: 1, bag_size: 5819\n",
      "batch 619, loss: 1.0193, instance_loss: 1.2175, weighted_loss: 1.0788, label: 1, bag_size: 5637\n",
      "batch 639, loss: 0.5396, instance_loss: 0.5039, weighted_loss: 0.5289, label: 0, bag_size: 5877\n",
      "batch 659, loss: 1.0994, instance_loss: 1.4612, weighted_loss: 1.2080, label: 1, bag_size: 16451\n",
      "batch 679, loss: 0.1009, instance_loss: 0.0659, weighted_loss: 0.0904, label: 1, bag_size: 2126\n",
      "batch 699, loss: 0.8208, instance_loss: 0.9301, weighted_loss: 0.8536, label: 1, bag_size: 6759\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9350892857142857: correct 10473/11200\n",
      "class 1 clustering acc 0.4594642857142857: correct 2573/5600\n",
      "Epoch: 45, train_loss: 0.5805, train_clustering_loss:  0.7158, train_error: 0.3186\n",
      "class 0: acc 0.6696165191740413, correct 227/339\n",
      "class 1: acc 0.6925207756232687, correct 250/361\n",
      "\n",
      "Val Set, val_loss: 0.6254, val_error: 0.3299, auc: 0.6887\n",
      "class 0 clustering acc 0.9426546391752577: correct 1463/1552\n",
      "class 1 clustering acc 0.21391752577319587: correct 166/776\n",
      "class 0: acc 0.7457627118644068, correct 44/59\n",
      "class 1: acc 0.5526315789473685, correct 21/38\n",
      "EarlyStopping counter: 23 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.8497, instance_loss: 1.1350, weighted_loss: 0.9353, label: 0, bag_size: 1822\n",
      "batch 39, loss: 0.5004, instance_loss: 0.5657, weighted_loss: 0.5200, label: 0, bag_size: 10962\n",
      "batch 59, loss: 0.4192, instance_loss: 0.4921, weighted_loss: 0.4411, label: 0, bag_size: 8469\n",
      "batch 79, loss: 0.5664, instance_loss: 0.7296, weighted_loss: 0.6154, label: 1, bag_size: 4332\n",
      "batch 99, loss: 0.3813, instance_loss: 0.5098, weighted_loss: 0.4198, label: 0, bag_size: 2918\n",
      "batch 119, loss: 0.4057, instance_loss: 0.6025, weighted_loss: 0.4648, label: 0, bag_size: 5211\n",
      "batch 139, loss: 0.3507, instance_loss: 0.3366, weighted_loss: 0.3465, label: 0, bag_size: 26830\n",
      "batch 159, loss: 0.9171, instance_loss: 1.1148, weighted_loss: 0.9764, label: 0, bag_size: 2268\n",
      "batch 179, loss: 0.8667, instance_loss: 1.0960, weighted_loss: 0.9355, label: 1, bag_size: 21399\n",
      "batch 199, loss: 0.6522, instance_loss: 0.8106, weighted_loss: 0.6997, label: 0, bag_size: 2094\n",
      "batch 219, loss: 0.9391, instance_loss: 1.2616, weighted_loss: 1.0358, label: 1, bag_size: 12948\n",
      "batch 239, loss: 1.3992, instance_loss: 1.7626, weighted_loss: 1.5082, label: 0, bag_size: 4079\n",
      "batch 259, loss: 0.4283, instance_loss: 0.5787, weighted_loss: 0.4734, label: 0, bag_size: 7667\n",
      "batch 279, loss: 0.4502, instance_loss: 0.5137, weighted_loss: 0.4693, label: 0, bag_size: 3636\n",
      "batch 299, loss: 0.7499, instance_loss: 0.9824, weighted_loss: 0.8196, label: 0, bag_size: 4441\n",
      "batch 319, loss: 0.3329, instance_loss: 0.3142, weighted_loss: 0.3273, label: 0, bag_size: 5478\n",
      "batch 339, loss: 0.4670, instance_loss: 0.5272, weighted_loss: 0.4850, label: 1, bag_size: 1552\n",
      "batch 359, loss: 0.1178, instance_loss: 0.1598, weighted_loss: 0.1304, label: 1, bag_size: 2005\n",
      "batch 379, loss: 0.6672, instance_loss: 0.8312, weighted_loss: 0.7164, label: 0, bag_size: 2856\n",
      "batch 399, loss: 0.6499, instance_loss: 0.9220, weighted_loss: 0.7315, label: 0, bag_size: 6058\n",
      "batch 419, loss: 0.8775, instance_loss: 1.1731, weighted_loss: 0.9662, label: 1, bag_size: 15483\n",
      "batch 439, loss: 0.7568, instance_loss: 0.9265, weighted_loss: 0.8077, label: 1, bag_size: 5108\n",
      "batch 459, loss: 0.3677, instance_loss: 0.3501, weighted_loss: 0.3624, label: 0, bag_size: 1864\n",
      "batch 479, loss: 0.3475, instance_loss: 0.4047, weighted_loss: 0.3646, label: 0, bag_size: 3427\n",
      "batch 499, loss: 0.8276, instance_loss: 1.1064, weighted_loss: 0.9112, label: 0, bag_size: 21335\n",
      "batch 519, loss: 0.3048, instance_loss: 0.3594, weighted_loss: 0.3212, label: 0, bag_size: 7862\n",
      "batch 539, loss: 0.5725, instance_loss: 0.7180, weighted_loss: 0.6162, label: 1, bag_size: 1587\n",
      "batch 559, loss: 0.9878, instance_loss: 1.0316, weighted_loss: 1.0009, label: 0, bag_size: 2963\n",
      "batch 579, loss: 1.7155, instance_loss: 2.1465, weighted_loss: 1.8448, label: 1, bag_size: 5065\n",
      "batch 599, loss: 0.1235, instance_loss: 0.1114, weighted_loss: 0.1199, label: 0, bag_size: 18810\n",
      "batch 619, loss: 0.5127, instance_loss: 0.6655, weighted_loss: 0.5586, label: 1, bag_size: 22843\n",
      "batch 639, loss: 0.1597, instance_loss: 0.1798, weighted_loss: 0.1658, label: 0, bag_size: 5002\n",
      "batch 659, loss: 0.1657, instance_loss: 0.1870, weighted_loss: 0.1721, label: 1, bag_size: 6884\n",
      "batch 679, loss: 0.9388, instance_loss: 1.0821, weighted_loss: 0.9818, label: 1, bag_size: 5062\n",
      "batch 699, loss: 0.6530, instance_loss: 0.5228, weighted_loss: 0.6140, label: 0, bag_size: 1352\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9446428571428571: correct 10580/11200\n",
      "class 1 clustering acc 0.39357142857142857: correct 2204/5600\n",
      "Epoch: 46, train_loss: 0.5942, train_clustering_loss:  0.7348, train_error: 0.3243\n",
      "class 0: acc 0.6846590909090909, correct 241/352\n",
      "class 1: acc 0.6666666666666666, correct 232/348\n",
      "\n",
      "Val Set, val_loss: 0.6683, val_error: 0.3711, auc: 0.6905\n",
      "class 0 clustering acc 0.8086340206185567: correct 1255/1552\n",
      "class 1 clustering acc 0.33505154639175255: correct 260/776\n",
      "class 0: acc 0.576271186440678, correct 34/59\n",
      "class 1: acc 0.7105263157894737, correct 27/38\n",
      "EarlyStopping counter: 24 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6335, instance_loss: 0.7389, weighted_loss: 0.6651, label: 1, bag_size: 3962\n",
      "batch 39, loss: 0.1019, instance_loss: 0.1119, weighted_loss: 0.1049, label: 0, bag_size: 2420\n",
      "batch 59, loss: 1.1777, instance_loss: 1.5743, weighted_loss: 1.2967, label: 1, bag_size: 2701\n",
      "batch 79, loss: 0.1707, instance_loss: 0.2171, weighted_loss: 0.1846, label: 1, bag_size: 5071\n",
      "batch 99, loss: 0.9062, instance_loss: 1.0177, weighted_loss: 0.9396, label: 1, bag_size: 2036\n",
      "batch 119, loss: 0.6178, instance_loss: 0.7027, weighted_loss: 0.6433, label: 0, bag_size: 5527\n",
      "batch 139, loss: 0.3226, instance_loss: 0.4374, weighted_loss: 0.3571, label: 0, bag_size: 6209\n",
      "batch 159, loss: 0.4502, instance_loss: 0.5861, weighted_loss: 0.4910, label: 1, bag_size: 6878\n",
      "batch 179, loss: 0.0644, instance_loss: 0.0796, weighted_loss: 0.0689, label: 1, bag_size: 2005\n",
      "batch 199, loss: 0.3443, instance_loss: 0.3792, weighted_loss: 0.3548, label: 1, bag_size: 6463\n",
      "batch 219, loss: 0.8391, instance_loss: 1.0094, weighted_loss: 0.8902, label: 0, bag_size: 2244\n",
      "batch 239, loss: 0.1238, instance_loss: 0.1337, weighted_loss: 0.1268, label: 1, bag_size: 3533\n",
      "batch 259, loss: 0.8706, instance_loss: 1.0871, weighted_loss: 0.9356, label: 0, bag_size: 11797\n",
      "batch 279, loss: 0.6166, instance_loss: 0.8021, weighted_loss: 0.6722, label: 0, bag_size: 6391\n",
      "batch 299, loss: 0.2129, instance_loss: 0.2110, weighted_loss: 0.2123, label: 0, bag_size: 5382\n",
      "batch 319, loss: 0.5746, instance_loss: 0.7090, weighted_loss: 0.6149, label: 0, bag_size: 5331\n",
      "batch 339, loss: 0.5948, instance_loss: 0.7604, weighted_loss: 0.6445, label: 1, bag_size: 2701\n",
      "batch 359, loss: 1.2420, instance_loss: 1.6060, weighted_loss: 1.3512, label: 0, bag_size: 6058\n",
      "batch 379, loss: 1.0146, instance_loss: 1.3014, weighted_loss: 1.1006, label: 0, bag_size: 15139\n",
      "batch 399, loss: 0.5942, instance_loss: 0.7054, weighted_loss: 0.6276, label: 0, bag_size: 5894\n",
      "batch 419, loss: 0.4707, instance_loss: 0.5443, weighted_loss: 0.4928, label: 0, bag_size: 22936\n",
      "batch 439, loss: 0.5392, instance_loss: 0.6617, weighted_loss: 0.5760, label: 1, bag_size: 6760\n",
      "batch 459, loss: 0.5104, instance_loss: 0.6603, weighted_loss: 0.5553, label: 1, bag_size: 4087\n",
      "batch 479, loss: 0.0357, instance_loss: 0.0451, weighted_loss: 0.0385, label: 0, bag_size: 3448\n",
      "batch 499, loss: 0.2749, instance_loss: 0.3714, weighted_loss: 0.3038, label: 1, bag_size: 1552\n",
      "batch 519, loss: 0.1583, instance_loss: 0.1591, weighted_loss: 0.1585, label: 0, bag_size: 25933\n",
      "batch 539, loss: 1.0060, instance_loss: 1.2224, weighted_loss: 1.0709, label: 1, bag_size: 2905\n",
      "batch 559, loss: 1.2034, instance_loss: 1.5806, weighted_loss: 1.3166, label: 0, bag_size: 2766\n",
      "batch 579, loss: 0.3114, instance_loss: 0.3981, weighted_loss: 0.3374, label: 1, bag_size: 3208\n",
      "batch 599, loss: 0.2005, instance_loss: 0.2038, weighted_loss: 0.2015, label: 1, bag_size: 11563\n",
      "batch 619, loss: 0.1910, instance_loss: 0.1645, weighted_loss: 0.1831, label: 1, bag_size: 2905\n",
      "batch 639, loss: 0.1919, instance_loss: 0.1645, weighted_loss: 0.1836, label: 0, bag_size: 2908\n",
      "batch 659, loss: 0.1579, instance_loss: 0.1365, weighted_loss: 0.1515, label: 0, bag_size: 7285\n",
      "batch 679, loss: 0.5806, instance_loss: 0.7355, weighted_loss: 0.6271, label: 0, bag_size: 3372\n",
      "batch 699, loss: 0.5029, instance_loss: 0.6213, weighted_loss: 0.5384, label: 0, bag_size: 4737\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9346428571428571: correct 10468/11200\n",
      "class 1 clustering acc 0.4294642857142857: correct 2405/5600\n",
      "Epoch: 47, train_loss: 0.5708, train_clustering_loss:  0.7122, train_error: 0.2900\n",
      "class 0: acc 0.6965317919075145, correct 241/346\n",
      "class 1: acc 0.7231638418079096, correct 256/354\n",
      "\n",
      "Val Set, val_loss: 0.6344, val_error: 0.2887, auc: 0.6931\n",
      "class 0 clustering acc 0.7609536082474226: correct 1181/1552\n",
      "class 1 clustering acc 0.4536082474226804: correct 352/776\n",
      "class 0: acc 0.7627118644067796, correct 45/59\n",
      "class 1: acc 0.631578947368421, correct 24/38\n",
      "EarlyStopping counter: 25 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3879, instance_loss: 0.3903, weighted_loss: 0.3886, label: 0, bag_size: 5477\n",
      "batch 39, loss: 0.0597, instance_loss: 0.0771, weighted_loss: 0.0649, label: 1, bag_size: 11295\n",
      "batch 59, loss: 0.1105, instance_loss: 0.0911, weighted_loss: 0.1046, label: 0, bag_size: 14662\n",
      "batch 79, loss: 0.5314, instance_loss: 0.6733, weighted_loss: 0.5740, label: 1, bag_size: 3925\n",
      "batch 99, loss: 0.9743, instance_loss: 1.3530, weighted_loss: 1.0879, label: 1, bag_size: 18681\n",
      "batch 119, loss: 0.5373, instance_loss: 0.6141, weighted_loss: 0.5604, label: 1, bag_size: 2178\n",
      "batch 139, loss: 0.5000, instance_loss: 0.6035, weighted_loss: 0.5310, label: 1, bag_size: 6421\n",
      "batch 159, loss: 0.5029, instance_loss: 0.6083, weighted_loss: 0.5345, label: 1, bag_size: 5677\n",
      "batch 179, loss: 0.8057, instance_loss: 1.1040, weighted_loss: 0.8952, label: 1, bag_size: 3358\n",
      "batch 199, loss: 1.1589, instance_loss: 1.4610, weighted_loss: 1.2495, label: 0, bag_size: 4737\n",
      "batch 219, loss: 0.1525, instance_loss: 0.1266, weighted_loss: 0.1448, label: 1, bag_size: 5458\n",
      "batch 239, loss: 0.4363, instance_loss: 0.5098, weighted_loss: 0.4584, label: 0, bag_size: 2152\n",
      "batch 259, loss: 1.3684, instance_loss: 1.5114, weighted_loss: 1.4113, label: 0, bag_size: 2913\n",
      "batch 279, loss: 0.0304, instance_loss: 0.0296, weighted_loss: 0.0301, label: 0, bag_size: 11797\n",
      "batch 299, loss: 0.4717, instance_loss: 0.5515, weighted_loss: 0.4956, label: 0, bag_size: 2316\n",
      "batch 319, loss: 0.9902, instance_loss: 1.2175, weighted_loss: 1.0584, label: 1, bag_size: 5273\n",
      "batch 339, loss: 1.1873, instance_loss: 1.0734, weighted_loss: 1.1531, label: 1, bag_size: 1579\n",
      "batch 359, loss: 0.3898, instance_loss: 0.4001, weighted_loss: 0.3929, label: 0, bag_size: 2682\n",
      "batch 379, loss: 0.6037, instance_loss: 0.7043, weighted_loss: 0.6339, label: 1, bag_size: 11555\n",
      "batch 399, loss: 0.2021, instance_loss: 0.2211, weighted_loss: 0.2078, label: 1, bag_size: 4044\n",
      "batch 419, loss: 0.1751, instance_loss: 0.2332, weighted_loss: 0.1925, label: 0, bag_size: 1207\n",
      "batch 439, loss: 1.1430, instance_loss: 1.5066, weighted_loss: 1.2521, label: 0, bag_size: 6058\n",
      "batch 459, loss: 0.2571, instance_loss: 0.2823, weighted_loss: 0.2647, label: 1, bag_size: 15434\n",
      "batch 479, loss: 0.3051, instance_loss: 0.2931, weighted_loss: 0.3015, label: 0, bag_size: 4801\n",
      "batch 499, loss: 0.4917, instance_loss: 0.6304, weighted_loss: 0.5333, label: 1, bag_size: 3910\n",
      "batch 519, loss: 0.8459, instance_loss: 0.8377, weighted_loss: 0.8434, label: 1, bag_size: 12654\n",
      "batch 539, loss: 0.3633, instance_loss: 0.3687, weighted_loss: 0.3649, label: 0, bag_size: 7179\n",
      "batch 559, loss: 0.1950, instance_loss: 0.1793, weighted_loss: 0.1903, label: 1, bag_size: 5100\n",
      "batch 579, loss: 0.2617, instance_loss: 0.3259, weighted_loss: 0.2809, label: 0, bag_size: 5477\n",
      "batch 599, loss: 0.4281, instance_loss: 0.4673, weighted_loss: 0.4398, label: 1, bag_size: 6884\n",
      "batch 619, loss: 0.2660, instance_loss: 0.7044, weighted_loss: 0.3975, label: 1, bag_size: 3670\n",
      "batch 639, loss: 0.7819, instance_loss: 0.9735, weighted_loss: 0.8394, label: 0, bag_size: 22264\n",
      "batch 659, loss: 0.7697, instance_loss: 0.9351, weighted_loss: 0.8194, label: 1, bag_size: 7641\n",
      "batch 679, loss: 0.8858, instance_loss: 1.0759, weighted_loss: 0.9428, label: 1, bag_size: 2783\n",
      "batch 699, loss: 0.8126, instance_loss: 1.1248, weighted_loss: 0.9063, label: 1, bag_size: 3143\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9390178571428571: correct 10517/11200\n",
      "class 1 clustering acc 0.47660714285714284: correct 2669/5600\n",
      "Epoch: 48, train_loss: 0.5661, train_clustering_loss:  0.6980, train_error: 0.2986\n",
      "class 0: acc 0.7553191489361702, correct 284/376\n",
      "class 1: acc 0.6388888888888888, correct 207/324\n",
      "\n",
      "Val Set, val_loss: 0.6216, val_error: 0.2887, auc: 0.7110\n",
      "class 0 clustering acc 0.7326030927835051: correct 1137/1552\n",
      "class 1 clustering acc 0.3917525773195876: correct 304/776\n",
      "class 0: acc 0.864406779661017, correct 51/59\n",
      "class 1: acc 0.47368421052631576, correct 18/38\n",
      "EarlyStopping counter: 26 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0836, instance_loss: 0.1050, weighted_loss: 0.0900, label: 0, bag_size: 13411\n",
      "batch 39, loss: 0.1616, instance_loss: 0.1733, weighted_loss: 0.1651, label: 0, bag_size: 10568\n",
      "batch 59, loss: 0.4646, instance_loss: 0.7177, weighted_loss: 0.5405, label: 0, bag_size: 4801\n",
      "batch 79, loss: 0.7256, instance_loss: 1.0758, weighted_loss: 0.8307, label: 1, bag_size: 16451\n",
      "batch 99, loss: 0.5791, instance_loss: 0.7283, weighted_loss: 0.6239, label: 1, bag_size: 1004\n",
      "batch 119, loss: 0.3339, instance_loss: 0.3064, weighted_loss: 0.3256, label: 0, bag_size: 22594\n",
      "batch 139, loss: 0.1486, instance_loss: 0.1111, weighted_loss: 0.1373, label: 0, bag_size: 15682\n",
      "batch 159, loss: 0.0711, instance_loss: 0.0427, weighted_loss: 0.0626, label: 0, bag_size: 4086\n",
      "batch 179, loss: 1.0547, instance_loss: 1.3698, weighted_loss: 1.1492, label: 1, bag_size: 6319\n",
      "batch 199, loss: 0.6560, instance_loss: 0.7739, weighted_loss: 0.6914, label: 0, bag_size: 5499\n",
      "batch 219, loss: 0.8636, instance_loss: 1.2286, weighted_loss: 0.9731, label: 0, bag_size: 4558\n",
      "batch 239, loss: 0.4109, instance_loss: 0.5027, weighted_loss: 0.4384, label: 1, bag_size: 3672\n",
      "batch 259, loss: 0.5287, instance_loss: 0.7023, weighted_loss: 0.5808, label: 1, bag_size: 9643\n",
      "batch 279, loss: 0.7522, instance_loss: 0.9113, weighted_loss: 0.8000, label: 0, bag_size: 4228\n",
      "batch 299, loss: 0.2903, instance_loss: 0.3020, weighted_loss: 0.2938, label: 0, bag_size: 4228\n",
      "batch 319, loss: 0.5564, instance_loss: 0.6734, weighted_loss: 0.5915, label: 1, bag_size: 15434\n",
      "batch 339, loss: 0.7160, instance_loss: 0.8974, weighted_loss: 0.7704, label: 0, bag_size: 2013\n",
      "batch 359, loss: 0.3070, instance_loss: 0.2856, weighted_loss: 0.3006, label: 0, bag_size: 1053\n",
      "batch 379, loss: 0.6179, instance_loss: 0.6738, weighted_loss: 0.6347, label: 1, bag_size: 21711\n",
      "batch 399, loss: 0.0632, instance_loss: 0.0639, weighted_loss: 0.0634, label: 0, bag_size: 15122\n",
      "batch 419, loss: 0.2002, instance_loss: 0.8873, weighted_loss: 0.4063, label: 1, bag_size: 3338\n",
      "batch 439, loss: 1.3621, instance_loss: 1.7845, weighted_loss: 1.4888, label: 1, bag_size: 6759\n",
      "batch 459, loss: 0.4265, instance_loss: 0.5254, weighted_loss: 0.4562, label: 1, bag_size: 3990\n",
      "batch 479, loss: 0.3580, instance_loss: 0.4187, weighted_loss: 0.3762, label: 0, bag_size: 3372\n",
      "batch 499, loss: 0.6756, instance_loss: 0.8690, weighted_loss: 0.7336, label: 1, bag_size: 6759\n",
      "batch 519, loss: 0.5719, instance_loss: 0.7131, weighted_loss: 0.6143, label: 0, bag_size: 5504\n",
      "batch 539, loss: 0.2486, instance_loss: 0.2446, weighted_loss: 0.2474, label: 1, bag_size: 1579\n",
      "batch 559, loss: 0.6509, instance_loss: 0.8317, weighted_loss: 0.7051, label: 0, bag_size: 3674\n",
      "batch 579, loss: 2.1569, instance_loss: 2.6931, weighted_loss: 2.3177, label: 1, bag_size: 2250\n",
      "batch 599, loss: 0.9159, instance_loss: 1.3192, weighted_loss: 1.0368, label: 1, bag_size: 2301\n",
      "batch 619, loss: 0.2987, instance_loss: 0.2869, weighted_loss: 0.2952, label: 0, bag_size: 3090\n",
      "batch 639, loss: 0.5076, instance_loss: 0.6760, weighted_loss: 0.5581, label: 1, bag_size: 15213\n",
      "batch 659, loss: 1.7357, instance_loss: 2.3696, weighted_loss: 1.9259, label: 1, bag_size: 3159\n",
      "batch 679, loss: 0.4699, instance_loss: 0.5381, weighted_loss: 0.4904, label: 0, bag_size: 2006\n",
      "batch 699, loss: 0.6079, instance_loss: 0.7044, weighted_loss: 0.6368, label: 1, bag_size: 4458\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9417857142857143: correct 10548/11200\n",
      "class 1 clustering acc 0.48232142857142857: correct 2701/5600\n",
      "Epoch: 49, train_loss: 0.5412, train_clustering_loss:  0.6701, train_error: 0.2686\n",
      "class 0: acc 0.752112676056338, correct 267/355\n",
      "class 1: acc 0.7101449275362319, correct 245/345\n",
      "\n",
      "Val Set, val_loss: 0.6247, val_error: 0.2577, auc: 0.6998\n",
      "class 0 clustering acc 0.7268041237113402: correct 1128/1552\n",
      "class 1 clustering acc 0.48195876288659795: correct 374/776\n",
      "class 0: acc 0.8983050847457628, correct 53/59\n",
      "class 1: acc 0.5, correct 19/38\n",
      "EarlyStopping counter: 27 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3133, instance_loss: 0.3608, weighted_loss: 0.3276, label: 1, bag_size: 3338\n",
      "batch 39, loss: 0.4334, instance_loss: 0.4876, weighted_loss: 0.4496, label: 1, bag_size: 5939\n",
      "batch 59, loss: 0.1987, instance_loss: 0.2006, weighted_loss: 0.1993, label: 1, bag_size: 5108\n",
      "batch 79, loss: 0.0816, instance_loss: 0.0731, weighted_loss: 0.0790, label: 0, bag_size: 10536\n",
      "batch 99, loss: 0.4770, instance_loss: 0.5310, weighted_loss: 0.4932, label: 0, bag_size: 3674\n",
      "batch 119, loss: 0.1470, instance_loss: 0.2040, weighted_loss: 0.1641, label: 0, bag_size: 3090\n",
      "batch 139, loss: 0.2607, instance_loss: 0.2128, weighted_loss: 0.2463, label: 1, bag_size: 4737\n",
      "batch 159, loss: 0.2187, instance_loss: 0.2838, weighted_loss: 0.2382, label: 1, bag_size: 4815\n",
      "batch 179, loss: 0.0625, instance_loss: 0.0739, weighted_loss: 0.0659, label: 1, bag_size: 2493\n",
      "batch 199, loss: 0.5286, instance_loss: 0.4832, weighted_loss: 0.5150, label: 1, bag_size: 2356\n",
      "batch 219, loss: 0.6319, instance_loss: 0.7565, weighted_loss: 0.6693, label: 1, bag_size: 5677\n",
      "batch 239, loss: 0.7041, instance_loss: 0.9645, weighted_loss: 0.7822, label: 1, bag_size: 6760\n",
      "batch 259, loss: 0.5745, instance_loss: 0.7986, weighted_loss: 0.6417, label: 0, bag_size: 12517\n",
      "batch 279, loss: 0.7763, instance_loss: 0.9723, weighted_loss: 0.8351, label: 0, bag_size: 15139\n",
      "batch 299, loss: 0.2344, instance_loss: 0.2152, weighted_loss: 0.2287, label: 1, bag_size: 21059\n",
      "batch 319, loss: 1.5927, instance_loss: 2.0310, weighted_loss: 1.7242, label: 0, bag_size: 5426\n",
      "batch 339, loss: 0.8719, instance_loss: 1.2147, weighted_loss: 0.9747, label: 1, bag_size: 3966\n",
      "batch 359, loss: 1.1557, instance_loss: 1.3798, weighted_loss: 1.2230, label: 1, bag_size: 13348\n",
      "batch 379, loss: 0.6786, instance_loss: 1.0666, weighted_loss: 0.7950, label: 1, bag_size: 4387\n",
      "batch 399, loss: 0.0945, instance_loss: 0.1116, weighted_loss: 0.0996, label: 0, bag_size: 9172\n",
      "batch 419, loss: 0.6595, instance_loss: 0.8200, weighted_loss: 0.7077, label: 0, bag_size: 2302\n",
      "batch 439, loss: 0.3684, instance_loss: 0.3902, weighted_loss: 0.3749, label: 0, bag_size: 9499\n",
      "batch 459, loss: 0.7230, instance_loss: 0.9857, weighted_loss: 0.8018, label: 0, bag_size: 3773\n",
      "batch 479, loss: 0.1678, instance_loss: 0.1620, weighted_loss: 0.1660, label: 0, bag_size: 5121\n",
      "batch 499, loss: 0.3280, instance_loss: 0.3984, weighted_loss: 0.3492, label: 0, bag_size: 2428\n",
      "batch 519, loss: 0.4435, instance_loss: 0.4776, weighted_loss: 0.4537, label: 1, bag_size: 4007\n",
      "batch 539, loss: 0.7249, instance_loss: 0.9261, weighted_loss: 0.7853, label: 0, bag_size: 1648\n",
      "batch 559, loss: 0.7204, instance_loss: 0.8512, weighted_loss: 0.7597, label: 1, bag_size: 4510\n",
      "batch 579, loss: 0.2625, instance_loss: 0.2395, weighted_loss: 0.2556, label: 0, bag_size: 19443\n",
      "batch 599, loss: 0.3728, instance_loss: 0.4283, weighted_loss: 0.3894, label: 1, bag_size: 1587\n",
      "batch 619, loss: 0.9534, instance_loss: 1.0938, weighted_loss: 0.9955, label: 1, bag_size: 3191\n",
      "batch 639, loss: 0.3704, instance_loss: 0.4694, weighted_loss: 0.4001, label: 1, bag_size: 19013\n",
      "batch 659, loss: 0.3955, instance_loss: 0.5220, weighted_loss: 0.4335, label: 1, bag_size: 4112\n",
      "batch 679, loss: 0.4616, instance_loss: 0.4517, weighted_loss: 0.4586, label: 1, bag_size: 4698\n",
      "batch 699, loss: 0.4356, instance_loss: 0.5278, weighted_loss: 0.4633, label: 0, bag_size: 3548\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9327678571428571: correct 10447/11200\n",
      "class 1 clustering acc 0.4669642857142857: correct 2615/5600\n",
      "Epoch: 50, train_loss: 0.5603, train_clustering_loss:  0.7076, train_error: 0.2786\n",
      "class 0: acc 0.7180232558139535, correct 247/344\n",
      "class 1: acc 0.7247191011235955, correct 258/356\n",
      "\n",
      "Val Set, val_loss: 0.6458, val_error: 0.2680, auc: 0.6909\n",
      "class 0 clustering acc 0.7197164948453608: correct 1117/1552\n",
      "class 1 clustering acc 0.5038659793814433: correct 391/776\n",
      "class 0: acc 0.8305084745762712, correct 49/59\n",
      "class 1: acc 0.5789473684210527, correct 22/38\n",
      "EarlyStopping counter: 28 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2675, instance_loss: 0.2607, weighted_loss: 0.2655, label: 0, bag_size: 4468\n",
      "batch 39, loss: 1.8457, instance_loss: 2.1447, weighted_loss: 1.9354, label: 0, bag_size: 2762\n",
      "batch 59, loss: 0.1446, instance_loss: 0.1333, weighted_loss: 0.1412, label: 1, bag_size: 6878\n",
      "batch 79, loss: 0.0416, instance_loss: 0.0426, weighted_loss: 0.0419, label: 0, bag_size: 2048\n",
      "batch 99, loss: 0.1592, instance_loss: 0.1554, weighted_loss: 0.1581, label: 1, bag_size: 3374\n",
      "batch 119, loss: 0.0769, instance_loss: 0.1969, weighted_loss: 0.1129, label: 1, bag_size: 2126\n",
      "batch 139, loss: 1.0007, instance_loss: 1.4504, weighted_loss: 1.1356, label: 1, bag_size: 2524\n",
      "batch 159, loss: 0.4120, instance_loss: 0.5320, weighted_loss: 0.4480, label: 0, bag_size: 4180\n",
      "batch 179, loss: 0.4090, instance_loss: 0.4835, weighted_loss: 0.4313, label: 1, bag_size: 1251\n",
      "batch 199, loss: 0.6084, instance_loss: 0.5374, weighted_loss: 0.5871, label: 1, bag_size: 11295\n",
      "batch 219, loss: 1.1360, instance_loss: 1.2369, weighted_loss: 1.1663, label: 0, bag_size: 3843\n",
      "batch 239, loss: 0.7709, instance_loss: 1.0549, weighted_loss: 0.8561, label: 0, bag_size: 10568\n",
      "batch 259, loss: 0.7871, instance_loss: 0.8746, weighted_loss: 0.8134, label: 1, bag_size: 3191\n",
      "batch 279, loss: 0.8428, instance_loss: 1.0997, weighted_loss: 0.9199, label: 1, bag_size: 4243\n",
      "batch 299, loss: 0.2053, instance_loss: 0.1772, weighted_loss: 0.1969, label: 0, bag_size: 5996\n",
      "batch 319, loss: 0.1604, instance_loss: 0.1600, weighted_loss: 0.1603, label: 1, bag_size: 20435\n",
      "batch 339, loss: 0.1976, instance_loss: 0.2112, weighted_loss: 0.2017, label: 0, bag_size: 15912\n",
      "batch 359, loss: 1.8468, instance_loss: 2.3593, weighted_loss: 2.0006, label: 1, bag_size: 16538\n",
      "batch 379, loss: 0.0250, instance_loss: 0.0260, weighted_loss: 0.0253, label: 1, bag_size: 21473\n",
      "batch 399, loss: 1.1623, instance_loss: 1.6426, weighted_loss: 1.3064, label: 0, bag_size: 3635\n",
      "batch 419, loss: 0.1448, instance_loss: 0.1221, weighted_loss: 0.1380, label: 1, bag_size: 5428\n",
      "batch 439, loss: 1.6400, instance_loss: 2.1865, weighted_loss: 1.8039, label: 0, bag_size: 3149\n",
      "batch 459, loss: 0.7630, instance_loss: 1.0163, weighted_loss: 0.8390, label: 1, bag_size: 3925\n",
      "batch 479, loss: 1.4151, instance_loss: 1.7822, weighted_loss: 1.5252, label: 1, bag_size: 21711\n",
      "batch 499, loss: 0.1726, instance_loss: 0.2139, weighted_loss: 0.1850, label: 0, bag_size: 4146\n",
      "batch 519, loss: 0.2612, instance_loss: 0.3531, weighted_loss: 0.2888, label: 0, bag_size: 3161\n",
      "batch 539, loss: 0.7279, instance_loss: 0.9021, weighted_loss: 0.7802, label: 0, bag_size: 6605\n",
      "batch 559, loss: 0.6999, instance_loss: 0.8642, weighted_loss: 0.7492, label: 1, bag_size: 3143\n",
      "batch 579, loss: 0.0490, instance_loss: 0.0234, weighted_loss: 0.0413, label: 0, bag_size: 3065\n",
      "batch 599, loss: 0.3770, instance_loss: 0.4041, weighted_loss: 0.3851, label: 1, bag_size: 5570\n",
      "batch 619, loss: 1.0930, instance_loss: 1.3531, weighted_loss: 1.1710, label: 0, bag_size: 1648\n",
      "batch 639, loss: 0.6340, instance_loss: 0.8045, weighted_loss: 0.6852, label: 1, bag_size: 4795\n",
      "batch 659, loss: 0.2462, instance_loss: 0.2981, weighted_loss: 0.2618, label: 0, bag_size: 3764\n",
      "batch 679, loss: 0.0360, instance_loss: 0.0227, weighted_loss: 0.0320, label: 0, bag_size: 3699\n",
      "batch 699, loss: 1.5351, instance_loss: 1.9171, weighted_loss: 1.6497, label: 1, bag_size: 1374\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9444642857142858: correct 10578/11200\n",
      "class 1 clustering acc 0.4717857142857143: correct 2642/5600\n",
      "Epoch: 51, train_loss: 0.5633, train_clustering_loss:  0.6919, train_error: 0.3029\n",
      "class 0: acc 0.6745562130177515, correct 228/338\n",
      "class 1: acc 0.7182320441988951, correct 260/362\n",
      "\n",
      "Val Set, val_loss: 0.6362, val_error: 0.2784, auc: 0.7025\n",
      "class 0 clustering acc 0.7210051546391752: correct 1119/1552\n",
      "class 1 clustering acc 0.4948453608247423: correct 384/776\n",
      "class 0: acc 0.8305084745762712, correct 49/59\n",
      "class 1: acc 0.5526315789473685, correct 21/38\n",
      "EarlyStopping counter: 29 out of 20\n",
      "Early stopping\n",
      "Val error: 0.3093, ROC AUC: 0.6967\n",
      "Test error: 0.4430, ROC AUC: 0.5592\n",
      "class 0: acc 0.6938775510204082, correct 34/49\n",
      "class 1: acc 0.3333333333333333, correct 10/30\n",
      "\n",
      "Training Fold 1!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 702 samples\n",
      "Validating on 89 samples\n",
      "Testing on 85 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Setting tau to 1.0\n",
      "Done!\n",
      "TransformerMIL_SB(\n",
      "  (instance_loss_fn): SmoothTop1SVM()\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (transformer): TransformerEncoder_PerformerAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): SelfAttention(\n",
      "            (fast_attention): FastAttention(\n",
      "              (kernel_fn): ReLU()\n",
      "            )\n",
      "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attention): Attn_Net_Gated(\n",
      "    (attention_a): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_b): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (attention_V2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (attention_U2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (attention_weights2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Total number of parameters: 8406537\n",
      "Total number of trainable parameters: 8406537\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0006, instance_loss: 2.3584, weighted_loss: 0.7079, label: 0, bag_size: 10588\n",
      "batch 39, loss: 0.2517, instance_loss: 2.8332, weighted_loss: 1.0261, label: 1, bag_size: 6410\n",
      "batch 59, loss: 3.4523, instance_loss: 0.9858, weighted_loss: 2.7123, label: 0, bag_size: 2452\n",
      "batch 79, loss: 0.7294, instance_loss: 2.0607, weighted_loss: 1.1288, label: 1, bag_size: 4722\n",
      "batch 99, loss: 0.5372, instance_loss: 1.1445, weighted_loss: 0.7194, label: 1, bag_size: 3627\n",
      "batch 119, loss: 0.0289, instance_loss: 1.1301, weighted_loss: 0.3593, label: 0, bag_size: 2892\n",
      "batch 139, loss: 3.6328, instance_loss: 1.2839, weighted_loss: 2.9281, label: 0, bag_size: 11451\n",
      "batch 159, loss: 3.0502, instance_loss: 1.2518, weighted_loss: 2.5107, label: 0, bag_size: 5360\n",
      "batch 179, loss: 0.2580, instance_loss: 0.9267, weighted_loss: 0.4586, label: 0, bag_size: 2338\n",
      "batch 199, loss: 0.5927, instance_loss: 1.0873, weighted_loss: 0.7411, label: 1, bag_size: 6463\n",
      "batch 219, loss: 3.7145, instance_loss: 1.1110, weighted_loss: 2.9335, label: 0, bag_size: 4737\n",
      "batch 239, loss: 1.9885, instance_loss: 0.8050, weighted_loss: 1.6334, label: 1, bag_size: 2493\n",
      "batch 259, loss: 0.0927, instance_loss: 0.8328, weighted_loss: 0.3147, label: 1, bag_size: 4737\n",
      "batch 279, loss: 0.1898, instance_loss: 0.7454, weighted_loss: 0.3565, label: 1, bag_size: 3208\n",
      "batch 299, loss: 0.0766, instance_loss: 0.6818, weighted_loss: 0.2582, label: 1, bag_size: 2550\n",
      "batch 319, loss: 0.8258, instance_loss: 0.8304, weighted_loss: 0.8272, label: 1, bag_size: 2985\n",
      "batch 339, loss: 2.7666, instance_loss: 1.0518, weighted_loss: 2.2522, label: 1, bag_size: 5738\n",
      "batch 359, loss: 0.9274, instance_loss: 0.9906, weighted_loss: 0.9464, label: 1, bag_size: 5817\n",
      "batch 379, loss: 1.3211, instance_loss: 1.2802, weighted_loss: 1.3088, label: 0, bag_size: 14856\n",
      "batch 399, loss: 1.1896, instance_loss: 0.9753, weighted_loss: 1.1253, label: 0, bag_size: 15474\n",
      "batch 419, loss: 0.3049, instance_loss: 1.6828, weighted_loss: 0.7183, label: 1, bag_size: 5665\n",
      "batch 439, loss: 0.9220, instance_loss: 1.0290, weighted_loss: 0.9541, label: 1, bag_size: 2783\n",
      "batch 459, loss: 1.2357, instance_loss: 2.0625, weighted_loss: 1.4838, label: 1, bag_size: 5677\n",
      "batch 479, loss: 3.5148, instance_loss: 1.0138, weighted_loss: 2.7645, label: 0, bag_size: 18807\n",
      "batch 499, loss: 1.0409, instance_loss: 1.3025, weighted_loss: 1.1194, label: 0, bag_size: 5056\n",
      "batch 519, loss: 0.0785, instance_loss: 1.0529, weighted_loss: 0.3708, label: 0, bag_size: 3070\n",
      "batch 539, loss: 2.2767, instance_loss: 0.6973, weighted_loss: 1.8029, label: 1, bag_size: 5507\n",
      "batch 559, loss: 0.1088, instance_loss: 0.9621, weighted_loss: 0.3648, label: 0, bag_size: 2533\n",
      "batch 579, loss: 0.0339, instance_loss: 0.9604, weighted_loss: 0.3118, label: 0, bag_size: 4808\n",
      "batch 599, loss: 1.2527, instance_loss: 1.0283, weighted_loss: 1.1854, label: 0, bag_size: 4179\n",
      "batch 619, loss: 0.1080, instance_loss: 0.6770, weighted_loss: 0.2787, label: 1, bag_size: 1579\n",
      "batch 639, loss: 3.2962, instance_loss: 1.0734, weighted_loss: 2.6294, label: 1, bag_size: 6235\n",
      "batch 659, loss: 3.3809, instance_loss: 1.6064, weighted_loss: 2.8485, label: 1, bag_size: 27072\n",
      "batch 679, loss: 0.1431, instance_loss: 1.2703, weighted_loss: 0.4813, label: 0, bag_size: 7484\n",
      "batch 699, loss: 0.0555, instance_loss: 1.5261, weighted_loss: 0.4967, label: 0, bag_size: 4338\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9312678062678063: correct 10460/11232\n",
      "class 1 clustering acc 0.0633903133903134: correct 356/5616\n",
      "Epoch: 0, train_loss: 1.3187, train_clustering_loss:  1.2215, train_error: 0.4929\n",
      "class 0: acc 0.5263157894736842, correct 190/361\n",
      "class 1: acc 0.4868035190615836, correct 166/341\n",
      "\n",
      "Val Set, val_loss: 1.6111, val_error: 0.3933, auc: 0.4757\n",
      "class 0 clustering acc 1.0: correct 1424/1424\n",
      "class 1 clustering acc 0.0: correct 0/712\n",
      "class 0: acc 1.0, correct 54/54\n",
      "class 1: acc 0.0, correct 0/35\n",
      "Validation loss decreased (inf --> 1.611058).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1304, instance_loss: 1.5044, weighted_loss: 0.5426, label: 0, bag_size: 22936\n",
      "batch 39, loss: 1.4488, instance_loss: 1.2973, weighted_loss: 1.4033, label: 1, bag_size: 11657\n",
      "batch 59, loss: 1.0875, instance_loss: 0.9168, weighted_loss: 1.0363, label: 0, bag_size: 1927\n",
      "batch 79, loss: 1.9284, instance_loss: 1.0310, weighted_loss: 1.6592, label: 0, bag_size: 2540\n",
      "batch 99, loss: 0.1926, instance_loss: 1.2007, weighted_loss: 0.4950, label: 1, bag_size: 3518\n",
      "batch 119, loss: 2.1119, instance_loss: 1.4052, weighted_loss: 1.8999, label: 0, bag_size: 20134\n",
      "batch 139, loss: 1.5292, instance_loss: 0.9048, weighted_loss: 1.3419, label: 1, bag_size: 5887\n",
      "batch 159, loss: 0.3394, instance_loss: 0.9952, weighted_loss: 0.5361, label: 0, bag_size: 1671\n",
      "batch 179, loss: 0.0375, instance_loss: 0.6931, weighted_loss: 0.2342, label: 0, bag_size: 5924\n",
      "batch 199, loss: 1.0658, instance_loss: 1.0353, weighted_loss: 1.0567, label: 0, bag_size: 4320\n",
      "batch 219, loss: 0.8207, instance_loss: 0.8356, weighted_loss: 0.8252, label: 1, bag_size: 16451\n",
      "batch 239, loss: 0.9829, instance_loss: 1.6100, weighted_loss: 1.1710, label: 1, bag_size: 4458\n",
      "batch 259, loss: 1.8982, instance_loss: 1.0385, weighted_loss: 1.6403, label: 0, bag_size: 5590\n",
      "batch 279, loss: 0.7127, instance_loss: 0.9439, weighted_loss: 0.7821, label: 0, bag_size: 5536\n",
      "batch 299, loss: 2.7776, instance_loss: 1.5447, weighted_loss: 2.4077, label: 1, bag_size: 3402\n",
      "batch 319, loss: 2.1490, instance_loss: 1.1584, weighted_loss: 1.8518, label: 1, bag_size: 2829\n",
      "batch 339, loss: 1.4206, instance_loss: 0.7419, weighted_loss: 1.2170, label: 1, bag_size: 3627\n",
      "batch 359, loss: 0.9503, instance_loss: 1.1352, weighted_loss: 1.0058, label: 1, bag_size: 3020\n",
      "batch 379, loss: 0.9544, instance_loss: 1.5575, weighted_loss: 1.1353, label: 0, bag_size: 2678\n",
      "batch 399, loss: 1.8343, instance_loss: 0.9657, weighted_loss: 1.5737, label: 1, bag_size: 3893\n",
      "batch 419, loss: 0.6005, instance_loss: 0.7812, weighted_loss: 0.6547, label: 1, bag_size: 3949\n",
      "batch 439, loss: 0.3457, instance_loss: 1.7746, weighted_loss: 0.7743, label: 1, bag_size: 8331\n",
      "batch 459, loss: 0.5098, instance_loss: 1.2918, weighted_loss: 0.7444, label: 0, bag_size: 5021\n",
      "batch 479, loss: 0.4366, instance_loss: 0.8119, weighted_loss: 0.5492, label: 0, bag_size: 18376\n",
      "batch 499, loss: 0.2218, instance_loss: 1.1291, weighted_loss: 0.4940, label: 0, bag_size: 941\n",
      "batch 519, loss: 1.1788, instance_loss: 0.9268, weighted_loss: 1.1032, label: 1, bag_size: 5539\n",
      "batch 539, loss: 0.5010, instance_loss: 0.6630, weighted_loss: 0.5496, label: 1, bag_size: 6190\n",
      "batch 559, loss: 0.9308, instance_loss: 0.9964, weighted_loss: 0.9504, label: 1, bag_size: 13348\n",
      "batch 579, loss: 0.1025, instance_loss: 0.7311, weighted_loss: 0.2911, label: 0, bag_size: 4649\n",
      "batch 599, loss: 1.8310, instance_loss: 1.2421, weighted_loss: 1.6543, label: 0, bag_size: 2682\n",
      "batch 619, loss: 0.2969, instance_loss: 0.9760, weighted_loss: 0.5007, label: 0, bag_size: 3184\n",
      "batch 639, loss: 1.7100, instance_loss: 1.2482, weighted_loss: 1.5715, label: 1, bag_size: 2626\n",
      "batch 659, loss: 0.6506, instance_loss: 1.0381, weighted_loss: 0.7669, label: 0, bag_size: 3928\n",
      "batch 679, loss: 3.1838, instance_loss: 1.5102, weighted_loss: 2.6817, label: 1, bag_size: 3627\n",
      "batch 699, loss: 0.6208, instance_loss: 0.7370, weighted_loss: 0.6556, label: 1, bag_size: 22171\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9736467236467237: correct 10936/11232\n",
      "class 1 clustering acc 0.026887464387464387: correct 151/5616\n",
      "Epoch: 1, train_loss: 0.9283, train_clustering_loss:  1.0528, train_error: 0.5071\n",
      "class 0: acc 0.49719101123595505, correct 177/356\n",
      "class 1: acc 0.4884393063583815, correct 169/346\n",
      "\n",
      "Val Set, val_loss: 1.5484, val_error: 0.6067, auc: 0.4762\n",
      "class 0 clustering acc 0.702247191011236: correct 1000/1424\n",
      "class 1 clustering acc 0.35674157303370785: correct 254/712\n",
      "class 0: acc 0.0, correct 0/54\n",
      "class 1: acc 1.0, correct 35/35\n",
      "Validation loss decreased (1.611058 --> 1.548442).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 1.7835, instance_loss: 1.0947, weighted_loss: 1.5769, label: 1, bag_size: 4243\n",
      "batch 39, loss: 0.1639, instance_loss: 1.8606, weighted_loss: 0.6729, label: 0, bag_size: 17546\n",
      "batch 59, loss: 0.4539, instance_loss: 0.9567, weighted_loss: 0.6048, label: 1, bag_size: 2754\n",
      "batch 79, loss: 1.5877, instance_loss: 1.0938, weighted_loss: 1.4396, label: 1, bag_size: 5072\n",
      "batch 99, loss: 0.9998, instance_loss: 0.6319, weighted_loss: 0.8895, label: 1, bag_size: 4722\n",
      "batch 119, loss: 0.1794, instance_loss: 0.9404, weighted_loss: 0.4077, label: 0, bag_size: 3529\n",
      "batch 139, loss: 2.0943, instance_loss: 1.1148, weighted_loss: 1.8005, label: 0, bag_size: 2388\n",
      "batch 159, loss: 0.4803, instance_loss: 0.9154, weighted_loss: 0.6108, label: 1, bag_size: 15213\n",
      "batch 179, loss: 0.2076, instance_loss: 0.6792, weighted_loss: 0.3491, label: 0, bag_size: 14142\n",
      "batch 199, loss: 2.1733, instance_loss: 0.7916, weighted_loss: 1.7588, label: 1, bag_size: 3962\n",
      "batch 219, loss: 0.5106, instance_loss: 0.8328, weighted_loss: 0.6072, label: 1, bag_size: 3208\n",
      "batch 239, loss: 0.1411, instance_loss: 1.0693, weighted_loss: 0.4196, label: 1, bag_size: 2465\n",
      "batch 259, loss: 1.3215, instance_loss: 0.7267, weighted_loss: 1.1431, label: 0, bag_size: 5697\n",
      "batch 279, loss: 0.9926, instance_loss: 1.4724, weighted_loss: 1.1365, label: 1, bag_size: 5819\n",
      "batch 299, loss: 0.6144, instance_loss: 0.6873, weighted_loss: 0.6363, label: 1, bag_size: 3126\n",
      "batch 319, loss: 1.2240, instance_loss: 0.7762, weighted_loss: 1.0897, label: 1, bag_size: 3081\n",
      "batch 339, loss: 0.7568, instance_loss: 0.9126, weighted_loss: 0.8035, label: 0, bag_size: 2540\n",
      "batch 359, loss: 0.2033, instance_loss: 0.7459, weighted_loss: 0.3661, label: 1, bag_size: 3243\n",
      "batch 379, loss: 0.6955, instance_loss: 0.6865, weighted_loss: 0.6928, label: 0, bag_size: 4162\n",
      "batch 399, loss: 0.5052, instance_loss: 1.2095, weighted_loss: 0.7164, label: 0, bag_size: 16663\n",
      "batch 419, loss: 0.4062, instance_loss: 1.8504, weighted_loss: 0.8394, label: 1, bag_size: 24092\n",
      "batch 439, loss: 0.8436, instance_loss: 1.6517, weighted_loss: 1.0860, label: 0, bag_size: 3778\n",
      "batch 459, loss: 1.2250, instance_loss: 0.8222, weighted_loss: 1.1042, label: 1, bag_size: 2877\n",
      "batch 479, loss: 1.0118, instance_loss: 1.0620, weighted_loss: 1.0269, label: 1, bag_size: 3184\n",
      "batch 499, loss: 0.3290, instance_loss: 1.0974, weighted_loss: 0.5595, label: 1, bag_size: 20256\n",
      "batch 519, loss: 0.7291, instance_loss: 1.2822, weighted_loss: 0.8951, label: 1, bag_size: 2465\n",
      "batch 539, loss: 0.0903, instance_loss: 0.9407, weighted_loss: 0.3454, label: 0, bag_size: 26374\n",
      "batch 559, loss: 1.0254, instance_loss: 1.2303, weighted_loss: 1.0869, label: 0, bag_size: 3615\n",
      "batch 579, loss: 0.3770, instance_loss: 1.3109, weighted_loss: 0.6572, label: 0, bag_size: 3553\n",
      "batch 599, loss: 0.9311, instance_loss: 0.9487, weighted_loss: 0.9364, label: 0, bag_size: 1669\n",
      "batch 619, loss: 0.7680, instance_loss: 1.4120, weighted_loss: 0.9612, label: 1, bag_size: 19173\n",
      "batch 639, loss: 0.1659, instance_loss: 0.7598, weighted_loss: 0.3441, label: 0, bag_size: 19055\n",
      "batch 659, loss: 1.7049, instance_loss: 1.1281, weighted_loss: 1.5319, label: 0, bag_size: 3869\n",
      "batch 679, loss: 0.5939, instance_loss: 1.3776, weighted_loss: 0.8290, label: 1, bag_size: 3454\n",
      "batch 699, loss: 0.1721, instance_loss: 0.8550, weighted_loss: 0.3770, label: 0, bag_size: 5056\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9764066951566952: correct 10967/11232\n",
      "class 1 clustering acc 0.03632478632478633: correct 204/5616\n",
      "Epoch: 2, train_loss: 0.8283, train_clustering_loss:  1.0650, train_error: 0.4729\n",
      "class 0: acc 0.5142045454545454, correct 181/352\n",
      "class 1: acc 0.54, correct 189/350\n",
      "\n",
      "Val Set, val_loss: 0.8377, val_error: 0.3933, auc: 0.6074\n",
      "class 0 clustering acc 0.8405898876404494: correct 1197/1424\n",
      "class 1 clustering acc 0.526685393258427: correct 375/712\n",
      "class 0: acc 1.0, correct 54/54\n",
      "class 1: acc 0.0, correct 0/35\n",
      "Validation loss decreased (1.548442 --> 0.837701).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1681, instance_loss: 1.1745, weighted_loss: 0.4700, label: 0, bag_size: 3317\n",
      "batch 39, loss: 1.3678, instance_loss: 0.9337, weighted_loss: 1.2376, label: 0, bag_size: 5697\n",
      "batch 59, loss: 0.8472, instance_loss: 1.6024, weighted_loss: 1.0737, label: 1, bag_size: 20056\n",
      "batch 79, loss: 0.9505, instance_loss: 1.5547, weighted_loss: 1.1318, label: 0, bag_size: 4175\n",
      "batch 99, loss: 1.0407, instance_loss: 0.9735, weighted_loss: 1.0205, label: 0, bag_size: 3161\n",
      "batch 119, loss: 1.0628, instance_loss: 1.0170, weighted_loss: 1.0491, label: 0, bag_size: 5108\n",
      "batch 139, loss: 0.7124, instance_loss: 0.6765, weighted_loss: 0.7016, label: 0, bag_size: 2244\n",
      "batch 159, loss: 1.4592, instance_loss: 0.9916, weighted_loss: 1.3189, label: 0, bag_size: 5338\n",
      "batch 179, loss: 2.6607, instance_loss: 0.8514, weighted_loss: 2.1179, label: 1, bag_size: 5817\n",
      "batch 199, loss: 0.4425, instance_loss: 0.9204, weighted_loss: 0.5859, label: 1, bag_size: 2646\n",
      "batch 219, loss: 1.1705, instance_loss: 0.9034, weighted_loss: 1.0904, label: 0, bag_size: 3087\n",
      "batch 239, loss: 0.3754, instance_loss: 1.4020, weighted_loss: 0.6834, label: 1, bag_size: 6878\n",
      "batch 259, loss: 1.6812, instance_loss: 0.7258, weighted_loss: 1.3946, label: 1, bag_size: 3243\n",
      "batch 279, loss: 0.0262, instance_loss: 0.6574, weighted_loss: 0.2156, label: 0, bag_size: 5382\n",
      "batch 299, loss: 0.6471, instance_loss: 1.0989, weighted_loss: 0.7826, label: 1, bag_size: 3578\n",
      "batch 319, loss: 0.4955, instance_loss: 0.9087, weighted_loss: 0.6194, label: 1, bag_size: 5665\n",
      "batch 339, loss: 0.0974, instance_loss: 0.7921, weighted_loss: 0.3058, label: 1, bag_size: 2938\n",
      "batch 359, loss: 0.9908, instance_loss: 0.8340, weighted_loss: 0.9437, label: 1, bag_size: 3672\n",
      "batch 379, loss: 0.4516, instance_loss: 0.8826, weighted_loss: 0.5809, label: 1, bag_size: 4268\n",
      "batch 399, loss: 0.4890, instance_loss: 0.9437, weighted_loss: 0.6254, label: 0, bag_size: 13609\n",
      "batch 419, loss: 0.2005, instance_loss: 0.9504, weighted_loss: 0.4254, label: 0, bag_size: 4597\n",
      "batch 439, loss: 1.1294, instance_loss: 0.8850, weighted_loss: 1.0561, label: 0, bag_size: 5924\n",
      "batch 459, loss: 1.1806, instance_loss: 0.8314, weighted_loss: 1.0758, label: 0, bag_size: 6534\n",
      "batch 479, loss: 0.2849, instance_loss: 1.2430, weighted_loss: 0.5724, label: 0, bag_size: 7667\n",
      "batch 499, loss: 1.0972, instance_loss: 0.8989, weighted_loss: 1.0377, label: 0, bag_size: 4506\n",
      "batch 519, loss: 0.5613, instance_loss: 1.0114, weighted_loss: 0.6963, label: 0, bag_size: 575\n",
      "batch 539, loss: 0.1864, instance_loss: 0.6917, weighted_loss: 0.3380, label: 1, bag_size: 5428\n",
      "batch 559, loss: 0.7898, instance_loss: 1.5551, weighted_loss: 1.0194, label: 1, bag_size: 3518\n",
      "batch 579, loss: 0.6362, instance_loss: 1.0964, weighted_loss: 0.7742, label: 0, bag_size: 6135\n",
      "batch 599, loss: 0.6676, instance_loss: 0.9751, weighted_loss: 0.7598, label: 1, bag_size: 3020\n",
      "batch 619, loss: 0.4248, instance_loss: 1.4076, weighted_loss: 0.7196, label: 0, bag_size: 4754\n",
      "batch 639, loss: 0.4942, instance_loss: 0.8965, weighted_loss: 0.6149, label: 1, bag_size: 11555\n",
      "batch 659, loss: 0.2768, instance_loss: 0.7464, weighted_loss: 0.4177, label: 0, bag_size: 4548\n",
      "batch 679, loss: 0.2362, instance_loss: 1.1174, weighted_loss: 0.5006, label: 0, bag_size: 4573\n",
      "batch 699, loss: 1.3269, instance_loss: 0.9961, weighted_loss: 1.2276, label: 1, bag_size: 1825\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9903846153846154: correct 11124/11232\n",
      "class 1 clustering acc 0.012464387464387465: correct 70/5616\n",
      "Epoch: 3, train_loss: 0.8026, train_clustering_loss:  1.0300, train_error: 0.5028\n",
      "class 0: acc 0.48158640226628896, correct 170/353\n",
      "class 1: acc 0.5128939828080229, correct 179/349\n",
      "\n",
      "Val Set, val_loss: 0.7812, val_error: 0.3933, auc: 0.5672\n",
      "class 0 clustering acc 1.0: correct 1424/1424\n",
      "class 1 clustering acc 0.0: correct 0/712\n",
      "class 0: acc 1.0, correct 54/54\n",
      "class 1: acc 0.0, correct 0/35\n",
      "Validation loss decreased (0.837701 --> 0.781194).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 2.4435, instance_loss: 0.8272, weighted_loss: 1.9586, label: 0, bag_size: 2856\n",
      "batch 39, loss: 1.8252, instance_loss: 0.9315, weighted_loss: 1.5571, label: 0, bag_size: 4628\n",
      "batch 59, loss: 0.1414, instance_loss: 0.9308, weighted_loss: 0.3782, label: 1, bag_size: 3020\n",
      "batch 79, loss: 1.2626, instance_loss: 1.0480, weighted_loss: 1.1982, label: 0, bag_size: 4560\n",
      "batch 99, loss: 0.7060, instance_loss: 0.7059, weighted_loss: 0.7059, label: 0, bag_size: 2238\n",
      "batch 119, loss: 1.5720, instance_loss: 1.3522, weighted_loss: 1.5060, label: 1, bag_size: 2178\n",
      "batch 139, loss: 0.8210, instance_loss: 0.9362, weighted_loss: 0.8556, label: 0, bag_size: 12673\n",
      "batch 159, loss: 0.2203, instance_loss: 0.7067, weighted_loss: 0.3662, label: 0, bag_size: 13846\n",
      "batch 179, loss: 0.6064, instance_loss: 1.4208, weighted_loss: 0.8507, label: 0, bag_size: 2394\n",
      "batch 199, loss: 2.0625, instance_loss: 2.0194, weighted_loss: 2.0496, label: 1, bag_size: 3287\n",
      "batch 219, loss: 1.4179, instance_loss: 0.9581, weighted_loss: 1.2799, label: 0, bag_size: 7484\n",
      "batch 239, loss: 2.0656, instance_loss: 1.5507, weighted_loss: 1.9111, label: 1, bag_size: 2293\n",
      "batch 259, loss: 1.3761, instance_loss: 0.7534, weighted_loss: 1.1893, label: 1, bag_size: 3454\n",
      "batch 279, loss: 0.3171, instance_loss: 0.7133, weighted_loss: 0.4360, label: 0, bag_size: 5527\n",
      "batch 299, loss: 0.6438, instance_loss: 1.1119, weighted_loss: 0.7843, label: 1, bag_size: 3226\n",
      "batch 319, loss: 1.2613, instance_loss: 0.9518, weighted_loss: 1.1684, label: 0, bag_size: 3714\n",
      "batch 339, loss: 0.3793, instance_loss: 0.5630, weighted_loss: 0.4344, label: 1, bag_size: 2899\n",
      "batch 359, loss: 0.5911, instance_loss: 0.8342, weighted_loss: 0.6640, label: 0, bag_size: 3489\n",
      "batch 379, loss: 0.7770, instance_loss: 0.9114, weighted_loss: 0.8174, label: 0, bag_size: 3463\n",
      "batch 399, loss: 0.2660, instance_loss: 1.0400, weighted_loss: 0.4982, label: 1, bag_size: 2890\n",
      "batch 419, loss: 1.5174, instance_loss: 0.7018, weighted_loss: 1.2727, label: 1, bag_size: 5298\n",
      "batch 439, loss: 0.6755, instance_loss: 1.2413, weighted_loss: 0.8452, label: 1, bag_size: 20149\n",
      "batch 459, loss: 0.4756, instance_loss: 1.1299, weighted_loss: 0.6719, label: 1, bag_size: 1004\n",
      "batch 479, loss: 0.4622, instance_loss: 0.7778, weighted_loss: 0.5569, label: 1, bag_size: 4046\n",
      "batch 499, loss: 1.5597, instance_loss: 0.9447, weighted_loss: 1.3752, label: 1, bag_size: 2005\n",
      "batch 519, loss: 0.3027, instance_loss: 0.9765, weighted_loss: 0.5048, label: 1, bag_size: 2176\n",
      "batch 539, loss: 0.7800, instance_loss: 0.9773, weighted_loss: 0.8392, label: 1, bag_size: 3358\n",
      "batch 559, loss: 0.3251, instance_loss: 0.9193, weighted_loss: 0.5034, label: 0, bag_size: 5536\n",
      "batch 579, loss: 0.5138, instance_loss: 0.9471, weighted_loss: 0.6438, label: 1, bag_size: 5935\n",
      "batch 599, loss: 1.8132, instance_loss: 0.9731, weighted_loss: 1.5612, label: 0, bag_size: 3149\n",
      "batch 619, loss: 0.3414, instance_loss: 1.0222, weighted_loss: 0.5456, label: 1, bag_size: 3764\n",
      "batch 639, loss: 0.1589, instance_loss: 0.8599, weighted_loss: 0.3692, label: 1, bag_size: 2961\n",
      "batch 659, loss: 0.3442, instance_loss: 1.4159, weighted_loss: 0.6657, label: 0, bag_size: 6463\n",
      "batch 679, loss: 0.2289, instance_loss: 0.9878, weighted_loss: 0.4566, label: 1, bag_size: 2343\n",
      "batch 699, loss: 1.2005, instance_loss: 1.1952, weighted_loss: 1.1989, label: 1, bag_size: 3405\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9886039886039886: correct 11104/11232\n",
      "class 1 clustering acc 0.02047720797720798: correct 115/5616\n",
      "Epoch: 4, train_loss: 0.7910, train_clustering_loss:  1.0110, train_error: 0.4843\n",
      "class 0: acc 0.5487465181058496, correct 197/359\n",
      "class 1: acc 0.48104956268221577, correct 165/343\n",
      "\n",
      "Val Set, val_loss: 0.6714, val_error: 0.3933, auc: 0.5111\n",
      "class 0 clustering acc 0.9943820224719101: correct 1416/1424\n",
      "class 1 clustering acc 0.011235955056179775: correct 8/712\n",
      "class 0: acc 1.0, correct 54/54\n",
      "class 1: acc 0.0, correct 0/35\n",
      "Validation loss decreased (0.781194 --> 0.671394).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4992, instance_loss: 0.7275, weighted_loss: 0.5677, label: 1, bag_size: 4268\n",
      "batch 39, loss: 1.6713, instance_loss: 0.7358, weighted_loss: 1.3907, label: 1, bag_size: 3533\n",
      "batch 59, loss: 0.6638, instance_loss: 1.0134, weighted_loss: 0.7687, label: 1, bag_size: 3243\n",
      "batch 79, loss: 0.0831, instance_loss: 0.7814, weighted_loss: 0.2926, label: 0, bag_size: 14635\n",
      "batch 99, loss: 0.8365, instance_loss: 1.0419, weighted_loss: 0.8981, label: 1, bag_size: 5651\n",
      "batch 119, loss: 0.7260, instance_loss: 0.9475, weighted_loss: 0.7924, label: 1, bag_size: 3454\n",
      "batch 139, loss: 0.0465, instance_loss: 0.6549, weighted_loss: 0.2290, label: 1, bag_size: 2301\n",
      "batch 159, loss: 0.4445, instance_loss: 1.0905, weighted_loss: 0.6383, label: 0, bag_size: 4692\n",
      "batch 179, loss: 0.0591, instance_loss: 0.9250, weighted_loss: 0.3189, label: 1, bag_size: 3374\n",
      "batch 199, loss: 1.3709, instance_loss: 0.9058, weighted_loss: 1.2314, label: 0, bag_size: 3517\n",
      "batch 219, loss: 0.4784, instance_loss: 0.9545, weighted_loss: 0.6212, label: 0, bag_size: 1349\n",
      "batch 239, loss: 0.5264, instance_loss: 0.8553, weighted_loss: 0.6251, label: 0, bag_size: 2856\n",
      "batch 259, loss: 1.0007, instance_loss: 1.4099, weighted_loss: 1.1234, label: 0, bag_size: 2766\n",
      "batch 279, loss: 1.1128, instance_loss: 1.5398, weighted_loss: 1.2409, label: 1, bag_size: 12948\n",
      "batch 299, loss: 0.4586, instance_loss: 1.1917, weighted_loss: 0.6785, label: 0, bag_size: 5590\n",
      "batch 319, loss: 0.5490, instance_loss: 1.0218, weighted_loss: 0.6909, label: 1, bag_size: 6151\n",
      "batch 339, loss: 0.0538, instance_loss: 0.7284, weighted_loss: 0.2562, label: 0, bag_size: 4500\n",
      "batch 359, loss: 1.3291, instance_loss: 1.1350, weighted_loss: 1.2709, label: 0, bag_size: 7484\n",
      "batch 379, loss: 0.4535, instance_loss: 1.0624, weighted_loss: 0.6362, label: 0, bag_size: 2655\n",
      "batch 399, loss: 2.6127, instance_loss: 0.8189, weighted_loss: 2.0745, label: 0, bag_size: 5642\n",
      "batch 419, loss: 1.1573, instance_loss: 1.1742, weighted_loss: 1.1624, label: 0, bag_size: 3087\n",
      "batch 439, loss: 0.2783, instance_loss: 1.0699, weighted_loss: 0.5158, label: 0, bag_size: 6058\n",
      "batch 459, loss: 0.9718, instance_loss: 1.1350, weighted_loss: 1.0208, label: 0, bag_size: 5527\n",
      "batch 479, loss: 0.6665, instance_loss: 0.9772, weighted_loss: 0.7597, label: 0, bag_size: 972\n",
      "batch 499, loss: 0.8168, instance_loss: 0.8584, weighted_loss: 0.8293, label: 0, bag_size: 3066\n",
      "batch 519, loss: 0.3364, instance_loss: 0.7793, weighted_loss: 0.4693, label: 0, bag_size: 1207\n",
      "batch 539, loss: 0.9266, instance_loss: 0.9881, weighted_loss: 0.9450, label: 1, bag_size: 3533\n",
      "batch 559, loss: 0.1385, instance_loss: 0.7639, weighted_loss: 0.3261, label: 0, bag_size: 27817\n",
      "batch 579, loss: 0.3966, instance_loss: 0.8608, weighted_loss: 0.5359, label: 0, bag_size: 3295\n",
      "batch 599, loss: 0.2615, instance_loss: 0.7098, weighted_loss: 0.3960, label: 1, bag_size: 2250\n",
      "batch 619, loss: 0.3262, instance_loss: 0.7351, weighted_loss: 0.4489, label: 1, bag_size: 3126\n",
      "batch 639, loss: 0.9148, instance_loss: 0.6455, weighted_loss: 0.8340, label: 0, bag_size: 26374\n",
      "batch 659, loss: 0.8947, instance_loss: 1.3304, weighted_loss: 1.0254, label: 0, bag_size: 4661\n",
      "batch 679, loss: 0.7970, instance_loss: 0.8442, weighted_loss: 0.8111, label: 0, bag_size: 15682\n",
      "batch 699, loss: 1.1131, instance_loss: 1.0020, weighted_loss: 1.0798, label: 1, bag_size: 16427\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.990295584045584: correct 11123/11232\n",
      "class 1 clustering acc 0.028133903133903133: correct 158/5616\n",
      "Epoch: 5, train_loss: 0.7559, train_clustering_loss:  0.9973, train_error: 0.4829\n",
      "class 0: acc 0.49707602339181284, correct 170/342\n",
      "class 1: acc 0.5361111111111111, correct 193/360\n",
      "\n",
      "Val Set, val_loss: 0.7433, val_error: 0.5843, auc: 0.5074\n",
      "class 0 clustering acc 1.0: correct 1424/1424\n",
      "class 1 clustering acc 0.0: correct 0/712\n",
      "class 0: acc 0.12962962962962962, correct 7/54\n",
      "class 1: acc 0.8571428571428571, correct 30/35\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.2431, instance_loss: 0.8207, weighted_loss: 1.1164, label: 1, bag_size: 3159\n",
      "batch 39, loss: 1.2906, instance_loss: 1.2592, weighted_loss: 1.2812, label: 1, bag_size: 21473\n",
      "batch 59, loss: 0.5882, instance_loss: 0.7272, weighted_loss: 0.6299, label: 1, bag_size: 3834\n",
      "batch 79, loss: 1.1497, instance_loss: 0.9168, weighted_loss: 1.0799, label: 0, bag_size: 938\n",
      "batch 99, loss: 0.2750, instance_loss: 0.8946, weighted_loss: 0.4609, label: 0, bag_size: 4332\n",
      "batch 119, loss: 0.8058, instance_loss: 0.7801, weighted_loss: 0.7981, label: 1, bag_size: 14564\n",
      "batch 139, loss: 0.7799, instance_loss: 1.0523, weighted_loss: 0.8616, label: 1, bag_size: 5458\n",
      "batch 159, loss: 0.5483, instance_loss: 0.8971, weighted_loss: 0.6530, label: 1, bag_size: 2005\n",
      "batch 179, loss: 0.4109, instance_loss: 0.9026, weighted_loss: 0.5584, label: 1, bag_size: 5672\n",
      "batch 199, loss: 0.4970, instance_loss: 0.5901, weighted_loss: 0.5249, label: 1, bag_size: 16427\n",
      "batch 219, loss: 0.8997, instance_loss: 0.8499, weighted_loss: 0.8848, label: 0, bag_size: 7027\n",
      "batch 239, loss: 2.0095, instance_loss: 1.0019, weighted_loss: 1.7072, label: 1, bag_size: 21711\n",
      "batch 259, loss: 1.0690, instance_loss: 1.0073, weighted_loss: 1.0505, label: 0, bag_size: 4390\n",
      "batch 279, loss: 0.4267, instance_loss: 0.9627, weighted_loss: 0.5875, label: 1, bag_size: 16451\n",
      "batch 299, loss: 0.5003, instance_loss: 0.8945, weighted_loss: 0.6186, label: 1, bag_size: 3910\n",
      "batch 319, loss: 0.6129, instance_loss: 0.8463, weighted_loss: 0.6829, label: 0, bag_size: 2146\n",
      "batch 339, loss: 2.0382, instance_loss: 1.0461, weighted_loss: 1.7406, label: 1, bag_size: 5677\n",
      "batch 359, loss: 0.8187, instance_loss: 1.1940, weighted_loss: 0.9313, label: 1, bag_size: 1699\n",
      "batch 379, loss: 0.2354, instance_loss: 0.7556, weighted_loss: 0.3915, label: 0, bag_size: 11451\n",
      "batch 399, loss: 0.5569, instance_loss: 0.9594, weighted_loss: 0.6776, label: 1, bag_size: 7090\n",
      "batch 419, loss: 0.9661, instance_loss: 1.5256, weighted_loss: 1.1340, label: 0, bag_size: 4597\n",
      "batch 439, loss: 0.6773, instance_loss: 0.9412, weighted_loss: 0.7565, label: 0, bag_size: 15193\n",
      "batch 459, loss: 0.2987, instance_loss: 0.8821, weighted_loss: 0.4737, label: 1, bag_size: 4436\n",
      "batch 479, loss: 0.5402, instance_loss: 0.9165, weighted_loss: 0.6531, label: 1, bag_size: 4510\n",
      "batch 499, loss: 0.4711, instance_loss: 1.4035, weighted_loss: 0.7508, label: 0, bag_size: 3277\n",
      "batch 519, loss: 0.3855, instance_loss: 0.7912, weighted_loss: 0.5072, label: 1, bag_size: 5651\n",
      "batch 539, loss: 0.6939, instance_loss: 1.0720, weighted_loss: 0.8073, label: 0, bag_size: 3869\n",
      "batch 559, loss: 0.9852, instance_loss: 1.7704, weighted_loss: 1.2208, label: 1, bag_size: 13348\n",
      "batch 579, loss: 0.5644, instance_loss: 1.0530, weighted_loss: 0.7110, label: 0, bag_size: 3521\n",
      "batch 599, loss: 0.3510, instance_loss: 0.7982, weighted_loss: 0.4851, label: 0, bag_size: 13846\n",
      "batch 619, loss: 0.3392, instance_loss: 0.7735, weighted_loss: 0.4695, label: 0, bag_size: 2432\n",
      "batch 639, loss: 0.8890, instance_loss: 0.7261, weighted_loss: 0.8401, label: 0, bag_size: 1433\n",
      "batch 659, loss: 0.8245, instance_loss: 1.1924, weighted_loss: 0.9349, label: 1, bag_size: 7717\n",
      "batch 679, loss: 2.7030, instance_loss: 2.3514, weighted_loss: 2.5975, label: 0, bag_size: 5100\n",
      "batch 699, loss: 0.7174, instance_loss: 1.0340, weighted_loss: 0.8124, label: 1, bag_size: 5677\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9917200854700855: correct 11139/11232\n",
      "class 1 clustering acc 0.014957264957264958: correct 84/5616\n",
      "Epoch: 6, train_loss: 0.7599, train_clustering_loss:  0.9914, train_error: 0.4915\n",
      "class 0: acc 0.5212464589235127, correct 184/353\n",
      "class 1: acc 0.49570200573065903, correct 173/349\n",
      "\n",
      "Val Set, val_loss: 0.6736, val_error: 0.3933, auc: 0.5153\n",
      "class 0 clustering acc 1.0: correct 1424/1424\n",
      "class 1 clustering acc 0.0: correct 0/712\n",
      "class 0: acc 1.0, correct 54/54\n",
      "class 1: acc 0.0, correct 0/35\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2911, instance_loss: 0.8833, weighted_loss: 0.4688, label: 1, bag_size: 4476\n",
      "batch 39, loss: 0.8213, instance_loss: 0.7419, weighted_loss: 0.7975, label: 1, bag_size: 6912\n",
      "batch 59, loss: 0.9522, instance_loss: 1.2130, weighted_loss: 1.0305, label: 0, bag_size: 4979\n",
      "batch 79, loss: 0.9803, instance_loss: 1.1407, weighted_loss: 1.0284, label: 0, bag_size: 3149\n",
      "batch 99, loss: 1.3752, instance_loss: 1.6304, weighted_loss: 1.4518, label: 1, bag_size: 6752\n",
      "batch 119, loss: 0.5948, instance_loss: 1.1019, weighted_loss: 0.7469, label: 0, bag_size: 5982\n",
      "batch 139, loss: 0.3381, instance_loss: 0.7158, weighted_loss: 0.4514, label: 1, bag_size: 3126\n",
      "batch 159, loss: 0.4738, instance_loss: 0.7559, weighted_loss: 0.5584, label: 0, bag_size: 22594\n",
      "batch 179, loss: 0.3851, instance_loss: 0.8910, weighted_loss: 0.5368, label: 0, bag_size: 5080\n",
      "batch 199, loss: 0.9233, instance_loss: 1.6631, weighted_loss: 1.1452, label: 1, bag_size: 1825\n",
      "batch 219, loss: 0.3641, instance_loss: 1.0250, weighted_loss: 0.5623, label: 1, bag_size: 6878\n",
      "batch 239, loss: 1.0238, instance_loss: 1.2485, weighted_loss: 1.0912, label: 0, bag_size: 2945\n",
      "batch 259, loss: 0.6374, instance_loss: 0.9420, weighted_loss: 0.7288, label: 0, bag_size: 4320\n",
      "batch 279, loss: 1.0010, instance_loss: 0.8112, weighted_loss: 0.9441, label: 0, bag_size: 2244\n",
      "batch 299, loss: 0.3245, instance_loss: 1.2559, weighted_loss: 0.6039, label: 0, bag_size: 6908\n",
      "batch 319, loss: 1.1395, instance_loss: 1.2779, weighted_loss: 1.1811, label: 1, bag_size: 13685\n",
      "batch 339, loss: 0.6423, instance_loss: 0.9802, weighted_loss: 0.7436, label: 1, bag_size: 2081\n",
      "batch 359, loss: 1.0238, instance_loss: 0.9735, weighted_loss: 1.0087, label: 0, bag_size: 2244\n",
      "batch 379, loss: 0.5614, instance_loss: 1.2183, weighted_loss: 0.7584, label: 0, bag_size: 3240\n",
      "batch 399, loss: 1.6292, instance_loss: 1.1041, weighted_loss: 1.4717, label: 1, bag_size: 4387\n",
      "batch 419, loss: 1.0623, instance_loss: 0.9479, weighted_loss: 1.0280, label: 0, bag_size: 4979\n",
      "batch 439, loss: 0.9376, instance_loss: 0.9823, weighted_loss: 0.9510, label: 1, bag_size: 5738\n",
      "batch 459, loss: 0.7902, instance_loss: 0.9939, weighted_loss: 0.8513, label: 1, bag_size: 3420\n",
      "batch 479, loss: 0.3799, instance_loss: 1.0401, weighted_loss: 0.5780, label: 1, bag_size: 2877\n",
      "batch 499, loss: 0.6325, instance_loss: 0.7493, weighted_loss: 0.6675, label: 1, bag_size: 3081\n",
      "batch 519, loss: 1.3294, instance_loss: 1.2320, weighted_loss: 1.3002, label: 1, bag_size: 3962\n",
      "batch 539, loss: 0.3736, instance_loss: 0.8078, weighted_loss: 0.5039, label: 1, bag_size: 6410\n",
      "batch 559, loss: 0.4603, instance_loss: 0.8199, weighted_loss: 0.5682, label: 1, bag_size: 3962\n",
      "batch 579, loss: 1.1052, instance_loss: 0.7062, weighted_loss: 0.9855, label: 0, bag_size: 3277\n",
      "batch 599, loss: 0.3532, instance_loss: 1.0870, weighted_loss: 0.5734, label: 0, bag_size: 3484\n",
      "batch 619, loss: 0.3163, instance_loss: 1.0107, weighted_loss: 0.5246, label: 1, bag_size: 5100\n",
      "batch 639, loss: 1.2483, instance_loss: 0.7084, weighted_loss: 1.0864, label: 1, bag_size: 1178\n",
      "batch 659, loss: 0.9825, instance_loss: 0.9925, weighted_loss: 0.9855, label: 0, bag_size: 2022\n",
      "batch 679, loss: 1.1941, instance_loss: 1.1350, weighted_loss: 1.1764, label: 0, bag_size: 2963\n",
      "batch 699, loss: 0.7197, instance_loss: 0.9701, weighted_loss: 0.7948, label: 0, bag_size: 2069\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9916310541310541: correct 11138/11232\n",
      "class 1 clustering acc 0.02047720797720798: correct 115/5616\n",
      "Epoch: 7, train_loss: 0.7482, train_clustering_loss:  0.9869, train_error: 0.4729\n",
      "class 0: acc 0.5071633237822349, correct 177/349\n",
      "class 1: acc 0.546742209631728, correct 193/353\n",
      "\n",
      "Val Set, val_loss: 0.7453, val_error: 0.5955, auc: 0.5365\n",
      "class 0 clustering acc 1.0: correct 1424/1424\n",
      "class 1 clustering acc 0.0: correct 0/712\n",
      "class 0: acc 0.018518518518518517, correct 1/54\n",
      "class 1: acc 1.0, correct 35/35\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4882, instance_loss: 1.1963, weighted_loss: 0.7006, label: 0, bag_size: 5697\n",
      "batch 39, loss: 1.1872, instance_loss: 0.9829, weighted_loss: 1.1259, label: 0, bag_size: 2482\n",
      "batch 59, loss: 0.4077, instance_loss: 0.9797, weighted_loss: 0.5793, label: 1, bag_size: 6138\n",
      "batch 79, loss: 1.4515, instance_loss: 1.9306, weighted_loss: 1.5952, label: 0, bag_size: 1826\n",
      "batch 99, loss: 0.9719, instance_loss: 1.6122, weighted_loss: 1.1640, label: 1, bag_size: 5458\n",
      "batch 119, loss: 0.5802, instance_loss: 0.8849, weighted_loss: 0.6716, label: 0, bag_size: 2238\n",
      "batch 139, loss: 0.8462, instance_loss: 0.7784, weighted_loss: 0.8258, label: 0, bag_size: 5120\n",
      "batch 159, loss: 0.8637, instance_loss: 0.9722, weighted_loss: 0.8962, label: 1, bag_size: 4737\n",
      "batch 179, loss: 0.5501, instance_loss: 1.0129, weighted_loss: 0.6890, label: 0, bag_size: 2420\n",
      "batch 199, loss: 0.4761, instance_loss: 1.0577, weighted_loss: 0.6506, label: 0, bag_size: 972\n",
      "batch 219, loss: 0.8386, instance_loss: 0.9961, weighted_loss: 0.8858, label: 0, bag_size: 7290\n",
      "batch 239, loss: 0.8800, instance_loss: 1.0330, weighted_loss: 0.9259, label: 1, bag_size: 13685\n",
      "batch 259, loss: 0.5808, instance_loss: 1.2660, weighted_loss: 0.7864, label: 1, bag_size: 2626\n",
      "batch 279, loss: 0.7199, instance_loss: 1.1103, weighted_loss: 0.8370, label: 0, bag_size: 4012\n",
      "batch 299, loss: 0.4218, instance_loss: 1.2042, weighted_loss: 0.6565, label: 1, bag_size: 5072\n",
      "batch 319, loss: 0.5392, instance_loss: 0.6925, weighted_loss: 0.5852, label: 1, bag_size: 10736\n",
      "batch 339, loss: 0.3108, instance_loss: 1.0497, weighted_loss: 0.5325, label: 0, bag_size: 3022\n",
      "batch 359, loss: 0.8435, instance_loss: 0.7313, weighted_loss: 0.8098, label: 0, bag_size: 3317\n",
      "batch 379, loss: 1.3452, instance_loss: 0.8542, weighted_loss: 1.1979, label: 1, bag_size: 5539\n",
      "batch 399, loss: 0.9919, instance_loss: 0.8279, weighted_loss: 0.9427, label: 0, bag_size: 26374\n",
      "batch 419, loss: 1.6293, instance_loss: 1.3627, weighted_loss: 1.5493, label: 0, bag_size: 4390\n",
      "batch 439, loss: 0.6298, instance_loss: 1.0800, weighted_loss: 0.7649, label: 0, bag_size: 5382\n",
      "batch 459, loss: 0.4959, instance_loss: 0.7156, weighted_loss: 0.5618, label: 0, bag_size: 2482\n",
      "batch 479, loss: 0.6790, instance_loss: 0.8037, weighted_loss: 0.7164, label: 0, bag_size: 4808\n",
      "batch 499, loss: 1.0968, instance_loss: 1.0274, weighted_loss: 1.0760, label: 1, bag_size: 11773\n",
      "batch 519, loss: 0.5588, instance_loss: 0.7286, weighted_loss: 0.6098, label: 1, bag_size: 3405\n",
      "batch 539, loss: 1.0491, instance_loss: 1.2636, weighted_loss: 1.1134, label: 0, bag_size: 3312\n",
      "batch 559, loss: 0.6241, instance_loss: 0.9850, weighted_loss: 0.7324, label: 1, bag_size: 11773\n",
      "batch 579, loss: 0.2512, instance_loss: 0.9239, weighted_loss: 0.4530, label: 0, bag_size: 6209\n",
      "batch 599, loss: 0.9688, instance_loss: 1.6475, weighted_loss: 1.1724, label: 1, bag_size: 5935\n",
      "batch 619, loss: 0.0791, instance_loss: 1.7649, weighted_loss: 0.5849, label: 0, bag_size: 2482\n",
      "batch 639, loss: 2.7242, instance_loss: 1.7761, weighted_loss: 2.4397, label: 1, bag_size: 3557\n",
      "batch 659, loss: 0.5281, instance_loss: 0.9471, weighted_loss: 0.6538, label: 0, bag_size: 14662\n",
      "batch 679, loss: 0.9991, instance_loss: 1.3852, weighted_loss: 1.1149, label: 1, bag_size: 3159\n",
      "batch 699, loss: 0.4977, instance_loss: 1.1316, weighted_loss: 0.6879, label: 1, bag_size: 5592\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9842414529914529: correct 11055/11232\n",
      "class 1 clustering acc 0.03561253561253561: correct 200/5616\n",
      "Epoch: 8, train_loss: 0.7221, train_clustering_loss:  0.9930, train_error: 0.4630\n",
      "class 0: acc 0.554016620498615, correct 200/361\n",
      "class 1: acc 0.5190615835777126, correct 177/341\n",
      "\n",
      "Val Set, val_loss: 0.7109, val_error: 0.5393, auc: 0.5571\n",
      "class 0 clustering acc 1.0: correct 1424/1424\n",
      "class 1 clustering acc 0.0: correct 0/712\n",
      "class 0: acc 0.3148148148148148, correct 17/54\n",
      "class 1: acc 0.6857142857142857, correct 24/35\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4669, instance_loss: 0.7339, weighted_loss: 0.5470, label: 1, bag_size: 1525\n",
      "batch 39, loss: 1.1449, instance_loss: 0.9114, weighted_loss: 1.0748, label: 1, bag_size: 6477\n",
      "batch 59, loss: 0.5418, instance_loss: 1.3988, weighted_loss: 0.7989, label: 1, bag_size: 2656\n",
      "batch 79, loss: 0.6724, instance_loss: 0.8765, weighted_loss: 0.7336, label: 0, bag_size: 6524\n",
      "batch 99, loss: 0.3624, instance_loss: 0.8955, weighted_loss: 0.5223, label: 0, bag_size: 2094\n",
      "batch 119, loss: 0.6858, instance_loss: 1.2427, weighted_loss: 0.8529, label: 1, bag_size: 3143\n",
      "batch 139, loss: 0.2676, instance_loss: 0.9687, weighted_loss: 0.4779, label: 0, bag_size: 5080\n",
      "batch 159, loss: 0.6898, instance_loss: 0.9042, weighted_loss: 0.7541, label: 0, bag_size: 5121\n",
      "batch 179, loss: 0.9328, instance_loss: 0.9439, weighted_loss: 0.9362, label: 0, bag_size: 5982\n",
      "batch 199, loss: 0.1574, instance_loss: 0.9364, weighted_loss: 0.3911, label: 0, bag_size: 5369\n",
      "batch 219, loss: 0.4430, instance_loss: 0.6430, weighted_loss: 0.5030, label: 0, bag_size: 18810\n",
      "batch 239, loss: 0.9577, instance_loss: 0.8153, weighted_loss: 0.9150, label: 0, bag_size: 2432\n",
      "batch 259, loss: 1.4176, instance_loss: 1.2188, weighted_loss: 1.3580, label: 0, bag_size: 19535\n",
      "batch 279, loss: 0.4134, instance_loss: 0.9538, weighted_loss: 0.5755, label: 1, bag_size: 2493\n",
      "batch 299, loss: 0.3769, instance_loss: 1.0006, weighted_loss: 0.5640, label: 0, bag_size: 1619\n",
      "batch 319, loss: 1.0956, instance_loss: 0.9738, weighted_loss: 1.0591, label: 0, bag_size: 2851\n",
      "batch 339, loss: 0.5671, instance_loss: 0.8771, weighted_loss: 0.6601, label: 1, bag_size: 3571\n",
      "batch 359, loss: 0.6941, instance_loss: 0.7398, weighted_loss: 0.7078, label: 1, bag_size: 4510\n",
      "batch 379, loss: 0.3978, instance_loss: 0.8845, weighted_loss: 0.5438, label: 0, bag_size: 12784\n",
      "batch 399, loss: 0.3255, instance_loss: 0.9201, weighted_loss: 0.5039, label: 1, bag_size: 3117\n",
      "batch 419, loss: 1.2611, instance_loss: 0.7860, weighted_loss: 1.1186, label: 1, bag_size: 4458\n",
      "batch 439, loss: 0.4064, instance_loss: 0.9607, weighted_loss: 0.5727, label: 1, bag_size: 5108\n",
      "batch 459, loss: 0.6634, instance_loss: 0.5292, weighted_loss: 0.6231, label: 1, bag_size: 2176\n",
      "batch 479, loss: 0.3038, instance_loss: 0.7507, weighted_loss: 0.4379, label: 1, bag_size: 6500\n",
      "batch 499, loss: 0.4695, instance_loss: 0.9798, weighted_loss: 0.6226, label: 0, bag_size: 4500\n",
      "batch 519, loss: 0.3302, instance_loss: 0.9127, weighted_loss: 0.5049, label: 1, bag_size: 6760\n",
      "batch 539, loss: 0.3353, instance_loss: 0.7024, weighted_loss: 0.4455, label: 1, bag_size: 3020\n",
      "batch 559, loss: 1.7000, instance_loss: 1.6439, weighted_loss: 1.6832, label: 0, bag_size: 1943\n",
      "batch 579, loss: 0.5232, instance_loss: 0.7760, weighted_loss: 0.5990, label: 1, bag_size: 4815\n",
      "batch 599, loss: 0.8205, instance_loss: 0.7701, weighted_loss: 0.8054, label: 1, bag_size: 3374\n",
      "batch 619, loss: 0.3595, instance_loss: 1.0695, weighted_loss: 0.5725, label: 1, bag_size: 4722\n",
      "batch 639, loss: 1.7452, instance_loss: 0.9938, weighted_loss: 1.5198, label: 1, bag_size: 3893\n",
      "batch 659, loss: 0.8464, instance_loss: 1.2337, weighted_loss: 0.9626, label: 1, bag_size: 21711\n",
      "batch 679, loss: 1.1830, instance_loss: 1.5784, weighted_loss: 1.3016, label: 1, bag_size: 5298\n",
      "batch 699, loss: 1.3996, instance_loss: 1.0994, weighted_loss: 1.3095, label: 0, bag_size: 4808\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9894052706552706: correct 11113/11232\n",
      "class 1 clustering acc 0.028133903133903133: correct 158/5616\n",
      "Epoch: 9, train_loss: 0.7334, train_clustering_loss:  0.9653, train_error: 0.4829\n",
      "class 0: acc 0.4752186588921283, correct 163/343\n",
      "class 1: acc 0.5571030640668524, correct 200/359\n",
      "\n",
      "Val Set, val_loss: 0.8651, val_error: 0.6067, auc: 0.5550\n",
      "class 0 clustering acc 0.9634831460674157: correct 1372/1424\n",
      "class 1 clustering acc 0.03651685393258427: correct 26/712\n",
      "class 0: acc 0.0, correct 0/54\n",
      "class 1: acc 1.0, correct 35/35\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.3584, instance_loss: 1.3614, weighted_loss: 1.3593, label: 0, bag_size: 4687\n",
      "batch 39, loss: 0.5962, instance_loss: 0.6978, weighted_loss: 0.6266, label: 1, bag_size: 3824\n",
      "batch 59, loss: 0.4693, instance_loss: 0.9029, weighted_loss: 0.5994, label: 0, bag_size: 2152\n",
      "batch 79, loss: 0.8235, instance_loss: 1.2296, weighted_loss: 0.9453, label: 1, bag_size: 3571\n",
      "batch 99, loss: 0.9970, instance_loss: 1.4706, weighted_loss: 1.1391, label: 0, bag_size: 30392\n",
      "batch 119, loss: 0.3896, instance_loss: 0.5836, weighted_loss: 0.4478, label: 0, bag_size: 7290\n",
      "batch 139, loss: 0.9923, instance_loss: 1.1732, weighted_loss: 1.0465, label: 1, bag_size: 8007\n",
      "batch 159, loss: 0.3733, instance_loss: 0.7721, weighted_loss: 0.4929, label: 1, bag_size: 6752\n",
      "batch 179, loss: 1.3345, instance_loss: 0.9979, weighted_loss: 1.2336, label: 1, bag_size: 3962\n",
      "batch 199, loss: 1.5054, instance_loss: 0.8387, weighted_loss: 1.3054, label: 0, bag_size: 5724\n",
      "batch 219, loss: 0.2940, instance_loss: 1.0088, weighted_loss: 0.5084, label: 0, bag_size: 19055\n",
      "batch 239, loss: 1.7039, instance_loss: 1.3987, weighted_loss: 1.6123, label: 0, bag_size: 1532\n",
      "batch 259, loss: 0.8861, instance_loss: 1.3051, weighted_loss: 1.0118, label: 0, bag_size: 5108\n",
      "batch 279, loss: 0.5825, instance_loss: 1.1519, weighted_loss: 0.7533, label: 1, bag_size: 4268\n",
      "batch 299, loss: 1.0331, instance_loss: 0.9655, weighted_loss: 1.0128, label: 1, bag_size: 2985\n",
      "batch 319, loss: 0.4102, instance_loss: 0.7164, weighted_loss: 0.5021, label: 1, bag_size: 3391\n",
      "batch 339, loss: 0.7926, instance_loss: 0.8951, weighted_loss: 0.8233, label: 1, bag_size: 4510\n",
      "batch 359, loss: 0.8472, instance_loss: 0.9095, weighted_loss: 0.8659, label: 0, bag_size: 3843\n",
      "batch 379, loss: 0.5613, instance_loss: 0.8421, weighted_loss: 0.6456, label: 1, bag_size: 3358\n",
      "batch 399, loss: 1.1695, instance_loss: 1.3652, weighted_loss: 1.2282, label: 0, bag_size: 3615\n",
      "batch 419, loss: 0.4001, instance_loss: 0.9639, weighted_loss: 0.5692, label: 0, bag_size: 15139\n",
      "batch 439, loss: 0.9266, instance_loss: 1.0282, weighted_loss: 0.9571, label: 1, bag_size: 3126\n",
      "batch 459, loss: 0.4568, instance_loss: 0.6700, weighted_loss: 0.5208, label: 0, bag_size: 1379\n",
      "batch 479, loss: 0.4379, instance_loss: 0.8020, weighted_loss: 0.5471, label: 1, bag_size: 5218\n",
      "batch 499, loss: 1.1725, instance_loss: 1.2210, weighted_loss: 1.1871, label: 0, bag_size: 4500\n",
      "batch 519, loss: 0.7204, instance_loss: 1.1052, weighted_loss: 0.8358, label: 0, bag_size: 18376\n",
      "batch 539, loss: 0.8503, instance_loss: 0.9986, weighted_loss: 0.8948, label: 0, bag_size: 5257\n",
      "batch 559, loss: 0.7655, instance_loss: 1.0826, weighted_loss: 0.8606, label: 0, bag_size: 19612\n",
      "batch 579, loss: 1.1464, instance_loss: 0.9827, weighted_loss: 1.0973, label: 0, bag_size: 4468\n",
      "batch 599, loss: 0.7654, instance_loss: 0.9647, weighted_loss: 0.8252, label: 0, bag_size: 2089\n",
      "batch 619, loss: 0.5032, instance_loss: 0.6594, weighted_loss: 0.5501, label: 1, bag_size: 6295\n",
      "batch 639, loss: 0.7310, instance_loss: 1.0063, weighted_loss: 0.8136, label: 1, bag_size: 5689\n",
      "batch 659, loss: 0.7968, instance_loss: 1.0461, weighted_loss: 0.8716, label: 0, bag_size: 4737\n",
      "batch 679, loss: 0.6702, instance_loss: 1.0575, weighted_loss: 0.7864, label: 0, bag_size: 2533\n",
      "batch 699, loss: 0.9843, instance_loss: 1.0296, weighted_loss: 0.9979, label: 1, bag_size: 4094\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9891381766381766: correct 11110/11232\n",
      "class 1 clustering acc 0.03311965811965812: correct 186/5616\n",
      "Epoch: 10, train_loss: 0.7146, train_clustering_loss:  0.9582, train_error: 0.4601\n",
      "class 0: acc 0.46706586826347307, correct 156/334\n",
      "class 1: acc 0.6059782608695652, correct 223/368\n",
      "\n",
      "Val Set, val_loss: 0.6866, val_error: 0.4719, auc: 0.5767\n",
      "class 0 clustering acc 1.0: correct 1424/1424\n",
      "class 1 clustering acc 0.011235955056179775: correct 8/712\n",
      "class 0: acc 0.48148148148148145, correct 26/54\n",
      "class 1: acc 0.6, correct 21/35\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6263, instance_loss: 1.1515, weighted_loss: 0.7838, label: 0, bag_size: 6118\n",
      "batch 39, loss: 0.7341, instance_loss: 1.1351, weighted_loss: 0.8544, label: 1, bag_size: 6841\n",
      "batch 59, loss: 0.6427, instance_loss: 1.0222, weighted_loss: 0.7565, label: 0, bag_size: 16859\n",
      "batch 79, loss: 0.6383, instance_loss: 0.9844, weighted_loss: 0.7421, label: 1, bag_size: 3020\n",
      "batch 99, loss: 0.8374, instance_loss: 0.9928, weighted_loss: 0.8840, label: 1, bag_size: 1525\n",
      "batch 119, loss: 0.8642, instance_loss: 1.0587, weighted_loss: 0.9225, label: 0, bag_size: 14098\n",
      "batch 139, loss: 0.8715, instance_loss: 1.1563, weighted_loss: 0.9569, label: 1, bag_size: 3134\n",
      "batch 159, loss: 0.7749, instance_loss: 0.8336, weighted_loss: 0.7925, label: 1, bag_size: 21473\n",
      "batch 179, loss: 0.4885, instance_loss: 0.7482, weighted_loss: 0.5664, label: 1, bag_size: 6235\n",
      "batch 199, loss: 0.5320, instance_loss: 0.6699, weighted_loss: 0.5734, label: 1, bag_size: 19581\n",
      "batch 219, loss: 0.6859, instance_loss: 0.9090, weighted_loss: 0.7529, label: 0, bag_size: 4086\n",
      "batch 239, loss: 0.8652, instance_loss: 0.7569, weighted_loss: 0.8327, label: 0, bag_size: 4573\n",
      "batch 259, loss: 0.5362, instance_loss: 0.6066, weighted_loss: 0.5573, label: 1, bag_size: 3391\n",
      "batch 279, loss: 1.1415, instance_loss: 1.4183, weighted_loss: 1.2245, label: 1, bag_size: 10736\n",
      "batch 299, loss: 0.6927, instance_loss: 0.5641, weighted_loss: 0.6541, label: 1, bag_size: 2407\n",
      "batch 319, loss: 0.4747, instance_loss: 0.8449, weighted_loss: 0.5857, label: 1, bag_size: 5651\n",
      "batch 339, loss: 0.7107, instance_loss: 0.6754, weighted_loss: 0.7001, label: 1, bag_size: 2701\n",
      "batch 359, loss: 1.2838, instance_loss: 0.8930, weighted_loss: 1.1665, label: 1, bag_size: 7717\n",
      "batch 379, loss: 0.6006, instance_loss: 1.1960, weighted_loss: 0.7792, label: 1, bag_size: 2301\n",
      "batch 399, loss: 0.7770, instance_loss: 0.6654, weighted_loss: 0.7435, label: 1, bag_size: 6500\n",
      "batch 419, loss: 0.7311, instance_loss: 0.7875, weighted_loss: 0.7480, label: 0, bag_size: 14291\n",
      "batch 439, loss: 0.4952, instance_loss: 0.7468, weighted_loss: 0.5707, label: 1, bag_size: 5428\n",
      "batch 459, loss: 0.5541, instance_loss: 0.8321, weighted_loss: 0.6375, label: 0, bag_size: 19396\n",
      "batch 479, loss: 0.5037, instance_loss: 0.9178, weighted_loss: 0.6279, label: 1, bag_size: 5638\n",
      "batch 499, loss: 0.7696, instance_loss: 1.1439, weighted_loss: 0.8819, label: 0, bag_size: 3521\n",
      "batch 519, loss: 0.8206, instance_loss: 0.8945, weighted_loss: 0.8428, label: 1, bag_size: 7717\n",
      "batch 539, loss: 0.6761, instance_loss: 0.8818, weighted_loss: 0.7378, label: 1, bag_size: 4647\n",
      "batch 559, loss: 1.0465, instance_loss: 1.3647, weighted_loss: 1.1420, label: 1, bag_size: 6912\n",
      "batch 579, loss: 0.5459, instance_loss: 0.6297, weighted_loss: 0.5710, label: 1, bag_size: 1587\n",
      "batch 599, loss: 0.9124, instance_loss: 0.8984, weighted_loss: 0.9082, label: 1, bag_size: 1746\n",
      "batch 619, loss: 1.1834, instance_loss: 0.9603, weighted_loss: 1.1165, label: 0, bag_size: 3600\n",
      "batch 639, loss: 0.4556, instance_loss: 0.4875, weighted_loss: 0.4651, label: 1, bag_size: 5819\n",
      "batch 659, loss: 0.7062, instance_loss: 0.6850, weighted_loss: 0.6999, label: 0, bag_size: 3574\n",
      "batch 679, loss: 0.6785, instance_loss: 1.1256, weighted_loss: 0.8126, label: 0, bag_size: 4180\n",
      "batch 699, loss: 0.4377, instance_loss: 0.5433, weighted_loss: 0.4694, label: 1, bag_size: 4795\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9847756410256411: correct 11061/11232\n",
      "class 1 clustering acc 0.04415954415954416: correct 248/5616\n",
      "Epoch: 11, train_loss: 0.7175, train_clustering_loss:  0.9322, train_error: 0.4644\n",
      "class 0: acc 0.48502994011976047, correct 162/334\n",
      "class 1: acc 0.5815217391304348, correct 214/368\n",
      "\n",
      "Val Set, val_loss: 0.7516, val_error: 0.5955, auc: 0.5794\n",
      "class 0 clustering acc 0.9992977528089888: correct 1423/1424\n",
      "class 1 clustering acc 0.03089887640449438: correct 22/712\n",
      "class 0: acc 0.018518518518518517, correct 1/54\n",
      "class 1: acc 1.0, correct 35/35\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.1887, instance_loss: 1.0837, weighted_loss: 1.1572, label: 0, bag_size: 9973\n",
      "batch 39, loss: 0.6621, instance_loss: 0.7627, weighted_loss: 0.6923, label: 0, bag_size: 3290\n",
      "batch 59, loss: 1.5387, instance_loss: 1.3702, weighted_loss: 1.4882, label: 1, bag_size: 4243\n",
      "batch 79, loss: 0.2939, instance_loss: 0.4140, weighted_loss: 0.3299, label: 1, bag_size: 4233\n",
      "batch 99, loss: 0.1790, instance_loss: 0.2644, weighted_loss: 0.2046, label: 1, bag_size: 3824\n",
      "batch 119, loss: 0.5307, instance_loss: 0.5417, weighted_loss: 0.5340, label: 1, bag_size: 1937\n",
      "batch 139, loss: 0.6364, instance_loss: 0.9002, weighted_loss: 0.7156, label: 1, bag_size: 13217\n",
      "batch 159, loss: 0.5602, instance_loss: 0.7689, weighted_loss: 0.6228, label: 1, bag_size: 3949\n",
      "batch 179, loss: 0.4958, instance_loss: 0.6266, weighted_loss: 0.5350, label: 0, bag_size: 3764\n",
      "batch 199, loss: 0.6332, instance_loss: 0.8611, weighted_loss: 0.7016, label: 1, bag_size: 2081\n",
      "batch 219, loss: 0.2512, instance_loss: 0.6639, weighted_loss: 0.3750, label: 1, bag_size: 4853\n",
      "batch 239, loss: 0.9295, instance_loss: 1.1171, weighted_loss: 0.9858, label: 1, bag_size: 13348\n",
      "batch 259, loss: 0.8175, instance_loss: 1.1383, weighted_loss: 0.9137, label: 1, bag_size: 15483\n",
      "batch 279, loss: 0.5587, instance_loss: 0.6145, weighted_loss: 0.5754, label: 1, bag_size: 2938\n",
      "batch 299, loss: 0.6145, instance_loss: 1.0154, weighted_loss: 0.7348, label: 1, bag_size: 19173\n",
      "batch 319, loss: 1.0620, instance_loss: 1.3883, weighted_loss: 1.1599, label: 0, bag_size: 6605\n",
      "batch 339, loss: 0.5382, instance_loss: 0.5871, weighted_loss: 0.5529, label: 0, bag_size: 3843\n",
      "batch 359, loss: 0.7604, instance_loss: 0.9338, weighted_loss: 0.8124, label: 0, bag_size: 3737\n",
      "batch 379, loss: 0.5528, instance_loss: 0.9020, weighted_loss: 0.6576, label: 0, bag_size: 3917\n",
      "batch 399, loss: 0.5659, instance_loss: 0.7087, weighted_loss: 0.6087, label: 0, bag_size: 4168\n",
      "batch 419, loss: 0.6530, instance_loss: 0.7776, weighted_loss: 0.6904, label: 0, bag_size: 14142\n",
      "batch 439, loss: 1.0412, instance_loss: 1.3034, weighted_loss: 1.1198, label: 0, bag_size: 10096\n",
      "batch 459, loss: 0.3176, instance_loss: 0.5584, weighted_loss: 0.3898, label: 0, bag_size: 6076\n",
      "batch 479, loss: 0.9534, instance_loss: 1.1290, weighted_loss: 1.0061, label: 0, bag_size: 5455\n",
      "batch 499, loss: 1.2073, instance_loss: 1.3792, weighted_loss: 1.2589, label: 1, bag_size: 4950\n",
      "batch 519, loss: 0.9492, instance_loss: 0.8557, weighted_loss: 0.9211, label: 0, bag_size: 1638\n",
      "batch 539, loss: 0.2346, instance_loss: 0.4753, weighted_loss: 0.3068, label: 1, bag_size: 6319\n",
      "batch 559, loss: 1.1862, instance_loss: 1.2472, weighted_loss: 1.2045, label: 1, bag_size: 4046\n",
      "batch 579, loss: 1.4026, instance_loss: 1.6371, weighted_loss: 1.4730, label: 0, bag_size: 19535\n",
      "batch 599, loss: 0.7141, instance_loss: 0.8027, weighted_loss: 0.7407, label: 1, bag_size: 4387\n",
      "batch 619, loss: 0.3888, instance_loss: 0.5396, weighted_loss: 0.4341, label: 0, bag_size: 19223\n",
      "batch 639, loss: 0.3267, instance_loss: 0.4177, weighted_loss: 0.3540, label: 0, bag_size: 9742\n",
      "batch 659, loss: 0.5710, instance_loss: 0.8112, weighted_loss: 0.6431, label: 0, bag_size: 3264\n",
      "batch 679, loss: 0.6374, instance_loss: 0.6805, weighted_loss: 0.6503, label: 0, bag_size: 586\n",
      "batch 699, loss: 0.8581, instance_loss: 0.9583, weighted_loss: 0.8882, label: 1, bag_size: 5810\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9724893162393162: correct 10923/11232\n",
      "class 1 clustering acc 0.15865384615384615: correct 891/5616\n",
      "Epoch: 12, train_loss: 0.6973, train_clustering_loss:  0.8622, train_error: 0.4473\n",
      "class 0: acc 0.48484848484848486, correct 160/330\n",
      "class 1: acc 0.6129032258064516, correct 228/372\n",
      "\n",
      "Val Set, val_loss: 0.6892, val_error: 0.4831, auc: 0.5905\n",
      "class 0 clustering acc 0.9775280898876404: correct 1392/1424\n",
      "class 1 clustering acc 0.11235955056179775: correct 80/712\n",
      "class 0: acc 0.48148148148148145, correct 26/54\n",
      "class 1: acc 0.5714285714285714, correct 20/35\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.5949, instance_loss: 2.1178, weighted_loss: 1.7518, label: 1, bag_size: 3134\n",
      "batch 39, loss: 0.7738, instance_loss: 0.8844, weighted_loss: 0.8070, label: 1, bag_size: 15118\n",
      "batch 59, loss: 0.4157, instance_loss: 0.7099, weighted_loss: 0.5040, label: 0, bag_size: 15747\n",
      "batch 79, loss: 1.0369, instance_loss: 1.2545, weighted_loss: 1.1022, label: 1, bag_size: 3533\n",
      "batch 99, loss: 1.1479, instance_loss: 1.4002, weighted_loss: 1.2236, label: 0, bag_size: 1652\n",
      "batch 119, loss: 0.7270, instance_loss: 1.0920, weighted_loss: 0.8365, label: 0, bag_size: 3778\n",
      "batch 139, loss: 0.4323, instance_loss: 0.4121, weighted_loss: 0.4262, label: 1, bag_size: 5672\n",
      "batch 159, loss: 0.6349, instance_loss: 0.6530, weighted_loss: 0.6403, label: 1, bag_size: 21399\n",
      "batch 179, loss: 0.6409, instance_loss: 0.7403, weighted_loss: 0.6708, label: 1, bag_size: 2189\n",
      "batch 199, loss: 0.4924, instance_loss: 0.5013, weighted_loss: 0.4951, label: 1, bag_size: 1374\n",
      "batch 219, loss: 0.9879, instance_loss: 1.2513, weighted_loss: 1.0669, label: 0, bag_size: 4692\n",
      "batch 239, loss: 0.5050, instance_loss: 0.4635, weighted_loss: 0.4926, label: 1, bag_size: 4731\n",
      "batch 259, loss: 0.5823, instance_loss: 0.5224, weighted_loss: 0.5643, label: 0, bag_size: 5120\n",
      "batch 279, loss: 0.6025, instance_loss: 0.6261, weighted_loss: 0.6096, label: 0, bag_size: 3295\n",
      "batch 299, loss: 0.7643, instance_loss: 0.8375, weighted_loss: 0.7863, label: 0, bag_size: 15682\n",
      "batch 319, loss: 0.6531, instance_loss: 0.8592, weighted_loss: 0.7149, label: 1, bag_size: 2207\n",
      "batch 339, loss: 1.0435, instance_loss: 0.8620, weighted_loss: 0.9890, label: 0, bag_size: 15682\n",
      "batch 359, loss: 0.8561, instance_loss: 1.1138, weighted_loss: 0.9334, label: 1, bag_size: 2039\n",
      "batch 379, loss: 0.4324, instance_loss: 0.5024, weighted_loss: 0.4534, label: 1, bag_size: 6760\n",
      "batch 399, loss: 0.4167, instance_loss: 0.6310, weighted_loss: 0.4810, label: 0, bag_size: 23860\n",
      "batch 419, loss: 0.4886, instance_loss: 0.6576, weighted_loss: 0.5393, label: 1, bag_size: 2189\n",
      "batch 439, loss: 0.7486, instance_loss: 0.7312, weighted_loss: 0.7434, label: 1, bag_size: 1587\n",
      "batch 459, loss: 0.6015, instance_loss: 1.0816, weighted_loss: 0.7455, label: 1, bag_size: 4476\n",
      "batch 479, loss: 0.6102, instance_loss: 1.0738, weighted_loss: 0.7493, label: 1, bag_size: 3949\n",
      "batch 499, loss: 0.5331, instance_loss: 0.3859, weighted_loss: 0.4889, label: 1, bag_size: 6295\n",
      "batch 519, loss: 1.2830, instance_loss: 1.3405, weighted_loss: 1.3002, label: 1, bag_size: 4069\n",
      "batch 539, loss: 0.3694, instance_loss: 0.4865, weighted_loss: 0.4045, label: 1, bag_size: 3402\n",
      "batch 559, loss: 0.6361, instance_loss: 0.8276, weighted_loss: 0.6935, label: 0, bag_size: 5967\n",
      "batch 579, loss: 0.5074, instance_loss: 0.6755, weighted_loss: 0.5579, label: 0, bag_size: 4172\n",
      "batch 599, loss: 0.6940, instance_loss: 0.6910, weighted_loss: 0.6931, label: 0, bag_size: 3087\n",
      "batch 619, loss: 0.2754, instance_loss: 0.3491, weighted_loss: 0.2975, label: 1, bag_size: 4731\n",
      "batch 639, loss: 0.7887, instance_loss: 1.0559, weighted_loss: 0.8689, label: 0, bag_size: 4560\n",
      "batch 659, loss: 0.6837, instance_loss: 0.8211, weighted_loss: 0.7249, label: 1, bag_size: 20955\n",
      "batch 679, loss: 0.6298, instance_loss: 0.7864, weighted_loss: 0.6768, label: 1, bag_size: 3208\n",
      "batch 699, loss: 1.1096, instance_loss: 1.3235, weighted_loss: 1.1738, label: 1, bag_size: 3571\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9713319088319088: correct 10910/11232\n",
      "class 1 clustering acc 0.1071937321937322: correct 602/5616\n",
      "Epoch: 13, train_loss: 0.7030, train_clustering_loss:  0.8684, train_error: 0.4430\n",
      "class 0: acc 0.5103244837758112, correct 173/339\n",
      "class 1: acc 0.6005509641873278, correct 218/363\n",
      "\n",
      "Val Set, val_loss: 0.6314, val_error: 0.3034, auc: 0.6741\n",
      "class 0 clustering acc 0.9922752808988764: correct 1413/1424\n",
      "class 1 clustering acc 0.08146067415730338: correct 58/712\n",
      "class 0: acc 0.8703703703703703, correct 47/54\n",
      "class 1: acc 0.42857142857142855, correct 15/35\n",
      "Validation loss decreased (0.671394 --> 0.631397).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5528, instance_loss: 0.9227, weighted_loss: 0.6637, label: 0, bag_size: 2533\n",
      "batch 39, loss: 0.7438, instance_loss: 1.2678, weighted_loss: 0.9010, label: 1, bag_size: 6477\n",
      "batch 59, loss: 0.6994, instance_loss: 1.0015, weighted_loss: 0.7900, label: 0, bag_size: 2918\n",
      "batch 79, loss: 0.5659, instance_loss: 0.7277, weighted_loss: 0.6144, label: 1, bag_size: 3170\n",
      "batch 99, loss: 0.9165, instance_loss: 1.0651, weighted_loss: 0.9611, label: 0, bag_size: 3264\n",
      "batch 119, loss: 0.6208, instance_loss: 0.8820, weighted_loss: 0.6992, label: 0, bag_size: 6463\n",
      "batch 139, loss: 0.3969, instance_loss: 0.4968, weighted_loss: 0.4269, label: 0, bag_size: 4808\n",
      "batch 159, loss: 0.7240, instance_loss: 0.7505, weighted_loss: 0.7319, label: 1, bag_size: 2267\n",
      "batch 179, loss: 0.5908, instance_loss: 0.6715, weighted_loss: 0.6150, label: 1, bag_size: 5156\n",
      "batch 199, loss: 0.4505, instance_loss: 0.6028, weighted_loss: 0.4962, label: 0, bag_size: 2945\n",
      "batch 219, loss: 0.5826, instance_loss: 0.9485, weighted_loss: 0.6924, label: 1, bag_size: 2207\n",
      "batch 239, loss: 0.2643, instance_loss: 0.2641, weighted_loss: 0.2643, label: 1, bag_size: 2126\n",
      "batch 259, loss: 1.1166, instance_loss: 1.3665, weighted_loss: 1.1916, label: 0, bag_size: 2482\n",
      "batch 279, loss: 1.6124, instance_loss: 1.7534, weighted_loss: 1.6547, label: 1, bag_size: 1552\n",
      "batch 299, loss: 0.4061, instance_loss: 0.3295, weighted_loss: 0.3831, label: 0, bag_size: 18251\n",
      "batch 319, loss: 0.4014, instance_loss: 0.7858, weighted_loss: 0.5167, label: 1, bag_size: 6151\n",
      "batch 339, loss: 0.2951, instance_loss: 0.4030, weighted_loss: 0.3275, label: 1, bag_size: 5651\n",
      "batch 359, loss: 0.4887, instance_loss: 0.5661, weighted_loss: 0.5119, label: 1, bag_size: 24092\n",
      "batch 379, loss: 0.7341, instance_loss: 0.7332, weighted_loss: 0.7338, label: 0, bag_size: 5499\n",
      "batch 399, loss: 0.8857, instance_loss: 1.0005, weighted_loss: 0.9201, label: 1, bag_size: 2465\n",
      "batch 419, loss: 0.6634, instance_loss: 0.7465, weighted_loss: 0.6884, label: 0, bag_size: 2678\n",
      "batch 439, loss: 0.8375, instance_loss: 0.9920, weighted_loss: 0.8839, label: 1, bag_size: 3533\n",
      "batch 459, loss: 0.6142, instance_loss: 0.8134, weighted_loss: 0.6739, label: 1, bag_size: 6760\n",
      "batch 479, loss: 0.4244, instance_loss: 0.3834, weighted_loss: 0.4121, label: 0, bag_size: 2394\n",
      "batch 499, loss: 0.3559, instance_loss: 0.2685, weighted_loss: 0.3297, label: 1, bag_size: 4572\n",
      "batch 519, loss: 0.6548, instance_loss: 0.6942, weighted_loss: 0.6666, label: 1, bag_size: 5062\n",
      "batch 539, loss: 0.3675, instance_loss: 0.6075, weighted_loss: 0.4395, label: 1, bag_size: 2890\n",
      "batch 559, loss: 0.9666, instance_loss: 1.2951, weighted_loss: 1.0651, label: 0, bag_size: 2275\n",
      "batch 579, loss: 0.6684, instance_loss: 1.0667, weighted_loss: 0.7879, label: 0, bag_size: 5590\n",
      "batch 599, loss: 1.1764, instance_loss: 1.6160, weighted_loss: 1.3083, label: 1, bag_size: 1825\n",
      "batch 619, loss: 0.4184, instance_loss: 0.8282, weighted_loss: 0.5413, label: 0, bag_size: 6076\n",
      "batch 639, loss: 0.0494, instance_loss: 0.2611, weighted_loss: 0.1129, label: 1, bag_size: 3126\n",
      "batch 659, loss: 0.9965, instance_loss: 1.1656, weighted_loss: 1.0472, label: 0, bag_size: 5458\n",
      "batch 679, loss: 0.5842, instance_loss: 0.7420, weighted_loss: 0.6315, label: 0, bag_size: 2582\n",
      "batch 699, loss: 0.6341, instance_loss: 0.7140, weighted_loss: 0.6581, label: 0, bag_size: 3534\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9673254985754985: correct 10865/11232\n",
      "class 1 clustering acc 0.14316239316239315: correct 804/5616\n",
      "Epoch: 14, train_loss: 0.6799, train_clustering_loss:  0.8545, train_error: 0.4103\n",
      "class 0: acc 0.6229050279329609, correct 223/358\n",
      "class 1: acc 0.5552325581395349, correct 191/344\n",
      "\n",
      "Val Set, val_loss: 0.6554, val_error: 0.3708, auc: 0.6810\n",
      "class 0 clustering acc 0.9789325842696629: correct 1394/1424\n",
      "class 1 clustering acc 0.12078651685393259: correct 86/712\n",
      "class 0: acc 0.6296296296296297, correct 34/54\n",
      "class 1: acc 0.6285714285714286, correct 22/35\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4617, instance_loss: 0.6255, weighted_loss: 0.5108, label: 1, bag_size: 5539\n",
      "batch 39, loss: 0.2992, instance_loss: 0.3330, weighted_loss: 0.3094, label: 1, bag_size: 5801\n",
      "batch 59, loss: 0.7668, instance_loss: 1.2962, weighted_loss: 0.9256, label: 1, bag_size: 3571\n",
      "batch 79, loss: 0.5866, instance_loss: 0.4908, weighted_loss: 0.5578, label: 1, bag_size: 2754\n",
      "batch 99, loss: 0.6557, instance_loss: 0.8233, weighted_loss: 0.7060, label: 1, bag_size: 3548\n",
      "batch 119, loss: 0.6594, instance_loss: 0.8997, weighted_loss: 0.7315, label: 0, bag_size: 4146\n",
      "batch 139, loss: 0.5485, instance_loss: 0.8403, weighted_loss: 0.6361, label: 1, bag_size: 3287\n",
      "batch 159, loss: 0.8350, instance_loss: 1.2802, weighted_loss: 0.9685, label: 0, bag_size: 941\n",
      "batch 179, loss: 0.4892, instance_loss: 0.4507, weighted_loss: 0.4777, label: 0, bag_size: 15682\n",
      "batch 199, loss: 0.6979, instance_loss: 0.8312, weighted_loss: 0.7379, label: 0, bag_size: 1884\n",
      "batch 219, loss: 1.1553, instance_loss: 1.5860, weighted_loss: 1.2845, label: 1, bag_size: 5671\n",
      "batch 239, loss: 1.4287, instance_loss: 1.8798, weighted_loss: 1.5640, label: 1, bag_size: 10165\n",
      "batch 259, loss: 1.5231, instance_loss: 2.0209, weighted_loss: 1.6724, label: 0, bag_size: 2592\n",
      "batch 279, loss: 0.3983, instance_loss: 0.3598, weighted_loss: 0.3868, label: 0, bag_size: 22594\n",
      "batch 299, loss: 0.5213, instance_loss: 0.6486, weighted_loss: 0.5595, label: 1, bag_size: 3243\n",
      "batch 319, loss: 0.4170, instance_loss: 0.4683, weighted_loss: 0.4324, label: 1, bag_size: 6500\n",
      "batch 339, loss: 0.4671, instance_loss: 0.6247, weighted_loss: 0.5144, label: 1, bag_size: 12654\n",
      "batch 359, loss: 0.4286, instance_loss: 0.6136, weighted_loss: 0.4841, label: 0, bag_size: 6463\n",
      "batch 379, loss: 0.6269, instance_loss: 0.7772, weighted_loss: 0.6720, label: 0, bag_size: 2963\n",
      "batch 399, loss: 0.8141, instance_loss: 1.1542, weighted_loss: 0.9161, label: 1, bag_size: 6759\n",
      "batch 419, loss: 1.2933, instance_loss: 1.1077, weighted_loss: 1.2376, label: 1, bag_size: 2877\n",
      "batch 439, loss: 0.6960, instance_loss: 0.7669, weighted_loss: 0.7172, label: 0, bag_size: 12656\n",
      "batch 459, loss: 0.3267, instance_loss: 0.4134, weighted_loss: 0.3527, label: 0, bag_size: 10590\n",
      "batch 479, loss: 0.5939, instance_loss: 0.4282, weighted_loss: 0.5442, label: 1, bag_size: 5672\n",
      "batch 499, loss: 0.2679, instance_loss: 0.1927, weighted_loss: 0.2453, label: 0, bag_size: 5967\n",
      "batch 519, loss: 1.8397, instance_loss: 2.2353, weighted_loss: 1.9584, label: 1, bag_size: 3571\n",
      "batch 539, loss: 0.9587, instance_loss: 0.9418, weighted_loss: 0.9536, label: 0, bag_size: 2792\n",
      "batch 559, loss: 0.7139, instance_loss: 0.5094, weighted_loss: 0.6526, label: 1, bag_size: 4803\n",
      "batch 579, loss: 0.2288, instance_loss: 0.3712, weighted_loss: 0.2715, label: 0, bag_size: 19223\n",
      "batch 599, loss: 0.8421, instance_loss: 1.1195, weighted_loss: 0.9253, label: 0, bag_size: 25403\n",
      "batch 619, loss: 0.6618, instance_loss: 0.9039, weighted_loss: 0.7344, label: 0, bag_size: 2792\n",
      "batch 639, loss: 0.3110, instance_loss: 0.4779, weighted_loss: 0.3611, label: 0, bag_size: 7027\n",
      "batch 659, loss: 1.0807, instance_loss: 0.9266, weighted_loss: 1.0344, label: 0, bag_size: 3727\n",
      "batch 679, loss: 0.3049, instance_loss: 0.5571, weighted_loss: 0.3806, label: 0, bag_size: 1884\n",
      "batch 699, loss: 0.2510, instance_loss: 0.3428, weighted_loss: 0.2785, label: 1, bag_size: 4044\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9500534188034188: correct 10671/11232\n",
      "class 1 clustering acc 0.21545584045584046: correct 1210/5616\n",
      "Epoch: 15, train_loss: 0.6614, train_clustering_loss:  0.8421, train_error: 0.3889\n",
      "class 0: acc 0.6629834254143646, correct 240/362\n",
      "class 1: acc 0.5558823529411765, correct 189/340\n",
      "\n",
      "Val Set, val_loss: 0.7063, val_error: 0.4607, auc: 0.6169\n",
      "class 0 clustering acc 0.9859550561797753: correct 1404/1424\n",
      "class 1 clustering acc 0.08567415730337079: correct 61/712\n",
      "class 0: acc 0.48148148148148145, correct 26/54\n",
      "class 1: acc 0.6285714285714286, correct 22/35\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6275, instance_loss: 0.9531, weighted_loss: 0.7252, label: 0, bag_size: 3714\n",
      "batch 39, loss: 1.0409, instance_loss: 1.4115, weighted_loss: 1.1520, label: 0, bag_size: 3450\n",
      "batch 59, loss: 0.0400, instance_loss: 0.1042, weighted_loss: 0.0593, label: 1, bag_size: 5651\n",
      "batch 79, loss: 0.6540, instance_loss: 1.1088, weighted_loss: 0.7904, label: 0, bag_size: 5001\n",
      "batch 99, loss: 0.5648, instance_loss: 0.5346, weighted_loss: 0.5557, label: 0, bag_size: 15687\n",
      "batch 119, loss: 0.4194, instance_loss: 0.6440, weighted_loss: 0.4868, label: 1, bag_size: 3484\n",
      "batch 139, loss: 0.3032, instance_loss: 0.4786, weighted_loss: 0.3558, label: 0, bag_size: 3843\n",
      "batch 159, loss: 1.0049, instance_loss: 1.2767, weighted_loss: 1.0864, label: 0, bag_size: 2346\n",
      "batch 179, loss: 0.5782, instance_loss: 0.7534, weighted_loss: 0.6308, label: 0, bag_size: 4303\n",
      "batch 199, loss: 0.8615, instance_loss: 0.8953, weighted_loss: 0.8716, label: 0, bag_size: 4468\n",
      "batch 219, loss: 0.7503, instance_loss: 0.9090, weighted_loss: 0.7979, label: 1, bag_size: 2824\n",
      "batch 239, loss: 0.1401, instance_loss: 0.2781, weighted_loss: 0.1815, label: 0, bag_size: 14635\n",
      "batch 259, loss: 0.3960, instance_loss: 0.6228, weighted_loss: 0.4640, label: 1, bag_size: 6878\n",
      "batch 279, loss: 0.7620, instance_loss: 0.8933, weighted_loss: 0.8014, label: 1, bag_size: 13226\n",
      "batch 299, loss: 0.5529, instance_loss: 0.7710, weighted_loss: 0.6184, label: 0, bag_size: 14212\n",
      "batch 319, loss: 0.7012, instance_loss: 0.7759, weighted_loss: 0.7236, label: 0, bag_size: 10532\n",
      "batch 339, loss: 0.5415, instance_loss: 0.7025, weighted_loss: 0.5898, label: 1, bag_size: 5156\n",
      "batch 359, loss: 0.5216, instance_loss: 0.7691, weighted_loss: 0.5959, label: 0, bag_size: 4066\n",
      "batch 379, loss: 0.2052, instance_loss: 0.3516, weighted_loss: 0.2491, label: 0, bag_size: 1884\n",
      "batch 399, loss: 0.7445, instance_loss: 0.5351, weighted_loss: 0.6817, label: 0, bag_size: 6118\n",
      "batch 419, loss: 0.8714, instance_loss: 1.4498, weighted_loss: 1.0449, label: 0, bag_size: 4907\n",
      "batch 439, loss: 0.4489, instance_loss: 0.5965, weighted_loss: 0.4932, label: 0, bag_size: 18807\n",
      "batch 459, loss: 0.5541, instance_loss: 0.9760, weighted_loss: 0.6807, label: 1, bag_size: 21105\n",
      "batch 479, loss: 0.4529, instance_loss: 0.4604, weighted_loss: 0.4552, label: 1, bag_size: 1503\n",
      "batch 499, loss: 0.7455, instance_loss: 0.9090, weighted_loss: 0.7946, label: 1, bag_size: 12948\n",
      "batch 519, loss: 0.3489, instance_loss: 0.4109, weighted_loss: 0.3675, label: 1, bag_size: 4970\n",
      "batch 539, loss: 0.6297, instance_loss: 0.7658, weighted_loss: 0.6706, label: 0, bag_size: 5642\n",
      "batch 559, loss: 0.5636, instance_loss: 0.7248, weighted_loss: 0.6120, label: 0, bag_size: 6351\n",
      "batch 579, loss: 0.7744, instance_loss: 0.7141, weighted_loss: 0.7563, label: 1, bag_size: 2820\n",
      "batch 599, loss: 0.6509, instance_loss: 0.7111, weighted_loss: 0.6689, label: 0, bag_size: 2762\n",
      "batch 619, loss: 1.9411, instance_loss: 2.3881, weighted_loss: 2.0752, label: 1, bag_size: 14306\n",
      "batch 639, loss: 0.5553, instance_loss: 0.7101, weighted_loss: 0.6018, label: 0, bag_size: 4558\n",
      "batch 659, loss: 0.9925, instance_loss: 1.2089, weighted_loss: 1.0574, label: 1, bag_size: 4819\n",
      "batch 679, loss: 0.6381, instance_loss: 0.6356, weighted_loss: 0.6374, label: 0, bag_size: 2089\n",
      "batch 699, loss: 0.8596, instance_loss: 1.0194, weighted_loss: 0.9076, label: 1, bag_size: 4985\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9539707977207977: correct 10715/11232\n",
      "class 1 clustering acc 0.1891025641025641: correct 1062/5616\n",
      "Epoch: 16, train_loss: 0.6682, train_clustering_loss:  0.8392, train_error: 0.3832\n",
      "class 0: acc 0.6442577030812325, correct 230/357\n",
      "class 1: acc 0.5884057971014492, correct 203/345\n",
      "\n",
      "Val Set, val_loss: 0.6457, val_error: 0.3933, auc: 0.6407\n",
      "class 0 clustering acc 0.9838483146067416: correct 1401/1424\n",
      "class 1 clustering acc 0.12359550561797752: correct 88/712\n",
      "class 0: acc 0.7037037037037037, correct 38/54\n",
      "class 1: acc 0.45714285714285713, correct 16/35\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.1953, instance_loss: 1.4945, weighted_loss: 1.2850, label: 0, bag_size: 5487\n",
      "batch 39, loss: 0.9158, instance_loss: 0.9710, weighted_loss: 0.9324, label: 0, bag_size: 4093\n",
      "batch 59, loss: 0.5810, instance_loss: 0.6928, weighted_loss: 0.6146, label: 0, bag_size: 28144\n",
      "batch 79, loss: 0.6925, instance_loss: 0.8817, weighted_loss: 0.7493, label: 0, bag_size: 5093\n",
      "batch 99, loss: 0.6292, instance_loss: 1.0175, weighted_loss: 0.7457, label: 1, bag_size: 21473\n",
      "batch 119, loss: 0.3147, instance_loss: 0.3971, weighted_loss: 0.3394, label: 0, bag_size: 28144\n",
      "batch 139, loss: 1.2664, instance_loss: 1.2514, weighted_loss: 1.2619, label: 0, bag_size: 5894\n",
      "batch 159, loss: 0.6521, instance_loss: 1.2019, weighted_loss: 0.8170, label: 0, bag_size: 4155\n",
      "batch 179, loss: 1.6438, instance_loss: 1.7882, weighted_loss: 1.6872, label: 0, bag_size: 4390\n",
      "batch 199, loss: 0.4465, instance_loss: 0.6385, weighted_loss: 0.5041, label: 0, bag_size: 5982\n",
      "batch 219, loss: 0.8888, instance_loss: 0.9640, weighted_loss: 0.9113, label: 0, bag_size: 2682\n",
      "batch 239, loss: 0.5254, instance_loss: 0.6285, weighted_loss: 0.5564, label: 1, bag_size: 6190\n",
      "batch 259, loss: 0.7018, instance_loss: 0.7152, weighted_loss: 0.7058, label: 1, bag_size: 3170\n",
      "batch 279, loss: 0.3462, instance_loss: 0.3383, weighted_loss: 0.3438, label: 0, bag_size: 5338\n",
      "batch 299, loss: 0.4078, instance_loss: 0.5597, weighted_loss: 0.4533, label: 0, bag_size: 1944\n",
      "batch 319, loss: 0.3048, instance_loss: 0.7302, weighted_loss: 0.4324, label: 1, bag_size: 3126\n",
      "batch 339, loss: 0.5780, instance_loss: 0.7066, weighted_loss: 0.6166, label: 0, bag_size: 2529\n",
      "batch 359, loss: 0.3428, instance_loss: 0.4572, weighted_loss: 0.3771, label: 0, bag_size: 7471\n",
      "batch 379, loss: 0.5014, instance_loss: 0.6756, weighted_loss: 0.5536, label: 1, bag_size: 2669\n",
      "batch 399, loss: 0.2210, instance_loss: 0.2104, weighted_loss: 0.2178, label: 1, bag_size: 3081\n",
      "batch 419, loss: 0.6724, instance_loss: 1.0322, weighted_loss: 0.7803, label: 0, bag_size: 26830\n",
      "batch 439, loss: 0.2562, instance_loss: 0.3029, weighted_loss: 0.2702, label: 0, bag_size: 4175\n",
      "batch 459, loss: 1.0404, instance_loss: 0.7608, weighted_loss: 0.9565, label: 0, bag_size: 6135\n",
      "batch 479, loss: 1.2688, instance_loss: 2.0048, weighted_loss: 1.4896, label: 0, bag_size: 2923\n",
      "batch 499, loss: 0.6283, instance_loss: 0.8079, weighted_loss: 0.6822, label: 0, bag_size: 3928\n",
      "batch 519, loss: 0.3068, instance_loss: 0.4629, weighted_loss: 0.3536, label: 1, bag_size: 2005\n",
      "batch 539, loss: 1.6004, instance_loss: 1.6292, weighted_loss: 1.6090, label: 1, bag_size: 5065\n",
      "batch 559, loss: 0.6056, instance_loss: 1.1039, weighted_loss: 0.7551, label: 1, bag_size: 2829\n",
      "batch 579, loss: 1.0086, instance_loss: 0.9281, weighted_loss: 0.9844, label: 1, bag_size: 5671\n",
      "batch 599, loss: 0.5795, instance_loss: 0.6428, weighted_loss: 0.5985, label: 1, bag_size: 4802\n",
      "batch 619, loss: 0.9797, instance_loss: 1.2023, weighted_loss: 1.0465, label: 0, bag_size: 4175\n",
      "batch 639, loss: 0.7356, instance_loss: 0.7461, weighted_loss: 0.7388, label: 1, bag_size: 5592\n",
      "batch 659, loss: 0.4404, instance_loss: 0.6424, weighted_loss: 0.5010, label: 1, bag_size: 3588\n",
      "batch 679, loss: 0.5059, instance_loss: 0.7277, weighted_loss: 0.5725, label: 0, bag_size: 4175\n",
      "batch 699, loss: 0.8544, instance_loss: 0.8728, weighted_loss: 0.8599, label: 1, bag_size: 5738\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9569978632478633: correct 10749/11232\n",
      "class 1 clustering acc 0.1883903133903134: correct 1058/5616\n",
      "Epoch: 17, train_loss: 0.6791, train_clustering_loss:  0.8531, train_error: 0.4003\n",
      "class 0: acc 0.6394366197183099, correct 227/355\n",
      "class 1: acc 0.5590778097982709, correct 194/347\n",
      "\n",
      "Val Set, val_loss: 0.6414, val_error: 0.3820, auc: 0.6624\n",
      "class 0 clustering acc 0.9943820224719101: correct 1416/1424\n",
      "class 1 clustering acc 0.054775280898876406: correct 39/712\n",
      "class 0: acc 0.7407407407407407, correct 40/54\n",
      "class 1: acc 0.42857142857142855, correct 15/35\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2985, instance_loss: 0.4763, weighted_loss: 0.3519, label: 0, bag_size: 15706\n",
      "batch 39, loss: 0.2907, instance_loss: 0.3708, weighted_loss: 0.3148, label: 1, bag_size: 3081\n",
      "batch 59, loss: 1.1285, instance_loss: 1.3259, weighted_loss: 1.1877, label: 1, bag_size: 2701\n",
      "batch 79, loss: 0.8208, instance_loss: 0.9372, weighted_loss: 0.8557, label: 0, bag_size: 5499\n",
      "batch 99, loss: 0.6154, instance_loss: 0.7341, weighted_loss: 0.6510, label: 1, bag_size: 1937\n",
      "batch 119, loss: 0.9891, instance_loss: 1.2203, weighted_loss: 1.0585, label: 0, bag_size: 4098\n",
      "batch 139, loss: 0.6931, instance_loss: 0.8413, weighted_loss: 0.7375, label: 1, bag_size: 5689\n",
      "batch 159, loss: 0.2298, instance_loss: 0.2212, weighted_loss: 0.2272, label: 1, bag_size: 4233\n",
      "batch 179, loss: 0.6225, instance_loss: 0.7812, weighted_loss: 0.6701, label: 0, bag_size: 1539\n",
      "batch 199, loss: 0.4283, instance_loss: 0.5121, weighted_loss: 0.4534, label: 0, bag_size: 17546\n",
      "batch 219, loss: 0.8840, instance_loss: 0.9003, weighted_loss: 0.8889, label: 1, bag_size: 19013\n",
      "batch 239, loss: 1.2259, instance_loss: 1.1793, weighted_loss: 1.2119, label: 1, bag_size: 15434\n",
      "batch 259, loss: 0.7142, instance_loss: 0.9564, weighted_loss: 0.7869, label: 1, bag_size: 23277\n",
      "batch 279, loss: 0.5544, instance_loss: 0.9778, weighted_loss: 0.6814, label: 0, bag_size: 11299\n",
      "batch 299, loss: 0.3395, instance_loss: 0.7381, weighted_loss: 0.4591, label: 1, bag_size: 2178\n",
      "batch 319, loss: 0.1108, instance_loss: 0.3507, weighted_loss: 0.1828, label: 0, bag_size: 15588\n",
      "batch 339, loss: 0.5095, instance_loss: 0.7231, weighted_loss: 0.5736, label: 1, bag_size: 3824\n",
      "batch 359, loss: 0.3098, instance_loss: 0.5585, weighted_loss: 0.3844, label: 1, bag_size: 12654\n",
      "batch 379, loss: 1.1977, instance_loss: 2.5579, weighted_loss: 1.6057, label: 0, bag_size: 6057\n",
      "batch 399, loss: 0.7278, instance_loss: 1.2380, weighted_loss: 0.8809, label: 1, bag_size: 1178\n",
      "batch 419, loss: 0.6270, instance_loss: 0.7417, weighted_loss: 0.6614, label: 1, bag_size: 7717\n",
      "batch 439, loss: 0.7079, instance_loss: 0.9666, weighted_loss: 0.7855, label: 0, bag_size: 6827\n",
      "batch 459, loss: 0.8592, instance_loss: 1.0833, weighted_loss: 0.9264, label: 1, bag_size: 16538\n",
      "batch 479, loss: 1.0574, instance_loss: 1.3489, weighted_loss: 1.1449, label: 0, bag_size: 15139\n",
      "batch 499, loss: 0.8321, instance_loss: 1.1477, weighted_loss: 0.9268, label: 1, bag_size: 20955\n",
      "batch 519, loss: 0.4772, instance_loss: 0.7720, weighted_loss: 0.5657, label: 0, bag_size: 4694\n",
      "batch 539, loss: 0.7269, instance_loss: 1.1289, weighted_loss: 0.8475, label: 1, bag_size: 2036\n",
      "batch 559, loss: 0.8427, instance_loss: 1.0068, weighted_loss: 0.8919, label: 0, bag_size: 2428\n",
      "batch 579, loss: 0.4318, instance_loss: 0.6106, weighted_loss: 0.4854, label: 1, bag_size: 5156\n",
      "batch 599, loss: 0.2875, instance_loss: 0.4339, weighted_loss: 0.3314, label: 0, bag_size: 11018\n",
      "batch 619, loss: 0.3691, instance_loss: 0.2891, weighted_loss: 0.3451, label: 0, bag_size: 5996\n",
      "batch 639, loss: 0.5400, instance_loss: 0.5914, weighted_loss: 0.5555, label: 0, bag_size: 4922\n",
      "batch 659, loss: 0.4910, instance_loss: 0.5691, weighted_loss: 0.5144, label: 0, bag_size: 13573\n",
      "batch 679, loss: 1.1450, instance_loss: 1.4752, weighted_loss: 1.2441, label: 1, bag_size: 1178\n",
      "batch 699, loss: 0.2737, instance_loss: 0.2755, weighted_loss: 0.2742, label: 0, bag_size: 5478\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9648326210826211: correct 10837/11232\n",
      "class 1 clustering acc 0.15473646723646722: correct 869/5616\n",
      "Epoch: 18, train_loss: 0.6769, train_clustering_loss:  0.9302, train_error: 0.4316\n",
      "class 0: acc 0.5870786516853933, correct 209/356\n",
      "class 1: acc 0.5491329479768786, correct 190/346\n",
      "\n",
      "Val Set, val_loss: 0.6811, val_error: 0.4157, auc: 0.6413\n",
      "class 0 clustering acc 0.949438202247191: correct 1352/1424\n",
      "class 1 clustering acc 0.30337078651685395: correct 216/712\n",
      "class 0: acc 0.5740740740740741, correct 31/54\n",
      "class 1: acc 0.6, correct 21/35\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.3729, instance_loss: 1.6350, weighted_loss: 1.4515, label: 0, bag_size: 5536\n",
      "batch 39, loss: 0.3740, instance_loss: 0.7746, weighted_loss: 0.4942, label: 1, bag_size: 5671\n",
      "batch 59, loss: 0.1679, instance_loss: 0.1767, weighted_loss: 0.1705, label: 0, bag_size: 19396\n",
      "batch 79, loss: 0.4971, instance_loss: 0.6671, weighted_loss: 0.5481, label: 0, bag_size: 6301\n",
      "batch 99, loss: 0.5546, instance_loss: 0.6081, weighted_loss: 0.5706, label: 0, bag_size: 6994\n",
      "batch 119, loss: 1.2413, instance_loss: 1.6653, weighted_loss: 1.3685, label: 0, bag_size: 3093\n",
      "batch 139, loss: 0.3047, instance_loss: 0.4731, weighted_loss: 0.3552, label: 1, bag_size: 2081\n",
      "batch 159, loss: 0.3349, instance_loss: 0.3573, weighted_loss: 0.3417, label: 0, bag_size: 6209\n",
      "batch 179, loss: 0.7084, instance_loss: 0.5875, weighted_loss: 0.6721, label: 0, bag_size: 4253\n",
      "batch 199, loss: 0.5532, instance_loss: 0.6143, weighted_loss: 0.5715, label: 1, bag_size: 4800\n",
      "batch 219, loss: 0.5147, instance_loss: 0.7066, weighted_loss: 0.5723, label: 1, bag_size: 5152\n",
      "batch 239, loss: 1.2339, instance_loss: 1.2573, weighted_loss: 1.2409, label: 1, bag_size: 4384\n",
      "batch 259, loss: 0.5125, instance_loss: 0.5964, weighted_loss: 0.5376, label: 1, bag_size: 5152\n",
      "batch 279, loss: 0.6829, instance_loss: 0.9019, weighted_loss: 0.7486, label: 1, bag_size: 4243\n",
      "batch 299, loss: 0.6579, instance_loss: 0.8621, weighted_loss: 0.7192, label: 0, bag_size: 4406\n",
      "batch 319, loss: 0.7123, instance_loss: 0.8652, weighted_loss: 0.7582, label: 0, bag_size: 16663\n",
      "batch 339, loss: 0.7009, instance_loss: 0.8076, weighted_loss: 0.7329, label: 0, bag_size: 3022\n",
      "batch 359, loss: 0.2688, instance_loss: 0.6579, weighted_loss: 0.3855, label: 1, bag_size: 4800\n",
      "batch 379, loss: 0.2945, instance_loss: 0.2622, weighted_loss: 0.2848, label: 1, bag_size: 3081\n",
      "batch 399, loss: 0.8414, instance_loss: 0.9935, weighted_loss: 0.8870, label: 1, bag_size: 4476\n",
      "batch 419, loss: 0.2805, instance_loss: 0.3766, weighted_loss: 0.3093, label: 1, bag_size: 3764\n",
      "batch 439, loss: 0.4809, instance_loss: 0.5090, weighted_loss: 0.4893, label: 1, bag_size: 1746\n",
      "batch 459, loss: 0.4912, instance_loss: 0.5903, weighted_loss: 0.5209, label: 1, bag_size: 5577\n",
      "batch 479, loss: 0.2124, instance_loss: 0.2511, weighted_loss: 0.2240, label: 0, bag_size: 14635\n",
      "batch 499, loss: 0.5143, instance_loss: 0.7259, weighted_loss: 0.5778, label: 1, bag_size: 3020\n",
      "batch 519, loss: 0.6811, instance_loss: 0.7161, weighted_loss: 0.6916, label: 1, bag_size: 5810\n",
      "batch 539, loss: 0.6233, instance_loss: 0.7921, weighted_loss: 0.6739, label: 1, bag_size: 6235\n",
      "batch 559, loss: 1.3605, instance_loss: 1.7412, weighted_loss: 1.4747, label: 0, bag_size: 2346\n",
      "batch 579, loss: 0.1206, instance_loss: 0.1521, weighted_loss: 0.1301, label: 1, bag_size: 4815\n",
      "batch 599, loss: 0.6515, instance_loss: 0.9235, weighted_loss: 0.7331, label: 1, bag_size: 3405\n",
      "batch 619, loss: 0.6947, instance_loss: 0.9470, weighted_loss: 0.7704, label: 1, bag_size: 3548\n",
      "batch 639, loss: 0.2825, instance_loss: 0.4052, weighted_loss: 0.3193, label: 1, bag_size: 5817\n",
      "batch 659, loss: 0.8053, instance_loss: 1.0270, weighted_loss: 0.8718, label: 1, bag_size: 2840\n",
      "batch 679, loss: 0.6944, instance_loss: 0.9133, weighted_loss: 0.7601, label: 1, bag_size: 3548\n",
      "batch 699, loss: 0.3962, instance_loss: 0.5045, weighted_loss: 0.4287, label: 0, bag_size: 4641\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.948539886039886: correct 10654/11232\n",
      "class 1 clustering acc 0.22364672364672364: correct 1256/5616\n",
      "Epoch: 19, train_loss: 0.6579, train_clustering_loss:  0.8409, train_error: 0.3903\n",
      "class 0: acc 0.64, correct 224/350\n",
      "class 1: acc 0.5795454545454546, correct 204/352\n",
      "\n",
      "Val Set, val_loss: 0.6399, val_error: 0.3483, auc: 0.6947\n",
      "class 0 clustering acc 0.9712078651685393: correct 1383/1424\n",
      "class 1 clustering acc 0.16151685393258428: correct 115/712\n",
      "class 0: acc 0.6666666666666666, correct 36/54\n",
      "class 1: acc 0.6285714285714286, correct 22/35\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.8517, instance_loss: 0.9588, weighted_loss: 0.8839, label: 0, bag_size: 2540\n",
      "batch 39, loss: 0.2009, instance_loss: 0.3865, weighted_loss: 0.2566, label: 1, bag_size: 5592\n",
      "batch 59, loss: 0.8347, instance_loss: 1.1429, weighted_loss: 0.9272, label: 1, bag_size: 27072\n",
      "batch 79, loss: 0.2643, instance_loss: 0.3357, weighted_loss: 0.2858, label: 1, bag_size: 5062\n",
      "batch 99, loss: 0.3302, instance_loss: 0.3494, weighted_loss: 0.3359, label: 0, bag_size: 3448\n",
      "batch 119, loss: 0.9221, instance_loss: 1.1968, weighted_loss: 1.0045, label: 1, bag_size: 1961\n",
      "batch 139, loss: 0.6453, instance_loss: 0.6695, weighted_loss: 0.6525, label: 1, bag_size: 4795\n",
      "batch 159, loss: 0.6621, instance_loss: 0.8301, weighted_loss: 0.7125, label: 0, bag_size: 18810\n",
      "batch 179, loss: 0.2098, instance_loss: 0.2221, weighted_loss: 0.2135, label: 0, bag_size: 17335\n",
      "batch 199, loss: 1.4831, instance_loss: 2.4329, weighted_loss: 1.7681, label: 1, bag_size: 15118\n",
      "batch 219, loss: 1.2787, instance_loss: 1.5257, weighted_loss: 1.3528, label: 0, bag_size: 3661\n",
      "batch 239, loss: 0.7519, instance_loss: 0.8849, weighted_loss: 0.7918, label: 0, bag_size: 2969\n",
      "batch 259, loss: 0.4576, instance_loss: 0.5525, weighted_loss: 0.4861, label: 1, bag_size: 3626\n",
      "batch 279, loss: 0.7297, instance_loss: 0.9179, weighted_loss: 0.7862, label: 0, bag_size: 4093\n",
      "batch 299, loss: 0.6810, instance_loss: 1.0734, weighted_loss: 0.7987, label: 1, bag_size: 7184\n",
      "batch 319, loss: 0.4159, instance_loss: 0.5379, weighted_loss: 0.4525, label: 1, bag_size: 1552\n",
      "batch 339, loss: 0.4519, instance_loss: 0.7116, weighted_loss: 0.5298, label: 0, bag_size: 9001\n",
      "batch 359, loss: 1.1711, instance_loss: 1.5866, weighted_loss: 1.2958, label: 1, bag_size: 16427\n",
      "batch 379, loss: 0.4042, instance_loss: 0.4485, weighted_loss: 0.4175, label: 0, bag_size: 1880\n",
      "batch 399, loss: 0.4751, instance_loss: 0.5874, weighted_loss: 0.5088, label: 1, bag_size: 5570\n",
      "batch 419, loss: 0.0580, instance_loss: 0.7951, weighted_loss: 0.2791, label: 0, bag_size: 1488\n",
      "batch 439, loss: 0.4158, instance_loss: 0.5173, weighted_loss: 0.4463, label: 0, bag_size: 4086\n",
      "batch 459, loss: 0.8418, instance_loss: 1.0590, weighted_loss: 0.9069, label: 0, bag_size: 4172\n",
      "batch 479, loss: 0.2032, instance_loss: 0.1410, weighted_loss: 0.1846, label: 0, bag_size: 4181\n",
      "batch 499, loss: 0.7725, instance_loss: 0.5630, weighted_loss: 0.7097, label: 1, bag_size: 4795\n",
      "batch 519, loss: 0.5758, instance_loss: 0.9039, weighted_loss: 0.6742, label: 1, bag_size: 1178\n",
      "batch 539, loss: 0.7972, instance_loss: 1.1984, weighted_loss: 0.9176, label: 0, bag_size: 12656\n",
      "batch 559, loss: 0.7197, instance_loss: 0.8367, weighted_loss: 0.7548, label: 1, bag_size: 5665\n",
      "batch 579, loss: 0.5443, instance_loss: 0.6032, weighted_loss: 0.5620, label: 0, bag_size: 2856\n",
      "batch 599, loss: 0.5242, instance_loss: 0.6690, weighted_loss: 0.5676, label: 1, bag_size: 4853\n",
      "batch 619, loss: 0.2911, instance_loss: 0.4749, weighted_loss: 0.3462, label: 0, bag_size: 12517\n",
      "batch 639, loss: 0.8524, instance_loss: 1.2067, weighted_loss: 0.9587, label: 1, bag_size: 15483\n",
      "batch 659, loss: 0.5804, instance_loss: 0.7921, weighted_loss: 0.6439, label: 0, bag_size: 1638\n",
      "batch 679, loss: 0.5711, instance_loss: 0.7114, weighted_loss: 0.6132, label: 0, bag_size: 13609\n",
      "batch 699, loss: 0.3417, instance_loss: 0.4691, weighted_loss: 0.3799, label: 0, bag_size: 2766\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9512108262108262: correct 10684/11232\n",
      "class 1 clustering acc 0.2207977207977208: correct 1240/5616\n",
      "Epoch: 20, train_loss: 0.6517, train_clustering_loss:  0.8280, train_error: 0.3832\n",
      "class 0: acc 0.6775956284153005, correct 248/366\n",
      "class 1: acc 0.5505952380952381, correct 185/336\n",
      "\n",
      "Val Set, val_loss: 0.6099, val_error: 0.2921, auc: 0.7053\n",
      "class 0 clustering acc 0.9754213483146067: correct 1389/1424\n",
      "class 1 clustering acc 0.2247191011235955: correct 160/712\n",
      "class 0: acc 0.8703703703703703, correct 47/54\n",
      "class 1: acc 0.45714285714285713, correct 16/35\n",
      "Validation loss decreased (0.631397 --> 0.609920).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6114, instance_loss: 0.8881, weighted_loss: 0.6944, label: 1, bag_size: 2943\n",
      "batch 39, loss: 0.3699, instance_loss: 0.4383, weighted_loss: 0.3904, label: 0, bag_size: 6118\n",
      "batch 59, loss: 0.1988, instance_loss: 0.2657, weighted_loss: 0.2189, label: 1, bag_size: 1937\n",
      "batch 79, loss: 0.4803, instance_loss: 0.7244, weighted_loss: 0.5535, label: 0, bag_size: 4012\n",
      "batch 99, loss: 0.3101, instance_loss: 0.3459, weighted_loss: 0.3208, label: 0, bag_size: 6994\n",
      "batch 119, loss: 0.1900, instance_loss: 0.2228, weighted_loss: 0.1999, label: 1, bag_size: 3990\n",
      "batch 139, loss: 0.5474, instance_loss: 0.7629, weighted_loss: 0.6121, label: 1, bag_size: 2039\n",
      "batch 159, loss: 0.9215, instance_loss: 1.1975, weighted_loss: 1.0043, label: 0, bag_size: 3921\n",
      "batch 179, loss: 0.5025, instance_loss: 0.6898, weighted_loss: 0.5587, label: 0, bag_size: 5877\n",
      "batch 199, loss: 1.3541, instance_loss: 1.3009, weighted_loss: 1.3381, label: 0, bag_size: 4168\n",
      "batch 219, loss: 0.6008, instance_loss: 0.7642, weighted_loss: 0.6499, label: 1, bag_size: 1746\n",
      "batch 239, loss: 0.2949, instance_loss: 0.2783, weighted_loss: 0.2899, label: 0, bag_size: 6209\n",
      "batch 259, loss: 0.6214, instance_loss: 0.6945, weighted_loss: 0.6433, label: 1, bag_size: 3626\n",
      "batch 279, loss: 0.5788, instance_loss: 0.7288, weighted_loss: 0.6238, label: 0, bag_size: 3845\n",
      "batch 299, loss: 0.4137, instance_loss: 0.4778, weighted_loss: 0.4329, label: 0, bag_size: 17546\n",
      "batch 319, loss: 0.5668, instance_loss: 0.7935, weighted_loss: 0.6348, label: 1, bag_size: 7717\n",
      "batch 339, loss: 0.9535, instance_loss: 1.0783, weighted_loss: 0.9910, label: 1, bag_size: 4761\n",
      "batch 359, loss: 1.3200, instance_loss: 1.3096, weighted_loss: 1.3169, label: 0, bag_size: 5362\n",
      "batch 379, loss: 0.9527, instance_loss: 1.2001, weighted_loss: 1.0269, label: 0, bag_size: 2923\n",
      "batch 399, loss: 0.3667, instance_loss: 0.3343, weighted_loss: 0.3570, label: 1, bag_size: 4537\n",
      "batch 419, loss: 0.3786, instance_loss: 0.4880, weighted_loss: 0.4114, label: 0, bag_size: 10588\n",
      "batch 439, loss: 0.5867, instance_loss: 0.7281, weighted_loss: 0.6291, label: 1, bag_size: 5065\n",
      "batch 459, loss: 0.9313, instance_loss: 1.3524, weighted_loss: 1.0576, label: 0, bag_size: 3843\n",
      "batch 479, loss: 0.5601, instance_loss: 0.6563, weighted_loss: 0.5890, label: 0, bag_size: 4084\n",
      "batch 499, loss: 0.5413, instance_loss: 0.6755, weighted_loss: 0.5816, label: 1, bag_size: 4803\n",
      "batch 519, loss: 0.5128, instance_loss: 0.7456, weighted_loss: 0.5826, label: 1, bag_size: 5072\n",
      "batch 539, loss: 0.7968, instance_loss: 0.9185, weighted_loss: 0.8333, label: 1, bag_size: 20256\n",
      "batch 559, loss: 0.7549, instance_loss: 0.8121, weighted_loss: 0.7721, label: 0, bag_size: 9499\n",
      "batch 579, loss: 0.4795, instance_loss: 0.6944, weighted_loss: 0.5440, label: 0, bag_size: 3070\n",
      "batch 599, loss: 1.0896, instance_loss: 1.6641, weighted_loss: 1.2620, label: 1, bag_size: 3484\n",
      "batch 619, loss: 0.8274, instance_loss: 1.1658, weighted_loss: 0.9290, label: 1, bag_size: 3159\n",
      "batch 639, loss: 0.8801, instance_loss: 1.1083, weighted_loss: 0.9485, label: 0, bag_size: 5093\n",
      "batch 659, loss: 0.2422, instance_loss: 0.5170, weighted_loss: 0.3246, label: 0, bag_size: 3463\n",
      "batch 679, loss: 0.3674, instance_loss: 0.2501, weighted_loss: 0.3322, label: 0, bag_size: 3732\n",
      "batch 699, loss: 0.9263, instance_loss: 1.1877, weighted_loss: 1.0047, label: 1, bag_size: 4387\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9543269230769231: correct 10719/11232\n",
      "class 1 clustering acc 0.22257834757834757: correct 1250/5616\n",
      "Epoch: 21, train_loss: 0.6600, train_clustering_loss:  0.8443, train_error: 0.4046\n",
      "class 0: acc 0.6138328530259366, correct 213/347\n",
      "class 1: acc 0.5774647887323944, correct 205/355\n",
      "\n",
      "Val Set, val_loss: 0.6663, val_error: 0.4045, auc: 0.6847\n",
      "class 0 clustering acc 0.9915730337078652: correct 1412/1424\n",
      "class 1 clustering acc 0.06741573033707865: correct 48/712\n",
      "class 0: acc 0.5, correct 27/54\n",
      "class 1: acc 0.7428571428571429, correct 26/35\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6482, instance_loss: 0.7766, weighted_loss: 0.6867, label: 1, bag_size: 1525\n",
      "batch 39, loss: 0.2947, instance_loss: 0.3760, weighted_loss: 0.3191, label: 0, bag_size: 14142\n",
      "batch 59, loss: 1.0015, instance_loss: 1.0747, weighted_loss: 1.0235, label: 0, bag_size: 4066\n",
      "batch 79, loss: 0.7826, instance_loss: 1.0440, weighted_loss: 0.8610, label: 0, bag_size: 5360\n",
      "batch 99, loss: 0.4332, instance_loss: 0.5256, weighted_loss: 0.4610, label: 1, bag_size: 4803\n",
      "batch 119, loss: 0.5444, instance_loss: 0.7635, weighted_loss: 0.6101, label: 1, bag_size: 4332\n",
      "batch 139, loss: 0.3018, instance_loss: 0.3312, weighted_loss: 0.3106, label: 1, bag_size: 15434\n",
      "batch 159, loss: 0.5892, instance_loss: 0.4990, weighted_loss: 0.5621, label: 1, bag_size: 7641\n",
      "batch 179, loss: 0.4619, instance_loss: 0.5869, weighted_loss: 0.4994, label: 0, bag_size: 15588\n",
      "batch 199, loss: 0.5088, instance_loss: 0.5963, weighted_loss: 0.5351, label: 0, bag_size: 6994\n",
      "batch 219, loss: 0.4927, instance_loss: 0.5273, weighted_loss: 0.5031, label: 1, bag_size: 6235\n",
      "batch 239, loss: 0.6046, instance_loss: 0.7129, weighted_loss: 0.6371, label: 0, bag_size: 13846\n",
      "batch 259, loss: 0.6861, instance_loss: 1.0281, weighted_loss: 0.7887, label: 0, bag_size: 3161\n",
      "batch 279, loss: 0.0763, instance_loss: 0.1240, weighted_loss: 0.0906, label: 1, bag_size: 5651\n",
      "batch 299, loss: 0.1466, instance_loss: 0.2941, weighted_loss: 0.1909, label: 1, bag_size: 4970\n",
      "batch 319, loss: 1.3591, instance_loss: 1.7457, weighted_loss: 1.4751, label: 0, bag_size: 2167\n",
      "batch 339, loss: 0.5315, instance_loss: 0.5118, weighted_loss: 0.5256, label: 0, bag_size: 5080\n",
      "batch 359, loss: 0.2998, instance_loss: 0.1834, weighted_loss: 0.2649, label: 0, bag_size: 15806\n",
      "batch 379, loss: 0.4153, instance_loss: 0.5094, weighted_loss: 0.4435, label: 1, bag_size: 5570\n",
      "batch 399, loss: 0.5739, instance_loss: 0.8741, weighted_loss: 0.6639, label: 1, bag_size: 5677\n",
      "batch 419, loss: 0.9137, instance_loss: 1.3577, weighted_loss: 1.0469, label: 0, bag_size: 1619\n",
      "batch 439, loss: 0.0857, instance_loss: 0.1783, weighted_loss: 0.1135, label: 0, bag_size: 3307\n",
      "batch 459, loss: 0.4775, instance_loss: 0.6398, weighted_loss: 0.5262, label: 1, bag_size: 8331\n",
      "batch 479, loss: 0.3232, instance_loss: 0.5287, weighted_loss: 0.3848, label: 1, bag_size: 3405\n",
      "batch 499, loss: 0.4606, instance_loss: 0.7071, weighted_loss: 0.5345, label: 1, bag_size: 4162\n",
      "batch 519, loss: 0.6132, instance_loss: 0.8315, weighted_loss: 0.6787, label: 1, bag_size: 5062\n",
      "batch 539, loss: 0.4380, instance_loss: 0.6268, weighted_loss: 0.4946, label: 0, bag_size: 9367\n",
      "batch 559, loss: 0.3918, instance_loss: 0.4427, weighted_loss: 0.4071, label: 1, bag_size: 3557\n",
      "batch 579, loss: 0.2547, instance_loss: 0.4320, weighted_loss: 0.3079, label: 1, bag_size: 4268\n",
      "batch 599, loss: 0.3965, instance_loss: 0.6697, weighted_loss: 0.4785, label: 0, bag_size: 3724\n",
      "batch 619, loss: 1.0901, instance_loss: 1.3042, weighted_loss: 1.1543, label: 0, bag_size: 4012\n",
      "batch 639, loss: 0.1559, instance_loss: 0.1387, weighted_loss: 0.1508, label: 1, bag_size: 5428\n",
      "batch 659, loss: 0.7865, instance_loss: 0.9043, weighted_loss: 0.8218, label: 0, bag_size: 2963\n",
      "batch 679, loss: 1.0918, instance_loss: 1.5382, weighted_loss: 1.2257, label: 1, bag_size: 3020\n",
      "batch 699, loss: 0.7544, instance_loss: 1.1044, weighted_loss: 0.8594, label: 0, bag_size: 20045\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9529914529914529: correct 10704/11232\n",
      "class 1 clustering acc 0.2535612535612536: correct 1424/5616\n",
      "Epoch: 22, train_loss: 0.6405, train_clustering_loss:  0.8182, train_error: 0.3732\n",
      "class 0: acc 0.5880597014925373, correct 197/335\n",
      "class 1: acc 0.662125340599455, correct 243/367\n",
      "\n",
      "Val Set, val_loss: 0.6391, val_error: 0.3708, auc: 0.6905\n",
      "class 0 clustering acc 0.9662921348314607: correct 1376/1424\n",
      "class 1 clustering acc 0.1404494382022472: correct 100/712\n",
      "class 0: acc 0.6111111111111112, correct 33/54\n",
      "class 1: acc 0.6571428571428571, correct 23/35\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7389, instance_loss: 0.8534, weighted_loss: 0.7733, label: 0, bag_size: 4572\n",
      "batch 39, loss: 1.7291, instance_loss: 2.5023, weighted_loss: 1.9610, label: 0, bag_size: 4661\n",
      "batch 59, loss: 0.6362, instance_loss: 0.8819, weighted_loss: 0.7099, label: 0, bag_size: 8384\n",
      "batch 79, loss: 0.3971, instance_loss: 0.4286, weighted_loss: 0.4066, label: 0, bag_size: 5982\n",
      "batch 99, loss: 0.6829, instance_loss: 0.8593, weighted_loss: 0.7358, label: 1, bag_size: 6884\n",
      "batch 119, loss: 1.1250, instance_loss: 1.4564, weighted_loss: 1.2244, label: 1, bag_size: 19946\n",
      "batch 139, loss: 0.3128, instance_loss: 0.3411, weighted_loss: 0.3213, label: 0, bag_size: 4228\n",
      "batch 159, loss: 0.2973, instance_loss: 0.4237, weighted_loss: 0.3353, label: 1, bag_size: 1937\n",
      "batch 179, loss: 1.5361, instance_loss: 1.9649, weighted_loss: 1.6648, label: 0, bag_size: 4390\n",
      "batch 199, loss: 1.0706, instance_loss: 1.3306, weighted_loss: 1.1486, label: 1, bag_size: 2669\n",
      "batch 219, loss: 0.7169, instance_loss: 1.1024, weighted_loss: 0.8325, label: 1, bag_size: 2039\n",
      "batch 239, loss: 1.2119, instance_loss: 1.4312, weighted_loss: 1.2777, label: 1, bag_size: 2646\n",
      "batch 259, loss: 0.4481, instance_loss: 0.5791, weighted_loss: 0.4874, label: 0, bag_size: 5458\n",
      "batch 279, loss: 0.5827, instance_loss: 0.8041, weighted_loss: 0.6491, label: 1, bag_size: 4791\n",
      "batch 299, loss: 0.5854, instance_loss: 0.6838, weighted_loss: 0.6149, label: 0, bag_size: 3066\n",
      "batch 319, loss: 0.8058, instance_loss: 1.0643, weighted_loss: 0.8833, label: 0, bag_size: 3548\n",
      "batch 339, loss: 0.1900, instance_loss: 0.2011, weighted_loss: 0.1933, label: 1, bag_size: 2943\n",
      "batch 359, loss: 1.0734, instance_loss: 1.7762, weighted_loss: 1.2843, label: 0, bag_size: 1652\n",
      "batch 379, loss: 0.5657, instance_loss: 0.7712, weighted_loss: 0.6274, label: 0, bag_size: 4558\n",
      "batch 399, loss: 0.9000, instance_loss: 0.9343, weighted_loss: 0.9103, label: 0, bag_size: 1095\n",
      "batch 419, loss: 0.2727, instance_loss: 0.4128, weighted_loss: 0.3147, label: 1, bag_size: 3287\n",
      "batch 439, loss: 0.6462, instance_loss: 0.9102, weighted_loss: 0.7254, label: 1, bag_size: 13685\n",
      "batch 459, loss: 0.4820, instance_loss: 0.5556, weighted_loss: 0.5040, label: 1, bag_size: 6371\n",
      "batch 479, loss: 0.6558, instance_loss: 0.8422, weighted_loss: 0.7118, label: 0, bag_size: 15706\n",
      "batch 499, loss: 0.4850, instance_loss: 0.6348, weighted_loss: 0.5299, label: 1, bag_size: 2824\n",
      "batch 519, loss: 0.2841, instance_loss: 0.3097, weighted_loss: 0.2918, label: 1, bag_size: 4094\n",
      "batch 539, loss: 0.7312, instance_loss: 0.9910, weighted_loss: 0.8092, label: 1, bag_size: 6477\n",
      "batch 559, loss: 0.4681, instance_loss: 0.6225, weighted_loss: 0.5144, label: 1, bag_size: 27072\n",
      "batch 579, loss: 0.2147, instance_loss: 0.3559, weighted_loss: 0.2571, label: 0, bag_size: 14142\n",
      "batch 599, loss: 0.9494, instance_loss: 1.1122, weighted_loss: 0.9983, label: 1, bag_size: 2877\n",
      "batch 619, loss: 0.8860, instance_loss: 1.0185, weighted_loss: 0.9258, label: 0, bag_size: 2947\n",
      "batch 639, loss: 0.0148, instance_loss: 0.1966, weighted_loss: 0.0694, label: 1, bag_size: 3905\n",
      "batch 659, loss: 0.4533, instance_loss: 0.5246, weighted_loss: 0.4747, label: 0, bag_size: 14194\n",
      "batch 679, loss: 1.7493, instance_loss: 2.4626, weighted_loss: 1.9633, label: 0, bag_size: 7978\n",
      "batch 699, loss: 0.3557, instance_loss: 0.5102, weighted_loss: 0.4020, label: 0, bag_size: 5021\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9545049857549858: correct 10721/11232\n",
      "class 1 clustering acc 0.2336182336182336: correct 1312/5616\n",
      "Epoch: 23, train_loss: 0.6502, train_clustering_loss:  0.8202, train_error: 0.3775\n",
      "class 0: acc 0.6119402985074627, correct 205/335\n",
      "class 1: acc 0.6321525885558583, correct 232/367\n",
      "\n",
      "Val Set, val_loss: 0.5870, val_error: 0.3371, auc: 0.7815\n",
      "class 0 clustering acc 0.9747191011235955: correct 1388/1424\n",
      "class 1 clustering acc 0.28651685393258425: correct 204/712\n",
      "class 0: acc 0.9444444444444444, correct 51/54\n",
      "class 1: acc 0.22857142857142856, correct 8/35\n",
      "Validation loss decreased (0.609920 --> 0.587038).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7223, instance_loss: 1.1623, weighted_loss: 0.8543, label: 1, bag_size: 2081\n",
      "batch 39, loss: 0.4184, instance_loss: 0.6009, weighted_loss: 0.4732, label: 1, bag_size: 1746\n",
      "batch 59, loss: 0.9310, instance_loss: 1.3248, weighted_loss: 1.0491, label: 0, bag_size: 3484\n",
      "batch 79, loss: 0.4353, instance_loss: 0.4441, weighted_loss: 0.4380, label: 0, bag_size: 24280\n",
      "batch 99, loss: 0.4957, instance_loss: 0.6200, weighted_loss: 0.5330, label: 0, bag_size: 3099\n",
      "batch 119, loss: 0.3419, instance_loss: 0.4980, weighted_loss: 0.3887, label: 0, bag_size: 17711\n",
      "batch 139, loss: 0.5282, instance_loss: 0.7555, weighted_loss: 0.5964, label: 1, bag_size: 5801\n",
      "batch 159, loss: 0.5394, instance_loss: 0.6841, weighted_loss: 0.5828, label: 1, bag_size: 13226\n",
      "batch 179, loss: 0.2025, instance_loss: 0.4180, weighted_loss: 0.2671, label: 1, bag_size: 2646\n",
      "batch 199, loss: 0.3516, instance_loss: 0.4026, weighted_loss: 0.3669, label: 0, bag_size: 3489\n",
      "batch 219, loss: 0.8374, instance_loss: 1.1460, weighted_loss: 0.9300, label: 1, bag_size: 15118\n",
      "batch 239, loss: 0.1695, instance_loss: 0.1353, weighted_loss: 0.1593, label: 1, bag_size: 20256\n",
      "batch 259, loss: 0.9493, instance_loss: 1.2444, weighted_loss: 1.0378, label: 0, bag_size: 18810\n",
      "batch 279, loss: 0.8061, instance_loss: 1.0916, weighted_loss: 0.8917, label: 1, bag_size: 2250\n",
      "batch 299, loss: 0.0802, instance_loss: 0.0580, weighted_loss: 0.0735, label: 1, bag_size: 5507\n",
      "batch 319, loss: 1.1209, instance_loss: 1.5900, weighted_loss: 1.2616, label: 1, bag_size: 3571\n",
      "batch 339, loss: 0.2571, instance_loss: 0.3004, weighted_loss: 0.2701, label: 1, bag_size: 4044\n",
      "batch 359, loss: 0.7451, instance_loss: 1.0690, weighted_loss: 0.8422, label: 1, bag_size: 4791\n",
      "batch 379, loss: 0.2440, instance_loss: 0.3511, weighted_loss: 0.2761, label: 0, bag_size: 14291\n",
      "batch 399, loss: 1.0223, instance_loss: 1.3529, weighted_loss: 1.1215, label: 0, bag_size: 4079\n",
      "batch 419, loss: 0.4697, instance_loss: 0.4842, weighted_loss: 0.4740, label: 1, bag_size: 3391\n",
      "batch 439, loss: 1.1270, instance_loss: 1.4218, weighted_loss: 1.2155, label: 0, bag_size: 7862\n",
      "batch 459, loss: 2.7077, instance_loss: 3.3152, weighted_loss: 2.8899, label: 0, bag_size: 4282\n",
      "batch 479, loss: 1.5786, instance_loss: 1.9321, weighted_loss: 1.6846, label: 1, bag_size: 13217\n",
      "batch 499, loss: 0.4827, instance_loss: 0.5847, weighted_loss: 0.5133, label: 1, bag_size: 6861\n",
      "batch 519, loss: 0.5550, instance_loss: 0.7158, weighted_loss: 0.6032, label: 1, bag_size: 19013\n",
      "batch 539, loss: 0.4483, instance_loss: 0.6811, weighted_loss: 0.5181, label: 0, bag_size: 14662\n",
      "batch 559, loss: 1.0849, instance_loss: 1.3106, weighted_loss: 1.1526, label: 1, bag_size: 2669\n",
      "batch 579, loss: 0.7582, instance_loss: 1.1457, weighted_loss: 0.8745, label: 0, bag_size: 23860\n",
      "batch 599, loss: 0.5641, instance_loss: 0.7024, weighted_loss: 0.6056, label: 0, bag_size: 2582\n",
      "batch 619, loss: 0.6321, instance_loss: 0.7524, weighted_loss: 0.6682, label: 0, bag_size: 4226\n",
      "batch 639, loss: 0.3431, instance_loss: 0.4032, weighted_loss: 0.3612, label: 0, bag_size: 4332\n",
      "batch 659, loss: 1.1442, instance_loss: 1.5129, weighted_loss: 1.2548, label: 1, bag_size: 7641\n",
      "batch 679, loss: 1.6173, instance_loss: 2.0999, weighted_loss: 1.7621, label: 1, bag_size: 641\n",
      "batch 699, loss: 1.1656, instance_loss: 1.5475, weighted_loss: 1.2802, label: 0, bag_size: 7251\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.947738603988604: correct 10645/11232\n",
      "class 1 clustering acc 0.32603276353276356: correct 1831/5616\n",
      "Epoch: 24, train_loss: 0.6191, train_clustering_loss:  0.7733, train_error: 0.3476\n",
      "class 0: acc 0.6646884272997032, correct 224/337\n",
      "class 1: acc 0.6410958904109589, correct 234/365\n",
      "\n",
      "Val Set, val_loss: 0.6499, val_error: 0.3820, auc: 0.7180\n",
      "class 0 clustering acc 0.9599719101123596: correct 1367/1424\n",
      "class 1 clustering acc 0.19803370786516855: correct 141/712\n",
      "class 0: acc 0.5185185185185185, correct 28/54\n",
      "class 1: acc 0.7714285714285715, correct 27/35\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3492, instance_loss: 0.3698, weighted_loss: 0.3554, label: 1, bag_size: 6151\n",
      "batch 39, loss: 0.4181, instance_loss: 0.6303, weighted_loss: 0.4818, label: 1, bag_size: 8007\n",
      "batch 59, loss: 0.0807, instance_loss: 0.1238, weighted_loss: 0.0936, label: 1, bag_size: 5887\n",
      "batch 79, loss: 0.1182, instance_loss: 0.1126, weighted_loss: 0.1165, label: 1, bag_size: 4249\n",
      "batch 99, loss: 0.2932, instance_loss: 0.3238, weighted_loss: 0.3024, label: 0, bag_size: 3724\n",
      "batch 119, loss: 0.4606, instance_loss: 0.6469, weighted_loss: 0.5165, label: 1, bag_size: 11657\n",
      "batch 139, loss: 0.3527, instance_loss: 0.5925, weighted_loss: 0.4246, label: 0, bag_size: 11451\n",
      "batch 159, loss: 0.6023, instance_loss: 0.6969, weighted_loss: 0.6307, label: 0, bag_size: 11018\n",
      "batch 179, loss: 0.2949, instance_loss: 0.4241, weighted_loss: 0.3336, label: 0, bag_size: 15474\n",
      "batch 199, loss: 0.5681, instance_loss: 0.7439, weighted_loss: 0.6209, label: 1, bag_size: 5072\n",
      "batch 219, loss: 0.5390, instance_loss: 0.6989, weighted_loss: 0.5870, label: 0, bag_size: 3714\n",
      "batch 239, loss: 0.4485, instance_loss: 0.5568, weighted_loss: 0.4810, label: 1, bag_size: 16427\n",
      "batch 259, loss: 0.4273, instance_loss: 0.4907, weighted_loss: 0.4463, label: 1, bag_size: 3253\n",
      "batch 279, loss: 0.1736, instance_loss: 0.1722, weighted_loss: 0.1732, label: 1, bag_size: 2267\n",
      "batch 299, loss: 0.4842, instance_loss: 0.6934, weighted_loss: 0.5470, label: 0, bag_size: 6908\n",
      "batch 319, loss: 0.5135, instance_loss: 0.6196, weighted_loss: 0.5453, label: 0, bag_size: 15682\n",
      "batch 339, loss: 0.1549, instance_loss: 0.2293, weighted_loss: 0.1772, label: 0, bag_size: 1379\n",
      "batch 359, loss: 0.3406, instance_loss: 0.3378, weighted_loss: 0.3397, label: 0, bag_size: 2009\n",
      "batch 379, loss: 0.9891, instance_loss: 1.2726, weighted_loss: 1.0741, label: 1, bag_size: 19173\n",
      "batch 399, loss: 0.1483, instance_loss: 0.1492, weighted_loss: 0.1486, label: 0, bag_size: 19055\n",
      "batch 419, loss: 0.5001, instance_loss: 0.5709, weighted_loss: 0.5214, label: 1, bag_size: 3847\n",
      "batch 439, loss: 0.7474, instance_loss: 0.8592, weighted_loss: 0.7809, label: 0, bag_size: 3950\n",
      "batch 459, loss: 0.3707, instance_loss: 0.3933, weighted_loss: 0.3775, label: 1, bag_size: 23277\n",
      "batch 479, loss: 0.1527, instance_loss: 0.1570, weighted_loss: 0.1540, label: 1, bag_size: 19013\n",
      "batch 499, loss: 0.1886, instance_loss: 0.2417, weighted_loss: 0.2045, label: 1, bag_size: 4815\n",
      "batch 519, loss: 0.7355, instance_loss: 0.8962, weighted_loss: 0.7837, label: 0, bag_size: 30392\n",
      "batch 539, loss: 0.7123, instance_loss: 0.9724, weighted_loss: 0.7903, label: 0, bag_size: 9499\n",
      "batch 559, loss: 0.1795, instance_loss: 0.1619, weighted_loss: 0.1742, label: 0, bag_size: 4979\n",
      "batch 579, loss: 0.4494, instance_loss: 0.5100, weighted_loss: 0.4676, label: 1, bag_size: 5156\n",
      "batch 599, loss: 2.3135, instance_loss: 2.5906, weighted_loss: 2.3966, label: 1, bag_size: 4387\n",
      "batch 619, loss: 0.7364, instance_loss: 0.9669, weighted_loss: 0.8056, label: 1, bag_size: 4268\n",
      "batch 639, loss: 1.4791, instance_loss: 2.1475, weighted_loss: 1.6796, label: 1, bag_size: 3159\n",
      "batch 659, loss: 0.1263, instance_loss: 0.1072, weighted_loss: 0.1206, label: 1, bag_size: 3117\n",
      "batch 679, loss: 0.9396, instance_loss: 1.4180, weighted_loss: 1.0831, label: 0, bag_size: 3661\n",
      "batch 699, loss: 0.4726, instance_loss: 0.4470, weighted_loss: 0.4649, label: 0, bag_size: 2540\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9397257834757835: correct 10555/11232\n",
      "class 1 clustering acc 0.3468660968660969: correct 1948/5616\n",
      "Epoch: 25, train_loss: 0.6148, train_clustering_loss:  0.7704, train_error: 0.3390\n",
      "class 0: acc 0.6764705882352942, correct 230/340\n",
      "class 1: acc 0.6464088397790055, correct 234/362\n",
      "\n",
      "Val Set, val_loss: 0.6481, val_error: 0.3708, auc: 0.7228\n",
      "class 0 clustering acc 0.9382022471910112: correct 1336/1424\n",
      "class 1 clustering acc 0.33286516853932585: correct 237/712\n",
      "class 0: acc 0.5740740740740741, correct 31/54\n",
      "class 1: acc 0.7142857142857143, correct 25/35\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6201, instance_loss: 0.7870, weighted_loss: 0.6702, label: 0, bag_size: 4692\n",
      "batch 39, loss: 1.1956, instance_loss: 1.4040, weighted_loss: 1.2581, label: 1, bag_size: 14487\n",
      "batch 59, loss: 0.7640, instance_loss: 1.1138, weighted_loss: 0.8690, label: 1, bag_size: 6697\n",
      "batch 79, loss: 0.5477, instance_loss: 0.6958, weighted_loss: 0.5921, label: 1, bag_size: 1808\n",
      "batch 99, loss: 0.9761, instance_loss: 1.0566, weighted_loss: 1.0003, label: 1, bag_size: 22171\n",
      "batch 119, loss: 1.0832, instance_loss: 1.2924, weighted_loss: 1.1459, label: 0, bag_size: 2592\n",
      "batch 139, loss: 1.2578, instance_loss: 1.6603, weighted_loss: 1.3786, label: 1, bag_size: 6319\n",
      "batch 159, loss: 0.9801, instance_loss: 1.0756, weighted_loss: 1.0087, label: 0, bag_size: 26830\n",
      "batch 179, loss: 0.4143, instance_loss: 0.4961, weighted_loss: 0.4389, label: 0, bag_size: 4573\n",
      "batch 199, loss: 0.3633, instance_loss: 0.4111, weighted_loss: 0.3776, label: 0, bag_size: 3704\n",
      "batch 219, loss: 0.1419, instance_loss: 0.1221, weighted_loss: 0.1360, label: 1, bag_size: 2407\n",
      "batch 239, loss: 0.8219, instance_loss: 1.0619, weighted_loss: 0.8939, label: 0, bag_size: 4468\n",
      "batch 259, loss: 0.6700, instance_loss: 0.8178, weighted_loss: 0.7143, label: 0, bag_size: 1830\n",
      "batch 279, loss: 0.4556, instance_loss: 0.5518, weighted_loss: 0.4845, label: 1, bag_size: 7641\n",
      "batch 299, loss: 0.4835, instance_loss: 0.6322, weighted_loss: 0.5281, label: 0, bag_size: 11040\n",
      "batch 319, loss: 0.5805, instance_loss: 0.6292, weighted_loss: 0.5951, label: 0, bag_size: 21574\n",
      "batch 339, loss: 0.4362, instance_loss: 0.5974, weighted_loss: 0.4845, label: 0, bag_size: 4066\n",
      "batch 359, loss: 0.2660, instance_loss: 0.3213, weighted_loss: 0.2826, label: 1, bag_size: 6295\n",
      "batch 379, loss: 0.0959, instance_loss: 0.0825, weighted_loss: 0.0919, label: 1, bag_size: 4731\n",
      "batch 399, loss: 0.6517, instance_loss: 0.7657, weighted_loss: 0.6859, label: 0, bag_size: 3535\n",
      "batch 419, loss: 0.1866, instance_loss: 0.2001, weighted_loss: 0.1907, label: 0, bag_size: 2923\n",
      "batch 439, loss: 0.6833, instance_loss: 0.7085, weighted_loss: 0.6908, label: 0, bag_size: 2316\n",
      "batch 459, loss: 0.2997, instance_loss: 0.4219, weighted_loss: 0.3363, label: 0, bag_size: 14142\n",
      "batch 479, loss: 0.6390, instance_loss: 0.8305, weighted_loss: 0.6965, label: 1, bag_size: 3420\n",
      "batch 499, loss: 0.8494, instance_loss: 1.0112, weighted_loss: 0.8979, label: 1, bag_size: 3287\n",
      "batch 519, loss: 0.1030, instance_loss: 0.0941, weighted_loss: 0.1003, label: 1, bag_size: 2005\n",
      "batch 539, loss: 0.2415, instance_loss: 0.3024, weighted_loss: 0.2598, label: 0, bag_size: 3290\n",
      "batch 559, loss: 1.0500, instance_loss: 1.4770, weighted_loss: 1.1781, label: 1, bag_size: 5341\n",
      "batch 579, loss: 1.1160, instance_loss: 1.5057, weighted_loss: 1.2329, label: 0, bag_size: 3661\n",
      "batch 599, loss: 0.1869, instance_loss: 0.2313, weighted_loss: 0.2002, label: 0, bag_size: 16842\n",
      "batch 619, loss: 0.5008, instance_loss: 0.6689, weighted_loss: 0.5513, label: 1, bag_size: 2829\n",
      "batch 639, loss: 0.6436, instance_loss: 0.8730, weighted_loss: 0.7124, label: 0, bag_size: 6908\n",
      "batch 659, loss: 0.7880, instance_loss: 0.9137, weighted_loss: 0.8257, label: 0, bag_size: 4098\n",
      "batch 679, loss: 0.0600, instance_loss: 0.0333, weighted_loss: 0.0520, label: 1, bag_size: 3081\n",
      "batch 699, loss: 0.5317, instance_loss: 0.6845, weighted_loss: 0.5775, label: 0, bag_size: 9499\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9450676638176638: correct 10615/11232\n",
      "class 1 clustering acc 0.25925925925925924: correct 1456/5616\n",
      "Epoch: 26, train_loss: 0.6450, train_clustering_loss:  0.8092, train_error: 0.3689\n",
      "class 0: acc 0.692090395480226, correct 245/354\n",
      "class 1: acc 0.5689655172413793, correct 198/348\n",
      "\n",
      "Val Set, val_loss: 0.6986, val_error: 0.3820, auc: 0.7127\n",
      "class 0 clustering acc 0.8876404494382022: correct 1264/1424\n",
      "class 1 clustering acc 0.3539325842696629: correct 252/712\n",
      "class 0: acc 0.46296296296296297, correct 25/54\n",
      "class 1: acc 0.8571428571428571, correct 30/35\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1741, instance_loss: 0.1902, weighted_loss: 0.1790, label: 1, bag_size: 4069\n",
      "batch 39, loss: 1.9426, instance_loss: 2.3467, weighted_loss: 2.0639, label: 1, bag_size: 1699\n",
      "batch 59, loss: 0.1152, instance_loss: 0.0695, weighted_loss: 0.1015, label: 1, bag_size: 5887\n",
      "batch 79, loss: 0.2306, instance_loss: 0.2752, weighted_loss: 0.2440, label: 0, bag_size: 10590\n",
      "batch 99, loss: 1.1633, instance_loss: 1.5077, weighted_loss: 1.2666, label: 0, bag_size: 5001\n",
      "batch 119, loss: 0.5273, instance_loss: 0.7070, weighted_loss: 0.5812, label: 1, bag_size: 2343\n",
      "batch 139, loss: 0.4458, instance_loss: 0.4965, weighted_loss: 0.4610, label: 1, bag_size: 3962\n",
      "batch 159, loss: 0.7025, instance_loss: 0.8840, weighted_loss: 0.7570, label: 1, bag_size: 4950\n",
      "batch 179, loss: 0.4427, instance_loss: 0.4794, weighted_loss: 0.4537, label: 1, bag_size: 2253\n",
      "batch 199, loss: 0.2635, instance_loss: 0.4001, weighted_loss: 0.3045, label: 1, bag_size: 2877\n",
      "batch 219, loss: 0.4265, instance_loss: 0.5371, weighted_loss: 0.4597, label: 0, bag_size: 575\n",
      "batch 239, loss: 0.4855, instance_loss: 0.6265, weighted_loss: 0.5278, label: 0, bag_size: 5617\n",
      "batch 259, loss: 0.6195, instance_loss: 0.7653, weighted_loss: 0.6632, label: 0, bag_size: 3076\n",
      "batch 279, loss: 0.1894, instance_loss: 0.2754, weighted_loss: 0.2152, label: 0, bag_size: 14635\n",
      "batch 299, loss: 0.2380, instance_loss: 0.1914, weighted_loss: 0.2240, label: 1, bag_size: 4233\n",
      "batch 319, loss: 0.1714, instance_loss: 0.2206, weighted_loss: 0.1861, label: 1, bag_size: 1587\n",
      "batch 339, loss: 1.0336, instance_loss: 1.3467, weighted_loss: 1.1276, label: 1, bag_size: 16451\n",
      "batch 359, loss: 0.4671, instance_loss: 0.5540, weighted_loss: 0.4932, label: 1, bag_size: 3557\n",
      "batch 379, loss: 2.1374, instance_loss: 1.9352, weighted_loss: 2.0767, label: 1, bag_size: 10165\n",
      "batch 399, loss: 0.9061, instance_loss: 1.2570, weighted_loss: 1.0114, label: 0, bag_size: 3489\n",
      "batch 419, loss: 0.7451, instance_loss: 0.9303, weighted_loss: 0.8007, label: 0, bag_size: 5299\n",
      "batch 439, loss: 0.4957, instance_loss: 0.5742, weighted_loss: 0.5192, label: 1, bag_size: 4087\n",
      "batch 459, loss: 0.3900, instance_loss: 0.3900, weighted_loss: 0.3900, label: 0, bag_size: 2908\n",
      "batch 479, loss: 0.9198, instance_loss: 0.9958, weighted_loss: 0.9426, label: 0, bag_size: 3534\n",
      "batch 499, loss: 0.9778, instance_loss: 1.4687, weighted_loss: 1.1251, label: 0, bag_size: 6827\n",
      "batch 519, loss: 1.2152, instance_loss: 1.5981, weighted_loss: 1.3300, label: 0, bag_size: 2167\n",
      "batch 539, loss: 0.7740, instance_loss: 1.0486, weighted_loss: 0.8564, label: 0, bag_size: 3317\n",
      "batch 559, loss: 0.7183, instance_loss: 0.7211, weighted_loss: 0.7191, label: 0, bag_size: 3597\n",
      "batch 579, loss: 0.4412, instance_loss: 0.5002, weighted_loss: 0.4589, label: 1, bag_size: 6138\n",
      "batch 599, loss: 0.2323, instance_loss: 0.3860, weighted_loss: 0.2784, label: 0, bag_size: 22264\n",
      "batch 619, loss: 0.5241, instance_loss: 0.5024, weighted_loss: 0.5176, label: 0, bag_size: 3184\n",
      "batch 639, loss: 0.4876, instance_loss: 0.5939, weighted_loss: 0.5195, label: 0, bag_size: 2947\n",
      "batch 659, loss: 0.4843, instance_loss: 0.5755, weighted_loss: 0.5117, label: 1, bag_size: 2036\n",
      "batch 679, loss: 0.4137, instance_loss: 0.4584, weighted_loss: 0.4271, label: 1, bag_size: 1746\n",
      "batch 699, loss: 2.2162, instance_loss: 2.5406, weighted_loss: 2.3135, label: 0, bag_size: 5100\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9426638176638177: correct 10588/11232\n",
      "class 1 clustering acc 0.2829415954415954: correct 1589/5616\n",
      "Epoch: 27, train_loss: 0.6336, train_clustering_loss:  0.8016, train_error: 0.3547\n",
      "class 0: acc 0.6054216867469879, correct 201/332\n",
      "class 1: acc 0.6810810810810811, correct 252/370\n",
      "\n",
      "Val Set, val_loss: 0.7326, val_error: 0.4494, auc: 0.7106\n",
      "class 0 clustering acc 0.9438202247191011: correct 1344/1424\n",
      "class 1 clustering acc 0.1699438202247191: correct 121/712\n",
      "class 0: acc 0.3148148148148148, correct 17/54\n",
      "class 1: acc 0.9142857142857143, correct 32/35\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7760, instance_loss: 0.9496, weighted_loss: 0.8281, label: 1, bag_size: 4647\n",
      "batch 39, loss: 0.7258, instance_loss: 0.9711, weighted_loss: 0.7994, label: 1, bag_size: 1004\n",
      "batch 59, loss: 0.8637, instance_loss: 1.1976, weighted_loss: 0.9639, label: 0, bag_size: 18807\n",
      "batch 79, loss: 0.8244, instance_loss: 1.1038, weighted_loss: 0.9082, label: 0, bag_size: 21069\n",
      "batch 99, loss: 0.6641, instance_loss: 0.8975, weighted_loss: 0.7341, label: 0, bag_size: 1669\n",
      "batch 119, loss: 0.1908, instance_loss: 0.1619, weighted_loss: 0.1821, label: 0, bag_size: 14212\n",
      "batch 139, loss: 0.3931, instance_loss: 0.5539, weighted_loss: 0.4413, label: 1, bag_size: 5379\n",
      "batch 159, loss: 1.4604, instance_loss: 1.7022, weighted_loss: 1.5329, label: 0, bag_size: 15139\n",
      "batch 179, loss: 0.6469, instance_loss: 0.7158, weighted_loss: 0.6676, label: 0, bag_size: 6909\n",
      "batch 199, loss: 0.7707, instance_loss: 1.8084, weighted_loss: 1.0820, label: 1, bag_size: 11968\n",
      "batch 219, loss: 0.4830, instance_loss: 0.5793, weighted_loss: 0.5119, label: 1, bag_size: 5341\n",
      "batch 239, loss: 1.1108, instance_loss: 1.4638, weighted_loss: 1.2167, label: 1, bag_size: 3454\n",
      "batch 259, loss: 0.5020, instance_loss: 0.6015, weighted_loss: 0.5319, label: 1, bag_size: 4094\n",
      "batch 279, loss: 0.3614, instance_loss: 0.3737, weighted_loss: 0.3651, label: 1, bag_size: 3117\n",
      "batch 299, loss: 1.2312, instance_loss: 1.5135, weighted_loss: 1.3159, label: 1, bag_size: 2465\n",
      "batch 319, loss: 0.7040, instance_loss: 0.8914, weighted_loss: 0.7602, label: 1, bag_size: 2701\n",
      "batch 339, loss: 0.4075, instance_loss: 0.4987, weighted_loss: 0.4349, label: 1, bag_size: 6463\n",
      "batch 359, loss: 0.3970, instance_loss: 0.5072, weighted_loss: 0.4301, label: 1, bag_size: 27072\n",
      "batch 379, loss: 0.7257, instance_loss: 0.9520, weighted_loss: 0.7936, label: 0, bag_size: 3737\n",
      "batch 399, loss: 0.5539, instance_loss: 0.7117, weighted_loss: 0.6012, label: 1, bag_size: 7184\n",
      "batch 419, loss: 1.2497, instance_loss: 1.5840, weighted_loss: 1.3500, label: 1, bag_size: 2207\n",
      "batch 439, loss: 0.7297, instance_loss: 0.8815, weighted_loss: 0.7753, label: 1, bag_size: 4737\n",
      "batch 459, loss: 0.5819, instance_loss: 0.9143, weighted_loss: 0.6816, label: 1, bag_size: 6477\n",
      "batch 479, loss: 0.7489, instance_loss: 0.8329, weighted_loss: 0.7741, label: 1, bag_size: 1825\n",
      "batch 499, loss: 1.3079, instance_loss: 1.7614, weighted_loss: 1.4440, label: 1, bag_size: 1004\n",
      "batch 519, loss: 0.2056, instance_loss: 0.3364, weighted_loss: 0.2448, label: 1, bag_size: 3208\n",
      "batch 539, loss: 0.5836, instance_loss: 0.6879, weighted_loss: 0.6149, label: 0, bag_size: 26830\n",
      "batch 559, loss: 0.1508, instance_loss: 0.2163, weighted_loss: 0.1705, label: 0, bag_size: 11380\n",
      "batch 579, loss: 0.3680, instance_loss: 0.4433, weighted_loss: 0.3905, label: 0, bag_size: 2682\n",
      "batch 599, loss: 0.1392, instance_loss: 0.2101, weighted_loss: 0.1605, label: 1, bag_size: 4800\n",
      "batch 619, loss: 0.2410, instance_loss: 0.3368, weighted_loss: 0.2697, label: 0, bag_size: 10962\n",
      "batch 639, loss: 0.5791, instance_loss: 0.6739, weighted_loss: 0.6076, label: 0, bag_size: 4649\n",
      "batch 659, loss: 0.3617, instance_loss: 0.3261, weighted_loss: 0.3510, label: 0, bag_size: 11299\n",
      "batch 679, loss: 1.2090, instance_loss: 1.6301, weighted_loss: 1.3353, label: 1, bag_size: 1958\n",
      "batch 699, loss: 0.3871, instance_loss: 0.5228, weighted_loss: 0.4278, label: 0, bag_size: 2947\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9461360398860399: correct 10627/11232\n",
      "class 1 clustering acc 0.2841880341880342: correct 1596/5616\n",
      "Epoch: 28, train_loss: 0.6403, train_clustering_loss:  0.8073, train_error: 0.3689\n",
      "class 0: acc 0.6303724928366762, correct 220/349\n",
      "class 1: acc 0.6317280453257791, correct 223/353\n",
      "\n",
      "Val Set, val_loss: 0.5850, val_error: 0.3034, auc: 0.7487\n",
      "class 0 clustering acc 0.9375: correct 1335/1424\n",
      "class 1 clustering acc 0.4550561797752809: correct 324/712\n",
      "class 0: acc 0.9074074074074074, correct 49/54\n",
      "class 1: acc 0.37142857142857144, correct 13/35\n",
      "Validation loss decreased (0.587038 --> 0.584978).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6018, instance_loss: 0.7824, weighted_loss: 0.6560, label: 1, bag_size: 8331\n",
      "batch 39, loss: 0.6217, instance_loss: 0.8833, weighted_loss: 0.7002, label: 1, bag_size: 3191\n",
      "batch 59, loss: 1.5338, instance_loss: 1.8027, weighted_loss: 1.6144, label: 1, bag_size: 13217\n",
      "batch 79, loss: 0.5396, instance_loss: 0.5135, weighted_loss: 0.5317, label: 0, bag_size: 972\n",
      "batch 99, loss: 0.4111, instance_loss: 0.4405, weighted_loss: 0.4199, label: 0, bag_size: 1619\n",
      "batch 119, loss: 0.0946, instance_loss: 0.1014, weighted_loss: 0.0967, label: 1, bag_size: 4572\n",
      "batch 139, loss: 0.0561, instance_loss: 0.0953, weighted_loss: 0.0678, label: 1, bag_size: 5100\n",
      "batch 159, loss: 0.1319, instance_loss: 0.1241, weighted_loss: 0.1296, label: 0, bag_size: 24280\n",
      "batch 179, loss: 0.4328, instance_loss: 0.4928, weighted_loss: 0.4508, label: 1, bag_size: 1251\n",
      "batch 199, loss: 0.7383, instance_loss: 1.0030, weighted_loss: 0.8177, label: 1, bag_size: 3702\n",
      "batch 219, loss: 0.1249, instance_loss: 0.1250, weighted_loss: 0.1249, label: 1, bag_size: 4731\n",
      "batch 239, loss: 0.4778, instance_loss: 0.5697, weighted_loss: 0.5053, label: 0, bag_size: 12861\n",
      "batch 259, loss: 1.1065, instance_loss: 1.4791, weighted_loss: 1.2183, label: 1, bag_size: 12654\n",
      "batch 279, loss: 0.4386, instance_loss: 0.4382, weighted_loss: 0.4385, label: 1, bag_size: 5379\n",
      "batch 299, loss: 1.2265, instance_loss: 1.5466, weighted_loss: 1.3225, label: 1, bag_size: 11555\n",
      "batch 319, loss: 0.8215, instance_loss: 1.0332, weighted_loss: 0.8850, label: 1, bag_size: 4950\n",
      "batch 339, loss: 0.1054, instance_loss: 0.1185, weighted_loss: 0.1093, label: 1, bag_size: 5887\n",
      "batch 359, loss: 0.2754, instance_loss: 0.2830, weighted_loss: 0.2776, label: 0, bag_size: 4654\n",
      "batch 379, loss: 0.6940, instance_loss: 0.8167, weighted_loss: 0.7308, label: 0, bag_size: 20045\n",
      "batch 399, loss: 1.2746, instance_loss: 1.7001, weighted_loss: 1.4023, label: 1, bag_size: 3571\n",
      "batch 419, loss: 0.0605, instance_loss: 0.1636, weighted_loss: 0.0914, label: 0, bag_size: 938\n",
      "batch 439, loss: 0.5530, instance_loss: 0.7790, weighted_loss: 0.6208, label: 1, bag_size: 5341\n",
      "batch 459, loss: 0.6168, instance_loss: 0.8814, weighted_loss: 0.6962, label: 0, bag_size: 4468\n",
      "batch 479, loss: 0.7756, instance_loss: 0.9447, weighted_loss: 0.8263, label: 0, bag_size: 4098\n",
      "batch 499, loss: 0.3149, instance_loss: 0.3750, weighted_loss: 0.3329, label: 1, bag_size: 1503\n",
      "batch 519, loss: 0.3466, instance_loss: 0.3791, weighted_loss: 0.3564, label: 1, bag_size: 2646\n",
      "batch 539, loss: 0.2913, instance_loss: 0.2803, weighted_loss: 0.2880, label: 1, bag_size: 2178\n",
      "batch 559, loss: 0.3822, instance_loss: 0.4607, weighted_loss: 0.4057, label: 0, bag_size: 5724\n",
      "batch 579, loss: 0.9225, instance_loss: 1.2164, weighted_loss: 1.0106, label: 1, bag_size: 3484\n",
      "batch 599, loss: 0.5369, instance_loss: 0.6205, weighted_loss: 0.5620, label: 0, bag_size: 19223\n",
      "batch 619, loss: 0.6404, instance_loss: 0.7376, weighted_loss: 0.6695, label: 1, bag_size: 7184\n",
      "batch 639, loss: 1.8448, instance_loss: 2.4005, weighted_loss: 2.0115, label: 0, bag_size: 4687\n",
      "batch 659, loss: 0.9503, instance_loss: 1.3334, weighted_loss: 1.0652, label: 1, bag_size: 3391\n",
      "batch 679, loss: 0.8518, instance_loss: 1.0069, weighted_loss: 0.8983, label: 0, bag_size: 3076\n",
      "batch 699, loss: 0.3769, instance_loss: 0.3734, weighted_loss: 0.3758, label: 1, bag_size: 6884\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9398148148148148: correct 10556/11232\n",
      "class 1 clustering acc 0.33618233618233617: correct 1888/5616\n",
      "Epoch: 29, train_loss: 0.6225, train_clustering_loss:  0.7802, train_error: 0.3547\n",
      "class 0: acc 0.6938202247191011, correct 247/356\n",
      "class 1: acc 0.5953757225433526, correct 206/346\n",
      "\n",
      "Val Set, val_loss: 0.6017, val_error: 0.3146, auc: 0.7619\n",
      "class 0 clustering acc 0.9662921348314607: correct 1376/1424\n",
      "class 1 clustering acc 0.19803370786516855: correct 141/712\n",
      "class 0: acc 0.6666666666666666, correct 36/54\n",
      "class 1: acc 0.7142857142857143, correct 25/35\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6419, instance_loss: 0.7820, weighted_loss: 0.6839, label: 1, bag_size: 6760\n",
      "batch 39, loss: 1.0392, instance_loss: 1.1672, weighted_loss: 1.0776, label: 0, bag_size: 4254\n",
      "batch 59, loss: 0.0445, instance_loss: 0.0325, weighted_loss: 0.0409, label: 1, bag_size: 5068\n",
      "batch 79, loss: 1.5273, instance_loss: 2.0670, weighted_loss: 1.6892, label: 1, bag_size: 5298\n",
      "batch 99, loss: 0.7029, instance_loss: 0.9230, weighted_loss: 0.7689, label: 1, bag_size: 5671\n",
      "batch 119, loss: 1.3211, instance_loss: 1.6098, weighted_loss: 1.4078, label: 1, bag_size: 6759\n",
      "batch 139, loss: 0.6344, instance_loss: 0.7001, weighted_loss: 0.6541, label: 0, bag_size: 2432\n",
      "batch 159, loss: 0.6336, instance_loss: 0.7674, weighted_loss: 0.6737, label: 0, bag_size: 25641\n",
      "batch 179, loss: 0.7101, instance_loss: 0.9217, weighted_loss: 0.7736, label: 0, bag_size: 1885\n",
      "batch 199, loss: 0.1985, instance_loss: 0.2303, weighted_loss: 0.2080, label: 0, bag_size: 14662\n",
      "batch 219, loss: 1.0605, instance_loss: 1.2161, weighted_loss: 1.1072, label: 0, bag_size: 11299\n",
      "batch 239, loss: 0.4676, instance_loss: 0.5783, weighted_loss: 0.5008, label: 0, bag_size: 10532\n",
      "batch 259, loss: 0.2631, instance_loss: 0.3224, weighted_loss: 0.2809, label: 1, bag_size: 4819\n",
      "batch 279, loss: 0.2335, instance_loss: 0.2728, weighted_loss: 0.2453, label: 0, bag_size: 17273\n",
      "batch 299, loss: 0.5775, instance_loss: 0.5268, weighted_loss: 0.5623, label: 0, bag_size: 3011\n",
      "batch 319, loss: 0.7333, instance_loss: 0.8975, weighted_loss: 0.7826, label: 0, bag_size: 4320\n",
      "batch 339, loss: 0.6263, instance_loss: 0.8033, weighted_loss: 0.6794, label: 0, bag_size: 2923\n",
      "batch 359, loss: 0.5145, instance_loss: 0.5269, weighted_loss: 0.5182, label: 1, bag_size: 5823\n",
      "batch 379, loss: 0.7297, instance_loss: 0.9261, weighted_loss: 0.7886, label: 1, bag_size: 6752\n",
      "batch 399, loss: 0.5680, instance_loss: 0.6719, weighted_loss: 0.5992, label: 1, bag_size: 3910\n",
      "batch 419, loss: 0.5259, instance_loss: 0.6591, weighted_loss: 0.5659, label: 1, bag_size: 15118\n",
      "batch 439, loss: 0.5216, instance_loss: 0.5254, weighted_loss: 0.5227, label: 1, bag_size: 6295\n",
      "batch 459, loss: 0.8074, instance_loss: 1.0362, weighted_loss: 0.8761, label: 1, bag_size: 20435\n",
      "batch 479, loss: 0.3063, instance_loss: 0.4192, weighted_loss: 0.3402, label: 1, bag_size: 2820\n",
      "batch 499, loss: 0.1677, instance_loss: 0.1812, weighted_loss: 0.1717, label: 1, bag_size: 1579\n",
      "batch 519, loss: 0.6260, instance_loss: 0.8774, weighted_loss: 0.7014, label: 0, bag_size: 11256\n",
      "batch 539, loss: 0.5027, instance_loss: 0.5656, weighted_loss: 0.5216, label: 0, bag_size: 4654\n",
      "batch 559, loss: 0.3714, instance_loss: 0.4227, weighted_loss: 0.3868, label: 1, bag_size: 5671\n",
      "batch 579, loss: 0.2415, instance_loss: 0.2720, weighted_loss: 0.2506, label: 1, bag_size: 3391\n",
      "batch 599, loss: 0.7611, instance_loss: 0.9884, weighted_loss: 0.8293, label: 1, bag_size: 2779\n",
      "batch 619, loss: 0.9033, instance_loss: 1.2479, weighted_loss: 1.0067, label: 1, bag_size: 3548\n",
      "batch 639, loss: 0.6450, instance_loss: 0.7605, weighted_loss: 0.6797, label: 1, bag_size: 5379\n",
      "batch 659, loss: 0.4832, instance_loss: 0.5464, weighted_loss: 0.5022, label: 1, bag_size: 2938\n",
      "batch 679, loss: 0.4366, instance_loss: 0.5468, weighted_loss: 0.4697, label: 0, bag_size: 6152\n",
      "batch 699, loss: 1.8689, instance_loss: 2.3373, weighted_loss: 2.0094, label: 1, bag_size: 6697\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9445334757834758: correct 10609/11232\n",
      "class 1 clustering acc 0.2950498575498576: correct 1657/5616\n",
      "Epoch: 30, train_loss: 0.6330, train_clustering_loss:  0.7904, train_error: 0.3490\n",
      "class 0: acc 0.6416184971098265, correct 222/346\n",
      "class 1: acc 0.6601123595505618, correct 235/356\n",
      "\n",
      "Val Set, val_loss: 0.5547, val_error: 0.2809, auc: 0.7894\n",
      "class 0 clustering acc 0.9452247191011236: correct 1346/1424\n",
      "class 1 clustering acc 0.48314606741573035: correct 344/712\n",
      "class 0: acc 0.8888888888888888, correct 48/54\n",
      "class 1: acc 0.45714285714285713, correct 16/35\n",
      "Validation loss decreased (0.584978 --> 0.554661).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.8978, instance_loss: 1.0551, weighted_loss: 0.9450, label: 0, bag_size: 10096\n",
      "batch 39, loss: 0.4999, instance_loss: 0.6680, weighted_loss: 0.5504, label: 0, bag_size: 5642\n",
      "batch 59, loss: 0.4444, instance_loss: 0.5665, weighted_loss: 0.4811, label: 0, bag_size: 4098\n",
      "batch 79, loss: 0.5954, instance_loss: 0.6496, weighted_loss: 0.6116, label: 0, bag_size: 20045\n",
      "batch 99, loss: 0.4135, instance_loss: 0.5133, weighted_loss: 0.4434, label: 0, bag_size: 4649\n",
      "batch 119, loss: 0.4412, instance_loss: 0.5778, weighted_loss: 0.4822, label: 0, bag_size: 8866\n",
      "batch 139, loss: 0.1869, instance_loss: 0.1832, weighted_loss: 0.1858, label: 1, bag_size: 3847\n",
      "batch 159, loss: 0.9857, instance_loss: 1.1026, weighted_loss: 1.0208, label: 1, bag_size: 13226\n",
      "batch 179, loss: 0.2388, instance_loss: 0.2294, weighted_loss: 0.2360, label: 0, bag_size: 1207\n",
      "batch 199, loss: 0.8191, instance_loss: 1.0136, weighted_loss: 0.8774, label: 0, bag_size: 1669\n",
      "batch 219, loss: 0.6378, instance_loss: 0.7370, weighted_loss: 0.6675, label: 0, bag_size: 2529\n",
      "batch 239, loss: 0.2603, instance_loss: 0.2655, weighted_loss: 0.2618, label: 1, bag_size: 4731\n",
      "batch 259, loss: 0.6767, instance_loss: 1.0091, weighted_loss: 0.7764, label: 0, bag_size: 3598\n",
      "batch 279, loss: 0.4246, instance_loss: 0.4216, weighted_loss: 0.4237, label: 0, bag_size: 4303\n",
      "batch 299, loss: 0.8019, instance_loss: 0.9726, weighted_loss: 0.8531, label: 0, bag_size: 3240\n",
      "batch 319, loss: 0.0026, instance_loss: 0.2177, weighted_loss: 0.0671, label: 1, bag_size: 5695\n",
      "batch 339, loss: 0.6986, instance_loss: 0.7918, weighted_loss: 0.7266, label: 0, bag_size: 9499\n",
      "batch 359, loss: 0.5388, instance_loss: 0.8624, weighted_loss: 0.6359, label: 0, bag_size: 3184\n",
      "batch 379, loss: 0.6099, instance_loss: 0.7251, weighted_loss: 0.6445, label: 0, bag_size: 3240\n",
      "batch 399, loss: 0.5027, instance_loss: 0.7845, weighted_loss: 0.5872, label: 1, bag_size: 5379\n",
      "batch 419, loss: 0.5377, instance_loss: 0.8205, weighted_loss: 0.6225, label: 0, bag_size: 2586\n",
      "batch 439, loss: 0.3431, instance_loss: 0.5049, weighted_loss: 0.3916, label: 1, bag_size: 4853\n",
      "batch 459, loss: 0.1077, instance_loss: 0.3391, weighted_loss: 0.1771, label: 0, bag_size: 2682\n",
      "batch 479, loss: 0.2418, instance_loss: 0.3515, weighted_loss: 0.2747, label: 1, bag_size: 3243\n",
      "batch 499, loss: 1.1122, instance_loss: 1.3269, weighted_loss: 1.1766, label: 1, bag_size: 2835\n",
      "batch 519, loss: 0.1231, instance_loss: 0.0813, weighted_loss: 0.1105, label: 1, bag_size: 3578\n",
      "batch 539, loss: 1.5317, instance_loss: 2.0852, weighted_loss: 1.6978, label: 0, bag_size: 2609\n",
      "batch 559, loss: 0.3217, instance_loss: 0.4447, weighted_loss: 0.3586, label: 0, bag_size: 11018\n",
      "batch 579, loss: 0.4105, instance_loss: 0.5233, weighted_loss: 0.4444, label: 0, bag_size: 5360\n",
      "batch 599, loss: 0.8431, instance_loss: 1.1137, weighted_loss: 0.9243, label: 1, bag_size: 20056\n",
      "batch 619, loss: 0.7363, instance_loss: 0.9632, weighted_loss: 0.8044, label: 0, bag_size: 3843\n",
      "batch 639, loss: 0.4840, instance_loss: 0.9242, weighted_loss: 0.6161, label: 0, bag_size: 3535\n",
      "batch 659, loss: 0.8949, instance_loss: 1.1612, weighted_loss: 0.9748, label: 1, bag_size: 16995\n",
      "batch 679, loss: 0.3025, instance_loss: 0.3592, weighted_loss: 0.3195, label: 1, bag_size: 18681\n",
      "batch 699, loss: 0.4248, instance_loss: 0.6268, weighted_loss: 0.4854, label: 0, bag_size: 2529\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9412393162393162: correct 10572/11232\n",
      "class 1 clustering acc 0.318019943019943: correct 1786/5616\n",
      "Epoch: 31, train_loss: 0.6203, train_clustering_loss:  0.7877, train_error: 0.3376\n",
      "class 0: acc 0.6885714285714286, correct 241/350\n",
      "class 1: acc 0.6363636363636364, correct 224/352\n",
      "\n",
      "Val Set, val_loss: 0.5824, val_error: 0.3034, auc: 0.7513\n",
      "class 0 clustering acc 0.9831460674157303: correct 1400/1424\n",
      "class 1 clustering acc 0.16713483146067415: correct 119/712\n",
      "class 0: acc 0.7222222222222222, correct 39/54\n",
      "class 1: acc 0.6571428571428571, correct 23/35\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7458, instance_loss: 0.9600, weighted_loss: 0.8100, label: 1, bag_size: 7184\n",
      "batch 39, loss: 1.1782, instance_loss: 1.5440, weighted_loss: 1.2879, label: 1, bag_size: 2646\n",
      "batch 59, loss: 0.7707, instance_loss: 1.0273, weighted_loss: 0.8477, label: 0, bag_size: 1652\n",
      "batch 79, loss: 0.3512, instance_loss: 0.3304, weighted_loss: 0.3450, label: 0, bag_size: 4253\n",
      "batch 99, loss: 0.6090, instance_loss: 0.6227, weighted_loss: 0.6131, label: 1, bag_size: 4819\n",
      "batch 119, loss: 0.0192, instance_loss: 0.0454, weighted_loss: 0.0270, label: 1, bag_size: 3126\n",
      "batch 139, loss: 0.8401, instance_loss: 1.0338, weighted_loss: 0.8982, label: 1, bag_size: 5677\n",
      "batch 159, loss: 0.2300, instance_loss: 0.2043, weighted_loss: 0.2223, label: 1, bag_size: 5068\n",
      "batch 179, loss: 0.3290, instance_loss: 0.3578, weighted_loss: 0.3377, label: 0, bag_size: 18376\n",
      "batch 199, loss: 0.2030, instance_loss: 0.2392, weighted_loss: 0.2139, label: 0, bag_size: 11797\n",
      "batch 219, loss: 0.0314, instance_loss: 0.0438, weighted_loss: 0.0351, label: 1, bag_size: 4458\n",
      "batch 239, loss: 0.4561, instance_loss: 0.4991, weighted_loss: 0.4690, label: 1, bag_size: 3208\n",
      "batch 259, loss: 1.0204, instance_loss: 1.3130, weighted_loss: 1.1082, label: 0, bag_size: 4064\n",
      "batch 279, loss: 0.3073, instance_loss: 0.3112, weighted_loss: 0.3084, label: 1, bag_size: 3824\n",
      "batch 299, loss: 0.6088, instance_loss: 0.8136, weighted_loss: 0.6702, label: 1, bag_size: 14487\n",
      "batch 319, loss: 0.4583, instance_loss: 0.5627, weighted_loss: 0.4896, label: 0, bag_size: 2586\n",
      "batch 339, loss: 0.8313, instance_loss: 0.9728, weighted_loss: 0.8738, label: 1, bag_size: 2890\n",
      "batch 359, loss: 0.6216, instance_loss: 0.8880, weighted_loss: 0.7015, label: 1, bag_size: 13217\n",
      "batch 379, loss: 0.4227, instance_loss: 0.4891, weighted_loss: 0.4426, label: 0, bag_size: 4435\n",
      "batch 399, loss: 1.3952, instance_loss: 1.8346, weighted_loss: 1.5270, label: 1, bag_size: 5638\n",
      "batch 419, loss: 0.2704, instance_loss: 0.2963, weighted_loss: 0.2782, label: 0, bag_size: 1433\n",
      "batch 439, loss: 0.5788, instance_loss: 0.7592, weighted_loss: 0.6329, label: 0, bag_size: 19535\n",
      "batch 459, loss: 0.4542, instance_loss: 0.5336, weighted_loss: 0.4780, label: 0, bag_size: 1671\n",
      "batch 479, loss: 0.4172, instance_loss: 0.5335, weighted_loss: 0.4521, label: 0, bag_size: 2609\n",
      "batch 499, loss: 0.4122, instance_loss: 0.4732, weighted_loss: 0.4305, label: 1, bag_size: 2081\n",
      "batch 519, loss: 1.3121, instance_loss: 1.7169, weighted_loss: 1.4336, label: 0, bag_size: 2923\n",
      "batch 539, loss: 0.0835, instance_loss: 0.1280, weighted_loss: 0.0969, label: 0, bag_size: 4283\n",
      "batch 559, loss: 0.1268, instance_loss: 0.1397, weighted_loss: 0.1307, label: 1, bag_size: 4094\n",
      "batch 579, loss: 0.2382, instance_loss: 0.2379, weighted_loss: 0.2381, label: 1, bag_size: 6760\n",
      "batch 599, loss: 0.2341, instance_loss: 0.3710, weighted_loss: 0.2751, label: 0, bag_size: 5001\n",
      "batch 619, loss: 0.2780, instance_loss: 0.2217, weighted_loss: 0.2611, label: 0, bag_size: 4086\n",
      "batch 639, loss: 0.1526, instance_loss: 0.2013, weighted_loss: 0.1672, label: 1, bag_size: 3824\n",
      "batch 659, loss: 0.2710, instance_loss: 0.2758, weighted_loss: 0.2725, label: 0, bag_size: 7667\n",
      "batch 679, loss: 1.0345, instance_loss: 1.4160, weighted_loss: 1.1490, label: 1, bag_size: 3191\n",
      "batch 699, loss: 0.6352, instance_loss: 0.8429, weighted_loss: 0.6975, label: 1, bag_size: 13685\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9408831908831908: correct 10568/11232\n",
      "class 1 clustering acc 0.4184472934472934: correct 2350/5616\n",
      "Epoch: 32, train_loss: 0.5836, train_clustering_loss:  0.7229, train_error: 0.2934\n",
      "class 0: acc 0.7541899441340782, correct 270/358\n",
      "class 1: acc 0.6569767441860465, correct 226/344\n",
      "\n",
      "Val Set, val_loss: 0.7367, val_error: 0.3820, auc: 0.7249\n",
      "class 0 clustering acc 0.8981741573033708: correct 1279/1424\n",
      "class 1 clustering acc 0.3455056179775281: correct 246/712\n",
      "class 0: acc 0.4444444444444444, correct 24/54\n",
      "class 1: acc 0.8857142857142857, correct 31/35\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2645, instance_loss: 0.3541, weighted_loss: 0.2914, label: 0, bag_size: 18807\n",
      "batch 39, loss: 0.3864, instance_loss: 0.4810, weighted_loss: 0.4148, label: 1, bag_size: 3925\n",
      "batch 59, loss: 1.1451, instance_loss: 1.4673, weighted_loss: 1.2418, label: 0, bag_size: 4431\n",
      "batch 79, loss: 0.7726, instance_loss: 0.9166, weighted_loss: 0.8158, label: 0, bag_size: 15687\n",
      "batch 99, loss: 0.8822, instance_loss: 1.1857, weighted_loss: 0.9732, label: 0, bag_size: 2747\n",
      "batch 119, loss: 0.6945, instance_loss: 0.8198, weighted_loss: 0.7321, label: 0, bag_size: 2275\n",
      "batch 139, loss: 1.8768, instance_loss: 2.3273, weighted_loss: 2.0119, label: 1, bag_size: 2646\n",
      "batch 159, loss: 1.0710, instance_loss: 1.5679, weighted_loss: 1.2201, label: 1, bag_size: 1958\n",
      "batch 179, loss: 0.9164, instance_loss: 1.3308, weighted_loss: 1.0407, label: 1, bag_size: 2646\n",
      "batch 199, loss: 0.7262, instance_loss: 0.9926, weighted_loss: 0.8061, label: 0, bag_size: 3087\n",
      "batch 219, loss: 0.2042, instance_loss: 0.2804, weighted_loss: 0.2271, label: 1, bag_size: 5570\n",
      "batch 239, loss: 0.4998, instance_loss: 0.5497, weighted_loss: 0.5148, label: 1, bag_size: 5072\n",
      "batch 259, loss: 0.4054, instance_loss: 0.4258, weighted_loss: 0.4115, label: 1, bag_size: 3764\n",
      "batch 279, loss: 0.1752, instance_loss: 0.1585, weighted_loss: 0.1702, label: 1, bag_size: 2176\n",
      "batch 299, loss: 0.4310, instance_loss: 0.5147, weighted_loss: 0.4561, label: 0, bag_size: 20134\n",
      "batch 319, loss: 0.3713, instance_loss: 0.4402, weighted_loss: 0.3920, label: 0, bag_size: 3597\n",
      "batch 339, loss: 1.2623, instance_loss: 1.6253, weighted_loss: 1.3712, label: 0, bag_size: 2918\n",
      "batch 359, loss: 2.1441, instance_loss: 2.6514, weighted_loss: 2.2963, label: 1, bag_size: 2646\n",
      "batch 379, loss: 0.1306, instance_loss: 0.1160, weighted_loss: 0.1262, label: 1, bag_size: 5108\n",
      "batch 399, loss: 0.3245, instance_loss: 0.3699, weighted_loss: 0.3381, label: 0, bag_size: 6076\n",
      "batch 419, loss: 0.9121, instance_loss: 1.1607, weighted_loss: 0.9867, label: 0, bag_size: 5958\n",
      "batch 439, loss: 1.6183, instance_loss: 2.1845, weighted_loss: 1.7882, label: 1, bag_size: 21711\n",
      "batch 459, loss: 1.3352, instance_loss: 1.8210, weighted_loss: 1.4810, label: 0, bag_size: 6078\n",
      "batch 479, loss: 0.2415, instance_loss: 0.2185, weighted_loss: 0.2346, label: 0, bag_size: 7171\n",
      "batch 499, loss: 1.2139, instance_loss: 1.4845, weighted_loss: 1.2951, label: 1, bag_size: 3557\n",
      "batch 519, loss: 0.5804, instance_loss: 0.8195, weighted_loss: 0.6522, label: 1, bag_size: 2783\n",
      "batch 539, loss: 0.6891, instance_loss: 0.7804, weighted_loss: 0.7165, label: 1, bag_size: 6463\n",
      "batch 559, loss: 0.2646, instance_loss: 0.2236, weighted_loss: 0.2523, label: 0, bag_size: 10588\n",
      "batch 579, loss: 0.3821, instance_loss: 0.4098, weighted_loss: 0.3904, label: 0, bag_size: 3857\n",
      "batch 599, loss: 0.3281, instance_loss: 0.3919, weighted_loss: 0.3473, label: 0, bag_size: 15706\n",
      "batch 619, loss: 0.8685, instance_loss: 1.1902, weighted_loss: 0.9650, label: 0, bag_size: 3295\n",
      "batch 639, loss: 1.1651, instance_loss: 1.3693, weighted_loss: 1.2264, label: 1, bag_size: 6319\n",
      "batch 659, loss: 0.5266, instance_loss: 0.6910, weighted_loss: 0.5760, label: 1, bag_size: 4510\n",
      "batch 679, loss: 0.7493, instance_loss: 0.9199, weighted_loss: 0.8005, label: 1, bag_size: 1808\n",
      "batch 699, loss: 0.6257, instance_loss: 0.8135, weighted_loss: 0.6820, label: 1, bag_size: 16675\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9486289173789174: correct 10655/11232\n",
      "class 1 clustering acc 0.3222934472934473: correct 1810/5616\n",
      "Epoch: 33, train_loss: 0.6100, train_clustering_loss:  0.7625, train_error: 0.3504\n",
      "class 0: acc 0.6549707602339181, correct 224/342\n",
      "class 1: acc 0.6444444444444445, correct 232/360\n",
      "\n",
      "Val Set, val_loss: 0.7101, val_error: 0.4045, auc: 0.7381\n",
      "class 0 clustering acc 0.9269662921348315: correct 1320/1424\n",
      "class 1 clustering acc 0.3061797752808989: correct 218/712\n",
      "class 0: acc 0.42592592592592593, correct 23/54\n",
      "class 1: acc 0.8571428571428571, correct 30/35\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6991, instance_loss: 0.9274, weighted_loss: 0.7676, label: 1, bag_size: 2701\n",
      "batch 39, loss: 0.9121, instance_loss: 1.1162, weighted_loss: 0.9733, label: 0, bag_size: 941\n",
      "batch 59, loss: 0.9687, instance_loss: 1.2423, weighted_loss: 1.0508, label: 0, bag_size: 19223\n",
      "batch 79, loss: 0.4394, instance_loss: 0.5042, weighted_loss: 0.4589, label: 1, bag_size: 1552\n",
      "batch 99, loss: 0.5845, instance_loss: 0.7641, weighted_loss: 0.6384, label: 1, bag_size: 5638\n",
      "batch 119, loss: 0.1973, instance_loss: 0.2093, weighted_loss: 0.2009, label: 0, bag_size: 2094\n",
      "batch 139, loss: 0.4467, instance_loss: 0.5381, weighted_loss: 0.4741, label: 0, bag_size: 1468\n",
      "batch 159, loss: 0.2706, instance_loss: 0.2535, weighted_loss: 0.2654, label: 0, bag_size: 3597\n",
      "batch 179, loss: 0.9640, instance_loss: 1.2377, weighted_loss: 1.0461, label: 1, bag_size: 3226\n",
      "batch 199, loss: 3.0997, instance_loss: 3.8836, weighted_loss: 3.3349, label: 0, bag_size: 7227\n",
      "batch 219, loss: 0.1247, instance_loss: 0.1277, weighted_loss: 0.1256, label: 0, bag_size: 1379\n",
      "batch 239, loss: 0.4420, instance_loss: 0.5058, weighted_loss: 0.4611, label: 0, bag_size: 14856\n",
      "batch 259, loss: 0.2781, instance_loss: 0.3247, weighted_loss: 0.2921, label: 1, bag_size: 5817\n",
      "batch 279, loss: 0.3624, instance_loss: 0.3575, weighted_loss: 0.3609, label: 0, bag_size: 4548\n",
      "batch 299, loss: 1.1559, instance_loss: 1.5013, weighted_loss: 1.2595, label: 1, bag_size: 11968\n",
      "batch 319, loss: 1.0825, instance_loss: 1.3834, weighted_loss: 1.1727, label: 0, bag_size: 4649\n",
      "batch 339, loss: 1.1161, instance_loss: 1.5643, weighted_loss: 1.2505, label: 0, bag_size: 7251\n",
      "batch 359, loss: 0.7906, instance_loss: 1.0473, weighted_loss: 0.8677, label: 1, bag_size: 16675\n",
      "batch 379, loss: 0.9336, instance_loss: 1.1796, weighted_loss: 1.0074, label: 1, bag_size: 1700\n",
      "batch 399, loss: 0.3659, instance_loss: 0.4405, weighted_loss: 0.3883, label: 1, bag_size: 3405\n",
      "batch 419, loss: 0.1459, instance_loss: 0.1309, weighted_loss: 0.1414, label: 1, bag_size: 3391\n",
      "batch 439, loss: 0.2502, instance_loss: 0.3642, weighted_loss: 0.2844, label: 0, bag_size: 11040\n",
      "batch 459, loss: 0.3365, instance_loss: 0.3696, weighted_loss: 0.3464, label: 1, bag_size: 16675\n",
      "batch 479, loss: 0.3388, instance_loss: 0.4583, weighted_loss: 0.3746, label: 0, bag_size: 4506\n",
      "batch 499, loss: 0.4258, instance_loss: 0.5591, weighted_loss: 0.4658, label: 0, bag_size: 15747\n",
      "batch 519, loss: 0.3766, instance_loss: 0.4508, weighted_loss: 0.3989, label: 1, bag_size: 5817\n",
      "batch 539, loss: 1.3370, instance_loss: 1.7897, weighted_loss: 1.4728, label: 0, bag_size: 1822\n",
      "batch 559, loss: 0.0476, instance_loss: 0.0405, weighted_loss: 0.0454, label: 1, bag_size: 5199\n",
      "batch 579, loss: 0.4490, instance_loss: 0.5360, weighted_loss: 0.4751, label: 0, bag_size: 3240\n",
      "batch 599, loss: 0.4892, instance_loss: 0.5215, weighted_loss: 0.4989, label: 0, bag_size: 9499\n",
      "batch 619, loss: 0.7654, instance_loss: 1.0245, weighted_loss: 0.8431, label: 1, bag_size: 15118\n",
      "batch 639, loss: 0.4923, instance_loss: 0.6019, weighted_loss: 0.5252, label: 1, bag_size: 16675\n",
      "batch 659, loss: 0.0971, instance_loss: 0.0839, weighted_loss: 0.0932, label: 1, bag_size: 4698\n",
      "batch 679, loss: 0.5512, instance_loss: 0.7144, weighted_loss: 0.6002, label: 1, bag_size: 21399\n",
      "batch 699, loss: 0.7103, instance_loss: 0.8310, weighted_loss: 0.7465, label: 0, bag_size: 3764\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9464031339031339: correct 10630/11232\n",
      "class 1 clustering acc 0.3995726495726496: correct 2244/5616\n",
      "Epoch: 34, train_loss: 0.6026, train_clustering_loss:  0.7491, train_error: 0.3362\n",
      "class 0: acc 0.6204819277108434, correct 206/332\n",
      "class 1: acc 0.7027027027027027, correct 260/370\n",
      "\n",
      "Val Set, val_loss: 0.7901, val_error: 0.5169, auc: 0.7159\n",
      "class 0 clustering acc 0.8686797752808989: correct 1237/1424\n",
      "class 1 clustering acc 0.2893258426966292: correct 206/712\n",
      "class 0: acc 0.18518518518518517, correct 10/54\n",
      "class 1: acc 0.9428571428571428, correct 33/35\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0955, instance_loss: 0.0634, weighted_loss: 0.0858, label: 0, bag_size: 2945\n",
      "batch 39, loss: 0.1178, instance_loss: 0.0825, weighted_loss: 0.1072, label: 1, bag_size: 5810\n",
      "batch 59, loss: 0.2460, instance_loss: 0.3398, weighted_loss: 0.2741, label: 0, bag_size: 2923\n",
      "batch 79, loss: 0.7166, instance_loss: 0.8559, weighted_loss: 0.7584, label: 0, bag_size: 4587\n",
      "batch 99, loss: 0.6050, instance_loss: 0.7424, weighted_loss: 0.6462, label: 1, bag_size: 3184\n",
      "batch 119, loss: 0.2411, instance_loss: 0.2374, weighted_loss: 0.2400, label: 0, bag_size: 2388\n",
      "batch 139, loss: 0.5975, instance_loss: 0.7655, weighted_loss: 0.6479, label: 0, bag_size: 3830\n",
      "batch 159, loss: 0.6864, instance_loss: 0.8325, weighted_loss: 0.7303, label: 0, bag_size: 3540\n",
      "batch 179, loss: 0.4044, instance_loss: 0.4197, weighted_loss: 0.4090, label: 1, bag_size: 3996\n",
      "batch 199, loss: 1.4566, instance_loss: 1.8511, weighted_loss: 1.5750, label: 1, bag_size: 641\n",
      "batch 219, loss: 1.3863, instance_loss: 1.7403, weighted_loss: 1.4925, label: 0, bag_size: 2667\n",
      "batch 239, loss: 0.1293, instance_loss: 0.0955, weighted_loss: 0.1191, label: 1, bag_size: 5199\n",
      "batch 259, loss: 1.2993, instance_loss: 1.7415, weighted_loss: 1.4319, label: 1, bag_size: 20056\n",
      "batch 279, loss: 0.5118, instance_loss: 0.6277, weighted_loss: 0.5465, label: 0, bag_size: 22264\n",
      "batch 299, loss: 0.7713, instance_loss: 1.0520, weighted_loss: 0.8555, label: 1, bag_size: 5341\n",
      "batch 319, loss: 0.7658, instance_loss: 0.9660, weighted_loss: 0.8259, label: 1, bag_size: 11968\n",
      "batch 339, loss: 0.1188, instance_loss: 0.1056, weighted_loss: 0.1149, label: 1, bag_size: 2407\n",
      "batch 359, loss: 0.6007, instance_loss: 0.7122, weighted_loss: 0.6341, label: 0, bag_size: 2089\n",
      "batch 379, loss: 0.6567, instance_loss: 0.8232, weighted_loss: 0.7067, label: 0, bag_size: 5617\n",
      "batch 399, loss: 0.4606, instance_loss: 0.5253, weighted_loss: 0.4800, label: 0, bag_size: 21080\n",
      "batch 419, loss: 0.7577, instance_loss: 0.9612, weighted_loss: 0.8187, label: 0, bag_size: 15706\n",
      "batch 439, loss: 0.2614, instance_loss: 0.2843, weighted_loss: 0.2683, label: 0, bag_size: 13609\n",
      "batch 459, loss: 0.2355, instance_loss: 0.2839, weighted_loss: 0.2500, label: 0, bag_size: 4641\n",
      "batch 479, loss: 0.4686, instance_loss: 0.5142, weighted_loss: 0.4823, label: 0, bag_size: 2394\n",
      "batch 499, loss: 1.7132, instance_loss: 2.1409, weighted_loss: 1.8415, label: 1, bag_size: 5062\n",
      "batch 519, loss: 0.4640, instance_loss: 0.5304, weighted_loss: 0.4839, label: 0, bag_size: 5001\n",
      "batch 539, loss: 1.0871, instance_loss: 1.3241, weighted_loss: 1.1582, label: 1, bag_size: 14306\n",
      "batch 559, loss: 0.2006, instance_loss: 0.2468, weighted_loss: 0.2144, label: 1, bag_size: 3672\n",
      "batch 579, loss: 1.1907, instance_loss: 1.3828, weighted_loss: 1.2484, label: 0, bag_size: 2286\n",
      "batch 599, loss: 0.6810, instance_loss: 0.8090, weighted_loss: 0.7194, label: 1, bag_size: 3170\n",
      "batch 619, loss: 0.6144, instance_loss: 0.9065, weighted_loss: 0.7020, label: 1, bag_size: 5677\n",
      "batch 639, loss: 0.6392, instance_loss: 0.6579, weighted_loss: 0.6448, label: 0, bag_size: 2540\n",
      "batch 659, loss: 0.0405, instance_loss: 0.1077, weighted_loss: 0.0606, label: 0, bag_size: 2721\n",
      "batch 679, loss: 0.7079, instance_loss: 0.8173, weighted_loss: 0.7407, label: 1, bag_size: 4985\n",
      "batch 699, loss: 0.2461, instance_loss: 0.3639, weighted_loss: 0.2815, label: 0, bag_size: 9499\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9463141025641025: correct 10629/11232\n",
      "class 1 clustering acc 0.3507834757834758: correct 1970/5616\n",
      "Epoch: 35, train_loss: 0.6084, train_clustering_loss:  0.7642, train_error: 0.3305\n",
      "class 0: acc 0.7055555555555556, correct 254/360\n",
      "class 1: acc 0.631578947368421, correct 216/342\n",
      "\n",
      "Val Set, val_loss: 0.5483, val_error: 0.2921, auc: 0.7810\n",
      "class 0 clustering acc 0.9515449438202247: correct 1355/1424\n",
      "class 1 clustering acc 0.49719101123595505: correct 354/712\n",
      "class 0: acc 0.8148148148148148, correct 44/54\n",
      "class 1: acc 0.5428571428571428, correct 19/35\n",
      "Validation loss decreased (0.554661 --> 0.548331).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1929, instance_loss: 0.1937, weighted_loss: 0.1932, label: 0, bag_size: 8384\n",
      "batch 39, loss: 0.1819, instance_loss: 0.1603, weighted_loss: 0.1754, label: 0, bag_size: 13609\n",
      "batch 59, loss: 0.9327, instance_loss: 1.4229, weighted_loss: 1.0798, label: 1, bag_size: 3159\n",
      "batch 79, loss: 0.2773, instance_loss: 0.3184, weighted_loss: 0.2896, label: 0, bag_size: 2945\n",
      "batch 99, loss: 0.2522, instance_loss: 0.3002, weighted_loss: 0.2666, label: 1, bag_size: 3391\n",
      "batch 119, loss: 0.4775, instance_loss: 0.5864, weighted_loss: 0.5102, label: 0, bag_size: 3553\n",
      "batch 139, loss: 0.6852, instance_loss: 0.7444, weighted_loss: 0.7030, label: 1, bag_size: 4510\n",
      "batch 159, loss: 0.3378, instance_loss: 0.4516, weighted_loss: 0.3719, label: 0, bag_size: 3240\n",
      "batch 179, loss: 0.4378, instance_loss: 0.6808, weighted_loss: 0.5107, label: 0, bag_size: 972\n",
      "batch 199, loss: 1.2762, instance_loss: 1.4148, weighted_loss: 1.3178, label: 1, bag_size: 2687\n",
      "batch 219, loss: 0.6522, instance_loss: 0.8805, weighted_loss: 0.7207, label: 1, bag_size: 13217\n",
      "batch 239, loss: 0.2245, instance_loss: 0.3069, weighted_loss: 0.2492, label: 0, bag_size: 3845\n",
      "batch 259, loss: 0.3932, instance_loss: 0.4825, weighted_loss: 0.4200, label: 0, bag_size: 3534\n",
      "batch 279, loss: 0.3840, instance_loss: 0.4692, weighted_loss: 0.4096, label: 1, bag_size: 22171\n",
      "batch 299, loss: 0.7464, instance_loss: 0.9836, weighted_loss: 0.8175, label: 1, bag_size: 5341\n",
      "batch 319, loss: 0.1637, instance_loss: 0.1351, weighted_loss: 0.1551, label: 1, bag_size: 5428\n",
      "batch 339, loss: 0.3551, instance_loss: 0.4693, weighted_loss: 0.3894, label: 0, bag_size: 6118\n",
      "batch 359, loss: 0.2599, instance_loss: 0.3186, weighted_loss: 0.2775, label: 0, bag_size: 5455\n",
      "batch 379, loss: 1.0820, instance_loss: 1.3213, weighted_loss: 1.1538, label: 0, bag_size: 6058\n",
      "batch 399, loss: 0.4191, instance_loss: 0.4920, weighted_loss: 0.4409, label: 0, bag_size: 4181\n",
      "batch 419, loss: 0.6762, instance_loss: 1.0054, weighted_loss: 0.7749, label: 0, bag_size: 5093\n",
      "batch 439, loss: 0.7689, instance_loss: 0.9979, weighted_loss: 0.8376, label: 0, bag_size: 4291\n",
      "batch 459, loss: 0.5652, instance_loss: 0.7856, weighted_loss: 0.6313, label: 0, bag_size: 7471\n",
      "batch 479, loss: 0.3417, instance_loss: 0.4202, weighted_loss: 0.3653, label: 1, bag_size: 5458\n",
      "batch 499, loss: 0.3383, instance_loss: 0.3593, weighted_loss: 0.3446, label: 1, bag_size: 7090\n",
      "batch 519, loss: 0.0732, instance_loss: 0.0509, weighted_loss: 0.0665, label: 1, bag_size: 2407\n",
      "batch 539, loss: 0.3750, instance_loss: 0.3767, weighted_loss: 0.3755, label: 0, bag_size: 6994\n",
      "batch 559, loss: 0.3092, instance_loss: 0.3756, weighted_loss: 0.3291, label: 0, bag_size: 2094\n",
      "batch 579, loss: 0.4151, instance_loss: 0.4515, weighted_loss: 0.4260, label: 1, bag_size: 3764\n",
      "batch 599, loss: 0.4388, instance_loss: 0.5816, weighted_loss: 0.4817, label: 0, bag_size: 3597\n",
      "batch 619, loss: 1.0373, instance_loss: 1.2692, weighted_loss: 1.1069, label: 0, bag_size: 2908\n",
      "batch 639, loss: 0.3846, instance_loss: 0.3769, weighted_loss: 0.3823, label: 1, bag_size: 8007\n",
      "batch 659, loss: 0.0295, instance_loss: 0.0252, weighted_loss: 0.0282, label: 1, bag_size: 5507\n",
      "batch 679, loss: 0.7124, instance_loss: 0.9116, weighted_loss: 0.7722, label: 1, bag_size: 3672\n",
      "batch 699, loss: 0.5390, instance_loss: 0.6301, weighted_loss: 0.5663, label: 0, bag_size: 3099\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9401709401709402: correct 10560/11232\n",
      "class 1 clustering acc 0.37197293447293445: correct 2089/5616\n",
      "Epoch: 36, train_loss: 0.6154, train_clustering_loss:  0.7768, train_error: 0.3390\n",
      "class 0: acc 0.6960227272727273, correct 245/352\n",
      "class 1: acc 0.6257142857142857, correct 219/350\n",
      "\n",
      "Val Set, val_loss: 0.8139, val_error: 0.4719, auc: 0.7593\n",
      "class 0 clustering acc 0.8813202247191011: correct 1255/1424\n",
      "class 1 clustering acc 0.324438202247191: correct 231/712\n",
      "class 0: acc 0.25925925925925924, correct 14/54\n",
      "class 1: acc 0.9428571428571428, correct 33/35\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.9334, instance_loss: 1.1896, weighted_loss: 1.0103, label: 0, bag_size: 19223\n",
      "batch 39, loss: 0.5965, instance_loss: 0.7887, weighted_loss: 0.6542, label: 0, bag_size: 15588\n",
      "batch 59, loss: 1.6562, instance_loss: 2.1720, weighted_loss: 1.8109, label: 0, bag_size: 1358\n",
      "batch 79, loss: 0.1326, instance_loss: 0.1109, weighted_loss: 0.1261, label: 0, bag_size: 3070\n",
      "batch 99, loss: 0.4482, instance_loss: 0.5009, weighted_loss: 0.4640, label: 0, bag_size: 3612\n",
      "batch 119, loss: 0.8585, instance_loss: 1.0579, weighted_loss: 0.9183, label: 0, bag_size: 4922\n",
      "batch 139, loss: 0.1701, instance_loss: 0.1613, weighted_loss: 0.1674, label: 0, bag_size: 14291\n",
      "batch 159, loss: 0.9938, instance_loss: 1.3618, weighted_loss: 1.1042, label: 1, bag_size: 6878\n",
      "batch 179, loss: 0.3491, instance_loss: 0.4237, weighted_loss: 0.3715, label: 1, bag_size: 5458\n",
      "batch 199, loss: 0.1275, instance_loss: 0.0878, weighted_loss: 0.1156, label: 1, bag_size: 4458\n",
      "batch 219, loss: 0.1795, instance_loss: 0.2121, weighted_loss: 0.1893, label: 1, bag_size: 10736\n",
      "batch 239, loss: 0.7584, instance_loss: 0.8337, weighted_loss: 0.7810, label: 1, bag_size: 5152\n",
      "batch 259, loss: 0.3706, instance_loss: 0.5101, weighted_loss: 0.4125, label: 1, bag_size: 7717\n",
      "batch 279, loss: 1.5168, instance_loss: 1.8086, weighted_loss: 1.6043, label: 1, bag_size: 11968\n",
      "batch 299, loss: 1.2576, instance_loss: 1.7923, weighted_loss: 1.4180, label: 1, bag_size: 1699\n",
      "batch 319, loss: 0.7173, instance_loss: 0.8731, weighted_loss: 0.7641, label: 0, bag_size: 3392\n",
      "batch 339, loss: 0.1189, instance_loss: 0.1069, weighted_loss: 0.1153, label: 1, bag_size: 4572\n",
      "batch 359, loss: 1.2226, instance_loss: 1.6082, weighted_loss: 1.3383, label: 0, bag_size: 15139\n",
      "batch 379, loss: 0.4361, instance_loss: 0.5461, weighted_loss: 0.4691, label: 1, bag_size: 4087\n",
      "batch 399, loss: 0.6556, instance_loss: 0.8391, weighted_loss: 0.7106, label: 1, bag_size: 5665\n",
      "batch 419, loss: 1.1276, instance_loss: 1.5623, weighted_loss: 1.2580, label: 0, bag_size: 5958\n",
      "batch 439, loss: 0.3244, instance_loss: 0.3492, weighted_loss: 0.3319, label: 0, bag_size: 21574\n",
      "batch 459, loss: 0.3720, instance_loss: 0.4611, weighted_loss: 0.3987, label: 0, bag_size: 1892\n",
      "batch 479, loss: 0.2702, instance_loss: 0.3341, weighted_loss: 0.2894, label: 0, bag_size: 10590\n",
      "batch 499, loss: 0.1419, instance_loss: 0.1124, weighted_loss: 0.1330, label: 0, bag_size: 972\n",
      "batch 519, loss: 0.1961, instance_loss: 0.1988, weighted_loss: 0.1969, label: 0, bag_size: 3099\n",
      "batch 539, loss: 0.8722, instance_loss: 1.3322, weighted_loss: 1.0102, label: 1, bag_size: 5638\n",
      "batch 559, loss: 0.4174, instance_loss: 0.5271, weighted_loss: 0.4503, label: 1, bag_size: 21399\n",
      "batch 579, loss: 0.6485, instance_loss: 0.6973, weighted_loss: 0.6631, label: 0, bag_size: 3504\n",
      "batch 599, loss: 0.5574, instance_loss: 0.7020, weighted_loss: 0.6008, label: 1, bag_size: 4800\n",
      "batch 619, loss: 0.3244, instance_loss: 0.3576, weighted_loss: 0.3344, label: 1, bag_size: 18681\n",
      "batch 639, loss: 0.4117, instance_loss: 0.5033, weighted_loss: 0.4392, label: 1, bag_size: 11657\n",
      "batch 659, loss: 0.2623, instance_loss: 0.3224, weighted_loss: 0.2803, label: 0, bag_size: 2009\n",
      "batch 679, loss: 0.4493, instance_loss: 0.5551, weighted_loss: 0.4810, label: 0, bag_size: 4179\n",
      "batch 699, loss: 0.0998, instance_loss: 0.0931, weighted_loss: 0.0978, label: 1, bag_size: 5100\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9408831908831908: correct 10568/11232\n",
      "class 1 clustering acc 0.36485042735042733: correct 2049/5616\n",
      "Epoch: 37, train_loss: 0.6061, train_clustering_loss:  0.7550, train_error: 0.3276\n",
      "class 0: acc 0.6931818181818182, correct 244/352\n",
      "class 1: acc 0.6514285714285715, correct 228/350\n",
      "\n",
      "Val Set, val_loss: 0.6423, val_error: 0.3146, auc: 0.7661\n",
      "class 0 clustering acc 0.9508426966292135: correct 1354/1424\n",
      "class 1 clustering acc 0.24719101123595505: correct 176/712\n",
      "class 0: acc 0.5555555555555556, correct 30/54\n",
      "class 1: acc 0.8857142857142857, correct 31/35\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4493, instance_loss: 0.6244, weighted_loss: 0.5018, label: 1, bag_size: 6759\n",
      "batch 39, loss: 0.8931, instance_loss: 1.1359, weighted_loss: 0.9660, label: 1, bag_size: 15483\n",
      "batch 59, loss: 0.8298, instance_loss: 1.0778, weighted_loss: 0.9042, label: 0, bag_size: 5590\n",
      "batch 79, loss: 0.5080, instance_loss: 0.5070, weighted_loss: 0.5077, label: 0, bag_size: 3778\n",
      "batch 99, loss: 0.3561, instance_loss: 0.3820, weighted_loss: 0.3639, label: 0, bag_size: 2006\n",
      "batch 119, loss: 0.3029, instance_loss: 0.3691, weighted_loss: 0.3227, label: 0, bag_size: 1207\n",
      "batch 139, loss: 1.1807, instance_loss: 1.6527, weighted_loss: 1.3223, label: 0, bag_size: 1822\n",
      "batch 159, loss: 0.5710, instance_loss: 0.6996, weighted_loss: 0.6096, label: 1, bag_size: 19173\n",
      "batch 179, loss: 0.8699, instance_loss: 1.0322, weighted_loss: 0.9186, label: 0, bag_size: 28144\n",
      "batch 199, loss: 0.7228, instance_loss: 0.8653, weighted_loss: 0.7655, label: 1, bag_size: 2754\n",
      "batch 219, loss: 1.4262, instance_loss: 1.8654, weighted_loss: 1.5580, label: 0, bag_size: 4406\n",
      "batch 239, loss: 1.3152, instance_loss: 1.7706, weighted_loss: 1.4518, label: 0, bag_size: 5590\n",
      "batch 259, loss: 0.7064, instance_loss: 0.9686, weighted_loss: 0.7851, label: 0, bag_size: 4402\n",
      "batch 279, loss: 0.4665, instance_loss: 0.4698, weighted_loss: 0.4675, label: 0, bag_size: 13609\n",
      "batch 299, loss: 0.5414, instance_loss: 0.6289, weighted_loss: 0.5677, label: 0, bag_size: 9003\n",
      "batch 319, loss: 0.5090, instance_loss: 0.6179, weighted_loss: 0.5417, label: 0, bag_size: 4500\n",
      "batch 339, loss: 0.7875, instance_loss: 0.9723, weighted_loss: 0.8429, label: 0, bag_size: 1532\n",
      "batch 359, loss: 0.9403, instance_loss: 1.0573, weighted_loss: 0.9754, label: 0, bag_size: 2762\n",
      "batch 379, loss: 0.5142, instance_loss: 0.6080, weighted_loss: 0.5423, label: 1, bag_size: 3910\n",
      "batch 399, loss: 0.4432, instance_loss: 0.5413, weighted_loss: 0.4726, label: 0, bag_size: 22113\n",
      "batch 419, loss: 0.3295, instance_loss: 0.3424, weighted_loss: 0.3334, label: 0, bag_size: 4098\n",
      "batch 439, loss: 0.1996, instance_loss: 0.1473, weighted_loss: 0.1839, label: 0, bag_size: 16582\n",
      "batch 459, loss: 1.3531, instance_loss: 1.6821, weighted_loss: 1.4518, label: 0, bag_size: 5958\n",
      "batch 479, loss: 0.3279, instance_loss: 0.3519, weighted_loss: 0.3351, label: 0, bag_size: 3936\n",
      "batch 499, loss: 0.8363, instance_loss: 0.9963, weighted_loss: 0.8843, label: 1, bag_size: 14306\n",
      "batch 519, loss: 1.1646, instance_loss: 1.6581, weighted_loss: 1.3127, label: 0, bag_size: 4064\n",
      "batch 539, loss: 0.8461, instance_loss: 1.1232, weighted_loss: 0.9292, label: 0, bag_size: 6463\n",
      "batch 559, loss: 1.1005, instance_loss: 1.5399, weighted_loss: 1.2323, label: 0, bag_size: 3256\n",
      "batch 579, loss: 0.1329, instance_loss: 0.5480, weighted_loss: 0.2574, label: 0, bag_size: 1034\n",
      "batch 599, loss: 0.2607, instance_loss: 0.3829, weighted_loss: 0.2973, label: 0, bag_size: 19612\n",
      "batch 619, loss: 1.1577, instance_loss: 1.3243, weighted_loss: 1.2077, label: 0, bag_size: 2963\n",
      "batch 639, loss: 0.3190, instance_loss: 0.4187, weighted_loss: 0.3489, label: 0, bag_size: 9973\n",
      "batch 659, loss: 0.3639, instance_loss: 0.4525, weighted_loss: 0.3905, label: 1, bag_size: 3626\n",
      "batch 679, loss: 0.0939, instance_loss: 0.0589, weighted_loss: 0.0834, label: 1, bag_size: 5677\n",
      "batch 699, loss: 0.4903, instance_loss: 0.4934, weighted_loss: 0.4912, label: 1, bag_size: 4737\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.938301282051282: correct 10539/11232\n",
      "class 1 clustering acc 0.33956552706552706: correct 1907/5616\n",
      "Epoch: 38, train_loss: 0.6318, train_clustering_loss:  0.7834, train_error: 0.3575\n",
      "class 0: acc 0.6925207756232687, correct 250/361\n",
      "class 1: acc 0.5894428152492669, correct 201/341\n",
      "\n",
      "Val Set, val_loss: 0.5813, val_error: 0.3034, auc: 0.8048\n",
      "class 0 clustering acc 0.9859550561797753: correct 1404/1424\n",
      "class 1 clustering acc 0.175561797752809: correct 125/712\n",
      "class 0: acc 0.6851851851851852, correct 37/54\n",
      "class 1: acc 0.7142857142857143, correct 25/35\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7288, instance_loss: 0.8132, weighted_loss: 0.7541, label: 0, bag_size: 2316\n",
      "batch 39, loss: 0.7642, instance_loss: 0.8751, weighted_loss: 0.7975, label: 1, bag_size: 2506\n",
      "batch 59, loss: 0.4604, instance_loss: 0.5944, weighted_loss: 0.5006, label: 0, bag_size: 5499\n",
      "batch 79, loss: 0.1540, instance_loss: 0.1176, weighted_loss: 0.1431, label: 0, bag_size: 14098\n",
      "batch 99, loss: 0.0960, instance_loss: 0.0930, weighted_loss: 0.0951, label: 1, bag_size: 3081\n",
      "batch 119, loss: 0.2599, instance_loss: 0.1811, weighted_loss: 0.2362, label: 1, bag_size: 5428\n",
      "batch 139, loss: 0.2944, instance_loss: 0.3412, weighted_loss: 0.3084, label: 1, bag_size: 6841\n",
      "batch 159, loss: 0.2625, instance_loss: 0.3225, weighted_loss: 0.2805, label: 0, bag_size: 7171\n",
      "batch 179, loss: 0.8847, instance_loss: 1.1192, weighted_loss: 0.9550, label: 1, bag_size: 1374\n",
      "batch 199, loss: 0.0909, instance_loss: 0.0984, weighted_loss: 0.0931, label: 1, bag_size: 4069\n",
      "batch 219, loss: 0.7555, instance_loss: 1.1820, weighted_loss: 0.8834, label: 0, bag_size: 3307\n",
      "batch 239, loss: 1.0100, instance_loss: 1.3015, weighted_loss: 1.0974, label: 1, bag_size: 12654\n",
      "batch 259, loss: 0.6572, instance_loss: 0.9402, weighted_loss: 0.7421, label: 1, bag_size: 4722\n",
      "batch 279, loss: 0.6562, instance_loss: 0.7050, weighted_loss: 0.6708, label: 1, bag_size: 2938\n",
      "batch 299, loss: 0.6015, instance_loss: 0.7092, weighted_loss: 0.6338, label: 0, bag_size: 7290\n",
      "batch 319, loss: 0.6840, instance_loss: 0.9075, weighted_loss: 0.7510, label: 0, bag_size: 6994\n",
      "batch 339, loss: 0.2892, instance_loss: 0.3693, weighted_loss: 0.3132, label: 0, bag_size: 1207\n",
      "batch 359, loss: 0.1784, instance_loss: 0.1479, weighted_loss: 0.1693, label: 0, bag_size: 20134\n",
      "batch 379, loss: 0.6744, instance_loss: 1.0354, weighted_loss: 0.7827, label: 1, bag_size: 6190\n",
      "batch 399, loss: 1.1580, instance_loss: 1.0354, weighted_loss: 1.1212, label: 1, bag_size: 6878\n",
      "batch 419, loss: 0.4208, instance_loss: 0.6663, weighted_loss: 0.4944, label: 1, bag_size: 16675\n",
      "batch 439, loss: 0.1302, instance_loss: 0.0777, weighted_loss: 0.1145, label: 1, bag_size: 5068\n",
      "batch 459, loss: 0.4152, instance_loss: 0.4955, weighted_loss: 0.4393, label: 1, bag_size: 6463\n",
      "batch 479, loss: 0.5471, instance_loss: 0.5139, weighted_loss: 0.5372, label: 1, bag_size: 1587\n",
      "batch 499, loss: 0.6186, instance_loss: 0.6828, weighted_loss: 0.6378, label: 0, bag_size: 2388\n",
      "batch 519, loss: 0.3520, instance_loss: 0.4884, weighted_loss: 0.3929, label: 0, bag_size: 6777\n",
      "batch 539, loss: 0.2720, instance_loss: 0.2403, weighted_loss: 0.2625, label: 1, bag_size: 5810\n",
      "batch 559, loss: 0.5021, instance_loss: 0.6473, weighted_loss: 0.5457, label: 1, bag_size: 24092\n",
      "batch 579, loss: 0.1035, instance_loss: 0.0888, weighted_loss: 0.0991, label: 1, bag_size: 2005\n",
      "batch 599, loss: 0.1925, instance_loss: 0.1533, weighted_loss: 0.1807, label: 0, bag_size: 15912\n",
      "batch 619, loss: 0.2843, instance_loss: 0.2674, weighted_loss: 0.2792, label: 0, bag_size: 15015\n",
      "batch 639, loss: 0.5693, instance_loss: 0.7411, weighted_loss: 0.6209, label: 0, bag_size: 3943\n",
      "batch 659, loss: 1.6127, instance_loss: 2.1453, weighted_loss: 1.7725, label: 1, bag_size: 5065\n",
      "batch 679, loss: 0.3253, instance_loss: 0.3007, weighted_loss: 0.3179, label: 1, bag_size: 1587\n",
      "batch 699, loss: 0.8948, instance_loss: 1.1174, weighted_loss: 0.9616, label: 0, bag_size: 586\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9396367521367521: correct 10554/11232\n",
      "class 1 clustering acc 0.33938746438746437: correct 1906/5616\n",
      "Epoch: 39, train_loss: 0.6158, train_clustering_loss:  0.7667, train_error: 0.3305\n",
      "class 0: acc 0.6782608695652174, correct 234/345\n",
      "class 1: acc 0.6610644257703081, correct 236/357\n",
      "\n",
      "Val Set, val_loss: 0.6649, val_error: 0.3483, auc: 0.7746\n",
      "class 0 clustering acc 0.9445224719101124: correct 1345/1424\n",
      "class 1 clustering acc 0.20224719101123595: correct 144/712\n",
      "class 0: acc 0.48148148148148145, correct 26/54\n",
      "class 1: acc 0.9142857142857143, correct 32/35\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6428, instance_loss: 0.7718, weighted_loss: 0.6815, label: 1, bag_size: 12654\n",
      "batch 39, loss: 0.1888, instance_loss: 0.2142, weighted_loss: 0.1964, label: 1, bag_size: 4970\n",
      "batch 59, loss: 0.7055, instance_loss: 0.8492, weighted_loss: 0.7486, label: 1, bag_size: 2039\n",
      "batch 79, loss: 0.1375, instance_loss: 0.1155, weighted_loss: 0.1309, label: 1, bag_size: 5428\n",
      "batch 99, loss: 0.2741, instance_loss: 0.3753, weighted_loss: 0.3045, label: 1, bag_size: 2465\n",
      "batch 119, loss: 0.3337, instance_loss: 0.3362, weighted_loss: 0.3345, label: 0, bag_size: 2048\n",
      "batch 139, loss: 0.7773, instance_loss: 0.9962, weighted_loss: 0.8429, label: 0, bag_size: 4172\n",
      "batch 159, loss: 0.5406, instance_loss: 0.6376, weighted_loss: 0.5697, label: 0, bag_size: 2586\n",
      "batch 179, loss: 0.8874, instance_loss: 1.2123, weighted_loss: 0.9849, label: 1, bag_size: 4046\n",
      "batch 199, loss: 0.2689, instance_loss: 0.2868, weighted_loss: 0.2743, label: 0, bag_size: 18807\n",
      "batch 219, loss: 1.1638, instance_loss: 1.7382, weighted_loss: 1.3361, label: 1, bag_size: 14306\n",
      "batch 239, loss: 2.6884, instance_loss: 3.4555, weighted_loss: 2.9185, label: 0, bag_size: 4597\n",
      "batch 259, loss: 0.5468, instance_loss: 0.5418, weighted_loss: 0.5453, label: 1, bag_size: 3184\n",
      "batch 279, loss: 0.2831, instance_loss: 0.3307, weighted_loss: 0.2974, label: 1, bag_size: 21473\n",
      "batch 299, loss: 0.8834, instance_loss: 1.0963, weighted_loss: 0.9473, label: 1, bag_size: 2343\n",
      "batch 319, loss: 0.6758, instance_loss: 0.7546, weighted_loss: 0.6994, label: 1, bag_size: 3170\n",
      "batch 339, loss: 0.6227, instance_loss: 0.7364, weighted_loss: 0.6568, label: 1, bag_size: 16538\n",
      "batch 359, loss: 0.3384, instance_loss: 0.4007, weighted_loss: 0.3571, label: 0, bag_size: 9973\n",
      "batch 379, loss: 0.3081, instance_loss: 0.3835, weighted_loss: 0.3308, label: 0, bag_size: 14194\n",
      "batch 399, loss: 0.6148, instance_loss: 0.8237, weighted_loss: 0.6775, label: 1, bag_size: 2356\n",
      "batch 419, loss: 0.3136, instance_loss: 0.3495, weighted_loss: 0.3243, label: 1, bag_size: 2877\n",
      "batch 439, loss: 1.1194, instance_loss: 1.3388, weighted_loss: 1.1852, label: 1, bag_size: 15483\n",
      "batch 459, loss: 0.2577, instance_loss: 0.1952, weighted_loss: 0.2389, label: 0, bag_size: 6076\n",
      "batch 479, loss: 0.5254, instance_loss: 0.4683, weighted_loss: 0.5083, label: 0, bag_size: 26374\n",
      "batch 499, loss: 0.4754, instance_loss: 0.5339, weighted_loss: 0.4929, label: 1, bag_size: 16538\n",
      "batch 519, loss: 0.1014, instance_loss: 0.0901, weighted_loss: 0.0980, label: 1, bag_size: 2005\n",
      "batch 539, loss: 0.5959, instance_loss: 0.7401, weighted_loss: 0.6391, label: 0, bag_size: 1927\n",
      "batch 559, loss: 1.0960, instance_loss: 1.4278, weighted_loss: 1.1955, label: 1, bag_size: 21711\n",
      "batch 579, loss: 0.1098, instance_loss: 0.0966, weighted_loss: 0.1059, label: 0, bag_size: 2721\n",
      "batch 599, loss: 1.2551, instance_loss: 1.5537, weighted_loss: 1.3447, label: 1, bag_size: 2506\n",
      "batch 619, loss: 0.8958, instance_loss: 1.0032, weighted_loss: 0.9280, label: 0, bag_size: 4180\n",
      "batch 639, loss: 0.3434, instance_loss: 0.3855, weighted_loss: 0.3560, label: 1, bag_size: 1746\n",
      "batch 659, loss: 0.7382, instance_loss: 0.9830, weighted_loss: 0.8116, label: 0, bag_size: 4500\n",
      "batch 679, loss: 0.2205, instance_loss: 0.2365, weighted_loss: 0.2253, label: 1, bag_size: 2669\n",
      "batch 699, loss: 0.5498, instance_loss: 0.6989, weighted_loss: 0.5946, label: 0, bag_size: 3007\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9523682336182336: correct 10697/11232\n",
      "class 1 clustering acc 0.3815883190883191: correct 2143/5616\n",
      "Epoch: 40, train_loss: 0.5799, train_clustering_loss:  0.7154, train_error: 0.3077\n",
      "class 0: acc 0.6615853658536586, correct 217/328\n",
      "class 1: acc 0.7192513368983957, correct 269/374\n",
      "\n",
      "Val Set, val_loss: 0.5879, val_error: 0.3371, auc: 0.7767\n",
      "class 0 clustering acc 0.898876404494382: correct 1280/1424\n",
      "class 1 clustering acc 0.5674157303370787: correct 404/712\n",
      "class 0: acc 0.7592592592592593, correct 41/54\n",
      "class 1: acc 0.5142857142857142, correct 18/35\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0733, instance_loss: 0.0654, weighted_loss: 0.0709, label: 0, bag_size: 2945\n",
      "batch 39, loss: 0.2353, instance_loss: 0.2314, weighted_loss: 0.2341, label: 1, bag_size: 2407\n",
      "batch 59, loss: 1.0854, instance_loss: 1.3074, weighted_loss: 1.1520, label: 0, bag_size: 4338\n",
      "batch 79, loss: 0.3158, instance_loss: 0.4640, weighted_loss: 0.3603, label: 0, bag_size: 5370\n",
      "batch 99, loss: 1.0297, instance_loss: 1.3451, weighted_loss: 1.1243, label: 0, bag_size: 2286\n",
      "batch 119, loss: 0.3697, instance_loss: 0.4505, weighted_loss: 0.3940, label: 0, bag_size: 26830\n",
      "batch 139, loss: 0.1017, instance_loss: 0.1116, weighted_loss: 0.1046, label: 1, bag_size: 3402\n",
      "batch 159, loss: 0.5456, instance_loss: 0.6446, weighted_loss: 0.5753, label: 0, bag_size: 3936\n",
      "batch 179, loss: 0.2884, instance_loss: 0.2749, weighted_loss: 0.2843, label: 0, bag_size: 1619\n",
      "batch 199, loss: 0.8342, instance_loss: 1.0657, weighted_loss: 0.9037, label: 0, bag_size: 6463\n",
      "batch 219, loss: 0.4352, instance_loss: 0.4865, weighted_loss: 0.4506, label: 1, bag_size: 4377\n",
      "batch 239, loss: 0.4762, instance_loss: 0.5591, weighted_loss: 0.5011, label: 0, bag_size: 14212\n",
      "batch 259, loss: 0.1613, instance_loss: 0.1388, weighted_loss: 0.1545, label: 0, bag_size: 5724\n",
      "batch 279, loss: 0.3428, instance_loss: 0.3991, weighted_loss: 0.3597, label: 0, bag_size: 11256\n",
      "batch 299, loss: 0.8491, instance_loss: 1.2391, weighted_loss: 0.9661, label: 1, bag_size: 4384\n",
      "batch 319, loss: 0.4795, instance_loss: 0.4681, weighted_loss: 0.4761, label: 1, bag_size: 2687\n",
      "batch 339, loss: 0.2278, instance_loss: 0.2138, weighted_loss: 0.2236, label: 0, bag_size: 6909\n",
      "batch 359, loss: 0.5243, instance_loss: 0.6651, weighted_loss: 0.5665, label: 1, bag_size: 3990\n",
      "batch 379, loss: 0.4104, instance_loss: 0.5320, weighted_loss: 0.4469, label: 0, bag_size: 2609\n",
      "batch 399, loss: 0.9254, instance_loss: 1.1694, weighted_loss: 0.9986, label: 1, bag_size: 3990\n",
      "batch 419, loss: 1.1467, instance_loss: 1.4666, weighted_loss: 1.2427, label: 0, bag_size: 4687\n",
      "batch 439, loss: 0.2943, instance_loss: 0.2737, weighted_loss: 0.2881, label: 1, bag_size: 2253\n",
      "batch 459, loss: 0.4221, instance_loss: 0.4671, weighted_loss: 0.4356, label: 0, bag_size: 4031\n",
      "batch 479, loss: 1.4279, instance_loss: 1.6773, weighted_loss: 1.5027, label: 1, bag_size: 13217\n",
      "batch 499, loss: 0.3556, instance_loss: 0.4774, weighted_loss: 0.3922, label: 1, bag_size: 2754\n",
      "batch 519, loss: 0.5270, instance_loss: 0.6062, weighted_loss: 0.5507, label: 0, bag_size: 3778\n",
      "batch 539, loss: 0.1155, instance_loss: 0.1216, weighted_loss: 0.1173, label: 1, bag_size: 4069\n",
      "batch 559, loss: 0.1978, instance_loss: 0.2710, weighted_loss: 0.2197, label: 0, bag_size: 2586\n",
      "batch 579, loss: 0.5455, instance_loss: 0.7473, weighted_loss: 0.6061, label: 1, bag_size: 1004\n",
      "batch 599, loss: 0.9550, instance_loss: 1.1305, weighted_loss: 1.0077, label: 1, bag_size: 5385\n",
      "batch 619, loss: 1.6577, instance_loss: 2.3085, weighted_loss: 1.8529, label: 0, bag_size: 2146\n",
      "batch 639, loss: 1.1207, instance_loss: 1.4803, weighted_loss: 1.2285, label: 0, bag_size: 3921\n",
      "batch 659, loss: 0.3742, instance_loss: 0.4383, weighted_loss: 0.3934, label: 1, bag_size: 8331\n",
      "batch 679, loss: 0.6609, instance_loss: 0.8165, weighted_loss: 0.7076, label: 0, bag_size: 2923\n",
      "batch 699, loss: 0.4951, instance_loss: 0.5708, weighted_loss: 0.5178, label: 0, bag_size: 2244\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9403490028490028: correct 10562/11232\n",
      "class 1 clustering acc 0.45975783475783477: correct 2582/5616\n",
      "Epoch: 41, train_loss: 0.5698, train_clustering_loss:  0.7032, train_error: 0.3120\n",
      "class 0: acc 0.7267605633802817, correct 258/355\n",
      "class 1: acc 0.6484149855907781, correct 225/347\n",
      "\n",
      "Val Set, val_loss: 0.5519, val_error: 0.2472, auc: 0.7899\n",
      "class 0 clustering acc 0.949438202247191: correct 1352/1424\n",
      "class 1 clustering acc 0.42134831460674155: correct 300/712\n",
      "class 0: acc 0.7777777777777778, correct 42/54\n",
      "class 1: acc 0.7142857142857143, correct 25/35\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5026, instance_loss: 0.6301, weighted_loss: 0.5408, label: 1, bag_size: 4791\n",
      "batch 39, loss: 0.2972, instance_loss: 0.3245, weighted_loss: 0.3054, label: 0, bag_size: 13846\n",
      "batch 59, loss: 0.1253, instance_loss: 0.1019, weighted_loss: 0.1183, label: 1, bag_size: 5823\n",
      "batch 79, loss: 1.1129, instance_loss: 1.4908, weighted_loss: 1.2263, label: 1, bag_size: 6151\n",
      "batch 99, loss: 1.0895, instance_loss: 1.2711, weighted_loss: 1.1440, label: 1, bag_size: 6138\n",
      "batch 119, loss: 1.4720, instance_loss: 2.1165, weighted_loss: 1.6654, label: 1, bag_size: 18015\n",
      "batch 139, loss: 0.4736, instance_loss: 0.3644, weighted_loss: 0.4409, label: 1, bag_size: 1587\n",
      "batch 159, loss: 1.2633, instance_loss: 1.6831, weighted_loss: 1.3892, label: 1, bag_size: 15118\n",
      "batch 179, loss: 0.6750, instance_loss: 0.8924, weighted_loss: 0.7402, label: 0, bag_size: 4979\n",
      "batch 199, loss: 0.1554, instance_loss: 0.1784, weighted_loss: 0.1623, label: 0, bag_size: 3099\n",
      "batch 219, loss: 1.7406, instance_loss: 2.3937, weighted_loss: 1.9365, label: 0, bag_size: 6047\n",
      "batch 239, loss: 0.2654, instance_loss: 0.3141, weighted_loss: 0.2800, label: 1, bag_size: 6884\n",
      "batch 259, loss: 0.0551, instance_loss: 0.0461, weighted_loss: 0.0524, label: 0, bag_size: 3699\n",
      "batch 279, loss: 0.5303, instance_loss: 0.6192, weighted_loss: 0.5570, label: 1, bag_size: 2356\n",
      "batch 299, loss: 0.4570, instance_loss: 0.5269, weighted_loss: 0.4779, label: 0, bag_size: 5458\n",
      "batch 319, loss: 0.1499, instance_loss: 0.1226, weighted_loss: 0.1417, label: 1, bag_size: 6760\n",
      "batch 339, loss: 0.2889, instance_loss: 0.2480, weighted_loss: 0.2766, label: 1, bag_size: 6410\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python main.py --drop_out --early_stopping --lr 2e-4 --k 5 --label_frac 1 \\\n",
    "--exp_code nlst_100_level0_TransformerMIL_adam_FLASH --weighted_sample --bag_loss ce --inst_loss svm \\\n",
    "--task task_2_tumor_subtyping --model_type transmil --log_data --data_root_dir /home/sci/Disk_data/Datasets/NLST/FEATURES_level1 \\\n",
    "--split_dir /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/NLST_100 --subtyping \\\n",
    "--csv_path dataset_csv/NLST_offical.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load Dataset\n",
      "label column: label\n",
      "label dictionary: {'LUAD': 0, 'LUSC': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 0    552\n",
      "1    324\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 194\n",
      "Slide-LVL; Number of samples registered in class 0: 552\n",
      "Patient-LVL; Number of samples registered in class 1: 117\n",
      "Slide-LVL; Number of samples registered in class 1: 324\n",
      "split_dir:  /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/NLST_100\n",
      "################# Settings ###################\n",
      "num_splits:  5\n",
      "k_start:  4\n",
      "k_end:  -1\n",
      "task:  task_2_tumor_subtyping\n",
      "max_epochs:  200\n",
      "results_dir:  ./results\n",
      "lr:  0.0002\n",
      "experiment:  nlst_100_level13_mcbat_cat_depth1_adam_FLASHAttention_testfold4\n",
      "reg:  1e-05\n",
      "label_frac:  1.0\n",
      "bag_loss:  ce\n",
      "seed:  1\n",
      "model_type:  mcbat_sb\n",
      "model_size:  small\n",
      "use_drop_out:  True\n",
      "weighted_sample:  True\n",
      "opt:  adam\n",
      "bag_weight:  0.7\n",
      "inst_loss:  svm\n",
      "B:  8\n",
      "split_dir:  /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/NLST_100\n",
      "\n",
      "Training Fold 4!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 705 samples\n",
      "Validating on 80 samples\n",
      "Testing on 91 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Setting tau to 1.0\n",
      "Done!\n",
      "MCBAT_SB(\n",
      "  (instance_loss_fn): SmoothTop1SVM()\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (transformer_low): TransformerEncoder_FLASHAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FlashAttention(\n",
      "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
      "            (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer_high): TransformerEncoder_FLASHAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FlashAttention(\n",
      "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
      "            (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attention): Attn_Net_Gated(\n",
      "    (attention_a): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_b): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (fusion_encoder): FusionEncoder()\n",
      "  (attention_V2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (attention_U2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (attention_weights2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Total number of parameters: 8408075\n",
      "Total number of trainable parameters: 8408075\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "705.0\n",
      "2\n",
      "444\n",
      "261\n",
      "##################################################\n",
      "Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 5.2357, instance_loss: 0.9625, weighted_loss: 3.9538, label: 1, bag_size: 52\n",
      "batch 39, loss: 2.1689, instance_loss: 1.9327, weighted_loss: 2.0980, label: 0, bag_size: 45\n",
      "batch 59, loss: 0.0383, instance_loss: 0.7449, weighted_loss: 0.2503, label: 1, bag_size: 64\n",
      "batch 79, loss: 0.5059, instance_loss: 1.0128, weighted_loss: 0.6580, label: 0, bag_size: 31\n",
      "batch 99, loss: 0.8403, instance_loss: 0.9880, weighted_loss: 0.8846, label: 1, bag_size: 115\n",
      "batch 119, loss: 1.9322, instance_loss: 1.1168, weighted_loss: 1.6876, label: 0, bag_size: 82\n",
      "batch 139, loss: 0.5795, instance_loss: 1.1272, weighted_loss: 0.7438, label: 0, bag_size: 101\n",
      "batch 159, loss: 0.4308, instance_loss: 1.0568, weighted_loss: 0.6186, label: 0, bag_size: 44\n",
      "batch 179, loss: 0.2088, instance_loss: 0.8831, weighted_loss: 0.4111, label: 0, bag_size: 71\n",
      "batch 199, loss: 0.7689, instance_loss: 1.0776, weighted_loss: 0.8615, label: 0, bag_size: 79\n",
      "batch 219, loss: 0.1002, instance_loss: 0.7803, weighted_loss: 0.3043, label: 1, bag_size: 69\n",
      "batch 239, loss: 2.9070, instance_loss: 0.8176, weighted_loss: 2.2802, label: 0, bag_size: 75\n",
      "batch 259, loss: 0.4461, instance_loss: 1.2593, weighted_loss: 0.6901, label: 1, bag_size: 103\n",
      "batch 279, loss: 1.8290, instance_loss: 1.0027, weighted_loss: 1.5811, label: 1, bag_size: 52\n",
      "batch 299, loss: 1.3476, instance_loss: 1.0213, weighted_loss: 1.2497, label: 1, bag_size: 70\n",
      "batch 319, loss: 0.2747, instance_loss: 0.9378, weighted_loss: 0.4736, label: 0, bag_size: 67\n",
      "batch 339, loss: 1.9362, instance_loss: 0.9019, weighted_loss: 1.6259, label: 0, bag_size: 108\n",
      "batch 359, loss: 1.9800, instance_loss: 1.1001, weighted_loss: 1.7160, label: 1, bag_size: 30\n",
      "batch 379, loss: 0.6613, instance_loss: 1.0024, weighted_loss: 0.7636, label: 0, bag_size: 51\n",
      "batch 399, loss: 0.7773, instance_loss: 0.8032, weighted_loss: 0.7851, label: 0, bag_size: 64\n",
      "batch 419, loss: 0.0665, instance_loss: 0.8161, weighted_loss: 0.2914, label: 0, bag_size: 52\n",
      "batch 439, loss: 0.4062, instance_loss: 0.8924, weighted_loss: 0.5521, label: 1, bag_size: 98\n",
      "batch 459, loss: 0.4731, instance_loss: 0.7789, weighted_loss: 0.5648, label: 0, bag_size: 60\n",
      "batch 479, loss: 1.6155, instance_loss: 1.0729, weighted_loss: 1.4528, label: 1, bag_size: 69\n",
      "batch 499, loss: 0.4153, instance_loss: 1.0553, weighted_loss: 0.6073, label: 0, bag_size: 83\n",
      "batch 519, loss: 0.9859, instance_loss: 1.0692, weighted_loss: 1.0109, label: 1, bag_size: 76\n",
      "batch 539, loss: 0.6771, instance_loss: 1.2832, weighted_loss: 0.8590, label: 0, bag_size: 50\n",
      "batch 559, loss: 0.3842, instance_loss: 0.7973, weighted_loss: 0.5081, label: 0, bag_size: 87\n",
      "batch 579, loss: 1.1445, instance_loss: 1.0382, weighted_loss: 1.1126, label: 1, bag_size: 30\n",
      "batch 599, loss: 0.2055, instance_loss: 0.9192, weighted_loss: 0.4196, label: 0, bag_size: 57\n",
      "batch 619, loss: 0.3630, instance_loss: 1.1458, weighted_loss: 0.5978, label: 1, bag_size: 105\n",
      "batch 639, loss: 0.6535, instance_loss: 0.9221, weighted_loss: 0.7341, label: 0, bag_size: 34\n",
      "batch 659, loss: 1.2168, instance_loss: 1.0144, weighted_loss: 1.1561, label: 0, bag_size: 86\n",
      "batch 679, loss: 0.4853, instance_loss: 0.7607, weighted_loss: 0.5679, label: 1, bag_size: 46\n",
      "batch 699, loss: 1.2224, instance_loss: 1.1323, weighted_loss: 1.1954, label: 0, bag_size: 68\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9891843971631206: correct 11158/11280\n",
      "class 1 clustering acc 0.01152482269503546: correct 65/5640\n",
      "Epoch: 0, train_loss: 0.8877, train_clustering_loss:  1.0344, train_error: 0.4993\n",
      "class 0: acc 0.5, correct 179/358\n",
      "class 1: acc 0.5014409221902018, correct 174/347\n",
      "\n",
      "Val Set, val_loss: 0.6864, val_error: 0.3875, auc: 0.5181\n",
      "class 0 clustering acc 1.0: correct 1280/1280\n",
      "class 1 clustering acc 0.0: correct 0/640\n",
      "class 0: acc 1.0, correct 49/49\n",
      "class 1: acc 0.0, correct 0/31\n",
      "Validation loss decreased (inf --> 0.686427).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6212, instance_loss: 0.8837, weighted_loss: 0.7000, label: 0, bag_size: 28\n",
      "batch 39, loss: 0.1871, instance_loss: 0.8360, weighted_loss: 0.3818, label: 1, bag_size: 50\n",
      "batch 59, loss: 0.3982, instance_loss: 1.0838, weighted_loss: 0.6039, label: 0, bag_size: 97\n",
      "batch 79, loss: 0.2493, instance_loss: 1.0049, weighted_loss: 0.4759, label: 1, bag_size: 66\n",
      "batch 99, loss: 0.8596, instance_loss: 0.9294, weighted_loss: 0.8805, label: 1, bag_size: 87\n",
      "batch 119, loss: 0.4873, instance_loss: 0.8836, weighted_loss: 0.6062, label: 1, bag_size: 30\n",
      "batch 139, loss: 0.4485, instance_loss: 0.8394, weighted_loss: 0.5658, label: 0, bag_size: 96\n",
      "batch 159, loss: 0.9769, instance_loss: 0.9921, weighted_loss: 0.9815, label: 0, bag_size: 106\n",
      "batch 179, loss: 0.4906, instance_loss: 0.8901, weighted_loss: 0.6105, label: 1, bag_size: 77\n",
      "batch 199, loss: 0.5200, instance_loss: 0.8425, weighted_loss: 0.6167, label: 1, bag_size: 71\n",
      "batch 219, loss: 0.0789, instance_loss: 0.8759, weighted_loss: 0.3180, label: 0, bag_size: 57\n",
      "batch 239, loss: 0.8670, instance_loss: 1.0327, weighted_loss: 0.9167, label: 0, bag_size: 125\n",
      "batch 259, loss: 0.7901, instance_loss: 0.9502, weighted_loss: 0.8381, label: 0, bag_size: 57\n",
      "batch 279, loss: 1.4415, instance_loss: 1.0738, weighted_loss: 1.3311, label: 1, bag_size: 119\n",
      "batch 299, loss: 0.5615, instance_loss: 0.8185, weighted_loss: 0.6386, label: 0, bag_size: 88\n",
      "batch 319, loss: 0.6879, instance_loss: 0.9738, weighted_loss: 0.7736, label: 1, bag_size: 70\n",
      "batch 339, loss: 0.3799, instance_loss: 0.9045, weighted_loss: 0.5373, label: 1, bag_size: 41\n",
      "batch 359, loss: 0.4255, instance_loss: 1.0183, weighted_loss: 0.6034, label: 0, bag_size: 107\n",
      "batch 379, loss: 0.5935, instance_loss: 1.1526, weighted_loss: 0.7612, label: 1, bag_size: 76\n",
      "batch 399, loss: 0.3994, instance_loss: 0.9760, weighted_loss: 0.5724, label: 0, bag_size: 79\n",
      "batch 419, loss: 0.6793, instance_loss: 1.0957, weighted_loss: 0.8042, label: 1, bag_size: 46\n",
      "batch 439, loss: 0.8546, instance_loss: 0.9480, weighted_loss: 0.8826, label: 1, bag_size: 119\n",
      "batch 459, loss: 0.3812, instance_loss: 0.9032, weighted_loss: 0.5378, label: 0, bag_size: 24\n",
      "batch 479, loss: 0.6571, instance_loss: 1.1060, weighted_loss: 0.7918, label: 1, bag_size: 109\n",
      "batch 499, loss: 0.4481, instance_loss: 0.9405, weighted_loss: 0.5958, label: 0, bag_size: 65\n",
      "batch 519, loss: 0.2432, instance_loss: 0.7599, weighted_loss: 0.3982, label: 0, bag_size: 78\n",
      "batch 539, loss: 0.5121, instance_loss: 0.8218, weighted_loss: 0.6050, label: 0, bag_size: 74\n",
      "batch 559, loss: 1.2910, instance_loss: 1.0876, weighted_loss: 1.2300, label: 0, bag_size: 45\n",
      "batch 579, loss: 0.9403, instance_loss: 0.9505, weighted_loss: 0.9434, label: 1, bag_size: 120\n",
      "batch 599, loss: 0.4661, instance_loss: 0.9489, weighted_loss: 0.6109, label: 0, bag_size: 40\n",
      "batch 619, loss: 0.6561, instance_loss: 0.8989, weighted_loss: 0.7289, label: 0, bag_size: 85\n",
      "batch 639, loss: 0.6541, instance_loss: 0.9444, weighted_loss: 0.7412, label: 1, bag_size: 41\n",
      "batch 659, loss: 0.6086, instance_loss: 0.8209, weighted_loss: 0.6723, label: 1, bag_size: 89\n",
      "batch 679, loss: 0.3440, instance_loss: 0.8645, weighted_loss: 0.5002, label: 0, bag_size: 77\n",
      "batch 699, loss: 1.2162, instance_loss: 0.9543, weighted_loss: 1.1376, label: 0, bag_size: 31\n",
      "\n",
      "\n",
      "class 0 clustering acc 1.0: correct 11280/11280\n",
      "class 1 clustering acc 0.0: correct 0/5640\n",
      "Epoch: 1, train_loss: 0.7537, train_clustering_loss:  0.9996, train_error: 0.4780\n",
      "class 0: acc 0.5640326975476839, correct 207/367\n",
      "class 1: acc 0.47633136094674555, correct 161/338\n",
      "\n",
      "Val Set, val_loss: 0.7029, val_error: 0.6125, auc: 0.5168\n",
      "class 0 clustering acc 1.0: correct 1280/1280\n",
      "class 1 clustering acc 0.0: correct 0/640\n",
      "class 0: acc 0.0, correct 0/49\n",
      "class 1: acc 1.0, correct 31/31\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.0542, instance_loss: 1.0770, weighted_loss: 1.0610, label: 1, bag_size: 75\n",
      "batch 39, loss: 1.2745, instance_loss: 1.1884, weighted_loss: 1.2487, label: 0, bag_size: 49\n",
      "batch 59, loss: 0.8674, instance_loss: 1.1072, weighted_loss: 0.9393, label: 0, bag_size: 86\n",
      "batch 79, loss: 0.5697, instance_loss: 0.8288, weighted_loss: 0.6474, label: 1, bag_size: 79\n",
      "batch 99, loss: 0.5624, instance_loss: 0.9544, weighted_loss: 0.6800, label: 0, bag_size: 64\n",
      "batch 119, loss: 0.8042, instance_loss: 0.9809, weighted_loss: 0.8572, label: 0, bag_size: 80\n",
      "batch 139, loss: 0.4454, instance_loss: 0.7763, weighted_loss: 0.5447, label: 0, bag_size: 73\n",
      "batch 159, loss: 0.5748, instance_loss: 0.9323, weighted_loss: 0.6820, label: 1, bag_size: 46\n",
      "batch 179, loss: 1.4028, instance_loss: 1.0926, weighted_loss: 1.3097, label: 1, bag_size: 24\n",
      "batch 199, loss: 1.2194, instance_loss: 0.9243, weighted_loss: 1.1309, label: 0, bag_size: 54\n",
      "batch 219, loss: 0.5552, instance_loss: 0.9013, weighted_loss: 0.6590, label: 0, bag_size: 74\n",
      "batch 239, loss: 0.3985, instance_loss: 0.8269, weighted_loss: 0.5271, label: 1, bag_size: 70\n",
      "batch 259, loss: 0.7640, instance_loss: 1.1404, weighted_loss: 0.8769, label: 0, bag_size: 30\n",
      "batch 279, loss: 0.6389, instance_loss: 0.9583, weighted_loss: 0.7348, label: 1, bag_size: 55\n",
      "batch 299, loss: 0.4593, instance_loss: 0.8532, weighted_loss: 0.5775, label: 1, bag_size: 73\n",
      "batch 319, loss: 0.4529, instance_loss: 0.9059, weighted_loss: 0.5888, label: 0, bag_size: 74\n",
      "batch 339, loss: 0.7053, instance_loss: 0.7874, weighted_loss: 0.7299, label: 0, bag_size: 68\n",
      "batch 359, loss: 0.5443, instance_loss: 0.8109, weighted_loss: 0.6243, label: 0, bag_size: 62\n",
      "batch 379, loss: 1.0385, instance_loss: 1.2057, weighted_loss: 1.0887, label: 1, bag_size: 46\n",
      "batch 399, loss: 1.3523, instance_loss: 1.1359, weighted_loss: 1.2874, label: 0, bag_size: 96\n",
      "batch 419, loss: 0.3550, instance_loss: 1.0055, weighted_loss: 0.5502, label: 0, bag_size: 91\n",
      "batch 439, loss: 0.5786, instance_loss: 0.9506, weighted_loss: 0.6902, label: 1, bag_size: 11\n",
      "batch 459, loss: 0.8157, instance_loss: 1.0034, weighted_loss: 0.8720, label: 0, bag_size: 53\n",
      "batch 479, loss: 0.7207, instance_loss: 0.8540, weighted_loss: 0.7607, label: 1, bag_size: 81\n",
      "batch 499, loss: 0.5994, instance_loss: 0.9317, weighted_loss: 0.6991, label: 0, bag_size: 53\n",
      "batch 519, loss: 0.5843, instance_loss: 0.7599, weighted_loss: 0.6370, label: 0, bag_size: 83\n",
      "batch 539, loss: 1.1015, instance_loss: 1.1147, weighted_loss: 1.1054, label: 1, bag_size: 105\n",
      "batch 559, loss: 0.8073, instance_loss: 0.8772, weighted_loss: 0.8283, label: 0, bag_size: 78\n",
      "batch 579, loss: 1.0302, instance_loss: 1.0131, weighted_loss: 1.0251, label: 1, bag_size: 50\n",
      "batch 599, loss: 0.4945, instance_loss: 0.8191, weighted_loss: 0.5919, label: 1, bag_size: 69\n",
      "batch 619, loss: 1.0856, instance_loss: 1.3039, weighted_loss: 1.1511, label: 0, bag_size: 62\n",
      "batch 639, loss: 0.6221, instance_loss: 0.8560, weighted_loss: 0.6923, label: 0, bag_size: 81\n",
      "batch 659, loss: 0.5096, instance_loss: 0.7732, weighted_loss: 0.5887, label: 1, bag_size: 43\n",
      "batch 679, loss: 0.3410, instance_loss: 0.5227, weighted_loss: 0.3955, label: 1, bag_size: 43\n",
      "batch 699, loss: 0.5343, instance_loss: 0.8935, weighted_loss: 0.6421, label: 0, bag_size: 45\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9958333333333333: correct 11233/11280\n",
      "class 1 clustering acc 0.004964539007092199: correct 28/5640\n",
      "Epoch: 2, train_loss: 0.7333, train_clustering_loss:  0.9430, train_error: 0.5149\n",
      "class 0: acc 0.43352601156069365, correct 150/346\n",
      "class 1: acc 0.5348189415041783, correct 192/359\n",
      "\n",
      "Val Set, val_loss: 0.6729, val_error: 0.3875, auc: 0.5411\n",
      "class 0 clustering acc 1.0: correct 1280/1280\n",
      "class 1 clustering acc 0.0: correct 0/640\n",
      "class 0: acc 1.0, correct 49/49\n",
      "class 1: acc 0.0, correct 0/31\n",
      "Validation loss decreased (0.686427 --> 0.672899).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3999, instance_loss: 0.5802, weighted_loss: 0.4540, label: 0, bag_size: 96\n",
      "batch 39, loss: 0.5920, instance_loss: 0.8369, weighted_loss: 0.6655, label: 1, bag_size: 74\n",
      "batch 59, loss: 0.5588, instance_loss: 0.9201, weighted_loss: 0.6672, label: 0, bag_size: 26\n",
      "batch 79, loss: 0.4836, instance_loss: 0.6762, weighted_loss: 0.5414, label: 0, bag_size: 80\n",
      "batch 99, loss: 0.6917, instance_loss: 0.7302, weighted_loss: 0.7033, label: 0, bag_size: 47\n",
      "batch 119, loss: 0.9299, instance_loss: 1.0366, weighted_loss: 0.9619, label: 1, bag_size: 104\n",
      "batch 139, loss: 0.9773, instance_loss: 1.0490, weighted_loss: 0.9988, label: 0, bag_size: 83\n",
      "batch 159, loss: 0.8889, instance_loss: 0.9267, weighted_loss: 0.9002, label: 1, bag_size: 86\n",
      "batch 179, loss: 0.8852, instance_loss: 1.0326, weighted_loss: 0.9294, label: 1, bag_size: 89\n",
      "batch 199, loss: 0.9549, instance_loss: 0.9259, weighted_loss: 0.9462, label: 0, bag_size: 81\n",
      "batch 219, loss: 0.7735, instance_loss: 0.7790, weighted_loss: 0.7751, label: 1, bag_size: 107\n",
      "batch 239, loss: 0.7540, instance_loss: 0.9815, weighted_loss: 0.8222, label: 1, bag_size: 104\n",
      "batch 259, loss: 0.6237, instance_loss: 0.9532, weighted_loss: 0.7225, label: 1, bag_size: 82\n",
      "batch 279, loss: 0.5595, instance_loss: 0.7234, weighted_loss: 0.6087, label: 1, bag_size: 90\n",
      "batch 299, loss: 0.9178, instance_loss: 1.1630, weighted_loss: 0.9913, label: 0, bag_size: 97\n",
      "batch 319, loss: 0.6419, instance_loss: 0.8643, weighted_loss: 0.7086, label: 0, bag_size: 67\n",
      "batch 339, loss: 0.6818, instance_loss: 0.9560, weighted_loss: 0.7641, label: 1, bag_size: 30\n",
      "batch 359, loss: 0.2983, instance_loss: 0.5582, weighted_loss: 0.3763, label: 1, bag_size: 65\n",
      "batch 379, loss: 0.5592, instance_loss: 1.1518, weighted_loss: 0.7370, label: 0, bag_size: 127\n",
      "batch 399, loss: 0.6839, instance_loss: 0.8743, weighted_loss: 0.7410, label: 1, bag_size: 89\n",
      "batch 419, loss: 0.7283, instance_loss: 1.0234, weighted_loss: 0.8169, label: 1, bag_size: 33\n",
      "batch 439, loss: 0.4190, instance_loss: 0.6636, weighted_loss: 0.4924, label: 1, bag_size: 87\n",
      "batch 459, loss: 0.5334, instance_loss: 0.5101, weighted_loss: 0.5264, label: 1, bag_size: 31\n",
      "batch 479, loss: 0.8460, instance_loss: 1.0386, weighted_loss: 0.9038, label: 0, bag_size: 61\n",
      "batch 499, loss: 0.7972, instance_loss: 0.8116, weighted_loss: 0.8015, label: 0, bag_size: 95\n",
      "batch 519, loss: 0.4994, instance_loss: 0.7285, weighted_loss: 0.5681, label: 0, bag_size: 59\n",
      "batch 539, loss: 0.7695, instance_loss: 0.7429, weighted_loss: 0.7615, label: 0, bag_size: 70\n",
      "batch 559, loss: 0.9322, instance_loss: 1.2181, weighted_loss: 1.0180, label: 1, bag_size: 64\n",
      "batch 579, loss: 0.6557, instance_loss: 0.7212, weighted_loss: 0.6754, label: 0, bag_size: 64\n",
      "batch 599, loss: 1.1434, instance_loss: 1.3187, weighted_loss: 1.1960, label: 0, bag_size: 34\n",
      "batch 619, loss: 0.7809, instance_loss: 1.0432, weighted_loss: 0.8596, label: 0, bag_size: 79\n",
      "batch 639, loss: 0.8038, instance_loss: 1.0495, weighted_loss: 0.8775, label: 1, bag_size: 46\n",
      "batch 659, loss: 0.5123, instance_loss: 0.8024, weighted_loss: 0.5993, label: 1, bag_size: 117\n",
      "batch 679, loss: 0.9387, instance_loss: 1.2586, weighted_loss: 1.0347, label: 0, bag_size: 49\n",
      "batch 699, loss: 0.7283, instance_loss: 0.7800, weighted_loss: 0.7438, label: 1, bag_size: 61\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9918439716312056: correct 11188/11280\n",
      "class 1 clustering acc 0.02304964539007092: correct 130/5640\n",
      "Epoch: 3, train_loss: 0.7143, train_clustering_loss:  0.8867, train_error: 0.4922\n",
      "class 0: acc 0.47674418604651164, correct 164/344\n",
      "class 1: acc 0.5373961218836565, correct 194/361\n",
      "\n",
      "Val Set, val_loss: 0.6945, val_error: 0.5375, auc: 0.6346\n",
      "class 0 clustering acc 1.0: correct 1280/1280\n",
      "class 1 clustering acc 0.0: correct 0/640\n",
      "class 0: acc 0.14285714285714285, correct 7/49\n",
      "class 1: acc 0.967741935483871, correct 30/31\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.8562, instance_loss: 0.9899, weighted_loss: 0.8963, label: 0, bag_size: 84\n",
      "batch 39, loss: 0.7778, instance_loss: 0.8049, weighted_loss: 0.7859, label: 0, bag_size: 94\n",
      "batch 59, loss: 0.2504, instance_loss: 1.6101, weighted_loss: 0.6583, label: 0, bag_size: 30\n",
      "batch 79, loss: 0.5695, instance_loss: 0.7701, weighted_loss: 0.6296, label: 1, bag_size: 43\n",
      "batch 99, loss: 0.2328, instance_loss: 0.5245, weighted_loss: 0.3203, label: 0, bag_size: 44\n",
      "batch 119, loss: 0.9117, instance_loss: 1.0466, weighted_loss: 0.9522, label: 1, bag_size: 86\n",
      "batch 139, loss: 0.9816, instance_loss: 1.0181, weighted_loss: 0.9925, label: 1, bag_size: 33\n",
      "batch 159, loss: 0.5859, instance_loss: 0.7758, weighted_loss: 0.6429, label: 1, bag_size: 102\n",
      "batch 179, loss: 0.4274, instance_loss: 0.7698, weighted_loss: 0.5301, label: 0, bag_size: 29\n",
      "batch 199, loss: 0.7296, instance_loss: 0.8559, weighted_loss: 0.7675, label: 1, bag_size: 71\n",
      "batch 219, loss: 0.2966, instance_loss: 0.5396, weighted_loss: 0.3695, label: 1, bag_size: 89\n",
      "batch 239, loss: 2.2321, instance_loss: 2.3019, weighted_loss: 2.2530, label: 1, bag_size: 51\n",
      "batch 259, loss: 0.2915, instance_loss: 0.2623, weighted_loss: 0.2827, label: 1, bag_size: 98\n",
      "batch 279, loss: 1.7965, instance_loss: 1.8495, weighted_loss: 1.8124, label: 1, bag_size: 36\n",
      "batch 299, loss: 0.7996, instance_loss: 1.1503, weighted_loss: 0.9048, label: 1, bag_size: 59\n",
      "batch 319, loss: 0.3774, instance_loss: 0.6052, weighted_loss: 0.4457, label: 1, bag_size: 51\n",
      "batch 339, loss: 1.1845, instance_loss: 1.6132, weighted_loss: 1.3131, label: 0, bag_size: 55\n",
      "batch 359, loss: 0.6458, instance_loss: 0.9095, weighted_loss: 0.7249, label: 0, bag_size: 40\n",
      "batch 379, loss: 0.3779, instance_loss: 0.5598, weighted_loss: 0.4325, label: 0, bag_size: 64\n",
      "batch 399, loss: 0.6417, instance_loss: 0.7894, weighted_loss: 0.6860, label: 0, bag_size: 80\n",
      "batch 419, loss: 0.4401, instance_loss: 0.4762, weighted_loss: 0.4510, label: 0, bag_size: 74\n",
      "batch 439, loss: 0.8238, instance_loss: 0.8817, weighted_loss: 0.8412, label: 1, bag_size: 51\n",
      "batch 459, loss: 0.4953, instance_loss: 0.4920, weighted_loss: 0.4943, label: 1, bag_size: 75\n",
      "batch 479, loss: 0.4488, instance_loss: 0.7762, weighted_loss: 0.5470, label: 0, bag_size: 96\n",
      "batch 499, loss: 0.2962, instance_loss: 0.4695, weighted_loss: 0.3482, label: 1, bag_size: 88\n",
      "batch 519, loss: 0.5972, instance_loss: 0.5854, weighted_loss: 0.5936, label: 1, bag_size: 90\n",
      "batch 539, loss: 0.2191, instance_loss: 0.2481, weighted_loss: 0.2278, label: 0, bag_size: 44\n",
      "batch 559, loss: 0.7779, instance_loss: 1.0486, weighted_loss: 0.8591, label: 0, bag_size: 89\n",
      "batch 579, loss: 0.2196, instance_loss: 0.2233, weighted_loss: 0.2207, label: 0, bag_size: 68\n",
      "batch 599, loss: 2.0401, instance_loss: 1.8784, weighted_loss: 1.9916, label: 1, bag_size: 89\n",
      "batch 619, loss: 0.9772, instance_loss: 1.0098, weighted_loss: 0.9870, label: 0, bag_size: 46\n",
      "batch 639, loss: 0.6573, instance_loss: 1.1581, weighted_loss: 0.8075, label: 0, bag_size: 58\n",
      "batch 659, loss: 0.9607, instance_loss: 1.3131, weighted_loss: 1.0664, label: 1, bag_size: 95\n",
      "batch 679, loss: 0.5548, instance_loss: 0.8953, weighted_loss: 0.6570, label: 0, bag_size: 53\n",
      "batch 699, loss: 0.5999, instance_loss: 0.8540, weighted_loss: 0.6762, label: 0, bag_size: 87\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9521276595744681: correct 10740/11280\n",
      "class 1 clustering acc 0.18581560283687942: correct 1048/5640\n",
      "Epoch: 4, train_loss: 0.6780, train_clustering_loss:  0.8750, train_error: 0.3929\n",
      "class 0: acc 0.6256684491978609, correct 234/374\n",
      "class 1: acc 0.5861027190332326, correct 194/331\n",
      "\n",
      "Val Set, val_loss: 0.6656, val_error: 0.3500, auc: 0.6116\n",
      "class 0 clustering acc 0.9703125: correct 1242/1280\n",
      "class 1 clustering acc 0.0359375: correct 23/640\n",
      "class 0: acc 0.7142857142857143, correct 35/49\n",
      "class 1: acc 0.5483870967741935, correct 17/31\n",
      "Validation loss decreased (0.672899 --> 0.665635).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3528, instance_loss: 0.5628, weighted_loss: 0.4158, label: 0, bag_size: 46\n",
      "batch 39, loss: 0.3696, instance_loss: 0.4940, weighted_loss: 0.4069, label: 0, bag_size: 72\n",
      "batch 59, loss: 0.3130, instance_loss: 0.4128, weighted_loss: 0.3429, label: 1, bag_size: 92\n",
      "batch 79, loss: 0.4197, instance_loss: 0.4704, weighted_loss: 0.4349, label: 0, bag_size: 27\n",
      "batch 99, loss: 0.5645, instance_loss: 0.7835, weighted_loss: 0.6302, label: 1, bag_size: 38\n",
      "batch 119, loss: 0.4695, instance_loss: 0.7522, weighted_loss: 0.5543, label: 1, bag_size: 25\n",
      "batch 139, loss: 1.1891, instance_loss: 1.5229, weighted_loss: 1.2893, label: 0, bag_size: 97\n",
      "batch 159, loss: 0.2156, instance_loss: 0.5786, weighted_loss: 0.3245, label: 0, bag_size: 88\n",
      "batch 179, loss: 0.0781, instance_loss: 0.1518, weighted_loss: 0.1002, label: 1, bag_size: 131\n",
      "batch 199, loss: 0.2635, instance_loss: 0.4552, weighted_loss: 0.3210, label: 1, bag_size: 71\n",
      "batch 219, loss: 0.8111, instance_loss: 0.8465, weighted_loss: 0.8217, label: 0, bag_size: 69\n",
      "batch 239, loss: 0.4441, instance_loss: 0.3781, weighted_loss: 0.4243, label: 0, bag_size: 95\n",
      "batch 259, loss: 0.2880, instance_loss: 0.3239, weighted_loss: 0.2987, label: 0, bag_size: 34\n",
      "batch 279, loss: 0.6463, instance_loss: 1.0495, weighted_loss: 0.7672, label: 1, bag_size: 66\n",
      "batch 299, loss: 0.2492, instance_loss: 0.1058, weighted_loss: 0.2062, label: 1, bag_size: 89\n",
      "batch 319, loss: 0.6518, instance_loss: 0.7208, weighted_loss: 0.6725, label: 0, bag_size: 32\n",
      "batch 339, loss: 0.1563, instance_loss: 0.2020, weighted_loss: 0.1700, label: 0, bag_size: 104\n",
      "batch 359, loss: 0.8560, instance_loss: 1.1516, weighted_loss: 0.9447, label: 1, bag_size: 61\n",
      "batch 379, loss: 2.8925, instance_loss: 3.6420, weighted_loss: 3.1173, label: 0, bag_size: 96\n",
      "batch 399, loss: 0.9832, instance_loss: 1.7186, weighted_loss: 1.2038, label: 0, bag_size: 77\n",
      "batch 419, loss: 0.6345, instance_loss: 1.7945, weighted_loss: 0.9825, label: 1, bag_size: 71\n",
      "batch 439, loss: 1.0972, instance_loss: 0.8377, weighted_loss: 1.0194, label: 1, bag_size: 92\n",
      "batch 459, loss: 0.2555, instance_loss: 0.2161, weighted_loss: 0.2437, label: 0, bag_size: 105\n",
      "batch 479, loss: 2.1920, instance_loss: 2.3028, weighted_loss: 2.2253, label: 1, bag_size: 45\n",
      "batch 499, loss: 1.7640, instance_loss: 2.4573, weighted_loss: 1.9720, label: 0, bag_size: 43\n",
      "batch 519, loss: 1.0123, instance_loss: 1.2953, weighted_loss: 1.0972, label: 0, bag_size: 83\n",
      "batch 539, loss: 3.7348, instance_loss: 2.3532, weighted_loss: 3.3203, label: 0, bag_size: 18\n",
      "batch 559, loss: 0.1685, instance_loss: 0.4590, weighted_loss: 0.2556, label: 0, bag_size: 63\n",
      "batch 579, loss: 0.0841, instance_loss: 0.1592, weighted_loss: 0.1067, label: 1, bag_size: 86\n",
      "batch 599, loss: 0.9463, instance_loss: 1.3928, weighted_loss: 1.0802, label: 0, bag_size: 43\n",
      "batch 619, loss: 0.0690, instance_loss: 0.3209, weighted_loss: 0.1445, label: 0, bag_size: 53\n",
      "batch 639, loss: 0.4049, instance_loss: 1.1132, weighted_loss: 0.6174, label: 1, bag_size: 25\n",
      "batch 659, loss: 0.6662, instance_loss: 0.6164, weighted_loss: 0.6513, label: 1, bag_size: 60\n",
      "batch 679, loss: 0.7269, instance_loss: 0.4941, weighted_loss: 0.6570, label: 1, bag_size: 92\n",
      "batch 699, loss: 0.6505, instance_loss: 0.4374, weighted_loss: 0.5866, label: 1, bag_size: 47\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9444148936170212: correct 10653/11280\n",
      "class 1 clustering acc 0.3384751773049645: correct 1909/5640\n",
      "Epoch: 5, train_loss: 0.6409, train_clustering_loss:  0.7925, train_error: 0.3603\n",
      "class 0: acc 0.6041055718475073, correct 206/341\n",
      "class 1: acc 0.6730769230769231, correct 245/364\n",
      "\n",
      "Val Set, val_loss: 0.6333, val_error: 0.2875, auc: 0.7867\n",
      "class 0 clustering acc 0.978125: correct 1252/1280\n",
      "class 1 clustering acc 0.1296875: correct 83/640\n",
      "class 0: acc 0.7142857142857143, correct 35/49\n",
      "class 1: acc 0.7096774193548387, correct 22/31\n",
      "Validation loss decreased (0.665635 --> 0.633321).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.9884, instance_loss: 1.7634, weighted_loss: 1.2209, label: 1, bag_size: 94\n",
      "batch 39, loss: 0.5364, instance_loss: 0.3224, weighted_loss: 0.4722, label: 1, bag_size: 55\n",
      "batch 59, loss: 0.4355, instance_loss: 0.4089, weighted_loss: 0.4275, label: 0, bag_size: 40\n",
      "batch 79, loss: 0.3014, instance_loss: 0.5850, weighted_loss: 0.3865, label: 0, bag_size: 28\n",
      "batch 99, loss: 0.3752, instance_loss: 1.0317, weighted_loss: 0.5722, label: 1, bag_size: 103\n",
      "batch 119, loss: 0.4397, instance_loss: 1.1634, weighted_loss: 0.6568, label: 0, bag_size: 64\n",
      "batch 139, loss: 0.7824, instance_loss: 1.0192, weighted_loss: 0.8534, label: 1, bag_size: 65\n",
      "batch 159, loss: 0.6725, instance_loss: 0.5673, weighted_loss: 0.6409, label: 0, bag_size: 76\n",
      "batch 179, loss: 0.6064, instance_loss: 0.6394, weighted_loss: 0.6163, label: 0, bag_size: 33\n",
      "batch 199, loss: 0.2279, instance_loss: 0.2710, weighted_loss: 0.2408, label: 1, bag_size: 48\n",
      "batch 219, loss: 0.3582, instance_loss: 0.4978, weighted_loss: 0.4001, label: 0, bag_size: 72\n",
      "batch 239, loss: 0.6984, instance_loss: 0.5981, weighted_loss: 0.6683, label: 0, bag_size: 47\n",
      "batch 259, loss: 1.1165, instance_loss: 0.9000, weighted_loss: 1.0516, label: 1, bag_size: 59\n",
      "batch 279, loss: 0.4889, instance_loss: 0.8232, weighted_loss: 0.5892, label: 0, bag_size: 57\n",
      "batch 299, loss: 0.3246, instance_loss: 0.1723, weighted_loss: 0.2789, label: 1, bag_size: 55\n",
      "batch 319, loss: 0.4072, instance_loss: 0.1324, weighted_loss: 0.3248, label: 1, bag_size: 77\n",
      "batch 339, loss: 0.7000, instance_loss: 0.2701, weighted_loss: 0.5711, label: 1, bag_size: 112\n",
      "batch 359, loss: 0.4613, instance_loss: 0.8991, weighted_loss: 0.5927, label: 1, bag_size: 31\n",
      "batch 379, loss: 0.2786, instance_loss: 0.3326, weighted_loss: 0.2948, label: 0, bag_size: 106\n",
      "batch 399, loss: 0.9997, instance_loss: 1.9010, weighted_loss: 1.2701, label: 1, bag_size: 117\n",
      "batch 419, loss: 1.8085, instance_loss: 1.5769, weighted_loss: 1.7390, label: 0, bag_size: 62\n",
      "batch 439, loss: 0.3510, instance_loss: 0.3826, weighted_loss: 0.3605, label: 0, bag_size: 47\n",
      "batch 459, loss: 1.6923, instance_loss: 2.3108, weighted_loss: 1.8778, label: 1, bag_size: 119\n",
      "batch 479, loss: 0.4973, instance_loss: 0.7553, weighted_loss: 0.5747, label: 1, bag_size: 54\n",
      "batch 499, loss: 0.7840, instance_loss: 1.6487, weighted_loss: 1.0434, label: 1, bag_size: 77\n",
      "batch 519, loss: 0.4242, instance_loss: 0.5831, weighted_loss: 0.4719, label: 1, bag_size: 77\n",
      "batch 539, loss: 0.4124, instance_loss: 0.5378, weighted_loss: 0.4500, label: 0, bag_size: 105\n",
      "batch 559, loss: 0.4564, instance_loss: 1.1809, weighted_loss: 0.6738, label: 0, bag_size: 95\n",
      "batch 579, loss: 1.1691, instance_loss: 0.7101, weighted_loss: 1.0314, label: 1, bag_size: 56\n",
      "batch 599, loss: 0.4875, instance_loss: 1.0342, weighted_loss: 0.6515, label: 1, bag_size: 31\n",
      "batch 619, loss: 0.3354, instance_loss: 0.2062, weighted_loss: 0.2967, label: 1, bag_size: 55\n",
      "batch 639, loss: 1.1630, instance_loss: 1.2138, weighted_loss: 1.1782, label: 1, bag_size: 64\n",
      "batch 659, loss: 0.6783, instance_loss: 0.8257, weighted_loss: 0.7225, label: 0, bag_size: 135\n",
      "batch 679, loss: 0.5472, instance_loss: 0.7057, weighted_loss: 0.5947, label: 1, bag_size: 50\n",
      "batch 699, loss: 0.0732, instance_loss: 0.2357, weighted_loss: 0.1220, label: 0, bag_size: 68\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9449468085106383: correct 10659/11280\n",
      "class 1 clustering acc 0.2975177304964539: correct 1678/5640\n",
      "Epoch: 6, train_loss: 0.6431, train_clustering_loss:  0.7961, train_error: 0.3560\n",
      "class 0: acc 0.6713881019830028, correct 237/353\n",
      "class 1: acc 0.6164772727272727, correct 217/352\n",
      "\n",
      "Val Set, val_loss: 0.5199, val_error: 0.2000, auc: 0.8453\n",
      "class 0 clustering acc 0.88125: correct 1128/1280\n",
      "class 1 clustering acc 0.5796875: correct 371/640\n",
      "class 0: acc 0.9591836734693877, correct 47/49\n",
      "class 1: acc 0.5483870967741935, correct 17/31\n",
      "Validation loss decreased (0.633321 --> 0.519946).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2745, instance_loss: 0.6898, weighted_loss: 0.3991, label: 0, bag_size: 33\n",
      "batch 39, loss: 0.5899, instance_loss: 0.7709, weighted_loss: 0.6442, label: 0, bag_size: 52\n",
      "batch 59, loss: 0.3021, instance_loss: 0.7657, weighted_loss: 0.4412, label: 0, bag_size: 60\n",
      "batch 79, loss: 0.1466, instance_loss: 0.5888, weighted_loss: 0.2793, label: 1, bag_size: 43\n",
      "batch 99, loss: 0.5407, instance_loss: 1.0883, weighted_loss: 0.7050, label: 1, bag_size: 52\n",
      "batch 119, loss: 2.2753, instance_loss: 2.2632, weighted_loss: 2.2717, label: 1, bag_size: 57\n",
      "batch 139, loss: 0.2730, instance_loss: 0.3647, weighted_loss: 0.3006, label: 1, bag_size: 23\n",
      "batch 159, loss: 0.1384, instance_loss: 0.0537, weighted_loss: 0.1130, label: 1, bag_size: 61\n",
      "batch 179, loss: 0.3224, instance_loss: 0.8345, weighted_loss: 0.4760, label: 0, bag_size: 28\n",
      "batch 199, loss: 2.4642, instance_loss: 1.8680, weighted_loss: 2.2853, label: 0, bag_size: 78\n",
      "batch 219, loss: 1.5112, instance_loss: 1.6519, weighted_loss: 1.5534, label: 0, bag_size: 42\n",
      "batch 239, loss: 0.0228, instance_loss: 0.1051, weighted_loss: 0.0475, label: 1, bag_size: 84\n",
      "batch 259, loss: 0.0683, instance_loss: 0.1018, weighted_loss: 0.0783, label: 0, bag_size: 41\n",
      "batch 279, loss: 0.1242, instance_loss: 0.0161, weighted_loss: 0.0918, label: 1, bag_size: 53\n",
      "batch 299, loss: 0.0081, instance_loss: 0.1518, weighted_loss: 0.0512, label: 1, bag_size: 47\n",
      "batch 319, loss: 0.5685, instance_loss: 0.9665, weighted_loss: 0.6879, label: 1, bag_size: 109\n",
      "batch 339, loss: 0.5615, instance_loss: 0.7982, weighted_loss: 0.6325, label: 0, bag_size: 32\n",
      "batch 359, loss: 0.5856, instance_loss: 0.8482, weighted_loss: 0.6644, label: 0, bag_size: 86\n",
      "batch 379, loss: 0.3783, instance_loss: 0.4472, weighted_loss: 0.3990, label: 0, bag_size: 31\n",
      "batch 399, loss: 1.5373, instance_loss: 1.7055, weighted_loss: 1.5877, label: 1, bag_size: 55\n",
      "batch 419, loss: 1.0369, instance_loss: 1.4774, weighted_loss: 1.1691, label: 1, bag_size: 30\n",
      "batch 439, loss: 0.2430, instance_loss: 0.3409, weighted_loss: 0.2723, label: 1, bag_size: 84\n",
      "batch 459, loss: 0.5969, instance_loss: 0.6865, weighted_loss: 0.6238, label: 0, bag_size: 95\n",
      "batch 479, loss: 0.5794, instance_loss: 0.5934, weighted_loss: 0.5836, label: 1, bag_size: 115\n",
      "batch 499, loss: 0.5071, instance_loss: 0.7688, weighted_loss: 0.5856, label: 1, bag_size: 57\n",
      "batch 519, loss: 0.3113, instance_loss: 0.4186, weighted_loss: 0.3435, label: 0, bag_size: 70\n",
      "batch 539, loss: 0.2201, instance_loss: 0.5702, weighted_loss: 0.3251, label: 1, bag_size: 86\n",
      "batch 559, loss: 0.4303, instance_loss: 0.4313, weighted_loss: 0.4306, label: 1, bag_size: 117\n",
      "batch 579, loss: 0.4651, instance_loss: 0.6337, weighted_loss: 0.5157, label: 1, bag_size: 56\n",
      "batch 599, loss: 0.9821, instance_loss: 0.4778, weighted_loss: 0.8308, label: 0, bag_size: 73\n",
      "batch 619, loss: 0.9609, instance_loss: 0.7934, weighted_loss: 0.9107, label: 0, bag_size: 89\n",
      "batch 639, loss: 0.8103, instance_loss: 1.0581, weighted_loss: 0.8846, label: 0, bag_size: 84\n",
      "batch 659, loss: 0.6423, instance_loss: 2.9823, weighted_loss: 1.3443, label: 1, bag_size: 37\n",
      "batch 679, loss: 0.3594, instance_loss: 0.2034, weighted_loss: 0.3126, label: 1, bag_size: 52\n",
      "batch 699, loss: 0.2071, instance_loss: 0.1932, weighted_loss: 0.2029, label: 0, bag_size: 65\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9484929078014185: correct 10699/11280\n",
      "class 1 clustering acc 0.4186170212765957: correct 2361/5640\n",
      "Epoch: 7, train_loss: 0.5712, train_clustering_loss:  0.7093, train_error: 0.2837\n",
      "class 0: acc 0.6892307692307692, correct 224/325\n",
      "class 1: acc 0.7394736842105263, correct 281/380\n",
      "\n",
      "Val Set, val_loss: 0.6741, val_error: 0.2875, auc: 0.7584\n",
      "class 0 clustering acc 0.89921875: correct 1151/1280\n",
      "class 1 clustering acc 0.65: correct 416/640\n",
      "class 0: acc 0.9795918367346939, correct 48/49\n",
      "class 1: acc 0.2903225806451613, correct 9/31\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2388, instance_loss: 1.0487, weighted_loss: 0.4817, label: 1, bag_size: 115\n",
      "batch 39, loss: 0.7135, instance_loss: 1.3254, weighted_loss: 0.8971, label: 1, bag_size: 55\n",
      "batch 59, loss: 0.6101, instance_loss: 0.7829, weighted_loss: 0.6619, label: 0, bag_size: 59\n",
      "batch 79, loss: 1.3358, instance_loss: 2.1811, weighted_loss: 1.5894, label: 0, bag_size: 54\n",
      "batch 99, loss: 0.5138, instance_loss: 0.5434, weighted_loss: 0.5227, label: 1, bag_size: 30\n",
      "batch 119, loss: 0.8948, instance_loss: 1.6389, weighted_loss: 1.1180, label: 1, bag_size: 90\n",
      "batch 139, loss: 0.1738, instance_loss: 0.0422, weighted_loss: 0.1343, label: 1, bag_size: 48\n",
      "batch 159, loss: 1.1015, instance_loss: 1.5182, weighted_loss: 1.2265, label: 1, bag_size: 104\n",
      "batch 179, loss: 0.7509, instance_loss: 0.5160, weighted_loss: 0.6804, label: 0, bag_size: 58\n",
      "batch 199, loss: 0.2772, instance_loss: 0.6220, weighted_loss: 0.3806, label: 1, bag_size: 87\n",
      "batch 219, loss: 0.3754, instance_loss: 0.2179, weighted_loss: 0.3282, label: 0, bag_size: 55\n",
      "batch 239, loss: 2.3115, instance_loss: 2.9182, weighted_loss: 2.4935, label: 1, bag_size: 110\n",
      "batch 259, loss: 0.5273, instance_loss: 0.9648, weighted_loss: 0.6585, label: 0, bag_size: 71\n",
      "batch 279, loss: 0.2188, instance_loss: 0.3011, weighted_loss: 0.2435, label: 0, bag_size: 78\n",
      "batch 299, loss: 0.4839, instance_loss: 0.8202, weighted_loss: 0.5848, label: 1, bag_size: 40\n",
      "batch 319, loss: 1.1514, instance_loss: 0.6962, weighted_loss: 1.0148, label: 1, bag_size: 92\n",
      "batch 339, loss: 1.1034, instance_loss: 1.1539, weighted_loss: 1.1185, label: 0, bag_size: 76\n",
      "batch 359, loss: 0.7605, instance_loss: 1.1081, weighted_loss: 0.8648, label: 1, bag_size: 123\n",
      "batch 379, loss: 0.1463, instance_loss: 0.0498, weighted_loss: 0.1173, label: 0, bag_size: 60\n",
      "batch 399, loss: 1.5454, instance_loss: 1.9214, weighted_loss: 1.6582, label: 1, bag_size: 86\n",
      "batch 419, loss: 0.1582, instance_loss: 0.2803, weighted_loss: 0.1948, label: 1, bag_size: 30\n",
      "batch 439, loss: 0.4145, instance_loss: 0.5941, weighted_loss: 0.4684, label: 1, bag_size: 64\n",
      "batch 459, loss: 0.3284, instance_loss: 0.4956, weighted_loss: 0.3785, label: 1, bag_size: 79\n",
      "batch 479, loss: 0.3307, instance_loss: 0.7159, weighted_loss: 0.4463, label: 1, bag_size: 54\n",
      "batch 499, loss: 0.2299, instance_loss: 0.2998, weighted_loss: 0.2509, label: 0, bag_size: 84\n",
      "batch 519, loss: 0.1921, instance_loss: 0.4966, weighted_loss: 0.2835, label: 0, bag_size: 66\n",
      "batch 539, loss: 0.8323, instance_loss: 1.5902, weighted_loss: 1.0596, label: 1, bag_size: 90\n",
      "batch 559, loss: 0.9387, instance_loss: 0.7825, weighted_loss: 0.8918, label: 0, bag_size: 50\n",
      "batch 579, loss: 0.9432, instance_loss: 1.4079, weighted_loss: 1.0826, label: 0, bag_size: 131\n",
      "batch 599, loss: 0.4163, instance_loss: 0.3541, weighted_loss: 0.3977, label: 1, bag_size: 32\n",
      "batch 619, loss: 0.0728, instance_loss: 0.0667, weighted_loss: 0.0710, label: 1, bag_size: 90\n",
      "batch 639, loss: 1.9807, instance_loss: 2.5149, weighted_loss: 2.1410, label: 0, bag_size: 79\n",
      "batch 659, loss: 0.2832, instance_loss: 0.1583, weighted_loss: 0.2457, label: 0, bag_size: 31\n",
      "batch 679, loss: 0.3911, instance_loss: 0.6336, weighted_loss: 0.4639, label: 1, bag_size: 59\n",
      "batch 699, loss: 0.2091, instance_loss: 0.1211, weighted_loss: 0.1827, label: 1, bag_size: 140\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9393617021276596: correct 10596/11280\n",
      "class 1 clustering acc 0.34184397163120567: correct 1928/5640\n",
      "Epoch: 8, train_loss: 0.6373, train_clustering_loss:  0.7704, train_error: 0.3376\n",
      "class 0: acc 0.7050561797752809, correct 251/356\n",
      "class 1: acc 0.6189111747851003, correct 216/349\n",
      "\n",
      "Val Set, val_loss: 0.5691, val_error: 0.2625, auc: 0.7222\n",
      "class 0 clustering acc 0.95546875: correct 1223/1280\n",
      "class 1 clustering acc 0.35625: correct 228/640\n",
      "class 0: acc 0.8571428571428571, correct 42/49\n",
      "class 1: acc 0.5483870967741935, correct 17/31\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1682, instance_loss: 0.3091, weighted_loss: 0.2105, label: 0, bag_size: 89\n",
      "batch 39, loss: 0.4288, instance_loss: 0.2291, weighted_loss: 0.3689, label: 0, bag_size: 70\n",
      "batch 59, loss: 0.1749, instance_loss: 0.1352, weighted_loss: 0.1630, label: 0, bag_size: 48\n",
      "batch 79, loss: 0.5525, instance_loss: 0.7591, weighted_loss: 0.6145, label: 0, bag_size: 46\n",
      "batch 99, loss: 0.4590, instance_loss: 0.3772, weighted_loss: 0.4344, label: 1, bag_size: 53\n",
      "batch 119, loss: 1.2142, instance_loss: 1.4240, weighted_loss: 1.2771, label: 0, bag_size: 25\n",
      "batch 139, loss: 0.3805, instance_loss: 0.4976, weighted_loss: 0.4157, label: 0, bag_size: 88\n",
      "batch 159, loss: 0.0481, instance_loss: 0.0752, weighted_loss: 0.0562, label: 1, bag_size: 131\n",
      "batch 179, loss: 0.0268, instance_loss: 1.3056, weighted_loss: 0.4104, label: 1, bag_size: 28\n",
      "batch 199, loss: 0.2524, instance_loss: 0.2521, weighted_loss: 0.2523, label: 0, bag_size: 72\n",
      "batch 219, loss: 0.8233, instance_loss: 1.3598, weighted_loss: 0.9843, label: 1, bag_size: 23\n",
      "batch 239, loss: 0.1622, instance_loss: 0.1830, weighted_loss: 0.1684, label: 0, bag_size: 52\n",
      "batch 259, loss: 1.0037, instance_loss: 0.4712, weighted_loss: 0.8439, label: 1, bag_size: 76\n",
      "batch 279, loss: 0.3382, instance_loss: 0.4281, weighted_loss: 0.3652, label: 1, bag_size: 92\n",
      "batch 299, loss: 1.4461, instance_loss: 0.5349, weighted_loss: 1.1727, label: 1, bag_size: 76\n",
      "batch 319, loss: 9.1029, instance_loss: 2.0598, weighted_loss: 6.9900, label: 1, bag_size: 39\n",
      "batch 339, loss: 0.0316, instance_loss: 0.0784, weighted_loss: 0.0456, label: 1, bag_size: 120\n",
      "batch 359, loss: 0.8472, instance_loss: 1.9937, weighted_loss: 1.1912, label: 0, bag_size: 108\n",
      "batch 379, loss: 0.3172, instance_loss: 1.2021, weighted_loss: 0.5827, label: 0, bag_size: 61\n",
      "batch 399, loss: 0.4412, instance_loss: 0.2041, weighted_loss: 0.3701, label: 1, bag_size: 48\n",
      "batch 419, loss: 0.3929, instance_loss: 0.2822, weighted_loss: 0.3597, label: 1, bag_size: 69\n",
      "batch 439, loss: 0.2676, instance_loss: 0.7032, weighted_loss: 0.3983, label: 0, bag_size: 32\n",
      "batch 459, loss: 2.7108, instance_loss: 1.3401, weighted_loss: 2.2996, label: 0, bag_size: 54\n",
      "batch 479, loss: 0.2209, instance_loss: 0.5190, weighted_loss: 0.3103, label: 0, bag_size: 72\n",
      "batch 499, loss: 1.2249, instance_loss: 1.0141, weighted_loss: 1.1616, label: 0, bag_size: 80\n",
      "batch 519, loss: 0.4104, instance_loss: 0.5124, weighted_loss: 0.4410, label: 0, bag_size: 83\n",
      "batch 539, loss: 1.3724, instance_loss: 1.3087, weighted_loss: 1.3533, label: 1, bag_size: 89\n",
      "batch 559, loss: 0.3738, instance_loss: 0.6106, weighted_loss: 0.4448, label: 1, bag_size: 106\n",
      "batch 579, loss: 0.3955, instance_loss: 0.1649, weighted_loss: 0.3263, label: 1, bag_size: 48\n",
      "batch 599, loss: 0.3436, instance_loss: 1.1855, weighted_loss: 0.5962, label: 0, bag_size: 42\n",
      "batch 619, loss: 0.0769, instance_loss: 0.0483, weighted_loss: 0.0683, label: 1, bag_size: 28\n",
      "batch 639, loss: 0.6015, instance_loss: 0.3268, weighted_loss: 0.5191, label: 1, bag_size: 92\n",
      "batch 659, loss: 0.5754, instance_loss: 0.8668, weighted_loss: 0.6628, label: 0, bag_size: 33\n",
      "batch 679, loss: 0.6210, instance_loss: 0.2026, weighted_loss: 0.4955, label: 0, bag_size: 29\n",
      "batch 699, loss: 0.2500, instance_loss: 0.8567, weighted_loss: 0.4320, label: 0, bag_size: 45\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9405141843971632: correct 10609/11280\n",
      "class 1 clustering acc 0.4347517730496454: correct 2452/5640\n",
      "Epoch: 9, train_loss: 0.6592, train_clustering_loss:  0.7218, train_error: 0.3376\n",
      "class 0: acc 0.6910112359550562, correct 246/356\n",
      "class 1: acc 0.6332378223495702, correct 221/349\n",
      "\n",
      "Val Set, val_loss: 0.5397, val_error: 0.2875, auc: 0.7907\n",
      "class 0 clustering acc 0.96796875: correct 1239/1280\n",
      "class 1 clustering acc 0.2078125: correct 133/640\n",
      "class 0: acc 0.8775510204081632, correct 43/49\n",
      "class 1: acc 0.45161290322580644, correct 14/31\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6065, instance_loss: 0.1563, weighted_loss: 0.4714, label: 0, bag_size: 91\n",
      "batch 39, loss: 0.2844, instance_loss: 0.2075, weighted_loss: 0.2613, label: 0, bag_size: 29\n",
      "batch 59, loss: 1.6479, instance_loss: 2.7123, weighted_loss: 1.9672, label: 1, bag_size: 67\n",
      "batch 79, loss: 2.7982, instance_loss: 1.6528, weighted_loss: 2.4546, label: 0, bag_size: 28\n",
      "batch 99, loss: 1.4342, instance_loss: 1.8163, weighted_loss: 1.5488, label: 0, bag_size: 72\n",
      "batch 119, loss: 0.0270, instance_loss: 0.4490, weighted_loss: 0.1536, label: 0, bag_size: 109\n",
      "batch 139, loss: 1.0635, instance_loss: 0.6867, weighted_loss: 0.9505, label: 0, bag_size: 97\n",
      "batch 159, loss: 0.4605, instance_loss: 0.4925, weighted_loss: 0.4701, label: 1, bag_size: 61\n",
      "batch 179, loss: 0.2560, instance_loss: 0.2341, weighted_loss: 0.2494, label: 1, bag_size: 43\n",
      "batch 199, loss: 0.0217, instance_loss: 0.1979, weighted_loss: 0.0746, label: 0, bag_size: 77\n",
      "batch 219, loss: 1.8699, instance_loss: 1.3326, weighted_loss: 1.7087, label: 0, bag_size: 57\n",
      "batch 239, loss: 1.2559, instance_loss: 1.3191, weighted_loss: 1.2748, label: 1, bag_size: 90\n",
      "batch 259, loss: 0.9578, instance_loss: 0.8916, weighted_loss: 0.9380, label: 1, bag_size: 37\n",
      "batch 279, loss: 0.7657, instance_loss: 1.2282, weighted_loss: 0.9045, label: 0, bag_size: 102\n",
      "batch 299, loss: 0.0975, instance_loss: 0.0323, weighted_loss: 0.0779, label: 1, bag_size: 55\n",
      "batch 319, loss: 0.2530, instance_loss: 0.0922, weighted_loss: 0.2048, label: 1, bag_size: 55\n",
      "batch 339, loss: 0.0162, instance_loss: 0.0286, weighted_loss: 0.0199, label: 0, bag_size: 78\n",
      "batch 359, loss: 0.1897, instance_loss: 0.1005, weighted_loss: 0.1629, label: 0, bag_size: 60\n",
      "batch 379, loss: 0.3718, instance_loss: 0.1787, weighted_loss: 0.3139, label: 1, bag_size: 55\n",
      "batch 399, loss: 0.0615, instance_loss: 0.0517, weighted_loss: 0.0585, label: 1, bag_size: 71\n",
      "batch 419, loss: 1.0992, instance_loss: 2.6771, weighted_loss: 1.5726, label: 1, bag_size: 104\n",
      "batch 439, loss: 0.4490, instance_loss: 0.0415, weighted_loss: 0.3267, label: 1, bag_size: 105\n",
      "batch 459, loss: 0.1663, instance_loss: 0.6492, weighted_loss: 0.3112, label: 0, bag_size: 79\n",
      "batch 479, loss: 0.2148, instance_loss: 0.5009, weighted_loss: 0.3006, label: 0, bag_size: 65\n",
      "batch 499, loss: 0.3055, instance_loss: 0.9978, weighted_loss: 0.5132, label: 0, bag_size: 84\n",
      "batch 519, loss: 0.6397, instance_loss: 0.9859, weighted_loss: 0.7436, label: 0, bag_size: 93\n",
      "batch 539, loss: 0.3696, instance_loss: 0.3185, weighted_loss: 0.3543, label: 1, bag_size: 75\n",
      "batch 559, loss: 0.3886, instance_loss: 0.6162, weighted_loss: 0.4569, label: 0, bag_size: 71\n",
      "batch 579, loss: 1.4517, instance_loss: 1.9463, weighted_loss: 1.6001, label: 1, bag_size: 37\n",
      "batch 599, loss: 0.6188, instance_loss: 1.0590, weighted_loss: 0.7508, label: 0, bag_size: 95\n",
      "batch 619, loss: 0.7840, instance_loss: 1.4331, weighted_loss: 0.9787, label: 1, bag_size: 34\n",
      "batch 639, loss: 0.4488, instance_loss: 0.4639, weighted_loss: 0.4533, label: 0, bag_size: 104\n",
      "batch 659, loss: 0.1734, instance_loss: 0.5814, weighted_loss: 0.2958, label: 0, bag_size: 135\n",
      "batch 679, loss: 2.3837, instance_loss: 1.8916, weighted_loss: 2.2360, label: 0, bag_size: 84\n",
      "batch 699, loss: 0.1197, instance_loss: 0.0381, weighted_loss: 0.0953, label: 1, bag_size: 61\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9470744680851064: correct 10683/11280\n",
      "class 1 clustering acc 0.4836879432624113: correct 2728/5640\n",
      "Epoch: 10, train_loss: 0.5702, train_clustering_loss:  0.6861, train_error: 0.2879\n",
      "class 0: acc 0.7698209718670077, correct 301/391\n",
      "class 1: acc 0.6401273885350318, correct 201/314\n",
      "\n",
      "Val Set, val_loss: 0.7230, val_error: 0.3250, auc: 0.7722\n",
      "class 0 clustering acc 0.90703125: correct 1161/1280\n",
      "class 1 clustering acc 0.509375: correct 326/640\n",
      "class 0: acc 0.6122448979591837, correct 30/49\n",
      "class 1: acc 0.7741935483870968, correct 24/31\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.3799, instance_loss: 1.9701, weighted_loss: 1.5570, label: 0, bag_size: 94\n",
      "batch 39, loss: 1.6020, instance_loss: 1.6726, weighted_loss: 1.6232, label: 1, bag_size: 59\n",
      "batch 59, loss: 1.3013, instance_loss: 1.3764, weighted_loss: 1.3239, label: 0, bag_size: 27\n",
      "batch 79, loss: 0.2579, instance_loss: 0.3194, weighted_loss: 0.2763, label: 0, bag_size: 60\n",
      "batch 99, loss: 0.3772, instance_loss: 0.7375, weighted_loss: 0.4853, label: 0, bag_size: 71\n",
      "batch 119, loss: 0.4508, instance_loss: 0.2809, weighted_loss: 0.3998, label: 0, bag_size: 50\n",
      "batch 139, loss: 0.6051, instance_loss: 0.5302, weighted_loss: 0.5826, label: 0, bag_size: 44\n",
      "batch 159, loss: 0.7726, instance_loss: 0.6682, weighted_loss: 0.7413, label: 1, bag_size: 86\n",
      "batch 179, loss: 0.2997, instance_loss: 0.4195, weighted_loss: 0.3357, label: 1, bag_size: 28\n",
      "batch 199, loss: 2.0048, instance_loss: 1.1524, weighted_loss: 1.7491, label: 1, bag_size: 43\n",
      "batch 219, loss: 1.3740, instance_loss: 1.6384, weighted_loss: 1.4533, label: 1, bag_size: 11\n",
      "batch 239, loss: 0.3914, instance_loss: 0.4139, weighted_loss: 0.3981, label: 1, bag_size: 11\n",
      "batch 259, loss: 0.4102, instance_loss: 0.2137, weighted_loss: 0.3513, label: 0, bag_size: 50\n",
      "batch 279, loss: 0.0640, instance_loss: 0.1618, weighted_loss: 0.0934, label: 0, bag_size: 63\n",
      "batch 299, loss: 0.1510, instance_loss: 0.0722, weighted_loss: 0.1274, label: 0, bag_size: 95\n",
      "batch 319, loss: 0.4747, instance_loss: 1.0468, weighted_loss: 0.6463, label: 1, bag_size: 92\n",
      "batch 339, loss: 2.0502, instance_loss: 2.8702, weighted_loss: 2.2962, label: 0, bag_size: 27\n",
      "batch 359, loss: 0.0716, instance_loss: 0.5883, weighted_loss: 0.2266, label: 0, bag_size: 136\n",
      "batch 379, loss: 0.1238, instance_loss: 0.0733, weighted_loss: 0.1087, label: 1, bag_size: 70\n",
      "batch 399, loss: 0.2934, instance_loss: 0.0526, weighted_loss: 0.2212, label: 1, bag_size: 112\n",
      "batch 419, loss: 0.8066, instance_loss: 1.5611, weighted_loss: 1.0329, label: 1, bag_size: 71\n",
      "batch 439, loss: 0.3759, instance_loss: 0.6182, weighted_loss: 0.4486, label: 0, bag_size: 37\n",
      "batch 459, loss: 0.5709, instance_loss: 0.1330, weighted_loss: 0.4395, label: 0, bag_size: 54\n",
      "batch 479, loss: 0.1309, instance_loss: 1.4266, weighted_loss: 0.5196, label: 0, bag_size: 122\n",
      "batch 499, loss: 0.3052, instance_loss: 0.2357, weighted_loss: 0.2844, label: 0, bag_size: 136\n",
      "batch 519, loss: 0.3959, instance_loss: 1.2581, weighted_loss: 0.6546, label: 0, bag_size: 80\n",
      "batch 539, loss: 0.1736, instance_loss: 0.1871, weighted_loss: 0.1777, label: 0, bag_size: 52\n",
      "batch 559, loss: 0.0221, instance_loss: 0.0273, weighted_loss: 0.0237, label: 1, bag_size: 54\n",
      "batch 579, loss: 0.0176, instance_loss: 0.1258, weighted_loss: 0.0501, label: 0, bag_size: 22\n",
      "batch 599, loss: 0.3281, instance_loss: 0.5330, weighted_loss: 0.3895, label: 0, bag_size: 56\n",
      "batch 619, loss: 0.0431, instance_loss: 0.0359, weighted_loss: 0.0409, label: 1, bag_size: 92\n",
      "batch 639, loss: 2.1305, instance_loss: 1.5458, weighted_loss: 1.9551, label: 1, bag_size: 59\n",
      "batch 659, loss: 0.1456, instance_loss: 0.9645, weighted_loss: 0.3912, label: 1, bag_size: 98\n",
      "batch 679, loss: 0.0559, instance_loss: 0.3599, weighted_loss: 0.1471, label: 0, bag_size: 23\n",
      "batch 699, loss: 0.2906, instance_loss: 0.1139, weighted_loss: 0.2376, label: 0, bag_size: 92\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9389184397163121: correct 10591/11280\n",
      "class 1 clustering acc 0.5054964539007092: correct 2851/5640\n",
      "Epoch: 11, train_loss: 0.5393, train_clustering_loss:  0.6660, train_error: 0.2525\n",
      "class 0: acc 0.7952127659574468, correct 299/376\n",
      "class 1: acc 0.6930091185410334, correct 228/329\n",
      "\n",
      "Val Set, val_loss: 0.5197, val_error: 0.2250, auc: 0.8321\n",
      "class 0 clustering acc 0.94609375: correct 1211/1280\n",
      "class 1 clustering acc 0.4671875: correct 299/640\n",
      "class 0: acc 0.8571428571428571, correct 42/49\n",
      "class 1: acc 0.6451612903225806, correct 20/31\n",
      "Validation loss decreased (0.519946 --> 0.519685).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.8758, instance_loss: 0.5615, weighted_loss: 0.7815, label: 0, bag_size: 57\n",
      "batch 39, loss: 0.0095, instance_loss: 0.3512, weighted_loss: 0.1120, label: 1, bag_size: 81\n",
      "batch 59, loss: 0.0458, instance_loss: 0.0331, weighted_loss: 0.0420, label: 0, bag_size: 43\n",
      "batch 79, loss: 0.7846, instance_loss: 0.8399, weighted_loss: 0.8012, label: 1, bag_size: 28\n",
      "batch 99, loss: 1.0654, instance_loss: 0.8654, weighted_loss: 1.0054, label: 1, bag_size: 57\n",
      "batch 119, loss: 0.0187, instance_loss: 0.0160, weighted_loss: 0.0179, label: 1, bag_size: 86\n",
      "batch 139, loss: 0.6478, instance_loss: 0.6115, weighted_loss: 0.6369, label: 0, bag_size: 34\n",
      "batch 159, loss: 0.8987, instance_loss: 0.9430, weighted_loss: 0.9120, label: 1, bag_size: 55\n",
      "batch 179, loss: 0.1111, instance_loss: 0.0844, weighted_loss: 0.1031, label: 1, bag_size: 115\n",
      "batch 199, loss: 0.2365, instance_loss: 0.1844, weighted_loss: 0.2209, label: 1, bag_size: 98\n",
      "batch 219, loss: 0.6359, instance_loss: 0.2677, weighted_loss: 0.5255, label: 0, bag_size: 83\n",
      "batch 239, loss: 0.3181, instance_loss: 0.0757, weighted_loss: 0.2454, label: 1, bag_size: 41\n",
      "batch 259, loss: 0.0924, instance_loss: 0.1493, weighted_loss: 0.1095, label: 1, bag_size: 33\n",
      "batch 279, loss: 1.0943, instance_loss: 0.4568, weighted_loss: 0.9030, label: 1, bag_size: 87\n",
      "batch 299, loss: 0.4470, instance_loss: 0.2278, weighted_loss: 0.3812, label: 0, bag_size: 28\n",
      "batch 319, loss: 0.1913, instance_loss: 0.7805, weighted_loss: 0.3681, label: 1, bag_size: 25\n",
      "batch 339, loss: 0.0376, instance_loss: 0.2610, weighted_loss: 0.1046, label: 1, bag_size: 83\n",
      "batch 359, loss: 4.3047, instance_loss: 3.1790, weighted_loss: 3.9670, label: 0, bag_size: 68\n",
      "batch 379, loss: 0.1659, instance_loss: 0.4956, weighted_loss: 0.2648, label: 0, bag_size: 57\n",
      "batch 399, loss: 0.3951, instance_loss: 0.3258, weighted_loss: 0.3743, label: 0, bag_size: 100\n",
      "batch 419, loss: 1.2732, instance_loss: 1.9286, weighted_loss: 1.4698, label: 1, bag_size: 103\n",
      "batch 439, loss: 0.7767, instance_loss: 0.6385, weighted_loss: 0.7353, label: 1, bag_size: 64\n",
      "batch 459, loss: 0.4343, instance_loss: 0.7802, weighted_loss: 0.5381, label: 0, bag_size: 45\n",
      "batch 479, loss: 0.9077, instance_loss: 2.0811, weighted_loss: 1.2597, label: 1, bag_size: 86\n",
      "batch 499, loss: 0.5296, instance_loss: 0.7431, weighted_loss: 0.5937, label: 0, bag_size: 74\n",
      "batch 519, loss: 0.0916, instance_loss: 0.3896, weighted_loss: 0.1810, label: 1, bag_size: 83\n",
      "batch 539, loss: 0.0725, instance_loss: 0.0266, weighted_loss: 0.0587, label: 0, bag_size: 56\n",
      "batch 559, loss: 0.9156, instance_loss: 1.0476, weighted_loss: 0.9552, label: 1, bag_size: 33\n",
      "batch 579, loss: 0.0366, instance_loss: 0.1188, weighted_loss: 0.0612, label: 1, bag_size: 59\n",
      "batch 599, loss: 1.7859, instance_loss: 2.0896, weighted_loss: 1.8770, label: 0, bag_size: 63\n",
      "batch 619, loss: 0.1577, instance_loss: 0.6106, weighted_loss: 0.2936, label: 1, bag_size: 43\n",
      "batch 639, loss: 0.1602, instance_loss: 0.1246, weighted_loss: 0.1495, label: 0, bag_size: 73\n",
      "batch 659, loss: 0.1679, instance_loss: 0.3013, weighted_loss: 0.2079, label: 0, bag_size: 68\n",
      "batch 679, loss: 0.3880, instance_loss: 0.7581, weighted_loss: 0.4990, label: 1, bag_size: 57\n",
      "batch 699, loss: 0.3388, instance_loss: 0.4227, weighted_loss: 0.3639, label: 1, bag_size: 98\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9439716312056737: correct 10648/11280\n",
      "class 1 clustering acc 0.5182624113475177: correct 2923/5640\n",
      "Epoch: 12, train_loss: 0.5421, train_clustering_loss:  0.6582, train_error: 0.2610\n",
      "class 0: acc 0.7563739376770539, correct 267/353\n",
      "class 1: acc 0.7215909090909091, correct 254/352\n",
      "\n",
      "Val Set, val_loss: 0.5314, val_error: 0.2625, auc: 0.8486\n",
      "class 0 clustering acc 0.96171875: correct 1231/1280\n",
      "class 1 clustering acc 0.5015625: correct 321/640\n",
      "class 0: acc 0.6938775510204082, correct 34/49\n",
      "class 1: acc 0.8064516129032258, correct 25/31\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1911, instance_loss: 0.0614, weighted_loss: 0.1522, label: 0, bag_size: 47\n",
      "batch 39, loss: 0.0557, instance_loss: 0.0681, weighted_loss: 0.0594, label: 1, bag_size: 121\n",
      "batch 59, loss: 0.0762, instance_loss: 0.5789, weighted_loss: 0.2270, label: 0, bag_size: 33\n",
      "batch 79, loss: 0.7696, instance_loss: 0.0282, weighted_loss: 0.5472, label: 0, bag_size: 75\n",
      "batch 99, loss: 0.0215, instance_loss: 0.1103, weighted_loss: 0.0481, label: 1, bag_size: 51\n",
      "batch 119, loss: 0.2098, instance_loss: 1.7149, weighted_loss: 0.6614, label: 1, bag_size: 131\n",
      "batch 139, loss: 0.2040, instance_loss: 0.2984, weighted_loss: 0.2323, label: 0, bag_size: 24\n",
      "batch 159, loss: 0.0861, instance_loss: 0.2292, weighted_loss: 0.1290, label: 0, bag_size: 63\n",
      "batch 179, loss: 0.0543, instance_loss: 1.0484, weighted_loss: 0.3525, label: 0, bag_size: 44\n",
      "batch 199, loss: 0.1617, instance_loss: 0.0686, weighted_loss: 0.1338, label: 0, bag_size: 22\n",
      "batch 219, loss: 0.3129, instance_loss: 1.1450, weighted_loss: 0.5625, label: 0, bag_size: 51\n",
      "batch 239, loss: 0.2759, instance_loss: 0.9049, weighted_loss: 0.4646, label: 1, bag_size: 56\n",
      "batch 259, loss: 3.3557, instance_loss: 1.8909, weighted_loss: 2.9163, label: 0, bag_size: 33\n",
      "batch 279, loss: 0.2609, instance_loss: 0.1007, weighted_loss: 0.2129, label: 1, bag_size: 87\n",
      "batch 299, loss: 0.2008, instance_loss: 0.6357, weighted_loss: 0.3313, label: 0, bag_size: 54\n",
      "batch 319, loss: 0.2582, instance_loss: 0.2555, weighted_loss: 0.2574, label: 0, bag_size: 80\n",
      "batch 339, loss: 0.7977, instance_loss: 1.0370, weighted_loss: 0.8695, label: 1, bag_size: 46\n",
      "batch 359, loss: 0.0770, instance_loss: 0.0696, weighted_loss: 0.0748, label: 1, bag_size: 74\n",
      "batch 379, loss: 0.8373, instance_loss: 0.4851, weighted_loss: 0.7316, label: 0, bag_size: 66\n",
      "batch 399, loss: 0.1805, instance_loss: 0.2200, weighted_loss: 0.1923, label: 1, bag_size: 94\n",
      "batch 419, loss: 0.7439, instance_loss: 0.7727, weighted_loss: 0.7525, label: 0, bag_size: 88\n",
      "batch 439, loss: 0.2027, instance_loss: 0.4336, weighted_loss: 0.2719, label: 1, bag_size: 43\n",
      "batch 459, loss: 0.2679, instance_loss: 0.5976, weighted_loss: 0.3669, label: 1, bag_size: 46\n",
      "batch 479, loss: 0.1396, instance_loss: 0.0394, weighted_loss: 0.1095, label: 1, bag_size: 104\n",
      "batch 499, loss: 0.0311, instance_loss: 0.1858, weighted_loss: 0.0775, label: 0, bag_size: 43\n",
      "batch 519, loss: 0.2799, instance_loss: 0.1855, weighted_loss: 0.2516, label: 0, bag_size: 63\n",
      "batch 539, loss: 0.3473, instance_loss: 0.4469, weighted_loss: 0.3772, label: 0, bag_size: 70\n",
      "batch 559, loss: 0.1771, instance_loss: 0.2842, weighted_loss: 0.2093, label: 0, bag_size: 36\n",
      "batch 579, loss: 0.6718, instance_loss: 1.5910, weighted_loss: 0.9476, label: 0, bag_size: 14\n",
      "batch 599, loss: 0.2969, instance_loss: 1.0091, weighted_loss: 0.5106, label: 1, bag_size: 38\n",
      "batch 619, loss: 0.3001, instance_loss: 0.1444, weighted_loss: 0.2534, label: 1, bag_size: 64\n",
      "batch 639, loss: 0.3467, instance_loss: 0.5928, weighted_loss: 0.4205, label: 1, bag_size: 80\n",
      "batch 659, loss: 0.9158, instance_loss: 0.9508, weighted_loss: 0.9263, label: 1, bag_size: 43\n",
      "batch 679, loss: 0.0320, instance_loss: 0.2302, weighted_loss: 0.0915, label: 0, bag_size: 92\n",
      "batch 699, loss: 0.0795, instance_loss: 0.1187, weighted_loss: 0.0913, label: 0, bag_size: 104\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9466312056737589: correct 10678/11280\n",
      "class 1 clustering acc 0.5790780141843972: correct 3266/5640\n",
      "Epoch: 13, train_loss: 0.4854, train_clustering_loss:  0.6288, train_error: 0.2241\n",
      "class 0: acc 0.7777777777777778, correct 280/360\n",
      "class 1: acc 0.7739130434782608, correct 267/345\n",
      "\n",
      "Val Set, val_loss: 0.5761, val_error: 0.2375, auc: 0.8170\n",
      "class 0 clustering acc 0.88984375: correct 1139/1280\n",
      "class 1 clustering acc 0.63125: correct 404/640\n",
      "class 0: acc 0.8367346938775511, correct 41/49\n",
      "class 1: acc 0.6451612903225806, correct 20/31\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0151, instance_loss: 0.0102, weighted_loss: 0.0136, label: 1, bag_size: 47\n",
      "batch 39, loss: 0.4228, instance_loss: 0.2425, weighted_loss: 0.3687, label: 1, bag_size: 53\n",
      "batch 59, loss: 0.4525, instance_loss: 0.1331, weighted_loss: 0.3567, label: 0, bag_size: 77\n",
      "batch 79, loss: 0.4201, instance_loss: 0.4662, weighted_loss: 0.4339, label: 0, bag_size: 74\n",
      "batch 99, loss: 0.0084, instance_loss: 0.0362, weighted_loss: 0.0167, label: 1, bag_size: 84\n",
      "batch 119, loss: 0.0192, instance_loss: 0.0835, weighted_loss: 0.0385, label: 1, bag_size: 89\n",
      "batch 139, loss: 1.5219, instance_loss: 2.8045, weighted_loss: 1.9067, label: 1, bag_size: 89\n",
      "batch 159, loss: 0.0523, instance_loss: 0.2081, weighted_loss: 0.0991, label: 0, bag_size: 44\n",
      "batch 179, loss: 0.5257, instance_loss: 1.0140, weighted_loss: 0.6722, label: 1, bag_size: 82\n",
      "batch 199, loss: 1.4489, instance_loss: 1.9798, weighted_loss: 1.6082, label: 0, bag_size: 38\n",
      "batch 219, loss: 0.9047, instance_loss: 1.4794, weighted_loss: 1.0771, label: 1, bag_size: 40\n",
      "batch 239, loss: 0.8708, instance_loss: 0.6683, weighted_loss: 0.8101, label: 1, bag_size: 39\n",
      "batch 259, loss: 0.1913, instance_loss: 0.2254, weighted_loss: 0.2015, label: 0, bag_size: 64\n",
      "batch 279, loss: 1.5404, instance_loss: 1.8885, weighted_loss: 1.6448, label: 0, bag_size: 33\n",
      "batch 299, loss: 0.0895, instance_loss: 0.1717, weighted_loss: 0.1141, label: 1, bag_size: 27\n",
      "batch 319, loss: 0.1889, instance_loss: 0.6235, weighted_loss: 0.3193, label: 1, bag_size: 71\n",
      "batch 339, loss: 0.1206, instance_loss: 0.4169, weighted_loss: 0.2095, label: 0, bag_size: 122\n",
      "batch 359, loss: 0.6303, instance_loss: 0.9747, weighted_loss: 0.7336, label: 1, bag_size: 56\n",
      "batch 379, loss: 0.0459, instance_loss: 0.2396, weighted_loss: 0.1040, label: 1, bag_size: 95\n",
      "batch 399, loss: 0.4538, instance_loss: 0.4921, weighted_loss: 0.4653, label: 0, bag_size: 33\n",
      "batch 419, loss: 0.0275, instance_loss: 0.2933, weighted_loss: 0.1073, label: 1, bag_size: 90\n",
      "batch 439, loss: 0.7375, instance_loss: 0.3957, weighted_loss: 0.6350, label: 0, bag_size: 68\n",
      "batch 459, loss: 0.9004, instance_loss: 1.2616, weighted_loss: 1.0088, label: 0, bag_size: 22\n",
      "batch 479, loss: 0.1581, instance_loss: 0.0106, weighted_loss: 0.1138, label: 0, bag_size: 107\n",
      "batch 499, loss: 0.0888, instance_loss: 0.1668, weighted_loss: 0.1122, label: 1, bag_size: 84\n",
      "batch 519, loss: 0.0510, instance_loss: 0.0984, weighted_loss: 0.0652, label: 0, bag_size: 22\n",
      "batch 539, loss: 0.9567, instance_loss: 0.4801, weighted_loss: 0.8137, label: 0, bag_size: 54\n",
      "batch 559, loss: 0.5020, instance_loss: 0.6250, weighted_loss: 0.5389, label: 1, bag_size: 36\n",
      "batch 579, loss: 0.0553, instance_loss: 0.8378, weighted_loss: 0.2900, label: 0, bag_size: 44\n",
      "batch 599, loss: 0.0272, instance_loss: 0.0441, weighted_loss: 0.0323, label: 1, bag_size: 111\n",
      "batch 619, loss: 0.9073, instance_loss: 0.5202, weighted_loss: 0.7912, label: 0, bag_size: 122\n",
      "batch 639, loss: 0.6941, instance_loss: 0.8843, weighted_loss: 0.7512, label: 1, bag_size: 60\n",
      "batch 659, loss: 0.2358, instance_loss: 0.6655, weighted_loss: 0.3647, label: 1, bag_size: 55\n",
      "batch 679, loss: 0.7906, instance_loss: 0.5981, weighted_loss: 0.7328, label: 1, bag_size: 55\n",
      "batch 699, loss: 0.0999, instance_loss: 0.0518, weighted_loss: 0.0855, label: 1, bag_size: 69\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9444148936170212: correct 10653/11280\n",
      "class 1 clustering acc 0.6046099290780141: correct 3410/5640\n",
      "Epoch: 14, train_loss: 0.4619, train_clustering_loss:  0.5880, train_error: 0.2156\n",
      "class 0: acc 0.7851002865329513, correct 274/349\n",
      "class 1: acc 0.7837078651685393, correct 279/356\n",
      "\n",
      "Val Set, val_loss: 0.4980, val_error: 0.2500, auc: 0.8453\n",
      "class 0 clustering acc 0.91875: correct 1176/1280\n",
      "class 1 clustering acc 0.54375: correct 348/640\n",
      "class 0: acc 0.7755102040816326, correct 38/49\n",
      "class 1: acc 0.7096774193548387, correct 22/31\n",
      "Validation loss decreased (0.519685 --> 0.498014).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0608, instance_loss: 0.5872, weighted_loss: 0.2187, label: 0, bag_size: 70\n",
      "batch 39, loss: 0.1348, instance_loss: 0.1625, weighted_loss: 0.1431, label: 1, bag_size: 59\n",
      "batch 59, loss: 1.3524, instance_loss: 0.8521, weighted_loss: 1.2023, label: 0, bag_size: 74\n",
      "batch 79, loss: 0.8791, instance_loss: 0.1740, weighted_loss: 0.6676, label: 0, bag_size: 12\n",
      "batch 99, loss: 0.7546, instance_loss: 0.6069, weighted_loss: 0.7103, label: 0, bag_size: 65\n",
      "batch 119, loss: 0.1460, instance_loss: 2.3006, weighted_loss: 0.7924, label: 0, bag_size: 72\n",
      "batch 139, loss: 0.0741, instance_loss: 0.1421, weighted_loss: 0.0945, label: 0, bag_size: 52\n",
      "batch 159, loss: 0.2352, instance_loss: 0.1067, weighted_loss: 0.1966, label: 0, bag_size: 49\n",
      "batch 179, loss: 0.0236, instance_loss: 0.0750, weighted_loss: 0.0390, label: 0, bag_size: 70\n",
      "batch 199, loss: 0.8686, instance_loss: 1.1260, weighted_loss: 0.9458, label: 0, bag_size: 84\n",
      "batch 219, loss: 0.3548, instance_loss: 0.5935, weighted_loss: 0.4264, label: 0, bag_size: 42\n",
      "batch 239, loss: 0.7024, instance_loss: 0.3744, weighted_loss: 0.6040, label: 0, bag_size: 58\n",
      "batch 259, loss: 0.3275, instance_loss: 3.6860, weighted_loss: 1.3351, label: 0, bag_size: 80\n",
      "batch 279, loss: 0.1660, instance_loss: 0.0478, weighted_loss: 0.1305, label: 0, bag_size: 82\n",
      "batch 299, loss: 0.0847, instance_loss: 0.9786, weighted_loss: 0.3529, label: 1, bag_size: 25\n",
      "batch 319, loss: 0.1685, instance_loss: 0.0693, weighted_loss: 0.1388, label: 0, bag_size: 28\n",
      "batch 339, loss: 0.0641, instance_loss: 0.7735, weighted_loss: 0.2769, label: 0, bag_size: 110\n",
      "batch 359, loss: 1.1590, instance_loss: 0.6861, weighted_loss: 1.0171, label: 0, bag_size: 40\n",
      "batch 379, loss: 0.0084, instance_loss: 0.0148, weighted_loss: 0.0103, label: 1, bag_size: 28\n",
      "batch 399, loss: 0.0419, instance_loss: 0.0961, weighted_loss: 0.0582, label: 0, bag_size: 54\n",
      "batch 419, loss: 1.6968, instance_loss: 1.7021, weighted_loss: 1.6984, label: 1, bag_size: 71\n",
      "batch 439, loss: 0.1740, instance_loss: 0.2932, weighted_loss: 0.2097, label: 1, bag_size: 38\n",
      "batch 459, loss: 0.0493, instance_loss: 0.1135, weighted_loss: 0.0686, label: 0, bag_size: 29\n",
      "batch 479, loss: 0.3682, instance_loss: 1.1559, weighted_loss: 0.6045, label: 1, bag_size: 60\n",
      "batch 499, loss: 1.0110, instance_loss: 0.9390, weighted_loss: 0.9894, label: 0, bag_size: 66\n",
      "batch 519, loss: 0.1119, instance_loss: 0.4250, weighted_loss: 0.2058, label: 1, bag_size: 41\n",
      "batch 539, loss: 0.5091, instance_loss: 0.2099, weighted_loss: 0.4194, label: 1, bag_size: 31\n",
      "batch 559, loss: 0.0636, instance_loss: 0.0372, weighted_loss: 0.0557, label: 1, bag_size: 112\n",
      "batch 579, loss: 0.0747, instance_loss: 0.3088, weighted_loss: 0.1450, label: 0, bag_size: 88\n",
      "batch 599, loss: 0.4740, instance_loss: 3.0037, weighted_loss: 1.2329, label: 1, bag_size: 30\n",
      "batch 619, loss: 0.2143, instance_loss: 0.1350, weighted_loss: 0.1905, label: 1, bag_size: 52\n",
      "batch 639, loss: 0.1827, instance_loss: 0.0393, weighted_loss: 0.1396, label: 1, bag_size: 109\n",
      "batch 659, loss: 1.2088, instance_loss: 0.5647, weighted_loss: 1.0156, label: 1, bag_size: 27\n",
      "batch 679, loss: 0.8344, instance_loss: 1.3546, weighted_loss: 0.9904, label: 1, bag_size: 46\n",
      "batch 699, loss: 0.0257, instance_loss: 0.0090, weighted_loss: 0.0207, label: 1, bag_size: 55\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9486702127659574: correct 10701/11280\n",
      "class 1 clustering acc 0.6354609929078014: correct 3584/5640\n",
      "Epoch: 15, train_loss: 0.4948, train_clustering_loss:  0.5711, train_error: 0.2284\n",
      "class 0: acc 0.7855072463768116, correct 271/345\n",
      "class 1: acc 0.7583333333333333, correct 273/360\n",
      "\n",
      "Val Set, val_loss: 0.5269, val_error: 0.2625, auc: 0.8367\n",
      "class 0 clustering acc 0.93046875: correct 1191/1280\n",
      "class 1 clustering acc 0.546875: correct 350/640\n",
      "class 0: acc 0.7346938775510204, correct 36/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 2.1434, instance_loss: 2.7183, weighted_loss: 2.3159, label: 0, bag_size: 114\n",
      "batch 39, loss: 0.0557, instance_loss: 0.0654, weighted_loss: 0.0586, label: 0, bag_size: 51\n",
      "batch 59, loss: 0.2195, instance_loss: 0.3505, weighted_loss: 0.2588, label: 1, bag_size: 71\n",
      "batch 79, loss: 0.1632, instance_loss: 0.3576, weighted_loss: 0.2215, label: 0, bag_size: 81\n",
      "batch 99, loss: 0.1256, instance_loss: 0.8638, weighted_loss: 0.3470, label: 1, bag_size: 110\n",
      "batch 119, loss: 0.4017, instance_loss: 0.2293, weighted_loss: 0.3500, label: 1, bag_size: 103\n",
      "batch 139, loss: 0.1870, instance_loss: 0.0718, weighted_loss: 0.1525, label: 0, bag_size: 63\n",
      "batch 159, loss: 0.2166, instance_loss: 0.2158, weighted_loss: 0.2164, label: 1, bag_size: 43\n",
      "batch 179, loss: 0.0012, instance_loss: 0.0229, weighted_loss: 0.0077, label: 1, bag_size: 62\n",
      "batch 199, loss: 0.0304, instance_loss: 0.4762, weighted_loss: 0.1641, label: 0, bag_size: 82\n",
      "batch 219, loss: 0.3587, instance_loss: 0.3570, weighted_loss: 0.3582, label: 1, bag_size: 115\n",
      "batch 239, loss: 0.3275, instance_loss: 0.4360, weighted_loss: 0.3601, label: 0, bag_size: 102\n",
      "batch 259, loss: 0.0722, instance_loss: 0.0234, weighted_loss: 0.0575, label: 0, bag_size: 60\n",
      "batch 279, loss: 0.5118, instance_loss: 0.2415, weighted_loss: 0.4307, label: 1, bag_size: 59\n",
      "batch 299, loss: 0.4631, instance_loss: 0.5722, weighted_loss: 0.4958, label: 1, bag_size: 123\n",
      "batch 319, loss: 0.2395, instance_loss: 0.4586, weighted_loss: 0.3052, label: 0, bag_size: 76\n",
      "batch 339, loss: 0.1937, instance_loss: 0.2469, weighted_loss: 0.2096, label: 0, bag_size: 32\n",
      "batch 359, loss: 0.1857, instance_loss: 0.2841, weighted_loss: 0.2152, label: 1, bag_size: 71\n",
      "batch 379, loss: 0.5765, instance_loss: 1.3124, weighted_loss: 0.7973, label: 1, bag_size: 38\n",
      "batch 399, loss: 0.7643, instance_loss: 0.1835, weighted_loss: 0.5900, label: 1, bag_size: 95\n",
      "batch 419, loss: 0.0099, instance_loss: 0.0274, weighted_loss: 0.0152, label: 1, bag_size: 31\n",
      "batch 439, loss: 1.7106, instance_loss: 1.7309, weighted_loss: 1.7167, label: 1, bag_size: 90\n",
      "batch 459, loss: 0.3377, instance_loss: 0.0666, weighted_loss: 0.2564, label: 1, bag_size: 30\n",
      "batch 479, loss: 0.7309, instance_loss: 0.3284, weighted_loss: 0.6102, label: 0, bag_size: 49\n",
      "batch 499, loss: 0.2498, instance_loss: 0.3032, weighted_loss: 0.2658, label: 0, bag_size: 25\n",
      "batch 519, loss: 0.0953, instance_loss: 0.0175, weighted_loss: 0.0719, label: 1, bag_size: 102\n",
      "batch 539, loss: 0.1735, instance_loss: 0.1101, weighted_loss: 0.1544, label: 0, bag_size: 53\n",
      "batch 559, loss: 0.0048, instance_loss: 0.0069, weighted_loss: 0.0054, label: 1, bag_size: 47\n",
      "batch 579, loss: 0.4067, instance_loss: 0.4730, weighted_loss: 0.4266, label: 0, bag_size: 43\n",
      "batch 599, loss: 0.1009, instance_loss: 0.0852, weighted_loss: 0.0962, label: 0, bag_size: 74\n",
      "batch 619, loss: 0.2583, instance_loss: 0.0471, weighted_loss: 0.1950, label: 1, bag_size: 59\n",
      "batch 639, loss: 0.0207, instance_loss: 0.4623, weighted_loss: 0.1532, label: 0, bag_size: 43\n",
      "batch 659, loss: 0.0492, instance_loss: 0.0946, weighted_loss: 0.0628, label: 0, bag_size: 82\n",
      "batch 679, loss: 0.0478, instance_loss: 0.1217, weighted_loss: 0.0700, label: 0, bag_size: 91\n",
      "batch 699, loss: 0.1628, instance_loss: 0.0528, weighted_loss: 0.1298, label: 0, bag_size: 59\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9451241134751773: correct 10661/11280\n",
      "class 1 clustering acc 0.6326241134751773: correct 3568/5640\n",
      "Epoch: 16, train_loss: 0.4465, train_clustering_loss:  0.5630, train_error: 0.2085\n",
      "class 0: acc 0.8087431693989071, correct 296/366\n",
      "class 1: acc 0.7728613569321534, correct 262/339\n",
      "\n",
      "Val Set, val_loss: 0.4930, val_error: 0.2250, auc: 0.8460\n",
      "class 0 clustering acc 0.95625: correct 1224/1280\n",
      "class 1 clustering acc 0.4671875: correct 299/640\n",
      "class 0: acc 0.8163265306122449, correct 40/49\n",
      "class 1: acc 0.7096774193548387, correct 22/31\n",
      "Validation loss decreased (0.498014 --> 0.493028).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1939, instance_loss: 0.5917, weighted_loss: 0.3132, label: 1, bag_size: 46\n",
      "batch 39, loss: 0.0332, instance_loss: 0.2752, weighted_loss: 0.1058, label: 0, bag_size: 85\n",
      "batch 59, loss: 0.2697, instance_loss: 0.8646, weighted_loss: 0.4482, label: 0, bag_size: 74\n",
      "batch 79, loss: 0.0695, instance_loss: 0.0253, weighted_loss: 0.0562, label: 1, bag_size: 77\n",
      "batch 99, loss: 0.0765, instance_loss: 0.0842, weighted_loss: 0.0788, label: 0, bag_size: 28\n",
      "batch 119, loss: 0.2759, instance_loss: 0.0678, weighted_loss: 0.2135, label: 1, bag_size: 106\n",
      "batch 139, loss: 0.0881, instance_loss: 0.0308, weighted_loss: 0.0709, label: 0, bag_size: 68\n",
      "batch 159, loss: 0.0142, instance_loss: 0.0043, weighted_loss: 0.0112, label: 1, bag_size: 74\n",
      "batch 179, loss: 0.8916, instance_loss: 0.6867, weighted_loss: 0.8301, label: 0, bag_size: 52\n",
      "batch 199, loss: 1.8599, instance_loss: 0.9832, weighted_loss: 1.5969, label: 1, bag_size: 69\n",
      "batch 219, loss: 0.2149, instance_loss: 0.0922, weighted_loss: 0.1781, label: 0, bag_size: 47\n",
      "batch 239, loss: 0.2425, instance_loss: 0.0950, weighted_loss: 0.1983, label: 1, bag_size: 55\n",
      "batch 259, loss: 0.0014, instance_loss: 0.0777, weighted_loss: 0.0243, label: 0, bag_size: 96\n",
      "batch 279, loss: 0.0471, instance_loss: 0.1235, weighted_loss: 0.0700, label: 1, bag_size: 59\n",
      "batch 299, loss: 0.9944, instance_loss: 1.6101, weighted_loss: 1.1791, label: 0, bag_size: 96\n",
      "batch 319, loss: 0.2867, instance_loss: 0.0896, weighted_loss: 0.2276, label: 1, bag_size: 53\n",
      "batch 339, loss: 0.1162, instance_loss: 0.0803, weighted_loss: 0.1054, label: 1, bag_size: 59\n",
      "batch 359, loss: 0.0082, instance_loss: 0.0016, weighted_loss: 0.0062, label: 0, bag_size: 71\n",
      "batch 379, loss: 0.0092, instance_loss: 0.0166, weighted_loss: 0.0114, label: 1, bag_size: 70\n",
      "batch 399, loss: 0.0211, instance_loss: 0.0288, weighted_loss: 0.0234, label: 0, bag_size: 35\n",
      "batch 419, loss: 0.0176, instance_loss: 0.0750, weighted_loss: 0.0348, label: 0, bag_size: 67\n",
      "batch 439, loss: 0.0510, instance_loss: 0.2979, weighted_loss: 0.1251, label: 0, bag_size: 21\n",
      "batch 459, loss: 0.0331, instance_loss: 0.0284, weighted_loss: 0.0317, label: 1, bag_size: 80\n",
      "batch 479, loss: 0.7549, instance_loss: 0.2401, weighted_loss: 0.6004, label: 0, bag_size: 87\n",
      "batch 499, loss: 0.6425, instance_loss: 0.3524, weighted_loss: 0.5555, label: 0, bag_size: 65\n",
      "batch 519, loss: 0.1795, instance_loss: 0.8813, weighted_loss: 0.3901, label: 0, bag_size: 45\n",
      "batch 539, loss: 0.1997, instance_loss: 0.1002, weighted_loss: 0.1699, label: 0, bag_size: 62\n",
      "batch 559, loss: 0.1104, instance_loss: 0.4972, weighted_loss: 0.2265, label: 0, bag_size: 56\n",
      "batch 579, loss: 0.4933, instance_loss: 0.2111, weighted_loss: 0.4086, label: 0, bag_size: 62\n",
      "batch 599, loss: 0.1057, instance_loss: 0.2973, weighted_loss: 0.1632, label: 0, bag_size: 59\n",
      "batch 619, loss: 0.0641, instance_loss: 0.0316, weighted_loss: 0.0543, label: 1, bag_size: 62\n",
      "batch 639, loss: 0.0513, instance_loss: 0.2871, weighted_loss: 0.1220, label: 0, bag_size: 38\n",
      "batch 659, loss: 0.8411, instance_loss: 0.1982, weighted_loss: 0.6483, label: 0, bag_size: 59\n",
      "batch 679, loss: 0.0206, instance_loss: 0.0083, weighted_loss: 0.0169, label: 1, bag_size: 104\n",
      "batch 699, loss: 0.0947, instance_loss: 0.5249, weighted_loss: 0.2238, label: 1, bag_size: 37\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9535460992907802: correct 10756/11280\n",
      "class 1 clustering acc 0.699822695035461: correct 3947/5640\n",
      "Epoch: 17, train_loss: 0.3977, train_clustering_loss:  0.4773, train_error: 0.1830\n",
      "class 0: acc 0.8231884057971014, correct 284/345\n",
      "class 1: acc 0.8111111111111111, correct 292/360\n",
      "\n",
      "Val Set, val_loss: 0.4871, val_error: 0.1875, auc: 0.8506\n",
      "class 0 clustering acc 0.9234375: correct 1182/1280\n",
      "class 1 clustering acc 0.540625: correct 346/640\n",
      "class 0: acc 0.8571428571428571, correct 42/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "Validation loss decreased (0.493028 --> 0.487063).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1547, instance_loss: 0.7425, weighted_loss: 0.3311, label: 1, bag_size: 84\n",
      "batch 39, loss: 0.0389, instance_loss: 0.0181, weighted_loss: 0.0326, label: 1, bag_size: 69\n",
      "batch 59, loss: 1.3007, instance_loss: 2.9013, weighted_loss: 1.7809, label: 1, bag_size: 73\n",
      "batch 79, loss: 0.5500, instance_loss: 0.0207, weighted_loss: 0.3912, label: 1, bag_size: 102\n",
      "batch 99, loss: 0.0682, instance_loss: 0.0053, weighted_loss: 0.0493, label: 1, bag_size: 109\n",
      "batch 119, loss: 1.5364, instance_loss: 0.6148, weighted_loss: 1.2599, label: 0, bag_size: 45\n",
      "batch 139, loss: 0.2840, instance_loss: 0.4081, weighted_loss: 0.3212, label: 0, bag_size: 61\n",
      "batch 159, loss: 0.0189, instance_loss: 0.0041, weighted_loss: 0.0145, label: 1, bag_size: 94\n",
      "batch 179, loss: 1.2072, instance_loss: 0.8317, weighted_loss: 1.0946, label: 0, bag_size: 67\n",
      "batch 199, loss: 0.0346, instance_loss: 0.0342, weighted_loss: 0.0345, label: 1, bag_size: 28\n",
      "batch 219, loss: 0.0404, instance_loss: 0.0435, weighted_loss: 0.0414, label: 0, bag_size: 89\n",
      "batch 239, loss: 0.0833, instance_loss: 0.6418, weighted_loss: 0.2509, label: 0, bag_size: 65\n",
      "batch 259, loss: 0.0414, instance_loss: 0.7243, weighted_loss: 0.2463, label: 0, bag_size: 65\n",
      "batch 279, loss: 0.0354, instance_loss: 1.3368, weighted_loss: 0.4258, label: 1, bag_size: 103\n",
      "batch 299, loss: 0.0570, instance_loss: 0.4159, weighted_loss: 0.1647, label: 0, bag_size: 101\n",
      "batch 319, loss: 0.0174, instance_loss: 0.1749, weighted_loss: 0.0646, label: 0, bag_size: 43\n",
      "batch 339, loss: 0.0721, instance_loss: 0.0916, weighted_loss: 0.0780, label: 1, bag_size: 102\n",
      "batch 359, loss: 0.8089, instance_loss: 1.1293, weighted_loss: 0.9050, label: 0, bag_size: 56\n",
      "batch 379, loss: 1.8864, instance_loss: 2.7732, weighted_loss: 2.1524, label: 1, bag_size: 67\n",
      "batch 399, loss: 0.0022, instance_loss: 0.0168, weighted_loss: 0.0065, label: 1, bag_size: 94\n",
      "batch 419, loss: 0.2006, instance_loss: 1.0190, weighted_loss: 0.4461, label: 1, bag_size: 64\n",
      "batch 439, loss: 0.7588, instance_loss: 0.7375, weighted_loss: 0.7524, label: 0, bag_size: 66\n",
      "batch 459, loss: 0.0139, instance_loss: 0.0116, weighted_loss: 0.0132, label: 1, bag_size: 32\n",
      "batch 479, loss: 0.0992, instance_loss: 0.1075, weighted_loss: 0.1017, label: 1, bag_size: 118\n",
      "batch 499, loss: 0.3573, instance_loss: 0.3371, weighted_loss: 0.3512, label: 0, bag_size: 64\n",
      "batch 519, loss: 0.6671, instance_loss: 0.3317, weighted_loss: 0.5665, label: 1, bag_size: 81\n",
      "batch 539, loss: 0.0077, instance_loss: 0.0184, weighted_loss: 0.0109, label: 1, bag_size: 61\n",
      "batch 559, loss: 0.1219, instance_loss: 0.1046, weighted_loss: 0.1167, label: 1, bag_size: 84\n",
      "batch 579, loss: 0.8564, instance_loss: 0.6740, weighted_loss: 0.8017, label: 0, bag_size: 83\n",
      "batch 599, loss: 1.0943, instance_loss: 1.2413, weighted_loss: 1.1384, label: 1, bag_size: 84\n",
      "batch 619, loss: 0.1398, instance_loss: 0.0346, weighted_loss: 0.1083, label: 1, bag_size: 54\n",
      "batch 639, loss: 1.2327, instance_loss: 2.6211, weighted_loss: 1.6492, label: 0, bag_size: 84\n",
      "batch 659, loss: 0.0241, instance_loss: 0.4544, weighted_loss: 0.1532, label: 0, bag_size: 46\n",
      "batch 679, loss: 0.1054, instance_loss: 0.0747, weighted_loss: 0.0962, label: 1, bag_size: 38\n",
      "batch 699, loss: 0.1867, instance_loss: 1.6340, weighted_loss: 0.6209, label: 0, bag_size: 84\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9537234042553191: correct 10758/11280\n",
      "class 1 clustering acc 0.6363475177304965: correct 3589/5640\n",
      "Epoch: 18, train_loss: 0.4052, train_clustering_loss:  0.5291, train_error: 0.2014\n",
      "class 0: acc 0.804093567251462, correct 275/342\n",
      "class 1: acc 0.7933884297520661, correct 288/363\n",
      "\n",
      "Val Set, val_loss: 0.4974, val_error: 0.2000, auc: 0.8716\n",
      "class 0 clustering acc 0.91640625: correct 1173/1280\n",
      "class 1 clustering acc 0.628125: correct 402/640\n",
      "class 0: acc 0.8571428571428571, correct 42/49\n",
      "class 1: acc 0.7096774193548387, correct 22/31\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 5.9263, instance_loss: 2.2632, weighted_loss: 4.8274, label: 0, bag_size: 52\n",
      "batch 39, loss: 1.1732, instance_loss: 0.7373, weighted_loss: 1.0424, label: 1, bag_size: 66\n",
      "batch 59, loss: 0.0287, instance_loss: 0.0099, weighted_loss: 0.0231, label: 1, bag_size: 109\n",
      "batch 79, loss: 0.1253, instance_loss: 0.0543, weighted_loss: 0.1040, label: 1, bag_size: 87\n",
      "batch 99, loss: 0.0527, instance_loss: 0.5333, weighted_loss: 0.1969, label: 1, bag_size: 76\n",
      "batch 119, loss: 0.0050, instance_loss: 0.0032, weighted_loss: 0.0044, label: 0, bag_size: 60\n",
      "batch 139, loss: 0.3161, instance_loss: 0.4265, weighted_loss: 0.3492, label: 1, bag_size: 69\n",
      "batch 159, loss: 0.1238, instance_loss: 0.3443, weighted_loss: 0.1899, label: 0, bag_size: 76\n",
      "batch 179, loss: 0.1462, instance_loss: 1.0409, weighted_loss: 0.4146, label: 1, bag_size: 43\n",
      "batch 199, loss: 0.1571, instance_loss: 0.4394, weighted_loss: 0.2418, label: 1, bag_size: 90\n",
      "batch 219, loss: 0.0188, instance_loss: 0.0569, weighted_loss: 0.0302, label: 1, bag_size: 64\n",
      "batch 239, loss: 0.5022, instance_loss: 1.0340, weighted_loss: 0.6617, label: 1, bag_size: 79\n",
      "batch 259, loss: 3.4152, instance_loss: 2.1045, weighted_loss: 3.0220, label: 1, bag_size: 126\n",
      "batch 279, loss: 0.6782, instance_loss: 0.1895, weighted_loss: 0.5316, label: 0, bag_size: 82\n",
      "batch 299, loss: 0.0035, instance_loss: 0.0411, weighted_loss: 0.0148, label: 1, bag_size: 33\n",
      "batch 319, loss: 0.2108, instance_loss: 0.1871, weighted_loss: 0.2037, label: 0, bag_size: 74\n",
      "batch 339, loss: 0.0015, instance_loss: 0.0030, weighted_loss: 0.0020, label: 1, bag_size: 92\n",
      "batch 359, loss: 0.0923, instance_loss: 0.0119, weighted_loss: 0.0681, label: 1, bag_size: 62\n",
      "batch 379, loss: 0.0956, instance_loss: 0.3572, weighted_loss: 0.1740, label: 0, bag_size: 87\n",
      "batch 399, loss: 0.0069, instance_loss: 0.0426, weighted_loss: 0.0176, label: 0, bag_size: 101\n",
      "batch 419, loss: 0.1968, instance_loss: 0.4665, weighted_loss: 0.2777, label: 0, bag_size: 68\n",
      "batch 439, loss: 0.0854, instance_loss: 0.0120, weighted_loss: 0.0634, label: 1, bag_size: 52\n",
      "batch 459, loss: 0.4805, instance_loss: 1.8397, weighted_loss: 0.8883, label: 1, bag_size: 104\n",
      "batch 479, loss: 0.0031, instance_loss: 0.0053, weighted_loss: 0.0037, label: 1, bag_size: 83\n",
      "batch 499, loss: 0.2350, instance_loss: 0.0006, weighted_loss: 0.1647, label: 1, bag_size: 37\n",
      "batch 519, loss: 0.2884, instance_loss: 0.0253, weighted_loss: 0.2094, label: 0, bag_size: 109\n",
      "batch 539, loss: 0.0346, instance_loss: 0.2235, weighted_loss: 0.0913, label: 0, bag_size: 61\n",
      "batch 559, loss: 0.0653, instance_loss: 0.3892, weighted_loss: 0.1625, label: 0, bag_size: 72\n",
      "batch 579, loss: 0.1715, instance_loss: 1.4108, weighted_loss: 0.5433, label: 1, bag_size: 75\n",
      "batch 599, loss: 1.1027, instance_loss: 0.1122, weighted_loss: 0.8056, label: 1, bag_size: 65\n",
      "batch 619, loss: 0.1332, instance_loss: 0.2926, weighted_loss: 0.1810, label: 0, bag_size: 78\n",
      "batch 639, loss: 0.4452, instance_loss: 2.8546, weighted_loss: 1.1680, label: 1, bag_size: 73\n",
      "batch 659, loss: 0.0385, instance_loss: 0.0099, weighted_loss: 0.0299, label: 1, bag_size: 47\n",
      "batch 679, loss: 0.0667, instance_loss: 0.0165, weighted_loss: 0.0517, label: 0, bag_size: 57\n",
      "batch 699, loss: 0.4050, instance_loss: 0.2640, weighted_loss: 0.3627, label: 0, bag_size: 73\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.951950354609929: correct 10738/11280\n",
      "class 1 clustering acc 0.7088652482269504: correct 3998/5640\n",
      "Epoch: 19, train_loss: 0.3574, train_clustering_loss:  0.4850, train_error: 0.1574\n",
      "class 0: acc 0.8475073313782991, correct 289/341\n",
      "class 1: acc 0.8379120879120879, correct 305/364\n",
      "\n",
      "Val Set, val_loss: 0.7792, val_error: 0.2500, auc: 0.8558\n",
      "class 0 clustering acc 0.90390625: correct 1157/1280\n",
      "class 1 clustering acc 0.665625: correct 426/640\n",
      "class 0: acc 0.9387755102040817, correct 46/49\n",
      "class 1: acc 0.45161290322580644, correct 14/31\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0007, instance_loss: 0.0033, weighted_loss: 0.0015, label: 1, bag_size: 105\n",
      "batch 39, loss: 0.0223, instance_loss: 0.0091, weighted_loss: 0.0183, label: 1, bag_size: 71\n",
      "batch 59, loss: 0.0130, instance_loss: 0.1914, weighted_loss: 0.0665, label: 0, bag_size: 60\n",
      "batch 79, loss: 1.5497, instance_loss: 1.0315, weighted_loss: 1.3943, label: 0, bag_size: 33\n",
      "batch 99, loss: 0.0156, instance_loss: 0.1219, weighted_loss: 0.0475, label: 1, bag_size: 71\n",
      "batch 119, loss: 0.1889, instance_loss: 1.0539, weighted_loss: 0.4484, label: 0, bag_size: 74\n",
      "batch 139, loss: 0.4420, instance_loss: 0.2352, weighted_loss: 0.3799, label: 1, bag_size: 118\n",
      "batch 159, loss: 0.0373, instance_loss: 0.0245, weighted_loss: 0.0335, label: 0, bag_size: 76\n",
      "batch 179, loss: 0.2057, instance_loss: 0.1738, weighted_loss: 0.1961, label: 1, bag_size: 55\n",
      "batch 199, loss: 0.3708, instance_loss: 0.3683, weighted_loss: 0.3700, label: 0, bag_size: 18\n",
      "batch 219, loss: 0.0282, instance_loss: 0.0450, weighted_loss: 0.0332, label: 1, bag_size: 60\n",
      "batch 239, loss: 0.0855, instance_loss: 0.3753, weighted_loss: 0.1725, label: 0, bag_size: 80\n",
      "batch 259, loss: 0.0081, instance_loss: 0.4568, weighted_loss: 0.1427, label: 1, bag_size: 30\n",
      "batch 279, loss: 0.0393, instance_loss: 0.0982, weighted_loss: 0.0570, label: 0, bag_size: 101\n",
      "batch 299, loss: 0.9995, instance_loss: 1.5402, weighted_loss: 1.1617, label: 1, bag_size: 11\n",
      "batch 319, loss: 0.0919, instance_loss: 0.1851, weighted_loss: 0.1199, label: 0, bag_size: 63\n",
      "batch 339, loss: 2.8296, instance_loss: 0.4570, weighted_loss: 2.1179, label: 0, bag_size: 54\n",
      "batch 359, loss: 0.0658, instance_loss: 0.0701, weighted_loss: 0.0671, label: 1, bag_size: 76\n",
      "batch 379, loss: 4.7861, instance_loss: 1.6683, weighted_loss: 3.8508, label: 1, bag_size: 93\n",
      "batch 399, loss: 0.8422, instance_loss: 0.9282, weighted_loss: 0.8680, label: 0, bag_size: 43\n",
      "batch 419, loss: 0.7381, instance_loss: 1.1319, weighted_loss: 0.8562, label: 1, bag_size: 37\n",
      "batch 439, loss: 0.0005, instance_loss: 0.0024, weighted_loss: 0.0011, label: 1, bag_size: 105\n",
      "batch 459, loss: 0.9126, instance_loss: 0.0540, weighted_loss: 0.6550, label: 1, bag_size: 40\n",
      "batch 479, loss: 0.2275, instance_loss: 0.9114, weighted_loss: 0.4326, label: 1, bag_size: 66\n",
      "batch 499, loss: 0.1937, instance_loss: 0.1023, weighted_loss: 0.1663, label: 0, bag_size: 89\n",
      "batch 519, loss: 0.0053, instance_loss: 0.0063, weighted_loss: 0.0056, label: 1, bag_size: 92\n",
      "batch 539, loss: 0.0386, instance_loss: 0.3613, weighted_loss: 0.1354, label: 0, bag_size: 56\n",
      "batch 559, loss: 0.2256, instance_loss: 0.7751, weighted_loss: 0.3905, label: 0, bag_size: 40\n",
      "batch 579, loss: 0.4443, instance_loss: 0.3503, weighted_loss: 0.4161, label: 0, bag_size: 49\n",
      "batch 599, loss: 0.0789, instance_loss: 0.2700, weighted_loss: 0.1362, label: 1, bag_size: 51\n",
      "batch 619, loss: 0.0788, instance_loss: 0.0402, weighted_loss: 0.0672, label: 1, bag_size: 43\n",
      "batch 639, loss: 0.0015, instance_loss: 0.0155, weighted_loss: 0.0057, label: 1, bag_size: 37\n",
      "batch 659, loss: 0.1141, instance_loss: 0.0856, weighted_loss: 0.1056, label: 0, bag_size: 73\n",
      "batch 679, loss: 0.7387, instance_loss: 0.3241, weighted_loss: 0.6144, label: 0, bag_size: 66\n",
      "batch 699, loss: 0.0250, instance_loss: 0.0154, weighted_loss: 0.0221, label: 1, bag_size: 55\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9508865248226951: correct 10726/11280\n",
      "class 1 clustering acc 0.675531914893617: correct 3810/5640\n",
      "Epoch: 20, train_loss: 0.3710, train_clustering_loss:  0.5015, train_error: 0.1745\n",
      "class 0: acc 0.8197674418604651, correct 282/344\n",
      "class 1: acc 0.8310249307479224, correct 300/361\n",
      "\n",
      "Val Set, val_loss: 0.4758, val_error: 0.2125, auc: 0.8749\n",
      "class 0 clustering acc 0.93984375: correct 1203/1280\n",
      "class 1 clustering acc 0.690625: correct 442/640\n",
      "class 0: acc 0.8367346938775511, correct 41/49\n",
      "class 1: acc 0.7096774193548387, correct 22/31\n",
      "Validation loss decreased (0.487063 --> 0.475825).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5041, instance_loss: 0.2838, weighted_loss: 0.4380, label: 0, bag_size: 62\n",
      "batch 39, loss: 1.1164, instance_loss: 0.6153, weighted_loss: 0.9661, label: 0, bag_size: 73\n",
      "batch 59, loss: 0.7460, instance_loss: 0.2741, weighted_loss: 0.6044, label: 1, bag_size: 84\n",
      "batch 79, loss: 1.3104, instance_loss: 0.5528, weighted_loss: 1.0831, label: 0, bag_size: 102\n",
      "batch 99, loss: 0.0091, instance_loss: 0.8649, weighted_loss: 0.2658, label: 0, bag_size: 49\n",
      "batch 119, loss: 0.0228, instance_loss: 0.0037, weighted_loss: 0.0171, label: 1, bag_size: 25\n",
      "batch 139, loss: 0.0040, instance_loss: 0.2488, weighted_loss: 0.0774, label: 1, bag_size: 66\n",
      "batch 159, loss: 0.0314, instance_loss: 0.1803, weighted_loss: 0.0760, label: 0, bag_size: 28\n",
      "batch 179, loss: 0.0056, instance_loss: 0.0026, weighted_loss: 0.0047, label: 0, bag_size: 78\n",
      "batch 199, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 76\n",
      "batch 219, loss: 0.6371, instance_loss: 0.4617, weighted_loss: 0.5845, label: 0, bag_size: 82\n",
      "batch 239, loss: 0.0419, instance_loss: 0.0394, weighted_loss: 0.0411, label: 1, bag_size: 81\n",
      "batch 259, loss: 0.1484, instance_loss: 0.5701, weighted_loss: 0.2749, label: 1, bag_size: 46\n",
      "batch 279, loss: 0.0033, instance_loss: 0.0105, weighted_loss: 0.0055, label: 1, bag_size: 61\n",
      "batch 299, loss: 0.1947, instance_loss: 0.4935, weighted_loss: 0.2843, label: 0, bag_size: 45\n",
      "batch 319, loss: 0.0143, instance_loss: 0.0082, weighted_loss: 0.0125, label: 1, bag_size: 70\n",
      "batch 339, loss: 0.0166, instance_loss: 0.0735, weighted_loss: 0.0337, label: 0, bag_size: 88\n",
      "batch 359, loss: 0.1565, instance_loss: 0.1694, weighted_loss: 0.1603, label: 1, bag_size: 55\n",
      "batch 379, loss: 2.6000, instance_loss: 2.9126, weighted_loss: 2.6938, label: 0, bag_size: 46\n",
      "batch 399, loss: 0.5285, instance_loss: 0.2972, weighted_loss: 0.4591, label: 0, bag_size: 61\n",
      "batch 419, loss: 0.1000, instance_loss: 0.7005, weighted_loss: 0.2802, label: 1, bag_size: 80\n",
      "batch 439, loss: 0.4689, instance_loss: 0.1131, weighted_loss: 0.3622, label: 1, bag_size: 44\n",
      "batch 459, loss: 0.4174, instance_loss: 3.2337, weighted_loss: 1.2623, label: 0, bag_size: 63\n",
      "batch 479, loss: 0.2496, instance_loss: 0.0162, weighted_loss: 0.1796, label: 1, bag_size: 34\n",
      "batch 499, loss: 0.0021, instance_loss: 0.2804, weighted_loss: 0.0856, label: 0, bag_size: 77\n",
      "batch 519, loss: 2.8027, instance_loss: 1.9292, weighted_loss: 2.5406, label: 1, bag_size: 77\n",
      "batch 539, loss: 0.2828, instance_loss: 0.1220, weighted_loss: 0.2346, label: 1, bag_size: 25\n",
      "batch 559, loss: 0.0776, instance_loss: 0.1144, weighted_loss: 0.0886, label: 1, bag_size: 53\n",
      "batch 579, loss: 0.1264, instance_loss: 0.2376, weighted_loss: 0.1598, label: 0, bag_size: 91\n",
      "batch 599, loss: 0.0032, instance_loss: 0.0110, weighted_loss: 0.0055, label: 1, bag_size: 84\n",
      "batch 619, loss: 0.9284, instance_loss: 1.3673, weighted_loss: 1.0600, label: 0, bag_size: 80\n",
      "batch 639, loss: 2.8266, instance_loss: 2.3369, weighted_loss: 2.6797, label: 0, bag_size: 58\n",
      "batch 659, loss: 0.8763, instance_loss: 0.8006, weighted_loss: 0.8536, label: 1, bag_size: 62\n",
      "batch 679, loss: 0.0301, instance_loss: 0.0279, weighted_loss: 0.0295, label: 1, bag_size: 95\n",
      "batch 699, loss: 0.2299, instance_loss: 0.0909, weighted_loss: 0.1882, label: 0, bag_size: 106\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.949468085106383: correct 10710/11280\n",
      "class 1 clustering acc 0.7049645390070922: correct 3976/5640\n",
      "Epoch: 21, train_loss: 0.4041, train_clustering_loss:  0.4870, train_error: 0.1716\n",
      "class 0: acc 0.8244047619047619, correct 277/336\n",
      "class 1: acc 0.8319783197831978, correct 307/369\n",
      "\n",
      "Val Set, val_loss: 0.4004, val_error: 0.1875, auc: 0.8966\n",
      "class 0 clustering acc 0.90859375: correct 1163/1280\n",
      "class 1 clustering acc 0.6859375: correct 439/640\n",
      "class 0: acc 0.8367346938775511, correct 41/49\n",
      "class 1: acc 0.7741935483870968, correct 24/31\n",
      "Validation loss decreased (0.475825 --> 0.400438).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2747, instance_loss: 0.2149, weighted_loss: 0.2568, label: 1, bag_size: 48\n",
      "batch 39, loss: 0.0460, instance_loss: 0.1863, weighted_loss: 0.0881, label: 0, bag_size: 84\n",
      "batch 59, loss: 0.1080, instance_loss: 0.1672, weighted_loss: 0.1258, label: 0, bag_size: 50\n",
      "batch 79, loss: 0.0175, instance_loss: 0.0127, weighted_loss: 0.0161, label: 0, bag_size: 69\n",
      "batch 99, loss: 0.0026, instance_loss: 0.0222, weighted_loss: 0.0085, label: 1, bag_size: 62\n",
      "batch 119, loss: 0.1081, instance_loss: 0.0693, weighted_loss: 0.0965, label: 1, bag_size: 54\n",
      "batch 139, loss: 0.0305, instance_loss: 0.0045, weighted_loss: 0.0227, label: 1, bag_size: 103\n",
      "batch 159, loss: 0.1073, instance_loss: 1.5517, weighted_loss: 0.5406, label: 1, bag_size: 77\n",
      "batch 179, loss: 0.0005, instance_loss: 0.0010, weighted_loss: 0.0006, label: 0, bag_size: 96\n",
      "batch 199, loss: 0.0067, instance_loss: 0.7399, weighted_loss: 0.2266, label: 0, bag_size: 67\n",
      "batch 219, loss: 1.6980, instance_loss: 0.9040, weighted_loss: 1.4598, label: 0, bag_size: 86\n",
      "batch 239, loss: 0.1581, instance_loss: 0.4972, weighted_loss: 0.2598, label: 1, bag_size: 93\n",
      "batch 259, loss: 0.9745, instance_loss: 1.8258, weighted_loss: 1.2299, label: 1, bag_size: 93\n",
      "batch 279, loss: 0.3070, instance_loss: 0.9409, weighted_loss: 0.4972, label: 1, bag_size: 98\n",
      "batch 299, loss: 1.1225, instance_loss: 0.8433, weighted_loss: 1.0387, label: 1, bag_size: 57\n",
      "batch 319, loss: 1.1305, instance_loss: 0.0943, weighted_loss: 0.8197, label: 0, bag_size: 42\n",
      "batch 339, loss: 0.0094, instance_loss: 0.0697, weighted_loss: 0.0275, label: 0, bag_size: 129\n",
      "batch 359, loss: 0.0675, instance_loss: 0.0226, weighted_loss: 0.0540, label: 0, bag_size: 104\n",
      "batch 379, loss: 0.0202, instance_loss: 0.0088, weighted_loss: 0.0168, label: 0, bag_size: 71\n",
      "batch 399, loss: 0.4183, instance_loss: 0.1342, weighted_loss: 0.3331, label: 1, bag_size: 46\n",
      "batch 419, loss: 0.0285, instance_loss: 0.0079, weighted_loss: 0.0223, label: 1, bag_size: 54\n",
      "batch 439, loss: 0.6433, instance_loss: 1.1857, weighted_loss: 0.8060, label: 0, bag_size: 22\n",
      "batch 459, loss: 0.9204, instance_loss: 3.7636, weighted_loss: 1.7734, label: 1, bag_size: 70\n",
      "batch 479, loss: 0.0884, instance_loss: 0.1270, weighted_loss: 0.1000, label: 0, bag_size: 41\n",
      "batch 499, loss: 0.0022, instance_loss: 0.0301, weighted_loss: 0.0106, label: 1, bag_size: 40\n",
      "batch 519, loss: 0.7911, instance_loss: 0.8029, weighted_loss: 0.7947, label: 0, bag_size: 54\n",
      "batch 539, loss: 0.0580, instance_loss: 1.6459, weighted_loss: 0.5344, label: 1, bag_size: 86\n",
      "batch 559, loss: 0.0647, instance_loss: 0.0410, weighted_loss: 0.0576, label: 1, bag_size: 37\n",
      "batch 579, loss: 0.7245, instance_loss: 0.5576, weighted_loss: 0.6744, label: 1, bag_size: 44\n",
      "batch 599, loss: 0.0191, instance_loss: 0.0339, weighted_loss: 0.0236, label: 0, bag_size: 43\n",
      "batch 619, loss: 0.0251, instance_loss: 0.0164, weighted_loss: 0.0225, label: 0, bag_size: 70\n",
      "batch 639, loss: 0.1414, instance_loss: 0.0968, weighted_loss: 0.1280, label: 1, bag_size: 102\n",
      "batch 659, loss: 0.0610, instance_loss: 0.7235, weighted_loss: 0.2597, label: 1, bag_size: 73\n",
      "batch 679, loss: 0.0027, instance_loss: 0.0003, weighted_loss: 0.0020, label: 1, bag_size: 95\n",
      "batch 699, loss: 0.1807, instance_loss: 0.4898, weighted_loss: 0.2734, label: 0, bag_size: 95\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.951063829787234: correct 10728/11280\n",
      "class 1 clustering acc 0.7278368794326241: correct 4105/5640\n",
      "Epoch: 22, train_loss: 0.3831, train_clustering_loss:  0.4436, train_error: 0.1603\n",
      "class 0: acc 0.8424068767908309, correct 294/349\n",
      "class 1: acc 0.8370786516853933, correct 298/356\n",
      "\n",
      "Val Set, val_loss: 0.4398, val_error: 0.1500, auc: 0.9013\n",
      "class 0 clustering acc 0.92890625: correct 1189/1280\n",
      "class 1 clustering acc 0.640625: correct 410/640\n",
      "class 0: acc 0.8979591836734694, correct 44/49\n",
      "class 1: acc 0.7741935483870968, correct 24/31\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0201, instance_loss: 0.0106, weighted_loss: 0.0172, label: 0, bag_size: 76\n",
      "batch 39, loss: 0.2919, instance_loss: 0.0869, weighted_loss: 0.2304, label: 0, bag_size: 31\n",
      "batch 59, loss: 0.1434, instance_loss: 0.0078, weighted_loss: 0.1027, label: 1, bag_size: 43\n",
      "batch 79, loss: 0.0007, instance_loss: 0.0134, weighted_loss: 0.0045, label: 0, bag_size: 68\n",
      "batch 99, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 71\n",
      "batch 119, loss: 0.7462, instance_loss: 0.4009, weighted_loss: 0.6426, label: 0, bag_size: 107\n",
      "batch 139, loss: 0.0176, instance_loss: 0.0394, weighted_loss: 0.0242, label: 0, bag_size: 22\n",
      "batch 159, loss: 0.0058, instance_loss: 1.4944, weighted_loss: 0.4524, label: 0, bag_size: 105\n",
      "batch 179, loss: 0.0056, instance_loss: 0.0005, weighted_loss: 0.0041, label: 0, bag_size: 50\n",
      "batch 199, loss: 0.0105, instance_loss: 0.0498, weighted_loss: 0.0223, label: 1, bag_size: 106\n",
      "batch 219, loss: 0.1289, instance_loss: 0.2007, weighted_loss: 0.1504, label: 0, bag_size: 74\n",
      "batch 239, loss: 0.0167, instance_loss: 0.0129, weighted_loss: 0.0156, label: 0, bag_size: 79\n",
      "batch 259, loss: 0.0104, instance_loss: 0.0735, weighted_loss: 0.0293, label: 0, bag_size: 106\n",
      "batch 279, loss: 0.0062, instance_loss: 0.0069, weighted_loss: 0.0064, label: 1, bag_size: 66\n",
      "batch 299, loss: 0.0240, instance_loss: 0.0597, weighted_loss: 0.0347, label: 0, bag_size: 56\n",
      "batch 319, loss: 0.2295, instance_loss: 0.4860, weighted_loss: 0.3064, label: 0, bag_size: 44\n",
      "batch 339, loss: 3.1796, instance_loss: 3.8774, weighted_loss: 3.3890, label: 1, bag_size: 73\n",
      "batch 359, loss: 0.1000, instance_loss: 0.9188, weighted_loss: 0.3457, label: 1, bag_size: 51\n",
      "batch 379, loss: 0.1098, instance_loss: 0.0369, weighted_loss: 0.0879, label: 0, bag_size: 27\n",
      "batch 399, loss: 0.1650, instance_loss: 0.2089, weighted_loss: 0.1782, label: 1, bag_size: 87\n",
      "batch 419, loss: 0.4091, instance_loss: 0.1081, weighted_loss: 0.3188, label: 1, bag_size: 71\n",
      "batch 439, loss: 0.0692, instance_loss: 0.0436, weighted_loss: 0.0615, label: 0, bag_size: 49\n",
      "batch 459, loss: 0.0676, instance_loss: 0.0304, weighted_loss: 0.0564, label: 0, bag_size: 29\n",
      "batch 479, loss: 0.0238, instance_loss: 0.0412, weighted_loss: 0.0290, label: 1, bag_size: 115\n",
      "batch 499, loss: 0.3377, instance_loss: 0.3466, weighted_loss: 0.3404, label: 1, bag_size: 52\n",
      "batch 519, loss: 0.0273, instance_loss: 0.0174, weighted_loss: 0.0243, label: 0, bag_size: 39\n",
      "batch 539, loss: 0.0687, instance_loss: 0.1131, weighted_loss: 0.0820, label: 1, bag_size: 55\n",
      "batch 559, loss: 0.0317, instance_loss: 0.2717, weighted_loss: 0.1037, label: 0, bag_size: 68\n",
      "batch 579, loss: 0.0340, instance_loss: 0.0068, weighted_loss: 0.0258, label: 0, bag_size: 80\n",
      "batch 599, loss: 0.6997, instance_loss: 0.8894, weighted_loss: 0.7566, label: 1, bag_size: 30\n",
      "batch 619, loss: 0.3817, instance_loss: 0.1268, weighted_loss: 0.3053, label: 0, bag_size: 92\n",
      "batch 639, loss: 0.9921, instance_loss: 0.3003, weighted_loss: 0.7846, label: 1, bag_size: 103\n",
      "batch 659, loss: 0.0672, instance_loss: 0.1044, weighted_loss: 0.0783, label: 0, bag_size: 113\n",
      "batch 679, loss: 0.4188, instance_loss: 0.0380, weighted_loss: 0.3046, label: 1, bag_size: 115\n",
      "batch 699, loss: 0.0545, instance_loss: 0.0456, weighted_loss: 0.0518, label: 0, bag_size: 97\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9561170212765957: correct 10785/11280\n",
      "class 1 clustering acc 0.7317375886524823: correct 4127/5640\n",
      "Epoch: 23, train_loss: 0.3314, train_clustering_loss:  0.4332, train_error: 0.1418\n",
      "class 0: acc 0.8759894459102903, correct 332/379\n",
      "class 1: acc 0.8374233128834356, correct 273/326\n",
      "\n",
      "Val Set, val_loss: 0.6268, val_error: 0.2375, auc: 0.8657\n",
      "class 0 clustering acc 0.88828125: correct 1137/1280\n",
      "class 1 clustering acc 0.6078125: correct 389/640\n",
      "class 0: acc 0.7551020408163265, correct 37/49\n",
      "class 1: acc 0.7741935483870968, correct 24/31\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6137, instance_loss: 0.5107, weighted_loss: 0.5828, label: 1, bag_size: 55\n",
      "batch 39, loss: 0.0012, instance_loss: 0.0043, weighted_loss: 0.0021, label: 1, bag_size: 16\n",
      "batch 59, loss: 0.0214, instance_loss: 0.0264, weighted_loss: 0.0229, label: 0, bag_size: 51\n",
      "batch 79, loss: 0.0241, instance_loss: 0.1893, weighted_loss: 0.0737, label: 0, bag_size: 36\n",
      "batch 99, loss: 0.0185, instance_loss: 0.0208, weighted_loss: 0.0192, label: 0, bag_size: 47\n",
      "batch 119, loss: 0.1545, instance_loss: 0.0262, weighted_loss: 0.1160, label: 0, bag_size: 83\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0075, weighted_loss: 0.0023, label: 1, bag_size: 31\n",
      "batch 159, loss: 0.0102, instance_loss: 0.0032, weighted_loss: 0.0081, label: 1, bag_size: 109\n",
      "batch 179, loss: 0.0484, instance_loss: 0.1689, weighted_loss: 0.0845, label: 0, bag_size: 104\n",
      "batch 199, loss: 0.0522, instance_loss: 0.0318, weighted_loss: 0.0461, label: 1, bag_size: 74\n",
      "batch 219, loss: 0.0021, instance_loss: 0.0782, weighted_loss: 0.0250, label: 1, bag_size: 33\n",
      "batch 239, loss: 0.4829, instance_loss: 1.3449, weighted_loss: 0.7415, label: 1, bag_size: 117\n",
      "batch 259, loss: 0.0825, instance_loss: 0.0182, weighted_loss: 0.0632, label: 1, bag_size: 70\n",
      "batch 279, loss: 0.0132, instance_loss: 0.0268, weighted_loss: 0.0173, label: 0, bag_size: 57\n",
      "batch 299, loss: 0.0027, instance_loss: 0.0091, weighted_loss: 0.0046, label: 0, bag_size: 96\n",
      "batch 319, loss: 0.0589, instance_loss: 0.0008, weighted_loss: 0.0415, label: 1, bag_size: 52\n",
      "batch 339, loss: 0.4666, instance_loss: 0.2414, weighted_loss: 0.3991, label: 1, bag_size: 61\n",
      "batch 359, loss: 1.9706, instance_loss: 2.1731, weighted_loss: 2.0313, label: 1, bag_size: 89\n",
      "batch 379, loss: 0.8449, instance_loss: 1.0069, weighted_loss: 0.8935, label: 0, bag_size: 92\n",
      "batch 399, loss: 0.2858, instance_loss: 0.1520, weighted_loss: 0.2456, label: 1, bag_size: 98\n",
      "batch 419, loss: 0.0186, instance_loss: 0.0069, weighted_loss: 0.0151, label: 1, bag_size: 62\n",
      "batch 439, loss: 0.3563, instance_loss: 0.2030, weighted_loss: 0.3103, label: 0, bag_size: 66\n",
      "batch 459, loss: 0.0263, instance_loss: 0.0011, weighted_loss: 0.0187, label: 1, bag_size: 30\n",
      "batch 479, loss: 0.0131, instance_loss: 0.0200, weighted_loss: 0.0151, label: 0, bag_size: 29\n",
      "batch 499, loss: 0.0020, instance_loss: 0.0343, weighted_loss: 0.0117, label: 0, bag_size: 53\n",
      "batch 519, loss: 0.2343, instance_loss: 0.8636, weighted_loss: 0.4230, label: 1, bag_size: 117\n",
      "batch 539, loss: 0.0005, instance_loss: 0.0120, weighted_loss: 0.0039, label: 0, bag_size: 123\n",
      "batch 559, loss: 0.0011, instance_loss: 0.0013, weighted_loss: 0.0011, label: 1, bag_size: 89\n",
      "batch 579, loss: 0.0180, instance_loss: 0.0147, weighted_loss: 0.0170, label: 1, bag_size: 112\n",
      "batch 599, loss: 0.0058, instance_loss: 1.0249, weighted_loss: 0.3115, label: 1, bag_size: 43\n",
      "batch 619, loss: 0.0184, instance_loss: 0.0045, weighted_loss: 0.0143, label: 0, bag_size: 135\n",
      "batch 639, loss: 0.2393, instance_loss: 0.0493, weighted_loss: 0.1823, label: 0, bag_size: 62\n",
      "batch 659, loss: 0.1135, instance_loss: 0.1769, weighted_loss: 0.1325, label: 1, bag_size: 43\n",
      "batch 679, loss: 2.2952, instance_loss: 2.0411, weighted_loss: 2.2190, label: 0, bag_size: 27\n",
      "batch 699, loss: 0.0373, instance_loss: 0.0009, weighted_loss: 0.0264, label: 0, bag_size: 52\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9596631205673759: correct 10825/11280\n",
      "class 1 clustering acc 0.750886524822695: correct 4235/5640\n",
      "Epoch: 24, train_loss: 0.2988, train_clustering_loss:  0.4049, train_error: 0.1220\n",
      "class 0: acc 0.8753541076487252, correct 309/353\n",
      "class 1: acc 0.8806818181818182, correct 310/352\n",
      "\n",
      "Val Set, val_loss: 0.5270, val_error: 0.2000, auc: 0.8822\n",
      "class 0 clustering acc 0.93671875: correct 1199/1280\n",
      "class 1 clustering acc 0.709375: correct 454/640\n",
      "class 0: acc 0.8367346938775511, correct 41/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0061, instance_loss: 0.0042, weighted_loss: 0.0055, label: 1, bag_size: 111\n",
      "batch 39, loss: 0.0649, instance_loss: 0.1185, weighted_loss: 0.0810, label: 0, bag_size: 51\n",
      "batch 59, loss: 3.7069, instance_loss: 2.7969, weighted_loss: 3.4339, label: 1, bag_size: 126\n",
      "batch 79, loss: 0.5314, instance_loss: 1.5232, weighted_loss: 0.8289, label: 0, bag_size: 63\n",
      "batch 99, loss: 0.0519, instance_loss: 0.4551, weighted_loss: 0.1729, label: 1, bag_size: 47\n",
      "batch 119, loss: 0.1510, instance_loss: 0.0201, weighted_loss: 0.1117, label: 1, bag_size: 102\n",
      "batch 139, loss: 0.0160, instance_loss: 0.0743, weighted_loss: 0.0335, label: 1, bag_size: 66\n",
      "batch 159, loss: 0.0344, instance_loss: 0.0637, weighted_loss: 0.0432, label: 1, bag_size: 52\n",
      "batch 179, loss: 0.0010, instance_loss: 0.0178, weighted_loss: 0.0060, label: 0, bag_size: 56\n",
      "batch 199, loss: 0.1157, instance_loss: 0.0171, weighted_loss: 0.0861, label: 0, bag_size: 37\n",
      "batch 219, loss: 0.0034, instance_loss: 0.0053, weighted_loss: 0.0040, label: 0, bag_size: 30\n",
      "batch 239, loss: 3.1113, instance_loss: 1.4566, weighted_loss: 2.6149, label: 1, bag_size: 26\n",
      "batch 259, loss: 0.1300, instance_loss: 0.2741, weighted_loss: 0.1732, label: 1, bag_size: 46\n",
      "batch 279, loss: 0.0002, instance_loss: 0.0003, weighted_loss: 0.0002, label: 0, bag_size: 95\n",
      "batch 299, loss: 0.1209, instance_loss: 2.2870, weighted_loss: 0.7708, label: 1, bag_size: 63\n",
      "batch 319, loss: 0.3021, instance_loss: 0.0438, weighted_loss: 0.2246, label: 1, bag_size: 37\n",
      "batch 339, loss: 0.0509, instance_loss: 1.3072, weighted_loss: 0.4278, label: 0, bag_size: 131\n",
      "batch 359, loss: 0.0542, instance_loss: 0.0143, weighted_loss: 0.0422, label: 1, bag_size: 123\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0061, weighted_loss: 0.0019, label: 1, bag_size: 79\n",
      "batch 399, loss: 0.0112, instance_loss: 0.7844, weighted_loss: 0.2432, label: 0, bag_size: 52\n",
      "batch 419, loss: 0.0118, instance_loss: 0.0399, weighted_loss: 0.0202, label: 0, bag_size: 59\n",
      "batch 439, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 93\n",
      "batch 459, loss: 0.1032, instance_loss: 0.0332, weighted_loss: 0.0822, label: 1, bag_size: 59\n",
      "batch 479, loss: 0.0584, instance_loss: 0.2031, weighted_loss: 0.1018, label: 0, bag_size: 26\n",
      "batch 499, loss: 0.2393, instance_loss: 0.2775, weighted_loss: 0.2508, label: 0, bag_size: 44\n",
      "batch 519, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 30\n",
      "batch 539, loss: 0.3007, instance_loss: 0.4375, weighted_loss: 0.3418, label: 0, bag_size: 95\n",
      "batch 559, loss: 0.0002, instance_loss: 0.1422, weighted_loss: 0.0428, label: 1, bag_size: 98\n",
      "batch 579, loss: 0.1919, instance_loss: 0.8960, weighted_loss: 0.4031, label: 1, bag_size: 31\n",
      "batch 599, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 84\n",
      "batch 619, loss: 0.4297, instance_loss: 0.2261, weighted_loss: 0.3686, label: 0, bag_size: 73\n",
      "batch 639, loss: 0.0147, instance_loss: 0.0273, weighted_loss: 0.0185, label: 1, bag_size: 77\n",
      "batch 659, loss: 0.0176, instance_loss: 0.0292, weighted_loss: 0.0211, label: 1, bag_size: 25\n",
      "batch 679, loss: 0.0774, instance_loss: 1.7488, weighted_loss: 0.5788, label: 0, bag_size: 70\n",
      "batch 699, loss: 0.0054, instance_loss: 0.0136, weighted_loss: 0.0079, label: 0, bag_size: 71\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9568262411347518: correct 10793/11280\n",
      "class 1 clustering acc 0.7641843971631206: correct 4310/5640\n",
      "Epoch: 25, train_loss: 0.2984, train_clustering_loss:  0.3921, train_error: 0.1248\n",
      "class 0: acc 0.8864864864864865, correct 328/370\n",
      "class 1: acc 0.8626865671641791, correct 289/335\n",
      "\n",
      "Val Set, val_loss: 0.7656, val_error: 0.2375, auc: 0.8525\n",
      "class 0 clustering acc 0.8296875: correct 1062/1280\n",
      "class 1 clustering acc 0.6765625: correct 433/640\n",
      "class 0: acc 0.9387755102040817, correct 46/49\n",
      "class 1: acc 0.4838709677419355, correct 15/31\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0022, instance_loss: 0.0027, weighted_loss: 0.0023, label: 1, bag_size: 107\n",
      "batch 39, loss: 0.6260, instance_loss: 0.7894, weighted_loss: 0.6750, label: 1, bag_size: 57\n",
      "batch 59, loss: 0.1690, instance_loss: 0.1887, weighted_loss: 0.1749, label: 0, bag_size: 65\n",
      "batch 79, loss: 0.0664, instance_loss: 0.3636, weighted_loss: 0.1555, label: 1, bag_size: 69\n",
      "batch 99, loss: 0.4309, instance_loss: 0.0424, weighted_loss: 0.3143, label: 0, bag_size: 82\n",
      "batch 119, loss: 0.0089, instance_loss: 0.0762, weighted_loss: 0.0291, label: 0, bag_size: 59\n",
      "batch 139, loss: 0.0429, instance_loss: 0.0060, weighted_loss: 0.0318, label: 0, bag_size: 61\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 33\n",
      "batch 179, loss: 0.0053, instance_loss: 0.0219, weighted_loss: 0.0103, label: 0, bag_size: 96\n",
      "batch 199, loss: 1.0382, instance_loss: 0.2173, weighted_loss: 0.7919, label: 0, bag_size: 70\n",
      "batch 219, loss: 0.0041, instance_loss: 0.0109, weighted_loss: 0.0061, label: 1, bag_size: 52\n",
      "batch 239, loss: 0.0008, instance_loss: 0.0039, weighted_loss: 0.0017, label: 1, bag_size: 41\n",
      "batch 259, loss: 0.0030, instance_loss: 0.0001, weighted_loss: 0.0021, label: 1, bag_size: 119\n",
      "batch 279, loss: 0.7614, instance_loss: 0.0428, weighted_loss: 0.5458, label: 0, bag_size: 61\n",
      "batch 299, loss: 0.1916, instance_loss: 0.6810, weighted_loss: 0.3384, label: 0, bag_size: 32\n",
      "batch 319, loss: 0.0032, instance_loss: 0.0008, weighted_loss: 0.0025, label: 0, bag_size: 82\n",
      "batch 339, loss: 4.4556, instance_loss: 0.6303, weighted_loss: 3.3080, label: 0, bag_size: 49\n",
      "batch 359, loss: 0.5984, instance_loss: 0.0362, weighted_loss: 0.4297, label: 0, bag_size: 67\n",
      "batch 379, loss: 0.8060, instance_loss: 0.1142, weighted_loss: 0.5985, label: 0, bag_size: 62\n",
      "batch 399, loss: 0.1875, instance_loss: 0.1012, weighted_loss: 0.1616, label: 1, bag_size: 70\n",
      "batch 419, loss: 3.2333, instance_loss: 0.5053, weighted_loss: 2.4149, label: 0, bag_size: 49\n",
      "batch 439, loss: 0.3411, instance_loss: 0.7974, weighted_loss: 0.4780, label: 1, bag_size: 51\n",
      "batch 459, loss: 1.7100, instance_loss: 0.2046, weighted_loss: 1.2584, label: 1, bag_size: 73\n",
      "batch 479, loss: 0.0168, instance_loss: 0.0688, weighted_loss: 0.0324, label: 1, bag_size: 68\n",
      "batch 499, loss: 0.0121, instance_loss: 0.0427, weighted_loss: 0.0213, label: 0, bag_size: 37\n",
      "batch 519, loss: 2.2179, instance_loss: 2.9945, weighted_loss: 2.4509, label: 1, bag_size: 67\n",
      "batch 539, loss: 0.0007, instance_loss: 0.0005, weighted_loss: 0.0007, label: 1, bag_size: 79\n",
      "batch 559, loss: 0.0411, instance_loss: 0.0351, weighted_loss: 0.0393, label: 0, bag_size: 73\n",
      "batch 579, loss: 0.0029, instance_loss: 0.0104, weighted_loss: 0.0051, label: 1, bag_size: 131\n",
      "batch 599, loss: 0.1077, instance_loss: 0.0173, weighted_loss: 0.0806, label: 0, bag_size: 79\n",
      "batch 619, loss: 0.0026, instance_loss: 0.0145, weighted_loss: 0.0062, label: 1, bag_size: 62\n",
      "batch 639, loss: 0.0061, instance_loss: 0.0797, weighted_loss: 0.0282, label: 0, bag_size: 72\n",
      "batch 659, loss: 0.0238, instance_loss: 0.0435, weighted_loss: 0.0297, label: 1, bag_size: 103\n",
      "batch 679, loss: 0.0685, instance_loss: 0.4054, weighted_loss: 0.1695, label: 0, bag_size: 122\n",
      "batch 699, loss: 0.0048, instance_loss: 0.3242, weighted_loss: 0.1006, label: 0, bag_size: 93\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9593085106382979: correct 10821/11280\n",
      "class 1 clustering acc 0.7682624113475177: correct 4333/5640\n",
      "Epoch: 26, train_loss: 0.2981, train_clustering_loss:  0.4026, train_error: 0.1177\n",
      "class 0: acc 0.8898071625344353, correct 323/363\n",
      "class 1: acc 0.8742690058479532, correct 299/342\n",
      "\n",
      "Val Set, val_loss: 1.5628, val_error: 0.3000, auc: 0.8894\n",
      "class 0 clustering acc 0.90078125: correct 1153/1280\n",
      "class 1 clustering acc 0.7140625: correct 457/640\n",
      "class 0: acc 0.9795918367346939, correct 48/49\n",
      "class 1: acc 0.25806451612903225, correct 8/31\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0038, instance_loss: 0.0072, weighted_loss: 0.0048, label: 1, bag_size: 105\n",
      "batch 39, loss: 0.0031, instance_loss: 0.0343, weighted_loss: 0.0124, label: 0, bag_size: 103\n",
      "batch 59, loss: 0.6577, instance_loss: 1.1492, weighted_loss: 0.8051, label: 1, bag_size: 37\n",
      "batch 79, loss: 0.0321, instance_loss: 0.2460, weighted_loss: 0.0963, label: 1, bag_size: 63\n",
      "batch 99, loss: 0.0253, instance_loss: 0.8470, weighted_loss: 0.2718, label: 0, bag_size: 69\n",
      "batch 119, loss: 4.1046, instance_loss: 4.2101, weighted_loss: 4.1363, label: 1, bag_size: 126\n",
      "batch 139, loss: 0.0727, instance_loss: 0.4400, weighted_loss: 0.1829, label: 0, bag_size: 64\n",
      "batch 159, loss: 0.0025, instance_loss: 0.0193, weighted_loss: 0.0075, label: 1, bag_size: 37\n",
      "batch 179, loss: 0.0753, instance_loss: 0.2400, weighted_loss: 0.1247, label: 1, bag_size: 43\n",
      "batch 199, loss: 0.1100, instance_loss: 0.1737, weighted_loss: 0.1291, label: 0, bag_size: 73\n",
      "batch 219, loss: 0.0216, instance_loss: 0.0935, weighted_loss: 0.0432, label: 0, bag_size: 45\n",
      "batch 239, loss: 0.0461, instance_loss: 0.6691, weighted_loss: 0.2330, label: 1, bag_size: 43\n",
      "batch 259, loss: 0.0003, instance_loss: 0.0117, weighted_loss: 0.0037, label: 0, bag_size: 59\n",
      "batch 279, loss: 0.0060, instance_loss: 0.0084, weighted_loss: 0.0067, label: 1, bag_size: 87\n",
      "batch 299, loss: 0.0034, instance_loss: 0.0580, weighted_loss: 0.0198, label: 1, bag_size: 37\n",
      "batch 319, loss: 0.3428, instance_loss: 0.2485, weighted_loss: 0.3145, label: 1, bag_size: 114\n",
      "batch 339, loss: 0.0691, instance_loss: 0.0566, weighted_loss: 0.0654, label: 0, bag_size: 27\n",
      "batch 359, loss: 0.0556, instance_loss: 0.0184, weighted_loss: 0.0445, label: 1, bag_size: 76\n",
      "batch 379, loss: 0.0371, instance_loss: 0.0230, weighted_loss: 0.0329, label: 1, bag_size: 52\n",
      "batch 399, loss: 0.0081, instance_loss: 0.0623, weighted_loss: 0.0244, label: 1, bag_size: 69\n",
      "batch 419, loss: 0.0358, instance_loss: 0.1010, weighted_loss: 0.0553, label: 0, bag_size: 51\n",
      "batch 439, loss: 1.4637, instance_loss: 1.2065, weighted_loss: 1.3866, label: 1, bag_size: 93\n",
      "batch 459, loss: 0.0296, instance_loss: 0.2725, weighted_loss: 0.1025, label: 0, bag_size: 59\n",
      "batch 479, loss: 0.4875, instance_loss: 0.4927, weighted_loss: 0.4891, label: 0, bag_size: 128\n",
      "batch 499, loss: 0.0051, instance_loss: 0.0145, weighted_loss: 0.0079, label: 1, bag_size: 95\n",
      "batch 519, loss: 0.0219, instance_loss: 0.0068, weighted_loss: 0.0174, label: 1, bag_size: 115\n",
      "batch 539, loss: 0.0215, instance_loss: 0.0342, weighted_loss: 0.0253, label: 1, bag_size: 52\n",
      "batch 559, loss: 0.0187, instance_loss: 0.3653, weighted_loss: 0.1227, label: 0, bag_size: 40\n",
      "batch 579, loss: 0.0175, instance_loss: 0.0121, weighted_loss: 0.0159, label: 0, bag_size: 14\n",
      "batch 599, loss: 0.0971, instance_loss: 0.1457, weighted_loss: 0.1117, label: 0, bag_size: 95\n",
      "batch 619, loss: 0.0105, instance_loss: 0.0022, weighted_loss: 0.0080, label: 1, bag_size: 92\n",
      "batch 639, loss: 0.1039, instance_loss: 0.0740, weighted_loss: 0.0949, label: 1, bag_size: 37\n",
      "batch 659, loss: 0.0512, instance_loss: 0.0821, weighted_loss: 0.0605, label: 1, bag_size: 109\n",
      "batch 679, loss: 0.0053, instance_loss: 0.0029, weighted_loss: 0.0045, label: 0, bag_size: 44\n",
      "batch 699, loss: 0.0992, instance_loss: 0.8753, weighted_loss: 0.3320, label: 0, bag_size: 36\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9544326241134752: correct 10766/11280\n",
      "class 1 clustering acc 0.7546099290780142: correct 4256/5640\n",
      "Epoch: 27, train_loss: 0.3300, train_clustering_loss:  0.4307, train_error: 0.1362\n",
      "class 0: acc 0.8642659279778393, correct 312/361\n",
      "class 1: acc 0.8633720930232558, correct 297/344\n",
      "\n",
      "Val Set, val_loss: 0.4653, val_error: 0.1625, auc: 0.9151\n",
      "class 0 clustering acc 0.9125: correct 1168/1280\n",
      "class 1 clustering acc 0.6859375: correct 439/640\n",
      "class 0: acc 0.7959183673469388, correct 39/49\n",
      "class 1: acc 0.9032258064516129, correct 28/31\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0465, instance_loss: 0.0439, weighted_loss: 0.0457, label: 0, bag_size: 63\n",
      "batch 39, loss: 0.0779, instance_loss: 0.7981, weighted_loss: 0.2940, label: 0, bag_size: 77\n",
      "batch 59, loss: 0.0147, instance_loss: 0.0105, weighted_loss: 0.0135, label: 0, bag_size: 109\n",
      "batch 79, loss: 0.1134, instance_loss: 0.0126, weighted_loss: 0.0832, label: 0, bag_size: 27\n",
      "batch 99, loss: 0.0485, instance_loss: 0.0112, weighted_loss: 0.0373, label: 0, bag_size: 25\n",
      "batch 119, loss: 0.0518, instance_loss: 0.0035, weighted_loss: 0.0373, label: 1, bag_size: 70\n",
      "batch 139, loss: 0.0055, instance_loss: 0.0133, weighted_loss: 0.0079, label: 0, bag_size: 52\n",
      "batch 159, loss: 0.7091, instance_loss: 0.9292, weighted_loss: 0.7752, label: 1, bag_size: 36\n",
      "batch 179, loss: 0.2380, instance_loss: 0.3944, weighted_loss: 0.2849, label: 1, bag_size: 18\n",
      "batch 199, loss: 0.0034, instance_loss: 0.0303, weighted_loss: 0.0115, label: 0, bag_size: 36\n",
      "batch 219, loss: 0.0126, instance_loss: 0.0066, weighted_loss: 0.0108, label: 0, bag_size: 32\n",
      "batch 239, loss: 0.1228, instance_loss: 1.0416, weighted_loss: 0.3984, label: 0, bag_size: 36\n",
      "batch 259, loss: 0.1159, instance_loss: 0.3392, weighted_loss: 0.1829, label: 0, bag_size: 33\n",
      "batch 279, loss: 0.3398, instance_loss: 0.2914, weighted_loss: 0.3253, label: 1, bag_size: 28\n",
      "batch 299, loss: 0.1296, instance_loss: 0.2284, weighted_loss: 0.1592, label: 1, bag_size: 123\n",
      "batch 319, loss: 0.6143, instance_loss: 1.1509, weighted_loss: 0.7753, label: 1, bag_size: 105\n",
      "batch 339, loss: 0.0025, instance_loss: 0.0069, weighted_loss: 0.0038, label: 0, bag_size: 114\n",
      "batch 359, loss: 0.0490, instance_loss: 0.0222, weighted_loss: 0.0410, label: 0, bag_size: 71\n",
      "batch 379, loss: 0.0037, instance_loss: 0.0853, weighted_loss: 0.0282, label: 1, bag_size: 51\n",
      "batch 399, loss: 0.0438, instance_loss: 0.0697, weighted_loss: 0.0515, label: 0, bag_size: 20\n",
      "batch 419, loss: 1.2717, instance_loss: 1.2706, weighted_loss: 1.2714, label: 1, bag_size: 23\n",
      "batch 439, loss: 0.0021, instance_loss: 0.0036, weighted_loss: 0.0025, label: 1, bag_size: 52\n",
      "batch 459, loss: 0.0019, instance_loss: 0.0078, weighted_loss: 0.0036, label: 1, bag_size: 30\n",
      "batch 479, loss: 0.0408, instance_loss: 0.0720, weighted_loss: 0.0501, label: 0, bag_size: 88\n",
      "batch 499, loss: 1.1795, instance_loss: 0.3810, weighted_loss: 0.9399, label: 1, bag_size: 74\n",
      "batch 519, loss: 0.0822, instance_loss: 0.1798, weighted_loss: 0.1115, label: 0, bag_size: 51\n",
      "batch 539, loss: 0.0017, instance_loss: 0.0026, weighted_loss: 0.0020, label: 1, bag_size: 51\n",
      "batch 559, loss: 0.5307, instance_loss: 0.3138, weighted_loss: 0.4657, label: 0, bag_size: 73\n",
      "batch 579, loss: 0.6713, instance_loss: 2.2812, weighted_loss: 1.1543, label: 0, bag_size: 67\n",
      "batch 599, loss: 0.0610, instance_loss: 0.1158, weighted_loss: 0.0774, label: 1, bag_size: 98\n",
      "batch 619, loss: 0.0063, instance_loss: 0.0044, weighted_loss: 0.0057, label: 0, bag_size: 65\n",
      "batch 639, loss: 0.0295, instance_loss: 0.0250, weighted_loss: 0.0281, label: 0, bag_size: 107\n",
      "batch 659, loss: 1.6522, instance_loss: 2.1525, weighted_loss: 1.8023, label: 1, bag_size: 70\n",
      "batch 679, loss: 0.0030, instance_loss: 0.0713, weighted_loss: 0.0235, label: 1, bag_size: 38\n",
      "batch 699, loss: 0.1170, instance_loss: 0.0008, weighted_loss: 0.0821, label: 1, bag_size: 86\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9583333333333334: correct 10810/11280\n",
      "class 1 clustering acc 0.775: correct 4371/5640\n",
      "Epoch: 28, train_loss: 0.2835, train_clustering_loss:  0.3804, train_error: 0.1206\n",
      "class 0: acc 0.8808864265927978, correct 318/361\n",
      "class 1: acc 0.877906976744186, correct 302/344\n",
      "\n",
      "Val Set, val_loss: 0.5662, val_error: 0.1750, auc: 0.8808\n",
      "class 0 clustering acc 0.92265625: correct 1181/1280\n",
      "class 1 clustering acc 0.734375: correct 470/640\n",
      "class 0: acc 0.8571428571428571, correct 42/49\n",
      "class 1: acc 0.7741935483870968, correct 24/31\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1565, instance_loss: 0.2234, weighted_loss: 0.1766, label: 0, bag_size: 123\n",
      "batch 39, loss: 0.0101, instance_loss: 0.0154, weighted_loss: 0.0117, label: 1, bag_size: 107\n",
      "batch 59, loss: 0.2224, instance_loss: 0.0582, weighted_loss: 0.1731, label: 0, bag_size: 67\n",
      "batch 79, loss: 0.0156, instance_loss: 0.0028, weighted_loss: 0.0118, label: 1, bag_size: 71\n",
      "batch 99, loss: 0.2412, instance_loss: 1.4585, weighted_loss: 0.6064, label: 0, bag_size: 68\n",
      "batch 119, loss: 0.0014, instance_loss: 0.0061, weighted_loss: 0.0028, label: 1, bag_size: 95\n",
      "batch 139, loss: 0.0348, instance_loss: 0.0228, weighted_loss: 0.0312, label: 1, bag_size: 109\n",
      "batch 159, loss: 0.0163, instance_loss: 0.0298, weighted_loss: 0.0204, label: 0, bag_size: 50\n",
      "batch 179, loss: 0.1334, instance_loss: 0.1111, weighted_loss: 0.1267, label: 0, bag_size: 63\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0290, weighted_loss: 0.0087, label: 0, bag_size: 72\n",
      "batch 219, loss: 0.0019, instance_loss: 0.0210, weighted_loss: 0.0076, label: 1, bag_size: 46\n",
      "batch 239, loss: 0.0135, instance_loss: 0.0000, weighted_loss: 0.0094, label: 1, bag_size: 53\n",
      "batch 259, loss: 0.5229, instance_loss: 0.0926, weighted_loss: 0.3938, label: 1, bag_size: 46\n",
      "batch 279, loss: 0.1250, instance_loss: 0.0012, weighted_loss: 0.0878, label: 1, bag_size: 91\n",
      "batch 299, loss: 0.2862, instance_loss: 0.5376, weighted_loss: 0.3616, label: 1, bag_size: 62\n",
      "batch 319, loss: 0.4183, instance_loss: 0.1665, weighted_loss: 0.3427, label: 0, bag_size: 46\n",
      "batch 339, loss: 0.0079, instance_loss: 0.0081, weighted_loss: 0.0080, label: 0, bag_size: 108\n",
      "batch 359, loss: 0.0156, instance_loss: 0.2461, weighted_loss: 0.0847, label: 0, bag_size: 53\n",
      "batch 379, loss: 0.0115, instance_loss: 0.0089, weighted_loss: 0.0107, label: 0, bag_size: 35\n",
      "batch 399, loss: 1.0877, instance_loss: 0.0025, weighted_loss: 0.7621, label: 1, bag_size: 87\n",
      "batch 419, loss: 0.0036, instance_loss: 0.0016, weighted_loss: 0.0030, label: 1, bag_size: 30\n",
      "batch 439, loss: 0.2330, instance_loss: 1.0094, weighted_loss: 0.4659, label: 1, bag_size: 40\n",
      "batch 459, loss: 0.0003, instance_loss: 0.0100, weighted_loss: 0.0032, label: 0, bag_size: 85\n",
      "batch 479, loss: 1.6247, instance_loss: 0.6455, weighted_loss: 1.3310, label: 0, bag_size: 95\n",
      "batch 499, loss: 0.0123, instance_loss: 0.4151, weighted_loss: 0.1331, label: 0, bag_size: 68\n",
      "batch 519, loss: 0.0245, instance_loss: 0.0204, weighted_loss: 0.0233, label: 1, bag_size: 68\n",
      "batch 539, loss: 0.0035, instance_loss: 0.0000, weighted_loss: 0.0025, label: 0, bag_size: 74\n",
      "batch 559, loss: 0.0008, instance_loss: 0.0177, weighted_loss: 0.0059, label: 0, bag_size: 55\n",
      "batch 579, loss: 0.3279, instance_loss: 0.0679, weighted_loss: 0.2499, label: 0, bag_size: 30\n",
      "batch 599, loss: 0.7478, instance_loss: 0.0910, weighted_loss: 0.5508, label: 0, bag_size: 68\n",
      "batch 619, loss: 0.0423, instance_loss: 0.1351, weighted_loss: 0.0702, label: 1, bag_size: 71\n",
      "batch 639, loss: 0.7990, instance_loss: 1.0375, weighted_loss: 0.8705, label: 1, bag_size: 63\n",
      "batch 659, loss: 0.0067, instance_loss: 0.0015, weighted_loss: 0.0051, label: 1, bag_size: 62\n",
      "batch 679, loss: 0.0096, instance_loss: 0.0450, weighted_loss: 0.0202, label: 0, bag_size: 68\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 95\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9635638297872341: correct 10869/11280\n",
      "class 1 clustering acc 0.7962765957446809: correct 4491/5640\n",
      "Epoch: 29, train_loss: 0.2900, train_clustering_loss:  0.3519, train_error: 0.1191\n",
      "class 0: acc 0.8702064896755162, correct 295/339\n",
      "class 1: acc 0.8907103825136612, correct 326/366\n",
      "\n",
      "Val Set, val_loss: 0.8157, val_error: 0.3125, auc: 0.8901\n",
      "class 0 clustering acc 0.8859375: correct 1134/1280\n",
      "class 1 clustering acc 0.6125: correct 392/640\n",
      "class 0: acc 0.5510204081632653, correct 27/49\n",
      "class 1: acc 0.9032258064516129, correct 28/31\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1474, instance_loss: 2.3058, weighted_loss: 0.7949, label: 0, bag_size: 47\n",
      "batch 39, loss: 0.1814, instance_loss: 1.5115, weighted_loss: 0.5804, label: 1, bag_size: 51\n",
      "batch 59, loss: 0.2601, instance_loss: 0.1804, weighted_loss: 0.2362, label: 0, bag_size: 73\n",
      "batch 79, loss: 0.0415, instance_loss: 0.0248, weighted_loss: 0.0365, label: 0, bag_size: 37\n",
      "batch 99, loss: 2.3053, instance_loss: 2.1839, weighted_loss: 2.2689, label: 0, bag_size: 43\n",
      "batch 119, loss: 0.0285, instance_loss: 0.0169, weighted_loss: 0.0250, label: 1, bag_size: 46\n",
      "batch 139, loss: 0.0022, instance_loss: 0.0048, weighted_loss: 0.0030, label: 0, bag_size: 21\n",
      "batch 159, loss: 0.0095, instance_loss: 0.0016, weighted_loss: 0.0071, label: 0, bag_size: 70\n",
      "batch 179, loss: 0.0231, instance_loss: 0.0017, weighted_loss: 0.0167, label: 0, bag_size: 76\n",
      "batch 199, loss: 0.0524, instance_loss: 0.0160, weighted_loss: 0.0415, label: 1, bag_size: 45\n",
      "batch 219, loss: 0.2270, instance_loss: 2.5824, weighted_loss: 0.9336, label: 1, bag_size: 70\n",
      "batch 239, loss: 0.0009, instance_loss: 0.0021, weighted_loss: 0.0013, label: 1, bag_size: 79\n",
      "batch 259, loss: 0.0654, instance_loss: 1.4353, weighted_loss: 0.4764, label: 1, bag_size: 84\n",
      "batch 279, loss: 0.0194, instance_loss: 0.0040, weighted_loss: 0.0148, label: 1, bag_size: 52\n",
      "batch 299, loss: 0.1106, instance_loss: 0.0319, weighted_loss: 0.0870, label: 1, bag_size: 74\n",
      "batch 319, loss: 0.0050, instance_loss: 0.0808, weighted_loss: 0.0277, label: 0, bag_size: 96\n",
      "batch 339, loss: 0.0547, instance_loss: 0.4001, weighted_loss: 0.1583, label: 0, bag_size: 49\n",
      "batch 359, loss: 0.1706, instance_loss: 0.7809, weighted_loss: 0.3537, label: 0, bag_size: 31\n",
      "batch 379, loss: 0.4364, instance_loss: 0.2108, weighted_loss: 0.3687, label: 0, bag_size: 70\n",
      "batch 399, loss: 0.0856, instance_loss: 0.7638, weighted_loss: 0.2891, label: 1, bag_size: 66\n",
      "batch 419, loss: 0.1255, instance_loss: 0.0403, weighted_loss: 0.0999, label: 1, bag_size: 46\n",
      "batch 439, loss: 0.0077, instance_loss: 0.0921, weighted_loss: 0.0330, label: 0, bag_size: 36\n",
      "batch 459, loss: 0.0088, instance_loss: 0.9718, weighted_loss: 0.2977, label: 0, bag_size: 63\n",
      "batch 479, loss: 0.0135, instance_loss: 0.0336, weighted_loss: 0.0195, label: 0, bag_size: 99\n",
      "batch 499, loss: 0.0111, instance_loss: 0.0181, weighted_loss: 0.0132, label: 1, bag_size: 115\n",
      "batch 519, loss: 0.0042, instance_loss: 0.0000, weighted_loss: 0.0030, label: 1, bag_size: 27\n",
      "batch 539, loss: 0.0958, instance_loss: 0.1833, weighted_loss: 0.1221, label: 1, bag_size: 60\n",
      "batch 559, loss: 0.0341, instance_loss: 0.0823, weighted_loss: 0.0486, label: 0, bag_size: 83\n",
      "batch 579, loss: 0.0813, instance_loss: 0.0783, weighted_loss: 0.0804, label: 1, bag_size: 55\n",
      "batch 599, loss: 0.0010, instance_loss: 0.0287, weighted_loss: 0.0093, label: 0, bag_size: 25\n",
      "batch 619, loss: 0.7856, instance_loss: 0.8284, weighted_loss: 0.7985, label: 1, bag_size: 79\n",
      "batch 639, loss: 0.0032, instance_loss: 0.0969, weighted_loss: 0.0313, label: 0, bag_size: 36\n",
      "batch 659, loss: 0.0019, instance_loss: 0.6313, weighted_loss: 0.1907, label: 1, bag_size: 66\n",
      "batch 679, loss: 0.0165, instance_loss: 0.7436, weighted_loss: 0.2346, label: 0, bag_size: 33\n",
      "batch 699, loss: 0.2076, instance_loss: 0.2784, weighted_loss: 0.2289, label: 1, bag_size: 80\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9656914893617021: correct 10893/11280\n",
      "class 1 clustering acc 0.7884751773049645: correct 4447/5640\n",
      "Epoch: 30, train_loss: 0.2537, train_clustering_loss:  0.3492, train_error: 0.1007\n",
      "class 0: acc 0.8945868945868946, correct 314/351\n",
      "class 1: acc 0.903954802259887, correct 320/354\n",
      "\n",
      "Val Set, val_loss: 0.5738, val_error: 0.1750, auc: 0.8887\n",
      "class 0 clustering acc 0.90859375: correct 1163/1280\n",
      "class 1 clustering acc 0.725: correct 464/640\n",
      "class 0: acc 0.8775510204081632, correct 43/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.1033, instance_loss: 1.7271, weighted_loss: 1.2904, label: 1, bag_size: 94\n",
      "batch 39, loss: 4.6291, instance_loss: 3.0664, weighted_loss: 4.1603, label: 0, bag_size: 49\n",
      "batch 59, loss: 0.0381, instance_loss: 0.0824, weighted_loss: 0.0514, label: 0, bag_size: 52\n",
      "batch 79, loss: 0.6852, instance_loss: 0.1236, weighted_loss: 0.5167, label: 1, bag_size: 55\n",
      "batch 99, loss: 0.0036, instance_loss: 0.0018, weighted_loss: 0.0031, label: 1, bag_size: 41\n",
      "batch 119, loss: 0.4081, instance_loss: 1.5618, weighted_loss: 0.7542, label: 0, bag_size: 71\n",
      "batch 139, loss: 0.0415, instance_loss: 0.0377, weighted_loss: 0.0404, label: 0, bag_size: 53\n",
      "batch 159, loss: 0.0483, instance_loss: 0.1774, weighted_loss: 0.0871, label: 1, bag_size: 27\n",
      "batch 179, loss: 0.0166, instance_loss: 0.0130, weighted_loss: 0.0155, label: 0, bag_size: 110\n",
      "batch 199, loss: 0.0201, instance_loss: 0.0019, weighted_loss: 0.0146, label: 1, bag_size: 102\n",
      "batch 219, loss: 0.1825, instance_loss: 0.8696, weighted_loss: 0.3886, label: 0, bag_size: 24\n",
      "batch 239, loss: 0.0183, instance_loss: 0.0208, weighted_loss: 0.0190, label: 1, bag_size: 79\n",
      "batch 259, loss: 0.0588, instance_loss: 0.0083, weighted_loss: 0.0436, label: 0, bag_size: 64\n",
      "batch 279, loss: 0.0045, instance_loss: 0.0008, weighted_loss: 0.0034, label: 1, bag_size: 87\n",
      "batch 299, loss: 0.0069, instance_loss: 0.0023, weighted_loss: 0.0055, label: 1, bag_size: 36\n",
      "batch 319, loss: 0.0571, instance_loss: 0.0076, weighted_loss: 0.0423, label: 0, bag_size: 49\n",
      "batch 339, loss: 0.0011, instance_loss: 0.0009, weighted_loss: 0.0010, label: 1, bag_size: 76\n",
      "batch 359, loss: 0.0166, instance_loss: 0.3452, weighted_loss: 0.1152, label: 0, bag_size: 129\n",
      "batch 379, loss: 0.0662, instance_loss: 0.1462, weighted_loss: 0.0902, label: 0, bag_size: 75\n",
      "batch 399, loss: 1.5993, instance_loss: 0.0484, weighted_loss: 1.1341, label: 0, bag_size: 66\n",
      "batch 419, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 43\n",
      "batch 439, loss: 0.0344, instance_loss: 0.1977, weighted_loss: 0.0834, label: 1, bag_size: 23\n",
      "batch 459, loss: 0.0245, instance_loss: 0.0006, weighted_loss: 0.0173, label: 1, bag_size: 104\n",
      "batch 479, loss: 0.0333, instance_loss: 0.0045, weighted_loss: 0.0246, label: 0, bag_size: 84\n",
      "batch 499, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 52\n",
      "batch 519, loss: 0.0003, instance_loss: 0.0046, weighted_loss: 0.0016, label: 0, bag_size: 54\n",
      "batch 539, loss: 0.0058, instance_loss: 0.0100, weighted_loss: 0.0071, label: 0, bag_size: 25\n",
      "batch 559, loss: 5.4825, instance_loss: 5.2476, weighted_loss: 5.4120, label: 0, bag_size: 27\n",
      "batch 579, loss: 0.0039, instance_loss: 0.0004, weighted_loss: 0.0029, label: 0, bag_size: 40\n",
      "batch 599, loss: 0.0046, instance_loss: 0.0026, weighted_loss: 0.0040, label: 0, bag_size: 98\n",
      "batch 619, loss: 0.0613, instance_loss: 0.1600, weighted_loss: 0.0909, label: 0, bag_size: 69\n",
      "batch 639, loss: 0.0126, instance_loss: 0.0080, weighted_loss: 0.0112, label: 1, bag_size: 25\n",
      "batch 659, loss: 0.0021, instance_loss: 0.3962, weighted_loss: 0.1203, label: 0, bag_size: 122\n",
      "batch 679, loss: 0.1434, instance_loss: 0.0065, weighted_loss: 0.1023, label: 0, bag_size: 77\n",
      "batch 699, loss: 0.2859, instance_loss: 0.1876, weighted_loss: 0.2564, label: 0, bag_size: 41\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9639184397163121: correct 10873/11280\n",
      "class 1 clustering acc 0.8054964539007092: correct 4543/5640\n",
      "Epoch: 31, train_loss: 0.2838, train_clustering_loss:  0.3429, train_error: 0.1021\n",
      "class 0: acc 0.9046321525885559, correct 332/367\n",
      "class 1: acc 0.8905325443786982, correct 301/338\n",
      "\n",
      "Val Set, val_loss: 0.8968, val_error: 0.2375, auc: 0.8690\n",
      "class 0 clustering acc 0.92890625: correct 1189/1280\n",
      "class 1 clustering acc 0.6828125: correct 437/640\n",
      "class 0: acc 0.9795918367346939, correct 48/49\n",
      "class 1: acc 0.41935483870967744, correct 13/31\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1554, instance_loss: 0.3295, weighted_loss: 0.2076, label: 1, bag_size: 24\n",
      "batch 39, loss: 0.1833, instance_loss: 0.0661, weighted_loss: 0.1481, label: 1, bag_size: 84\n",
      "batch 59, loss: 0.2323, instance_loss: 0.0163, weighted_loss: 0.1675, label: 0, bag_size: 61\n",
      "batch 79, loss: 0.0036, instance_loss: 0.0022, weighted_loss: 0.0032, label: 1, bag_size: 31\n",
      "batch 99, loss: 0.0054, instance_loss: 0.0000, weighted_loss: 0.0038, label: 0, bag_size: 76\n",
      "batch 119, loss: 0.0802, instance_loss: 0.0946, weighted_loss: 0.0845, label: 0, bag_size: 41\n",
      "batch 139, loss: 0.1213, instance_loss: 0.0265, weighted_loss: 0.0929, label: 1, bag_size: 33\n",
      "batch 159, loss: 0.0002, instance_loss: 0.0005, weighted_loss: 0.0003, label: 1, bag_size: 81\n",
      "batch 179, loss: 0.0080, instance_loss: 0.0000, weighted_loss: 0.0056, label: 0, bag_size: 20\n",
      "batch 199, loss: 0.0011, instance_loss: 0.0269, weighted_loss: 0.0089, label: 1, bag_size: 102\n",
      "batch 219, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 59\n",
      "batch 239, loss: 0.0170, instance_loss: 0.1450, weighted_loss: 0.0554, label: 0, bag_size: 57\n",
      "batch 259, loss: 0.0047, instance_loss: 0.0948, weighted_loss: 0.0317, label: 1, bag_size: 52\n",
      "batch 279, loss: 0.0142, instance_loss: 0.0057, weighted_loss: 0.0116, label: 1, bag_size: 103\n",
      "batch 299, loss: 0.3927, instance_loss: 2.5237, weighted_loss: 1.0320, label: 0, bag_size: 97\n",
      "batch 319, loss: 0.0601, instance_loss: 0.7330, weighted_loss: 0.2620, label: 0, bag_size: 82\n",
      "batch 339, loss: 0.0114, instance_loss: 0.0000, weighted_loss: 0.0080, label: 1, bag_size: 52\n",
      "batch 359, loss: 0.3844, instance_loss: 0.8628, weighted_loss: 0.5279, label: 0, bag_size: 53\n",
      "batch 379, loss: 0.0374, instance_loss: 0.0249, weighted_loss: 0.0336, label: 1, bag_size: 25\n",
      "batch 399, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 30\n",
      "batch 419, loss: 0.0289, instance_loss: 0.0098, weighted_loss: 0.0232, label: 0, bag_size: 104\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 47\n",
      "batch 459, loss: 0.0053, instance_loss: 0.0063, weighted_loss: 0.0056, label: 0, bag_size: 85\n",
      "batch 479, loss: 1.3730, instance_loss: 1.5805, weighted_loss: 1.4353, label: 1, bag_size: 89\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 84\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0091, weighted_loss: 0.0028, label: 0, bag_size: 105\n",
      "batch 539, loss: 0.0041, instance_loss: 0.2232, weighted_loss: 0.0699, label: 1, bag_size: 51\n",
      "batch 559, loss: 0.0945, instance_loss: 0.1740, weighted_loss: 0.1184, label: 0, bag_size: 31\n",
      "batch 579, loss: 0.0005, instance_loss: 0.0004, weighted_loss: 0.0005, label: 0, bag_size: 72\n",
      "batch 599, loss: 0.2504, instance_loss: 0.2676, weighted_loss: 0.2556, label: 0, bag_size: 80\n",
      "batch 619, loss: 0.1240, instance_loss: 0.0861, weighted_loss: 0.1127, label: 1, bag_size: 25\n",
      "batch 639, loss: 0.0709, instance_loss: 0.0422, weighted_loss: 0.0623, label: 0, bag_size: 86\n",
      "batch 659, loss: 0.0954, instance_loss: 0.0277, weighted_loss: 0.0751, label: 1, bag_size: 42\n",
      "batch 679, loss: 1.2101, instance_loss: 6.4299, weighted_loss: 2.7761, label: 1, bag_size: 70\n",
      "batch 699, loss: 0.0013, instance_loss: 0.0077, weighted_loss: 0.0032, label: 0, bag_size: 60\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9662234042553192: correct 10899/11280\n",
      "class 1 clustering acc 0.8226950354609929: correct 4640/5640\n",
      "Epoch: 32, train_loss: 0.2263, train_clustering_loss:  0.3169, train_error: 0.0865\n",
      "class 0: acc 0.9213483146067416, correct 328/356\n",
      "class 1: acc 0.9054441260744985, correct 316/349\n",
      "\n",
      "Val Set, val_loss: 0.4243, val_error: 0.1375, auc: 0.9322\n",
      "class 0 clustering acc 0.9359375: correct 1198/1280\n",
      "class 1 clustering acc 0.65: correct 416/640\n",
      "class 0: acc 0.8571428571428571, correct 42/49\n",
      "class 1: acc 0.8709677419354839, correct 27/31\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2295, instance_loss: 0.9264, weighted_loss: 0.4386, label: 0, bag_size: 44\n",
      "batch 39, loss: 0.0070, instance_loss: 0.0000, weighted_loss: 0.0049, label: 1, bag_size: 61\n",
      "batch 59, loss: 2.1677, instance_loss: 2.7434, weighted_loss: 2.3404, label: 0, bag_size: 84\n",
      "batch 79, loss: 0.0101, instance_loss: 0.3419, weighted_loss: 0.1097, label: 0, bag_size: 54\n",
      "batch 99, loss: 0.0029, instance_loss: 0.1021, weighted_loss: 0.0327, label: 0, bag_size: 74\n",
      "batch 119, loss: 0.0993, instance_loss: 0.1916, weighted_loss: 0.1270, label: 1, bag_size: 76\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0021, weighted_loss: 0.0007, label: 1, bag_size: 101\n",
      "batch 159, loss: 0.0287, instance_loss: 0.0155, weighted_loss: 0.0247, label: 0, bag_size: 53\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0017, weighted_loss: 0.0005, label: 1, bag_size: 31\n",
      "batch 199, loss: 0.0010, instance_loss: 0.0824, weighted_loss: 0.0254, label: 1, bag_size: 73\n",
      "batch 219, loss: 0.0038, instance_loss: 0.0146, weighted_loss: 0.0070, label: 0, bag_size: 70\n",
      "batch 239, loss: 0.0007, instance_loss: 0.0005, weighted_loss: 0.0006, label: 0, bag_size: 57\n",
      "batch 259, loss: 0.0151, instance_loss: 0.0193, weighted_loss: 0.0164, label: 0, bag_size: 104\n",
      "batch 279, loss: 0.0005, instance_loss: 0.0024, weighted_loss: 0.0011, label: 1, bag_size: 64\n",
      "batch 299, loss: 0.0010, instance_loss: 0.0109, weighted_loss: 0.0040, label: 0, bag_size: 29\n",
      "batch 319, loss: 0.0018, instance_loss: 0.0001, weighted_loss: 0.0013, label: 0, bag_size: 29\n",
      "batch 339, loss: 0.0027, instance_loss: 0.0055, weighted_loss: 0.0035, label: 0, bag_size: 76\n",
      "batch 359, loss: 0.0013, instance_loss: 0.0007, weighted_loss: 0.0011, label: 0, bag_size: 64\n",
      "batch 379, loss: 0.0256, instance_loss: 0.0676, weighted_loss: 0.0382, label: 0, bag_size: 12\n",
      "batch 399, loss: 1.8087, instance_loss: 3.6820, weighted_loss: 2.3707, label: 1, bag_size: 61\n",
      "batch 419, loss: 0.1318, instance_loss: 0.0641, weighted_loss: 0.1115, label: 0, bag_size: 82\n",
      "batch 439, loss: 0.0265, instance_loss: 0.3033, weighted_loss: 0.1095, label: 1, bag_size: 98\n",
      "batch 459, loss: 0.0004, instance_loss: 0.0203, weighted_loss: 0.0064, label: 0, bag_size: 76\n",
      "batch 479, loss: 0.0014, instance_loss: 0.0285, weighted_loss: 0.0096, label: 0, bag_size: 76\n",
      "batch 499, loss: 0.0013, instance_loss: 0.0019, weighted_loss: 0.0015, label: 1, bag_size: 63\n",
      "batch 519, loss: 0.6007, instance_loss: 0.2115, weighted_loss: 0.4839, label: 1, bag_size: 84\n",
      "batch 539, loss: 3.0086, instance_loss: 0.2264, weighted_loss: 2.1739, label: 0, bag_size: 75\n",
      "batch 559, loss: 0.1718, instance_loss: 0.2688, weighted_loss: 0.2009, label: 0, bag_size: 46\n",
      "batch 579, loss: 0.0006, instance_loss: 0.0321, weighted_loss: 0.0100, label: 1, bag_size: 103\n",
      "batch 599, loss: 0.1418, instance_loss: 0.7232, weighted_loss: 0.3162, label: 1, bag_size: 51\n",
      "batch 619, loss: 0.2355, instance_loss: 1.1675, weighted_loss: 0.5151, label: 1, bag_size: 43\n",
      "batch 639, loss: 0.0011, instance_loss: 0.0146, weighted_loss: 0.0051, label: 1, bag_size: 103\n",
      "batch 659, loss: 0.0457, instance_loss: 0.0031, weighted_loss: 0.0329, label: 0, bag_size: 43\n",
      "batch 679, loss: 0.0025, instance_loss: 0.1920, weighted_loss: 0.0594, label: 0, bag_size: 83\n",
      "batch 699, loss: 0.0494, instance_loss: 0.4993, weighted_loss: 0.1843, label: 0, bag_size: 128\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9639184397163121: correct 10873/11280\n",
      "class 1 clustering acc 0.8205673758865248: correct 4628/5640\n",
      "Epoch: 33, train_loss: 0.2363, train_clustering_loss:  0.3453, train_error: 0.0965\n",
      "class 0: acc 0.9005681818181818, correct 317/352\n",
      "class 1: acc 0.9065155807365439, correct 320/353\n",
      "\n",
      "Val Set, val_loss: 0.5192, val_error: 0.2375, auc: 0.8762\n",
      "class 0 clustering acc 0.94921875: correct 1215/1280\n",
      "class 1 clustering acc 0.703125: correct 450/640\n",
      "class 0: acc 0.8571428571428571, correct 42/49\n",
      "class 1: acc 0.6129032258064516, correct 19/31\n",
      "EarlyStopping counter: 12 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2002, instance_loss: 1.4304, weighted_loss: 0.5693, label: 0, bag_size: 33\n",
      "batch 39, loss: 0.0007, instance_loss: 0.0004, weighted_loss: 0.0006, label: 1, bag_size: 55\n",
      "batch 59, loss: 1.8087, instance_loss: 1.3746, weighted_loss: 1.6785, label: 1, bag_size: 64\n",
      "batch 79, loss: 0.0232, instance_loss: 0.0069, weighted_loss: 0.0183, label: 0, bag_size: 74\n",
      "batch 99, loss: 0.0369, instance_loss: 0.0768, weighted_loss: 0.0489, label: 0, bag_size: 76\n",
      "batch 119, loss: 1.3293, instance_loss: 0.4703, weighted_loss: 1.0716, label: 1, bag_size: 113\n",
      "batch 139, loss: 0.1124, instance_loss: 0.0022, weighted_loss: 0.0794, label: 0, bag_size: 54\n",
      "batch 159, loss: 0.0169, instance_loss: 0.0231, weighted_loss: 0.0188, label: 1, bag_size: 43\n",
      "batch 179, loss: 0.0006, instance_loss: 0.0019, weighted_loss: 0.0010, label: 1, bag_size: 52\n",
      "batch 199, loss: 0.0006, instance_loss: 0.0002, weighted_loss: 0.0005, label: 0, bag_size: 80\n",
      "batch 219, loss: 0.0005, instance_loss: 0.0198, weighted_loss: 0.0063, label: 1, bag_size: 60\n",
      "batch 239, loss: 0.0066, instance_loss: 0.0000, weighted_loss: 0.0046, label: 1, bag_size: 28\n",
      "batch 259, loss: 0.1116, instance_loss: 1.3673, weighted_loss: 0.4883, label: 0, bag_size: 18\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0027, weighted_loss: 0.0008, label: 1, bag_size: 79\n",
      "batch 299, loss: 0.4839, instance_loss: 0.1849, weighted_loss: 0.3942, label: 1, bag_size: 56\n",
      "batch 319, loss: 0.0766, instance_loss: 0.0336, weighted_loss: 0.0637, label: 0, bag_size: 59\n",
      "batch 339, loss: 0.0206, instance_loss: 0.0000, weighted_loss: 0.0144, label: 1, bag_size: 30\n",
      "batch 359, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 112\n",
      "batch 379, loss: 0.0280, instance_loss: 0.2039, weighted_loss: 0.0808, label: 0, bag_size: 79\n",
      "batch 399, loss: 0.0852, instance_loss: 0.0460, weighted_loss: 0.0735, label: 0, bag_size: 45\n",
      "batch 419, loss: 0.6750, instance_loss: 0.0046, weighted_loss: 0.4739, label: 1, bag_size: 86\n",
      "batch 439, loss: 0.0068, instance_loss: 0.0737, weighted_loss: 0.0269, label: 0, bag_size: 64\n",
      "batch 459, loss: 0.0024, instance_loss: 0.1477, weighted_loss: 0.0459, label: 1, bag_size: 27\n",
      "batch 479, loss: 0.0278, instance_loss: 0.0639, weighted_loss: 0.0387, label: 1, bag_size: 61\n",
      "batch 499, loss: 0.0003, instance_loss: 0.0247, weighted_loss: 0.0076, label: 1, bag_size: 30\n",
      "batch 519, loss: 0.0012, instance_loss: 0.0914, weighted_loss: 0.0283, label: 0, bag_size: 36\n",
      "batch 539, loss: 0.0119, instance_loss: 0.0000, weighted_loss: 0.0083, label: 1, bag_size: 37\n",
      "batch 559, loss: 0.0006, instance_loss: 0.0235, weighted_loss: 0.0075, label: 0, bag_size: 62\n",
      "batch 579, loss: 0.0215, instance_loss: 0.0445, weighted_loss: 0.0284, label: 1, bag_size: 27\n",
      "batch 599, loss: 0.0900, instance_loss: 0.0007, weighted_loss: 0.0632, label: 0, bag_size: 64\n",
      "batch 619, loss: 0.0417, instance_loss: 0.1416, weighted_loss: 0.0716, label: 1, bag_size: 113\n",
      "batch 639, loss: 0.0007, instance_loss: 0.0071, weighted_loss: 0.0026, label: 1, bag_size: 43\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 92\n",
      "batch 679, loss: 0.9393, instance_loss: 1.4268, weighted_loss: 1.0856, label: 1, bag_size: 94\n",
      "batch 699, loss: 0.0165, instance_loss: 0.1507, weighted_loss: 0.0567, label: 1, bag_size: 80\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9708333333333333: correct 10951/11280\n",
      "class 1 clustering acc 0.849290780141844: correct 4790/5640\n",
      "Epoch: 34, train_loss: 0.2126, train_clustering_loss:  0.2979, train_error: 0.0794\n",
      "class 0: acc 0.9297297297297298, correct 344/370\n",
      "class 1: acc 0.9104477611940298, correct 305/335\n",
      "\n",
      "Val Set, val_loss: 0.5871, val_error: 0.1750, auc: 0.9105\n",
      "class 0 clustering acc 0.9078125: correct 1162/1280\n",
      "class 1 clustering acc 0.775: correct 496/640\n",
      "class 0: acc 0.9591836734693877, correct 47/49\n",
      "class 1: acc 0.6129032258064516, correct 19/31\n",
      "EarlyStopping counter: 13 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0181, instance_loss: 0.1557, weighted_loss: 0.0594, label: 1, bag_size: 115\n",
      "batch 39, loss: 0.0209, instance_loss: 0.0515, weighted_loss: 0.0301, label: 1, bag_size: 59\n",
      "batch 59, loss: 2.7893, instance_loss: 3.1880, weighted_loss: 2.9089, label: 1, bag_size: 46\n",
      "batch 79, loss: 0.0382, instance_loss: 0.0024, weighted_loss: 0.0275, label: 0, bag_size: 66\n",
      "batch 99, loss: 0.0003, instance_loss: 0.0113, weighted_loss: 0.0036, label: 0, bag_size: 34\n",
      "batch 119, loss: 0.2394, instance_loss: 0.0266, weighted_loss: 0.1756, label: 1, bag_size: 36\n",
      "batch 139, loss: 0.0144, instance_loss: 0.0512, weighted_loss: 0.0255, label: 0, bag_size: 54\n",
      "batch 159, loss: 0.0055, instance_loss: 0.0003, weighted_loss: 0.0039, label: 0, bag_size: 28\n",
      "batch 179, loss: 1.0108, instance_loss: 0.4031, weighted_loss: 0.8285, label: 1, bag_size: 92\n",
      "batch 199, loss: 1.4485, instance_loss: 1.5029, weighted_loss: 1.4648, label: 1, bag_size: 67\n",
      "batch 219, loss: 0.1622, instance_loss: 0.3454, weighted_loss: 0.2171, label: 1, bag_size: 46\n",
      "batch 239, loss: 0.4461, instance_loss: 1.9224, weighted_loss: 0.8890, label: 0, bag_size: 63\n",
      "batch 259, loss: 0.0498, instance_loss: 0.0429, weighted_loss: 0.0478, label: 0, bag_size: 44\n",
      "batch 279, loss: 0.0078, instance_loss: 0.0659, weighted_loss: 0.0252, label: 1, bag_size: 55\n",
      "batch 299, loss: 0.0085, instance_loss: 0.0720, weighted_loss: 0.0276, label: 0, bag_size: 46\n",
      "batch 319, loss: 0.7504, instance_loss: 0.2447, weighted_loss: 0.5987, label: 0, bag_size: 37\n",
      "batch 339, loss: 0.3152, instance_loss: 2.5678, weighted_loss: 0.9910, label: 1, bag_size: 65\n",
      "batch 359, loss: 1.3308, instance_loss: 0.5314, weighted_loss: 1.0910, label: 0, bag_size: 49\n",
      "batch 379, loss: 1.3536, instance_loss: 0.2385, weighted_loss: 1.0191, label: 0, bag_size: 104\n",
      "batch 399, loss: 0.0776, instance_loss: 0.1240, weighted_loss: 0.0915, label: 0, bag_size: 61\n",
      "batch 419, loss: 0.5075, instance_loss: 0.4065, weighted_loss: 0.4772, label: 0, bag_size: 54\n",
      "batch 439, loss: 0.1696, instance_loss: 0.0299, weighted_loss: 0.1277, label: 1, bag_size: 39\n",
      "batch 459, loss: 0.8513, instance_loss: 1.7560, weighted_loss: 1.1227, label: 1, bag_size: 39\n",
      "batch 479, loss: 1.4509, instance_loss: 0.1878, weighted_loss: 1.0720, label: 0, bag_size: 50\n",
      "batch 499, loss: 0.0587, instance_loss: 0.0014, weighted_loss: 0.0415, label: 0, bag_size: 76\n",
      "batch 519, loss: 0.0241, instance_loss: 0.1392, weighted_loss: 0.0587, label: 1, bag_size: 60\n",
      "batch 539, loss: 0.0582, instance_loss: 0.1774, weighted_loss: 0.0939, label: 0, bag_size: 61\n",
      "batch 559, loss: 0.0020, instance_loss: 0.0013, weighted_loss: 0.0017, label: 0, bag_size: 54\n",
      "batch 579, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 1, bag_size: 92\n",
      "batch 599, loss: 0.0009, instance_loss: 1.8597, weighted_loss: 0.5586, label: 0, bag_size: 51\n",
      "batch 619, loss: 0.0382, instance_loss: 0.0022, weighted_loss: 0.0274, label: 0, bag_size: 73\n",
      "batch 639, loss: 0.7119, instance_loss: 0.3679, weighted_loss: 0.6087, label: 0, bag_size: 79\n",
      "batch 659, loss: 1.2877, instance_loss: 1.0728, weighted_loss: 1.2232, label: 1, bag_size: 66\n",
      "batch 679, loss: 0.0189, instance_loss: 0.0016, weighted_loss: 0.0137, label: 1, bag_size: 79\n",
      "batch 699, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 65\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9663120567375887: correct 10900/11280\n",
      "class 1 clustering acc 0.8102836879432624: correct 4570/5640\n",
      "Epoch: 35, train_loss: 0.2485, train_clustering_loss:  0.3236, train_error: 0.1135\n",
      "class 0: acc 0.8994708994708994, correct 340/378\n",
      "class 1: acc 0.8715596330275229, correct 285/327\n",
      "\n",
      "Val Set, val_loss: 0.3976, val_error: 0.1375, auc: 0.9309\n",
      "class 0 clustering acc 0.95: correct 1216/1280\n",
      "class 1 clustering acc 0.7484375: correct 479/640\n",
      "class 0: acc 0.9183673469387755, correct 45/49\n",
      "class 1: acc 0.7741935483870968, correct 24/31\n",
      "Validation loss decreased (0.400438 --> 0.397565).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2745, instance_loss: 0.7709, weighted_loss: 0.4234, label: 1, bag_size: 31\n",
      "batch 39, loss: 0.0118, instance_loss: 0.0858, weighted_loss: 0.0340, label: 1, bag_size: 30\n",
      "batch 59, loss: 0.0012, instance_loss: 0.0005, weighted_loss: 0.0010, label: 1, bag_size: 92\n",
      "batch 79, loss: 0.0892, instance_loss: 0.0816, weighted_loss: 0.0869, label: 0, bag_size: 47\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0006, label: 1, bag_size: 59\n",
      "batch 119, loss: 0.9335, instance_loss: 0.4512, weighted_loss: 0.7888, label: 0, bag_size: 87\n",
      "batch 139, loss: 0.1607, instance_loss: 0.0173, weighted_loss: 0.1177, label: 0, bag_size: 76\n",
      "batch 159, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 70\n",
      "batch 179, loss: 0.0023, instance_loss: 0.0511, weighted_loss: 0.0169, label: 0, bag_size: 28\n",
      "batch 199, loss: 0.0103, instance_loss: 0.0019, weighted_loss: 0.0078, label: 0, bag_size: 136\n",
      "batch 219, loss: 0.1032, instance_loss: 1.0842, weighted_loss: 0.3975, label: 0, bag_size: 56\n",
      "batch 239, loss: 0.0117, instance_loss: 0.0287, weighted_loss: 0.0168, label: 0, bag_size: 81\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0019, weighted_loss: 0.0006, label: 0, bag_size: 69\n",
      "batch 279, loss: 0.0057, instance_loss: 0.5587, weighted_loss: 0.1716, label: 1, bag_size: 95\n",
      "batch 299, loss: 0.0039, instance_loss: 0.0037, weighted_loss: 0.0039, label: 0, bag_size: 97\n",
      "batch 319, loss: 0.1553, instance_loss: 0.5834, weighted_loss: 0.2837, label: 0, bag_size: 29\n",
      "batch 339, loss: 0.0051, instance_loss: 0.0021, weighted_loss: 0.0042, label: 0, bag_size: 72\n",
      "batch 359, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 90\n",
      "batch 379, loss: 0.0079, instance_loss: 0.0006, weighted_loss: 0.0057, label: 0, bag_size: 95\n",
      "batch 399, loss: 0.0579, instance_loss: 1.3087, weighted_loss: 0.4332, label: 0, bag_size: 69\n",
      "batch 419, loss: 0.0384, instance_loss: 0.0149, weighted_loss: 0.0314, label: 0, bag_size: 127\n",
      "batch 439, loss: 0.0026, instance_loss: 0.0000, weighted_loss: 0.0018, label: 1, bag_size: 59\n",
      "batch 459, loss: 0.0164, instance_loss: 0.0095, weighted_loss: 0.0143, label: 1, bag_size: 59\n",
      "batch 479, loss: 0.2166, instance_loss: 0.8910, weighted_loss: 0.4190, label: 1, bag_size: 75\n",
      "batch 499, loss: 0.0123, instance_loss: 0.0000, weighted_loss: 0.0086, label: 1, bag_size: 79\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 55\n",
      "batch 539, loss: 0.0022, instance_loss: 0.1398, weighted_loss: 0.0435, label: 1, bag_size: 64\n",
      "batch 559, loss: 0.1688, instance_loss: 1.2423, weighted_loss: 0.4909, label: 0, bag_size: 45\n",
      "batch 579, loss: 0.0008, instance_loss: 0.0065, weighted_loss: 0.0025, label: 0, bag_size: 68\n",
      "batch 599, loss: 0.0602, instance_loss: 0.8691, weighted_loss: 0.3028, label: 0, bag_size: 108\n",
      "batch 619, loss: 0.1146, instance_loss: 0.0893, weighted_loss: 0.1070, label: 1, bag_size: 43\n",
      "batch 639, loss: 0.0614, instance_loss: 0.0903, weighted_loss: 0.0701, label: 0, bag_size: 61\n",
      "batch 659, loss: 0.0858, instance_loss: 0.8446, weighted_loss: 0.3135, label: 1, bag_size: 63\n",
      "batch 679, loss: 0.0005, instance_loss: 0.0017, weighted_loss: 0.0009, label: 0, bag_size: 30\n",
      "batch 699, loss: 0.7607, instance_loss: 0.0570, weighted_loss: 0.5496, label: 0, bag_size: 88\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9754432624113475: correct 11003/11280\n",
      "class 1 clustering acc 0.8531914893617021: correct 4812/5640\n",
      "Epoch: 36, train_loss: 0.1862, train_clustering_loss:  0.2479, train_error: 0.0809\n",
      "class 0: acc 0.925207756232687, correct 334/361\n",
      "class 1: acc 0.9127906976744186, correct 314/344\n",
      "\n",
      "Val Set, val_loss: 0.6906, val_error: 0.2000, auc: 0.9006\n",
      "class 0 clustering acc 0.9265625: correct 1186/1280\n",
      "class 1 clustering acc 0.8125: correct 520/640\n",
      "class 0: acc 0.7551020408163265, correct 37/49\n",
      "class 1: acc 0.8709677419354839, correct 27/31\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0005, instance_loss: 0.0211, weighted_loss: 0.0066, label: 0, bag_size: 26\n",
      "batch 39, loss: 0.2577, instance_loss: 0.2573, weighted_loss: 0.2576, label: 0, bag_size: 79\n",
      "batch 59, loss: 0.5076, instance_loss: 0.0030, weighted_loss: 0.3562, label: 1, bag_size: 90\n",
      "batch 79, loss: 3.8406, instance_loss: 3.6595, weighted_loss: 3.7862, label: 0, bag_size: 73\n",
      "batch 99, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 52\n",
      "batch 119, loss: 0.0441, instance_loss: 0.2652, weighted_loss: 0.1104, label: 0, bag_size: 24\n",
      "batch 139, loss: 0.0025, instance_loss: 0.0003, weighted_loss: 0.0018, label: 1, bag_size: 92\n",
      "batch 159, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 34\n",
      "batch 179, loss: 0.0040, instance_loss: 0.0212, weighted_loss: 0.0092, label: 0, bag_size: 60\n",
      "batch 199, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 60\n",
      "batch 219, loss: 0.0702, instance_loss: 0.0467, weighted_loss: 0.0632, label: 1, bag_size: 92\n",
      "batch 239, loss: 0.8783, instance_loss: 2.1350, weighted_loss: 1.2553, label: 1, bag_size: 73\n",
      "batch 259, loss: 0.4376, instance_loss: 0.5063, weighted_loss: 0.4582, label: 0, bag_size: 75\n",
      "batch 279, loss: 0.0245, instance_loss: 0.0021, weighted_loss: 0.0178, label: 1, bag_size: 120\n",
      "batch 299, loss: 0.0002, instance_loss: 0.0010, weighted_loss: 0.0004, label: 0, bag_size: 98\n",
      "batch 319, loss: 0.1638, instance_loss: 0.1297, weighted_loss: 0.1535, label: 0, bag_size: 61\n",
      "batch 339, loss: 0.0385, instance_loss: 0.0066, weighted_loss: 0.0289, label: 1, bag_size: 55\n",
      "batch 359, loss: 0.0258, instance_loss: 0.0454, weighted_loss: 0.0317, label: 0, bag_size: 84\n",
      "batch 379, loss: 0.0286, instance_loss: 1.7913, weighted_loss: 0.5574, label: 0, bag_size: 91\n",
      "batch 399, loss: 0.0043, instance_loss: 0.0025, weighted_loss: 0.0038, label: 0, bag_size: 97\n",
      "batch 419, loss: 0.0215, instance_loss: 0.0003, weighted_loss: 0.0151, label: 0, bag_size: 79\n",
      "batch 439, loss: 0.0887, instance_loss: 0.0569, weighted_loss: 0.0792, label: 1, bag_size: 37\n",
      "batch 459, loss: 0.0269, instance_loss: 0.4344, weighted_loss: 0.1491, label: 0, bag_size: 72\n",
      "batch 479, loss: 0.0389, instance_loss: 0.0581, weighted_loss: 0.0447, label: 1, bag_size: 54\n",
      "batch 499, loss: 0.0289, instance_loss: 0.0275, weighted_loss: 0.0285, label: 1, bag_size: 46\n",
      "batch 519, loss: 0.0013, instance_loss: 0.0016, weighted_loss: 0.0014, label: 1, bag_size: 101\n",
      "batch 539, loss: 0.0010, instance_loss: 0.0020, weighted_loss: 0.0013, label: 1, bag_size: 113\n",
      "batch 559, loss: 0.4080, instance_loss: 0.1719, weighted_loss: 0.3371, label: 1, bag_size: 92\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0029, weighted_loss: 0.0009, label: 0, bag_size: 95\n",
      "batch 599, loss: 0.4021, instance_loss: 1.4837, weighted_loss: 0.7266, label: 1, bag_size: 76\n",
      "batch 619, loss: 0.0030, instance_loss: 0.0023, weighted_loss: 0.0028, label: 1, bag_size: 102\n",
      "batch 639, loss: 0.0016, instance_loss: 0.0056, weighted_loss: 0.0028, label: 0, bag_size: 87\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 55\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 71\n",
      "batch 699, loss: 0.0096, instance_loss: 0.0006, weighted_loss: 0.0069, label: 1, bag_size: 62\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9725177304964538: correct 10970/11280\n",
      "class 1 clustering acc 0.8439716312056738: correct 4760/5640\n",
      "Epoch: 37, train_loss: 0.2047, train_clustering_loss:  0.2764, train_error: 0.0794\n",
      "class 0: acc 0.9202127659574468, correct 346/376\n",
      "class 1: acc 0.9209726443768997, correct 303/329\n",
      "\n",
      "Val Set, val_loss: 0.8101, val_error: 0.2000, auc: 0.9019\n",
      "class 0 clustering acc 0.93046875: correct 1191/1280\n",
      "class 1 clustering acc 0.7625: correct 488/640\n",
      "class 0: acc 0.9795918367346939, correct 48/49\n",
      "class 1: acc 0.5161290322580645, correct 16/31\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0021, instance_loss: 0.0350, weighted_loss: 0.0120, label: 1, bag_size: 78\n",
      "batch 39, loss: 1.1452, instance_loss: 0.0555, weighted_loss: 0.8183, label: 1, bag_size: 60\n",
      "batch 59, loss: 0.0090, instance_loss: 0.0000, weighted_loss: 0.0063, label: 0, bag_size: 45\n",
      "batch 79, loss: 0.0641, instance_loss: 0.0000, weighted_loss: 0.0449, label: 0, bag_size: 92\n",
      "batch 99, loss: 1.4580, instance_loss: 0.4429, weighted_loss: 1.1535, label: 0, bag_size: 128\n",
      "batch 119, loss: 0.0022, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 47\n",
      "batch 139, loss: 0.0003, instance_loss: 0.0166, weighted_loss: 0.0052, label: 1, bag_size: 50\n",
      "batch 159, loss: 0.0024, instance_loss: 0.0075, weighted_loss: 0.0039, label: 0, bag_size: 68\n",
      "batch 179, loss: 0.0802, instance_loss: 0.0122, weighted_loss: 0.0598, label: 1, bag_size: 107\n",
      "batch 199, loss: 0.0018, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 101\n",
      "batch 219, loss: 0.2797, instance_loss: 0.6171, weighted_loss: 0.3809, label: 1, bag_size: 26\n",
      "batch 239, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 140\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 97\n",
      "batch 279, loss: 0.0104, instance_loss: 0.0030, weighted_loss: 0.0082, label: 0, bag_size: 70\n",
      "batch 299, loss: 0.0016, instance_loss: 0.0020, weighted_loss: 0.0017, label: 1, bag_size: 80\n",
      "batch 319, loss: 0.8258, instance_loss: 1.2637, weighted_loss: 0.9571, label: 1, bag_size: 86\n",
      "batch 339, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 77\n",
      "batch 359, loss: 0.0714, instance_loss: 0.0237, weighted_loss: 0.0571, label: 0, bag_size: 66\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 91\n",
      "batch 399, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 53\n",
      "batch 419, loss: 2.3721, instance_loss: 0.0385, weighted_loss: 1.6720, label: 1, bag_size: 75\n",
      "batch 439, loss: 3.8140, instance_loss: 2.2991, weighted_loss: 3.3595, label: 0, bag_size: 76\n",
      "batch 459, loss: 0.0149, instance_loss: 0.5335, weighted_loss: 0.1705, label: 1, bag_size: 95\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0003, weighted_loss: 0.0002, label: 0, bag_size: 41\n",
      "batch 499, loss: 0.1631, instance_loss: 0.0029, weighted_loss: 0.1150, label: 0, bag_size: 89\n",
      "batch 519, loss: 0.0165, instance_loss: 0.0654, weighted_loss: 0.0312, label: 1, bag_size: 57\n",
      "batch 539, loss: 0.0070, instance_loss: 0.0000, weighted_loss: 0.0049, label: 0, bag_size: 73\n",
      "batch 559, loss: 0.6464, instance_loss: 1.1561, weighted_loss: 0.7993, label: 0, bag_size: 29\n",
      "batch 579, loss: 0.7015, instance_loss: 0.4037, weighted_loss: 0.6121, label: 1, bag_size: 11\n",
      "batch 599, loss: 2.1874, instance_loss: 1.1554, weighted_loss: 1.8778, label: 0, bag_size: 61\n",
      "batch 619, loss: 0.0614, instance_loss: 0.4233, weighted_loss: 0.1700, label: 0, bag_size: 42\n",
      "batch 639, loss: 0.0134, instance_loss: 0.2254, weighted_loss: 0.0770, label: 1, bag_size: 106\n",
      "batch 659, loss: 0.0001, instance_loss: 0.0024, weighted_loss: 0.0008, label: 1, bag_size: 45\n",
      "batch 679, loss: 0.0385, instance_loss: 0.0320, weighted_loss: 0.0366, label: 1, bag_size: 71\n",
      "batch 699, loss: 0.0062, instance_loss: 0.0026, weighted_loss: 0.0051, label: 1, bag_size: 92\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9743794326241135: correct 10991/11280\n",
      "class 1 clustering acc 0.8562056737588652: correct 4829/5640\n",
      "Epoch: 38, train_loss: 0.2228, train_clustering_loss:  0.2573, train_error: 0.0979\n",
      "class 0: acc 0.9142857142857143, correct 320/350\n",
      "class 1: acc 0.8901408450704226, correct 316/355\n",
      "\n",
      "Val Set, val_loss: 0.8320, val_error: 0.2125, auc: 0.8966\n",
      "class 0 clustering acc 0.91953125: correct 1177/1280\n",
      "class 1 clustering acc 0.7515625: correct 481/640\n",
      "class 0: acc 0.9795918367346939, correct 48/49\n",
      "class 1: acc 0.4838709677419355, correct 15/31\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1868, instance_loss: 0.0470, weighted_loss: 0.1448, label: 0, bag_size: 54\n",
      "batch 39, loss: 0.0343, instance_loss: 0.0180, weighted_loss: 0.0294, label: 0, bag_size: 104\n",
      "batch 59, loss: 0.0063, instance_loss: 0.0011, weighted_loss: 0.0048, label: 1, bag_size: 53\n",
      "batch 79, loss: 0.0014, instance_loss: 0.0050, weighted_loss: 0.0025, label: 1, bag_size: 92\n",
      "batch 99, loss: 0.0020, instance_loss: 0.0019, weighted_loss: 0.0020, label: 0, bag_size: 88\n",
      "batch 119, loss: 0.0574, instance_loss: 0.0661, weighted_loss: 0.0600, label: 1, bag_size: 38\n",
      "batch 139, loss: 0.0011, instance_loss: 0.0699, weighted_loss: 0.0218, label: 0, bag_size: 51\n",
      "batch 159, loss: 0.0021, instance_loss: 0.0036, weighted_loss: 0.0026, label: 0, bag_size: 41\n",
      "batch 179, loss: 0.1875, instance_loss: 0.0665, weighted_loss: 0.1512, label: 1, bag_size: 91\n",
      "batch 199, loss: 0.0125, instance_loss: 0.4193, weighted_loss: 0.1345, label: 1, bag_size: 84\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 55\n",
      "batch 239, loss: 0.0160, instance_loss: 0.0014, weighted_loss: 0.0116, label: 1, bag_size: 40\n",
      "batch 259, loss: 0.8913, instance_loss: 2.1093, weighted_loss: 1.2567, label: 0, bag_size: 80\n",
      "batch 279, loss: 0.2647, instance_loss: 4.0478, weighted_loss: 1.3996, label: 0, bag_size: 102\n",
      "batch 299, loss: 0.4084, instance_loss: 0.0005, weighted_loss: 0.2860, label: 0, bag_size: 64\n",
      "batch 319, loss: 0.0201, instance_loss: 0.2029, weighted_loss: 0.0749, label: 1, bag_size: 37\n",
      "batch 339, loss: 0.0062, instance_loss: 0.0351, weighted_loss: 0.0149, label: 0, bag_size: 35\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0056, weighted_loss: 0.0017, label: 1, bag_size: 65\n",
      "batch 379, loss: 0.0002, instance_loss: 0.0018, weighted_loss: 0.0007, label: 1, bag_size: 43\n",
      "batch 399, loss: 0.0011, instance_loss: 0.0001, weighted_loss: 0.0008, label: 0, bag_size: 95\n",
      "batch 419, loss: 0.0004, instance_loss: 0.0024, weighted_loss: 0.0010, label: 1, bag_size: 62\n",
      "batch 439, loss: 0.0056, instance_loss: 0.0038, weighted_loss: 0.0051, label: 1, bag_size: 25\n",
      "batch 459, loss: 0.0110, instance_loss: 0.0038, weighted_loss: 0.0088, label: 1, bag_size: 54\n",
      "batch 479, loss: 0.0138, instance_loss: 0.0000, weighted_loss: 0.0097, label: 0, bag_size: 74\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 81\n",
      "batch 519, loss: 0.1192, instance_loss: 0.0271, weighted_loss: 0.0916, label: 0, bag_size: 108\n",
      "batch 539, loss: 0.0558, instance_loss: 1.2254, weighted_loss: 0.4066, label: 0, bag_size: 40\n",
      "batch 559, loss: 0.0061, instance_loss: 0.4010, weighted_loss: 0.1246, label: 0, bag_size: 60\n",
      "batch 579, loss: 0.0150, instance_loss: 0.0201, weighted_loss: 0.0165, label: 0, bag_size: 74\n",
      "batch 599, loss: 0.0140, instance_loss: 1.0171, weighted_loss: 0.3149, label: 0, bag_size: 38\n",
      "batch 619, loss: 0.0049, instance_loss: 0.0007, weighted_loss: 0.0036, label: 0, bag_size: 42\n",
      "batch 639, loss: 0.0110, instance_loss: 0.0003, weighted_loss: 0.0078, label: 0, bag_size: 61\n",
      "batch 659, loss: 0.0028, instance_loss: 0.1011, weighted_loss: 0.0323, label: 0, bag_size: 65\n",
      "batch 679, loss: 0.0002, instance_loss: 0.0110, weighted_loss: 0.0035, label: 1, bag_size: 51\n",
      "batch 699, loss: 0.0020, instance_loss: 0.0002, weighted_loss: 0.0014, label: 0, bag_size: 45\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.973404255319149: correct 10980/11280\n",
      "class 1 clustering acc 0.8523049645390071: correct 4807/5640\n",
      "Epoch: 39, train_loss: 0.2026, train_clustering_loss:  0.2740, train_error: 0.0766\n",
      "class 0: acc 0.9224137931034483, correct 321/348\n",
      "class 1: acc 0.9243697478991597, correct 330/357\n",
      "\n",
      "Val Set, val_loss: 0.5261, val_error: 0.1625, auc: 0.9197\n",
      "class 0 clustering acc 0.91328125: correct 1169/1280\n",
      "class 1 clustering acc 0.778125: correct 498/640\n",
      "class 0: acc 0.8163265306122449, correct 40/49\n",
      "class 1: acc 0.8709677419354839, correct 27/31\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0099, instance_loss: 0.1338, weighted_loss: 0.0471, label: 1, bag_size: 34\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0036, weighted_loss: 0.0011, label: 1, bag_size: 62\n",
      "batch 59, loss: 0.0028, instance_loss: 0.0042, weighted_loss: 0.0032, label: 1, bag_size: 39\n",
      "batch 79, loss: 0.0165, instance_loss: 0.0166, weighted_loss: 0.0165, label: 1, bag_size: 60\n",
      "batch 99, loss: 0.0006, instance_loss: 0.0006, weighted_loss: 0.0006, label: 1, bag_size: 90\n",
      "batch 119, loss: 0.0034, instance_loss: 0.0071, weighted_loss: 0.0045, label: 1, bag_size: 101\n",
      "batch 139, loss: 0.0033, instance_loss: 0.0000, weighted_loss: 0.0023, label: 0, bag_size: 68\n",
      "batch 159, loss: 0.0273, instance_loss: 0.0026, weighted_loss: 0.0199, label: 1, bag_size: 54\n",
      "batch 179, loss: 0.0423, instance_loss: 0.0806, weighted_loss: 0.0538, label: 0, bag_size: 69\n",
      "batch 199, loss: 0.0218, instance_loss: 0.0642, weighted_loss: 0.0345, label: 0, bag_size: 70\n",
      "batch 219, loss: 0.0768, instance_loss: 0.1630, weighted_loss: 0.1026, label: 1, bag_size: 91\n",
      "batch 239, loss: 0.0016, instance_loss: 0.0198, weighted_loss: 0.0071, label: 1, bag_size: 52\n",
      "batch 259, loss: 0.0423, instance_loss: 0.3769, weighted_loss: 0.1427, label: 1, bag_size: 66\n",
      "batch 279, loss: 0.0061, instance_loss: 0.3141, weighted_loss: 0.0985, label: 1, bag_size: 73\n",
      "batch 299, loss: 0.0020, instance_loss: 0.0028, weighted_loss: 0.0022, label: 1, bag_size: 52\n",
      "batch 319, loss: 0.2489, instance_loss: 0.3315, weighted_loss: 0.2737, label: 0, bag_size: 73\n",
      "batch 339, loss: 1.6243, instance_loss: 0.4212, weighted_loss: 1.2633, label: 1, bag_size: 91\n",
      "batch 359, loss: 0.0036, instance_loss: 0.0004, weighted_loss: 0.0026, label: 0, bag_size: 71\n",
      "batch 379, loss: 0.0647, instance_loss: 0.0065, weighted_loss: 0.0472, label: 0, bag_size: 42\n",
      "batch 399, loss: 0.1443, instance_loss: 0.0657, weighted_loss: 0.1208, label: 0, bag_size: 61\n",
      "batch 419, loss: 0.0032, instance_loss: 0.0007, weighted_loss: 0.0024, label: 0, bag_size: 127\n",
      "batch 439, loss: 0.0144, instance_loss: 0.0112, weighted_loss: 0.0135, label: 0, bag_size: 44\n",
      "batch 459, loss: 0.4458, instance_loss: 0.0185, weighted_loss: 0.3176, label: 1, bag_size: 89\n",
      "batch 479, loss: 0.0033, instance_loss: 0.0001, weighted_loss: 0.0023, label: 0, bag_size: 67\n",
      "batch 499, loss: 0.1015, instance_loss: 0.0702, weighted_loss: 0.0921, label: 0, bag_size: 95\n",
      "batch 519, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 66\n",
      "batch 539, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 44\n",
      "batch 559, loss: 0.0013, instance_loss: 0.3093, weighted_loss: 0.0937, label: 1, bag_size: 84\n",
      "batch 579, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 40\n",
      "batch 599, loss: 0.0320, instance_loss: 0.0001, weighted_loss: 0.0224, label: 1, bag_size: 41\n",
      "batch 619, loss: 0.0070, instance_loss: 0.0025, weighted_loss: 0.0057, label: 1, bag_size: 92\n",
      "batch 639, loss: 0.0173, instance_loss: 0.3929, weighted_loss: 0.1300, label: 0, bag_size: 95\n",
      "batch 659, loss: 0.0042, instance_loss: 0.0065, weighted_loss: 0.0049, label: 0, bag_size: 73\n",
      "batch 679, loss: 0.0298, instance_loss: 0.0006, weighted_loss: 0.0211, label: 0, bag_size: 30\n",
      "batch 699, loss: 0.0024, instance_loss: 0.0026, weighted_loss: 0.0025, label: 0, bag_size: 50\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.974113475177305: correct 10988/11280\n",
      "class 1 clustering acc 0.8709219858156029: correct 4912/5640\n",
      "Epoch: 40, train_loss: 0.1943, train_clustering_loss:  0.2677, train_error: 0.0823\n",
      "class 0: acc 0.9232954545454546, correct 325/352\n",
      "class 1: acc 0.9121813031161473, correct 322/353\n",
      "\n",
      "Val Set, val_loss: 0.6119, val_error: 0.1750, auc: 0.8940\n",
      "class 0 clustering acc 0.91328125: correct 1169/1280\n",
      "class 1 clustering acc 0.7625: correct 488/640\n",
      "class 0: acc 0.8367346938775511, correct 41/49\n",
      "class 1: acc 0.8064516129032258, correct 25/31\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0003, instance_loss: 0.0433, weighted_loss: 0.0132, label: 0, bag_size: 91\n",
      "batch 39, loss: 0.7737, instance_loss: 1.0135, weighted_loss: 0.8456, label: 1, bag_size: 66\n",
      "batch 59, loss: 0.0390, instance_loss: 0.0291, weighted_loss: 0.0361, label: 1, bag_size: 55\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 41\n",
      "batch 99, loss: 0.0760, instance_loss: 0.0969, weighted_loss: 0.0823, label: 1, bag_size: 25\n",
      "batch 119, loss: 0.0039, instance_loss: 0.0029, weighted_loss: 0.0036, label: 1, bag_size: 30\n",
      "batch 139, loss: 0.0150, instance_loss: 0.0000, weighted_loss: 0.0105, label: 1, bag_size: 115\n",
      "batch 159, loss: 1.6356, instance_loss: 0.2052, weighted_loss: 1.2065, label: 1, bag_size: 110\n",
      "batch 179, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 73\n",
      "batch 199, loss: 0.0029, instance_loss: 0.0166, weighted_loss: 0.0070, label: 0, bag_size: 54\n",
      "batch 219, loss: 0.0843, instance_loss: 0.0731, weighted_loss: 0.0810, label: 1, bag_size: 11\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 77\n",
      "batch 259, loss: 0.2762, instance_loss: 0.0010, weighted_loss: 0.1936, label: 0, bag_size: 107\n",
      "batch 279, loss: 0.3817, instance_loss: 0.0242, weighted_loss: 0.2745, label: 1, bag_size: 89\n",
      "batch 299, loss: 0.0059, instance_loss: 0.0019, weighted_loss: 0.0047, label: 1, bag_size: 37\n",
      "batch 319, loss: 0.0145, instance_loss: 0.0268, weighted_loss: 0.0182, label: 1, bag_size: 62\n",
      "batch 339, loss: 0.8195, instance_loss: 2.8949, weighted_loss: 1.4421, label: 1, bag_size: 46\n",
      "batch 359, loss: 0.8224, instance_loss: 0.5282, weighted_loss: 0.7341, label: 0, bag_size: 43\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 65\n",
      "batch 399, loss: 0.1303, instance_loss: 0.2418, weighted_loss: 0.1637, label: 0, bag_size: 56\n",
      "batch 419, loss: 0.0122, instance_loss: 0.0295, weighted_loss: 0.0174, label: 1, bag_size: 98\n",
      "batch 439, loss: 0.0997, instance_loss: 0.0358, weighted_loss: 0.0805, label: 1, bag_size: 84\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 60\n",
      "batch 479, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 54\n",
      "batch 499, loss: 0.1727, instance_loss: 0.0694, weighted_loss: 0.1417, label: 1, bag_size: 30\n",
      "batch 519, loss: 0.0709, instance_loss: 0.3282, weighted_loss: 0.1481, label: 0, bag_size: 81\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 27\n",
      "batch 559, loss: 0.0949, instance_loss: 0.4957, weighted_loss: 0.2151, label: 0, bag_size: 31\n",
      "batch 579, loss: 0.0007, instance_loss: 0.0712, weighted_loss: 0.0218, label: 1, bag_size: 65\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 0, bag_size: 95\n",
      "batch 619, loss: 0.0263, instance_loss: 0.7548, weighted_loss: 0.2448, label: 0, bag_size: 53\n",
      "batch 639, loss: 0.0073, instance_loss: 1.0992, weighted_loss: 0.3349, label: 1, bag_size: 107\n",
      "batch 659, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 27\n",
      "batch 679, loss: 0.0068, instance_loss: 0.0538, weighted_loss: 0.0209, label: 1, bag_size: 21\n",
      "batch 699, loss: 0.0447, instance_loss: 0.0000, weighted_loss: 0.0313, label: 1, bag_size: 120\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9744680851063829: correct 10992/11280\n",
      "class 1 clustering acc 0.8801418439716312: correct 4964/5640\n",
      "Epoch: 41, train_loss: 0.1455, train_clustering_loss:  0.2337, train_error: 0.0525\n",
      "class 0: acc 0.9552631578947368, correct 363/380\n",
      "class 1: acc 0.9384615384615385, correct 305/325\n",
      "\n",
      "Val Set, val_loss: 0.6942, val_error: 0.1500, auc: 0.8986\n",
      "class 0 clustering acc 0.91796875: correct 1175/1280\n",
      "class 1 clustering acc 0.7640625: correct 489/640\n",
      "class 0: acc 0.9183673469387755, correct 45/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.9679, instance_loss: 0.5345, weighted_loss: 1.5379, label: 0, bag_size: 84\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0029, weighted_loss: 0.0009, label: 0, bag_size: 44\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0044, weighted_loss: 0.0013, label: 1, bag_size: 71\n",
      "batch 79, loss: 0.1479, instance_loss: 0.1420, weighted_loss: 0.1461, label: 0, bag_size: 54\n",
      "batch 99, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 62\n",
      "batch 119, loss: 0.0023, instance_loss: 0.0416, weighted_loss: 0.0141, label: 0, bag_size: 61\n",
      "batch 139, loss: 0.0070, instance_loss: 0.0101, weighted_loss: 0.0079, label: 1, bag_size: 84\n",
      "batch 159, loss: 1.5831, instance_loss: 0.1125, weighted_loss: 1.1419, label: 0, bag_size: 70\n",
      "batch 179, loss: 0.7106, instance_loss: 0.3014, weighted_loss: 0.5878, label: 1, bag_size: 57\n",
      "batch 199, loss: 0.0003, instance_loss: 0.0008, weighted_loss: 0.0004, label: 0, bag_size: 40\n",
      "batch 219, loss: 0.0017, instance_loss: 0.0021, weighted_loss: 0.0019, label: 1, bag_size: 52\n",
      "batch 239, loss: 0.0011, instance_loss: 0.0021, weighted_loss: 0.0014, label: 1, bag_size: 33\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 105\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 54\n",
      "batch 299, loss: 0.0012, instance_loss: 0.1644, weighted_loss: 0.0501, label: 0, bag_size: 40\n",
      "batch 319, loss: 0.0020, instance_loss: 0.3891, weighted_loss: 0.1182, label: 0, bag_size: 97\n",
      "batch 339, loss: 1.1041, instance_loss: 3.4199, weighted_loss: 1.7988, label: 1, bag_size: 44\n",
      "batch 359, loss: 0.0340, instance_loss: 0.3513, weighted_loss: 0.1292, label: 0, bag_size: 80\n",
      "batch 379, loss: 0.0072, instance_loss: 0.0106, weighted_loss: 0.0082, label: 1, bag_size: 89\n",
      "batch 399, loss: 0.0057, instance_loss: 0.0075, weighted_loss: 0.0062, label: 0, bag_size: 51\n",
      "batch 419, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 84\n",
      "batch 439, loss: 0.0211, instance_loss: 0.0111, weighted_loss: 0.0181, label: 1, bag_size: 55\n",
      "batch 459, loss: 0.0011, instance_loss: 0.0520, weighted_loss: 0.0164, label: 1, bag_size: 66\n",
      "batch 479, loss: 0.0079, instance_loss: 0.0122, weighted_loss: 0.0092, label: 1, bag_size: 98\n",
      "batch 499, loss: 0.0567, instance_loss: 0.0062, weighted_loss: 0.0416, label: 1, bag_size: 114\n",
      "batch 519, loss: 0.0064, instance_loss: 0.0062, weighted_loss: 0.0064, label: 0, bag_size: 31\n",
      "batch 539, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 46\n",
      "batch 559, loss: 0.0038, instance_loss: 0.0006, weighted_loss: 0.0028, label: 1, bag_size: 43\n",
      "batch 579, loss: 0.1428, instance_loss: 1.4060, weighted_loss: 0.5218, label: 1, bag_size: 42\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 92\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 114\n",
      "batch 639, loss: 0.0004, instance_loss: 0.0183, weighted_loss: 0.0058, label: 1, bag_size: 54\n",
      "batch 659, loss: 0.0556, instance_loss: 0.3851, weighted_loss: 0.1544, label: 0, bag_size: 84\n",
      "batch 679, loss: 0.0002, instance_loss: 0.0021, weighted_loss: 0.0008, label: 0, bag_size: 60\n",
      "batch 699, loss: 0.3045, instance_loss: 0.0806, weighted_loss: 0.2373, label: 1, bag_size: 51\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9762411347517731: correct 11012/11280\n",
      "class 1 clustering acc 0.8918439716312057: correct 5030/5640\n",
      "Epoch: 42, train_loss: 0.1656, train_clustering_loss:  0.2111, train_error: 0.0681\n",
      "class 0: acc 0.9346049046321526, correct 343/367\n",
      "class 1: acc 0.9289940828402367, correct 314/338\n",
      "\n",
      "Val Set, val_loss: 0.7042, val_error: 0.1625, auc: 0.8999\n",
      "class 0 clustering acc 0.91328125: correct 1169/1280\n",
      "class 1 clustering acc 0.75625: correct 484/640\n",
      "class 0: acc 0.8979591836734694, correct 44/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 34\n",
      "batch 39, loss: 0.6619, instance_loss: 2.2905, weighted_loss: 1.1505, label: 0, bag_size: 31\n",
      "batch 59, loss: 0.0028, instance_loss: 0.0063, weighted_loss: 0.0039, label: 1, bag_size: 11\n",
      "batch 79, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 36\n",
      "batch 99, loss: 0.1201, instance_loss: 0.4582, weighted_loss: 0.2215, label: 1, bag_size: 69\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 39\n",
      "batch 139, loss: 0.0031, instance_loss: 0.0000, weighted_loss: 0.0021, label: 0, bag_size: 80\n",
      "batch 159, loss: 0.0017, instance_loss: 0.0149, weighted_loss: 0.0056, label: 0, bag_size: 84\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0219, weighted_loss: 0.0066, label: 1, bag_size: 42\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 115\n",
      "batch 219, loss: 0.2073, instance_loss: 1.0617, weighted_loss: 0.4636, label: 0, bag_size: 64\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 65\n",
      "batch 259, loss: 0.2009, instance_loss: 2.9394, weighted_loss: 1.0225, label: 1, bag_size: 92\n",
      "batch 279, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 70\n",
      "batch 299, loss: 0.0530, instance_loss: 0.1692, weighted_loss: 0.0878, label: 1, bag_size: 69\n",
      "batch 319, loss: 0.0019, instance_loss: 0.0012, weighted_loss: 0.0017, label: 1, bag_size: 57\n",
      "batch 339, loss: 0.0015, instance_loss: 0.0326, weighted_loss: 0.0108, label: 1, bag_size: 64\n",
      "batch 359, loss: 0.0047, instance_loss: 0.0110, weighted_loss: 0.0066, label: 1, bag_size: 57\n",
      "batch 379, loss: 0.0139, instance_loss: 0.1776, weighted_loss: 0.0630, label: 1, bag_size: 60\n",
      "batch 399, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 87\n",
      "batch 419, loss: 0.0353, instance_loss: 0.0083, weighted_loss: 0.0272, label: 1, bag_size: 70\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 95\n",
      "batch 459, loss: 0.0410, instance_loss: 0.0839, weighted_loss: 0.0539, label: 1, bag_size: 23\n",
      "batch 479, loss: 0.0002, instance_loss: 0.0052, weighted_loss: 0.0017, label: 0, bag_size: 65\n",
      "batch 499, loss: 0.0091, instance_loss: 0.0513, weighted_loss: 0.0218, label: 0, bag_size: 54\n",
      "batch 519, loss: 0.0080, instance_loss: 0.0000, weighted_loss: 0.0056, label: 0, bag_size: 36\n",
      "batch 539, loss: 0.0063, instance_loss: 0.1026, weighted_loss: 0.0352, label: 0, bag_size: 93\n",
      "batch 559, loss: 0.0003, instance_loss: 0.3000, weighted_loss: 0.0902, label: 1, bag_size: 60\n",
      "batch 579, loss: 1.8147, instance_loss: 1.2780, weighted_loss: 1.6537, label: 1, bag_size: 28\n",
      "batch 599, loss: 0.0096, instance_loss: 0.0163, weighted_loss: 0.0116, label: 1, bag_size: 57\n",
      "batch 619, loss: 0.0003, instance_loss: 0.0014, weighted_loss: 0.0007, label: 0, bag_size: 92\n",
      "batch 639, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 80\n",
      "batch 659, loss: 0.0038, instance_loss: 0.0206, weighted_loss: 0.0089, label: 0, bag_size: 52\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 95\n",
      "batch 699, loss: 0.0070, instance_loss: 0.0020, weighted_loss: 0.0055, label: 0, bag_size: 58\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9743794326241135: correct 10991/11280\n",
      "class 1 clustering acc 0.8831560283687944: correct 4981/5640\n",
      "Epoch: 43, train_loss: 0.1430, train_clustering_loss:  0.2229, train_error: 0.0525\n",
      "class 0: acc 0.9515669515669516, correct 334/351\n",
      "class 1: acc 0.943502824858757, correct 334/354\n",
      "\n",
      "Val Set, val_loss: 0.7240, val_error: 0.1875, auc: 0.8855\n",
      "class 0 clustering acc 0.9296875: correct 1190/1280\n",
      "class 1 clustering acc 0.7359375: correct 471/640\n",
      "class 0: acc 0.8571428571428571, correct 42/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0458, instance_loss: 0.1035, weighted_loss: 0.0631, label: 1, bag_size: 39\n",
      "batch 39, loss: 0.5598, instance_loss: 0.9618, weighted_loss: 0.6804, label: 1, bag_size: 62\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 92\n",
      "batch 79, loss: 0.0008, instance_loss: 2.2669, weighted_loss: 0.6807, label: 0, bag_size: 18\n",
      "batch 99, loss: 0.0648, instance_loss: 0.0148, weighted_loss: 0.0498, label: 0, bag_size: 80\n",
      "batch 119, loss: 0.0843, instance_loss: 0.7828, weighted_loss: 0.2939, label: 1, bag_size: 71\n",
      "batch 139, loss: 0.0009, instance_loss: 0.0180, weighted_loss: 0.0060, label: 1, bag_size: 62\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 79\n",
      "batch 179, loss: 0.0044, instance_loss: 1.1197, weighted_loss: 0.3390, label: 1, bag_size: 86\n",
      "batch 199, loss: 0.0311, instance_loss: 0.0032, weighted_loss: 0.0227, label: 1, bag_size: 31\n",
      "batch 219, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 75\n",
      "batch 239, loss: 0.0231, instance_loss: 0.0219, weighted_loss: 0.0227, label: 1, bag_size: 119\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 54\n",
      "batch 279, loss: 0.3468, instance_loss: 3.7496, weighted_loss: 1.3677, label: 0, bag_size: 18\n",
      "batch 299, loss: 0.4201, instance_loss: 0.0748, weighted_loss: 0.3165, label: 1, bag_size: 39\n",
      "batch 319, loss: 0.2475, instance_loss: 4.9450, weighted_loss: 1.6568, label: 1, bag_size: 84\n",
      "batch 339, loss: 0.0100, instance_loss: 0.1869, weighted_loss: 0.0630, label: 1, bag_size: 54\n",
      "batch 359, loss: 0.0115, instance_loss: 0.2709, weighted_loss: 0.0893, label: 0, bag_size: 29\n",
      "batch 379, loss: 0.0007, instance_loss: 0.0019, weighted_loss: 0.0011, label: 0, bag_size: 80\n",
      "batch 399, loss: 0.0156, instance_loss: 0.1397, weighted_loss: 0.0529, label: 1, bag_size: 86\n",
      "batch 419, loss: 0.0031, instance_loss: 0.0073, weighted_loss: 0.0044, label: 1, bag_size: 119\n",
      "batch 439, loss: 0.0005, instance_loss: 0.0037, weighted_loss: 0.0014, label: 1, bag_size: 27\n",
      "batch 459, loss: 0.0155, instance_loss: 0.1627, weighted_loss: 0.0597, label: 1, bag_size: 106\n",
      "batch 479, loss: 0.0014, instance_loss: 0.0274, weighted_loss: 0.0092, label: 0, bag_size: 102\n",
      "batch 499, loss: 0.8738, instance_loss: 1.0250, weighted_loss: 0.9191, label: 0, bag_size: 64\n",
      "batch 519, loss: 0.0002, instance_loss: 0.5889, weighted_loss: 0.1768, label: 0, bag_size: 67\n",
      "batch 539, loss: 0.0406, instance_loss: 0.0020, weighted_loss: 0.0290, label: 0, bag_size: 12\n",
      "batch 559, loss: 0.5619, instance_loss: 0.2502, weighted_loss: 0.4684, label: 1, bag_size: 117\n",
      "batch 579, loss: 0.0681, instance_loss: 0.2090, weighted_loss: 0.1104, label: 1, bag_size: 44\n",
      "batch 599, loss: 0.0013, instance_loss: 0.0094, weighted_loss: 0.0037, label: 0, bag_size: 38\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 90\n",
      "batch 639, loss: 0.0288, instance_loss: 0.0190, weighted_loss: 0.0259, label: 1, bag_size: 55\n",
      "batch 659, loss: 0.0051, instance_loss: 0.0503, weighted_loss: 0.0187, label: 1, bag_size: 23\n",
      "batch 679, loss: 0.0723, instance_loss: 0.0172, weighted_loss: 0.0558, label: 0, bag_size: 66\n",
      "batch 699, loss: 0.0026, instance_loss: 0.7322, weighted_loss: 0.2215, label: 1, bag_size: 65\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9711879432624113: correct 10955/11280\n",
      "class 1 clustering acc 0.8537234042553191: correct 4815/5640\n",
      "Epoch: 44, train_loss: 0.1925, train_clustering_loss:  0.2553, train_error: 0.0780\n",
      "class 0: acc 0.925414364640884, correct 335/362\n",
      "class 1: acc 0.9183673469387755, correct 315/343\n",
      "\n",
      "Val Set, val_loss: 0.7647, val_error: 0.2000, auc: 0.8723\n",
      "class 0 clustering acc 0.90546875: correct 1159/1280\n",
      "class 1 clustering acc 0.7203125: correct 461/640\n",
      "class 0: acc 0.8571428571428571, correct 42/49\n",
      "class 1: acc 0.7096774193548387, correct 22/31\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0069, instance_loss: 0.0192, weighted_loss: 0.0106, label: 0, bag_size: 49\n",
      "batch 39, loss: 0.0041, instance_loss: 0.5465, weighted_loss: 0.1668, label: 1, bag_size: 98\n",
      "batch 59, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 57\n",
      "batch 79, loss: 0.0002, instance_loss: 0.0016, weighted_loss: 0.0006, label: 1, bag_size: 69\n",
      "batch 99, loss: 0.1916, instance_loss: 0.0099, weighted_loss: 0.1371, label: 0, bag_size: 53\n",
      "batch 119, loss: 0.0084, instance_loss: 0.0073, weighted_loss: 0.0081, label: 0, bag_size: 95\n",
      "batch 139, loss: 0.0046, instance_loss: 0.0072, weighted_loss: 0.0053, label: 0, bag_size: 57\n",
      "batch 159, loss: 0.1682, instance_loss: 0.2857, weighted_loss: 0.2035, label: 1, bag_size: 90\n",
      "batch 179, loss: 0.0008, instance_loss: 0.0168, weighted_loss: 0.0056, label: 0, bag_size: 60\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0082, weighted_loss: 0.0025, label: 0, bag_size: 40\n",
      "batch 219, loss: 0.0190, instance_loss: 0.3036, weighted_loss: 0.1044, label: 0, bag_size: 38\n",
      "batch 239, loss: 0.2153, instance_loss: 0.2199, weighted_loss: 0.2167, label: 1, bag_size: 57\n",
      "batch 259, loss: 0.0121, instance_loss: 0.0551, weighted_loss: 0.0250, label: 1, bag_size: 84\n",
      "batch 279, loss: 0.0033, instance_loss: 0.4668, weighted_loss: 0.1423, label: 1, bag_size: 84\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 37\n",
      "batch 319, loss: 0.0188, instance_loss: 0.0694, weighted_loss: 0.0340, label: 0, bag_size: 67\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 107\n",
      "batch 359, loss: 0.2172, instance_loss: 0.6099, weighted_loss: 0.3350, label: 0, bag_size: 57\n",
      "batch 379, loss: 0.0179, instance_loss: 0.0000, weighted_loss: 0.0125, label: 0, bag_size: 76\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0044, weighted_loss: 0.0013, label: 1, bag_size: 121\n",
      "batch 419, loss: 0.0150, instance_loss: 0.0090, weighted_loss: 0.0132, label: 1, bag_size: 41\n",
      "batch 439, loss: 0.1784, instance_loss: 0.0145, weighted_loss: 0.1292, label: 1, bag_size: 79\n",
      "batch 459, loss: 0.0010, instance_loss: 0.0022, weighted_loss: 0.0014, label: 1, bag_size: 94\n",
      "batch 479, loss: 0.5597, instance_loss: 0.0257, weighted_loss: 0.3995, label: 1, bag_size: 18\n",
      "batch 499, loss: 0.0538, instance_loss: 0.2683, weighted_loss: 0.1181, label: 0, bag_size: 56\n",
      "batch 519, loss: 0.0010, instance_loss: 0.0192, weighted_loss: 0.0065, label: 1, bag_size: 34\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 52\n",
      "batch 559, loss: 0.0005, instance_loss: 0.0007, weighted_loss: 0.0006, label: 1, bag_size: 92\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 55\n",
      "batch 599, loss: 0.0000, instance_loss: 0.7947, weighted_loss: 0.2384, label: 0, bag_size: 24\n",
      "batch 619, loss: 0.3304, instance_loss: 0.0440, weighted_loss: 0.2445, label: 0, bag_size: 51\n",
      "batch 639, loss: 0.0073, instance_loss: 0.0058, weighted_loss: 0.0068, label: 0, bag_size: 48\n",
      "batch 659, loss: 0.0003, instance_loss: 0.0123, weighted_loss: 0.0039, label: 0, bag_size: 51\n",
      "batch 679, loss: 0.1133, instance_loss: 0.0211, weighted_loss: 0.0856, label: 0, bag_size: 55\n",
      "batch 699, loss: 0.0502, instance_loss: 0.0580, weighted_loss: 0.0525, label: 0, bag_size: 43\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9736702127659574: correct 10983/11280\n",
      "class 1 clustering acc 0.8948581560283688: correct 5047/5640\n",
      "Epoch: 45, train_loss: 0.1662, train_clustering_loss:  0.2623, train_error: 0.0567\n",
      "class 0: acc 0.9473684210526315, correct 342/361\n",
      "class 1: acc 0.938953488372093, correct 323/344\n",
      "\n",
      "Val Set, val_loss: 0.8306, val_error: 0.2125, auc: 0.8677\n",
      "class 0 clustering acc 0.92421875: correct 1183/1280\n",
      "class 1 clustering acc 0.7765625: correct 497/640\n",
      "class 0: acc 0.8367346938775511, correct 41/49\n",
      "class 1: acc 0.7096774193548387, correct 22/31\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0004, instance_loss: 0.0108, weighted_loss: 0.0036, label: 0, bag_size: 59\n",
      "batch 39, loss: 0.0012, instance_loss: 0.0053, weighted_loss: 0.0024, label: 1, bag_size: 55\n",
      "batch 59, loss: 0.0023, instance_loss: 0.0206, weighted_loss: 0.0078, label: 0, bag_size: 123\n",
      "batch 79, loss: 0.0184, instance_loss: 0.0304, weighted_loss: 0.0220, label: 1, bag_size: 76\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0037, weighted_loss: 0.0011, label: 0, bag_size: 80\n",
      "batch 119, loss: 0.0377, instance_loss: 0.0053, weighted_loss: 0.0280, label: 0, bag_size: 45\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 92\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 55\n",
      "batch 179, loss: 0.0003, instance_loss: 0.0007, weighted_loss: 0.0004, label: 1, bag_size: 64\n",
      "batch 199, loss: 0.0003, instance_loss: 0.0028, weighted_loss: 0.0011, label: 1, bag_size: 37\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 105\n",
      "batch 239, loss: 0.0014, instance_loss: 0.0687, weighted_loss: 0.0216, label: 0, bag_size: 65\n",
      "batch 259, loss: 0.0201, instance_loss: 0.0199, weighted_loss: 0.0200, label: 1, bag_size: 41\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 64\n",
      "batch 299, loss: 0.1924, instance_loss: 0.0336, weighted_loss: 0.1448, label: 0, bag_size: 57\n",
      "batch 319, loss: 0.0035, instance_loss: 0.0005, weighted_loss: 0.0026, label: 1, bag_size: 72\n",
      "batch 339, loss: 0.0171, instance_loss: 0.0053, weighted_loss: 0.0135, label: 1, bag_size: 89\n",
      "batch 359, loss: 0.0003, instance_loss: 0.0041, weighted_loss: 0.0015, label: 0, bag_size: 105\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 48\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0004, label: 1, bag_size: 52\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 92\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 81\n",
      "batch 459, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 73\n",
      "batch 479, loss: 0.0010, instance_loss: 0.0430, weighted_loss: 0.0136, label: 0, bag_size: 74\n",
      "batch 499, loss: 0.0003, instance_loss: 0.2144, weighted_loss: 0.0645, label: 1, bag_size: 80\n",
      "batch 519, loss: 0.0295, instance_loss: 0.0000, weighted_loss: 0.0207, label: 1, bag_size: 55\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 64\n",
      "batch 559, loss: 0.0340, instance_loss: 0.0009, weighted_loss: 0.0241, label: 1, bag_size: 27\n",
      "batch 579, loss: 0.0059, instance_loss: 0.0204, weighted_loss: 0.0102, label: 1, bag_size: 55\n",
      "batch 599, loss: 0.0150, instance_loss: 0.0023, weighted_loss: 0.0112, label: 0, bag_size: 45\n",
      "batch 619, loss: 0.0023, instance_loss: 0.0100, weighted_loss: 0.0046, label: 1, bag_size: 91\n",
      "batch 639, loss: 0.0042, instance_loss: 0.0030, weighted_loss: 0.0038, label: 0, bag_size: 83\n",
      "batch 659, loss: 0.2066, instance_loss: 0.0104, weighted_loss: 0.1477, label: 0, bag_size: 66\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 83\n",
      "batch 699, loss: 0.0258, instance_loss: 0.0121, weighted_loss: 0.0216, label: 1, bag_size: 37\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9813829787234043: correct 11070/11280\n",
      "class 1 clustering acc 0.8946808510638298: correct 5046/5640\n",
      "Epoch: 46, train_loss: 0.1510, train_clustering_loss:  0.1993, train_error: 0.0539\n",
      "class 0: acc 0.9460227272727273, correct 333/352\n",
      "class 1: acc 0.9461756373937678, correct 334/353\n",
      "\n",
      "Val Set, val_loss: 0.5691, val_error: 0.1625, auc: 0.9039\n",
      "class 0 clustering acc 0.93046875: correct 1191/1280\n",
      "class 1 clustering acc 0.775: correct 496/640\n",
      "class 0: acc 0.9183673469387755, correct 45/49\n",
      "class 1: acc 0.7096774193548387, correct 22/31\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0691, instance_loss: 0.2406, weighted_loss: 0.1206, label: 1, bag_size: 39\n",
      "batch 39, loss: 0.0030, instance_loss: 0.0372, weighted_loss: 0.0133, label: 0, bag_size: 51\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0044, weighted_loss: 0.0013, label: 1, bag_size: 70\n",
      "batch 79, loss: 0.0256, instance_loss: 0.0374, weighted_loss: 0.0291, label: 0, bag_size: 37\n",
      "batch 99, loss: 0.6216, instance_loss: 1.9594, weighted_loss: 1.0229, label: 1, bag_size: 73\n",
      "batch 119, loss: 0.6865, instance_loss: 0.3460, weighted_loss: 0.5843, label: 0, bag_size: 104\n",
      "batch 139, loss: 0.0797, instance_loss: 0.0048, weighted_loss: 0.0573, label: 0, bag_size: 54\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 52\n",
      "batch 179, loss: 0.4719, instance_loss: 1.0811, weighted_loss: 0.6546, label: 0, bag_size: 29\n",
      "batch 199, loss: 0.0298, instance_loss: 0.0291, weighted_loss: 0.0296, label: 0, bag_size: 53\n",
      "batch 219, loss: 0.0017, instance_loss: 0.0037, weighted_loss: 0.0023, label: 0, bag_size: 61\n",
      "batch 239, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 85\n",
      "batch 259, loss: 0.0023, instance_loss: 0.0009, weighted_loss: 0.0018, label: 1, bag_size: 57\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 12\n",
      "batch 299, loss: 0.0011, instance_loss: 0.0017, weighted_loss: 0.0013, label: 0, bag_size: 78\n",
      "batch 319, loss: 0.0027, instance_loss: 0.0000, weighted_loss: 0.0019, label: 1, bag_size: 140\n",
      "batch 339, loss: 0.0253, instance_loss: 0.1345, weighted_loss: 0.0581, label: 1, bag_size: 90\n",
      "batch 359, loss: 0.0004, instance_loss: 0.0013, weighted_loss: 0.0006, label: 1, bag_size: 89\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0300, weighted_loss: 0.0090, label: 1, bag_size: 62\n",
      "batch 399, loss: 1.3895, instance_loss: 0.6132, weighted_loss: 1.1566, label: 0, bag_size: 106\n",
      "batch 419, loss: 0.0585, instance_loss: 0.0948, weighted_loss: 0.0694, label: 0, bag_size: 70\n",
      "batch 439, loss: 0.0110, instance_loss: 0.0019, weighted_loss: 0.0083, label: 1, bag_size: 54\n",
      "batch 459, loss: 0.8784, instance_loss: 0.5754, weighted_loss: 0.7875, label: 0, bag_size: 54\n",
      "batch 479, loss: 0.0602, instance_loss: 0.2679, weighted_loss: 0.1225, label: 1, bag_size: 30\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 80\n",
      "batch 519, loss: 0.0012, instance_loss: 0.0009, weighted_loss: 0.0011, label: 1, bag_size: 25\n",
      "batch 539, loss: 0.0076, instance_loss: 0.0007, weighted_loss: 0.0055, label: 1, bag_size: 102\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 22\n",
      "batch 579, loss: 0.0004, instance_loss: 0.0001, weighted_loss: 0.0003, label: 0, bag_size: 42\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 54\n",
      "batch 619, loss: 0.0168, instance_loss: 0.0007, weighted_loss: 0.0120, label: 1, bag_size: 75\n",
      "batch 639, loss: 0.0105, instance_loss: 0.0681, weighted_loss: 0.0278, label: 1, bag_size: 96\n",
      "batch 659, loss: 0.0116, instance_loss: 1.4541, weighted_loss: 0.4443, label: 1, bag_size: 34\n",
      "batch 679, loss: 0.0005, instance_loss: 0.0002, weighted_loss: 0.0004, label: 0, bag_size: 66\n",
      "batch 699, loss: 0.1035, instance_loss: 0.7466, weighted_loss: 0.2964, label: 0, bag_size: 92\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9804964539007093: correct 11060/11280\n",
      "class 1 clustering acc 0.899645390070922: correct 5074/5640\n",
      "Epoch: 47, train_loss: 0.1265, train_clustering_loss:  0.2052, train_error: 0.0426\n",
      "class 0: acc 0.9672131147540983, correct 354/366\n",
      "class 1: acc 0.9469026548672567, correct 321/339\n",
      "\n",
      "Val Set, val_loss: 0.8192, val_error: 0.2000, auc: 0.9092\n",
      "class 0 clustering acc 0.878125: correct 1124/1280\n",
      "class 1 clustering acc 0.671875: correct 430/640\n",
      "class 0: acc 0.7346938775510204, correct 36/49\n",
      "class 1: acc 0.9032258064516129, correct 28/31\n",
      "EarlyStopping counter: 12 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0018, instance_loss: 0.0956, weighted_loss: 0.0299, label: 1, bag_size: 60\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 31\n",
      "batch 59, loss: 5.9387, instance_loss: 3.0920, weighted_loss: 5.0847, label: 1, bag_size: 67\n",
      "batch 79, loss: 0.0033, instance_loss: 0.0257, weighted_loss: 0.0100, label: 0, bag_size: 81\n",
      "batch 99, loss: 0.0036, instance_loss: 0.0454, weighted_loss: 0.0161, label: 0, bag_size: 67\n",
      "batch 119, loss: 1.6775, instance_loss: 0.1135, weighted_loss: 1.2083, label: 1, bag_size: 75\n",
      "batch 139, loss: 0.0085, instance_loss: 0.0010, weighted_loss: 0.0063, label: 0, bag_size: 49\n",
      "batch 159, loss: 0.0129, instance_loss: 0.0553, weighted_loss: 0.0257, label: 0, bag_size: 79\n",
      "batch 179, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 59\n",
      "batch 199, loss: 0.0268, instance_loss: 0.0822, weighted_loss: 0.0434, label: 0, bag_size: 81\n",
      "batch 219, loss: 0.3122, instance_loss: 0.0000, weighted_loss: 0.2186, label: 0, bag_size: 66\n",
      "batch 239, loss: 0.0271, instance_loss: 0.2176, weighted_loss: 0.0843, label: 0, bag_size: 54\n",
      "batch 259, loss: 0.0101, instance_loss: 0.0000, weighted_loss: 0.0071, label: 1, bag_size: 55\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 52\n",
      "batch 299, loss: 0.0032, instance_loss: 0.0169, weighted_loss: 0.0074, label: 1, bag_size: 69\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 95\n",
      "batch 339, loss: 0.0108, instance_loss: 0.0000, weighted_loss: 0.0076, label: 0, bag_size: 79\n",
      "batch 359, loss: 0.0048, instance_loss: 0.0022, weighted_loss: 0.0040, label: 1, bag_size: 110\n",
      "batch 379, loss: 0.0554, instance_loss: 0.0200, weighted_loss: 0.0448, label: 1, bag_size: 89\n",
      "batch 399, loss: 0.0002, instance_loss: 0.0828, weighted_loss: 0.0250, label: 1, bag_size: 80\n",
      "batch 419, loss: 0.0013, instance_loss: 0.0012, weighted_loss: 0.0012, label: 1, bag_size: 54\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0067, weighted_loss: 0.0020, label: 1, bag_size: 98\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 67\n",
      "batch 479, loss: 0.0005, instance_loss: 0.0118, weighted_loss: 0.0039, label: 1, bag_size: 82\n",
      "batch 499, loss: 0.0009, instance_loss: 0.0026, weighted_loss: 0.0015, label: 1, bag_size: 38\n",
      "batch 519, loss: 0.0302, instance_loss: 0.0179, weighted_loss: 0.0265, label: 0, bag_size: 106\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 92\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 75\n",
      "batch 579, loss: 0.0028, instance_loss: 0.0657, weighted_loss: 0.0216, label: 0, bag_size: 32\n",
      "batch 599, loss: 0.1296, instance_loss: 0.2469, weighted_loss: 0.1648, label: 0, bag_size: 58\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 66\n",
      "batch 639, loss: 0.2522, instance_loss: 0.0008, weighted_loss: 0.1768, label: 0, bag_size: 75\n",
      "batch 659, loss: 0.8634, instance_loss: 2.8516, weighted_loss: 1.4599, label: 0, bag_size: 53\n",
      "batch 679, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 53\n",
      "batch 699, loss: 0.0150, instance_loss: 0.0811, weighted_loss: 0.0348, label: 0, bag_size: 62\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9823581560283688: correct 11081/11280\n",
      "class 1 clustering acc 0.9304964539007092: correct 5248/5640\n",
      "Epoch: 48, train_loss: 0.1184, train_clustering_loss:  0.1437, train_error: 0.0397\n",
      "class 0: acc 0.9563953488372093, correct 329/344\n",
      "class 1: acc 0.96398891966759, correct 348/361\n",
      "\n",
      "Val Set, val_loss: 0.7584, val_error: 0.1750, auc: 0.8999\n",
      "class 0 clustering acc 0.88515625: correct 1133/1280\n",
      "class 1 clustering acc 0.7125: correct 456/640\n",
      "class 0: acc 0.8571428571428571, correct 42/49\n",
      "class 1: acc 0.7741935483870968, correct 24/31\n",
      "EarlyStopping counter: 13 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 27\n",
      "batch 39, loss: 2.8157, instance_loss: 0.1403, weighted_loss: 2.0131, label: 1, bag_size: 18\n",
      "batch 59, loss: 0.0446, instance_loss: 0.0000, weighted_loss: 0.0313, label: 0, bag_size: 52\n",
      "batch 79, loss: 0.0023, instance_loss: 0.0007, weighted_loss: 0.0018, label: 0, bag_size: 40\n",
      "batch 99, loss: 0.0111, instance_loss: 0.0665, weighted_loss: 0.0277, label: 0, bag_size: 49\n",
      "batch 119, loss: 0.0041, instance_loss: 0.0001, weighted_loss: 0.0029, label: 0, bag_size: 79\n",
      "batch 139, loss: 0.1784, instance_loss: 0.0279, weighted_loss: 0.1333, label: 1, bag_size: 51\n",
      "batch 159, loss: 0.0374, instance_loss: 0.0471, weighted_loss: 0.0403, label: 0, bag_size: 30\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 85\n",
      "batch 199, loss: 0.0522, instance_loss: 0.3406, weighted_loss: 0.1387, label: 1, bag_size: 79\n",
      "batch 219, loss: 0.0003, instance_loss: 0.0003, weighted_loss: 0.0003, label: 0, bag_size: 43\n",
      "batch 239, loss: 0.0252, instance_loss: 0.0100, weighted_loss: 0.0206, label: 0, bag_size: 106\n",
      "batch 259, loss: 0.5480, instance_loss: 0.0315, weighted_loss: 0.3930, label: 0, bag_size: 31\n",
      "batch 279, loss: 0.0016, instance_loss: 0.5301, weighted_loss: 0.1602, label: 0, bag_size: 67\n",
      "batch 299, loss: 0.0110, instance_loss: 0.0027, weighted_loss: 0.0085, label: 1, bag_size: 86\n",
      "batch 319, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0015, label: 1, bag_size: 77\n",
      "batch 339, loss: 0.0145, instance_loss: 0.0287, weighted_loss: 0.0187, label: 1, bag_size: 88\n",
      "batch 359, loss: 0.5721, instance_loss: 0.3365, weighted_loss: 0.5014, label: 1, bag_size: 31\n",
      "batch 379, loss: 0.0006, instance_loss: 0.0242, weighted_loss: 0.0077, label: 1, bag_size: 54\n",
      "batch 399, loss: 0.0008, instance_loss: 0.0099, weighted_loss: 0.0035, label: 1, bag_size: 26\n",
      "batch 419, loss: 0.0018, instance_loss: 0.0058, weighted_loss: 0.0030, label: 1, bag_size: 64\n",
      "batch 439, loss: 0.0220, instance_loss: 0.1453, weighted_loss: 0.0590, label: 1, bag_size: 52\n",
      "batch 459, loss: 0.0059, instance_loss: 0.0674, weighted_loss: 0.0243, label: 0, bag_size: 66\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 109\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 83\n",
      "batch 519, loss: 0.1718, instance_loss: 1.2107, weighted_loss: 0.4835, label: 1, bag_size: 89\n",
      "batch 539, loss: 0.0627, instance_loss: 2.1544, weighted_loss: 0.6902, label: 1, bag_size: 89\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0028, weighted_loss: 0.0009, label: 0, bag_size: 76\n",
      "batch 579, loss: 0.0150, instance_loss: 0.0158, weighted_loss: 0.0152, label: 0, bag_size: 74\n",
      "batch 599, loss: 0.0015, instance_loss: 0.0039, weighted_loss: 0.0022, label: 0, bag_size: 44\n",
      "batch 619, loss: 0.4564, instance_loss: 0.0022, weighted_loss: 0.3202, label: 1, bag_size: 69\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0202, weighted_loss: 0.0061, label: 1, bag_size: 64\n",
      "batch 659, loss: 0.4484, instance_loss: 0.0322, weighted_loss: 0.3235, label: 1, bag_size: 31\n",
      "batch 679, loss: 0.0003, instance_loss: 0.0516, weighted_loss: 0.0157, label: 1, bag_size: 93\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 76\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9745567375886525: correct 10993/11280\n",
      "class 1 clustering acc 0.8780141843971632: correct 4952/5640\n",
      "Epoch: 49, train_loss: 0.2048, train_clustering_loss:  0.2331, train_error: 0.0667\n",
      "class 0: acc 0.9353932584269663, correct 333/356\n",
      "class 1: acc 0.9312320916905444, correct 325/349\n",
      "\n",
      "Val Set, val_loss: 0.7287, val_error: 0.1500, auc: 0.9098\n",
      "class 0 clustering acc 0.92265625: correct 1181/1280\n",
      "class 1 clustering acc 0.8125: correct 520/640\n",
      "class 0: acc 0.8775510204081632, correct 43/49\n",
      "class 1: acc 0.8064516129032258, correct 25/31\n",
      "EarlyStopping counter: 14 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0003, instance_loss: 0.0170, weighted_loss: 0.0053, label: 0, bag_size: 64\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 101\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 94\n",
      "batch 79, loss: 0.0002, instance_loss: 0.0012, weighted_loss: 0.0005, label: 0, bag_size: 85\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 92\n",
      "batch 119, loss: 0.0014, instance_loss: 0.0095, weighted_loss: 0.0039, label: 0, bag_size: 30\n",
      "batch 139, loss: 0.0004, instance_loss: 0.0010, weighted_loss: 0.0006, label: 0, bag_size: 72\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 86\n",
      "batch 179, loss: 0.0013, instance_loss: 0.0077, weighted_loss: 0.0032, label: 1, bag_size: 89\n",
      "batch 199, loss: 0.0053, instance_loss: 0.0003, weighted_loss: 0.0038, label: 1, bag_size: 75\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 64\n",
      "batch 239, loss: 0.0021, instance_loss: 0.0003, weighted_loss: 0.0016, label: 0, bag_size: 52\n",
      "batch 259, loss: 0.0031, instance_loss: 0.0105, weighted_loss: 0.0053, label: 0, bag_size: 85\n",
      "batch 279, loss: 3.9789, instance_loss: 4.8996, weighted_loss: 4.2551, label: 1, bag_size: 126\n",
      "batch 299, loss: 0.0067, instance_loss: 0.0098, weighted_loss: 0.0076, label: 1, bag_size: 75\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 62\n",
      "batch 339, loss: 0.0012, instance_loss: 0.0270, weighted_loss: 0.0089, label: 0, bag_size: 31\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 91\n",
      "batch 379, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 90\n",
      "batch 399, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 75\n",
      "batch 419, loss: 0.0026, instance_loss: 0.0005, weighted_loss: 0.0020, label: 0, bag_size: 59\n",
      "batch 439, loss: 0.0003, instance_loss: 0.0030, weighted_loss: 0.0011, label: 1, bag_size: 98\n",
      "batch 459, loss: 0.1062, instance_loss: 0.0253, weighted_loss: 0.0819, label: 0, bag_size: 70\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 47\n",
      "batch 499, loss: 0.0012, instance_loss: 0.0009, weighted_loss: 0.0011, label: 1, bag_size: 109\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 63\n",
      "batch 539, loss: 0.3363, instance_loss: 0.1526, weighted_loss: 0.2812, label: 0, bag_size: 87\n",
      "batch 559, loss: 0.0030, instance_loss: 0.0904, weighted_loss: 0.0292, label: 0, bag_size: 63\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0023, weighted_loss: 0.0008, label: 1, bag_size: 55\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 35\n",
      "batch 619, loss: 0.2592, instance_loss: 0.0083, weighted_loss: 0.1839, label: 0, bag_size: 61\n",
      "batch 639, loss: 0.0061, instance_loss: 0.0000, weighted_loss: 0.0043, label: 0, bag_size: 122\n",
      "batch 659, loss: 0.0052, instance_loss: 0.0000, weighted_loss: 0.0036, label: 1, bag_size: 71\n",
      "batch 679, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 70\n",
      "batch 699, loss: 0.0198, instance_loss: 0.3825, weighted_loss: 0.1286, label: 1, bag_size: 56\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9841312056737589: correct 11101/11280\n",
      "class 1 clustering acc 0.9358156028368795: correct 5278/5640\n",
      "Epoch: 50, train_loss: 0.1145, train_clustering_loss:  0.1602, train_error: 0.0355\n",
      "class 0: acc 0.9659090909090909, correct 340/352\n",
      "class 1: acc 0.9631728045325779, correct 340/353\n",
      "\n",
      "Val Set, val_loss: 0.6408, val_error: 0.1500, auc: 0.9200\n",
      "class 0 clustering acc 0.9265625: correct 1186/1280\n",
      "class 1 clustering acc 0.8265625: correct 529/640\n",
      "class 0: acc 0.9183673469387755, correct 45/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 15 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0414, instance_loss: 0.0194, weighted_loss: 0.0348, label: 0, bag_size: 85\n",
      "batch 39, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 109\n",
      "batch 59, loss: 0.0016, instance_loss: 0.0218, weighted_loss: 0.0077, label: 1, bag_size: 31\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 103\n",
      "batch 99, loss: 0.1033, instance_loss: 0.0152, weighted_loss: 0.0769, label: 1, bag_size: 93\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 75\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 55\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 91\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 91\n",
      "batch 199, loss: 0.6541, instance_loss: 1.3670, weighted_loss: 0.8680, label: 1, bag_size: 93\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 79\n",
      "batch 239, loss: 0.0020, instance_loss: 0.0008, weighted_loss: 0.0016, label: 1, bag_size: 75\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 69\n",
      "batch 279, loss: 0.0022, instance_loss: 0.0179, weighted_loss: 0.0070, label: 1, bag_size: 37\n",
      "batch 299, loss: 0.0826, instance_loss: 0.0021, weighted_loss: 0.0584, label: 1, bag_size: 91\n",
      "batch 319, loss: 0.0004, instance_loss: 0.0026, weighted_loss: 0.0011, label: 1, bag_size: 64\n",
      "batch 339, loss: 0.0006, instance_loss: 0.0168, weighted_loss: 0.0055, label: 1, bag_size: 84\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 79\n",
      "batch 379, loss: 0.0079, instance_loss: 0.0225, weighted_loss: 0.0123, label: 0, bag_size: 78\n",
      "batch 399, loss: 0.0026, instance_loss: 0.0001, weighted_loss: 0.0019, label: 0, bag_size: 83\n",
      "batch 419, loss: 1.7335, instance_loss: 1.6043, weighted_loss: 1.6947, label: 0, bag_size: 82\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 27\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 55\n",
      "batch 479, loss: 0.0130, instance_loss: 0.0195, weighted_loss: 0.0149, label: 0, bag_size: 55\n",
      "batch 499, loss: 0.0006, instance_loss: 0.0549, weighted_loss: 0.0169, label: 1, bag_size: 92\n",
      "batch 519, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 1, bag_size: 70\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 88\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 50\n",
      "batch 579, loss: 0.0097, instance_loss: 0.0081, weighted_loss: 0.0092, label: 0, bag_size: 82\n",
      "batch 599, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 30\n",
      "batch 619, loss: 1.2625, instance_loss: 0.0548, weighted_loss: 0.9002, label: 0, bag_size: 69\n",
      "batch 639, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 1, bag_size: 75\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 43\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 79\n",
      "batch 699, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 34\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9866134751773049: correct 11129/11280\n",
      "class 1 clustering acc 0.9441489361702128: correct 5325/5640\n",
      "Epoch: 51, train_loss: 0.0907, train_clustering_loss:  0.1183, train_error: 0.0411\n",
      "class 0: acc 0.957983193277311, correct 342/357\n",
      "class 1: acc 0.9597701149425287, correct 334/348\n",
      "\n",
      "Val Set, val_loss: 1.1707, val_error: 0.1750, auc: 0.8776\n",
      "class 0 clustering acc 0.9078125: correct 1162/1280\n",
      "class 1 clustering acc 0.771875: correct 494/640\n",
      "class 0: acc 0.9183673469387755, correct 45/49\n",
      "class 1: acc 0.6774193548387096, correct 21/31\n",
      "EarlyStopping counter: 16 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0028, instance_loss: 0.0000, weighted_loss: 0.0020, label: 0, bag_size: 88\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 76\n",
      "batch 59, loss: 1.4046, instance_loss: 0.0315, weighted_loss: 0.9927, label: 1, bag_size: 37\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 87\n",
      "batch 99, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 90\n",
      "batch 119, loss: 0.0004, instance_loss: 0.0044, weighted_loss: 0.0016, label: 1, bag_size: 52\n",
      "batch 139, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 111\n",
      "batch 159, loss: 0.0000, instance_loss: 0.2839, weighted_loss: 0.0852, label: 1, bag_size: 61\n",
      "batch 179, loss: 0.0008, instance_loss: 0.0057, weighted_loss: 0.0023, label: 0, bag_size: 54\n",
      "batch 199, loss: 0.0006, instance_loss: 0.0099, weighted_loss: 0.0034, label: 1, bag_size: 53\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 110\n",
      "batch 239, loss: 0.0139, instance_loss: 0.0957, weighted_loss: 0.0385, label: 0, bag_size: 46\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0033, weighted_loss: 0.0010, label: 1, bag_size: 69\n",
      "batch 279, loss: 0.0049, instance_loss: 0.5445, weighted_loss: 0.1668, label: 1, bag_size: 64\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 25\n",
      "batch 319, loss: 0.0926, instance_loss: 0.0529, weighted_loss: 0.0807, label: 0, bag_size: 105\n",
      "batch 339, loss: 0.0174, instance_loss: 0.0045, weighted_loss: 0.0135, label: 1, bag_size: 55\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 64\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 40\n",
      "batch 399, loss: 0.0039, instance_loss: 0.0004, weighted_loss: 0.0029, label: 0, bag_size: 68\n",
      "batch 419, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 30\n",
      "batch 439, loss: 0.0025, instance_loss: 0.0296, weighted_loss: 0.0106, label: 0, bag_size: 54\n",
      "batch 459, loss: 0.0097, instance_loss: 0.0029, weighted_loss: 0.0076, label: 1, bag_size: 64\n",
      "batch 479, loss: 0.0232, instance_loss: 0.1537, weighted_loss: 0.0623, label: 0, bag_size: 28\n",
      "batch 499, loss: 0.0505, instance_loss: 0.0302, weighted_loss: 0.0444, label: 0, bag_size: 94\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 84\n",
      "batch 539, loss: 0.0065, instance_loss: 0.0135, weighted_loss: 0.0086, label: 1, bag_size: 52\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 76\n",
      "batch 579, loss: 0.0098, instance_loss: 1.3849, weighted_loss: 0.4223, label: 1, bag_size: 67\n",
      "batch 599, loss: 0.0011, instance_loss: 0.0007, weighted_loss: 0.0010, label: 0, bag_size: 35\n",
      "batch 619, loss: 0.0004, instance_loss: 0.0807, weighted_loss: 0.0245, label: 1, bag_size: 106\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 0, bag_size: 108\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 79\n",
      "batch 679, loss: 0.0030, instance_loss: 0.0001, weighted_loss: 0.0021, label: 1, bag_size: 78\n",
      "batch 699, loss: 0.0424, instance_loss: 0.0028, weighted_loss: 0.0305, label: 0, bag_size: 94\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9812056737588652: correct 11068/11280\n",
      "class 1 clustering acc 0.9124113475177305: correct 5146/5640\n",
      "Epoch: 52, train_loss: 0.1456, train_clustering_loss:  0.1854, train_error: 0.0539\n",
      "class 0: acc 0.9542857142857143, correct 334/350\n",
      "class 1: acc 0.9380281690140845, correct 333/355\n",
      "\n",
      "Val Set, val_loss: 1.3719, val_error: 0.2625, auc: 0.8703\n",
      "class 0 clustering acc 0.928125: correct 1188/1280\n",
      "class 1 clustering acc 0.79375: correct 508/640\n",
      "class 0: acc 0.673469387755102, correct 33/49\n",
      "class 1: acc 0.8387096774193549, correct 26/31\n",
      "EarlyStopping counter: 17 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.5485, instance_loss: 0.3366, weighted_loss: 1.1850, label: 1, bag_size: 89\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0119, weighted_loss: 0.0036, label: 1, bag_size: 52\n",
      "batch 59, loss: 0.0000, instance_loss: 0.6601, weighted_loss: 0.1980, label: 1, bag_size: 52\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0059, weighted_loss: 0.0018, label: 1, bag_size: 131\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0018, weighted_loss: 0.0006, label: 1, bag_size: 37\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 81\n",
      "batch 139, loss: 0.0454, instance_loss: 0.0019, weighted_loss: 0.0324, label: 0, bag_size: 66\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 55\n",
      "batch 179, loss: 0.5133, instance_loss: 1.1209, weighted_loss: 0.6956, label: 0, bag_size: 71\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 81\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0151, weighted_loss: 0.0047, label: 1, bag_size: 71\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 97\n",
      "batch 259, loss: 1.6341, instance_loss: 0.0357, weighted_loss: 1.1546, label: 0, bag_size: 101\n",
      "batch 279, loss: 0.0209, instance_loss: 0.0032, weighted_loss: 0.0156, label: 0, bag_size: 109\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0087, weighted_loss: 0.0027, label: 0, bag_size: 69\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0054, weighted_loss: 0.0016, label: 1, bag_size: 27\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0044, weighted_loss: 0.0013, label: 1, bag_size: 101\n",
      "batch 359, loss: 0.0262, instance_loss: 0.0053, weighted_loss: 0.0199, label: 1, bag_size: 88\n",
      "batch 379, loss: 1.3226, instance_loss: 0.6070, weighted_loss: 1.1079, label: 1, bag_size: 89\n",
      "batch 399, loss: 0.8424, instance_loss: 0.5612, weighted_loss: 0.7580, label: 0, bag_size: 42\n",
      "batch 419, loss: 0.2363, instance_loss: 0.0776, weighted_loss: 0.1887, label: 0, bag_size: 51\n",
      "batch 439, loss: 0.0009, instance_loss: 0.0013, weighted_loss: 0.0011, label: 1, bag_size: 82\n",
      "batch 459, loss: 0.0013, instance_loss: 0.0023, weighted_loss: 0.0016, label: 0, bag_size: 60\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0043, weighted_loss: 0.0013, label: 1, bag_size: 33\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 105\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0031, weighted_loss: 0.0009, label: 0, bag_size: 60\n",
      "batch 539, loss: 0.8768, instance_loss: 0.0440, weighted_loss: 0.6270, label: 1, bag_size: 103\n",
      "batch 559, loss: 5.6196, instance_loss: 0.4534, weighted_loss: 4.0697, label: 0, bag_size: 63\n",
      "batch 579, loss: 0.0083, instance_loss: 0.0000, weighted_loss: 0.0058, label: 0, bag_size: 89\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 59\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0019, weighted_loss: 0.0007, label: 1, bag_size: 23\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 47\n",
      "batch 659, loss: 0.0053, instance_loss: 0.0034, weighted_loss: 0.0047, label: 1, bag_size: 56\n",
      "batch 679, loss: 0.0004, instance_loss: 0.0032, weighted_loss: 0.0012, label: 0, bag_size: 80\n",
      "batch 699, loss: 2.3274, instance_loss: 0.4746, weighted_loss: 1.7715, label: 1, bag_size: 73\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9731382978723404: correct 10977/11280\n",
      "class 1 clustering acc 0.9035460992907801: correct 5096/5640\n",
      "Epoch: 53, train_loss: 0.1769, train_clustering_loss:  0.2023, train_error: 0.0723\n",
      "class 0: acc 0.9219653179190751, correct 319/346\n",
      "class 1: acc 0.9331476323119777, correct 335/359\n",
      "\n",
      "Val Set, val_loss: 0.9206, val_error: 0.1750, auc: 0.8776\n",
      "class 0 clustering acc 0.91484375: correct 1171/1280\n",
      "class 1 clustering acc 0.7546875: correct 483/640\n",
      "class 0: acc 0.8979591836734694, correct 44/49\n",
      "class 1: acc 0.7096774193548387, correct 22/31\n",
      "EarlyStopping counter: 18 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0004, instance_loss: 0.0054, weighted_loss: 0.0019, label: 1, bag_size: 103\n",
      "batch 39, loss: 0.0027, instance_loss: 0.0037, weighted_loss: 0.0030, label: 0, bag_size: 22\n",
      "batch 59, loss: 0.0192, instance_loss: 0.0434, weighted_loss: 0.0265, label: 1, bag_size: 124\n",
      "batch 79, loss: 0.0045, instance_loss: 0.0000, weighted_loss: 0.0031, label: 0, bag_size: 29\n",
      "batch 99, loss: 0.0137, instance_loss: 0.0000, weighted_loss: 0.0096, label: 0, bag_size: 44\n",
      "batch 119, loss: 0.0709, instance_loss: 0.0619, weighted_loss: 0.0682, label: 0, bag_size: 33\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 54\n",
      "batch 159, loss: 0.0004, instance_loss: 0.0006, weighted_loss: 0.0005, label: 1, bag_size: 61\n",
      "batch 179, loss: 0.0002, instance_loss: 0.0001, weighted_loss: 0.0002, label: 0, bag_size: 78\n",
      "batch 199, loss: 0.0047, instance_loss: 0.0079, weighted_loss: 0.0057, label: 1, bag_size: 84\n",
      "batch 219, loss: 0.1271, instance_loss: 0.8489, weighted_loss: 0.3437, label: 0, bag_size: 63\n",
      "batch 239, loss: 0.0592, instance_loss: 0.0250, weighted_loss: 0.0489, label: 1, bag_size: 59\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 53\n",
      "batch 279, loss: 0.0040, instance_loss: 0.0005, weighted_loss: 0.0030, label: 0, bag_size: 64\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 69\n",
      "batch 319, loss: 0.0022, instance_loss: 0.0000, weighted_loss: 0.0015, label: 0, bag_size: 73\n",
      "batch 339, loss: 0.0029, instance_loss: 0.4649, weighted_loss: 0.1415, label: 0, bag_size: 44\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0017, weighted_loss: 0.0005, label: 1, bag_size: 53\n",
      "batch 379, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0015, label: 0, bag_size: 54\n",
      "batch 399, loss: 0.0000, instance_loss: 0.6493, weighted_loss: 0.1948, label: 1, bag_size: 57\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 60\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0140, weighted_loss: 0.0042, label: 0, bag_size: 66\n",
      "batch 459, loss: 0.0001, instance_loss: 4.8338, weighted_loss: 1.4502, label: 0, bag_size: 77\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 95\n",
      "batch 499, loss: 0.1171, instance_loss: 0.0224, weighted_loss: 0.0887, label: 1, bag_size: 74\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 0, bag_size: 109\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 80\n",
      "batch 559, loss: 3.0425, instance_loss: 0.0141, weighted_loss: 2.1340, label: 1, bag_size: 103\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 49\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 52\n",
      "batch 619, loss: 0.0065, instance_loss: 0.0446, weighted_loss: 0.0180, label: 1, bag_size: 34\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0016, weighted_loss: 0.0005, label: 0, bag_size: 62\n",
      "batch 659, loss: 0.0071, instance_loss: 0.1530, weighted_loss: 0.0509, label: 1, bag_size: 117\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 0, bag_size: 71\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0039, weighted_loss: 0.0012, label: 1, bag_size: 25\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9851063829787234: correct 11112/11280\n",
      "class 1 clustering acc 0.9274822695035461: correct 5231/5640\n",
      "Epoch: 54, train_loss: 0.1227, train_clustering_loss:  0.1617, train_error: 0.0426\n",
      "class 0: acc 0.957983193277311, correct 342/357\n",
      "class 1: acc 0.9568965517241379, correct 333/348\n",
      "\n",
      "Val Set, val_loss: 0.8001, val_error: 0.2000, auc: 0.8871\n",
      "class 0 clustering acc 0.87890625: correct 1125/1280\n",
      "class 1 clustering acc 0.7625: correct 488/640\n",
      "class 0: acc 0.8367346938775511, correct 41/49\n",
      "class 1: acc 0.7419354838709677, correct 23/31\n",
      "EarlyStopping counter: 19 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0037, instance_loss: 0.0081, weighted_loss: 0.0050, label: 1, bag_size: 27\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 86\n",
      "batch 59, loss: 0.0000, instance_loss: 3.2169, weighted_loss: 0.9651, label: 0, bag_size: 24\n",
      "batch 79, loss: 0.0078, instance_loss: 0.0003, weighted_loss: 0.0056, label: 0, bag_size: 43\n",
      "batch 99, loss: 0.0004, instance_loss: 0.3814, weighted_loss: 0.1147, label: 0, bag_size: 63\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 111\n",
      "batch 139, loss: 0.0040, instance_loss: 0.0057, weighted_loss: 0.0045, label: 0, bag_size: 74\n",
      "batch 159, loss: 0.0002, instance_loss: 0.0022, weighted_loss: 0.0008, label: 1, bag_size: 50\n",
      "batch 179, loss: 1.4121, instance_loss: 0.0042, weighted_loss: 0.9897, label: 1, bag_size: 54\n",
      "batch 199, loss: 0.0078, instance_loss: 0.2516, weighted_loss: 0.0810, label: 1, bag_size: 45\n",
      "batch 219, loss: 0.0115, instance_loss: 0.0001, weighted_loss: 0.0081, label: 1, bag_size: 76\n",
      "batch 239, loss: 0.0835, instance_loss: 0.4939, weighted_loss: 0.2066, label: 1, bag_size: 106\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 77\n",
      "batch 279, loss: 0.0028, instance_loss: 0.0000, weighted_loss: 0.0020, label: 0, bag_size: 95\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 82\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 51\n",
      "batch 339, loss: 0.0186, instance_loss: 0.0000, weighted_loss: 0.0130, label: 0, bag_size: 82\n",
      "batch 359, loss: 0.1119, instance_loss: 0.0718, weighted_loss: 0.0999, label: 0, bag_size: 97\n",
      "batch 379, loss: 2.0715, instance_loss: 0.4617, weighted_loss: 1.5886, label: 0, bag_size: 68\n",
      "batch 399, loss: 0.0013, instance_loss: 0.0004, weighted_loss: 0.0011, label: 1, bag_size: 103\n",
      "batch 419, loss: 0.0019, instance_loss: 0.2351, weighted_loss: 0.0719, label: 0, bag_size: 51\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 105\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 101\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 46\n",
      "batch 499, loss: 0.0005, instance_loss: 0.0661, weighted_loss: 0.0202, label: 1, bag_size: 77\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0034, weighted_loss: 0.0010, label: 1, bag_size: 94\n",
      "batch 539, loss: 0.0006, instance_loss: 0.0004, weighted_loss: 0.0005, label: 1, bag_size: 73\n",
      "batch 559, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 111\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 101\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0026, weighted_loss: 0.0008, label: 1, bag_size: 57\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 85\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 50\n",
      "batch 659, loss: 0.0312, instance_loss: 0.0000, weighted_loss: 0.0218, label: 0, bag_size: 127\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 64\n",
      "batch 699, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 37\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9877659574468085: correct 11142/11280\n",
      "class 1 clustering acc 0.9409574468085107: correct 5307/5640\n",
      "Epoch: 55, train_loss: 0.1147, train_clustering_loss:  0.1133, train_error: 0.0426\n",
      "class 0: acc 0.9581005586592178, correct 343/358\n",
      "class 1: acc 0.9567723342939481, correct 332/347\n",
      "\n",
      "Val Set, val_loss: 0.7911, val_error: 0.1625, auc: 0.9157\n",
      "class 0 clustering acc 0.9296875: correct 1190/1280\n",
      "class 1 clustering acc 0.821875: correct 526/640\n",
      "class 0: acc 0.9183673469387755, correct 45/49\n",
      "class 1: acc 0.7096774193548387, correct 22/31\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Val error: 0.1375, ROC AUC: 0.9309\n",
      "Test error: 0.3187, ROC AUC: 0.7839\n",
      "class 0: acc 0.6610169491525424, correct 39/59\n",
      "class 1: acc 0.71875, correct 23/32\n",
      "finished!\n",
      "end script\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python main_myself.py --drop_out --early_stopping --lr 2e-4 --max_lr 2e-3 --k 5 --label_frac 1 --opt adam\\\n",
    "--exp_code nlst_100_level13_mcbat_cat_depth1_adam_FLASHAttention_testfold4 --weighted_sample --bag_loss ce --inst_loss svm \\\n",
    "--task task_2_tumor_subtyping --model_type mcbat_sb --log_data --data_low_dir /home/sci/Disk_data/Datasets/NLST/FEATURES_level3 \\\n",
    "--data_high_dir /home/sci/Disk_data/Datasets/NLST/FEATURES_level1 \\\n",
    "--split_dir /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/NLST_100 --subtyping \\\n",
    "--csv_path dataset_csv/NLST_offical.csv --k_start 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA_VISIBLE_DEVICES=0 python eval.py \\\n",
    "--drop_out \\\n",
    "--k 10 \\\n",
    "--models_exp_code task_1_tumor_vs_normal_CLAM_50_s1 \\\n",
    "--save_exp_code task_1_tumor_vs_normal_CLAM_50_s1_cv \\\n",
    "--task task_1_tumor_vs_normal \\\n",
    "--model_type clam_sb \\\n",
    "--results_dir results \\\n",
    "--data_root_dir /media/yuansh/14THHD/CLAM/FEATURES_DIRECTORY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': 'task_1_tumor_vs_normal', 'split': 'test', 'save_dir': './eval_results/EVAL_tcga_crc_5_cv', 'models_dir': 'results/tcga_crc_5_s1', 'model_type': 'clam_sb', 'drop_out': True, 'model_size': 'small'}\n",
      "label column: label\n",
      "label dictionary: {'normal_tissue': 0, 'tumor_tissue': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 1    1342\n",
      "0      91\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 3\n",
      "Slide-LVL; Number of samples registered in class 0: 91\n",
      "Patient-LVL; Number of samples registered in class 1: 497\n",
      "Slide-LVL; Number of samples registered in class 1: 1342\n",
      "Init Model\n",
      "CLAM_SB(\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (instance_loss_fn): CrossEntropyLoss()\n",
      ")\n",
      "Total number of parameters: 790791\n",
      "Total number of trainable parameters: 790791\n",
      "Init Loaders\n",
      "133\n",
      "test_error:  0.022556390977443608\n",
      "auc:  0.43333333333333335\n",
      "Init Model\n",
      "CLAM_SB(\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (instance_loss_fn): CrossEntropyLoss()\n",
      ")\n",
      "Total number of parameters: 790791\n",
      "Total number of trainable parameters: 790791\n",
      "Init Loaders\n",
      "140\n",
      "test_error:  0.04285714285714286\n",
      "auc:  0.5634328358208955\n",
      "Init Model\n",
      "CLAM_SB(\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (instance_loss_fn): CrossEntropyLoss()\n",
      ")\n",
      "Total number of parameters: 790791\n",
      "Total number of trainable parameters: 790791\n",
      "Init Loaders\n",
      "146\n",
      "test_error:  0.0821917808219178\n",
      "auc:  0.34079601990049746\n",
      "Init Model\n",
      "CLAM_SB(\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (instance_loss_fn): CrossEntropyLoss()\n",
      ")\n",
      "Total number of parameters: 790791\n",
      "Total number of trainable parameters: 790791\n",
      "Init Loaders\n",
      "149\n",
      "test_error:  0.08053691275167785\n",
      "auc:  0.3652676399026764\n",
      "Init Model\n",
      "CLAM_SB(\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (instance_loss_fn): CrossEntropyLoss()\n",
      ")\n",
      "Total number of parameters: 790791\n",
      "Total number of trainable parameters: 790791\n",
      "Init Loaders\n",
      "153\n",
      "test_error:  0.08496732026143791\n",
      "auc:  0.4582417582417582\n"
     ]
    }
   ],
   "source": [
    "!python eval.py --drop_out --k 5 --models_exp_code tcga_crc_5_s1 \\\n",
    "    --save_exp_code tcga_crc_5_cv \\\n",
    "    --task task_1_tumor_vs_normal --model_type clam_sb --results_dir results \\\n",
    "    --data_root_dir /home/sci/Disk2/tcga_crc/FEATURES_DIRECTORY_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "exp_arguments\n",
      "n_classes : 2\n",
      "save_exp_code : NSCLC_OUTPUT\n",
      "raw_save_dir : heatmaps/nsclc_raw_results\n",
      "production_save_dir : heatmaps/nsclc_production_results\n",
      "batch_size : 384\n",
      "\n",
      "data_arguments\n",
      "data_dir : heatmaps/demo/slides/\n",
      "data_dir_key : source\n",
      "process_list : myself.csv\n",
      "preset : presets/tcga.csv\n",
      "slide_ext : .svs\n",
      "label_dict : {'LUAD': 0, 'LUSC': 1}\n",
      "\n",
      "patching_arguments\n",
      "patch_size : 256\n",
      "overlap : 0.5\n",
      "patch_level : 1\n",
      "custom_downsample : 1\n",
      "\n",
      "model_arguments\n",
      "ckpt_path : heatmaps/demo/ckpts/tcga_nsclc_sb_fold4.pt\n",
      "model_type : clam_sb\n",
      "initiate_fn : initiate_model\n",
      "model_size : small\n",
      "drop_out : True\n",
      "\n",
      "heatmap_arguments\n",
      "vis_level : 1\n",
      "alpha : 0.4\n",
      "blank_canvas : False\n",
      "save_orig : True\n",
      "save_ext : jpg\n",
      "use_ref_scores : False\n",
      "blur : False\n",
      "use_center_shift : True\n",
      "use_roi : False\n",
      "calc_heatmap : True\n",
      "binarize : False\n",
      "binary_thresh : -1\n",
      "custom_downsample : 1\n",
      "cmap : jet\n",
      "\n",
      "sample_arguments\n",
      "samples : [{'name': 'topk_high_attention', 'sample': True, 'seed': 1, 'k': 10, 'mode': 'topk'}]\n",
      "Continue? Y/N ^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sci/PycharmProjects/chaofan/projects/CLAM/create_heatmaps.py\", line 94, in <module>\n",
      "    decision = input('Continue? Y/N ')\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python create_heatmaps.py --config config_template.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('lcf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dddf4cc44ccb2cf796823d3e6277e76da85a74a2d70ce7f32ed1ef0f62e61e3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
