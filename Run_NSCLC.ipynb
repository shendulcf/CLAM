{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wsi处理 \n",
    "step1 = False\n",
    "step2 = False\n",
    "step3 = True\n",
    "## step1 清理低质量图\n",
    "import openslide \n",
    "import os \n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "path = r'/home/sci/Disk2/tcga_crc/DATA_DIRECTORY'\n",
    "trash_path = r'/home/sci/Disk2/tcga_crc/trash'\n",
    "slides = os.listdir(path)\n",
    "slides = [slide for slide in slides if os.path.isfile(os.path.join(path,slide))]\n",
    "# print(slides)\n",
    "i = 0\n",
    "if step1:\n",
    "    for slide in slides:\n",
    "        slide_path = os.path.join(path,slide)\n",
    "        slide_object = openslide.OpenSlide(slide_path)\n",
    "        if slide_object.level_count == 2:\n",
    "            shutil.move(slide_path,trash_path)\n",
    "            i += 1\n",
    "            print(f'already process {i} slides to trash')\n",
    "\n",
    "## step 2 20/40 划分\n",
    "if step2:\n",
    "    for slide in slides:\n",
    "        slide_path = os.path.join(path,slide)\n",
    "        slide_object = openslide.OpenSlide(slide_path)\n",
    "        if slide_object.properties['aperio.AppMag'] == '40':\n",
    "            shutil.move(slide_path, os.path.join(path,'40'))\n",
    "        else:\n",
    "            shutil.move(slide_path, os.path.join(path,'20'))\n",
    "        i += 1\n",
    "        print(f'already process {i} slides to magfilefolder')\n",
    "## 删除不能用的WSI\n",
    "if step3:\n",
    "    \n",
    "    data_dir = r'/home/sci/Disk_data/Datasets/TCGA-NSCLC/WSI'\n",
    "    # trash_20 = r'/home/sci/Disk2/tcga_crc/trash/trash_20'\n",
    "    # os.makedirs(trash_20,exist_ok=True)\n",
    "    all_slides = os.listdir(data_dir)\n",
    "    patch_dir = r'/home/sci/Disk_data/Datasets/TCGA-NSCLC/BLOCKS_level-1/patches'\n",
    "    feat_dir = r'/home/sci/Disk_data/Datasets/TCGA-NSCLC/FEATURES_levle-1/pt_files'\n",
    "    all_patches = os.listdir(patch_dir)\n",
    "    for slide in all_slides:\n",
    "        slide_name,_ = os.path.splitext(slide)\n",
    "        slide_path = os.path.join(data_dir,slide)\n",
    "        # patch_path = os.path.join(patch_5_dir,f'{slide_name}.h5')\n",
    "        if not os.path.exists(os.path.join(patch_dir,f'{slide_name}.h5')):\n",
    "            shutil.move(slide_path, '/home/sci/Disk_data/Datasets/error')\n",
    "            os.remove(f'/home/sci/Disk_data/Datasets/TCGA-NSCLC/BLOCKS_level-1/masks/{slide_name}.jpg')\n",
    "            # os.remove(f'/home/sci/Disk_data/Datasets/TCGA-NSCLC/BLOCKS/thumbnails/{slide_name}_thumb.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## csv_file process\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df_csv = pd.read_csv('dataset_csv/TCGA-NSCLC.csv')\n",
    "dataset_dir = '/home/sci/Disk_data/Datasets/TCGA-NSCLC-sub/WSI'\n",
    "index = df_csv.index\n",
    "rows_to_delete = []\n",
    "for name in os.listdir(dataset_dir):\n",
    "    slide_id = os.path.splitext(name)[0]\n",
    "\n",
    "    row_index = df_csv[df_csv['slide_id'] == slide_id].index\n",
    "    rows_to_delete.append(row_index.item())\n",
    "    \n",
    "remaining_index = index.difference(rows_to_delete)\n",
    "df_delete = df_csv.drop(remaining_index).reset_index(drop=True)\n",
    "df_delete\n",
    "# df = df_csv.drop(~rows_to_delete)\n",
    "\n",
    "df_delete.to_csv('dataset_csv/tcga_lung_subtyping_sub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WSI 属性查看\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import openslide\n",
    "import os\n",
    "import torch\n",
    "slide_path = r'/home/sci/Disk_data/Datasets/TCGA-NSCLC/WSI/TCGA-05-4244-01A-01-BS1.svs'\n",
    "path = r'/home/sci/Disk_data/Datasets/TCGA-NSCLC-sub/WSI'\n",
    "# # ---->> for single slide\n",
    "# slide = openslide.OpenSlide(slide_path)\n",
    "# slide.dimensions\n",
    "# slide.associated_images\n",
    "# slide.level_dimensions\n",
    "# slide.level_count\n",
    "# slide.properties['aperio.AppMag']\n",
    "# slide.properties['aperio.MPP']\n",
    "\n",
    "# ---->> for slide folder\n",
    "slides = os.listdir(path)\n",
    "dimentions_list = []\n",
    "for slide in slides:\n",
    "    slide = os.path.join(path,slide)\n",
    "    slide = openslide.OpenSlide(slide)\n",
    "    print(slide.level_dimensions)\n",
    "    print(slide.properties['aperio.AppMag'])\n",
    "    print(slide.level_count)\n",
    "    # slide = slide.get_thumbnail((slide.level_dimensions[-1]))\n",
    "    # slide.save(r'/home/sci/Disk2/tcga_crc/DATA_DIRECTORY/test')\n",
    "    # print('save_success')\n",
    "    # dimentions_list.append(slide.level_dimensions[-1])\n",
    "# dimentions_list=torch.tensor(dimentions_list)\n",
    "# index = torch.topk(dimentions_list, 1, dim=1)\n",
    "# maxMap = torch.index_select(dimentions_list, index=index, dim=0 )\n",
    "# maxMap\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----> wsi transfer\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import openslide\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "path = r\"/home/sci/Disk_data/Datasets/TCGA-NSCLC-sub/WSI\"\n",
    "path_20 = os.path.join(path, '20')\n",
    "path_40 = os.path.join(path, '40')\n",
    "os.makedirs(path_20, exist_ok=True)\n",
    "os.makedirs(path_40, exist_ok=True)\n",
    "slide_list = glob.glob(os.path.join(path, '*.svs'))\n",
    "\n",
    "# for slide_path in slide_list:\n",
    "#     # slide_path = os.path.join(path, slide)\n",
    "#     slide_obj = openslide.open_slide(slide_path)\n",
    "\n",
    "#     if slide_obj.level_count == 3:\n",
    "#         shutil.move(slide_path, path_20)\n",
    "#     elif slide_obj.level_count == 4:\n",
    "#         shutil.move(slide_path, path_40)\n",
    "\n",
    "# ----> trans back\n",
    "allslide_path = glob.glob(os.path.join(path, '*/*.svs'))\n",
    "for slide_path in allslide_path:\n",
    "    shutil.move(slide_path, path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "path = r'/home/sci/Disk2/CPTAC-LUNG/FEATURES_level3/h5_files'\n",
    "num_patches = 0\n",
    "num_wsis = 0\n",
    "for i, h5_file in enumerate(os.listdir(path)):\n",
    "    fpath = os.path.join(path, h5_file)\n",
    "    f = h5py.File(fpath,'r')\n",
    "    # f.keys()\n",
    "    data = f.get('coords')\n",
    "    # print(data)\n",
    "    num_patches += data.shape[0]\n",
    "    num_wsis += 1\n",
    "print(f'Patches: {num_patches}')\n",
    "print(f'Average patches: {num_patches/num_wsis}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step_1 get patch\n",
    "\n",
    "source svs 文件地址\\\n",
    "save_dir 结果文件\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python create_patches_fp.py \\\n",
    "--source 'svs_dir' \\\n",
    "--save_dir 'result_dir' \\\n",
    "--patch_size 256 \\\n",
    "--seg \\ # 分割=True\n",
    "--patch \\ # 切分patch=True\n",
    "--stitch \\ # \n",
    "process_list_autogen.csv # 这里记录了每张 image 的处理的参数\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##改 savedir and patch_level\n",
    "!python create_patches_fp.py --source /home/sci/Disk_data/Datasets/TCGA-NSCLC-sub/WSI/20 --save_dir /home/sci/Disk_data/Datasets/TCGA-NSCLC-sub/BLOCKS_level1 --patch_level 0 --patch_size 256 --seg --patch --stitch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step_2 get patch features\n",
    "data_h5_dir  输出文件地址\\\n",
    "data_slide_dir svs文件地址\\\n",
    "上一步生成的csv csv_path\\\n",
    "feat_dir 输出文件地址\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CUDA_VISIBLE_DEVICES=0 python extract_features_fp.py \\\n",
    "--data_h5_dir data/RESULTS_DIRECTORY/patches \\\n",
    "--data_slide_dir /media/yuansh/14THHD/CLAM/DataSet/toy_example \\\n",
    "--csv_path /media/yuansh/14THHD/CLAM/Step_2.csv \\\n",
    "--feat_dir /media/yuansh/14THHD/CLAM/FEATURES_DIRECTORY \\\n",
    "--batch_size 512 \\\n",
    "--slide_ext .svs\n",
    "'''\n",
    "!python extract_features_fp.py\\\n",
    "    --data_h5_dir E:\\Workspace\\Project\\CLAM\\data\\RESULTS_DIRECTORY \\\n",
    "    --data_slide_dir F:/Download/TCGA \\\n",
    "    --csv_path data\\RESULTS_DIRECTORY\\Step_2.csv \\\n",
    "    --feat_dir E:\\Workspace\\Project\\CLAM\\data\\FEATURES_DIRECTORY \\\n",
    "    --batch_size 256 \\\n",
    "    --slide_ext .svs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成第2步骤需要的csv文件\n",
    "from utils.csv_gen import *\n",
    "\n",
    "csv_dir = r'/home/sci/Disk_data/Datasets/TCGA-NSCLC-sub/BLOCKS_level1/process_list_autogen.csv'\n",
    "# sort_csv = pd.read_csv(csv_dir).sort_values('slide_id')\n",
    "result_dir = r'/home/sci/Disk_data/Datasets/TCGA-NSCLC-sub/BLOCKS_level1/step2_get_features.csv'\n",
    "patch_dir = r'/home/sci/Disk_data/Datasets/TCGA-NSCLC-sub/BLOCKS_level1/patches'\n",
    "csv_gen_step1(csv_dir,result_dir,patch_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python extract_features_fp.py --data_h5_dir /home/sci/Disk_data/Datasets/TCGA-NSCLC-sub/BLOCKS_level1 \\\n",
    "--data_slide_dir /home/sci/Disk_data/Datasets/TCGA-NSCLC-sub/WSI \\\n",
    "--csv_path /home/sci/Disk_data/Datasets/TCGA-NSCLC-sub/BLOCKS_level1/step2_get_features.csv \\\n",
    "--feat_dir /home/sci/Disk_data/Datasets/TCGA-NSCLC-sub/FEATURES_level1 --batch_size 512 --slide_ext .svs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step3 Create split\n",
    "注意修改Create_split_seq.py文件中的csv路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.csv_gen import * \n",
    "path = r'/home/sci/Disk_data/TCGA-NSCLC/WSI'\n",
    "# sort_csv = pd.read_csv(csv_dir).sort_values('slide_id')\n",
    "result_dir = r'/home/sci/Disk_data/TCGA-NSCLC/RESULTS_DIRECTORY/step3_get_splits.csv' ## 5 + 20\n",
    "patch_dir = r'/home/sci/Disk_data/TCGA-NSCLC/RESULTS_DIRECTORY/patches'\n",
    "csv_gen_test(path,result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "label column: label\n",
      "label dictionary: {'LUAD': 0, 'LUSC': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 0    394\n",
      "1    434\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 333\n",
      "Slide-LVL; Number of samples registered in class 0: 394\n",
      "Patient-LVL; Number of samples registered in class 1: 400\n",
      "Slide-LVL; Number of samples registered in class 1: 434\n",
      "\n",
      "number of training samples: 663\n",
      "number of samples in cls 0: 309\n",
      "number of samples in cls 1: 354\n",
      "\n",
      "number of val samples: 85\n",
      "number of samples in cls 0: 45\n",
      "number of samples in cls 1: 40\n",
      "\n",
      "number of test samples: 80\n",
      "number of samples in cls 0: 40\n",
      "number of samples in cls 1: 40\n",
      "\n",
      "\n",
      "\n",
      "number of training samples: 671\n",
      "number of samples in cls 0: 318\n",
      "number of samples in cls 1: 353\n",
      "\n",
      "number of val samples: 84\n",
      "number of samples in cls 0: 43\n",
      "number of samples in cls 1: 41\n",
      "\n",
      "number of test samples: 73\n",
      "number of samples in cls 0: 33\n",
      "number of samples in cls 1: 40\n",
      "\n",
      "\n",
      "\n",
      "number of training samples: 660\n",
      "number of samples in cls 0: 321\n",
      "number of samples in cls 1: 339\n",
      "\n",
      "number of val samples: 84\n",
      "number of samples in cls 0: 39\n",
      "number of samples in cls 1: 45\n",
      "\n",
      "number of test samples: 84\n",
      "number of samples in cls 0: 34\n",
      "number of samples in cls 1: 50\n",
      "\n",
      "\n",
      "\n",
      "number of training samples: 660\n",
      "number of samples in cls 0: 319\n",
      "number of samples in cls 1: 341\n",
      "\n",
      "number of val samples: 74\n",
      "number of samples in cls 0: 34\n",
      "number of samples in cls 1: 40\n",
      "\n",
      "number of test samples: 94\n",
      "number of samples in cls 0: 41\n",
      "number of samples in cls 1: 53\n",
      "\n",
      "\n",
      "\n",
      "number of training samples: 650\n",
      "number of samples in cls 0: 303\n",
      "number of samples in cls 1: 347\n",
      "\n",
      "number of val samples: 93\n",
      "number of samples in cls 0: 47\n",
      "number of samples in cls 1: 46\n",
      "\n",
      "number of test samples: 85\n",
      "number of samples in cls 0: 44\n",
      "number of samples in cls 1: 41\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python create_splits_seq.py --task task_2_tumor_subtyping --seed 1 --label_frac 1 --k 5 --csv_path dataset_csv/tcga_lung_subtyping_sub.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 Train\n",
    "注意修改main.py中的csv路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA_VISIBLE_DEVICES=0 python main.py \\\n",
    "--drop_out \\\n",
    "--early_stopping \\\n",
    "--lr 2e-4 \\\n",
    "--k 10 \\\n",
    "--label_frac 0.75 \\\n",
    "--exp_code task_1_tumor_vs_normal_CLAM_50 --weighted_sample --bag_loss ce --inst_loss svm --task task_1_tumor_vs_normal --model_type clam_sb --log_data \\\n",
    "--data_root_dir /media/yuansh/14THHD/CLAM/FEATURES_DIRECTORY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python main.py --drop_out --early_stopping --lr 2e-4 --k 5 --label_frac 1\\\n",
    "--exp_code tcga_nsclc_sub_100_level3_mb --weighted_sample --bag_loss ce --inst_loss svm \\\n",
    "--task task_2_tumor_subtyping --model_type clam_mb --log_data --data_root_dir /home/sci/Disk_data/Datasets/TCGA-NSCLC-sub/FEATURES_level3 \\\n",
    "--split_dir /home/sci/Disk_data/SCI_Projects/chaofan/projects/CLAM/splits/tcga_nsclc_100 --subtyping \\\n",
    "--csv_path dataset_csv/tcga_lung_subtyping_sub.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python main.py --drop_out --early_stopping --lr 2e-4 --k 5 --label_frac 0.75 \\\n",
    "--exp_code tcga_nsclc_75_level1_transmil --weighted_sample --bag_loss ce --inst_loss svm \\\n",
    "--task task_2_tumor_subtyping --model_type transmil --log_data --data_root_dir /home/sci/Disk_data/Datasets/TCGA-NSCLC/FEATURES_level1 \\\n",
    "--split_dir /home/sci/Disk_data/SCI_Projects/chaofan/projects/CLAM/splits/task_2_tcga_subtyping_75 --subtyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransformerMIL实验结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python main.py --drop_out --early_stopping --lr 2e-4 --k 5 --label_frac 1 \\\n",
    "--exp_code tcga_nsclc_sub_100_level3_transmil --weighted_sample --bag_loss ce --inst_loss svm \\\n",
    "--task task_2_tumor_subtyping --model_type transmil --log_data --data_root_dir /home/sci/Disk_data/Datasets/TCGA-NSCLC-sub/FEATURES_level3 \\\n",
    "--split_dir /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/tcga_nsclc_100 --subtyping \\\n",
    "--csv_path dataset_csv/tcga_lung_subtyping_sub.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python main.py --drop_out --early_stopping --lr 2e-4 --k 5 --label_frac 1 \\\n",
    "--exp_code tcga_nsclc_sub_100_level1_transmil --weighted_sample --bag_loss ce --inst_loss svm \\\n",
    "--task task_2_tumor_subtyping --model_type transmil --log_data --data_root_dir /home/sci/Disk_data/Datasets/TCGA-NSCLC-sub/FEATURES_level1 \\\n",
    "--split_dir /home/sci/Disk_data/SCI_Projects/chaofan/projects/CLAM/splits/tcga_nsclc_100 --subtyping \\\n",
    "--csv_path dataset_csv/tcga_lung_subtyping_sub.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### proposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load Dataset\n",
      "label column: label\n",
      "label dictionary: {'LUAD': 0, 'LUSC': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 0    394\n",
      "1    434\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 333\n",
      "Slide-LVL; Number of samples registered in class 0: 394\n",
      "Patient-LVL; Number of samples registered in class 1: 400\n",
      "Slide-LVL; Number of samples registered in class 1: 434\n",
      "split_dir:  /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/tcga_nsclc_100\n",
      "################# Settings ###################\n",
      "num_splits:  5\n",
      "k_start:  -1\n",
      "k_end:  -1\n",
      "task:  task_2_tumor_subtyping\n",
      "max_epochs:  200\n",
      "results_dir:  ./results\n",
      "lr:  0.0002\n",
      "experiment:  tcga_nsclc_sub_100_level13_MCBAT_d1f2l3\n",
      "reg:  1e-05\n",
      "label_frac:  1.0\n",
      "bag_loss:  ce\n",
      "seed:  1\n",
      "model_type:  mcbat_sb\n",
      "model_size:  small\n",
      "use_drop_out:  True\n",
      "weighted_sample:  True\n",
      "opt:  adam\n",
      "bag_weight:  0.7\n",
      "inst_loss:  svm\n",
      "B:  8\n",
      "split_dir:  /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/tcga_nsclc_100\n",
      "\n",
      "Training Fold 0!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 663 samples\n",
      "Validating on 85 samples\n",
      "Testing on 80 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Setting tau to 1.0\n",
      "Done!\n",
      "MCBAT_SB(\n",
      "  (instance_loss_fn): SmoothTop1SVM()\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (transformer_low): TransformerEncoder_FLASHAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FlashAttention(\n",
      "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
      "            (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer_high): TransformerEncoder_FLASHAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FlashAttention(\n",
      "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
      "            (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attention): Attn_Net_Gated(\n",
      "    (attention_a): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_b): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (attention_V2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (attention_U2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (attention_weights2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Total number of parameters: 8408073\n",
      "Total number of trainable parameters: 8408073\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "663.0\n",
      "2\n",
      "309\n",
      "354\n",
      "##################################################\n",
      "Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 8.4393, instance_loss: 1.3039, weighted_loss: 6.2987, label: 0, bag_size: 58\n",
      "batch 39, loss: 0.1566, instance_loss: 0.6974, weighted_loss: 0.3189, label: 0, bag_size: 63\n",
      "batch 59, loss: 1.6792, instance_loss: 0.8567, weighted_loss: 1.4324, label: 1, bag_size: 100\n",
      "batch 79, loss: 0.0583, instance_loss: 1.1312, weighted_loss: 0.3802, label: 1, bag_size: 72\n",
      "batch 99, loss: 2.0463, instance_loss: 1.0175, weighted_loss: 1.7376, label: 0, bag_size: 77\n",
      "batch 119, loss: 4.4552, instance_loss: 1.0085, weighted_loss: 3.4212, label: 0, bag_size: 114\n",
      "batch 139, loss: 0.5357, instance_loss: 1.1608, weighted_loss: 0.7233, label: 0, bag_size: 80\n",
      "batch 159, loss: 4.7830, instance_loss: 1.2501, weighted_loss: 3.7231, label: 0, bag_size: 88\n",
      "batch 179, loss: 1.0679, instance_loss: 1.2217, weighted_loss: 1.1140, label: 0, bag_size: 93\n",
      "batch 199, loss: 0.2679, instance_loss: 1.1549, weighted_loss: 0.5340, label: 1, bag_size: 52\n",
      "batch 219, loss: 1.3534, instance_loss: 0.9005, weighted_loss: 1.2175, label: 0, bag_size: 20\n",
      "batch 239, loss: 0.2476, instance_loss: 1.4452, weighted_loss: 0.6068, label: 1, bag_size: 46\n",
      "batch 259, loss: 0.1851, instance_loss: 0.9186, weighted_loss: 0.4051, label: 1, bag_size: 23\n",
      "batch 279, loss: 0.6049, instance_loss: 0.8967, weighted_loss: 0.6924, label: 1, bag_size: 70\n",
      "batch 299, loss: 0.4472, instance_loss: 1.1233, weighted_loss: 0.6500, label: 0, bag_size: 49\n",
      "batch 319, loss: 0.9830, instance_loss: 0.7301, weighted_loss: 0.9071, label: 1, bag_size: 76\n",
      "batch 339, loss: 2.1001, instance_loss: 0.8879, weighted_loss: 1.7364, label: 0, bag_size: 58\n",
      "batch 359, loss: 1.4080, instance_loss: 1.4088, weighted_loss: 1.4082, label: 1, bag_size: 97\n",
      "batch 379, loss: 1.0335, instance_loss: 0.8384, weighted_loss: 0.9750, label: 1, bag_size: 51\n",
      "batch 399, loss: 2.6752, instance_loss: 0.8140, weighted_loss: 2.1169, label: 0, bag_size: 28\n",
      "batch 419, loss: 1.2935, instance_loss: 0.9978, weighted_loss: 1.2048, label: 0, bag_size: 38\n",
      "batch 439, loss: 0.2004, instance_loss: 0.8581, weighted_loss: 0.3977, label: 0, bag_size: 43\n",
      "batch 459, loss: 0.0919, instance_loss: 0.8434, weighted_loss: 0.3174, label: 1, bag_size: 17\n",
      "batch 479, loss: 2.2134, instance_loss: 1.5684, weighted_loss: 2.0199, label: 0, bag_size: 79\n",
      "batch 499, loss: 0.1758, instance_loss: 1.1519, weighted_loss: 0.4687, label: 1, bag_size: 56\n",
      "batch 519, loss: 1.8025, instance_loss: 0.9754, weighted_loss: 1.5544, label: 1, bag_size: 119\n",
      "batch 539, loss: 2.1344, instance_loss: 0.9131, weighted_loss: 1.7680, label: 1, bag_size: 49\n",
      "batch 559, loss: 0.3431, instance_loss: 0.9376, weighted_loss: 0.5214, label: 0, bag_size: 50\n",
      "batch 579, loss: 2.1597, instance_loss: 0.9905, weighted_loss: 1.8090, label: 1, bag_size: 55\n",
      "batch 599, loss: 0.8029, instance_loss: 1.0326, weighted_loss: 0.8718, label: 1, bag_size: 83\n",
      "batch 619, loss: 1.6719, instance_loss: 1.4316, weighted_loss: 1.5998, label: 1, bag_size: 19\n",
      "batch 639, loss: 1.6304, instance_loss: 1.7998, weighted_loss: 1.6812, label: 0, bag_size: 31\n",
      "batch 659, loss: 1.6329, instance_loss: 1.1250, weighted_loss: 1.4805, label: 0, bag_size: 71\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9896304675716441: correct 10498/10608\n",
      "class 1 clustering acc 0.024132730015082957: correct 128/5304\n",
      "Epoch: 0, train_loss: 1.0749, train_clustering_loss:  1.0047, train_error: 0.4630\n",
      "class 0: acc 0.5335365853658537, correct 175/328\n",
      "class 1: acc 0.5402985074626866, correct 181/335\n",
      "\n",
      "Val Set, val_loss: 0.5723, val_error: 0.2706, auc: 0.8150\n",
      "class 0 clustering acc 0.9360294117647059: correct 1273/1360\n",
      "class 1 clustering acc 0.18382352941176472: correct 125/680\n",
      "class 0: acc 0.6, correct 27/45\n",
      "class 1: acc 0.875, correct 35/40\n",
      "Validation loss decreased (inf --> 0.572297).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0989, instance_loss: 0.5553, weighted_loss: 0.2358, label: 0, bag_size: 53\n",
      "batch 39, loss: 0.5629, instance_loss: 1.7194, weighted_loss: 0.9099, label: 0, bag_size: 57\n",
      "batch 59, loss: 0.6773, instance_loss: 1.7065, weighted_loss: 0.9861, label: 1, bag_size: 38\n",
      "batch 79, loss: 0.5812, instance_loss: 0.8317, weighted_loss: 0.6563, label: 0, bag_size: 88\n",
      "batch 99, loss: 0.0389, instance_loss: 0.9250, weighted_loss: 0.3047, label: 1, bag_size: 17\n",
      "batch 119, loss: 2.4077, instance_loss: 0.8663, weighted_loss: 1.9453, label: 0, bag_size: 110\n",
      "batch 139, loss: 0.1214, instance_loss: 0.8258, weighted_loss: 0.3328, label: 0, bag_size: 51\n",
      "batch 159, loss: 0.1578, instance_loss: 0.7452, weighted_loss: 0.3340, label: 1, bag_size: 97\n",
      "batch 179, loss: 0.3635, instance_loss: 1.0486, weighted_loss: 0.5690, label: 1, bag_size: 35\n",
      "batch 199, loss: 0.2431, instance_loss: 0.7092, weighted_loss: 0.3829, label: 0, bag_size: 65\n",
      "batch 219, loss: 5.8509, instance_loss: 1.1390, weighted_loss: 4.4373, label: 0, bag_size: 28\n",
      "batch 239, loss: 0.3048, instance_loss: 1.3304, weighted_loss: 0.6125, label: 0, bag_size: 77\n",
      "batch 259, loss: 0.4990, instance_loss: 1.4567, weighted_loss: 0.7863, label: 1, bag_size: 45\n",
      "batch 279, loss: 0.5432, instance_loss: 0.9260, weighted_loss: 0.6581, label: 1, bag_size: 34\n",
      "batch 299, loss: 0.4835, instance_loss: 0.9224, weighted_loss: 0.6152, label: 1, bag_size: 52\n",
      "batch 319, loss: 2.1576, instance_loss: 0.9034, weighted_loss: 1.7814, label: 1, bag_size: 94\n",
      "batch 339, loss: 2.2369, instance_loss: 1.2542, weighted_loss: 1.9421, label: 0, bag_size: 29\n",
      "batch 359, loss: 1.2415, instance_loss: 1.1201, weighted_loss: 1.2051, label: 0, bag_size: 66\n",
      "batch 379, loss: 0.3394, instance_loss: 0.9468, weighted_loss: 0.5216, label: 1, bag_size: 73\n",
      "batch 399, loss: 0.6103, instance_loss: 1.1149, weighted_loss: 0.7617, label: 1, bag_size: 41\n",
      "batch 419, loss: 0.4729, instance_loss: 1.0939, weighted_loss: 0.6592, label: 0, bag_size: 54\n",
      "batch 439, loss: 0.6367, instance_loss: 0.7584, weighted_loss: 0.6732, label: 1, bag_size: 121\n",
      "batch 459, loss: 0.3011, instance_loss: 0.9914, weighted_loss: 0.5082, label: 0, bag_size: 99\n",
      "batch 479, loss: 0.0555, instance_loss: 0.9483, weighted_loss: 0.3233, label: 1, bag_size: 98\n",
      "batch 499, loss: 1.2681, instance_loss: 0.9367, weighted_loss: 1.1687, label: 1, bag_size: 95\n",
      "batch 519, loss: 0.0008, instance_loss: 1.0832, weighted_loss: 0.3255, label: 0, bag_size: 40\n",
      "batch 539, loss: 1.1685, instance_loss: 1.0042, weighted_loss: 1.1192, label: 0, bag_size: 31\n",
      "batch 559, loss: 0.1654, instance_loss: 0.9652, weighted_loss: 0.4053, label: 0, bag_size: 94\n",
      "batch 579, loss: 0.1200, instance_loss: 0.7421, weighted_loss: 0.3066, label: 0, bag_size: 78\n",
      "batch 599, loss: 0.3034, instance_loss: 0.7140, weighted_loss: 0.4266, label: 0, bag_size: 106\n",
      "batch 619, loss: 0.0796, instance_loss: 0.7022, weighted_loss: 0.2664, label: 0, bag_size: 30\n",
      "batch 639, loss: 0.0967, instance_loss: 1.4713, weighted_loss: 0.5091, label: 1, bag_size: 100\n",
      "batch 659, loss: 0.5425, instance_loss: 0.9867, weighted_loss: 0.6758, label: 0, bag_size: 81\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9793552036199095: correct 10389/10608\n",
      "class 1 clustering acc 0.05448717948717949: correct 289/5304\n",
      "Epoch: 1, train_loss: 0.8023, train_clustering_loss:  0.9844, train_error: 0.3635\n",
      "class 0: acc 0.6215384615384615, correct 202/325\n",
      "class 1: acc 0.650887573964497, correct 220/338\n",
      "\n",
      "Val Set, val_loss: 0.7661, val_error: 0.4706, auc: 0.8450\n",
      "class 0 clustering acc 1.0: correct 1360/1360\n",
      "class 1 clustering acc 0.0: correct 0/680\n",
      "class 0: acc 1.0, correct 45/45\n",
      "class 1: acc 0.0, correct 0/40\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3829, instance_loss: 0.8045, weighted_loss: 0.5094, label: 1, bag_size: 48\n",
      "batch 39, loss: 0.7454, instance_loss: 0.6928, weighted_loss: 0.7296, label: 0, bag_size: 75\n",
      "batch 59, loss: 0.1682, instance_loss: 0.5185, weighted_loss: 0.2733, label: 1, bag_size: 52\n",
      "batch 79, loss: 0.0185, instance_loss: 0.5159, weighted_loss: 0.1677, label: 1, bag_size: 85\n",
      "batch 99, loss: 0.8249, instance_loss: 0.7526, weighted_loss: 0.8032, label: 1, bag_size: 83\n",
      "batch 119, loss: 0.0798, instance_loss: 0.5139, weighted_loss: 0.2101, label: 1, bag_size: 63\n",
      "batch 139, loss: 0.1411, instance_loss: 0.9964, weighted_loss: 0.3977, label: 1, bag_size: 21\n",
      "batch 159, loss: 0.0008, instance_loss: 0.1427, weighted_loss: 0.0433, label: 1, bag_size: 76\n",
      "batch 179, loss: 2.3044, instance_loss: 0.9374, weighted_loss: 1.8943, label: 0, bag_size: 32\n",
      "batch 199, loss: 0.2624, instance_loss: 0.3972, weighted_loss: 0.3028, label: 0, bag_size: 20\n",
      "batch 219, loss: 3.8130, instance_loss: 0.7917, weighted_loss: 2.9066, label: 0, bag_size: 18\n",
      "batch 239, loss: 0.3231, instance_loss: 0.3816, weighted_loss: 0.3406, label: 1, bag_size: 111\n",
      "batch 259, loss: 0.0665, instance_loss: 0.9993, weighted_loss: 0.3463, label: 1, bag_size: 56\n",
      "batch 279, loss: 0.0976, instance_loss: 0.2480, weighted_loss: 0.1427, label: 1, bag_size: 28\n",
      "batch 299, loss: 1.0690, instance_loss: 1.0198, weighted_loss: 1.0543, label: 0, bag_size: 82\n",
      "batch 319, loss: 0.8726, instance_loss: 1.7894, weighted_loss: 1.1476, label: 1, bag_size: 40\n",
      "batch 339, loss: 1.4789, instance_loss: 1.6414, weighted_loss: 1.5277, label: 1, bag_size: 67\n",
      "batch 359, loss: 0.0326, instance_loss: 0.4225, weighted_loss: 0.1495, label: 0, bag_size: 74\n",
      "batch 379, loss: 0.0104, instance_loss: 0.2760, weighted_loss: 0.0901, label: 0, bag_size: 86\n",
      "batch 399, loss: 0.5239, instance_loss: 0.6276, weighted_loss: 0.5550, label: 0, bag_size: 25\n",
      "batch 419, loss: 0.4240, instance_loss: 0.8653, weighted_loss: 0.5564, label: 1, bag_size: 87\n",
      "batch 439, loss: 1.5173, instance_loss: 2.4037, weighted_loss: 1.7832, label: 1, bag_size: 69\n",
      "batch 459, loss: 0.3000, instance_loss: 1.0475, weighted_loss: 0.5242, label: 0, bag_size: 67\n",
      "batch 479, loss: 0.3867, instance_loss: 0.1178, weighted_loss: 0.3061, label: 0, bag_size: 39\n",
      "batch 499, loss: 0.2937, instance_loss: 0.6461, weighted_loss: 0.3994, label: 0, bag_size: 25\n",
      "batch 519, loss: 0.4864, instance_loss: 0.5753, weighted_loss: 0.5130, label: 1, bag_size: 64\n",
      "batch 539, loss: 0.0451, instance_loss: 0.8021, weighted_loss: 0.2722, label: 0, bag_size: 50\n",
      "batch 559, loss: 1.1471, instance_loss: 1.1855, weighted_loss: 1.1586, label: 0, bag_size: 93\n",
      "batch 579, loss: 0.1037, instance_loss: 0.9802, weighted_loss: 0.3666, label: 1, bag_size: 50\n",
      "batch 599, loss: 4.2562, instance_loss: 1.0077, weighted_loss: 3.2816, label: 0, bag_size: 40\n",
      "batch 619, loss: 0.4941, instance_loss: 0.6656, weighted_loss: 0.5456, label: 0, bag_size: 35\n",
      "batch 639, loss: 0.5924, instance_loss: 0.8167, weighted_loss: 0.6597, label: 1, bag_size: 97\n",
      "batch 659, loss: 0.6668, instance_loss: 1.1054, weighted_loss: 0.7984, label: 0, bag_size: 49\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9407051282051282: correct 9979/10608\n",
      "class 1 clustering acc 0.3721719457013575: correct 1974/5304\n",
      "Epoch: 2, train_loss: 0.6371, train_clustering_loss:  0.7984, train_error: 0.2941\n",
      "class 0: acc 0.680379746835443, correct 215/316\n",
      "class 1: acc 0.729106628242075, correct 253/347\n",
      "\n",
      "Val Set, val_loss: 0.6080, val_error: 0.2118, auc: 0.8594\n",
      "class 0 clustering acc 0.9169117647058823: correct 1247/1360\n",
      "class 1 clustering acc 0.5588235294117647: correct 380/680\n",
      "class 0: acc 0.8444444444444444, correct 38/45\n",
      "class 1: acc 0.725, correct 29/40\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.0508, weighted_loss: 0.0153, label: 0, bag_size: 90\n",
      "batch 39, loss: 0.3630, instance_loss: 0.2916, weighted_loss: 0.3416, label: 0, bag_size: 68\n",
      "batch 59, loss: 0.9026, instance_loss: 0.4505, weighted_loss: 0.7670, label: 1, bag_size: 42\n",
      "batch 79, loss: 0.0234, instance_loss: 0.1519, weighted_loss: 0.0620, label: 0, bag_size: 64\n",
      "batch 99, loss: 0.6111, instance_loss: 0.3697, weighted_loss: 0.5387, label: 0, bag_size: 51\n",
      "batch 119, loss: 0.1184, instance_loss: 0.3176, weighted_loss: 0.1782, label: 0, bag_size: 78\n",
      "batch 139, loss: 1.3031, instance_loss: 0.8596, weighted_loss: 1.1701, label: 0, bag_size: 18\n",
      "batch 159, loss: 0.2578, instance_loss: 1.4815, weighted_loss: 0.6249, label: 0, bag_size: 27\n",
      "batch 179, loss: 0.1625, instance_loss: 0.4637, weighted_loss: 0.2528, label: 0, bag_size: 71\n",
      "batch 199, loss: 0.4158, instance_loss: 0.9483, weighted_loss: 0.5756, label: 1, bag_size: 29\n",
      "batch 219, loss: 1.4584, instance_loss: 0.7643, weighted_loss: 1.2502, label: 1, bag_size: 67\n",
      "batch 239, loss: 0.5348, instance_loss: 0.4339, weighted_loss: 0.5045, label: 1, bag_size: 32\n",
      "batch 259, loss: 0.0057, instance_loss: 0.0215, weighted_loss: 0.0105, label: 1, bag_size: 77\n",
      "batch 279, loss: 1.2668, instance_loss: 0.6354, weighted_loss: 1.0774, label: 0, bag_size: 57\n",
      "batch 299, loss: 0.0160, instance_loss: 0.2169, weighted_loss: 0.0763, label: 1, bag_size: 31\n",
      "batch 319, loss: 0.4670, instance_loss: 0.6710, weighted_loss: 0.5282, label: 1, bag_size: 54\n",
      "batch 339, loss: 0.0534, instance_loss: 0.1525, weighted_loss: 0.0831, label: 1, bag_size: 121\n",
      "batch 359, loss: 0.1889, instance_loss: 0.1983, weighted_loss: 0.1917, label: 1, bag_size: 46\n",
      "batch 379, loss: 0.0191, instance_loss: 0.1571, weighted_loss: 0.0605, label: 0, bag_size: 33\n",
      "batch 399, loss: 0.2356, instance_loss: 0.1675, weighted_loss: 0.2152, label: 0, bag_size: 97\n",
      "batch 419, loss: 0.9235, instance_loss: 1.4344, weighted_loss: 1.0768, label: 1, bag_size: 30\n",
      "batch 439, loss: 0.0251, instance_loss: 0.0866, weighted_loss: 0.0436, label: 0, bag_size: 53\n",
      "batch 459, loss: 1.7742, instance_loss: 1.2705, weighted_loss: 1.6231, label: 1, bag_size: 88\n",
      "batch 479, loss: 0.0197, instance_loss: 0.1750, weighted_loss: 0.0663, label: 1, bag_size: 95\n",
      "batch 499, loss: 0.0002, instance_loss: 0.1018, weighted_loss: 0.0307, label: 0, bag_size: 67\n",
      "batch 519, loss: 0.0026, instance_loss: 0.1018, weighted_loss: 0.0323, label: 1, bag_size: 89\n",
      "batch 539, loss: 0.0006, instance_loss: 0.1073, weighted_loss: 0.0327, label: 1, bag_size: 39\n",
      "batch 559, loss: 0.0107, instance_loss: 0.0255, weighted_loss: 0.0152, label: 0, bag_size: 94\n",
      "batch 579, loss: 0.0065, instance_loss: 0.0533, weighted_loss: 0.0205, label: 1, bag_size: 91\n",
      "batch 599, loss: 0.0005, instance_loss: 0.0143, weighted_loss: 0.0046, label: 0, bag_size: 38\n",
      "batch 619, loss: 0.2078, instance_loss: 0.1112, weighted_loss: 0.1788, label: 0, bag_size: 38\n",
      "batch 639, loss: 0.0932, instance_loss: 0.6889, weighted_loss: 0.2719, label: 1, bag_size: 41\n",
      "batch 659, loss: 0.2237, instance_loss: 0.0635, weighted_loss: 0.1757, label: 1, bag_size: 68\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9371229260935143: correct 9941/10608\n",
      "class 1 clustering acc 0.6146304675716441: correct 3260/5304\n",
      "Epoch: 3, train_loss: 0.5132, train_clustering_loss:  0.6048, train_error: 0.2368\n",
      "class 0: acc 0.7598784194528876, correct 250/329\n",
      "class 1: acc 0.7664670658682635, correct 256/334\n",
      "\n",
      "Val Set, val_loss: 0.6087, val_error: 0.2706, auc: 0.9233\n",
      "class 0 clustering acc 0.8948529411764706: correct 1217/1360\n",
      "class 1 clustering acc 0.6308823529411764: correct 429/680\n",
      "class 0: acc 0.5555555555555556, correct 25/45\n",
      "class 1: acc 0.925, correct 37/40\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 4.3083, instance_loss: 0.7294, weighted_loss: 3.2346, label: 0, bag_size: 82\n",
      "batch 39, loss: 0.1948, instance_loss: 1.1405, weighted_loss: 0.4785, label: 1, bag_size: 84\n",
      "batch 59, loss: 0.0248, instance_loss: 0.1839, weighted_loss: 0.0725, label: 0, bag_size: 66\n",
      "batch 79, loss: 0.0366, instance_loss: 0.0572, weighted_loss: 0.0428, label: 0, bag_size: 88\n",
      "batch 99, loss: 0.1860, instance_loss: 0.0730, weighted_loss: 0.1521, label: 1, bag_size: 94\n",
      "batch 119, loss: 0.0040, instance_loss: 0.0209, weighted_loss: 0.0090, label: 0, bag_size: 51\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0360, weighted_loss: 0.0109, label: 0, bag_size: 31\n",
      "batch 159, loss: 3.6238, instance_loss: 1.6354, weighted_loss: 3.0273, label: 0, bag_size: 42\n",
      "batch 179, loss: 0.0698, instance_loss: 0.0832, weighted_loss: 0.0738, label: 0, bag_size: 102\n",
      "batch 199, loss: 0.1180, instance_loss: 0.5638, weighted_loss: 0.2517, label: 1, bag_size: 86\n",
      "batch 219, loss: 0.6847, instance_loss: 1.6392, weighted_loss: 0.9710, label: 0, bag_size: 88\n",
      "batch 239, loss: 0.0001, instance_loss: 0.0525, weighted_loss: 0.0158, label: 0, bag_size: 37\n",
      "batch 259, loss: 0.0000, instance_loss: 0.7666, weighted_loss: 0.2300, label: 0, bag_size: 87\n",
      "batch 279, loss: 0.0128, instance_loss: 0.6695, weighted_loss: 0.2098, label: 1, bag_size: 52\n",
      "batch 299, loss: 0.8351, instance_loss: 1.6815, weighted_loss: 1.0890, label: 1, bag_size: 35\n",
      "batch 319, loss: 0.1495, instance_loss: 0.1893, weighted_loss: 0.1614, label: 1, bag_size: 27\n",
      "batch 339, loss: 0.0641, instance_loss: 0.1810, weighted_loss: 0.0992, label: 1, bag_size: 77\n",
      "batch 359, loss: 0.0013, instance_loss: 0.6925, weighted_loss: 0.2087, label: 0, bag_size: 88\n",
      "batch 379, loss: 0.2020, instance_loss: 2.0944, weighted_loss: 0.7697, label: 0, bag_size: 41\n",
      "batch 399, loss: 0.9449, instance_loss: 0.7068, weighted_loss: 0.8735, label: 1, bag_size: 39\n",
      "batch 419, loss: 0.0150, instance_loss: 0.4882, weighted_loss: 0.1569, label: 0, bag_size: 83\n",
      "batch 439, loss: 0.0973, instance_loss: 0.1629, weighted_loss: 0.1169, label: 0, bag_size: 88\n",
      "batch 459, loss: 0.0725, instance_loss: 0.0440, weighted_loss: 0.0639, label: 0, bag_size: 25\n",
      "batch 479, loss: 0.6908, instance_loss: 0.1410, weighted_loss: 0.5258, label: 1, bag_size: 96\n",
      "batch 499, loss: 0.0054, instance_loss: 0.0281, weighted_loss: 0.0122, label: 0, bag_size: 21\n",
      "batch 519, loss: 0.0005, instance_loss: 0.0165, weighted_loss: 0.0053, label: 1, bag_size: 109\n",
      "batch 539, loss: 0.0370, instance_loss: 0.2681, weighted_loss: 0.1063, label: 0, bag_size: 65\n",
      "batch 559, loss: 4.4503, instance_loss: 3.7897, weighted_loss: 4.2521, label: 1, bag_size: 31\n",
      "batch 579, loss: 0.0110, instance_loss: 0.0723, weighted_loss: 0.0294, label: 1, bag_size: 85\n",
      "batch 599, loss: 0.0045, instance_loss: 0.0163, weighted_loss: 0.0080, label: 0, bag_size: 85\n",
      "batch 619, loss: 0.0087, instance_loss: 0.0863, weighted_loss: 0.0320, label: 1, bag_size: 27\n",
      "batch 639, loss: 0.3147, instance_loss: 0.8265, weighted_loss: 0.4683, label: 1, bag_size: 67\n",
      "batch 659, loss: 0.7401, instance_loss: 0.8923, weighted_loss: 0.7858, label: 1, bag_size: 82\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9346719457013575: correct 9915/10608\n",
      "class 1 clustering acc 0.6027526395173454: correct 3197/5304\n",
      "Epoch: 4, train_loss: 0.5409, train_clustering_loss:  0.6110, train_error: 0.2247\n",
      "class 0: acc 0.7735849056603774, correct 246/318\n",
      "class 1: acc 0.7768115942028986, correct 268/345\n",
      "\n",
      "Val Set, val_loss: 0.9937, val_error: 0.4000, auc: 0.8972\n",
      "class 0 clustering acc 0.9713235294117647: correct 1321/1360\n",
      "class 1 clustering acc 0.49117647058823527: correct 334/680\n",
      "class 0: acc 0.24444444444444444, correct 11/45\n",
      "class 1: acc 1.0, correct 40/40\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 2.5104, instance_loss: 0.4573, weighted_loss: 1.8945, label: 1, bag_size: 42\n",
      "batch 39, loss: 0.1545, instance_loss: 0.1997, weighted_loss: 0.1681, label: 1, bag_size: 88\n",
      "batch 59, loss: 0.0342, instance_loss: 0.1411, weighted_loss: 0.0663, label: 1, bag_size: 58\n",
      "batch 79, loss: 0.0003, instance_loss: 0.1081, weighted_loss: 0.0326, label: 0, bag_size: 40\n",
      "batch 99, loss: 0.0689, instance_loss: 0.2145, weighted_loss: 0.1126, label: 1, bag_size: 118\n",
      "batch 119, loss: 4.0703, instance_loss: 0.7601, weighted_loss: 3.0773, label: 0, bag_size: 48\n",
      "batch 139, loss: 0.0000, instance_loss: 0.5700, weighted_loss: 0.1710, label: 1, bag_size: 69\n",
      "batch 159, loss: 0.2071, instance_loss: 0.8460, weighted_loss: 0.3988, label: 1, bag_size: 52\n",
      "batch 179, loss: 0.0191, instance_loss: 0.0823, weighted_loss: 0.0380, label: 1, bag_size: 26\n",
      "batch 199, loss: 1.3279, instance_loss: 0.2812, weighted_loss: 1.0139, label: 0, bag_size: 35\n",
      "batch 219, loss: 0.0313, instance_loss: 0.3332, weighted_loss: 0.1219, label: 0, bag_size: 74\n",
      "batch 239, loss: 0.4758, instance_loss: 0.2046, weighted_loss: 0.3945, label: 0, bag_size: 98\n",
      "batch 259, loss: 3.4198, instance_loss: 1.6153, weighted_loss: 2.8785, label: 0, bag_size: 43\n",
      "batch 279, loss: 0.0079, instance_loss: 0.0748, weighted_loss: 0.0279, label: 0, bag_size: 63\n",
      "batch 299, loss: 0.0295, instance_loss: 0.0755, weighted_loss: 0.0433, label: 1, bag_size: 99\n",
      "batch 319, loss: 0.0249, instance_loss: 0.0584, weighted_loss: 0.0349, label: 1, bag_size: 75\n",
      "batch 339, loss: 0.0273, instance_loss: 0.2219, weighted_loss: 0.0857, label: 0, bag_size: 41\n",
      "batch 359, loss: 0.3566, instance_loss: 0.0511, weighted_loss: 0.2650, label: 1, bag_size: 52\n",
      "batch 379, loss: 0.1172, instance_loss: 0.1229, weighted_loss: 0.1189, label: 1, bag_size: 27\n",
      "batch 399, loss: 1.2913, instance_loss: 0.9090, weighted_loss: 1.1766, label: 0, bag_size: 87\n",
      "batch 419, loss: 0.7843, instance_loss: 1.2150, weighted_loss: 0.9135, label: 0, bag_size: 42\n",
      "batch 439, loss: 0.4604, instance_loss: 1.3709, weighted_loss: 0.7335, label: 1, bag_size: 96\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0101, weighted_loss: 0.0031, label: 0, bag_size: 51\n",
      "batch 479, loss: 0.1167, instance_loss: 0.0387, weighted_loss: 0.0933, label: 0, bag_size: 90\n",
      "batch 499, loss: 0.0163, instance_loss: 0.0657, weighted_loss: 0.0311, label: 0, bag_size: 37\n",
      "batch 519, loss: 1.2027, instance_loss: 0.8993, weighted_loss: 1.1117, label: 1, bag_size: 65\n",
      "batch 539, loss: 0.0013, instance_loss: 0.1275, weighted_loss: 0.0392, label: 0, bag_size: 41\n",
      "batch 559, loss: 1.5848, instance_loss: 1.3565, weighted_loss: 1.5163, label: 0, bag_size: 27\n",
      "batch 579, loss: 0.0259, instance_loss: 0.0608, weighted_loss: 0.0363, label: 1, bag_size: 94\n",
      "batch 599, loss: 2.8157, instance_loss: 0.7181, weighted_loss: 2.1864, label: 1, bag_size: 29\n",
      "batch 619, loss: 0.0003, instance_loss: 0.0919, weighted_loss: 0.0278, label: 0, bag_size: 26\n",
      "batch 639, loss: 0.0009, instance_loss: 0.1562, weighted_loss: 0.0475, label: 0, bag_size: 93\n",
      "batch 659, loss: 0.6774, instance_loss: 0.3534, weighted_loss: 0.5802, label: 1, bag_size: 73\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.947209653092006: correct 10048/10608\n",
      "class 1 clustering acc 0.698340874811463: correct 3704/5304\n",
      "Epoch: 5, train_loss: 0.5191, train_clustering_loss:  0.5403, train_error: 0.2051\n",
      "class 0: acc 0.7760252365930599, correct 246/317\n",
      "class 1: acc 0.8121387283236994, correct 281/346\n",
      "\n",
      "Val Set, val_loss: 0.5603, val_error: 0.1882, auc: 0.8811\n",
      "class 0 clustering acc 0.975735294117647: correct 1327/1360\n",
      "class 1 clustering acc 0.525: correct 357/680\n",
      "class 0: acc 0.8888888888888888, correct 40/45\n",
      "class 1: acc 0.725, correct 29/40\n",
      "Validation loss decreased (0.572297 --> 0.560301).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 1.6363, instance_loss: 0.6490, weighted_loss: 1.3401, label: 1, bag_size: 111\n",
      "batch 39, loss: 0.0026, instance_loss: 0.2193, weighted_loss: 0.0676, label: 0, bag_size: 49\n",
      "batch 59, loss: 1.4876, instance_loss: 0.1878, weighted_loss: 1.0977, label: 0, bag_size: 75\n",
      "batch 79, loss: 0.1027, instance_loss: 0.1305, weighted_loss: 0.1110, label: 0, bag_size: 51\n",
      "batch 99, loss: 0.0295, instance_loss: 0.1417, weighted_loss: 0.0632, label: 1, bag_size: 111\n",
      "batch 119, loss: 0.0012, instance_loss: 0.0400, weighted_loss: 0.0129, label: 1, bag_size: 31\n",
      "batch 139, loss: 5.5766, instance_loss: 3.3227, weighted_loss: 4.9004, label: 0, bag_size: 95\n",
      "batch 159, loss: 0.0750, instance_loss: 0.2747, weighted_loss: 0.1349, label: 0, bag_size: 63\n",
      "batch 179, loss: 0.0771, instance_loss: 0.1484, weighted_loss: 0.0985, label: 1, bag_size: 84\n",
      "batch 199, loss: 0.3132, instance_loss: 0.1571, weighted_loss: 0.2664, label: 1, bag_size: 82\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0056, weighted_loss: 0.0017, label: 0, bag_size: 88\n",
      "batch 239, loss: 0.0024, instance_loss: 0.2770, weighted_loss: 0.0848, label: 1, bag_size: 74\n",
      "batch 259, loss: 3.7658, instance_loss: 1.9308, weighted_loss: 3.2153, label: 0, bag_size: 88\n",
      "batch 279, loss: 0.0183, instance_loss: 1.6823, weighted_loss: 0.5175, label: 0, bag_size: 116\n",
      "batch 299, loss: 0.5265, instance_loss: 0.4860, weighted_loss: 0.5144, label: 1, bag_size: 53\n",
      "batch 319, loss: 0.0084, instance_loss: 0.1427, weighted_loss: 0.0487, label: 1, bag_size: 85\n",
      "batch 339, loss: 0.0306, instance_loss: 0.1986, weighted_loss: 0.0810, label: 0, bag_size: 43\n",
      "batch 359, loss: 0.0528, instance_loss: 0.0545, weighted_loss: 0.0533, label: 1, bag_size: 36\n",
      "batch 379, loss: 1.6610, instance_loss: 0.9741, weighted_loss: 1.4549, label: 1, bag_size: 22\n",
      "batch 399, loss: 0.0011, instance_loss: 0.0514, weighted_loss: 0.0162, label: 0, bag_size: 91\n",
      "batch 419, loss: 0.0324, instance_loss: 0.7893, weighted_loss: 0.2594, label: 0, bag_size: 51\n",
      "batch 439, loss: 2.4607, instance_loss: 0.3999, weighted_loss: 1.8425, label: 1, bag_size: 14\n",
      "batch 459, loss: 0.0644, instance_loss: 0.3570, weighted_loss: 0.1522, label: 0, bag_size: 35\n",
      "batch 479, loss: 0.0704, instance_loss: 0.0120, weighted_loss: 0.0529, label: 1, bag_size: 109\n",
      "batch 499, loss: 0.0002, instance_loss: 0.0034, weighted_loss: 0.0011, label: 0, bag_size: 57\n",
      "batch 519, loss: 0.0169, instance_loss: 0.0093, weighted_loss: 0.0146, label: 0, bag_size: 80\n",
      "batch 539, loss: 0.0027, instance_loss: 0.2041, weighted_loss: 0.0631, label: 1, bag_size: 63\n",
      "batch 559, loss: 0.4097, instance_loss: 0.2427, weighted_loss: 0.3596, label: 1, bag_size: 53\n",
      "batch 579, loss: 0.0004, instance_loss: 0.0221, weighted_loss: 0.0069, label: 1, bag_size: 69\n",
      "batch 599, loss: 0.0136, instance_loss: 0.0335, weighted_loss: 0.0196, label: 0, bag_size: 110\n",
      "batch 619, loss: 0.0007, instance_loss: 0.0181, weighted_loss: 0.0059, label: 0, bag_size: 64\n",
      "batch 639, loss: 0.1438, instance_loss: 0.0428, weighted_loss: 0.1135, label: 1, bag_size: 62\n",
      "batch 659, loss: 1.5610, instance_loss: 0.9571, weighted_loss: 1.3798, label: 1, bag_size: 70\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.944947209653092: correct 10024/10608\n",
      "class 1 clustering acc 0.7285067873303167: correct 3864/5304\n",
      "Epoch: 6, train_loss: 0.4813, train_clustering_loss:  0.4906, train_error: 0.1719\n",
      "class 0: acc 0.817629179331307, correct 269/329\n",
      "class 1: acc 0.8383233532934131, correct 280/334\n",
      "\n",
      "Val Set, val_loss: 0.6936, val_error: 0.2941, auc: 0.8967\n",
      "class 0 clustering acc 0.9066176470588235: correct 1233/1360\n",
      "class 1 clustering acc 0.6294117647058823: correct 428/680\n",
      "class 0: acc 0.5111111111111111, correct 23/45\n",
      "class 1: acc 0.925, correct 37/40\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.0491, weighted_loss: 0.0148, label: 0, bag_size: 38\n",
      "batch 39, loss: 3.1592, instance_loss: 0.4453, weighted_loss: 2.3450, label: 1, bag_size: 99\n",
      "batch 59, loss: 0.0745, instance_loss: 1.1264, weighted_loss: 0.3901, label: 0, bag_size: 49\n",
      "batch 79, loss: 3.2745, instance_loss: 0.8116, weighted_loss: 2.5356, label: 0, bag_size: 35\n",
      "batch 99, loss: 0.0030, instance_loss: 1.0602, weighted_loss: 0.3202, label: 0, bag_size: 78\n",
      "batch 119, loss: 0.0361, instance_loss: 0.1292, weighted_loss: 0.0641, label: 1, bag_size: 68\n",
      "batch 139, loss: 0.0001, instance_loss: 0.3106, weighted_loss: 0.0932, label: 1, bag_size: 111\n",
      "batch 159, loss: 6.6346, instance_loss: 1.7938, weighted_loss: 5.1824, label: 0, bag_size: 110\n",
      "batch 179, loss: 0.0923, instance_loss: 1.0027, weighted_loss: 0.3654, label: 1, bag_size: 70\n",
      "batch 199, loss: 0.9257, instance_loss: 0.7899, weighted_loss: 0.8849, label: 0, bag_size: 76\n",
      "batch 219, loss: 0.0104, instance_loss: 1.0799, weighted_loss: 0.3312, label: 0, bag_size: 90\n",
      "batch 239, loss: 0.0000, instance_loss: 0.5928, weighted_loss: 0.1778, label: 0, bag_size: 79\n",
      "batch 259, loss: 0.0000, instance_loss: 0.8345, weighted_loss: 0.2504, label: 0, bag_size: 36\n",
      "batch 279, loss: 0.0005, instance_loss: 0.9134, weighted_loss: 0.2743, label: 1, bag_size: 121\n",
      "batch 299, loss: 0.0455, instance_loss: 0.4955, weighted_loss: 0.1805, label: 1, bag_size: 29\n",
      "batch 319, loss: 1.0778, instance_loss: 0.5442, weighted_loss: 0.9177, label: 0, bag_size: 25\n",
      "batch 339, loss: 1.0495, instance_loss: 1.7351, weighted_loss: 1.2552, label: 0, bag_size: 28\n",
      "batch 359, loss: 0.0000, instance_loss: 0.5459, weighted_loss: 0.1638, label: 1, bag_size: 98\n",
      "batch 379, loss: 0.0003, instance_loss: 0.7127, weighted_loss: 0.2140, label: 1, bag_size: 85\n",
      "batch 399, loss: 0.0141, instance_loss: 0.6491, weighted_loss: 0.2046, label: 1, bag_size: 47\n",
      "batch 419, loss: 0.7575, instance_loss: 0.7533, weighted_loss: 0.7562, label: 0, bag_size: 28\n",
      "batch 439, loss: 0.0948, instance_loss: 0.6838, weighted_loss: 0.2715, label: 1, bag_size: 69\n",
      "batch 459, loss: 0.0084, instance_loss: 0.3110, weighted_loss: 0.0992, label: 1, bag_size: 52\n",
      "batch 479, loss: 0.0000, instance_loss: 0.5148, weighted_loss: 0.1544, label: 1, bag_size: 32\n",
      "batch 499, loss: 0.0012, instance_loss: 0.2461, weighted_loss: 0.0747, label: 1, bag_size: 85\n",
      "batch 519, loss: 0.0007, instance_loss: 0.2017, weighted_loss: 0.0610, label: 0, bag_size: 24\n",
      "batch 539, loss: 0.0000, instance_loss: 0.1325, weighted_loss: 0.0397, label: 1, bag_size: 73\n",
      "batch 559, loss: 0.0000, instance_loss: 0.1030, weighted_loss: 0.0309, label: 1, bag_size: 61\n",
      "batch 579, loss: 0.0000, instance_loss: 0.2665, weighted_loss: 0.0800, label: 0, bag_size: 85\n",
      "batch 599, loss: 0.0000, instance_loss: 0.5955, weighted_loss: 0.1787, label: 0, bag_size: 78\n",
      "batch 619, loss: 0.0010, instance_loss: 0.4558, weighted_loss: 0.1374, label: 1, bag_size: 49\n",
      "batch 639, loss: 0.0000, instance_loss: 0.5292, weighted_loss: 0.1588, label: 1, bag_size: 140\n",
      "batch 659, loss: 9.2673, instance_loss: 0.8569, weighted_loss: 6.7442, label: 0, bag_size: 55\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9112933634992458: correct 9667/10608\n",
      "class 1 clustering acc 0.4928355957767723: correct 2614/5304\n",
      "Epoch: 7, train_loss: 0.7382, train_clustering_loss:  0.7063, train_error: 0.2172\n",
      "class 0: acc 0.7794561933534743, correct 258/331\n",
      "class 1: acc 0.786144578313253, correct 261/332\n",
      "\n",
      "Val Set, val_loss: 0.4824, val_error: 0.1412, auc: 0.9183\n",
      "class 0 clustering acc 0.9397058823529412: correct 1278/1360\n",
      "class 1 clustering acc 0.40588235294117647: correct 276/680\n",
      "class 0: acc 0.8, correct 36/45\n",
      "class 1: acc 0.925, correct 37/40\n",
      "Validation loss decreased (0.560301 --> 0.482390).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0006, instance_loss: 0.4781, weighted_loss: 0.1439, label: 1, bag_size: 87\n",
      "batch 39, loss: 4.0756, instance_loss: 1.9445, weighted_loss: 3.4363, label: 1, bag_size: 28\n",
      "batch 59, loss: 0.0545, instance_loss: 0.9445, weighted_loss: 0.3215, label: 1, bag_size: 32\n",
      "batch 79, loss: 0.0040, instance_loss: 0.7778, weighted_loss: 0.2361, label: 0, bag_size: 25\n",
      "batch 99, loss: 0.2145, instance_loss: 0.9402, weighted_loss: 0.4322, label: 0, bag_size: 54\n",
      "batch 119, loss: 0.0343, instance_loss: 0.9100, weighted_loss: 0.2970, label: 1, bag_size: 92\n",
      "batch 139, loss: 0.0000, instance_loss: 0.6646, weighted_loss: 0.1994, label: 1, bag_size: 95\n",
      "batch 159, loss: 0.0004, instance_loss: 0.3431, weighted_loss: 0.1032, label: 1, bag_size: 26\n",
      "batch 179, loss: 0.0003, instance_loss: 0.5914, weighted_loss: 0.1776, label: 0, bag_size: 21\n",
      "batch 199, loss: 1.0359, instance_loss: 0.6886, weighted_loss: 0.9317, label: 0, bag_size: 50\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0528, weighted_loss: 0.0158, label: 1, bag_size: 123\n",
      "batch 239, loss: 1.5654, instance_loss: 1.5232, weighted_loss: 1.5527, label: 1, bag_size: 23\n",
      "batch 259, loss: 0.0005, instance_loss: 0.0524, weighted_loss: 0.0160, label: 1, bag_size: 70\n",
      "batch 279, loss: 0.6478, instance_loss: 0.7293, weighted_loss: 0.6722, label: 1, bag_size: 111\n",
      "batch 299, loss: 0.3382, instance_loss: 0.4894, weighted_loss: 0.3836, label: 1, bag_size: 114\n",
      "batch 319, loss: 2.7929, instance_loss: 2.1878, weighted_loss: 2.6114, label: 0, bag_size: 35\n",
      "batch 339, loss: 0.0082, instance_loss: 0.1113, weighted_loss: 0.0392, label: 1, bag_size: 84\n",
      "batch 359, loss: 0.1498, instance_loss: 0.4092, weighted_loss: 0.2276, label: 0, bag_size: 78\n",
      "batch 379, loss: 0.0506, instance_loss: 0.5065, weighted_loss: 0.1874, label: 1, bag_size: 78\n",
      "batch 399, loss: 0.0026, instance_loss: 0.1636, weighted_loss: 0.0509, label: 0, bag_size: 66\n",
      "batch 419, loss: 0.4626, instance_loss: 0.2098, weighted_loss: 0.3868, label: 1, bag_size: 82\n",
      "batch 439, loss: 0.0029, instance_loss: 0.1342, weighted_loss: 0.0423, label: 1, bag_size: 114\n",
      "batch 459, loss: 2.6189, instance_loss: 0.9292, weighted_loss: 2.1120, label: 1, bag_size: 98\n",
      "batch 479, loss: 0.0000, instance_loss: 0.2732, weighted_loss: 0.0820, label: 0, bag_size: 49\n",
      "batch 499, loss: 1.5468, instance_loss: 1.4044, weighted_loss: 1.5041, label: 0, bag_size: 24\n",
      "batch 519, loss: 2.5830, instance_loss: 1.1220, weighted_loss: 2.1447, label: 0, bag_size: 43\n",
      "batch 539, loss: 0.0019, instance_loss: 0.7568, weighted_loss: 0.2283, label: 1, bag_size: 68\n",
      "batch 559, loss: 0.0060, instance_loss: 0.5719, weighted_loss: 0.1758, label: 1, bag_size: 14\n",
      "batch 579, loss: 0.0004, instance_loss: 0.2748, weighted_loss: 0.0827, label: 0, bag_size: 74\n",
      "batch 599, loss: 0.1506, instance_loss: 0.7092, weighted_loss: 0.3182, label: 0, bag_size: 88\n",
      "batch 619, loss: 0.0072, instance_loss: 0.7896, weighted_loss: 0.2419, label: 0, bag_size: 86\n",
      "batch 639, loss: 0.0782, instance_loss: 0.5301, weighted_loss: 0.2137, label: 1, bag_size: 37\n",
      "batch 659, loss: 0.0001, instance_loss: 0.2822, weighted_loss: 0.0847, label: 1, bag_size: 89\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9154411764705882: correct 9711/10608\n",
      "class 1 clustering acc 0.505656108597285: correct 2682/5304\n",
      "Epoch: 8, train_loss: 0.5738, train_clustering_loss:  0.7269, train_error: 0.1795\n",
      "class 0: acc 0.8208955223880597, correct 275/335\n",
      "class 1: acc 0.8201219512195121, correct 269/328\n",
      "\n",
      "Val Set, val_loss: 0.6066, val_error: 0.2118, auc: 0.8983\n",
      "class 0 clustering acc 0.8794117647058823: correct 1196/1360\n",
      "class 1 clustering acc 0.6691176470588235: correct 455/680\n",
      "class 0: acc 0.6888888888888889, correct 31/45\n",
      "class 1: acc 0.9, correct 36/40\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0795, instance_loss: 0.1374, weighted_loss: 0.0969, label: 1, bag_size: 35\n",
      "batch 39, loss: 0.0019, instance_loss: 0.1835, weighted_loss: 0.0564, label: 1, bag_size: 61\n",
      "batch 59, loss: 0.1629, instance_loss: 0.5408, weighted_loss: 0.2763, label: 0, bag_size: 68\n",
      "batch 79, loss: 0.0374, instance_loss: 0.1470, weighted_loss: 0.0703, label: 0, bag_size: 82\n",
      "batch 99, loss: 0.0018, instance_loss: 0.0781, weighted_loss: 0.0247, label: 0, bag_size: 97\n",
      "batch 119, loss: 0.0522, instance_loss: 1.7281, weighted_loss: 0.5550, label: 1, bag_size: 96\n",
      "batch 139, loss: 0.7902, instance_loss: 1.5554, weighted_loss: 1.0198, label: 0, bag_size: 50\n",
      "batch 159, loss: 0.1521, instance_loss: 0.4711, weighted_loss: 0.2478, label: 1, bag_size: 31\n",
      "batch 179, loss: 0.3448, instance_loss: 0.4576, weighted_loss: 0.3787, label: 1, bag_size: 30\n",
      "batch 199, loss: 0.0006, instance_loss: 0.1718, weighted_loss: 0.0520, label: 0, bag_size: 65\n",
      "batch 219, loss: 0.0372, instance_loss: 0.2114, weighted_loss: 0.0894, label: 1, bag_size: 94\n",
      "batch 239, loss: 0.0000, instance_loss: 0.1759, weighted_loss: 0.0528, label: 0, bag_size: 67\n",
      "batch 259, loss: 0.2412, instance_loss: 0.1839, weighted_loss: 0.2240, label: 0, bag_size: 57\n",
      "batch 279, loss: 0.0157, instance_loss: 0.4031, weighted_loss: 0.1319, label: 0, bag_size: 53\n",
      "batch 299, loss: 0.0304, instance_loss: 0.0312, weighted_loss: 0.0307, label: 1, bag_size: 53\n",
      "batch 319, loss: 0.0564, instance_loss: 0.0603, weighted_loss: 0.0576, label: 0, bag_size: 110\n",
      "batch 339, loss: 0.0099, instance_loss: 0.0533, weighted_loss: 0.0229, label: 0, bag_size: 106\n",
      "batch 359, loss: 0.0169, instance_loss: 0.2396, weighted_loss: 0.0837, label: 0, bag_size: 63\n",
      "batch 379, loss: 1.3001, instance_loss: 0.6355, weighted_loss: 1.1007, label: 1, bag_size: 25\n",
      "batch 399, loss: 1.5717, instance_loss: 3.5187, weighted_loss: 2.1558, label: 0, bag_size: 79\n",
      "batch 419, loss: 0.0373, instance_loss: 0.2101, weighted_loss: 0.0892, label: 1, bag_size: 71\n",
      "batch 439, loss: 1.2231, instance_loss: 1.7984, weighted_loss: 1.3957, label: 0, bag_size: 32\n",
      "batch 459, loss: 0.0897, instance_loss: 0.7475, weighted_loss: 0.2870, label: 1, bag_size: 35\n",
      "batch 479, loss: 0.0243, instance_loss: 0.3495, weighted_loss: 0.1218, label: 1, bag_size: 38\n",
      "batch 499, loss: 0.3965, instance_loss: 0.7243, weighted_loss: 0.4948, label: 0, bag_size: 96\n",
      "batch 519, loss: 0.0635, instance_loss: 0.4073, weighted_loss: 0.1666, label: 0, bag_size: 61\n",
      "batch 539, loss: 0.0013, instance_loss: 1.0268, weighted_loss: 0.3089, label: 0, bag_size: 25\n",
      "batch 559, loss: 0.4218, instance_loss: 0.6675, weighted_loss: 0.4955, label: 0, bag_size: 65\n",
      "batch 579, loss: 0.2952, instance_loss: 1.2031, weighted_loss: 0.5676, label: 1, bag_size: 22\n",
      "batch 599, loss: 0.1279, instance_loss: 0.1621, weighted_loss: 0.1382, label: 0, bag_size: 40\n",
      "batch 619, loss: 0.0290, instance_loss: 0.2193, weighted_loss: 0.0861, label: 1, bag_size: 36\n",
      "batch 639, loss: 0.2649, instance_loss: 1.7591, weighted_loss: 0.7132, label: 1, bag_size: 35\n",
      "batch 659, loss: 0.6129, instance_loss: 0.6290, weighted_loss: 0.6178, label: 0, bag_size: 41\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9407993966817496: correct 9980/10608\n",
      "class 1 clustering acc 0.6889140271493213: correct 3654/5304\n",
      "Epoch: 9, train_loss: 0.4818, train_clustering_loss:  0.5359, train_error: 0.1870\n",
      "class 0: acc 0.7951807228915663, correct 264/332\n",
      "class 1: acc 0.8308157099697885, correct 275/331\n",
      "\n",
      "Val Set, val_loss: 0.4454, val_error: 0.2235, auc: 0.8950\n",
      "class 0 clustering acc 0.8816176470588235: correct 1199/1360\n",
      "class 1 clustering acc 0.6617647058823529: correct 450/680\n",
      "class 0: acc 0.7111111111111111, correct 32/45\n",
      "class 1: acc 0.85, correct 34/40\n",
      "Validation loss decreased (0.482390 --> 0.445361).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0181, weighted_loss: 0.0056, label: 0, bag_size: 88\n",
      "batch 39, loss: 0.1771, instance_loss: 2.1121, weighted_loss: 0.7576, label: 0, bag_size: 78\n",
      "batch 59, loss: 0.3047, instance_loss: 1.1628, weighted_loss: 0.5622, label: 1, bag_size: 30\n",
      "batch 79, loss: 0.0006, instance_loss: 0.0155, weighted_loss: 0.0051, label: 1, bag_size: 76\n",
      "batch 99, loss: 0.0022, instance_loss: 0.0246, weighted_loss: 0.0089, label: 0, bag_size: 33\n",
      "batch 119, loss: 1.6515, instance_loss: 0.7805, weighted_loss: 1.3902, label: 1, bag_size: 96\n",
      "batch 139, loss: 0.0237, instance_loss: 0.1290, weighted_loss: 0.0553, label: 1, bag_size: 30\n",
      "batch 159, loss: 0.0049, instance_loss: 0.0090, weighted_loss: 0.0061, label: 0, bag_size: 88\n",
      "batch 179, loss: 1.4154, instance_loss: 0.7088, weighted_loss: 1.2034, label: 1, bag_size: 109\n",
      "batch 199, loss: 0.7674, instance_loss: 0.5865, weighted_loss: 0.7132, label: 0, bag_size: 106\n",
      "batch 219, loss: 1.7564, instance_loss: 1.5477, weighted_loss: 1.6938, label: 1, bag_size: 32\n",
      "batch 239, loss: 0.0226, instance_loss: 0.0713, weighted_loss: 0.0372, label: 0, bag_size: 20\n",
      "batch 259, loss: 0.0380, instance_loss: 0.1268, weighted_loss: 0.0647, label: 1, bag_size: 48\n",
      "batch 279, loss: 5.7462, instance_loss: 4.6132, weighted_loss: 5.4063, label: 1, bag_size: 38\n",
      "batch 299, loss: 0.0118, instance_loss: 0.1110, weighted_loss: 0.0416, label: 1, bag_size: 65\n",
      "batch 319, loss: 0.0443, instance_loss: 1.3622, weighted_loss: 0.4397, label: 0, bag_size: 31\n",
      "batch 339, loss: 0.0004, instance_loss: 0.0401, weighted_loss: 0.0123, label: 0, bag_size: 51\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0033, weighted_loss: 0.0010, label: 0, bag_size: 53\n",
      "batch 379, loss: 0.0109, instance_loss: 0.0604, weighted_loss: 0.0258, label: 1, bag_size: 60\n",
      "batch 399, loss: 0.0011, instance_loss: 0.0390, weighted_loss: 0.0124, label: 1, bag_size: 88\n",
      "batch 419, loss: 0.0012, instance_loss: 0.0086, weighted_loss: 0.0034, label: 0, bag_size: 50\n",
      "batch 439, loss: 0.8193, instance_loss: 2.3388, weighted_loss: 1.2751, label: 0, bag_size: 54\n",
      "batch 459, loss: 0.0141, instance_loss: 0.4602, weighted_loss: 0.1479, label: 0, bag_size: 25\n",
      "batch 479, loss: 0.0134, instance_loss: 0.1863, weighted_loss: 0.0652, label: 1, bag_size: 30\n",
      "batch 499, loss: 0.0207, instance_loss: 0.4721, weighted_loss: 0.1561, label: 0, bag_size: 99\n",
      "batch 519, loss: 0.0001, instance_loss: 0.1811, weighted_loss: 0.0544, label: 0, bag_size: 83\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0138, weighted_loss: 0.0042, label: 0, bag_size: 59\n",
      "batch 559, loss: 0.5768, instance_loss: 0.3672, weighted_loss: 0.5139, label: 1, bag_size: 99\n",
      "batch 579, loss: 0.0064, instance_loss: 0.0292, weighted_loss: 0.0132, label: 1, bag_size: 48\n",
      "batch 599, loss: 6.1238, instance_loss: 3.9095, weighted_loss: 5.4595, label: 1, bag_size: 42\n",
      "batch 619, loss: 0.0393, instance_loss: 0.0752, weighted_loss: 0.0501, label: 0, bag_size: 91\n",
      "batch 639, loss: 0.0279, instance_loss: 0.0343, weighted_loss: 0.0298, label: 0, bag_size: 40\n",
      "batch 659, loss: 3.8750, instance_loss: 0.5151, weighted_loss: 2.8670, label: 0, bag_size: 58\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9435331825037707: correct 10009/10608\n",
      "class 1 clustering acc 0.7232277526395173: correct 3836/5304\n",
      "Epoch: 10, train_loss: 0.4637, train_clustering_loss:  0.4806, train_error: 0.1825\n",
      "class 0: acc 0.8125, correct 273/336\n",
      "class 1: acc 0.8226299694189603, correct 269/327\n",
      "\n",
      "Val Set, val_loss: 0.6707, val_error: 0.1765, auc: 0.8950\n",
      "class 0 clustering acc 0.9419117647058823: correct 1281/1360\n",
      "class 1 clustering acc 0.7308823529411764: correct 497/680\n",
      "class 0: acc 0.9333333333333333, correct 42/45\n",
      "class 1: acc 0.7, correct 28/40\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0116, weighted_loss: 0.0035, label: 0, bag_size: 91\n",
      "batch 39, loss: 0.3938, instance_loss: 0.3009, weighted_loss: 0.3659, label: 0, bag_size: 45\n",
      "batch 59, loss: 0.1969, instance_loss: 0.2195, weighted_loss: 0.2036, label: 1, bag_size: 119\n",
      "batch 79, loss: 1.3647, instance_loss: 2.1324, weighted_loss: 1.5950, label: 0, bag_size: 29\n",
      "batch 99, loss: 2.3401, instance_loss: 1.3346, weighted_loss: 2.0385, label: 1, bag_size: 52\n",
      "batch 119, loss: 0.2966, instance_loss: 0.0669, weighted_loss: 0.2277, label: 1, bag_size: 39\n",
      "batch 139, loss: 0.1749, instance_loss: 0.3561, weighted_loss: 0.2293, label: 1, bag_size: 82\n",
      "batch 159, loss: 0.0073, instance_loss: 0.0077, weighted_loss: 0.0074, label: 0, bag_size: 37\n",
      "batch 179, loss: 0.4972, instance_loss: 0.3133, weighted_loss: 0.4420, label: 1, bag_size: 67\n",
      "batch 199, loss: 0.0039, instance_loss: 0.0425, weighted_loss: 0.0155, label: 1, bag_size: 52\n",
      "batch 219, loss: 0.0024, instance_loss: 0.0997, weighted_loss: 0.0316, label: 0, bag_size: 12\n",
      "batch 239, loss: 0.0129, instance_loss: 0.0184, weighted_loss: 0.0145, label: 0, bag_size: 88\n",
      "batch 259, loss: 1.2151, instance_loss: 0.1133, weighted_loss: 0.8845, label: 1, bag_size: 85\n",
      "batch 279, loss: 1.4866, instance_loss: 0.3363, weighted_loss: 1.1415, label: 0, bag_size: 106\n",
      "batch 299, loss: 0.1154, instance_loss: 0.2213, weighted_loss: 0.1472, label: 1, bag_size: 56\n",
      "batch 319, loss: 0.0861, instance_loss: 0.0573, weighted_loss: 0.0775, label: 1, bag_size: 69\n",
      "batch 339, loss: 0.0049, instance_loss: 0.0342, weighted_loss: 0.0137, label: 1, bag_size: 21\n",
      "batch 359, loss: 0.5440, instance_loss: 1.1011, weighted_loss: 0.7111, label: 1, bag_size: 19\n",
      "batch 379, loss: 0.0013, instance_loss: 0.0298, weighted_loss: 0.0099, label: 0, bag_size: 66\n",
      "batch 399, loss: 0.0002, instance_loss: 0.0512, weighted_loss: 0.0155, label: 0, bag_size: 114\n",
      "batch 419, loss: 0.1249, instance_loss: 0.7646, weighted_loss: 0.3168, label: 1, bag_size: 84\n",
      "batch 439, loss: 0.3825, instance_loss: 0.4566, weighted_loss: 0.4047, label: 1, bag_size: 35\n",
      "batch 459, loss: 0.0008, instance_loss: 0.0155, weighted_loss: 0.0052, label: 1, bag_size: 70\n",
      "batch 479, loss: 0.0413, instance_loss: 0.3749, weighted_loss: 0.1414, label: 1, bag_size: 53\n",
      "batch 499, loss: 1.4718, instance_loss: 1.1508, weighted_loss: 1.3755, label: 0, bag_size: 49\n",
      "batch 519, loss: 3.0759, instance_loss: 1.8430, weighted_loss: 2.7061, label: 0, bag_size: 73\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0019, weighted_loss: 0.0006, label: 0, bag_size: 103\n",
      "batch 559, loss: 0.0146, instance_loss: 0.0346, weighted_loss: 0.0206, label: 0, bag_size: 107\n",
      "batch 579, loss: 0.0003, instance_loss: 0.2652, weighted_loss: 0.0798, label: 0, bag_size: 40\n",
      "batch 599, loss: 1.6944, instance_loss: 2.4252, weighted_loss: 1.9137, label: 0, bag_size: 75\n",
      "batch 619, loss: 0.0606, instance_loss: 0.0137, weighted_loss: 0.0465, label: 1, bag_size: 77\n",
      "batch 639, loss: 0.0010, instance_loss: 0.0167, weighted_loss: 0.0057, label: 1, bag_size: 72\n",
      "batch 659, loss: 0.0128, instance_loss: 0.3729, weighted_loss: 0.1208, label: 1, bag_size: 72\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9583333333333334: correct 10166/10608\n",
      "class 1 clustering acc 0.798265460030166: correct 4234/5304\n",
      "Epoch: 11, train_loss: 0.3656, train_clustering_loss:  0.3753, train_error: 0.1463\n",
      "class 0: acc 0.8516320474777448, correct 287/337\n",
      "class 1: acc 0.8558282208588958, correct 279/326\n",
      "\n",
      "Val Set, val_loss: 0.8051, val_error: 0.2588, auc: 0.8833\n",
      "class 0 clustering acc 0.9522058823529411: correct 1295/1360\n",
      "class 1 clustering acc 0.7102941176470589: correct 483/680\n",
      "class 0: acc 0.9111111111111111, correct 41/45\n",
      "class 1: acc 0.55, correct 22/40\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0074, weighted_loss: 0.0022, label: 0, bag_size: 51\n",
      "batch 39, loss: 0.0147, instance_loss: 0.0791, weighted_loss: 0.0340, label: 1, bag_size: 68\n",
      "batch 59, loss: 1.4470, instance_loss: 1.3673, weighted_loss: 1.4231, label: 1, bag_size: 59\n",
      "batch 79, loss: 0.0054, instance_loss: 0.0181, weighted_loss: 0.0092, label: 1, bag_size: 91\n",
      "batch 99, loss: 1.0603, instance_loss: 1.0776, weighted_loss: 1.0655, label: 1, bag_size: 93\n",
      "batch 119, loss: 0.1115, instance_loss: 0.0797, weighted_loss: 0.1019, label: 0, bag_size: 67\n",
      "batch 139, loss: 1.7683, instance_loss: 0.7542, weighted_loss: 1.4640, label: 0, bag_size: 36\n",
      "batch 159, loss: 0.0002, instance_loss: 0.0119, weighted_loss: 0.0037, label: 0, bag_size: 81\n",
      "batch 179, loss: 0.0013, instance_loss: 0.0324, weighted_loss: 0.0106, label: 0, bag_size: 61\n",
      "batch 199, loss: 0.0003, instance_loss: 0.0632, weighted_loss: 0.0192, label: 1, bag_size: 100\n",
      "batch 219, loss: 2.0991, instance_loss: 1.4650, weighted_loss: 1.9089, label: 1, bag_size: 20\n",
      "batch 239, loss: 0.0267, instance_loss: 0.9931, weighted_loss: 0.3166, label: 0, bag_size: 31\n",
      "batch 259, loss: 0.0001, instance_loss: 0.6246, weighted_loss: 0.1875, label: 1, bag_size: 32\n",
      "batch 279, loss: 0.0005, instance_loss: 0.4918, weighted_loss: 0.1479, label: 0, bag_size: 31\n",
      "batch 299, loss: 3.6468, instance_loss: 1.6380, weighted_loss: 3.0442, label: 0, bag_size: 54\n",
      "batch 319, loss: 0.2846, instance_loss: 1.5573, weighted_loss: 0.6664, label: 1, bag_size: 94\n",
      "batch 339, loss: 0.2859, instance_loss: 0.7045, weighted_loss: 0.4115, label: 1, bag_size: 71\n",
      "batch 359, loss: 0.0008, instance_loss: 0.7282, weighted_loss: 0.2190, label: 1, bag_size: 94\n",
      "batch 379, loss: 0.0003, instance_loss: 0.6144, weighted_loss: 0.1845, label: 1, bag_size: 81\n",
      "batch 399, loss: 0.0013, instance_loss: 0.8691, weighted_loss: 0.2617, label: 0, bag_size: 31\n",
      "batch 419, loss: 0.0000, instance_loss: 0.5415, weighted_loss: 0.1624, label: 0, bag_size: 88\n",
      "batch 439, loss: 0.0000, instance_loss: 0.5904, weighted_loss: 0.1771, label: 0, bag_size: 93\n",
      "batch 459, loss: 10.2822, instance_loss: 1.5941, weighted_loss: 7.6758, label: 0, bag_size: 57\n",
      "batch 479, loss: 0.0004, instance_loss: 0.6745, weighted_loss: 0.2026, label: 0, bag_size: 50\n",
      "batch 499, loss: 0.0002, instance_loss: 0.6632, weighted_loss: 0.1991, label: 1, bag_size: 75\n",
      "batch 519, loss: 0.0000, instance_loss: 0.6568, weighted_loss: 0.1971, label: 1, bag_size: 39\n",
      "batch 539, loss: 0.0000, instance_loss: 0.5723, weighted_loss: 0.1717, label: 1, bag_size: 119\n",
      "batch 559, loss: 0.0000, instance_loss: 1.0533, weighted_loss: 0.3160, label: 1, bag_size: 107\n",
      "batch 579, loss: 0.0000, instance_loss: 0.6216, weighted_loss: 0.1865, label: 0, bag_size: 110\n",
      "batch 599, loss: 5.6734, instance_loss: 1.5034, weighted_loss: 4.4224, label: 0, bag_size: 60\n",
      "batch 619, loss: 5.3714, instance_loss: 1.7658, weighted_loss: 4.2897, label: 1, bag_size: 59\n",
      "batch 639, loss: 0.0379, instance_loss: 0.7389, weighted_loss: 0.2482, label: 1, bag_size: 86\n",
      "batch 659, loss: 0.0248, instance_loss: 0.9939, weighted_loss: 0.3155, label: 0, bag_size: 26\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9078996983408748: correct 9631/10608\n",
      "class 1 clustering acc 0.47492458521870284: correct 2519/5304\n",
      "Epoch: 12, train_loss: 0.8491, train_clustering_loss:  0.7416, train_error: 0.1961\n",
      "class 0: acc 0.8137535816618912, correct 284/349\n",
      "class 1: acc 0.7929936305732485, correct 249/314\n",
      "\n",
      "Val Set, val_loss: 0.4469, val_error: 0.1647, auc: 0.9111\n",
      "class 0 clustering acc 0.9595588235294118: correct 1305/1360\n",
      "class 1 clustering acc 0.33676470588235297: correct 229/680\n",
      "class 0: acc 0.7777777777777778, correct 35/45\n",
      "class 1: acc 0.9, correct 36/40\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0014, instance_loss: 0.5318, weighted_loss: 0.1605, label: 1, bag_size: 52\n",
      "batch 39, loss: 0.0000, instance_loss: 0.5599, weighted_loss: 0.1680, label: 1, bag_size: 74\n",
      "batch 59, loss: 0.0300, instance_loss: 0.5559, weighted_loss: 0.1878, label: 1, bag_size: 73\n",
      "batch 79, loss: 0.1559, instance_loss: 0.9200, weighted_loss: 0.3851, label: 1, bag_size: 60\n",
      "batch 99, loss: 0.0000, instance_loss: 0.6936, weighted_loss: 0.2081, label: 1, bag_size: 20\n",
      "batch 119, loss: 0.1098, instance_loss: 0.6565, weighted_loss: 0.2738, label: 0, bag_size: 94\n",
      "batch 139, loss: 0.0569, instance_loss: 0.8404, weighted_loss: 0.2919, label: 0, bag_size: 31\n",
      "batch 159, loss: 1.0367, instance_loss: 0.5446, weighted_loss: 0.8890, label: 1, bag_size: 73\n",
      "batch 179, loss: 0.0079, instance_loss: 0.4489, weighted_loss: 0.1402, label: 0, bag_size: 66\n",
      "batch 199, loss: 0.0448, instance_loss: 0.9047, weighted_loss: 0.3028, label: 1, bag_size: 108\n",
      "batch 219, loss: 0.0014, instance_loss: 0.5256, weighted_loss: 0.1587, label: 0, bag_size: 26\n",
      "batch 239, loss: 0.0010, instance_loss: 0.6241, weighted_loss: 0.1879, label: 0, bag_size: 30\n",
      "batch 259, loss: 0.1478, instance_loss: 0.6661, weighted_loss: 0.3033, label: 0, bag_size: 33\n",
      "batch 279, loss: 0.0061, instance_loss: 0.4517, weighted_loss: 0.1398, label: 0, bag_size: 101\n",
      "batch 299, loss: 0.0000, instance_loss: 0.4653, weighted_loss: 0.1396, label: 1, bag_size: 39\n",
      "batch 319, loss: 0.0015, instance_loss: 0.4976, weighted_loss: 0.1504, label: 0, bag_size: 30\n",
      "batch 339, loss: 0.0000, instance_loss: 0.4901, weighted_loss: 0.1470, label: 0, bag_size: 67\n",
      "batch 359, loss: 3.3220, instance_loss: 1.6308, weighted_loss: 2.8147, label: 1, bag_size: 63\n",
      "batch 379, loss: 0.0050, instance_loss: 0.6024, weighted_loss: 0.1842, label: 0, bag_size: 35\n",
      "batch 399, loss: 0.0004, instance_loss: 0.1581, weighted_loss: 0.0477, label: 0, bag_size: 53\n",
      "batch 419, loss: 0.0311, instance_loss: 0.5669, weighted_loss: 0.1918, label: 0, bag_size: 35\n",
      "batch 439, loss: 0.0086, instance_loss: 0.5420, weighted_loss: 0.1686, label: 1, bag_size: 30\n",
      "batch 459, loss: 0.0000, instance_loss: 0.5683, weighted_loss: 0.1705, label: 1, bag_size: 58\n",
      "batch 479, loss: 0.0111, instance_loss: 1.0491, weighted_loss: 0.3225, label: 1, bag_size: 67\n",
      "batch 499, loss: 0.0019, instance_loss: 1.2346, weighted_loss: 0.3717, label: 0, bag_size: 12\n",
      "batch 519, loss: 0.0034, instance_loss: 0.6501, weighted_loss: 0.1974, label: 0, bag_size: 87\n",
      "batch 539, loss: 0.0000, instance_loss: 0.3689, weighted_loss: 0.1107, label: 1, bag_size: 73\n",
      "batch 559, loss: 0.0101, instance_loss: 0.6369, weighted_loss: 0.1981, label: 1, bag_size: 45\n",
      "batch 579, loss: 0.0000, instance_loss: 0.2360, weighted_loss: 0.0708, label: 1, bag_size: 83\n",
      "batch 599, loss: 0.0000, instance_loss: 0.1723, weighted_loss: 0.0517, label: 1, bag_size: 126\n",
      "batch 619, loss: 0.0001, instance_loss: 0.5867, weighted_loss: 0.1761, label: 0, bag_size: 114\n",
      "batch 639, loss: 0.0907, instance_loss: 0.8025, weighted_loss: 0.3042, label: 0, bag_size: 66\n",
      "batch 659, loss: 0.0002, instance_loss: 0.1054, weighted_loss: 0.0318, label: 0, bag_size: 94\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9239253393665159: correct 9801/10608\n",
      "class 1 clustering acc 0.48981900452488686: correct 2598/5304\n",
      "Epoch: 13, train_loss: 0.4953, train_clustering_loss:  0.6858, train_error: 0.1297\n",
      "class 0: acc 0.8581081081081081, correct 254/296\n",
      "class 1: acc 0.8801089918256131, correct 323/367\n",
      "\n",
      "Val Set, val_loss: 0.6018, val_error: 0.2471, auc: 0.8833\n",
      "class 0 clustering acc 0.7816176470588235: correct 1063/1360\n",
      "class 1 clustering acc 0.6647058823529411: correct 452/680\n",
      "class 0: acc 0.8888888888888888, correct 40/45\n",
      "class 1: acc 0.6, correct 24/40\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.1266, weighted_loss: 0.0380, label: 1, bag_size: 26\n",
      "batch 39, loss: 0.0002, instance_loss: 0.5166, weighted_loss: 0.1551, label: 1, bag_size: 31\n",
      "batch 59, loss: 2.5964, instance_loss: 0.2448, weighted_loss: 1.8910, label: 0, bag_size: 75\n",
      "batch 79, loss: 0.3643, instance_loss: 0.1863, weighted_loss: 0.3109, label: 1, bag_size: 108\n",
      "batch 99, loss: 0.0614, instance_loss: 1.8196, weighted_loss: 0.5889, label: 1, bag_size: 20\n",
      "batch 119, loss: 0.0000, instance_loss: 0.3524, weighted_loss: 0.1057, label: 0, bag_size: 77\n",
      "batch 139, loss: 0.0041, instance_loss: 0.5840, weighted_loss: 0.1781, label: 0, bag_size: 29\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0294, weighted_loss: 0.0089, label: 1, bag_size: 100\n",
      "batch 179, loss: 0.0024, instance_loss: 0.1778, weighted_loss: 0.0550, label: 0, bag_size: 24\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0652, weighted_loss: 0.0195, label: 0, bag_size: 31\n",
      "batch 219, loss: 1.0066, instance_loss: 2.8647, weighted_loss: 1.5641, label: 0, bag_size: 63\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0733, weighted_loss: 0.0220, label: 1, bag_size: 40\n",
      "batch 259, loss: 0.0001, instance_loss: 0.7263, weighted_loss: 0.2180, label: 0, bag_size: 45\n",
      "batch 279, loss: 0.0000, instance_loss: 0.3795, weighted_loss: 0.1138, label: 0, bag_size: 21\n",
      "batch 299, loss: 0.0931, instance_loss: 0.1278, weighted_loss: 0.1035, label: 1, bag_size: 89\n",
      "batch 319, loss: 0.0092, instance_loss: 0.0382, weighted_loss: 0.0179, label: 1, bag_size: 84\n",
      "batch 339, loss: 0.0001, instance_loss: 0.4296, weighted_loss: 0.1289, label: 0, bag_size: 53\n",
      "batch 359, loss: 0.0035, instance_loss: 0.2228, weighted_loss: 0.0693, label: 0, bag_size: 60\n",
      "batch 379, loss: 0.0487, instance_loss: 0.2354, weighted_loss: 0.1047, label: 0, bag_size: 80\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0058, weighted_loss: 0.0017, label: 1, bag_size: 61\n",
      "batch 419, loss: 0.0000, instance_loss: 0.3588, weighted_loss: 0.1076, label: 0, bag_size: 93\n",
      "batch 439, loss: 0.0011, instance_loss: 0.0765, weighted_loss: 0.0237, label: 1, bag_size: 89\n",
      "batch 459, loss: 0.0075, instance_loss: 0.1725, weighted_loss: 0.0570, label: 1, bag_size: 48\n",
      "batch 479, loss: 7.0975, instance_loss: 3.0329, weighted_loss: 5.8781, label: 0, bag_size: 77\n",
      "batch 499, loss: 0.2663, instance_loss: 0.7592, weighted_loss: 0.4142, label: 0, bag_size: 63\n",
      "batch 519, loss: 0.2541, instance_loss: 1.1214, weighted_loss: 0.5143, label: 0, bag_size: 73\n",
      "batch 539, loss: 1.9321, instance_loss: 0.4229, weighted_loss: 1.4794, label: 0, bag_size: 50\n",
      "batch 559, loss: 6.1403, instance_loss: 1.4144, weighted_loss: 4.7225, label: 0, bag_size: 48\n",
      "batch 579, loss: 0.0124, instance_loss: 0.0462, weighted_loss: 0.0225, label: 0, bag_size: 107\n",
      "batch 599, loss: 0.0011, instance_loss: 0.0046, weighted_loss: 0.0021, label: 1, bag_size: 16\n",
      "batch 619, loss: 0.0006, instance_loss: 0.0057, weighted_loss: 0.0022, label: 1, bag_size: 72\n",
      "batch 639, loss: 0.7112, instance_loss: 1.8690, weighted_loss: 1.0585, label: 1, bag_size: 24\n",
      "batch 659, loss: 1.2941, instance_loss: 0.7075, weighted_loss: 1.1181, label: 1, bag_size: 32\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9467383107088989: correct 10043/10608\n",
      "class 1 clustering acc 0.7383107088989442: correct 3916/5304\n",
      "Epoch: 14, train_loss: 0.5754, train_clustering_loss:  0.4956, train_error: 0.1478\n",
      "class 0: acc 0.8429003021148036, correct 279/331\n",
      "class 1: acc 0.8614457831325302, correct 286/332\n",
      "\n",
      "Val Set, val_loss: 0.7339, val_error: 0.1882, auc: 0.9089\n",
      "class 0 clustering acc 0.8742647058823529: correct 1189/1360\n",
      "class 1 clustering acc 0.6852941176470588: correct 466/680\n",
      "class 0: acc 0.7333333333333333, correct 33/45\n",
      "class 1: acc 0.9, correct 36/40\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.2300, weighted_loss: 0.0690, label: 0, bag_size: 87\n",
      "batch 39, loss: 0.0242, instance_loss: 0.7609, weighted_loss: 0.2452, label: 1, bag_size: 140\n",
      "batch 59, loss: 0.0298, instance_loss: 0.4790, weighted_loss: 0.1645, label: 0, bag_size: 40\n",
      "batch 79, loss: 0.0030, instance_loss: 0.0405, weighted_loss: 0.0142, label: 1, bag_size: 153\n",
      "batch 99, loss: 0.0158, instance_loss: 0.3001, weighted_loss: 0.1011, label: 1, bag_size: 23\n",
      "batch 119, loss: 0.0009, instance_loss: 0.2489, weighted_loss: 0.0753, label: 0, bag_size: 89\n",
      "batch 139, loss: 0.0000, instance_loss: 1.1189, weighted_loss: 0.3357, label: 0, bag_size: 21\n",
      "batch 159, loss: 4.2328, instance_loss: 0.5144, weighted_loss: 3.1173, label: 0, bag_size: 58\n",
      "batch 179, loss: 0.2871, instance_loss: 0.5864, weighted_loss: 0.3769, label: 1, bag_size: 49\n",
      "batch 199, loss: 0.0000, instance_loss: 0.4778, weighted_loss: 0.1433, label: 0, bag_size: 57\n",
      "batch 219, loss: 0.1869, instance_loss: 0.8819, weighted_loss: 0.3954, label: 1, bag_size: 25\n",
      "batch 239, loss: 0.4975, instance_loss: 0.8166, weighted_loss: 0.5932, label: 1, bag_size: 95\n",
      "batch 259, loss: 2.1968, instance_loss: 2.4576, weighted_loss: 2.2750, label: 0, bag_size: 45\n",
      "batch 279, loss: 0.0000, instance_loss: 0.4668, weighted_loss: 0.1401, label: 1, bag_size: 107\n",
      "batch 299, loss: 0.0600, instance_loss: 0.6992, weighted_loss: 0.2518, label: 0, bag_size: 64\n",
      "batch 319, loss: 0.2019, instance_loss: 1.0664, weighted_loss: 0.4612, label: 1, bag_size: 67\n",
      "batch 339, loss: 0.0000, instance_loss: 0.4688, weighted_loss: 0.1406, label: 0, bag_size: 78\n",
      "batch 359, loss: 0.0000, instance_loss: 0.2877, weighted_loss: 0.0863, label: 1, bag_size: 73\n",
      "batch 379, loss: 0.0000, instance_loss: 0.3097, weighted_loss: 0.0929, label: 0, bag_size: 65\n",
      "batch 399, loss: 0.0004, instance_loss: 0.5790, weighted_loss: 0.1740, label: 0, bag_size: 107\n",
      "batch 419, loss: 0.1221, instance_loss: 0.2026, weighted_loss: 0.1463, label: 1, bag_size: 75\n",
      "batch 439, loss: 0.0056, instance_loss: 0.4837, weighted_loss: 0.1490, label: 1, bag_size: 50\n",
      "batch 459, loss: 0.0001, instance_loss: 0.7817, weighted_loss: 0.2346, label: 1, bag_size: 20\n",
      "batch 479, loss: 0.2760, instance_loss: 0.2176, weighted_loss: 0.2585, label: 0, bag_size: 45\n",
      "batch 499, loss: 0.0026, instance_loss: 0.3131, weighted_loss: 0.0958, label: 0, bag_size: 45\n",
      "batch 519, loss: 1.0864, instance_loss: 1.7688, weighted_loss: 1.2911, label: 1, bag_size: 32\n",
      "batch 539, loss: 0.0741, instance_loss: 0.3519, weighted_loss: 0.1575, label: 0, bag_size: 87\n",
      "batch 559, loss: 0.0020, instance_loss: 0.0662, weighted_loss: 0.0212, label: 1, bag_size: 20\n",
      "batch 579, loss: 0.0010, instance_loss: 0.1057, weighted_loss: 0.0324, label: 1, bag_size: 39\n",
      "batch 599, loss: 0.0032, instance_loss: 0.2598, weighted_loss: 0.0802, label: 0, bag_size: 85\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0876, weighted_loss: 0.0263, label: 1, bag_size: 70\n",
      "batch 639, loss: 0.0068, instance_loss: 0.4296, weighted_loss: 0.1336, label: 1, bag_size: 62\n",
      "batch 659, loss: 0.1109, instance_loss: 0.3854, weighted_loss: 0.1933, label: 0, bag_size: 93\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9250565610859729: correct 9813/10608\n",
      "class 1 clustering acc 0.6036953242835595: correct 3202/5304\n",
      "Epoch: 15, train_loss: 0.6483, train_clustering_loss:  0.6551, train_error: 0.1629\n",
      "class 0: acc 0.8398791540785498, correct 278/331\n",
      "class 1: acc 0.8343373493975904, correct 277/332\n",
      "\n",
      "Val Set, val_loss: 0.5731, val_error: 0.2235, auc: 0.9200\n",
      "class 0 clustering acc 0.8860294117647058: correct 1205/1360\n",
      "class 1 clustering acc 0.6058823529411764: correct 412/680\n",
      "class 0: acc 0.6444444444444445, correct 29/45\n",
      "class 1: acc 0.925, correct 37/40\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0326, instance_loss: 0.2361, weighted_loss: 0.0936, label: 1, bag_size: 30\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0324, weighted_loss: 0.0098, label: 1, bag_size: 21\n",
      "batch 59, loss: 3.9965, instance_loss: 3.0950, weighted_loss: 3.7260, label: 1, bag_size: 51\n",
      "batch 79, loss: 0.4942, instance_loss: 0.3486, weighted_loss: 0.4505, label: 1, bag_size: 88\n",
      "batch 99, loss: 0.0220, instance_loss: 0.1433, weighted_loss: 0.0584, label: 0, bag_size: 103\n",
      "batch 119, loss: 0.1440, instance_loss: 0.7126, weighted_loss: 0.3146, label: 1, bag_size: 51\n",
      "batch 139, loss: 0.0003, instance_loss: 0.2699, weighted_loss: 0.0812, label: 0, bag_size: 35\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0745, weighted_loss: 0.0224, label: 0, bag_size: 78\n",
      "batch 179, loss: 0.0002, instance_loss: 0.0591, weighted_loss: 0.0179, label: 1, bag_size: 81\n",
      "batch 199, loss: 0.7153, instance_loss: 2.9533, weighted_loss: 1.3867, label: 0, bag_size: 21\n",
      "batch 219, loss: 0.2815, instance_loss: 0.2159, weighted_loss: 0.2618, label: 1, bag_size: 42\n",
      "batch 239, loss: 0.0000, instance_loss: 0.1799, weighted_loss: 0.0540, label: 0, bag_size: 25\n",
      "batch 259, loss: 0.0694, instance_loss: 0.1770, weighted_loss: 0.1017, label: 0, bag_size: 79\n",
      "batch 279, loss: 0.0095, instance_loss: 0.0542, weighted_loss: 0.0229, label: 1, bag_size: 35\n",
      "batch 299, loss: 0.3021, instance_loss: 0.7724, weighted_loss: 0.4432, label: 1, bag_size: 24\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0674, weighted_loss: 0.0202, label: 0, bag_size: 77\n",
      "batch 339, loss: 0.0177, instance_loss: 0.2895, weighted_loss: 0.0992, label: 1, bag_size: 87\n",
      "batch 359, loss: 0.3322, instance_loss: 0.0721, weighted_loss: 0.2542, label: 1, bag_size: 32\n",
      "batch 379, loss: 0.0068, instance_loss: 0.0187, weighted_loss: 0.0104, label: 1, bag_size: 100\n",
      "batch 399, loss: 0.0600, instance_loss: 0.0703, weighted_loss: 0.0631, label: 1, bag_size: 105\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0971, weighted_loss: 0.0291, label: 0, bag_size: 45\n",
      "batch 439, loss: 0.0000, instance_loss: 0.2392, weighted_loss: 0.0718, label: 0, bag_size: 65\n",
      "batch 459, loss: 0.0488, instance_loss: 0.6982, weighted_loss: 0.2436, label: 1, bag_size: 38\n",
      "batch 479, loss: 0.0068, instance_loss: 0.3287, weighted_loss: 0.1034, label: 0, bag_size: 71\n",
      "batch 499, loss: 0.0198, instance_loss: 0.1700, weighted_loss: 0.0648, label: 1, bag_size: 22\n",
      "batch 519, loss: 0.0057, instance_loss: 0.1057, weighted_loss: 0.0357, label: 1, bag_size: 45\n",
      "batch 539, loss: 0.4832, instance_loss: 1.0131, weighted_loss: 0.6421, label: 0, bag_size: 33\n",
      "batch 559, loss: 0.0986, instance_loss: 1.0764, weighted_loss: 0.3919, label: 1, bag_size: 48\n",
      "batch 579, loss: 0.0087, instance_loss: 0.1274, weighted_loss: 0.0443, label: 0, bag_size: 51\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0505, weighted_loss: 0.0152, label: 1, bag_size: 32\n",
      "batch 619, loss: 1.1557, instance_loss: 0.0527, weighted_loss: 0.8248, label: 0, bag_size: 43\n",
      "batch 639, loss: 0.1718, instance_loss: 0.6102, weighted_loss: 0.3033, label: 1, bag_size: 57\n",
      "batch 659, loss: 0.0432, instance_loss: 0.0172, weighted_loss: 0.0354, label: 1, bag_size: 67\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9599358974358975: correct 10183/10608\n",
      "class 1 clustering acc 0.7577300150829562: correct 4019/5304\n",
      "Epoch: 16, train_loss: 0.3011, train_clustering_loss:  0.4067, train_error: 0.1252\n",
      "class 0: acc 0.8693009118541033, correct 286/329\n",
      "class 1: acc 0.8802395209580839, correct 294/334\n",
      "\n",
      "Val Set, val_loss: 0.5839, val_error: 0.1412, auc: 0.9100\n",
      "class 0 clustering acc 0.9161764705882353: correct 1246/1360\n",
      "class 1 clustering acc 0.663235294117647: correct 451/680\n",
      "class 0: acc 0.8222222222222222, correct 37/45\n",
      "class 1: acc 0.9, correct 36/40\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0405, weighted_loss: 0.0123, label: 1, bag_size: 107\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0184, weighted_loss: 0.0056, label: 1, bag_size: 56\n",
      "batch 59, loss: 0.0475, instance_loss: 0.8393, weighted_loss: 0.2850, label: 0, bag_size: 95\n",
      "batch 79, loss: 0.2602, instance_loss: 0.4057, weighted_loss: 0.3039, label: 1, bag_size: 21\n",
      "batch 99, loss: 0.0132, instance_loss: 1.8398, weighted_loss: 0.5612, label: 1, bag_size: 50\n",
      "batch 119, loss: 0.0006, instance_loss: 0.0927, weighted_loss: 0.0282, label: 1, bag_size: 25\n",
      "batch 139, loss: 1.2027, instance_loss: 0.0866, weighted_loss: 0.8679, label: 0, bag_size: 20\n",
      "batch 159, loss: 0.0015, instance_loss: 0.1577, weighted_loss: 0.0484, label: 0, bag_size: 114\n",
      "batch 179, loss: 0.0120, instance_loss: 0.0924, weighted_loss: 0.0361, label: 0, bag_size: 79\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0646, weighted_loss: 0.0194, label: 0, bag_size: 93\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0073, weighted_loss: 0.0023, label: 1, bag_size: 128\n",
      "batch 239, loss: 0.0004, instance_loss: 0.0371, weighted_loss: 0.0114, label: 0, bag_size: 48\n",
      "batch 259, loss: 0.0005, instance_loss: 0.1576, weighted_loss: 0.0477, label: 0, bag_size: 101\n",
      "batch 279, loss: 0.2175, instance_loss: 0.1659, weighted_loss: 0.2020, label: 0, bag_size: 96\n",
      "batch 299, loss: 0.0895, instance_loss: 0.5366, weighted_loss: 0.2237, label: 0, bag_size: 42\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0028, weighted_loss: 0.0008, label: 1, bag_size: 79\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0967, weighted_loss: 0.0290, label: 0, bag_size: 108\n",
      "batch 359, loss: 0.0173, instance_loss: 1.8807, weighted_loss: 0.5763, label: 0, bag_size: 35\n",
      "batch 379, loss: 0.0363, instance_loss: 0.3807, weighted_loss: 0.1396, label: 1, bag_size: 31\n",
      "batch 399, loss: 0.0494, instance_loss: 0.4452, weighted_loss: 0.1681, label: 1, bag_size: 98\n",
      "batch 419, loss: 4.6820, instance_loss: 2.8338, weighted_loss: 4.1276, label: 0, bag_size: 55\n",
      "batch 439, loss: 2.1809, instance_loss: 0.7722, weighted_loss: 1.7583, label: 0, bag_size: 24\n",
      "batch 459, loss: 0.8951, instance_loss: 1.5898, weighted_loss: 1.1035, label: 1, bag_size: 68\n",
      "batch 479, loss: 0.0011, instance_loss: 0.4099, weighted_loss: 0.1238, label: 0, bag_size: 89\n",
      "batch 499, loss: 0.0004, instance_loss: 0.3555, weighted_loss: 0.1070, label: 0, bag_size: 66\n",
      "batch 519, loss: 0.0006, instance_loss: 0.3082, weighted_loss: 0.0929, label: 1, bag_size: 80\n",
      "batch 539, loss: 0.3382, instance_loss: 1.4661, weighted_loss: 0.6766, label: 0, bag_size: 55\n",
      "batch 559, loss: 0.0543, instance_loss: 0.9174, weighted_loss: 0.3133, label: 1, bag_size: 94\n",
      "batch 579, loss: 0.0140, instance_loss: 0.0094, weighted_loss: 0.0126, label: 1, bag_size: 107\n",
      "batch 599, loss: 0.0106, instance_loss: 0.1083, weighted_loss: 0.0399, label: 1, bag_size: 51\n",
      "batch 619, loss: 0.0036, instance_loss: 0.1858, weighted_loss: 0.0582, label: 1, bag_size: 78\n",
      "batch 639, loss: 1.1180, instance_loss: 1.2194, weighted_loss: 1.1484, label: 1, bag_size: 72\n",
      "batch 659, loss: 0.0139, instance_loss: 0.7192, weighted_loss: 0.2255, label: 0, bag_size: 65\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9395739064856712: correct 9967/10608\n",
      "class 1 clustering acc 0.6562971342383107: correct 3481/5304\n",
      "Epoch: 17, train_loss: 0.4876, train_clustering_loss:  0.5892, train_error: 0.1554\n",
      "class 0: acc 0.845679012345679, correct 274/324\n",
      "class 1: acc 0.8436578171091446, correct 286/339\n",
      "\n",
      "Val Set, val_loss: 0.8708, val_error: 0.1882, auc: 0.8767\n",
      "class 0 clustering acc 0.950735294117647: correct 1293/1360\n",
      "class 1 clustering acc 0.4823529411764706: correct 328/680\n",
      "class 0: acc 0.9111111111111111, correct 41/45\n",
      "class 1: acc 0.7, correct 28/40\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0822, weighted_loss: 0.0247, label: 0, bag_size: 46\n",
      "batch 39, loss: 4.9513, instance_loss: 0.6865, weighted_loss: 3.6719, label: 0, bag_size: 13\n",
      "batch 59, loss: 0.9308, instance_loss: 1.0763, weighted_loss: 0.9745, label: 0, bag_size: 91\n",
      "batch 79, loss: 0.6981, instance_loss: 0.3080, weighted_loss: 0.5810, label: 0, bag_size: 106\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0163, weighted_loss: 0.0049, label: 0, bag_size: 36\n",
      "batch 119, loss: 1.2830, instance_loss: 0.2968, weighted_loss: 0.9871, label: 0, bag_size: 37\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0304, weighted_loss: 0.0091, label: 1, bag_size: 94\n",
      "batch 159, loss: 0.0071, instance_loss: 0.0172, weighted_loss: 0.0101, label: 1, bag_size: 94\n",
      "batch 179, loss: 0.0001, instance_loss: 0.4239, weighted_loss: 0.1272, label: 1, bag_size: 86\n",
      "batch 199, loss: 0.0000, instance_loss: 0.1303, weighted_loss: 0.0391, label: 1, bag_size: 84\n",
      "batch 219, loss: 0.0220, instance_loss: 1.2878, weighted_loss: 0.4018, label: 0, bag_size: 35\n",
      "batch 239, loss: 5.3884, instance_loss: 3.9253, weighted_loss: 4.9495, label: 1, bag_size: 32\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0135, weighted_loss: 0.0040, label: 0, bag_size: 90\n",
      "batch 279, loss: 0.4011, instance_loss: 0.8147, weighted_loss: 0.5252, label: 0, bag_size: 21\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0298, weighted_loss: 0.0089, label: 1, bag_size: 114\n",
      "batch 319, loss: 0.0070, instance_loss: 0.3877, weighted_loss: 0.1212, label: 0, bag_size: 66\n",
      "batch 339, loss: 0.0077, instance_loss: 0.0349, weighted_loss: 0.0158, label: 0, bag_size: 43\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0024, weighted_loss: 0.0007, label: 1, bag_size: 96\n",
      "batch 379, loss: 0.0708, instance_loss: 0.0172, weighted_loss: 0.0547, label: 1, bag_size: 73\n",
      "batch 399, loss: 2.2566, instance_loss: 0.9362, weighted_loss: 1.8605, label: 1, bag_size: 51\n",
      "batch 419, loss: 0.1797, instance_loss: 0.2668, weighted_loss: 0.2058, label: 1, bag_size: 14\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0268, weighted_loss: 0.0081, label: 1, bag_size: 114\n",
      "batch 459, loss: 0.0030, instance_loss: 0.0065, weighted_loss: 0.0040, label: 1, bag_size: 99\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0043, weighted_loss: 0.0013, label: 1, bag_size: 109\n",
      "batch 499, loss: 0.0973, instance_loss: 4.2970, weighted_loss: 1.3572, label: 0, bag_size: 51\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0022, weighted_loss: 0.0007, label: 1, bag_size: 68\n",
      "batch 539, loss: 0.1166, instance_loss: 0.7046, weighted_loss: 0.2930, label: 0, bag_size: 40\n",
      "batch 559, loss: 0.0003, instance_loss: 0.4468, weighted_loss: 0.1343, label: 0, bag_size: 27\n",
      "batch 579, loss: 0.0003, instance_loss: 0.0072, weighted_loss: 0.0024, label: 0, bag_size: 93\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 114\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 98\n",
      "batch 639, loss: 0.4625, instance_loss: 0.5022, weighted_loss: 0.4744, label: 0, bag_size: 86\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0100, weighted_loss: 0.0030, label: 1, bag_size: 128\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9474924585218703: correct 10051/10608\n",
      "class 1 clustering acc 0.7356711915535445: correct 3902/5304\n",
      "Epoch: 18, train_loss: 0.5899, train_clustering_loss:  0.5121, train_error: 0.1840\n",
      "class 0: acc 0.8076923076923077, correct 252/312\n",
      "class 1: acc 0.8233618233618234, correct 289/351\n",
      "\n",
      "Val Set, val_loss: 0.5808, val_error: 0.1647, auc: 0.9044\n",
      "class 0 clustering acc 0.9477941176470588: correct 1289/1360\n",
      "class 1 clustering acc 0.8102941176470588: correct 551/680\n",
      "class 0: acc 0.8, correct 36/45\n",
      "class 1: acc 0.875, correct 35/40\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0065, instance_loss: 0.0407, weighted_loss: 0.0167, label: 1, bag_size: 94\n",
      "batch 39, loss: 0.8916, instance_loss: 0.3252, weighted_loss: 0.7216, label: 1, bag_size: 110\n",
      "batch 59, loss: 0.0002, instance_loss: 0.0260, weighted_loss: 0.0079, label: 1, bag_size: 78\n",
      "batch 79, loss: 9.0319, instance_loss: 0.6829, weighted_loss: 6.5272, label: 1, bag_size: 82\n",
      "batch 99, loss: 0.0000, instance_loss: 0.1440, weighted_loss: 0.0432, label: 0, bag_size: 58\n",
      "batch 119, loss: 8.1135, instance_loss: 1.9078, weighted_loss: 6.2518, label: 1, bag_size: 82\n",
      "batch 139, loss: 0.0173, instance_loss: 0.0103, weighted_loss: 0.0152, label: 1, bag_size: 99\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0537, weighted_loss: 0.0161, label: 1, bag_size: 86\n",
      "batch 179, loss: 0.0029, instance_loss: 0.0713, weighted_loss: 0.0234, label: 1, bag_size: 77\n",
      "batch 199, loss: 0.0004, instance_loss: 0.2690, weighted_loss: 0.0810, label: 0, bag_size: 78\n",
      "batch 219, loss: 0.0634, instance_loss: 0.5303, weighted_loss: 0.2034, label: 0, bag_size: 33\n",
      "batch 239, loss: 0.0000, instance_loss: 0.3386, weighted_loss: 0.1016, label: 0, bag_size: 89\n",
      "batch 259, loss: 2.1778, instance_loss: 1.2000, weighted_loss: 1.8845, label: 0, bag_size: 87\n",
      "batch 279, loss: 0.0000, instance_loss: 0.1397, weighted_loss: 0.0419, label: 0, bag_size: 63\n",
      "batch 299, loss: 0.0055, instance_loss: 1.6339, weighted_loss: 0.4940, label: 1, bag_size: 19\n",
      "batch 319, loss: 6.4553, instance_loss: 1.3601, weighted_loss: 4.9267, label: 0, bag_size: 57\n",
      "batch 339, loss: 0.0000, instance_loss: 0.4347, weighted_loss: 0.1304, label: 1, bag_size: 61\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0268, weighted_loss: 0.0080, label: 1, bag_size: 58\n",
      "batch 379, loss: 0.0000, instance_loss: 0.4262, weighted_loss: 0.1279, label: 0, bag_size: 116\n",
      "batch 399, loss: 0.0003, instance_loss: 0.0476, weighted_loss: 0.0145, label: 1, bag_size: 44\n",
      "batch 419, loss: 0.0000, instance_loss: 0.6808, weighted_loss: 0.2042, label: 1, bag_size: 24\n",
      "batch 439, loss: 0.0035, instance_loss: 0.0225, weighted_loss: 0.0092, label: 1, bag_size: 17\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0211, weighted_loss: 0.0063, label: 1, bag_size: 58\n",
      "batch 479, loss: 0.0000, instance_loss: 0.4779, weighted_loss: 0.1434, label: 0, bag_size: 75\n",
      "batch 499, loss: 0.0001, instance_loss: 0.2718, weighted_loss: 0.0816, label: 0, bag_size: 36\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0077, weighted_loss: 0.0023, label: 0, bag_size: 65\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0061, weighted_loss: 0.0018, label: 0, bag_size: 53\n",
      "batch 559, loss: 0.0107, instance_loss: 0.0708, weighted_loss: 0.0287, label: 1, bag_size: 65\n",
      "batch 579, loss: 0.0003, instance_loss: 0.1474, weighted_loss: 0.0444, label: 1, bag_size: 34\n",
      "batch 599, loss: 0.0008, instance_loss: 0.0602, weighted_loss: 0.0186, label: 0, bag_size: 80\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0451, weighted_loss: 0.0135, label: 1, bag_size: 30\n",
      "batch 639, loss: 0.0219, instance_loss: 0.0407, weighted_loss: 0.0275, label: 0, bag_size: 56\n",
      "batch 659, loss: 0.0031, instance_loss: 0.0351, weighted_loss: 0.0127, label: 0, bag_size: 114\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9357088989441931: correct 9926/10608\n",
      "class 1 clustering acc 0.7226621417797888: correct 3833/5304\n",
      "Epoch: 19, train_loss: 0.8300, train_clustering_loss:  0.5259, train_error: 0.1659\n",
      "class 0: acc 0.8095238095238095, correct 238/294\n",
      "class 1: acc 0.8536585365853658, correct 315/369\n",
      "\n",
      "Val Set, val_loss: 0.9633, val_error: 0.2118, auc: 0.8956\n",
      "class 0 clustering acc 0.9522058823529411: correct 1295/1360\n",
      "class 1 clustering acc 0.6970588235294117: correct 474/680\n",
      "class 0: acc 0.6666666666666666, correct 30/45\n",
      "class 1: acc 0.925, correct 37/40\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0393, weighted_loss: 0.0118, label: 1, bag_size: 58\n",
      "batch 39, loss: 0.0000, instance_loss: 0.3113, weighted_loss: 0.0934, label: 1, bag_size: 30\n",
      "batch 59, loss: 0.0115, instance_loss: 0.0222, weighted_loss: 0.0147, label: 1, bag_size: 60\n",
      "batch 79, loss: 0.4485, instance_loss: 2.8504, weighted_loss: 1.1690, label: 0, bag_size: 32\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0135, weighted_loss: 0.0041, label: 1, bag_size: 61\n",
      "batch 119, loss: 0.0267, instance_loss: 0.0550, weighted_loss: 0.0352, label: 0, bag_size: 38\n",
      "batch 139, loss: 0.0010, instance_loss: 0.0139, weighted_loss: 0.0048, label: 0, bag_size: 25\n",
      "batch 159, loss: 0.0228, instance_loss: 0.0996, weighted_loss: 0.0458, label: 1, bag_size: 44\n",
      "batch 179, loss: 0.0143, instance_loss: 0.0310, weighted_loss: 0.0193, label: 1, bag_size: 53\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0076, weighted_loss: 0.0023, label: 0, bag_size: 80\n",
      "batch 219, loss: 0.0043, instance_loss: 1.0403, weighted_loss: 0.3151, label: 1, bag_size: 17\n",
      "batch 239, loss: 0.0005, instance_loss: 0.0088, weighted_loss: 0.0030, label: 1, bag_size: 49\n",
      "batch 259, loss: 5.1743, instance_loss: 3.5230, weighted_loss: 4.6789, label: 1, bag_size: 35\n",
      "batch 279, loss: 0.0000, instance_loss: 0.2748, weighted_loss: 0.0824, label: 1, bag_size: 107\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0620, weighted_loss: 0.0186, label: 1, bag_size: 107\n",
      "batch 319, loss: 0.0095, instance_loss: 0.5164, weighted_loss: 0.1616, label: 0, bag_size: 55\n",
      "batch 339, loss: 0.0029, instance_loss: 0.1300, weighted_loss: 0.0410, label: 1, bag_size: 85\n",
      "batch 359, loss: 0.0567, instance_loss: 0.0081, weighted_loss: 0.0421, label: 1, bag_size: 31\n",
      "batch 379, loss: 0.0007, instance_loss: 0.0362, weighted_loss: 0.0113, label: 1, bag_size: 85\n",
      "batch 399, loss: 0.0003, instance_loss: 0.4358, weighted_loss: 0.1309, label: 1, bag_size: 30\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0064, weighted_loss: 0.0019, label: 0, bag_size: 89\n",
      "batch 439, loss: 0.0007, instance_loss: 0.0487, weighted_loss: 0.0151, label: 0, bag_size: 114\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0138, weighted_loss: 0.0042, label: 0, bag_size: 56\n",
      "batch 479, loss: 0.1531, instance_loss: 0.3196, weighted_loss: 0.2031, label: 0, bag_size: 86\n",
      "batch 499, loss: 0.0157, instance_loss: 0.1225, weighted_loss: 0.0477, label: 1, bag_size: 98\n",
      "batch 519, loss: 0.0018, instance_loss: 0.2659, weighted_loss: 0.0810, label: 1, bag_size: 100\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0157, weighted_loss: 0.0047, label: 0, bag_size: 81\n",
      "batch 559, loss: 0.0182, instance_loss: 0.1644, weighted_loss: 0.0621, label: 0, bag_size: 31\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0107, weighted_loss: 0.0033, label: 0, bag_size: 114\n",
      "batch 599, loss: 0.0111, instance_loss: 0.0372, weighted_loss: 0.0190, label: 1, bag_size: 97\n",
      "batch 619, loss: 0.0075, instance_loss: 0.1098, weighted_loss: 0.0382, label: 0, bag_size: 35\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0322, weighted_loss: 0.0097, label: 1, bag_size: 100\n",
      "batch 659, loss: 0.0040, instance_loss: 0.0048, weighted_loss: 0.0042, label: 0, bag_size: 87\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9562594268476622: correct 10144/10608\n",
      "class 1 clustering acc 0.8065610859728507: correct 4278/5304\n",
      "Epoch: 20, train_loss: 0.3081, train_clustering_loss:  0.3907, train_error: 0.1086\n",
      "class 0: acc 0.8878504672897196, correct 285/321\n",
      "class 1: acc 0.8947368421052632, correct 306/342\n",
      "\n",
      "Val Set, val_loss: 0.6635, val_error: 0.1647, auc: 0.9056\n",
      "class 0 clustering acc 0.9558823529411765: correct 1300/1360\n",
      "class 1 clustering acc 0.6882352941176471: correct 468/680\n",
      "class 0: acc 0.8888888888888888, correct 40/45\n",
      "class 1: acc 0.775, correct 31/40\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.9074, instance_loss: 0.4416, weighted_loss: 0.7677, label: 0, bag_size: 51\n",
      "batch 39, loss: 0.0004, instance_loss: 0.0570, weighted_loss: 0.0174, label: 1, bag_size: 14\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0748, weighted_loss: 0.0224, label: 1, bag_size: 53\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0094, weighted_loss: 0.0028, label: 1, bag_size: 72\n",
      "batch 99, loss: 0.0015, instance_loss: 0.0069, weighted_loss: 0.0031, label: 0, bag_size: 101\n",
      "batch 119, loss: 1.3882, instance_loss: 0.4161, weighted_loss: 1.0966, label: 1, bag_size: 75\n",
      "batch 139, loss: 0.0009, instance_loss: 0.0057, weighted_loss: 0.0024, label: 0, bag_size: 57\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0054, weighted_loss: 0.0016, label: 0, bag_size: 59\n",
      "batch 179, loss: 0.0010, instance_loss: 0.0065, weighted_loss: 0.0027, label: 0, bag_size: 63\n",
      "batch 199, loss: 1.1721, instance_loss: 0.7038, weighted_loss: 1.0316, label: 0, bag_size: 28\n",
      "batch 219, loss: 0.0016, instance_loss: 0.0024, weighted_loss: 0.0018, label: 0, bag_size: 61\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0092, weighted_loss: 0.0028, label: 1, bag_size: 77\n",
      "batch 259, loss: 0.0026, instance_loss: 0.1055, weighted_loss: 0.0334, label: 1, bag_size: 39\n",
      "batch 279, loss: 2.1817, instance_loss: 0.0216, weighted_loss: 1.5337, label: 1, bag_size: 38\n",
      "batch 299, loss: 5.0130, instance_loss: 1.0345, weighted_loss: 3.8194, label: 1, bag_size: 28\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0046, weighted_loss: 0.0014, label: 0, bag_size: 108\n",
      "batch 339, loss: 0.6681, instance_loss: 2.6711, weighted_loss: 1.2690, label: 0, bag_size: 93\n",
      "batch 359, loss: 0.1357, instance_loss: 0.7283, weighted_loss: 0.3135, label: 0, bag_size: 29\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0223, weighted_loss: 0.0067, label: 0, bag_size: 21\n",
      "batch 399, loss: 2.1702, instance_loss: 1.5424, weighted_loss: 1.9818, label: 0, bag_size: 41\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0061, weighted_loss: 0.0018, label: 0, bag_size: 49\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0192, weighted_loss: 0.0058, label: 0, bag_size: 35\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0199, weighted_loss: 0.0060, label: 1, bag_size: 123\n",
      "batch 479, loss: 0.0134, instance_loss: 0.2670, weighted_loss: 0.0895, label: 0, bag_size: 79\n",
      "batch 499, loss: 0.0009, instance_loss: 0.0226, weighted_loss: 0.0074, label: 0, bag_size: 55\n",
      "batch 519, loss: 0.0310, instance_loss: 3.2230, weighted_loss: 0.9886, label: 1, bag_size: 87\n",
      "batch 539, loss: 0.1670, instance_loss: 0.0977, weighted_loss: 0.1462, label: 1, bag_size: 45\n",
      "batch 559, loss: 0.0000, instance_loss: 0.1382, weighted_loss: 0.0415, label: 0, bag_size: 87\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0078, weighted_loss: 0.0023, label: 1, bag_size: 31\n",
      "batch 599, loss: 0.0000, instance_loss: 0.2852, weighted_loss: 0.0856, label: 0, bag_size: 50\n",
      "batch 619, loss: 0.3655, instance_loss: 0.2051, weighted_loss: 0.3174, label: 1, bag_size: 88\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0022, weighted_loss: 0.0007, label: 1, bag_size: 25\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 0, bag_size: 39\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9585218702865762: correct 10168/10608\n",
      "class 1 clustering acc 0.7796003016591252: correct 4135/5304\n",
      "Epoch: 21, train_loss: 0.5820, train_clustering_loss:  0.4241, train_error: 0.1523\n",
      "class 0: acc 0.8528528528528528, correct 284/333\n",
      "class 1: acc 0.8424242424242424, correct 278/330\n",
      "\n",
      "Val Set, val_loss: 0.6920, val_error: 0.2471, auc: 0.9389\n",
      "class 0 clustering acc 0.9235294117647059: correct 1256/1360\n",
      "class 1 clustering acc 0.6955882352941176: correct 473/680\n",
      "class 0: acc 0.9555555555555556, correct 43/45\n",
      "class 1: acc 0.525, correct 21/40\n",
      "EarlyStopping counter: 12 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0065, weighted_loss: 0.0020, label: 0, bag_size: 103\n",
      "batch 39, loss: 0.0008, instance_loss: 0.0318, weighted_loss: 0.0101, label: 0, bag_size: 72\n",
      "batch 59, loss: 0.0001, instance_loss: 0.0086, weighted_loss: 0.0026, label: 0, bag_size: 49\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0579, weighted_loss: 0.0174, label: 0, bag_size: 48\n",
      "batch 99, loss: 0.0008, instance_loss: 0.0285, weighted_loss: 0.0091, label: 0, bag_size: 72\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0263, weighted_loss: 0.0079, label: 1, bag_size: 38\n",
      "batch 139, loss: 0.0067, instance_loss: 0.3599, weighted_loss: 0.1126, label: 0, bag_size: 33\n",
      "batch 159, loss: 0.0003, instance_loss: 0.3258, weighted_loss: 0.0980, label: 1, bag_size: 83\n",
      "batch 179, loss: 1.6095, instance_loss: 1.7437, weighted_loss: 1.6498, label: 0, bag_size: 51\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0115, weighted_loss: 0.0034, label: 1, bag_size: 70\n",
      "batch 219, loss: 0.0296, instance_loss: 0.0189, weighted_loss: 0.0264, label: 1, bag_size: 31\n",
      "batch 239, loss: 3.0190, instance_loss: 0.3038, weighted_loss: 2.2044, label: 1, bag_size: 24\n",
      "batch 259, loss: 5.0448, instance_loss: 3.6663, weighted_loss: 4.6313, label: 0, bag_size: 78\n",
      "batch 279, loss: 0.5516, instance_loss: 0.1009, weighted_loss: 0.4164, label: 1, bag_size: 36\n",
      "batch 299, loss: 0.0003, instance_loss: 0.0011, weighted_loss: 0.0005, label: 0, bag_size: 49\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0055, weighted_loss: 0.0017, label: 0, bag_size: 78\n",
      "batch 339, loss: 0.4260, instance_loss: 0.2115, weighted_loss: 0.3617, label: 1, bag_size: 31\n",
      "batch 359, loss: 0.1537, instance_loss: 0.6458, weighted_loss: 0.3013, label: 1, bag_size: 18\n",
      "batch 379, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 35\n",
      "batch 399, loss: 0.1686, instance_loss: 0.0385, weighted_loss: 0.1296, label: 0, bag_size: 95\n",
      "batch 419, loss: 0.0124, instance_loss: 0.0057, weighted_loss: 0.0104, label: 1, bag_size: 53\n",
      "batch 439, loss: 1.3517, instance_loss: 2.8280, weighted_loss: 1.7946, label: 0, bag_size: 90\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0017, weighted_loss: 0.0005, label: 0, bag_size: 80\n",
      "batch 479, loss: 0.3439, instance_loss: 0.2193, weighted_loss: 0.3065, label: 0, bag_size: 24\n",
      "batch 499, loss: 0.0413, instance_loss: 0.0679, weighted_loss: 0.0493, label: 1, bag_size: 55\n",
      "batch 519, loss: 0.0932, instance_loss: 0.8230, weighted_loss: 0.3121, label: 0, bag_size: 29\n",
      "batch 539, loss: 0.0159, instance_loss: 0.0259, weighted_loss: 0.0189, label: 1, bag_size: 85\n",
      "batch 559, loss: 0.0002, instance_loss: 0.0488, weighted_loss: 0.0148, label: 0, bag_size: 78\n",
      "batch 579, loss: 0.0203, instance_loss: 0.2586, weighted_loss: 0.0918, label: 1, bag_size: 153\n",
      "batch 599, loss: 4.1999, instance_loss: 0.6966, weighted_loss: 3.1489, label: 1, bag_size: 42\n",
      "batch 619, loss: 0.3856, instance_loss: 0.9469, weighted_loss: 0.5540, label: 1, bag_size: 28\n",
      "batch 639, loss: 0.0049, instance_loss: 0.1878, weighted_loss: 0.0598, label: 1, bag_size: 41\n",
      "batch 659, loss: 0.0016, instance_loss: 0.0432, weighted_loss: 0.0140, label: 0, bag_size: 96\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.958239064856712: correct 10165/10608\n",
      "class 1 clustering acc 0.8058069381598794: correct 4274/5304\n",
      "Epoch: 22, train_loss: 0.3382, train_clustering_loss:  0.3676, train_error: 0.1192\n",
      "class 0: acc 0.8672839506172839, correct 281/324\n",
      "class 1: acc 0.8938053097345132, correct 303/339\n",
      "\n",
      "Val Set, val_loss: 0.6265, val_error: 0.2000, auc: 0.9139\n",
      "class 0 clustering acc 0.9382352941176471: correct 1276/1360\n",
      "class 1 clustering acc 0.7573529411764706: correct 515/680\n",
      "class 0: acc 0.9555555555555556, correct 43/45\n",
      "class 1: acc 0.625, correct 25/40\n",
      "EarlyStopping counter: 13 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0022, weighted_loss: 0.0007, label: 1, bag_size: 77\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 76\n",
      "batch 59, loss: 0.0017, instance_loss: 0.0219, weighted_loss: 0.0077, label: 1, bag_size: 71\n",
      "batch 79, loss: 0.2795, instance_loss: 0.6955, weighted_loss: 0.4043, label: 1, bag_size: 84\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0032, weighted_loss: 0.0009, label: 1, bag_size: 31\n",
      "batch 119, loss: 0.0055, instance_loss: 0.0735, weighted_loss: 0.0259, label: 1, bag_size: 89\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0026, weighted_loss: 0.0008, label: 1, bag_size: 71\n",
      "batch 159, loss: 0.1662, instance_loss: 0.0574, weighted_loss: 0.1336, label: 0, bag_size: 24\n",
      "batch 179, loss: 0.6923, instance_loss: 0.3721, weighted_loss: 0.5962, label: 0, bag_size: 107\n",
      "batch 199, loss: 0.0605, instance_loss: 0.2233, weighted_loss: 0.1093, label: 1, bag_size: 21\n",
      "batch 219, loss: 1.2572, instance_loss: 1.0747, weighted_loss: 1.2025, label: 1, bag_size: 31\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0066, weighted_loss: 0.0020, label: 1, bag_size: 56\n",
      "batch 259, loss: 0.1710, instance_loss: 1.3230, weighted_loss: 0.5166, label: 1, bag_size: 82\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 69\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0094, weighted_loss: 0.0028, label: 1, bag_size: 88\n",
      "batch 319, loss: 0.0036, instance_loss: 0.2172, weighted_loss: 0.0677, label: 1, bag_size: 69\n",
      "batch 339, loss: 1.4404, instance_loss: 2.3748, weighted_loss: 1.7207, label: 0, bag_size: 32\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0170, weighted_loss: 0.0051, label: 1, bag_size: 61\n",
      "batch 379, loss: 4.8761, instance_loss: 0.3917, weighted_loss: 3.5308, label: 1, bag_size: 83\n",
      "batch 399, loss: 0.0477, instance_loss: 0.0953, weighted_loss: 0.0619, label: 0, bag_size: 33\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0050, weighted_loss: 0.0015, label: 1, bag_size: 35\n",
      "batch 439, loss: 0.0005, instance_loss: 0.0013, weighted_loss: 0.0007, label: 0, bag_size: 78\n",
      "batch 459, loss: 1.6313, instance_loss: 0.0654, weighted_loss: 1.1615, label: 1, bag_size: 28\n",
      "batch 479, loss: 3.8703, instance_loss: 3.3426, weighted_loss: 3.7120, label: 0, bag_size: 56\n",
      "batch 499, loss: 0.0631, instance_loss: 0.2560, weighted_loss: 0.1209, label: 1, bag_size: 41\n",
      "batch 519, loss: 0.0021, instance_loss: 0.0092, weighted_loss: 0.0042, label: 1, bag_size: 94\n",
      "batch 539, loss: 0.0020, instance_loss: 0.0054, weighted_loss: 0.0030, label: 1, bag_size: 77\n",
      "batch 559, loss: 0.0004, instance_loss: 0.0147, weighted_loss: 0.0047, label: 0, bag_size: 63\n",
      "batch 579, loss: 0.0158, instance_loss: 0.0396, weighted_loss: 0.0230, label: 0, bag_size: 61\n",
      "batch 599, loss: 0.2053, instance_loss: 0.1470, weighted_loss: 0.1878, label: 0, bag_size: 70\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 58\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0039, weighted_loss: 0.0012, label: 0, bag_size: 42\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 72\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9667232277526395: correct 10255/10608\n",
      "class 1 clustering acc 0.8261689291101055: correct 4382/5304\n",
      "Epoch: 23, train_loss: 0.2876, train_clustering_loss:  0.3050, train_error: 0.0965\n",
      "class 0: acc 0.910828025477707, correct 286/314\n",
      "class 1: acc 0.8968481375358166, correct 313/349\n",
      "\n",
      "Val Set, val_loss: 1.1672, val_error: 0.3059, auc: 0.9186\n",
      "class 0 clustering acc 0.861764705882353: correct 1172/1360\n",
      "class 1 clustering acc 0.6529411764705882: correct 444/680\n",
      "class 0: acc 0.4444444444444444, correct 20/45\n",
      "class 1: acc 0.975, correct 39/40\n",
      "EarlyStopping counter: 14 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.0330, instance_loss: 0.7009, weighted_loss: 0.9333, label: 1, bag_size: 68\n",
      "batch 39, loss: 0.0002, instance_loss: 0.0033, weighted_loss: 0.0011, label: 1, bag_size: 123\n",
      "batch 59, loss: 0.0002, instance_loss: 0.0035, weighted_loss: 0.0012, label: 0, bag_size: 53\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0409, weighted_loss: 0.0123, label: 1, bag_size: 71\n",
      "batch 99, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 86\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0016, weighted_loss: 0.0005, label: 0, bag_size: 21\n",
      "batch 139, loss: 0.0507, instance_loss: 0.0372, weighted_loss: 0.0467, label: 0, bag_size: 30\n",
      "batch 159, loss: 0.0773, instance_loss: 0.6042, weighted_loss: 0.2354, label: 0, bag_size: 20\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 72\n",
      "batch 199, loss: 1.1838, instance_loss: 1.7180, weighted_loss: 1.3441, label: 1, bag_size: 57\n",
      "batch 219, loss: 0.3128, instance_loss: 0.0672, weighted_loss: 0.2391, label: 1, bag_size: 80\n",
      "batch 239, loss: 0.1293, instance_loss: 0.4601, weighted_loss: 0.2285, label: 1, bag_size: 24\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 123\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 95\n",
      "batch 299, loss: 0.0025, instance_loss: 0.0087, weighted_loss: 0.0044, label: 1, bag_size: 95\n",
      "batch 319, loss: 0.1735, instance_loss: 0.8834, weighted_loss: 0.3865, label: 1, bag_size: 35\n",
      "batch 339, loss: 0.0006, instance_loss: 0.0032, weighted_loss: 0.0014, label: 1, bag_size: 84\n",
      "batch 359, loss: 0.1433, instance_loss: 0.2190, weighted_loss: 0.1660, label: 0, bag_size: 33\n",
      "batch 379, loss: 0.0569, instance_loss: 0.0225, weighted_loss: 0.0466, label: 0, bag_size: 29\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 94\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 66\n",
      "batch 439, loss: 0.0002, instance_loss: 0.0001, weighted_loss: 0.0002, label: 0, bag_size: 65\n",
      "batch 459, loss: 0.1416, instance_loss: 0.1746, weighted_loss: 0.1515, label: 0, bag_size: 38\n",
      "batch 479, loss: 0.0010, instance_loss: 0.0008, weighted_loss: 0.0010, label: 1, bag_size: 29\n",
      "batch 499, loss: 0.0053, instance_loss: 0.0000, weighted_loss: 0.0037, label: 1, bag_size: 84\n",
      "batch 519, loss: 0.0009, instance_loss: 0.0053, weighted_loss: 0.0022, label: 1, bag_size: 49\n",
      "batch 539, loss: 1.7739, instance_loss: 1.3247, weighted_loss: 1.6392, label: 1, bag_size: 52\n",
      "batch 559, loss: 0.0751, instance_loss: 0.0006, weighted_loss: 0.0527, label: 0, bag_size: 35\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0036, weighted_loss: 0.0011, label: 0, bag_size: 103\n",
      "batch 599, loss: 0.0008, instance_loss: 0.0059, weighted_loss: 0.0023, label: 1, bag_size: 17\n",
      "batch 619, loss: 0.0016, instance_loss: 0.0004, weighted_loss: 0.0012, label: 0, bag_size: 48\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 73\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 30\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9692684766214178: correct 10282/10608\n",
      "class 1 clustering acc 0.8384238310708899: correct 4447/5304\n",
      "Epoch: 24, train_loss: 0.2234, train_clustering_loss:  0.2785, train_error: 0.0905\n",
      "class 0: acc 0.9209039548022598, correct 326/354\n",
      "class 1: acc 0.8964401294498382, correct 277/309\n",
      "\n",
      "Val Set, val_loss: 1.3006, val_error: 0.3176, auc: 0.9144\n",
      "class 0 clustering acc 0.8705882352941177: correct 1184/1360\n",
      "class 1 clustering acc 0.6544117647058824: correct 445/680\n",
      "class 0: acc 0.4444444444444444, correct 20/45\n",
      "class 1: acc 0.95, correct 38/40\n",
      "EarlyStopping counter: 15 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0055, instance_loss: 0.2934, weighted_loss: 0.0918, label: 0, bag_size: 93\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 43\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 78\n",
      "batch 79, loss: 0.0013, instance_loss: 0.0512, weighted_loss: 0.0163, label: 1, bag_size: 100\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 61\n",
      "batch 119, loss: 0.0008, instance_loss: 0.0259, weighted_loss: 0.0083, label: 0, bag_size: 86\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0031, weighted_loss: 0.0010, label: 0, bag_size: 93\n",
      "batch 159, loss: 2.1473, instance_loss: 4.0885, weighted_loss: 2.7296, label: 1, bag_size: 51\n",
      "batch 179, loss: 0.0012, instance_loss: 0.0148, weighted_loss: 0.0053, label: 1, bag_size: 92\n",
      "batch 199, loss: 2.9698, instance_loss: 1.6617, weighted_loss: 2.5774, label: 0, bag_size: 58\n",
      "batch 219, loss: 0.0014, instance_loss: 0.0025, weighted_loss: 0.0017, label: 0, bag_size: 93\n",
      "batch 239, loss: 0.2774, instance_loss: 0.0956, weighted_loss: 0.2228, label: 0, bag_size: 103\n",
      "batch 259, loss: 0.0004, instance_loss: 0.0042, weighted_loss: 0.0015, label: 1, bag_size: 48\n",
      "batch 279, loss: 0.0004, instance_loss: 0.0702, weighted_loss: 0.0213, label: 1, bag_size: 62\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 75\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0764, weighted_loss: 0.0229, label: 0, bag_size: 80\n",
      "batch 339, loss: 0.0141, instance_loss: 0.3574, weighted_loss: 0.1171, label: 0, bag_size: 35\n",
      "batch 359, loss: 0.0408, instance_loss: 0.1171, weighted_loss: 0.0637, label: 1, bag_size: 48\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 46\n",
      "batch 399, loss: 0.0001, instance_loss: 0.0137, weighted_loss: 0.0042, label: 1, bag_size: 109\n",
      "batch 419, loss: 0.0724, instance_loss: 0.1430, weighted_loss: 0.0935, label: 1, bag_size: 107\n",
      "batch 439, loss: 2.0409, instance_loss: 0.4705, weighted_loss: 1.5698, label: 1, bag_size: 16\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0843, weighted_loss: 0.0253, label: 0, bag_size: 50\n",
      "batch 479, loss: 0.0008, instance_loss: 0.0245, weighted_loss: 0.0079, label: 0, bag_size: 66\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0030, weighted_loss: 0.0009, label: 0, bag_size: 72\n",
      "batch 519, loss: 0.0306, instance_loss: 0.0341, weighted_loss: 0.0316, label: 0, bag_size: 33\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0071, weighted_loss: 0.0021, label: 1, bag_size: 35\n",
      "batch 559, loss: 0.0572, instance_loss: 0.7045, weighted_loss: 0.2514, label: 0, bag_size: 50\n",
      "batch 579, loss: 0.0040, instance_loss: 0.1543, weighted_loss: 0.0491, label: 0, bag_size: 63\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0017, weighted_loss: 0.0005, label: 1, bag_size: 38\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 0, bag_size: 46\n",
      "batch 639, loss: 0.0006, instance_loss: 0.0035, weighted_loss: 0.0014, label: 0, bag_size: 48\n",
      "batch 659, loss: 0.0007, instance_loss: 0.0162, weighted_loss: 0.0054, label: 1, bag_size: 70\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9689856711915535: correct 10279/10608\n",
      "class 1 clustering acc 0.8503016591251885: correct 4510/5304\n",
      "Epoch: 25, train_loss: 0.2891, train_clustering_loss:  0.2899, train_error: 0.0965\n",
      "class 0: acc 0.8988439306358381, correct 311/346\n",
      "class 1: acc 0.9085173501577287, correct 288/317\n",
      "\n",
      "Val Set, val_loss: 0.6915, val_error: 0.1412, auc: 0.9214\n",
      "class 0 clustering acc 0.9367647058823529: correct 1274/1360\n",
      "class 1 clustering acc 0.7808823529411765: correct 531/680\n",
      "class 0: acc 0.8222222222222222, correct 37/45\n",
      "class 1: acc 0.9, correct 36/40\n",
      "EarlyStopping counter: 16 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 7.0715, instance_loss: 7.8256, weighted_loss: 7.2977, label: 0, bag_size: 66\n",
      "batch 39, loss: 0.0004, instance_loss: 0.0004, weighted_loss: 0.0004, label: 0, bag_size: 107\n",
      "batch 59, loss: 0.0076, instance_loss: 0.1726, weighted_loss: 0.0571, label: 1, bag_size: 30\n",
      "batch 79, loss: 0.0008, instance_loss: 0.3783, weighted_loss: 0.1141, label: 1, bag_size: 18\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0003, weighted_loss: 0.0002, label: 1, bag_size: 79\n",
      "batch 119, loss: 0.0417, instance_loss: 0.0258, weighted_loss: 0.0370, label: 1, bag_size: 53\n",
      "batch 139, loss: 0.0275, instance_loss: 0.0069, weighted_loss: 0.0214, label: 0, bag_size: 116\n",
      "batch 159, loss: 0.3901, instance_loss: 1.7954, weighted_loss: 0.8117, label: 1, bag_size: 59\n",
      "batch 179, loss: 0.0032, instance_loss: 0.0517, weighted_loss: 0.0178, label: 1, bag_size: 17\n",
      "batch 199, loss: 0.0015, instance_loss: 0.0281, weighted_loss: 0.0095, label: 0, bag_size: 107\n",
      "batch 219, loss: 0.4702, instance_loss: 0.3662, weighted_loss: 0.4390, label: 1, bag_size: 52\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 35\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 49\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0088, weighted_loss: 0.0026, label: 0, bag_size: 65\n",
      "batch 299, loss: 0.0066, instance_loss: 0.0370, weighted_loss: 0.0157, label: 0, bag_size: 48\n",
      "batch 319, loss: 0.0092, instance_loss: 0.0256, weighted_loss: 0.0141, label: 1, bag_size: 88\n",
      "batch 339, loss: 0.0195, instance_loss: 0.0284, weighted_loss: 0.0222, label: 0, bag_size: 96\n",
      "batch 359, loss: 0.0028, instance_loss: 0.1410, weighted_loss: 0.0443, label: 1, bag_size: 40\n",
      "batch 379, loss: 0.0842, instance_loss: 0.2618, weighted_loss: 0.1375, label: 1, bag_size: 32\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 102\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 100\n",
      "batch 439, loss: 0.0026, instance_loss: 0.0881, weighted_loss: 0.0282, label: 0, bag_size: 109\n",
      "batch 459, loss: 0.0873, instance_loss: 0.0701, weighted_loss: 0.0822, label: 1, bag_size: 62\n",
      "batch 479, loss: 2.0918, instance_loss: 2.7839, weighted_loss: 2.2994, label: 0, bag_size: 35\n",
      "batch 499, loss: 0.0061, instance_loss: 0.0442, weighted_loss: 0.0175, label: 1, bag_size: 30\n",
      "batch 519, loss: 0.6567, instance_loss: 0.5144, weighted_loss: 0.6140, label: 1, bag_size: 84\n",
      "batch 539, loss: 3.6328, instance_loss: 0.2464, weighted_loss: 2.6168, label: 0, bag_size: 57\n",
      "batch 559, loss: 1.3091, instance_loss: 0.4404, weighted_loss: 1.0485, label: 1, bag_size: 65\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 71\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 116\n",
      "batch 619, loss: 0.0055, instance_loss: 0.0395, weighted_loss: 0.0157, label: 1, bag_size: 88\n",
      "batch 639, loss: 0.0004, instance_loss: 0.0545, weighted_loss: 0.0166, label: 1, bag_size: 29\n",
      "batch 659, loss: 0.3383, instance_loss: 0.1236, weighted_loss: 0.2739, label: 1, bag_size: 48\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9683257918552036: correct 10272/10608\n",
      "class 1 clustering acc 0.8465309200603318: correct 4490/5304\n",
      "Epoch: 26, train_loss: 0.3412, train_clustering_loss:  0.3021, train_error: 0.0995\n",
      "class 0: acc 0.9017857142857143, correct 303/336\n",
      "class 1: acc 0.8990825688073395, correct 294/327\n",
      "\n",
      "Val Set, val_loss: 0.5505, val_error: 0.1294, auc: 0.9311\n",
      "class 0 clustering acc 0.9602941176470589: correct 1306/1360\n",
      "class 1 clustering acc 0.7455882352941177: correct 507/680\n",
      "class 0: acc 0.9555555555555556, correct 43/45\n",
      "class 1: acc 0.775, correct 31/40\n",
      "EarlyStopping counter: 17 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0027, instance_loss: 0.0319, weighted_loss: 0.0115, label: 0, bag_size: 68\n",
      "batch 39, loss: 0.0011, instance_loss: 0.0044, weighted_loss: 0.0021, label: 1, bag_size: 38\n",
      "batch 59, loss: 0.0048, instance_loss: 0.0058, weighted_loss: 0.0051, label: 1, bag_size: 68\n",
      "batch 79, loss: 0.7821, instance_loss: 2.2631, weighted_loss: 1.2264, label: 0, bag_size: 28\n",
      "batch 99, loss: 0.0700, instance_loss: 0.0000, weighted_loss: 0.0490, label: 0, bag_size: 61\n",
      "batch 119, loss: 0.0747, instance_loss: 0.1413, weighted_loss: 0.0947, label: 0, bag_size: 89\n",
      "batch 139, loss: 0.0034, instance_loss: 0.0180, weighted_loss: 0.0078, label: 0, bag_size: 91\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0007, weighted_loss: 0.0003, label: 0, bag_size: 85\n",
      "batch 179, loss: 0.7128, instance_loss: 0.6692, weighted_loss: 0.6997, label: 0, bag_size: 18\n",
      "batch 199, loss: 0.0003, instance_loss: 0.1470, weighted_loss: 0.0443, label: 0, bag_size: 28\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0958, weighted_loss: 0.0288, label: 0, bag_size: 36\n",
      "batch 239, loss: 0.0004, instance_loss: 0.0883, weighted_loss: 0.0267, label: 0, bag_size: 78\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0271, weighted_loss: 0.0081, label: 0, bag_size: 103\n",
      "batch 279, loss: 2.0512, instance_loss: 0.5501, weighted_loss: 1.6009, label: 1, bag_size: 93\n",
      "batch 299, loss: 0.0010, instance_loss: 0.2268, weighted_loss: 0.0687, label: 1, bag_size: 85\n",
      "batch 319, loss: 0.0017, instance_loss: 0.0007, weighted_loss: 0.0014, label: 0, bag_size: 110\n",
      "batch 339, loss: 5.0550, instance_loss: 1.1650, weighted_loss: 3.8880, label: 1, bag_size: 100\n",
      "batch 359, loss: 0.0530, instance_loss: 0.4152, weighted_loss: 0.1617, label: 1, bag_size: 32\n",
      "batch 379, loss: 1.5761, instance_loss: 0.2565, weighted_loss: 1.1802, label: 1, bag_size: 35\n",
      "batch 399, loss: 0.0260, instance_loss: 0.2318, weighted_loss: 0.0877, label: 1, bag_size: 18\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0088, weighted_loss: 0.0026, label: 1, bag_size: 50\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 41\n",
      "batch 459, loss: 0.0003, instance_loss: 0.0674, weighted_loss: 0.0204, label: 0, bag_size: 30\n",
      "batch 479, loss: 3.0570, instance_loss: 0.1097, weighted_loss: 2.1728, label: 1, bag_size: 96\n",
      "batch 499, loss: 10.1070, instance_loss: 2.9852, weighted_loss: 7.9704, label: 1, bag_size: 51\n",
      "batch 519, loss: 0.0003, instance_loss: 0.0013, weighted_loss: 0.0006, label: 0, bag_size: 78\n",
      "batch 539, loss: 0.0027, instance_loss: 0.0986, weighted_loss: 0.0315, label: 1, bag_size: 83\n",
      "batch 559, loss: 0.0044, instance_loss: 0.1979, weighted_loss: 0.0624, label: 1, bag_size: 86\n",
      "batch 579, loss: 0.0002, instance_loss: 0.0080, weighted_loss: 0.0025, label: 0, bag_size: 73\n",
      "batch 599, loss: 0.0000, instance_loss: 0.4489, weighted_loss: 0.1347, label: 0, bag_size: 64\n",
      "batch 619, loss: 0.7683, instance_loss: 0.1274, weighted_loss: 0.5760, label: 0, bag_size: 54\n",
      "batch 639, loss: 4.1952, instance_loss: 2.6861, weighted_loss: 3.7425, label: 1, bag_size: 96\n",
      "batch 659, loss: 0.0028, instance_loss: 0.0318, weighted_loss: 0.0115, label: 0, bag_size: 40\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9555995475113123: correct 10137/10608\n",
      "class 1 clustering acc 0.7875188536953243: correct 4177/5304\n",
      "Epoch: 27, train_loss: 0.7792, train_clustering_loss:  0.4000, train_error: 0.1644\n",
      "class 0: acc 0.8397626112759644, correct 283/337\n",
      "class 1: acc 0.8312883435582822, correct 271/326\n",
      "\n",
      "Val Set, val_loss: 0.6089, val_error: 0.1412, auc: 0.9267\n",
      "class 0 clustering acc 0.9441176470588235: correct 1284/1360\n",
      "class 1 clustering acc 0.7323529411764705: correct 498/680\n",
      "class 0: acc 0.8666666666666667, correct 39/45\n",
      "class 1: acc 0.85, correct 34/40\n",
      "EarlyStopping counter: 18 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0258, instance_loss: 0.0249, weighted_loss: 0.0256, label: 1, bag_size: 57\n",
      "batch 39, loss: 0.0011, instance_loss: 0.0711, weighted_loss: 0.0221, label: 0, bag_size: 46\n",
      "batch 59, loss: 0.0019, instance_loss: 0.0446, weighted_loss: 0.0147, label: 1, bag_size: 153\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 97\n",
      "batch 99, loss: 0.0473, instance_loss: 1.1866, weighted_loss: 0.3891, label: 1, bag_size: 69\n",
      "batch 119, loss: 0.0102, instance_loss: 0.7311, weighted_loss: 0.2264, label: 0, bag_size: 116\n",
      "batch 139, loss: 0.0004, instance_loss: 0.0399, weighted_loss: 0.0123, label: 0, bag_size: 114\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0085, weighted_loss: 0.0026, label: 1, bag_size: 84\n",
      "batch 179, loss: 0.0934, instance_loss: 0.9658, weighted_loss: 0.3551, label: 1, bag_size: 73\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 35\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0037, weighted_loss: 0.0011, label: 1, bag_size: 80\n",
      "batch 239, loss: 0.0027, instance_loss: 0.0324, weighted_loss: 0.0116, label: 0, bag_size: 79\n",
      "batch 259, loss: 0.0005, instance_loss: 0.0311, weighted_loss: 0.0097, label: 0, bag_size: 65\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0155, weighted_loss: 0.0047, label: 0, bag_size: 35\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 1, bag_size: 81\n",
      "batch 319, loss: 1.0321, instance_loss: 0.6391, weighted_loss: 0.9142, label: 1, bag_size: 69\n",
      "batch 339, loss: 0.0045, instance_loss: 0.0144, weighted_loss: 0.0075, label: 0, bag_size: 64\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0028, weighted_loss: 0.0008, label: 1, bag_size: 94\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0101, weighted_loss: 0.0030, label: 0, bag_size: 67\n",
      "batch 399, loss: 0.0034, instance_loss: 0.0099, weighted_loss: 0.0054, label: 0, bag_size: 107\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0179, weighted_loss: 0.0054, label: 0, bag_size: 97\n",
      "batch 439, loss: 0.0002, instance_loss: 0.0056, weighted_loss: 0.0018, label: 1, bag_size: 60\n",
      "batch 459, loss: 0.0029, instance_loss: 0.2808, weighted_loss: 0.0863, label: 1, bag_size: 62\n",
      "batch 479, loss: 0.0267, instance_loss: 0.0976, weighted_loss: 0.0480, label: 0, bag_size: 86\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 1, bag_size: 31\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 43\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0567, weighted_loss: 0.0170, label: 1, bag_size: 72\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0031, weighted_loss: 0.0009, label: 0, bag_size: 89\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0217, weighted_loss: 0.0065, label: 0, bag_size: 77\n",
      "batch 599, loss: 0.5109, instance_loss: 0.3639, weighted_loss: 0.4668, label: 0, bag_size: 55\n",
      "batch 619, loss: 0.0039, instance_loss: 0.0608, weighted_loss: 0.0210, label: 0, bag_size: 21\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0047, weighted_loss: 0.0014, label: 1, bag_size: 44\n",
      "batch 659, loss: 0.1312, instance_loss: 0.3206, weighted_loss: 0.1880, label: 1, bag_size: 47\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9637066365007542: correct 10223/10608\n",
      "class 1 clustering acc 0.8171191553544495: correct 4334/5304\n",
      "Epoch: 28, train_loss: 0.3570, train_clustering_loss:  0.3368, train_error: 0.1011\n",
      "class 0: acc 0.9029411764705882, correct 307/340\n",
      "class 1: acc 0.8947368421052632, correct 289/323\n",
      "\n",
      "Val Set, val_loss: 0.5804, val_error: 0.1412, auc: 0.9144\n",
      "class 0 clustering acc 0.9352941176470588: correct 1272/1360\n",
      "class 1 clustering acc 0.8205882352941176: correct 558/680\n",
      "class 0: acc 0.8444444444444444, correct 38/45\n",
      "class 1: acc 0.875, correct 35/40\n",
      "EarlyStopping counter: 19 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0605, instance_loss: 0.1540, weighted_loss: 0.0885, label: 1, bag_size: 153\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 45\n",
      "batch 59, loss: 0.0180, instance_loss: 0.0161, weighted_loss: 0.0174, label: 0, bag_size: 25\n",
      "batch 79, loss: 0.0008, instance_loss: 0.0227, weighted_loss: 0.0074, label: 0, bag_size: 95\n",
      "batch 99, loss: 0.0005, instance_loss: 0.0052, weighted_loss: 0.0019, label: 0, bag_size: 91\n",
      "batch 119, loss: 0.0000, instance_loss: 0.1424, weighted_loss: 0.0427, label: 1, bag_size: 69\n",
      "batch 139, loss: 1.0116, instance_loss: 0.6646, weighted_loss: 0.9075, label: 0, bag_size: 19\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 43\n",
      "batch 179, loss: 0.0000, instance_loss: 0.1341, weighted_loss: 0.0402, label: 1, bag_size: 110\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0029, weighted_loss: 0.0009, label: 0, bag_size: 74\n",
      "batch 219, loss: 0.0011, instance_loss: 0.0310, weighted_loss: 0.0101, label: 1, bag_size: 78\n",
      "batch 239, loss: 9.9006, instance_loss: 3.7168, weighted_loss: 8.0454, label: 0, bag_size: 32\n",
      "batch 259, loss: 0.0104, instance_loss: 0.8810, weighted_loss: 0.2716, label: 1, bag_size: 74\n",
      "batch 279, loss: 3.0331, instance_loss: 0.4341, weighted_loss: 2.2534, label: 1, bag_size: 45\n",
      "batch 299, loss: 0.0012, instance_loss: 0.0015, weighted_loss: 0.0013, label: 0, bag_size: 75\n",
      "batch 319, loss: 0.0838, instance_loss: 0.0026, weighted_loss: 0.0594, label: 1, bag_size: 48\n",
      "batch 339, loss: 0.0002, instance_loss: 0.0308, weighted_loss: 0.0094, label: 0, bag_size: 64\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0449, weighted_loss: 0.0135, label: 1, bag_size: 39\n",
      "batch 379, loss: 2.7387, instance_loss: 2.5342, weighted_loss: 2.6773, label: 0, bag_size: 73\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0566, weighted_loss: 0.0170, label: 1, bag_size: 114\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 93\n",
      "batch 439, loss: 0.0003, instance_loss: 0.0267, weighted_loss: 0.0082, label: 1, bag_size: 99\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0005, label: 1, bag_size: 114\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0359, weighted_loss: 0.0108, label: 1, bag_size: 25\n",
      "batch 499, loss: 0.0058, instance_loss: 0.4809, weighted_loss: 0.1483, label: 0, bag_size: 103\n",
      "batch 519, loss: 0.0244, instance_loss: 0.2831, weighted_loss: 0.1020, label: 1, bag_size: 82\n",
      "batch 539, loss: 0.1290, instance_loss: 0.4852, weighted_loss: 0.2359, label: 1, bag_size: 67\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0437, weighted_loss: 0.0132, label: 0, bag_size: 87\n",
      "batch 579, loss: 0.0006, instance_loss: 0.0058, weighted_loss: 0.0021, label: 0, bag_size: 75\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0255, weighted_loss: 0.0077, label: 0, bag_size: 22\n",
      "batch 619, loss: 0.0210, instance_loss: 0.0148, weighted_loss: 0.0191, label: 0, bag_size: 36\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 88\n",
      "batch 659, loss: 5.3500, instance_loss: 3.4454, weighted_loss: 4.7786, label: 0, bag_size: 73\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9646493212669683: correct 10233/10608\n",
      "class 1 clustering acc 0.8227752639517345: correct 4364/5304\n",
      "Epoch: 29, train_loss: 0.5495, train_clustering_loss:  0.3349, train_error: 0.1327\n",
      "class 0: acc 0.8698224852071006, correct 294/338\n",
      "class 1: acc 0.8646153846153846, correct 281/325\n",
      "\n",
      "Val Set, val_loss: 0.5956, val_error: 0.1294, auc: 0.9122\n",
      "class 0 clustering acc 0.95: correct 1292/1360\n",
      "class 1 clustering acc 0.8308823529411765: correct 565/680\n",
      "class 0: acc 0.9111111111111111, correct 41/45\n",
      "class 1: acc 0.825, correct 33/40\n",
      "EarlyStopping counter: 20 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0732, instance_loss: 1.8223, weighted_loss: 0.5979, label: 0, bag_size: 35\n",
      "batch 39, loss: 0.0011, instance_loss: 0.0627, weighted_loss: 0.0196, label: 0, bag_size: 90\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0161, weighted_loss: 0.0048, label: 1, bag_size: 31\n",
      "batch 79, loss: 0.0142, instance_loss: 0.2046, weighted_loss: 0.0714, label: 1, bag_size: 99\n",
      "batch 99, loss: 0.0002, instance_loss: 0.0154, weighted_loss: 0.0048, label: 1, bag_size: 49\n",
      "batch 119, loss: 0.0064, instance_loss: 0.0823, weighted_loss: 0.0292, label: 1, bag_size: 53\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 88\n",
      "batch 159, loss: 0.1245, instance_loss: 0.4600, weighted_loss: 0.2251, label: 1, bag_size: 51\n",
      "batch 179, loss: 0.0048, instance_loss: 0.0423, weighted_loss: 0.0160, label: 1, bag_size: 100\n",
      "batch 199, loss: 0.1443, instance_loss: 0.0427, weighted_loss: 0.1138, label: 1, bag_size: 42\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 93\n",
      "batch 239, loss: 0.0003, instance_loss: 0.0053, weighted_loss: 0.0018, label: 1, bag_size: 78\n",
      "batch 259, loss: 0.1328, instance_loss: 0.1964, weighted_loss: 0.1519, label: 1, bag_size: 153\n",
      "batch 279, loss: 0.0900, instance_loss: 0.0161, weighted_loss: 0.0678, label: 0, bag_size: 63\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 21\n",
      "batch 319, loss: 0.0331, instance_loss: 0.3659, weighted_loss: 0.1330, label: 0, bag_size: 33\n",
      "batch 339, loss: 0.0012, instance_loss: 0.0320, weighted_loss: 0.0104, label: 1, bag_size: 89\n",
      "batch 359, loss: 0.1281, instance_loss: 0.1953, weighted_loss: 0.1483, label: 1, bag_size: 78\n",
      "batch 379, loss: 0.1016, instance_loss: 1.4877, weighted_loss: 0.5175, label: 0, bag_size: 18\n",
      "batch 399, loss: 0.0680, instance_loss: 0.0474, weighted_loss: 0.0618, label: 1, bag_size: 63\n",
      "batch 419, loss: 0.5462, instance_loss: 1.8439, weighted_loss: 0.9355, label: 0, bag_size: 89\n",
      "batch 439, loss: 0.0002, instance_loss: 0.0129, weighted_loss: 0.0040, label: 1, bag_size: 76\n",
      "batch 459, loss: 0.1427, instance_loss: 0.4280, weighted_loss: 0.2283, label: 0, bag_size: 58\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0025, weighted_loss: 0.0008, label: 0, bag_size: 67\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 40\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0019, weighted_loss: 0.0006, label: 0, bag_size: 88\n",
      "batch 539, loss: 0.0116, instance_loss: 0.0179, weighted_loss: 0.0135, label: 1, bag_size: 107\n",
      "batch 559, loss: 0.0002, instance_loss: 0.0597, weighted_loss: 0.0180, label: 0, bag_size: 62\n",
      "batch 579, loss: 0.0412, instance_loss: 0.1035, weighted_loss: 0.0599, label: 0, bag_size: 49\n",
      "batch 599, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 103\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 94\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0014, weighted_loss: 0.0005, label: 0, bag_size: 97\n",
      "batch 659, loss: 0.0012, instance_loss: 0.0009, weighted_loss: 0.0011, label: 0, bag_size: 78\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9706825037707391: correct 10297/10608\n",
      "class 1 clustering acc 0.8582202111613876: correct 4552/5304\n",
      "Epoch: 30, train_loss: 0.3032, train_clustering_loss:  0.2763, train_error: 0.1041\n",
      "class 0: acc 0.8978328173374613, correct 290/323\n",
      "class 1: acc 0.8941176470588236, correct 304/340\n",
      "\n",
      "Val Set, val_loss: 0.7554, val_error: 0.1647, auc: 0.9317\n",
      "class 0 clustering acc 0.924264705882353: correct 1257/1360\n",
      "class 1 clustering acc 0.8205882352941176: correct 558/680\n",
      "class 0: acc 0.9777777777777777, correct 44/45\n",
      "class 1: acc 0.675, correct 27/40\n",
      "EarlyStopping counter: 21 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 63\n",
      "batch 39, loss: 0.0047, instance_loss: 0.0623, weighted_loss: 0.0220, label: 0, bag_size: 75\n",
      "batch 59, loss: 0.0350, instance_loss: 0.0022, weighted_loss: 0.0252, label: 0, bag_size: 62\n",
      "batch 79, loss: 0.0008, instance_loss: 0.0051, weighted_loss: 0.0021, label: 0, bag_size: 26\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0005, label: 0, bag_size: 103\n",
      "batch 119, loss: 0.0108, instance_loss: 0.0477, weighted_loss: 0.0219, label: 0, bag_size: 87\n",
      "batch 139, loss: 1.8437, instance_loss: 2.0145, weighted_loss: 1.8950, label: 1, bag_size: 96\n",
      "batch 159, loss: 0.0013, instance_loss: 0.0066, weighted_loss: 0.0029, label: 0, bag_size: 30\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 78\n",
      "batch 199, loss: 0.0079, instance_loss: 0.0019, weighted_loss: 0.0061, label: 1, bag_size: 100\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0060, weighted_loss: 0.0018, label: 0, bag_size: 91\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 42\n",
      "batch 259, loss: 0.0011, instance_loss: 0.0161, weighted_loss: 0.0056, label: 1, bag_size: 91\n",
      "batch 279, loss: 0.8293, instance_loss: 1.0224, weighted_loss: 0.8872, label: 0, bag_size: 35\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 25\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 65\n",
      "batch 339, loss: 0.0497, instance_loss: 0.1933, weighted_loss: 0.0928, label: 0, bag_size: 82\n",
      "batch 359, loss: 0.3330, instance_loss: 0.2203, weighted_loss: 0.2992, label: 1, bag_size: 57\n",
      "batch 379, loss: 0.0035, instance_loss: 0.0011, weighted_loss: 0.0028, label: 0, bag_size: 103\n",
      "batch 399, loss: 0.0019, instance_loss: 0.0018, weighted_loss: 0.0019, label: 0, bag_size: 35\n",
      "batch 419, loss: 0.0001, instance_loss: 0.0056, weighted_loss: 0.0017, label: 0, bag_size: 94\n",
      "batch 439, loss: 0.0045, instance_loss: 0.0006, weighted_loss: 0.0034, label: 0, bag_size: 67\n",
      "batch 459, loss: 0.0039, instance_loss: 0.2170, weighted_loss: 0.0678, label: 0, bag_size: 42\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0474, weighted_loss: 0.0142, label: 0, bag_size: 63\n",
      "batch 499, loss: 0.0159, instance_loss: 0.9976, weighted_loss: 0.3104, label: 0, bag_size: 89\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0005, label: 1, bag_size: 140\n",
      "batch 539, loss: 0.4542, instance_loss: 1.0587, weighted_loss: 0.6356, label: 0, bag_size: 33\n",
      "batch 559, loss: 0.0241, instance_loss: 0.1700, weighted_loss: 0.0679, label: 0, bag_size: 33\n",
      "batch 579, loss: 1.2488, instance_loss: 0.8773, weighted_loss: 1.1374, label: 1, bag_size: 60\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0133, weighted_loss: 0.0041, label: 0, bag_size: 37\n",
      "batch 619, loss: 0.0021, instance_loss: 0.0055, weighted_loss: 0.0031, label: 0, bag_size: 28\n",
      "batch 639, loss: 0.0179, instance_loss: 0.0118, weighted_loss: 0.0161, label: 0, bag_size: 98\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0020, weighted_loss: 0.0006, label: 1, bag_size: 31\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9663461538461539: correct 10251/10608\n",
      "class 1 clustering acc 0.8363499245852187: correct 4436/5304\n",
      "Epoch: 31, train_loss: 0.3098, train_clustering_loss:  0.2982, train_error: 0.0920\n",
      "class 0: acc 0.9054878048780488, correct 297/328\n",
      "class 1: acc 0.9104477611940298, correct 305/335\n",
      "\n",
      "Val Set, val_loss: 0.8748, val_error: 0.1765, auc: 0.9156\n",
      "class 0 clustering acc 0.9426470588235294: correct 1282/1360\n",
      "class 1 clustering acc 0.7955882352941176: correct 541/680\n",
      "class 0: acc 0.9555555555555556, correct 43/45\n",
      "class 1: acc 0.675, correct 27/40\n",
      "EarlyStopping counter: 22 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 64\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 0, bag_size: 101\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 67\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0020, weighted_loss: 0.0006, label: 0, bag_size: 48\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 61\n",
      "batch 119, loss: 3.4089, instance_loss: 8.1967, weighted_loss: 4.8453, label: 1, bag_size: 55\n",
      "batch 139, loss: 4.2720, instance_loss: 1.2984, weighted_loss: 3.3799, label: 1, bag_size: 24\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0052, weighted_loss: 0.0016, label: 1, bag_size: 111\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0308, weighted_loss: 0.0093, label: 0, bag_size: 43\n",
      "batch 199, loss: 0.0092, instance_loss: 0.0546, weighted_loss: 0.0228, label: 0, bag_size: 50\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 49\n",
      "batch 239, loss: 0.0011, instance_loss: 0.0090, weighted_loss: 0.0035, label: 0, bag_size: 40\n",
      "batch 259, loss: 0.0009, instance_loss: 0.0591, weighted_loss: 0.0183, label: 1, bag_size: 83\n",
      "batch 279, loss: 0.0002, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 62\n",
      "batch 299, loss: 0.2362, instance_loss: 0.3642, weighted_loss: 0.2746, label: 0, bag_size: 28\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0025, weighted_loss: 0.0008, label: 0, bag_size: 38\n",
      "batch 339, loss: 0.0641, instance_loss: 0.0214, weighted_loss: 0.0513, label: 0, bag_size: 88\n",
      "batch 359, loss: 0.0013, instance_loss: 0.0688, weighted_loss: 0.0215, label: 1, bag_size: 31\n",
      "batch 379, loss: 0.0003, instance_loss: 0.0067, weighted_loss: 0.0022, label: 1, bag_size: 78\n",
      "batch 399, loss: 0.7561, instance_loss: 0.4970, weighted_loss: 0.6784, label: 0, bag_size: 50\n",
      "batch 419, loss: 0.0022, instance_loss: 0.0150, weighted_loss: 0.0060, label: 0, bag_size: 20\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0073, weighted_loss: 0.0022, label: 1, bag_size: 86\n",
      "batch 459, loss: 0.0024, instance_loss: 0.0362, weighted_loss: 0.0125, label: 0, bag_size: 62\n",
      "batch 479, loss: 0.3565, instance_loss: 0.0931, weighted_loss: 0.2774, label: 0, bag_size: 67\n",
      "batch 499, loss: 0.0258, instance_loss: 0.6230, weighted_loss: 0.2049, label: 1, bag_size: 53\n",
      "batch 519, loss: 0.0157, instance_loss: 0.1061, weighted_loss: 0.0429, label: 0, bag_size: 90\n",
      "batch 539, loss: 0.0002, instance_loss: 0.0026, weighted_loss: 0.0010, label: 0, bag_size: 65\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 126\n",
      "batch 579, loss: 0.0040, instance_loss: 0.0811, weighted_loss: 0.0271, label: 1, bag_size: 34\n",
      "batch 599, loss: 0.0469, instance_loss: 0.0167, weighted_loss: 0.0378, label: 1, bag_size: 21\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0005, weighted_loss: 0.0002, label: 1, bag_size: 120\n",
      "batch 639, loss: 0.0418, instance_loss: 0.1984, weighted_loss: 0.0888, label: 0, bag_size: 18\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0059, weighted_loss: 0.0018, label: 0, bag_size: 97\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9675716440422323: correct 10264/10608\n",
      "class 1 clustering acc 0.845211161387632: correct 4483/5304\n",
      "Epoch: 32, train_loss: 0.4514, train_clustering_loss:  0.3125, train_error: 0.1237\n",
      "class 0: acc 0.8858858858858859, correct 295/333\n",
      "class 1: acc 0.8666666666666667, correct 286/330\n",
      "\n",
      "Val Set, val_loss: 2.4349, val_error: 0.3294, auc: 0.9183\n",
      "class 0 clustering acc 0.924264705882353: correct 1257/1360\n",
      "class 1 clustering acc 0.7147058823529412: correct 486/680\n",
      "class 0: acc 0.37777777777777777, correct 17/45\n",
      "class 1: acc 1.0, correct 40/40\n",
      "EarlyStopping counter: 23 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 2.3069, instance_loss: 0.1223, weighted_loss: 1.6515, label: 1, bag_size: 17\n",
      "batch 39, loss: 6.6253, instance_loss: 0.9732, weighted_loss: 4.9297, label: 1, bag_size: 102\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0024, weighted_loss: 0.0007, label: 1, bag_size: 48\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 52\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 100\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0032, weighted_loss: 0.0010, label: 0, bag_size: 36\n",
      "batch 139, loss: 2.1810, instance_loss: 0.0536, weighted_loss: 1.5428, label: 1, bag_size: 42\n",
      "batch 159, loss: 2.3485, instance_loss: 0.0954, weighted_loss: 1.6726, label: 1, bag_size: 99\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 0, bag_size: 93\n",
      "batch 199, loss: 0.0000, instance_loss: 0.3306, weighted_loss: 0.0992, label: 0, bag_size: 107\n",
      "batch 219, loss: 0.0041, instance_loss: 0.0070, weighted_loss: 0.0050, label: 0, bag_size: 109\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0158, weighted_loss: 0.0047, label: 1, bag_size: 123\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0353, weighted_loss: 0.0106, label: 1, bag_size: 87\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 126\n",
      "batch 299, loss: 0.0263, instance_loss: 0.0489, weighted_loss: 0.0331, label: 1, bag_size: 44\n",
      "batch 319, loss: 0.0011, instance_loss: 0.0509, weighted_loss: 0.0160, label: 0, bag_size: 58\n",
      "batch 339, loss: 0.0257, instance_loss: 0.1283, weighted_loss: 0.0565, label: 1, bag_size: 84\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0139, weighted_loss: 0.0042, label: 1, bag_size: 55\n",
      "batch 379, loss: 0.0028, instance_loss: 0.4381, weighted_loss: 0.1334, label: 0, bag_size: 50\n",
      "batch 399, loss: 0.0369, instance_loss: 0.5908, weighted_loss: 0.2030, label: 0, bag_size: 89\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 20\n",
      "batch 439, loss: 0.8929, instance_loss: 0.1306, weighted_loss: 0.6642, label: 1, bag_size: 33\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 25\n",
      "batch 479, loss: 1.1318, instance_loss: 0.7323, weighted_loss: 1.0119, label: 1, bag_size: 61\n",
      "batch 499, loss: 0.0171, instance_loss: 0.0224, weighted_loss: 0.0187, label: 1, bag_size: 108\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 0, bag_size: 37\n",
      "batch 539, loss: 0.0871, instance_loss: 0.4819, weighted_loss: 0.2056, label: 0, bag_size: 81\n",
      "batch 559, loss: 0.0026, instance_loss: 0.0019, weighted_loss: 0.0024, label: 1, bag_size: 80\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0062, weighted_loss: 0.0019, label: 0, bag_size: 27\n",
      "batch 599, loss: 7.6324, instance_loss: 1.5770, weighted_loss: 5.8158, label: 0, bag_size: 35\n",
      "batch 619, loss: 1.0699, instance_loss: 0.0587, weighted_loss: 0.7665, label: 1, bag_size: 70\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0169, weighted_loss: 0.0051, label: 1, bag_size: 31\n",
      "batch 659, loss: 0.3489, instance_loss: 0.6324, weighted_loss: 0.4340, label: 1, bag_size: 52\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9654977375565611: correct 10242/10608\n",
      "class 1 clustering acc 0.8357843137254902: correct 4433/5304\n",
      "Epoch: 33, train_loss: 0.8554, train_clustering_loss:  0.3157, train_error: 0.1297\n",
      "class 0: acc 0.8753623188405797, correct 302/345\n",
      "class 1: acc 0.8647798742138365, correct 275/318\n",
      "\n",
      "Val Set, val_loss: 1.4546, val_error: 0.2000, auc: 0.9122\n",
      "class 0 clustering acc 0.9235294117647059: correct 1256/1360\n",
      "class 1 clustering acc 0.8014705882352942: correct 545/680\n",
      "class 0: acc 0.9777777777777777, correct 44/45\n",
      "class 1: acc 0.6, correct 24/40\n",
      "EarlyStopping counter: 24 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 30\n",
      "batch 39, loss: 0.0011, instance_loss: 0.0002, weighted_loss: 0.0008, label: 0, bag_size: 62\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 76\n",
      "batch 79, loss: 0.6011, instance_loss: 0.0234, weighted_loss: 0.4278, label: 1, bag_size: 70\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0097, weighted_loss: 0.0029, label: 1, bag_size: 91\n",
      "batch 119, loss: 0.0512, instance_loss: 0.1693, weighted_loss: 0.0866, label: 0, bag_size: 95\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 26\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 49\n",
      "batch 179, loss: 0.0003, instance_loss: 0.0018, weighted_loss: 0.0008, label: 1, bag_size: 75\n",
      "batch 199, loss: 0.0002, instance_loss: 0.0106, weighted_loss: 0.0033, label: 1, bag_size: 71\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 63\n",
      "batch 239, loss: 0.6510, instance_loss: 2.5002, weighted_loss: 1.2058, label: 1, bag_size: 38\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0061, weighted_loss: 0.0019, label: 1, bag_size: 84\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 0, bag_size: 12\n",
      "batch 299, loss: 0.0155, instance_loss: 1.3843, weighted_loss: 0.4261, label: 1, bag_size: 17\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0001, label: 0, bag_size: 48\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 29\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0115, weighted_loss: 0.0035, label: 0, bag_size: 107\n",
      "batch 379, loss: 0.0014, instance_loss: 0.0230, weighted_loss: 0.0078, label: 0, bag_size: 91\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 107\n",
      "batch 419, loss: 1.9171, instance_loss: 0.9494, weighted_loss: 1.6268, label: 0, bag_size: 77\n",
      "batch 439, loss: 4.8308, instance_loss: 0.4537, weighted_loss: 3.5177, label: 0, bag_size: 45\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 53\n",
      "batch 479, loss: 0.0000, instance_loss: 0.2033, weighted_loss: 0.0610, label: 0, bag_size: 75\n",
      "batch 499, loss: 5.6023, instance_loss: 3.4657, weighted_loss: 4.9613, label: 0, bag_size: 79\n",
      "batch 519, loss: 0.0104, instance_loss: 0.3225, weighted_loss: 0.1040, label: 1, bag_size: 79\n",
      "batch 539, loss: 0.0026, instance_loss: 0.1808, weighted_loss: 0.0560, label: 0, bag_size: 41\n",
      "batch 559, loss: 0.0018, instance_loss: 0.0014, weighted_loss: 0.0017, label: 0, bag_size: 65\n",
      "batch 579, loss: 0.1208, instance_loss: 0.3147, weighted_loss: 0.1790, label: 0, bag_size: 66\n",
      "batch 599, loss: 0.0151, instance_loss: 0.0800, weighted_loss: 0.0346, label: 1, bag_size: 36\n",
      "batch 619, loss: 0.0002, instance_loss: 0.0067, weighted_loss: 0.0022, label: 0, bag_size: 89\n",
      "batch 639, loss: 0.1090, instance_loss: 0.1816, weighted_loss: 0.1308, label: 1, bag_size: 35\n",
      "batch 659, loss: 0.0685, instance_loss: 0.1128, weighted_loss: 0.0818, label: 0, bag_size: 41\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.967948717948718: correct 10268/10608\n",
      "class 1 clustering acc 0.8504901960784313: correct 4511/5304\n",
      "Epoch: 34, train_loss: 0.3763, train_clustering_loss:  0.2762, train_error: 0.1056\n",
      "class 0: acc 0.8928571428571429, correct 300/336\n",
      "class 1: acc 0.8960244648318043, correct 293/327\n",
      "\n",
      "Val Set, val_loss: 1.0077, val_error: 0.1412, auc: 0.9206\n",
      "class 0 clustering acc 0.9602941176470589: correct 1306/1360\n",
      "class 1 clustering acc 0.7867647058823529: correct 535/680\n",
      "class 0: acc 0.8, correct 36/45\n",
      "class 1: acc 0.925, correct 37/40\n",
      "EarlyStopping counter: 25 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0013, instance_loss: 0.2640, weighted_loss: 0.0801, label: 1, bag_size: 17\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 83\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 36\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 56\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 54\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0024, weighted_loss: 0.0007, label: 0, bag_size: 94\n",
      "batch 139, loss: 0.0673, instance_loss: 0.1774, weighted_loss: 0.1003, label: 0, bag_size: 114\n",
      "batch 159, loss: 0.1056, instance_loss: 0.2856, weighted_loss: 0.1596, label: 0, bag_size: 103\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0027, weighted_loss: 0.0008, label: 1, bag_size: 62\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 1, bag_size: 63\n",
      "batch 219, loss: 14.2897, instance_loss: 0.7454, weighted_loss: 10.2264, label: 1, bag_size: 60\n",
      "batch 239, loss: 0.0001, instance_loss: 0.7852, weighted_loss: 0.2356, label: 0, bag_size: 43\n",
      "batch 259, loss: 0.0005, instance_loss: 0.4460, weighted_loss: 0.1342, label: 1, bag_size: 38\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0639, weighted_loss: 0.0192, label: 1, bag_size: 95\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0613, weighted_loss: 0.0184, label: 1, bag_size: 83\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0036, weighted_loss: 0.0011, label: 1, bag_size: 85\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0317, weighted_loss: 0.0095, label: 1, bag_size: 78\n",
      "batch 359, loss: 0.0033, instance_loss: 0.0246, weighted_loss: 0.0097, label: 0, bag_size: 63\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0059, weighted_loss: 0.0018, label: 1, bag_size: 86\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0397, weighted_loss: 0.0119, label: 0, bag_size: 51\n",
      "batch 419, loss: 6.1835, instance_loss: 1.4036, weighted_loss: 4.7495, label: 0, bag_size: 69\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0069, weighted_loss: 0.0021, label: 0, bag_size: 81\n",
      "batch 459, loss: 0.0205, instance_loss: 0.0996, weighted_loss: 0.0442, label: 1, bag_size: 24\n",
      "batch 479, loss: 0.0150, instance_loss: 0.0306, weighted_loss: 0.0197, label: 0, bag_size: 63\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0167, weighted_loss: 0.0051, label: 0, bag_size: 65\n",
      "batch 519, loss: 0.0108, instance_loss: 0.0667, weighted_loss: 0.0276, label: 1, bag_size: 46\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0032, weighted_loss: 0.0010, label: 0, bag_size: 26\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0022, weighted_loss: 0.0006, label: 0, bag_size: 29\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 55\n",
      "batch 599, loss: 0.0064, instance_loss: 1.3055, weighted_loss: 0.3962, label: 1, bag_size: 73\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 65\n",
      "batch 639, loss: 0.0015, instance_loss: 0.0194, weighted_loss: 0.0069, label: 0, bag_size: 132\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 31\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9749245852187028: correct 10342/10608\n",
      "class 1 clustering acc 0.8776395173453997: correct 4655/5304\n",
      "Epoch: 35, train_loss: 0.4298, train_clustering_loss:  0.2741, train_error: 0.0890\n",
      "class 0: acc 0.9104477611940298, correct 305/335\n",
      "class 1: acc 0.9115853658536586, correct 299/328\n",
      "\n",
      "Val Set, val_loss: 1.0609, val_error: 0.1765, auc: 0.9094\n",
      "class 0 clustering acc 0.924264705882353: correct 1257/1360\n",
      "class 1 clustering acc 0.8132352941176471: correct 553/680\n",
      "class 0: acc 0.8, correct 36/45\n",
      "class 1: acc 0.85, correct 34/40\n",
      "EarlyStopping counter: 26 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 59\n",
      "batch 39, loss: 14.3997, instance_loss: 6.9370, weighted_loss: 12.1609, label: 1, bag_size: 35\n",
      "batch 59, loss: 0.0338, instance_loss: 1.2243, weighted_loss: 0.3909, label: 1, bag_size: 41\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0117, weighted_loss: 0.0035, label: 0, bag_size: 20\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0022, weighted_loss: 0.0006, label: 1, bag_size: 40\n",
      "batch 119, loss: 0.0000, instance_loss: 0.1485, weighted_loss: 0.0446, label: 0, bag_size: 64\n",
      "batch 139, loss: 0.0348, instance_loss: 0.0070, weighted_loss: 0.0264, label: 1, bag_size: 99\n",
      "batch 159, loss: 0.4906, instance_loss: 0.1499, weighted_loss: 0.3884, label: 0, bag_size: 40\n",
      "batch 179, loss: 0.0078, instance_loss: 0.0017, weighted_loss: 0.0060, label: 1, bag_size: 18\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 35\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0116, weighted_loss: 0.0035, label: 0, bag_size: 45\n",
      "batch 239, loss: 0.2622, instance_loss: 0.2829, weighted_loss: 0.2684, label: 0, bag_size: 54\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 43\n",
      "batch 279, loss: 0.8919, instance_loss: 2.0468, weighted_loss: 1.2384, label: 0, bag_size: 18\n",
      "batch 299, loss: 0.0000, instance_loss: 0.1313, weighted_loss: 0.0394, label: 0, bag_size: 40\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0097, weighted_loss: 0.0029, label: 1, bag_size: 69\n",
      "batch 339, loss: 0.0065, instance_loss: 0.2064, weighted_loss: 0.0665, label: 1, bag_size: 119\n",
      "batch 359, loss: 0.0007, instance_loss: 0.0101, weighted_loss: 0.0035, label: 1, bag_size: 46\n",
      "batch 379, loss: 0.0008, instance_loss: 0.0285, weighted_loss: 0.0091, label: 1, bag_size: 99\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0005, label: 0, bag_size: 67\n",
      "batch 419, loss: 2.2257, instance_loss: 2.2951, weighted_loss: 2.2465, label: 0, bag_size: 28\n",
      "batch 439, loss: 0.0703, instance_loss: 0.8872, weighted_loss: 0.3153, label: 1, bag_size: 39\n",
      "batch 459, loss: 0.0030, instance_loss: 0.0484, weighted_loss: 0.0166, label: 0, bag_size: 102\n",
      "batch 479, loss: 0.0142, instance_loss: 0.3362, weighted_loss: 0.1108, label: 0, bag_size: 78\n",
      "batch 499, loss: 0.0001, instance_loss: 0.1404, weighted_loss: 0.0422, label: 0, bag_size: 69\n",
      "batch 519, loss: 0.0002, instance_loss: 0.0013, weighted_loss: 0.0005, label: 1, bag_size: 63\n",
      "batch 539, loss: 1.1755, instance_loss: 0.9912, weighted_loss: 1.1202, label: 0, bag_size: 48\n",
      "batch 559, loss: 0.0058, instance_loss: 0.1888, weighted_loss: 0.0607, label: 1, bag_size: 64\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 0, bag_size: 65\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0454, weighted_loss: 0.0136, label: 0, bag_size: 128\n",
      "batch 619, loss: 1.2566, instance_loss: 0.6877, weighted_loss: 1.0859, label: 1, bag_size: 100\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0116, weighted_loss: 0.0035, label: 0, bag_size: 37\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0056, weighted_loss: 0.0017, label: 0, bag_size: 94\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9645550527903469: correct 10232/10608\n",
      "class 1 clustering acc 0.8214555052790347: correct 4357/5304\n",
      "Epoch: 36, train_loss: 0.4509, train_clustering_loss:  0.3523, train_error: 0.1222\n",
      "class 0: acc 0.8681672025723473, correct 270/311\n",
      "class 1: acc 0.8863636363636364, correct 312/352\n",
      "\n",
      "Val Set, val_loss: 0.7539, val_error: 0.2000, auc: 0.8981\n",
      "class 0 clustering acc 0.9433823529411764: correct 1283/1360\n",
      "class 1 clustering acc 0.7338235294117647: correct 499/680\n",
      "class 0: acc 0.7555555555555555, correct 34/45\n",
      "class 1: acc 0.85, correct 34/40\n",
      "EarlyStopping counter: 27 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.1403, weighted_loss: 0.0421, label: 0, bag_size: 74\n",
      "batch 39, loss: 0.0321, instance_loss: 0.0128, weighted_loss: 0.0263, label: 0, bag_size: 20\n",
      "batch 59, loss: 1.8218, instance_loss: 0.9239, weighted_loss: 1.5524, label: 0, bag_size: 51\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 0, bag_size: 40\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0255, weighted_loss: 0.0077, label: 0, bag_size: 72\n",
      "batch 119, loss: 0.0085, instance_loss: 0.0000, weighted_loss: 0.0060, label: 0, bag_size: 67\n",
      "batch 139, loss: 0.0003, instance_loss: 0.0033, weighted_loss: 0.0012, label: 1, bag_size: 103\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 77\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0039, weighted_loss: 0.0012, label: 0, bag_size: 33\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0152, weighted_loss: 0.0046, label: 1, bag_size: 42\n",
      "batch 219, loss: 0.0004, instance_loss: 0.0134, weighted_loss: 0.0043, label: 0, bag_size: 78\n",
      "batch 239, loss: 0.0010, instance_loss: 0.0039, weighted_loss: 0.0019, label: 0, bag_size: 31\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 36\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0031, weighted_loss: 0.0009, label: 0, bag_size: 94\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0298, weighted_loss: 0.0090, label: 0, bag_size: 88\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0023, weighted_loss: 0.0008, label: 1, bag_size: 84\n",
      "batch 339, loss: 0.0008, instance_loss: 0.0010, weighted_loss: 0.0009, label: 1, bag_size: 30\n",
      "batch 359, loss: 0.0492, instance_loss: 0.0487, weighted_loss: 0.0490, label: 1, bag_size: 24\n",
      "batch 379, loss: 0.0123, instance_loss: 0.0121, weighted_loss: 0.0123, label: 0, bag_size: 41\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0038, weighted_loss: 0.0012, label: 0, bag_size: 36\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0000, label: 0, bag_size: 67\n",
      "batch 439, loss: 0.0410, instance_loss: 0.1022, weighted_loss: 0.0594, label: 1, bag_size: 14\n",
      "batch 459, loss: 0.0011, instance_loss: 0.0098, weighted_loss: 0.0037, label: 0, bag_size: 58\n",
      "batch 479, loss: 0.0016, instance_loss: 0.0017, weighted_loss: 0.0016, label: 0, bag_size: 25\n",
      "batch 499, loss: 0.0160, instance_loss: 0.0062, weighted_loss: 0.0130, label: 0, bag_size: 79\n",
      "batch 519, loss: 0.0296, instance_loss: 0.1924, weighted_loss: 0.0785, label: 1, bag_size: 29\n",
      "batch 539, loss: 0.7179, instance_loss: 0.9585, weighted_loss: 0.7901, label: 0, bag_size: 73\n",
      "batch 559, loss: 0.0022, instance_loss: 0.0053, weighted_loss: 0.0031, label: 1, bag_size: 75\n",
      "batch 579, loss: 0.0151, instance_loss: 0.0011, weighted_loss: 0.0109, label: 1, bag_size: 65\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 72\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 35\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0027, weighted_loss: 0.0008, label: 1, bag_size: 71\n",
      "batch 659, loss: 0.0004, instance_loss: 0.0093, weighted_loss: 0.0031, label: 0, bag_size: 36\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9777526395173454: correct 10372/10608\n",
      "class 1 clustering acc 0.8827300150829562: correct 4682/5304\n",
      "Epoch: 37, train_loss: 0.3121, train_clustering_loss:  0.2042, train_error: 0.0890\n",
      "class 0: acc 0.908256880733945, correct 297/327\n",
      "class 1: acc 0.9136904761904762, correct 307/336\n",
      "\n",
      "Val Set, val_loss: 0.5885, val_error: 0.1294, auc: 0.9211\n",
      "class 0 clustering acc 0.9558823529411765: correct 1300/1360\n",
      "class 1 clustering acc 0.8029411764705883: correct 546/680\n",
      "class 0: acc 0.8888888888888888, correct 40/45\n",
      "class 1: acc 0.85, correct 34/40\n",
      "EarlyStopping counter: 28 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0813, instance_loss: 0.0932, weighted_loss: 0.0849, label: 0, bag_size: 80\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 24\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 91\n",
      "batch 79, loss: 0.0041, instance_loss: 0.0417, weighted_loss: 0.0154, label: 1, bag_size: 121\n",
      "batch 99, loss: 0.0047, instance_loss: 0.2338, weighted_loss: 0.0734, label: 0, bag_size: 50\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 75\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 40\n",
      "batch 159, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 64\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 37\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 93\n",
      "batch 219, loss: 3.5563, instance_loss: 2.6269, weighted_loss: 3.2775, label: 1, bag_size: 82\n",
      "batch 239, loss: 0.8995, instance_loss: 0.9478, weighted_loss: 0.9140, label: 0, bag_size: 28\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 99\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 63\n",
      "batch 299, loss: 0.0082, instance_loss: 0.0134, weighted_loss: 0.0098, label: 0, bag_size: 45\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 18\n",
      "batch 339, loss: 0.0106, instance_loss: 0.0396, weighted_loss: 0.0193, label: 0, bag_size: 71\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0025, weighted_loss: 0.0008, label: 1, bag_size: 30\n",
      "batch 379, loss: 0.0137, instance_loss: 0.1469, weighted_loss: 0.0537, label: 0, bag_size: 14\n",
      "batch 399, loss: 0.0019, instance_loss: 0.0195, weighted_loss: 0.0072, label: 0, bag_size: 114\n",
      "batch 419, loss: 0.0107, instance_loss: 0.0296, weighted_loss: 0.0163, label: 0, bag_size: 80\n",
      "batch 439, loss: 1.1230, instance_loss: 1.1378, weighted_loss: 1.1275, label: 0, bag_size: 28\n",
      "batch 459, loss: 0.0021, instance_loss: 0.0210, weighted_loss: 0.0078, label: 0, bag_size: 97\n",
      "batch 479, loss: 0.0031, instance_loss: 0.0106, weighted_loss: 0.0054, label: 1, bag_size: 63\n",
      "batch 499, loss: 0.9322, instance_loss: 0.0379, weighted_loss: 0.6639, label: 1, bag_size: 89\n",
      "batch 519, loss: 0.3628, instance_loss: 0.8095, weighted_loss: 0.4968, label: 1, bag_size: 76\n",
      "batch 539, loss: 9.5606, instance_loss: 1.3667, weighted_loss: 7.1024, label: 0, bag_size: 79\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 126\n",
      "batch 579, loss: 4.2587, instance_loss: 0.3108, weighted_loss: 3.0743, label: 0, bag_size: 66\n",
      "batch 599, loss: 0.0000, instance_loss: 1.1332, weighted_loss: 0.3400, label: 0, bag_size: 48\n",
      "batch 619, loss: 0.0045, instance_loss: 1.1142, weighted_loss: 0.3374, label: 1, bag_size: 84\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0801, weighted_loss: 0.0240, label: 1, bag_size: 100\n",
      "batch 659, loss: 0.0000, instance_loss: 1.2829, weighted_loss: 0.3849, label: 0, bag_size: 43\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9642722473604827: correct 10229/10608\n",
      "class 1 clustering acc 0.8316365007541479: correct 4411/5304\n",
      "Epoch: 38, train_loss: 0.8166, train_clustering_loss:  0.3112, train_error: 0.1176\n",
      "class 0: acc 0.8868501529051988, correct 290/327\n",
      "class 1: acc 0.8779761904761905, correct 295/336\n",
      "\n",
      "Val Set, val_loss: 2.0055, val_error: 0.1412, auc: 0.9244\n",
      "class 0 clustering acc 0.9316176470588236: correct 1267/1360\n",
      "class 1 clustering acc 0.3911764705882353: correct 266/680\n",
      "class 0: acc 0.8222222222222222, correct 37/45\n",
      "class 1: acc 0.9, correct 36/40\n",
      "EarlyStopping counter: 29 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 40.3475, instance_loss: 1.7428, weighted_loss: 28.7661, label: 0, bag_size: 56\n",
      "batch 39, loss: 0.0000, instance_loss: 1.0867, weighted_loss: 0.3260, label: 0, bag_size: 66\n",
      "batch 59, loss: 0.0000, instance_loss: 0.6559, weighted_loss: 0.1968, label: 0, bag_size: 35\n",
      "batch 79, loss: 0.0000, instance_loss: 0.5176, weighted_loss: 0.1553, label: 0, bag_size: 110\n",
      "batch 99, loss: 0.0000, instance_loss: 0.5344, weighted_loss: 0.1603, label: 0, bag_size: 37\n",
      "batch 119, loss: 0.0000, instance_loss: 0.6382, weighted_loss: 0.1915, label: 0, bag_size: 57\n",
      "batch 139, loss: 0.0594, instance_loss: 0.5915, weighted_loss: 0.2190, label: 1, bag_size: 120\n",
      "batch 159, loss: 0.0000, instance_loss: 0.4878, weighted_loss: 0.1463, label: 0, bag_size: 45\n",
      "batch 179, loss: 0.0000, instance_loss: 0.8365, weighted_loss: 0.2509, label: 1, bag_size: 95\n",
      "batch 199, loss: 0.0827, instance_loss: 1.1006, weighted_loss: 0.3881, label: 0, bag_size: 28\n",
      "batch 219, loss: 0.0006, instance_loss: 0.6352, weighted_loss: 0.1910, label: 1, bag_size: 45\n",
      "batch 239, loss: 4.4142, instance_loss: 0.7845, weighted_loss: 3.3253, label: 1, bag_size: 59\n",
      "batch 259, loss: 0.0068, instance_loss: 0.3396, weighted_loss: 0.1066, label: 1, bag_size: 36\n",
      "batch 279, loss: 7.3998, instance_loss: 4.8115, weighted_loss: 6.6233, label: 0, bag_size: 81\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0071, weighted_loss: 0.0021, label: 0, bag_size: 37\n",
      "batch 319, loss: 0.0601, instance_loss: 0.3107, weighted_loss: 0.1353, label: 0, bag_size: 68\n",
      "batch 339, loss: 4.9207, instance_loss: 0.1873, weighted_loss: 3.5007, label: 0, bag_size: 78\n",
      "batch 359, loss: 4.4303, instance_loss: 0.2841, weighted_loss: 3.1865, label: 0, bag_size: 41\n",
      "batch 379, loss: 3.6631, instance_loss: 1.1322, weighted_loss: 2.9038, label: 1, bag_size: 96\n",
      "batch 399, loss: 0.0079, instance_loss: 1.7491, weighted_loss: 0.5302, label: 0, bag_size: 81\n",
      "batch 419, loss: 0.0494, instance_loss: 0.1962, weighted_loss: 0.0935, label: 1, bag_size: 47\n",
      "batch 439, loss: 15.4415, instance_loss: 2.5210, weighted_loss: 11.5654, label: 0, bag_size: 50\n",
      "batch 459, loss: 0.0000, instance_loss: 1.0262, weighted_loss: 0.3079, label: 0, bag_size: 39\n",
      "batch 479, loss: 0.0160, instance_loss: 0.6891, weighted_loss: 0.2179, label: 0, bag_size: 66\n",
      "batch 499, loss: 0.0014, instance_loss: 0.4836, weighted_loss: 0.1461, label: 0, bag_size: 78\n",
      "batch 519, loss: 2.7350, instance_loss: 2.1123, weighted_loss: 2.5482, label: 0, bag_size: 31\n",
      "batch 539, loss: 0.2742, instance_loss: 1.2953, weighted_loss: 0.5805, label: 1, bag_size: 42\n",
      "batch 559, loss: 0.0011, instance_loss: 0.5513, weighted_loss: 0.1661, label: 0, bag_size: 41\n",
      "batch 579, loss: 0.0000, instance_loss: 0.5107, weighted_loss: 0.1532, label: 0, bag_size: 88\n",
      "batch 599, loss: 0.0002, instance_loss: 0.6156, weighted_loss: 0.1848, label: 1, bag_size: 68\n",
      "batch 619, loss: 0.0000, instance_loss: 0.6744, weighted_loss: 0.2023, label: 1, bag_size: 74\n",
      "batch 639, loss: 0.0000, instance_loss: 0.6717, weighted_loss: 0.2015, label: 0, bag_size: 63\n",
      "batch 659, loss: 0.0021, instance_loss: 0.5246, weighted_loss: 0.1589, label: 0, bag_size: 89\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.8840497737556561: correct 9378/10608\n",
      "class 1 clustering acc 0.5622171945701357: correct 2982/5304\n",
      "Epoch: 39, train_loss: 0.7981, train_clustering_loss:  0.6603, train_error: 0.1101\n",
      "class 0: acc 0.8925373134328358, correct 299/335\n",
      "class 1: acc 0.8871951219512195, correct 291/328\n",
      "\n",
      "Val Set, val_loss: 0.7707, val_error: 0.1412, auc: 0.9264\n",
      "class 0 clustering acc 0.9235294117647059: correct 1256/1360\n",
      "class 1 clustering acc 0.6367647058823529: correct 433/680\n",
      "class 0: acc 0.8, correct 36/45\n",
      "class 1: acc 0.925, correct 37/40\n",
      "EarlyStopping counter: 30 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.1141, weighted_loss: 0.0342, label: 0, bag_size: 116\n",
      "batch 39, loss: 0.0004, instance_loss: 0.0597, weighted_loss: 0.0182, label: 0, bag_size: 42\n",
      "batch 59, loss: 0.0000, instance_loss: 0.1474, weighted_loss: 0.0442, label: 0, bag_size: 92\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0221, weighted_loss: 0.0066, label: 0, bag_size: 40\n",
      "batch 99, loss: 0.0002, instance_loss: 0.0161, weighted_loss: 0.0050, label: 0, bag_size: 28\n",
      "batch 119, loss: 0.0002, instance_loss: 0.0442, weighted_loss: 0.0134, label: 0, bag_size: 38\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0134, weighted_loss: 0.0040, label: 1, bag_size: 91\n",
      "batch 159, loss: 0.0000, instance_loss: 0.1103, weighted_loss: 0.0331, label: 1, bag_size: 77\n",
      "batch 179, loss: 0.0039, instance_loss: 0.2040, weighted_loss: 0.0639, label: 1, bag_size: 121\n",
      "batch 199, loss: 0.0081, instance_loss: 0.0131, weighted_loss: 0.0096, label: 0, bag_size: 79\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0132, weighted_loss: 0.0041, label: 0, bag_size: 72\n",
      "batch 239, loss: 0.0002, instance_loss: 0.0061, weighted_loss: 0.0019, label: 0, bag_size: 67\n",
      "batch 259, loss: 0.0018, instance_loss: 0.0204, weighted_loss: 0.0074, label: 0, bag_size: 51\n",
      "batch 279, loss: 0.2733, instance_loss: 0.5369, weighted_loss: 0.3524, label: 1, bag_size: 44\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 94\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 76\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0370, weighted_loss: 0.0111, label: 1, bag_size: 90\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 76\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0033, weighted_loss: 0.0010, label: 1, bag_size: 25\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0422, weighted_loss: 0.0127, label: 1, bag_size: 68\n",
      "batch 419, loss: 0.0000, instance_loss: 0.1196, weighted_loss: 0.0359, label: 0, bag_size: 56\n",
      "batch 439, loss: 0.0021, instance_loss: 0.6947, weighted_loss: 0.2098, label: 1, bag_size: 28\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 1, bag_size: 43\n",
      "batch 479, loss: 0.0000, instance_loss: 0.2559, weighted_loss: 0.0768, label: 0, bag_size: 58\n",
      "batch 499, loss: 4.2563, instance_loss: 0.6610, weighted_loss: 3.1777, label: 1, bag_size: 46\n",
      "batch 519, loss: 0.0149, instance_loss: 0.0922, weighted_loss: 0.0381, label: 0, bag_size: 58\n",
      "batch 539, loss: 0.0170, instance_loss: 0.2351, weighted_loss: 0.0824, label: 1, bag_size: 70\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0127, weighted_loss: 0.0039, label: 0, bag_size: 40\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0403, weighted_loss: 0.0121, label: 0, bag_size: 65\n",
      "batch 599, loss: 0.0231, instance_loss: 0.3957, weighted_loss: 0.1349, label: 0, bag_size: 54\n",
      "batch 619, loss: 0.0160, instance_loss: 0.7511, weighted_loss: 0.2365, label: 1, bag_size: 36\n",
      "batch 639, loss: 0.0000, instance_loss: 0.2345, weighted_loss: 0.0703, label: 1, bag_size: 56\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0301, weighted_loss: 0.0090, label: 1, bag_size: 36\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9514517345399698: correct 10093/10608\n",
      "class 1 clustering acc 0.8163650075414781: correct 4330/5304\n",
      "Epoch: 40, train_loss: 0.5343, train_clustering_loss:  0.3355, train_error: 0.1267\n",
      "class 0: acc 0.8792134831460674, correct 313/356\n",
      "class 1: acc 0.8664495114006515, correct 266/307\n",
      "\n",
      "Val Set, val_loss: 0.8878, val_error: 0.1882, auc: 0.9228\n",
      "class 0 clustering acc 0.9455882352941176: correct 1286/1360\n",
      "class 1 clustering acc 0.7: correct 476/680\n",
      "class 0: acc 0.9555555555555556, correct 43/45\n",
      "class 1: acc 0.65, correct 26/40\n",
      "EarlyStopping counter: 31 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0975, weighted_loss: 0.0293, label: 0, bag_size: 67\n",
      "batch 39, loss: 3.6103, instance_loss: 3.1572, weighted_loss: 3.4744, label: 1, bag_size: 28\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 0, bag_size: 37\n",
      "batch 79, loss: 0.1841, instance_loss: 0.1597, weighted_loss: 0.1767, label: 0, bag_size: 32\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0046, weighted_loss: 0.0014, label: 0, bag_size: 13\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 88\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0001, label: 0, bag_size: 40\n",
      "batch 159, loss: 0.0007, instance_loss: 0.0865, weighted_loss: 0.0265, label: 0, bag_size: 65\n",
      "batch 179, loss: 4.9962, instance_loss: 2.1025, weighted_loss: 4.1281, label: 1, bag_size: 69\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0036, weighted_loss: 0.0011, label: 0, bag_size: 50\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 93\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0425, weighted_loss: 0.0128, label: 0, bag_size: 94\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 0, bag_size: 36\n",
      "batch 279, loss: 10.3918, instance_loss: 3.2257, weighted_loss: 8.2420, label: 1, bag_size: 69\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0074, weighted_loss: 0.0022, label: 0, bag_size: 101\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0477, weighted_loss: 0.0143, label: 1, bag_size: 31\n",
      "batch 339, loss: 0.0000, instance_loss: 0.3445, weighted_loss: 0.1034, label: 1, bag_size: 89\n",
      "batch 359, loss: 0.0000, instance_loss: 0.5394, weighted_loss: 0.1618, label: 0, bag_size: 73\n",
      "batch 379, loss: 0.0000, instance_loss: 0.4310, weighted_loss: 0.1293, label: 0, bag_size: 69\n",
      "batch 399, loss: 0.3557, instance_loss: 0.2375, weighted_loss: 0.3202, label: 0, bag_size: 50\n",
      "batch 419, loss: 1.2314, instance_loss: 1.5981, weighted_loss: 1.3414, label: 1, bag_size: 78\n",
      "batch 439, loss: 0.4105, instance_loss: 0.5451, weighted_loss: 0.4509, label: 1, bag_size: 67\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0189, weighted_loss: 0.0057, label: 1, bag_size: 34\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 65\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0037, weighted_loss: 0.0011, label: 1, bag_size: 99\n",
      "batch 519, loss: 1.8869, instance_loss: 0.6522, weighted_loss: 1.5165, label: 0, bag_size: 27\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0052, weighted_loss: 0.0016, label: 0, bag_size: 30\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0209, weighted_loss: 0.0063, label: 1, bag_size: 85\n",
      "batch 579, loss: 0.1513, instance_loss: 0.0070, weighted_loss: 0.1080, label: 1, bag_size: 88\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0092, weighted_loss: 0.0028, label: 0, bag_size: 71\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 45\n",
      "batch 639, loss: 8.1609, instance_loss: 2.3655, weighted_loss: 6.4223, label: 0, bag_size: 132\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0942, weighted_loss: 0.0283, label: 1, bag_size: 74\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9555052790346908: correct 10136/10608\n",
      "class 1 clustering acc 0.8254147812971342: correct 4378/5304\n",
      "Epoch: 41, train_loss: 0.6945, train_clustering_loss:  0.3342, train_error: 0.1388\n",
      "class 0: acc 0.8710601719197708, correct 304/349\n",
      "class 1: acc 0.8503184713375797, correct 267/314\n",
      "\n",
      "Val Set, val_loss: 1.1076, val_error: 0.1765, auc: 0.9153\n",
      "class 0 clustering acc 0.9529411764705882: correct 1296/1360\n",
      "class 1 clustering acc 0.7558823529411764: correct 514/680\n",
      "class 0: acc 0.8, correct 36/45\n",
      "class 1: acc 0.85, correct 34/40\n",
      "EarlyStopping counter: 32 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0157, weighted_loss: 0.0047, label: 1, bag_size: 14\n",
      "batch 39, loss: 0.0000, instance_loss: 0.2270, weighted_loss: 0.0681, label: 0, bag_size: 108\n",
      "batch 59, loss: 6.4188, instance_loss: 4.3204, weighted_loss: 5.7893, label: 1, bag_size: 30\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0051, weighted_loss: 0.0015, label: 1, bag_size: 39\n",
      "batch 99, loss: 0.0048, instance_loss: 0.1614, weighted_loss: 0.0518, label: 1, bag_size: 86\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0087, weighted_loss: 0.0026, label: 1, bag_size: 35\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0565, weighted_loss: 0.0170, label: 0, bag_size: 106\n",
      "batch 159, loss: 0.0246, instance_loss: 0.4593, weighted_loss: 0.1550, label: 0, bag_size: 63\n",
      "batch 179, loss: 0.0001, instance_loss: 1.5100, weighted_loss: 0.4531, label: 0, bag_size: 93\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0079, weighted_loss: 0.0024, label: 0, bag_size: 21\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0105, weighted_loss: 0.0032, label: 1, bag_size: 30\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0100, weighted_loss: 0.0030, label: 0, bag_size: 21\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0041, weighted_loss: 0.0013, label: 0, bag_size: 43\n",
      "batch 279, loss: 0.0945, instance_loss: 0.0430, weighted_loss: 0.0791, label: 0, bag_size: 89\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0047, weighted_loss: 0.0014, label: 1, bag_size: 31\n",
      "batch 319, loss: 0.1571, instance_loss: 0.2064, weighted_loss: 0.1719, label: 0, bag_size: 42\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0109, weighted_loss: 0.0033, label: 0, bag_size: 110\n",
      "batch 359, loss: 1.2337, instance_loss: 0.4977, weighted_loss: 1.0129, label: 1, bag_size: 42\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0033, weighted_loss: 0.0010, label: 0, bag_size: 31\n",
      "batch 399, loss: 5.3752, instance_loss: 2.1982, weighted_loss: 4.4221, label: 1, bag_size: 25\n",
      "batch 419, loss: 0.7704, instance_loss: 0.0586, weighted_loss: 0.5569, label: 1, bag_size: 82\n",
      "batch 439, loss: 2.3633, instance_loss: 1.6075, weighted_loss: 2.1366, label: 1, bag_size: 25\n",
      "batch 459, loss: 0.0227, instance_loss: 0.5495, weighted_loss: 0.1808, label: 0, bag_size: 89\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0159, weighted_loss: 0.0048, label: 0, bag_size: 46\n",
      "batch 499, loss: 0.0006, instance_loss: 0.1759, weighted_loss: 0.0532, label: 0, bag_size: 82\n",
      "batch 519, loss: 0.0012, instance_loss: 0.0527, weighted_loss: 0.0166, label: 1, bag_size: 69\n",
      "batch 539, loss: 0.0017, instance_loss: 0.0159, weighted_loss: 0.0060, label: 1, bag_size: 86\n",
      "batch 559, loss: 0.0110, instance_loss: 0.0142, weighted_loss: 0.0119, label: 1, bag_size: 36\n",
      "batch 579, loss: 0.0005, instance_loss: 0.0396, weighted_loss: 0.0123, label: 1, bag_size: 61\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0005, weighted_loss: 0.0002, label: 0, bag_size: 27\n",
      "batch 619, loss: 0.0004, instance_loss: 0.0228, weighted_loss: 0.0071, label: 1, bag_size: 98\n",
      "batch 639, loss: 2.6630, instance_loss: 0.7633, weighted_loss: 2.0931, label: 0, bag_size: 69\n",
      "batch 659, loss: 0.0001, instance_loss: 0.0175, weighted_loss: 0.0053, label: 1, bag_size: 22\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9633295625942685: correct 10219/10608\n",
      "class 1 clustering acc 0.8305052790346908: correct 4405/5304\n",
      "Epoch: 42, train_loss: 0.4929, train_clustering_loss:  0.3287, train_error: 0.1146\n",
      "class 0: acc 0.8829113924050633, correct 279/316\n",
      "class 1: acc 0.8876080691642652, correct 308/347\n",
      "\n",
      "Val Set, val_loss: 0.7734, val_error: 0.1647, auc: 0.9111\n",
      "class 0 clustering acc 0.9588235294117647: correct 1304/1360\n",
      "class 1 clustering acc 0.825: correct 561/680\n",
      "class 0: acc 0.8666666666666667, correct 39/45\n",
      "class 1: acc 0.8, correct 32/40\n",
      "EarlyStopping counter: 33 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0004, instance_loss: 0.1078, weighted_loss: 0.0326, label: 0, bag_size: 71\n",
      "batch 39, loss: 0.0008, instance_loss: 0.0408, weighted_loss: 0.0128, label: 1, bag_size: 29\n",
      "batch 59, loss: 0.1162, instance_loss: 0.3602, weighted_loss: 0.1894, label: 0, bag_size: 28\n",
      "batch 79, loss: 0.0045, instance_loss: 0.1165, weighted_loss: 0.0381, label: 0, bag_size: 13\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0034, weighted_loss: 0.0010, label: 1, bag_size: 62\n",
      "batch 119, loss: 0.0125, instance_loss: 0.2559, weighted_loss: 0.0855, label: 0, bag_size: 33\n",
      "batch 139, loss: 0.0003, instance_loss: 0.0148, weighted_loss: 0.0046, label: 1, bag_size: 62\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 0, bag_size: 87\n",
      "batch 179, loss: 0.0009, instance_loss: 0.2230, weighted_loss: 0.0675, label: 0, bag_size: 78\n",
      "batch 199, loss: 3.2539, instance_loss: 1.8796, weighted_loss: 2.8416, label: 0, bag_size: 72\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21\n",
      "batch 239, loss: 0.0246, instance_loss: 0.4936, weighted_loss: 0.1653, label: 0, bag_size: 90\n",
      "batch 259, loss: 3.1201, instance_loss: 3.0574, weighted_loss: 3.1013, label: 1, bag_size: 51\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 81\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 71\n",
      "batch 319, loss: 0.0009, instance_loss: 0.0020, weighted_loss: 0.0013, label: 0, bag_size: 28\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0021, weighted_loss: 0.0006, label: 1, bag_size: 100\n",
      "batch 359, loss: 0.0005, instance_loss: 0.0004, weighted_loss: 0.0004, label: 1, bag_size: 123\n",
      "batch 379, loss: 0.4067, instance_loss: 0.2237, weighted_loss: 0.3518, label: 0, bag_size: 28\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 25\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 65\n",
      "batch 439, loss: 0.0000, instance_loss: 0.1895, weighted_loss: 0.0569, label: 1, bag_size: 71\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0570, weighted_loss: 0.0171, label: 0, bag_size: 35\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 85\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0016, weighted_loss: 0.0005, label: 1, bag_size: 109\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0047, weighted_loss: 0.0014, label: 1, bag_size: 36\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0091, weighted_loss: 0.0028, label: 1, bag_size: 61\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 99\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 43\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 85\n",
      "batch 619, loss: 0.3697, instance_loss: 0.7763, weighted_loss: 0.4917, label: 0, bag_size: 50\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 73\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 85\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9755844645550528: correct 10349/10608\n",
      "class 1 clustering acc 0.8934766214177979: correct 4739/5304\n",
      "Epoch: 43, train_loss: 0.3188, train_clustering_loss:  0.2006, train_error: 0.0754\n",
      "class 0: acc 0.9304347826086956, correct 321/345\n",
      "class 1: acc 0.9182389937106918, correct 292/318\n",
      "\n",
      "Val Set, val_loss: 1.1658, val_error: 0.1882, auc: 0.9114\n",
      "class 0 clustering acc 0.9323529411764706: correct 1268/1360\n",
      "class 1 clustering acc 0.8: correct 544/680\n",
      "class 0: acc 0.7333333333333333, correct 33/45\n",
      "class 1: acc 0.9, correct 36/40\n",
      "EarlyStopping counter: 34 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0104, instance_loss: 0.0082, weighted_loss: 0.0098, label: 0, bag_size: 90\n",
      "batch 39, loss: 0.0010, instance_loss: 0.0070, weighted_loss: 0.0028, label: 1, bag_size: 46\n",
      "batch 59, loss: 0.0000, instance_loss: 0.2845, weighted_loss: 0.0854, label: 1, bag_size: 128\n",
      "batch 79, loss: 0.0007, instance_loss: 0.0392, weighted_loss: 0.0122, label: 0, bag_size: 22\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0527, weighted_loss: 0.0159, label: 1, bag_size: 82\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0577, weighted_loss: 0.0173, label: 0, bag_size: 71\n",
      "batch 139, loss: 3.3299, instance_loss: 1.2090, weighted_loss: 2.6936, label: 1, bag_size: 61\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0172, weighted_loss: 0.0052, label: 1, bag_size: 41\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0105, weighted_loss: 0.0032, label: 0, bag_size: 81\n",
      "batch 199, loss: 0.1301, instance_loss: 0.0495, weighted_loss: 0.1059, label: 0, bag_size: 67\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0076, weighted_loss: 0.0023, label: 0, bag_size: 28\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0029, weighted_loss: 0.0009, label: 1, bag_size: 83\n",
      "batch 259, loss: 2.5330, instance_loss: 3.5283, weighted_loss: 2.8316, label: 1, bag_size: 13\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0983, weighted_loss: 0.0295, label: 0, bag_size: 28\n",
      "batch 299, loss: 0.1208, instance_loss: 0.0844, weighted_loss: 0.1099, label: 0, bag_size: 33\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 40\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0384, weighted_loss: 0.0116, label: 1, bag_size: 31\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0063, weighted_loss: 0.0019, label: 0, bag_size: 93\n",
      "batch 379, loss: 0.6436, instance_loss: 1.6228, weighted_loss: 0.9374, label: 1, bag_size: 83\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0348, weighted_loss: 0.0104, label: 1, bag_size: 89\n",
      "batch 419, loss: 0.0096, instance_loss: 0.0003, weighted_loss: 0.0068, label: 1, bag_size: 25\n",
      "batch 439, loss: 0.0000, instance_loss: 0.3116, weighted_loss: 0.0935, label: 0, bag_size: 66\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 30\n",
      "batch 479, loss: 0.0004, instance_loss: 0.0113, weighted_loss: 0.0037, label: 1, bag_size: 84\n",
      "batch 499, loss: 0.0065, instance_loss: 0.0095, weighted_loss: 0.0074, label: 0, bag_size: 35\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 61\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 1, bag_size: 71\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0337, weighted_loss: 0.0101, label: 0, bag_size: 30\n",
      "batch 579, loss: 2.6130, instance_loss: 0.0015, weighted_loss: 1.8295, label: 1, bag_size: 86\n",
      "batch 599, loss: 19.0454, instance_loss: 0.8310, weighted_loss: 13.5811, label: 1, bag_size: 92\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 36\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 114\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 71\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.967288838612368: correct 10261/10608\n",
      "class 1 clustering acc 0.8487933634992458: correct 4502/5304\n",
      "Epoch: 44, train_loss: 0.6267, train_clustering_loss:  0.3105, train_error: 0.0995\n",
      "class 0: acc 0.9077809798270894, correct 315/347\n",
      "class 1: acc 0.8924050632911392, correct 282/316\n",
      "\n",
      "Val Set, val_loss: 6.2136, val_error: 0.3529, auc: 0.8928\n",
      "class 0 clustering acc 0.9419117647058823: correct 1281/1360\n",
      "class 1 clustering acc 0.7323529411764705: correct 498/680\n",
      "class 0: acc 0.9777777777777777, correct 44/45\n",
      "class 1: acc 0.275, correct 11/40\n",
      "EarlyStopping counter: 35 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0032, weighted_loss: 0.0010, label: 1, bag_size: 94\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 49\n",
      "batch 59, loss: 0.0094, instance_loss: 0.5809, weighted_loss: 0.1809, label: 1, bag_size: 16\n",
      "batch 79, loss: 8.2423, instance_loss: 1.1380, weighted_loss: 6.1110, label: 1, bag_size: 39\n",
      "batch 99, loss: 0.0000, instance_loss: 0.1283, weighted_loss: 0.0385, label: 1, bag_size: 22\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0049, weighted_loss: 0.0015, label: 0, bag_size: 115\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0611, weighted_loss: 0.0183, label: 0, bag_size: 78\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0060, weighted_loss: 0.0018, label: 1, bag_size: 70\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0102, weighted_loss: 0.0031, label: 0, bag_size: 116\n",
      "batch 199, loss: 4.3155, instance_loss: 0.8745, weighted_loss: 3.2832, label: 1, bag_size: 119\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 57\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0060, weighted_loss: 0.0018, label: 0, bag_size: 40\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0022, weighted_loss: 0.0007, label: 1, bag_size: 31\n",
      "batch 279, loss: 0.0106, instance_loss: 0.0391, weighted_loss: 0.0191, label: 0, bag_size: 45\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 75\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0898, weighted_loss: 0.0270, label: 0, bag_size: 87\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 0, bag_size: 110\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0016, weighted_loss: 0.0005, label: 1, bag_size: 77\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0005, label: 1, bag_size: 38\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 72\n",
      "batch 419, loss: 0.0017, instance_loss: 0.0240, weighted_loss: 0.0084, label: 0, bag_size: 68\n",
      "batch 439, loss: 0.4158, instance_loss: 0.2329, weighted_loss: 0.3610, label: 0, bag_size: 67\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 14\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 1, bag_size: 30\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0000, label: 0, bag_size: 87\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0027, weighted_loss: 0.0009, label: 0, bag_size: 43\n",
      "batch 539, loss: 6.3712, instance_loss: 4.1758, weighted_loss: 5.7126, label: 0, bag_size: 43\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0162, weighted_loss: 0.0048, label: 1, bag_size: 89\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 153\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 105\n",
      "batch 619, loss: 0.0039, instance_loss: 0.0640, weighted_loss: 0.0220, label: 0, bag_size: 34\n",
      "batch 639, loss: 0.0011, instance_loss: 0.0326, weighted_loss: 0.0106, label: 0, bag_size: 29\n",
      "batch 659, loss: 0.0002, instance_loss: 0.0451, weighted_loss: 0.0137, label: 1, bag_size: 38\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9696455505279035: correct 10286/10608\n",
      "class 1 clustering acc 0.8634992458521871: correct 4580/5304\n",
      "Epoch: 45, train_loss: 0.7232, train_clustering_loss:  0.2764, train_error: 0.1041\n",
      "class 0: acc 0.8961424332344213, correct 302/337\n",
      "class 1: acc 0.8957055214723927, correct 292/326\n",
      "\n",
      "Val Set, val_loss: 0.7485, val_error: 0.1412, auc: 0.9289\n",
      "class 0 clustering acc 0.9551470588235295: correct 1299/1360\n",
      "class 1 clustering acc 0.8044117647058824: correct 547/680\n",
      "class 0: acc 0.8444444444444444, correct 38/45\n",
      "class 1: acc 0.875, correct 35/40\n",
      "EarlyStopping counter: 36 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0053, weighted_loss: 0.0016, label: 1, bag_size: 44\n",
      "batch 39, loss: 0.0033, instance_loss: 0.0155, weighted_loss: 0.0069, label: 1, bag_size: 65\n",
      "batch 59, loss: 0.0001, instance_loss: 0.0545, weighted_loss: 0.0164, label: 1, bag_size: 119\n",
      "batch 79, loss: 0.0001, instance_loss: 0.2811, weighted_loss: 0.0844, label: 1, bag_size: 92\n",
      "batch 99, loss: 0.0046, instance_loss: 0.1653, weighted_loss: 0.0528, label: 0, bag_size: 35\n",
      "batch 119, loss: 0.1208, instance_loss: 0.0751, weighted_loss: 0.1071, label: 0, bag_size: 24\n",
      "batch 139, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 29\n",
      "batch 159, loss: 0.0002, instance_loss: 0.0348, weighted_loss: 0.0106, label: 0, bag_size: 25\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 95\n",
      "batch 199, loss: 0.0863, instance_loss: 0.9800, weighted_loss: 0.3544, label: 1, bag_size: 79\n",
      "batch 219, loss: 0.0002, instance_loss: 0.1137, weighted_loss: 0.0343, label: 1, bag_size: 96\n",
      "batch 239, loss: 0.0001, instance_loss: 0.0051, weighted_loss: 0.0016, label: 1, bag_size: 118\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 25\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 20\n",
      "batch 299, loss: 0.0002, instance_loss: 0.0036, weighted_loss: 0.0012, label: 0, bag_size: 25\n",
      "batch 319, loss: 0.0003, instance_loss: 0.1517, weighted_loss: 0.0457, label: 0, bag_size: 90\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 13\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 51\n",
      "batch 379, loss: 0.0139, instance_loss: 0.0277, weighted_loss: 0.0180, label: 0, bag_size: 40\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0023, weighted_loss: 0.0007, label: 1, bag_size: 78\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 88\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 42\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 68\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 109\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 73\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 84\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 49\n",
      "batch 559, loss: 0.1837, instance_loss: 0.2350, weighted_loss: 0.1991, label: 1, bag_size: 21\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 48\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 76\n",
      "batch 619, loss: 0.0007, instance_loss: 0.0131, weighted_loss: 0.0045, label: 1, bag_size: 62\n",
      "batch 639, loss: 0.0636, instance_loss: 0.6050, weighted_loss: 0.2261, label: 1, bag_size: 57\n",
      "batch 659, loss: 0.3694, instance_loss: 0.3085, weighted_loss: 0.3511, label: 1, bag_size: 79\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9807692307692307: correct 10404/10608\n",
      "class 1 clustering acc 0.8963046757164405: correct 4754/5304\n",
      "Epoch: 46, train_loss: 0.2252, train_clustering_loss:  0.2061, train_error: 0.0649\n",
      "class 0: acc 0.9331210191082803, correct 293/314\n",
      "class 1: acc 0.9369627507163324, correct 327/349\n",
      "\n",
      "Val Set, val_loss: 0.7597, val_error: 0.1412, auc: 0.9172\n",
      "class 0 clustering acc 0.9647058823529412: correct 1312/1360\n",
      "class 1 clustering acc 0.8117647058823529: correct 552/680\n",
      "class 0: acc 0.7777777777777778, correct 35/45\n",
      "class 1: acc 0.95, correct 38/40\n",
      "EarlyStopping counter: 37 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 75\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0030, weighted_loss: 0.0010, label: 0, bag_size: 88\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 48\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 108\n",
      "batch 99, loss: 0.0001, instance_loss: 1.5300, weighted_loss: 0.4591, label: 0, bag_size: 28\n",
      "batch 119, loss: 10.6061, instance_loss: 1.7921, weighted_loss: 7.9619, label: 1, bag_size: 46\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0035, weighted_loss: 0.0010, label: 1, bag_size: 74\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0043, weighted_loss: 0.0013, label: 0, bag_size: 40\n",
      "batch 179, loss: 0.0000, instance_loss: 0.8726, weighted_loss: 0.2618, label: 0, bag_size: 56\n",
      "batch 199, loss: 0.0000, instance_loss: 0.7295, weighted_loss: 0.2188, label: 1, bag_size: 68\n",
      "batch 219, loss: 0.0020, instance_loss: 0.4699, weighted_loss: 0.1424, label: 0, bag_size: 96\n",
      "batch 239, loss: 0.0000, instance_loss: 0.3115, weighted_loss: 0.0935, label: 0, bag_size: 78\n",
      "batch 259, loss: 0.1601, instance_loss: 0.3046, weighted_loss: 0.2034, label: 0, bag_size: 27\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0647, weighted_loss: 0.0194, label: 0, bag_size: 76\n",
      "batch 299, loss: 0.0466, instance_loss: 0.3925, weighted_loss: 0.1504, label: 0, bag_size: 78\n",
      "batch 319, loss: 0.0000, instance_loss: 0.8698, weighted_loss: 0.2609, label: 1, bag_size: 87\n",
      "batch 339, loss: 0.0031, instance_loss: 0.4960, weighted_loss: 0.1510, label: 1, bag_size: 99\n",
      "batch 359, loss: 0.0000, instance_loss: 1.2223, weighted_loss: 0.3667, label: 0, bag_size: 93\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0906, weighted_loss: 0.0272, label: 0, bag_size: 63\n",
      "batch 399, loss: 0.0000, instance_loss: 0.4932, weighted_loss: 0.1479, label: 1, bag_size: 65\n",
      "batch 419, loss: 12.9952, instance_loss: 1.4336, weighted_loss: 9.5267, label: 0, bag_size: 81\n",
      "batch 439, loss: 0.0000, instance_loss: 0.1862, weighted_loss: 0.0559, label: 1, bag_size: 69\n",
      "batch 459, loss: 0.0000, instance_loss: 0.6027, weighted_loss: 0.1808, label: 1, bag_size: 79\n",
      "batch 479, loss: 0.0429, instance_loss: 0.0888, weighted_loss: 0.0567, label: 0, bag_size: 64\n",
      "batch 499, loss: 0.0001, instance_loss: 0.2210, weighted_loss: 0.0663, label: 1, bag_size: 84\n",
      "batch 519, loss: 0.0004, instance_loss: 0.1595, weighted_loss: 0.0481, label: 1, bag_size: 34\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0336, weighted_loss: 0.0101, label: 1, bag_size: 94\n",
      "batch 559, loss: 9.5637, instance_loss: 2.8881, weighted_loss: 7.5610, label: 0, bag_size: 81\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0049, weighted_loss: 0.0015, label: 0, bag_size: 88\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0233, weighted_loss: 0.0070, label: 0, bag_size: 43\n",
      "batch 619, loss: 0.0073, instance_loss: 1.3319, weighted_loss: 0.4047, label: 0, bag_size: 40\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0334, weighted_loss: 0.0100, label: 0, bag_size: 45\n",
      "batch 659, loss: 0.0000, instance_loss: 0.4531, weighted_loss: 0.1359, label: 1, bag_size: 61\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9320324283559578: correct 9887/10608\n",
      "class 1 clustering acc 0.7345399698340875: correct 3896/5304\n",
      "Epoch: 47, train_loss: 1.3391, train_clustering_loss:  0.5114, train_error: 0.1644\n",
      "class 0: acc 0.8367952522255193, correct 282/337\n",
      "class 1: acc 0.8343558282208589, correct 272/326\n",
      "\n",
      "Val Set, val_loss: 2.5260, val_error: 0.3294, auc: 0.9083\n",
      "class 0 clustering acc 0.9323529411764706: correct 1268/1360\n",
      "class 1 clustering acc 0.7294117647058823: correct 496/680\n",
      "class 0: acc 0.4222222222222222, correct 19/45\n",
      "class 1: acc 0.95, correct 38/40\n",
      "EarlyStopping counter: 38 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.1469, weighted_loss: 0.0441, label: 1, bag_size: 37\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 0, bag_size: 87\n",
      "batch 59, loss: 0.0185, instance_loss: 0.2436, weighted_loss: 0.0860, label: 1, bag_size: 84\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 73\n",
      "batch 99, loss: 0.0080, instance_loss: 0.9036, weighted_loss: 0.2767, label: 1, bag_size: 35\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0978, weighted_loss: 0.0293, label: 1, bag_size: 35\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0354, weighted_loss: 0.0106, label: 0, bag_size: 94\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0001, label: 0, bag_size: 56\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 118\n",
      "batch 199, loss: 0.0047, instance_loss: 0.1384, weighted_loss: 0.0448, label: 0, bag_size: 38\n",
      "batch 219, loss: 0.0000, instance_loss: 0.1051, weighted_loss: 0.0315, label: 0, bag_size: 64\n",
      "batch 239, loss: 1.9277, instance_loss: 0.6678, weighted_loss: 1.5498, label: 1, bag_size: 38\n",
      "batch 259, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 113\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0047, weighted_loss: 0.0015, label: 1, bag_size: 87\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0316, weighted_loss: 0.0096, label: 1, bag_size: 34\n",
      "batch 319, loss: 0.0424, instance_loss: 0.0239, weighted_loss: 0.0368, label: 0, bag_size: 132\n",
      "batch 339, loss: 5.0632, instance_loss: 1.5519, weighted_loss: 4.0098, label: 0, bag_size: 28\n",
      "batch 359, loss: 0.0005, instance_loss: 0.0037, weighted_loss: 0.0015, label: 0, bag_size: 78\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0158, weighted_loss: 0.0047, label: 1, bag_size: 38\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 0, bag_size: 67\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 66\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0125, weighted_loss: 0.0038, label: 1, bag_size: 17\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0011, weighted_loss: 0.0004, label: 0, bag_size: 64\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 103\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 88\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0042, weighted_loss: 0.0013, label: 1, bag_size: 35\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0725, weighted_loss: 0.0217, label: 1, bag_size: 30\n",
      "batch 559, loss: 0.0038, instance_loss: 0.0087, weighted_loss: 0.0053, label: 1, bag_size: 58\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0010, weighted_loss: 0.0004, label: 0, bag_size: 109\n",
      "batch 599, loss: 0.0005, instance_loss: 0.0311, weighted_loss: 0.0097, label: 1, bag_size: 24\n",
      "batch 619, loss: 0.0002, instance_loss: 0.0166, weighted_loss: 0.0051, label: 0, bag_size: 76\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0035, weighted_loss: 0.0011, label: 0, bag_size: 35\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 111\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9706825037707391: correct 10297/10608\n",
      "class 1 clustering acc 0.8665158371040724: correct 4596/5304\n",
      "Epoch: 48, train_loss: 0.4434, train_clustering_loss:  0.2691, train_error: 0.0950\n",
      "class 0: acc 0.9024390243902439, correct 296/328\n",
      "class 1: acc 0.9074626865671642, correct 304/335\n",
      "\n",
      "Val Set, val_loss: 0.8010, val_error: 0.1647, auc: 0.9156\n",
      "class 0 clustering acc 0.9301470588235294: correct 1265/1360\n",
      "class 1 clustering acc 0.8132352941176471: correct 553/680\n",
      "class 0: acc 0.9555555555555556, correct 43/45\n",
      "class 1: acc 0.7, correct 28/40\n",
      "EarlyStopping counter: 39 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 4.3679, instance_loss: 2.0621, weighted_loss: 3.6762, label: 0, bag_size: 110\n",
      "batch 39, loss: 0.0003, instance_loss: 0.0176, weighted_loss: 0.0055, label: 0, bag_size: 29\n",
      "batch 59, loss: 0.0007, instance_loss: 0.0842, weighted_loss: 0.0257, label: 1, bag_size: 44\n",
      "batch 79, loss: 0.0016, instance_loss: 0.1408, weighted_loss: 0.0434, label: 0, bag_size: 50\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 0, bag_size: 64\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 1, bag_size: 98\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 116\n",
      "batch 159, loss: 0.0006, instance_loss: 0.0143, weighted_loss: 0.0047, label: 0, bag_size: 94\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 0, bag_size: 85\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 0, bag_size: 67\n",
      "batch 219, loss: 0.0671, instance_loss: 2.0595, weighted_loss: 0.6648, label: 1, bag_size: 31\n",
      "batch 239, loss: 0.0021, instance_loss: 0.0236, weighted_loss: 0.0085, label: 1, bag_size: 16\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0025, weighted_loss: 0.0008, label: 0, bag_size: 80\n",
      "batch 279, loss: 8.0209, instance_loss: 1.7588, weighted_loss: 6.1422, label: 1, bag_size: 82\n",
      "batch 299, loss: 1.9166, instance_loss: 3.1108, weighted_loss: 2.2748, label: 1, bag_size: 64\n",
      "batch 319, loss: 0.0010, instance_loss: 0.0087, weighted_loss: 0.0033, label: 1, bag_size: 78\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0009, weighted_loss: 0.0003, label: 0, bag_size: 36\n",
      "batch 359, loss: 1.8225, instance_loss: 0.4003, weighted_loss: 1.3958, label: 1, bag_size: 31\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 99\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 1, bag_size: 128\n",
      "batch 419, loss: 0.0032, instance_loss: 0.0187, weighted_loss: 0.0079, label: 0, bag_size: 61\n",
      "batch 439, loss: 0.1819, instance_loss: 0.4677, weighted_loss: 0.2677, label: 1, bag_size: 100\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0600, weighted_loss: 0.0180, label: 1, bag_size: 95\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 63\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 72\n",
      "batch 519, loss: 1.7396, instance_loss: 1.1586, weighted_loss: 1.5653, label: 1, bag_size: 102\n",
      "batch 539, loss: 0.0159, instance_loss: 0.3051, weighted_loss: 0.1026, label: 1, bag_size: 21\n",
      "batch 559, loss: 0.0630, instance_loss: 0.2295, weighted_loss: 0.1129, label: 1, bag_size: 78\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0147, weighted_loss: 0.0044, label: 1, bag_size: 84\n",
      "batch 599, loss: 0.0006, instance_loss: 0.0040, weighted_loss: 0.0016, label: 1, bag_size: 68\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0152, weighted_loss: 0.0046, label: 1, bag_size: 73\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 1, bag_size: 77\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0022, weighted_loss: 0.0007, label: 1, bag_size: 85\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9736990950226244: correct 10329/10608\n",
      "class 1 clustering acc 0.8823529411764706: correct 4680/5304\n",
      "Epoch: 49, train_loss: 0.2852, train_clustering_loss:  0.2235, train_error: 0.0920\n",
      "class 0: acc 0.9073482428115016, correct 284/313\n",
      "class 1: acc 0.9085714285714286, correct 318/350\n",
      "\n",
      "Val Set, val_loss: 0.9322, val_error: 0.2471, auc: 0.9133\n",
      "class 0 clustering acc 0.9220588235294118: correct 1254/1360\n",
      "class 1 clustering acc 0.7617647058823529: correct 518/680\n",
      "class 0: acc 0.6, correct 27/45\n",
      "class 1: acc 0.925, correct 37/40\n",
      "EarlyStopping counter: 40 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 21\n",
      "batch 39, loss: 0.0772, instance_loss: 0.0015, weighted_loss: 0.0545, label: 1, bag_size: 40\n",
      "batch 59, loss: 0.0014, instance_loss: 0.2446, weighted_loss: 0.0743, label: 0, bag_size: 93\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 80\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 102\n",
      "batch 119, loss: 0.0082, instance_loss: 0.0249, weighted_loss: 0.0132, label: 0, bag_size: 82\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 75\n",
      "batch 159, loss: 0.8841, instance_loss: 2.7506, weighted_loss: 1.4440, label: 1, bag_size: 67\n",
      "batch 179, loss: 0.0003, instance_loss: 0.0006, weighted_loss: 0.0004, label: 1, bag_size: 62\n",
      "batch 199, loss: 0.2942, instance_loss: 0.7583, weighted_loss: 0.4334, label: 1, bag_size: 70\n",
      "batch 219, loss: 0.8090, instance_loss: 1.1449, weighted_loss: 0.9098, label: 1, bag_size: 44\n",
      "batch 239, loss: 0.0002, instance_loss: 0.0107, weighted_loss: 0.0033, label: 1, bag_size: 42\n",
      "batch 259, loss: 0.0144, instance_loss: 0.0041, weighted_loss: 0.0113, label: 1, bag_size: 102\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 46\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 72\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 89\n",
      "batch 339, loss: 0.0018, instance_loss: 0.0102, weighted_loss: 0.0043, label: 0, bag_size: 26\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 110\n",
      "batch 379, loss: 0.0042, instance_loss: 0.0644, weighted_loss: 0.0222, label: 1, bag_size: 68\n",
      "batch 399, loss: 0.0000, instance_loss: 0.2882, weighted_loss: 0.0865, label: 1, bag_size: 29\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 75\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0048, weighted_loss: 0.0015, label: 0, bag_size: 57\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 107\n",
      "batch 479, loss: 0.0039, instance_loss: 0.0024, weighted_loss: 0.0034, label: 0, bag_size: 87\n",
      "batch 499, loss: 0.0815, instance_loss: 0.0303, weighted_loss: 0.0661, label: 1, bag_size: 70\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0033, weighted_loss: 0.0010, label: 1, bag_size: 53\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0667, weighted_loss: 0.0200, label: 1, bag_size: 85\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 116\n",
      "batch 579, loss: 10.1268, instance_loss: 2.1581, weighted_loss: 7.7362, label: 1, bag_size: 57\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 1, bag_size: 45\n",
      "batch 619, loss: 0.0000, instance_loss: 0.2593, weighted_loss: 0.0778, label: 1, bag_size: 109\n",
      "batch 639, loss: 0.0000, instance_loss: 0.1484, weighted_loss: 0.0445, label: 1, bag_size: 68\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0493, weighted_loss: 0.0148, label: 0, bag_size: 116\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9671003016591252: correct 10259/10608\n",
      "class 1 clustering acc 0.8702865761689291: correct 4616/5304\n",
      "Epoch: 50, train_loss: 0.5735, train_clustering_loss:  0.2715, train_error: 0.1116\n",
      "class 0: acc 0.8895522388059701, correct 298/335\n",
      "class 1: acc 0.8871951219512195, correct 291/328\n",
      "\n",
      "Val Set, val_loss: 0.9651, val_error: 0.1529, auc: 0.9261\n",
      "class 0 clustering acc 0.8382352941176471: correct 1140/1360\n",
      "class 1 clustering acc 0.6602941176470588: correct 449/680\n",
      "class 0: acc 0.8444444444444444, correct 38/45\n",
      "class 1: acc 0.85, correct 34/40\n",
      "EarlyStopping counter: 41 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.5039, weighted_loss: 0.1513, label: 1, bag_size: 21\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0297, weighted_loss: 0.0089, label: 1, bag_size: 120\n",
      "batch 59, loss: 0.0000, instance_loss: 0.2047, weighted_loss: 0.0614, label: 0, bag_size: 72\n",
      "batch 79, loss: 0.0000, instance_loss: 1.8745, weighted_loss: 0.5624, label: 0, bag_size: 78\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0094, weighted_loss: 0.0028, label: 1, bag_size: 80\n",
      "batch 119, loss: 0.0001, instance_loss: 0.5089, weighted_loss: 0.1528, label: 0, bag_size: 21\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0351, weighted_loss: 0.0105, label: 1, bag_size: 100\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0646, weighted_loss: 0.0194, label: 0, bag_size: 31\n",
      "batch 179, loss: 0.0043, instance_loss: 0.4762, weighted_loss: 0.1459, label: 1, bag_size: 41\n",
      "batch 199, loss: 0.0005, instance_loss: 0.0443, weighted_loss: 0.0137, label: 1, bag_size: 83\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0541, weighted_loss: 0.0162, label: 0, bag_size: 43\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0027, weighted_loss: 0.0008, label: 0, bag_size: 29\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 1, bag_size: 44\n",
      "batch 279, loss: 0.0181, instance_loss: 1.4833, weighted_loss: 0.4577, label: 0, bag_size: 60\n",
      "batch 299, loss: 0.7381, instance_loss: 0.9800, weighted_loss: 0.8106, label: 0, bag_size: 54\n",
      "batch 319, loss: 0.0000, instance_loss: 0.3209, weighted_loss: 0.0963, label: 1, bag_size: 56\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0124, weighted_loss: 0.0037, label: 1, bag_size: 35\n",
      "batch 359, loss: 0.0183, instance_loss: 0.0378, weighted_loss: 0.0242, label: 0, bag_size: 104\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0662, weighted_loss: 0.0199, label: 0, bag_size: 25\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0302, weighted_loss: 0.0091, label: 0, bag_size: 33\n",
      "batch 419, loss: 9.0582, instance_loss: 0.1534, weighted_loss: 6.3868, label: 1, bag_size: 68\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0453, weighted_loss: 0.0136, label: 1, bag_size: 100\n",
      "batch 459, loss: 0.0105, instance_loss: 0.0681, weighted_loss: 0.0278, label: 1, bag_size: 87\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0135, weighted_loss: 0.0040, label: 1, bag_size: 31\n",
      "batch 499, loss: 0.0002, instance_loss: 0.2642, weighted_loss: 0.0794, label: 0, bag_size: 18\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0061, weighted_loss: 0.0019, label: 1, bag_size: 63\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0418, weighted_loss: 0.0125, label: 0, bag_size: 33\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0055, weighted_loss: 0.0016, label: 1, bag_size: 36\n",
      "batch 579, loss: 0.0000, instance_loss: 1.6194, weighted_loss: 0.4858, label: 1, bag_size: 41\n",
      "batch 599, loss: 0.0012, instance_loss: 0.6822, weighted_loss: 0.2055, label: 0, bag_size: 68\n",
      "batch 619, loss: 0.1070, instance_loss: 0.8322, weighted_loss: 0.3246, label: 1, bag_size: 100\n",
      "batch 639, loss: 0.0224, instance_loss: 0.5522, weighted_loss: 0.1813, label: 0, bag_size: 91\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 47\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9489064856711915: correct 10066/10608\n",
      "class 1 clustering acc 0.7861990950226244: correct 4170/5304\n",
      "Epoch: 51, train_loss: 1.0533, train_clustering_loss:  0.4467, train_error: 0.1327\n",
      "class 0: acc 0.8668639053254438, correct 293/338\n",
      "class 1: acc 0.8676923076923077, correct 282/325\n",
      "\n",
      "Val Set, val_loss: 0.9116, val_error: 0.1647, auc: 0.9322\n",
      "class 0 clustering acc 0.9169117647058823: correct 1247/1360\n",
      "class 1 clustering acc 0.7897058823529411: correct 537/680\n",
      "class 0: acc 0.7777777777777778, correct 35/45\n",
      "class 1: acc 0.9, correct 36/40\n",
      "EarlyStopping counter: 42 out of 20\n",
      "Early stopping\n",
      "Val error: 0.2235, ROC AUC: 0.8950\n",
      "Test error: 0.2125, ROC AUC: 0.9050\n",
      "class 0: acc 0.7, correct 28/40\n",
      "class 1: acc 0.875, correct 35/40\n",
      "\n",
      "Training Fold 1!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 671 samples\n",
      "Validating on 84 samples\n",
      "Testing on 73 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Setting tau to 1.0\n",
      "Done!\n",
      "MCBAT_SB(\n",
      "  (instance_loss_fn): SmoothTop1SVM()\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (transformer_low): TransformerEncoder_FLASHAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FlashAttention(\n",
      "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
      "            (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer_high): TransformerEncoder_FLASHAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FlashAttention(\n",
      "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
      "            (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attention): Attn_Net_Gated(\n",
      "    (attention_a): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_b): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (attention_V2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (attention_U2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (attention_weights2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Total number of parameters: 8408073\n",
      "Total number of trainable parameters: 8408073\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "671.0\n",
      "2\n",
      "318\n",
      "353\n",
      "##################################################\n",
      "Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 1.7067, instance_loss: 1.2883, weighted_loss: 1.5812, label: 0, bag_size: 58\n",
      "batch 39, loss: 0.3690, instance_loss: 0.6932, weighted_loss: 0.4663, label: 0, bag_size: 82\n",
      "batch 59, loss: 1.3974, instance_loss: 0.8822, weighted_loss: 1.2429, label: 1, bag_size: 100\n",
      "batch 79, loss: 0.0901, instance_loss: 1.1246, weighted_loss: 0.4004, label: 1, bag_size: 34\n",
      "batch 99, loss: 2.2075, instance_loss: 0.9390, weighted_loss: 1.8269, label: 0, bag_size: 62\n",
      "batch 119, loss: 6.8951, instance_loss: 0.9849, weighted_loss: 5.1220, label: 0, bag_size: 114\n",
      "batch 139, loss: 0.2756, instance_loss: 1.0877, weighted_loss: 0.5192, label: 0, bag_size: 63\n",
      "batch 159, loss: 2.4989, instance_loss: 1.6241, weighted_loss: 2.2365, label: 0, bag_size: 48\n",
      "batch 179, loss: 1.2132, instance_loss: 1.3997, weighted_loss: 1.2692, label: 0, bag_size: 44\n",
      "batch 199, loss: 0.2690, instance_loss: 0.9900, weighted_loss: 0.4853, label: 1, bag_size: 85\n",
      "batch 219, loss: 1.1043, instance_loss: 0.8536, weighted_loss: 1.0290, label: 0, bag_size: 20\n",
      "batch 239, loss: 0.3657, instance_loss: 1.5445, weighted_loss: 0.7193, label: 1, bag_size: 46\n",
      "batch 259, loss: 0.1484, instance_loss: 0.8532, weighted_loss: 0.3599, label: 1, bag_size: 32\n",
      "batch 279, loss: 0.4827, instance_loss: 0.9802, weighted_loss: 0.6319, label: 1, bag_size: 40\n",
      "batch 299, loss: 0.2113, instance_loss: 1.1907, weighted_loss: 0.5051, label: 0, bag_size: 98\n",
      "batch 319, loss: 1.4418, instance_loss: 0.7944, weighted_loss: 1.2476, label: 1, bag_size: 39\n",
      "batch 339, loss: 3.6862, instance_loss: 0.8667, weighted_loss: 2.8404, label: 0, bag_size: 71\n",
      "batch 359, loss: 0.2515, instance_loss: 1.4068, weighted_loss: 0.5981, label: 1, bag_size: 89\n",
      "batch 379, loss: 1.3630, instance_loss: 0.7228, weighted_loss: 1.1710, label: 1, bag_size: 97\n",
      "batch 399, loss: 2.4863, instance_loss: 0.6759, weighted_loss: 1.9432, label: 0, bag_size: 46\n",
      "batch 419, loss: 0.6085, instance_loss: 1.0885, weighted_loss: 0.7525, label: 0, bag_size: 78\n",
      "batch 439, loss: 0.2961, instance_loss: 1.0203, weighted_loss: 0.5134, label: 0, bag_size: 52\n",
      "batch 459, loss: 0.1833, instance_loss: 0.6805, weighted_loss: 0.3324, label: 1, bag_size: 90\n",
      "batch 479, loss: 1.3800, instance_loss: 0.8845, weighted_loss: 1.2313, label: 0, bag_size: 47\n",
      "batch 499, loss: 0.1450, instance_loss: 0.9601, weighted_loss: 0.3895, label: 1, bag_size: 26\n",
      "batch 519, loss: 1.7094, instance_loss: 1.3164, weighted_loss: 1.5915, label: 1, bag_size: 27\n",
      "batch 539, loss: 2.1864, instance_loss: 0.5558, weighted_loss: 1.6972, label: 1, bag_size: 50\n",
      "batch 559, loss: 0.3584, instance_loss: 1.1835, weighted_loss: 0.6059, label: 0, bag_size: 96\n",
      "batch 579, loss: 1.1969, instance_loss: 0.9156, weighted_loss: 1.1125, label: 1, bag_size: 77\n",
      "batch 599, loss: 1.7766, instance_loss: 1.1040, weighted_loss: 1.5748, label: 1, bag_size: 17\n",
      "batch 619, loss: 3.0070, instance_loss: 1.7322, weighted_loss: 2.6246, label: 1, bag_size: 56\n",
      "batch 639, loss: 0.7561, instance_loss: 1.3462, weighted_loss: 0.9331, label: 0, bag_size: 25\n",
      "batch 659, loss: 2.0030, instance_loss: 1.2620, weighted_loss: 1.7807, label: 0, bag_size: 54\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9763412816691506: correct 10482/10736\n",
      "class 1 clustering acc 0.06557377049180328: correct 352/5368\n",
      "Epoch: 0, train_loss: 1.0500, train_clustering_loss:  1.0028, train_error: 0.4784\n",
      "class 0: acc 0.5045317220543807, correct 167/331\n",
      "class 1: acc 0.538235294117647, correct 183/340\n",
      "\n",
      "Val Set, val_loss: 0.5558, val_error: 0.2738, auc: 0.8803\n",
      "class 0 clustering acc 0.9717261904761905: correct 1306/1344\n",
      "class 1 clustering acc 0.2113095238095238: correct 142/672\n",
      "class 0: acc 0.5581395348837209, correct 24/43\n",
      "class 1: acc 0.9024390243902439, correct 37/41\n",
      "Validation loss decreased (inf --> 0.555833).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0133, instance_loss: 0.8796, weighted_loss: 0.2732, label: 1, bag_size: 85\n",
      "batch 39, loss: 0.0730, instance_loss: 0.8789, weighted_loss: 0.3147, label: 0, bag_size: 28\n",
      "batch 59, loss: 0.5568, instance_loss: 0.7472, weighted_loss: 0.6139, label: 0, bag_size: 48\n",
      "batch 79, loss: 2.7187, instance_loss: 1.1038, weighted_loss: 2.2342, label: 0, bag_size: 67\n",
      "batch 99, loss: 0.2900, instance_loss: 0.9339, weighted_loss: 0.4832, label: 0, bag_size: 28\n",
      "batch 119, loss: 0.1549, instance_loss: 0.8602, weighted_loss: 0.3665, label: 0, bag_size: 107\n",
      "batch 139, loss: 0.1362, instance_loss: 0.6542, weighted_loss: 0.2916, label: 0, bag_size: 74\n",
      "batch 159, loss: 0.0623, instance_loss: 0.7484, weighted_loss: 0.2681, label: 0, bag_size: 73\n",
      "batch 179, loss: 0.0088, instance_loss: 0.9251, weighted_loss: 0.2837, label: 0, bag_size: 103\n",
      "batch 199, loss: 2.4809, instance_loss: 1.3733, weighted_loss: 2.1486, label: 1, bag_size: 42\n",
      "batch 219, loss: 1.5347, instance_loss: 1.0007, weighted_loss: 1.3745, label: 1, bag_size: 88\n",
      "batch 239, loss: 0.1526, instance_loss: 0.9420, weighted_loss: 0.3894, label: 0, bag_size: 85\n",
      "batch 259, loss: 2.7228, instance_loss: 1.7914, weighted_loss: 2.4434, label: 0, bag_size: 55\n",
      "batch 279, loss: 0.0768, instance_loss: 0.7251, weighted_loss: 0.2713, label: 0, bag_size: 79\n",
      "batch 299, loss: 0.9502, instance_loss: 1.1215, weighted_loss: 1.0016, label: 1, bag_size: 59\n",
      "batch 319, loss: 2.5165, instance_loss: 1.3194, weighted_loss: 2.1574, label: 0, bag_size: 43\n",
      "batch 339, loss: 0.0063, instance_loss: 0.6452, weighted_loss: 0.1979, label: 0, bag_size: 67\n",
      "batch 359, loss: 0.5470, instance_loss: 0.7713, weighted_loss: 0.6143, label: 0, bag_size: 53\n",
      "batch 379, loss: 0.5265, instance_loss: 0.8093, weighted_loss: 0.6113, label: 0, bag_size: 88\n",
      "batch 399, loss: 1.5280, instance_loss: 1.9505, weighted_loss: 1.6547, label: 0, bag_size: 55\n",
      "batch 419, loss: 0.0051, instance_loss: 0.4747, weighted_loss: 0.1460, label: 1, bag_size: 61\n",
      "batch 439, loss: 0.4533, instance_loss: 0.7562, weighted_loss: 0.5441, label: 0, bag_size: 27\n",
      "batch 459, loss: 0.9401, instance_loss: 0.8071, weighted_loss: 0.9002, label: 0, bag_size: 40\n",
      "batch 479, loss: 0.0961, instance_loss: 0.5518, weighted_loss: 0.2328, label: 1, bag_size: 29\n",
      "batch 499, loss: 1.5479, instance_loss: 0.9107, weighted_loss: 1.3567, label: 0, bag_size: 46\n",
      "batch 519, loss: 0.0396, instance_loss: 0.6886, weighted_loss: 0.2343, label: 0, bag_size: 37\n",
      "batch 539, loss: 0.1947, instance_loss: 0.2888, weighted_loss: 0.2229, label: 1, bag_size: 36\n",
      "batch 559, loss: 0.3437, instance_loss: 0.8753, weighted_loss: 0.5032, label: 1, bag_size: 126\n",
      "batch 579, loss: 0.5363, instance_loss: 1.0328, weighted_loss: 0.6852, label: 1, bag_size: 65\n",
      "batch 599, loss: 0.0203, instance_loss: 0.1349, weighted_loss: 0.0547, label: 1, bag_size: 74\n",
      "batch 619, loss: 0.0000, instance_loss: 0.6331, weighted_loss: 0.1899, label: 0, bag_size: 93\n",
      "batch 639, loss: 0.0297, instance_loss: 0.7367, weighted_loss: 0.2418, label: 0, bag_size: 37\n",
      "batch 659, loss: 0.0570, instance_loss: 0.6225, weighted_loss: 0.2267, label: 0, bag_size: 110\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9637667660208644: correct 10347/10736\n",
      "class 1 clustering acc 0.1954172876304024: correct 1049/5368\n",
      "Epoch: 1, train_loss: 0.7604, train_clustering_loss:  0.9211, train_error: 0.3458\n",
      "class 0: acc 0.6413373860182371, correct 211/329\n",
      "class 1: acc 0.6666666666666666, correct 228/342\n",
      "\n",
      "Val Set, val_loss: 0.4853, val_error: 0.2024, auc: 0.8815\n",
      "class 0 clustering acc 0.9821428571428571: correct 1320/1344\n",
      "class 1 clustering acc 0.29910714285714285: correct 201/672\n",
      "class 0: acc 0.6744186046511628, correct 29/43\n",
      "class 1: acc 0.926829268292683, correct 38/41\n",
      "Validation loss decreased (0.555833 --> 0.485293).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 3.6284, instance_loss: 1.8555, weighted_loss: 3.0966, label: 1, bag_size: 59\n",
      "batch 39, loss: 0.8252, instance_loss: 0.6270, weighted_loss: 0.7658, label: 1, bag_size: 74\n",
      "batch 59, loss: 0.1117, instance_loss: 0.4680, weighted_loss: 0.2186, label: 1, bag_size: 85\n",
      "batch 79, loss: 0.4169, instance_loss: 1.4432, weighted_loss: 0.7248, label: 0, bag_size: 28\n",
      "batch 99, loss: 0.0100, instance_loss: 0.4993, weighted_loss: 0.1568, label: 0, bag_size: 29\n",
      "batch 119, loss: 0.1317, instance_loss: 0.2995, weighted_loss: 0.1820, label: 1, bag_size: 99\n",
      "batch 139, loss: 0.0428, instance_loss: 0.1282, weighted_loss: 0.0684, label: 1, bag_size: 120\n",
      "batch 159, loss: 0.2314, instance_loss: 0.6346, weighted_loss: 0.3523, label: 0, bag_size: 18\n",
      "batch 179, loss: 0.9852, instance_loss: 1.4532, weighted_loss: 1.1256, label: 1, bag_size: 44\n",
      "batch 199, loss: 1.3343, instance_loss: 1.3793, weighted_loss: 1.3478, label: 1, bag_size: 53\n",
      "batch 219, loss: 0.5795, instance_loss: 0.4760, weighted_loss: 0.5484, label: 0, bag_size: 103\n",
      "batch 239, loss: 0.1917, instance_loss: 0.7300, weighted_loss: 0.3532, label: 0, bag_size: 28\n",
      "batch 259, loss: 0.1015, instance_loss: 0.1245, weighted_loss: 0.1084, label: 1, bag_size: 45\n",
      "batch 279, loss: 0.0763, instance_loss: 0.1267, weighted_loss: 0.0914, label: 1, bag_size: 51\n",
      "batch 299, loss: 0.1448, instance_loss: 0.8437, weighted_loss: 0.3544, label: 0, bag_size: 34\n",
      "batch 319, loss: 1.3986, instance_loss: 2.1502, weighted_loss: 1.6241, label: 0, bag_size: 77\n",
      "batch 339, loss: 0.0770, instance_loss: 0.4662, weighted_loss: 0.1938, label: 1, bag_size: 76\n",
      "batch 359, loss: 1.2078, instance_loss: 0.7465, weighted_loss: 1.0694, label: 1, bag_size: 59\n",
      "batch 379, loss: 0.1331, instance_loss: 0.4644, weighted_loss: 0.2325, label: 1, bag_size: 78\n",
      "batch 399, loss: 0.0326, instance_loss: 1.1383, weighted_loss: 0.3643, label: 1, bag_size: 70\n",
      "batch 419, loss: 0.0020, instance_loss: 0.3221, weighted_loss: 0.0980, label: 0, bag_size: 83\n",
      "batch 439, loss: 0.2238, instance_loss: 0.3603, weighted_loss: 0.2647, label: 1, bag_size: 127\n",
      "batch 459, loss: 0.3088, instance_loss: 0.2896, weighted_loss: 0.3031, label: 1, bag_size: 110\n",
      "batch 479, loss: 0.0108, instance_loss: 0.4344, weighted_loss: 0.1379, label: 0, bag_size: 94\n",
      "batch 499, loss: 0.3917, instance_loss: 0.3346, weighted_loss: 0.3746, label: 1, bag_size: 25\n",
      "batch 519, loss: 0.3928, instance_loss: 0.4509, weighted_loss: 0.4102, label: 1, bag_size: 71\n",
      "batch 539, loss: 0.5599, instance_loss: 1.4666, weighted_loss: 0.8319, label: 1, bag_size: 98\n",
      "batch 559, loss: 0.0246, instance_loss: 0.2557, weighted_loss: 0.0939, label: 1, bag_size: 62\n",
      "batch 579, loss: 0.0461, instance_loss: 0.5325, weighted_loss: 0.1921, label: 0, bag_size: 75\n",
      "batch 599, loss: 0.0670, instance_loss: 0.7524, weighted_loss: 0.2726, label: 1, bag_size: 35\n",
      "batch 619, loss: 0.0156, instance_loss: 1.3842, weighted_loss: 0.4262, label: 0, bag_size: 32\n",
      "batch 639, loss: 4.0278, instance_loss: 1.2691, weighted_loss: 3.2002, label: 0, bag_size: 88\n",
      "batch 659, loss: 0.4918, instance_loss: 1.5417, weighted_loss: 0.8067, label: 1, bag_size: 51\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9370342771982116: correct 10060/10736\n",
      "class 1 clustering acc 0.4090909090909091: correct 2196/5368\n",
      "Epoch: 2, train_loss: 0.6528, train_clustering_loss:  0.7867, train_error: 0.2817\n",
      "class 0: acc 0.7053291536050157, correct 225/319\n",
      "class 1: acc 0.7301136363636364, correct 257/352\n",
      "\n",
      "Val Set, val_loss: 0.5034, val_error: 0.2500, auc: 0.9075\n",
      "class 0 clustering acc 0.9642857142857143: correct 1296/1344\n",
      "class 1 clustering acc 0.3794642857142857: correct 255/672\n",
      "class 0: acc 0.9534883720930233, correct 41/43\n",
      "class 1: acc 0.5365853658536586, correct 22/41\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2589, instance_loss: 2.4735, weighted_loss: 0.9233, label: 0, bag_size: 66\n",
      "batch 39, loss: 0.0845, instance_loss: 0.2236, weighted_loss: 0.1262, label: 1, bag_size: 67\n",
      "batch 59, loss: 3.1007, instance_loss: 1.7561, weighted_loss: 2.6973, label: 1, bag_size: 18\n",
      "batch 79, loss: 1.0930, instance_loss: 0.6320, weighted_loss: 0.9547, label: 1, bag_size: 41\n",
      "batch 99, loss: 0.0076, instance_loss: 0.1543, weighted_loss: 0.0516, label: 0, bag_size: 78\n",
      "batch 119, loss: 0.0100, instance_loss: 0.4435, weighted_loss: 0.1401, label: 1, bag_size: 120\n",
      "batch 139, loss: 0.0670, instance_loss: 0.2506, weighted_loss: 0.1221, label: 0, bag_size: 88\n",
      "batch 159, loss: 0.1228, instance_loss: 0.9450, weighted_loss: 0.3694, label: 0, bag_size: 85\n",
      "batch 179, loss: 0.5412, instance_loss: 0.9099, weighted_loss: 0.6518, label: 0, bag_size: 77\n",
      "batch 199, loss: 0.2537, instance_loss: 0.7936, weighted_loss: 0.4157, label: 0, bag_size: 36\n",
      "batch 219, loss: 0.0722, instance_loss: 0.8950, weighted_loss: 0.3190, label: 1, bag_size: 26\n",
      "batch 239, loss: 0.2663, instance_loss: 1.2331, weighted_loss: 0.5563, label: 0, bag_size: 21\n",
      "batch 259, loss: 0.0479, instance_loss: 0.4273, weighted_loss: 0.1617, label: 0, bag_size: 83\n",
      "batch 279, loss: 3.0243, instance_loss: 1.9278, weighted_loss: 2.6953, label: 0, bag_size: 35\n",
      "batch 299, loss: 1.0514, instance_loss: 1.4208, weighted_loss: 1.1622, label: 1, bag_size: 71\n",
      "batch 319, loss: 0.0700, instance_loss: 0.4181, weighted_loss: 0.1744, label: 1, bag_size: 75\n",
      "batch 339, loss: 0.0099, instance_loss: 0.5845, weighted_loss: 0.1823, label: 1, bag_size: 38\n",
      "batch 359, loss: 3.4372, instance_loss: 0.4662, weighted_loss: 2.5459, label: 1, bag_size: 82\n",
      "batch 379, loss: 2.4712, instance_loss: 2.0316, weighted_loss: 2.3393, label: 1, bag_size: 73\n",
      "batch 399, loss: 0.6502, instance_loss: 0.4035, weighted_loss: 0.5762, label: 0, bag_size: 35\n",
      "batch 419, loss: 1.6978, instance_loss: 1.0967, weighted_loss: 1.5175, label: 0, bag_size: 110\n",
      "batch 439, loss: 0.1120, instance_loss: 2.1866, weighted_loss: 0.7344, label: 1, bag_size: 67\n",
      "batch 459, loss: 0.0182, instance_loss: 0.6809, weighted_loss: 0.2170, label: 1, bag_size: 78\n",
      "batch 479, loss: 0.4204, instance_loss: 0.2484, weighted_loss: 0.3688, label: 0, bag_size: 32\n",
      "batch 499, loss: 0.4772, instance_loss: 0.6959, weighted_loss: 0.5428, label: 1, bag_size: 53\n",
      "batch 519, loss: 1.0692, instance_loss: 0.1596, weighted_loss: 0.7963, label: 0, bag_size: 37\n",
      "batch 539, loss: 3.7304, instance_loss: 3.4103, weighted_loss: 3.6344, label: 0, bag_size: 73\n",
      "batch 559, loss: 0.0007, instance_loss: 0.8167, weighted_loss: 0.2455, label: 1, bag_size: 86\n",
      "batch 579, loss: 0.0125, instance_loss: 0.3847, weighted_loss: 0.1241, label: 0, bag_size: 108\n",
      "batch 599, loss: 0.2653, instance_loss: 0.5802, weighted_loss: 0.3597, label: 1, bag_size: 35\n",
      "batch 619, loss: 3.7726, instance_loss: 1.9482, weighted_loss: 3.2253, label: 0, bag_size: 72\n",
      "batch 639, loss: 0.0098, instance_loss: 0.1774, weighted_loss: 0.0601, label: 0, bag_size: 94\n",
      "batch 659, loss: 0.0721, instance_loss: 0.3625, weighted_loss: 0.1593, label: 1, bag_size: 65\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9292101341281669: correct 9976/10736\n",
      "class 1 clustering acc 0.39623695976154993: correct 2127/5368\n",
      "Epoch: 3, train_loss: 0.5903, train_clustering_loss:  0.7897, train_error: 0.2370\n",
      "class 0: acc 0.7590361445783133, correct 252/332\n",
      "class 1: acc 0.7669616519174042, correct 260/339\n",
      "\n",
      "Val Set, val_loss: 0.3618, val_error: 0.1905, auc: 0.9387\n",
      "class 0 clustering acc 0.9434523809523809: correct 1268/1344\n",
      "class 1 clustering acc 0.7589285714285714: correct 510/672\n",
      "class 0: acc 0.8837209302325582, correct 38/43\n",
      "class 1: acc 0.7317073170731707, correct 30/41\n",
      "Validation loss decreased (0.485293 --> 0.361845).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1720, instance_loss: 0.1669, weighted_loss: 0.1705, label: 1, bag_size: 33\n",
      "batch 39, loss: 0.0101, instance_loss: 0.0991, weighted_loss: 0.0368, label: 0, bag_size: 65\n",
      "batch 59, loss: 0.0233, instance_loss: 0.2245, weighted_loss: 0.0837, label: 1, bag_size: 47\n",
      "batch 79, loss: 0.0901, instance_loss: 0.9658, weighted_loss: 0.3528, label: 0, bag_size: 97\n",
      "batch 99, loss: 0.8308, instance_loss: 0.8700, weighted_loss: 0.8426, label: 1, bag_size: 98\n",
      "batch 119, loss: 1.1377, instance_loss: 0.6859, weighted_loss: 1.0022, label: 0, bag_size: 95\n",
      "batch 139, loss: 0.0041, instance_loss: 0.5114, weighted_loss: 0.1563, label: 0, bag_size: 50\n",
      "batch 159, loss: 0.0285, instance_loss: 0.1791, weighted_loss: 0.0737, label: 0, bag_size: 40\n",
      "batch 179, loss: 0.0001, instance_loss: 0.1533, weighted_loss: 0.0461, label: 1, bag_size: 72\n",
      "batch 199, loss: 0.0132, instance_loss: 0.0918, weighted_loss: 0.0368, label: 0, bag_size: 67\n",
      "batch 219, loss: 0.0004, instance_loss: 0.7075, weighted_loss: 0.2126, label: 1, bag_size: 86\n",
      "batch 239, loss: 0.8491, instance_loss: 0.3581, weighted_loss: 0.7018, label: 0, bag_size: 60\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0681, weighted_loss: 0.0204, label: 0, bag_size: 116\n",
      "batch 279, loss: 0.5612, instance_loss: 0.9703, weighted_loss: 0.6839, label: 1, bag_size: 42\n",
      "batch 299, loss: 1.1195, instance_loss: 1.0213, weighted_loss: 1.0901, label: 1, bag_size: 121\n",
      "batch 319, loss: 0.6244, instance_loss: 1.7566, weighted_loss: 0.9641, label: 0, bag_size: 85\n",
      "batch 339, loss: 0.2480, instance_loss: 1.0389, weighted_loss: 0.4853, label: 1, bag_size: 38\n",
      "batch 359, loss: 0.1281, instance_loss: 0.4847, weighted_loss: 0.2351, label: 1, bag_size: 75\n",
      "batch 379, loss: 0.0082, instance_loss: 0.0749, weighted_loss: 0.0282, label: 0, bag_size: 113\n",
      "batch 399, loss: 0.0234, instance_loss: 0.1575, weighted_loss: 0.0636, label: 1, bag_size: 62\n",
      "batch 419, loss: 0.0323, instance_loss: 0.0278, weighted_loss: 0.0310, label: 0, bag_size: 88\n",
      "batch 439, loss: 0.0016, instance_loss: 0.0428, weighted_loss: 0.0139, label: 1, bag_size: 84\n",
      "batch 459, loss: 1.8271, instance_loss: 0.9735, weighted_loss: 1.5710, label: 1, bag_size: 38\n",
      "batch 479, loss: 0.0031, instance_loss: 0.0337, weighted_loss: 0.0123, label: 1, bag_size: 123\n",
      "batch 499, loss: 0.0037, instance_loss: 0.0490, weighted_loss: 0.0173, label: 1, bag_size: 80\n",
      "batch 519, loss: 0.0005, instance_loss: 0.2528, weighted_loss: 0.0762, label: 1, bag_size: 109\n",
      "batch 539, loss: 0.0044, instance_loss: 0.0636, weighted_loss: 0.0221, label: 1, bag_size: 28\n",
      "batch 559, loss: 0.0032, instance_loss: 0.0773, weighted_loss: 0.0254, label: 0, bag_size: 114\n",
      "batch 579, loss: 0.0037, instance_loss: 0.0072, weighted_loss: 0.0047, label: 0, bag_size: 109\n",
      "batch 599, loss: 0.4976, instance_loss: 0.8022, weighted_loss: 0.5890, label: 0, bag_size: 85\n",
      "batch 619, loss: 0.0499, instance_loss: 0.0462, weighted_loss: 0.0488, label: 0, bag_size: 53\n",
      "batch 639, loss: 4.8920, instance_loss: 0.4633, weighted_loss: 3.5634, label: 1, bag_size: 76\n",
      "batch 659, loss: 0.0013, instance_loss: 0.0281, weighted_loss: 0.0094, label: 0, bag_size: 106\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9281855439642325: correct 9965/10736\n",
      "class 1 clustering acc 0.5620342771982116: correct 3017/5368\n",
      "Epoch: 4, train_loss: 0.5406, train_clustering_loss:  0.6643, train_error: 0.1833\n",
      "class 0: acc 0.794392523364486, correct 255/321\n",
      "class 1: acc 0.8371428571428572, correct 293/350\n",
      "\n",
      "Val Set, val_loss: 0.4706, val_error: 0.2024, auc: 0.9092\n",
      "class 0 clustering acc 0.9412202380952381: correct 1265/1344\n",
      "class 1 clustering acc 0.6622023809523809: correct 445/672\n",
      "class 0: acc 0.7441860465116279, correct 32/43\n",
      "class 1: acc 0.8536585365853658, correct 35/41\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1080, instance_loss: 0.0286, weighted_loss: 0.0841, label: 1, bag_size: 126\n",
      "batch 39, loss: 0.6858, instance_loss: 0.1488, weighted_loss: 0.5247, label: 0, bag_size: 29\n",
      "batch 59, loss: 0.0263, instance_loss: 0.2726, weighted_loss: 0.1002, label: 1, bag_size: 118\n",
      "batch 79, loss: 1.7590, instance_loss: 0.6564, weighted_loss: 1.4282, label: 0, bag_size: 48\n",
      "batch 99, loss: 0.0024, instance_loss: 0.0880, weighted_loss: 0.0281, label: 1, bag_size: 98\n",
      "batch 119, loss: 0.0576, instance_loss: 0.1569, weighted_loss: 0.0874, label: 1, bag_size: 39\n",
      "batch 139, loss: 0.0290, instance_loss: 0.1330, weighted_loss: 0.0602, label: 1, bag_size: 56\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0220, weighted_loss: 0.0066, label: 0, bag_size: 114\n",
      "batch 179, loss: 0.1049, instance_loss: 0.1790, weighted_loss: 0.1271, label: 0, bag_size: 57\n",
      "batch 199, loss: 0.0086, instance_loss: 0.2815, weighted_loss: 0.0905, label: 0, bag_size: 106\n",
      "batch 219, loss: 0.0078, instance_loss: 0.5441, weighted_loss: 0.1687, label: 0, bag_size: 36\n",
      "batch 239, loss: 0.0355, instance_loss: 0.5617, weighted_loss: 0.1934, label: 0, bag_size: 30\n",
      "batch 259, loss: 0.0144, instance_loss: 0.8440, weighted_loss: 0.2633, label: 1, bag_size: 83\n",
      "batch 279, loss: 0.0515, instance_loss: 0.0805, weighted_loss: 0.0602, label: 1, bag_size: 97\n",
      "batch 299, loss: 0.0000, instance_loss: 0.2047, weighted_loss: 0.0614, label: 0, bag_size: 80\n",
      "batch 319, loss: 0.4994, instance_loss: 0.4168, weighted_loss: 0.4746, label: 1, bag_size: 100\n",
      "batch 339, loss: 0.0224, instance_loss: 0.7297, weighted_loss: 0.2346, label: 1, bag_size: 26\n",
      "batch 359, loss: 0.0015, instance_loss: 0.0957, weighted_loss: 0.0297, label: 0, bag_size: 65\n",
      "batch 379, loss: 0.1988, instance_loss: 0.6489, weighted_loss: 0.3338, label: 0, bag_size: 46\n",
      "batch 399, loss: 0.0142, instance_loss: 0.2905, weighted_loss: 0.0971, label: 1, bag_size: 78\n",
      "batch 419, loss: 0.0944, instance_loss: 0.0511, weighted_loss: 0.0814, label: 0, bag_size: 51\n",
      "batch 439, loss: 0.0289, instance_loss: 0.1695, weighted_loss: 0.0711, label: 0, bag_size: 103\n",
      "batch 459, loss: 0.0019, instance_loss: 0.0076, weighted_loss: 0.0036, label: 0, bag_size: 96\n",
      "batch 479, loss: 0.0937, instance_loss: 0.2496, weighted_loss: 0.1405, label: 1, bag_size: 75\n",
      "batch 499, loss: 0.0250, instance_loss: 0.1705, weighted_loss: 0.0686, label: 0, bag_size: 63\n",
      "batch 519, loss: 0.0036, instance_loss: 0.0827, weighted_loss: 0.0274, label: 0, bag_size: 108\n",
      "batch 539, loss: 0.0000, instance_loss: 0.7625, weighted_loss: 0.2288, label: 1, bag_size: 68\n",
      "batch 559, loss: 0.1276, instance_loss: 0.1523, weighted_loss: 0.1350, label: 1, bag_size: 60\n",
      "batch 579, loss: 0.1790, instance_loss: 0.0289, weighted_loss: 0.1340, label: 0, bag_size: 26\n",
      "batch 599, loss: 1.8785, instance_loss: 0.8678, weighted_loss: 1.5753, label: 0, bag_size: 44\n",
      "batch 619, loss: 0.0030, instance_loss: 0.0269, weighted_loss: 0.0102, label: 1, bag_size: 83\n",
      "batch 639, loss: 0.3316, instance_loss: 2.0686, weighted_loss: 0.8527, label: 0, bag_size: 72\n",
      "batch 659, loss: 0.4122, instance_loss: 0.2140, weighted_loss: 0.3527, label: 0, bag_size: 66\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9298621460506706: correct 9983/10736\n",
      "class 1 clustering acc 0.6609538002980626: correct 3548/5368\n",
      "Epoch: 5, train_loss: 0.4658, train_clustering_loss:  0.5948, train_error: 0.1788\n",
      "class 0: acc 0.8042813455657493, correct 263/327\n",
      "class 1: acc 0.8372093023255814, correct 288/344\n",
      "\n",
      "Val Set, val_loss: 0.4767, val_error: 0.2619, auc: 0.9387\n",
      "class 0 clustering acc 0.9732142857142857: correct 1308/1344\n",
      "class 1 clustering acc 0.6041666666666666: correct 406/672\n",
      "class 0: acc 1.0, correct 43/43\n",
      "class 1: acc 0.4634146341463415, correct 19/41\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2303, instance_loss: 0.2812, weighted_loss: 0.2456, label: 1, bag_size: 65\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0552, weighted_loss: 0.0166, label: 0, bag_size: 74\n",
      "batch 59, loss: 0.0013, instance_loss: 0.1691, weighted_loss: 0.0516, label: 1, bag_size: 100\n",
      "batch 79, loss: 0.0588, instance_loss: 0.8692, weighted_loss: 0.3020, label: 0, bag_size: 132\n",
      "batch 99, loss: 0.0002, instance_loss: 0.1498, weighted_loss: 0.0451, label: 1, bag_size: 81\n",
      "batch 119, loss: 0.0003, instance_loss: 0.0558, weighted_loss: 0.0170, label: 0, bag_size: 91\n",
      "batch 139, loss: 0.1496, instance_loss: 0.8048, weighted_loss: 0.3461, label: 0, bag_size: 65\n",
      "batch 159, loss: 0.4716, instance_loss: 2.8787, weighted_loss: 1.1937, label: 0, bag_size: 35\n",
      "batch 179, loss: 0.0000, instance_loss: 0.3740, weighted_loss: 0.1122, label: 0, bag_size: 37\n",
      "batch 199, loss: 0.0276, instance_loss: 0.9330, weighted_loss: 0.2992, label: 1, bag_size: 69\n",
      "batch 219, loss: 0.0308, instance_loss: 1.0948, weighted_loss: 0.3500, label: 1, bag_size: 16\n",
      "batch 239, loss: 0.0062, instance_loss: 0.8689, weighted_loss: 0.2650, label: 0, bag_size: 90\n",
      "batch 259, loss: 0.0135, instance_loss: 0.5842, weighted_loss: 0.1847, label: 0, bag_size: 66\n",
      "batch 279, loss: 0.0529, instance_loss: 1.5545, weighted_loss: 0.5034, label: 0, bag_size: 73\n",
      "batch 299, loss: 0.0001, instance_loss: 0.6095, weighted_loss: 0.1829, label: 1, bag_size: 111\n",
      "batch 319, loss: 0.5152, instance_loss: 1.1866, weighted_loss: 0.7166, label: 1, bag_size: 61\n",
      "batch 339, loss: 0.0044, instance_loss: 0.4778, weighted_loss: 0.1464, label: 0, bag_size: 21\n",
      "batch 359, loss: 0.0004, instance_loss: 0.4625, weighted_loss: 0.1390, label: 0, bag_size: 79\n",
      "batch 379, loss: 0.0541, instance_loss: 0.3194, weighted_loss: 0.1337, label: 1, bag_size: 62\n",
      "batch 399, loss: 0.0000, instance_loss: 0.6125, weighted_loss: 0.1838, label: 1, bag_size: 32\n",
      "batch 419, loss: 0.1020, instance_loss: 0.7008, weighted_loss: 0.2817, label: 1, bag_size: 96\n",
      "batch 439, loss: 0.4139, instance_loss: 1.0502, weighted_loss: 0.6048, label: 1, bag_size: 42\n",
      "batch 459, loss: 0.6516, instance_loss: 1.0316, weighted_loss: 0.7656, label: 1, bag_size: 41\n",
      "batch 479, loss: 0.0004, instance_loss: 0.1318, weighted_loss: 0.0398, label: 0, bag_size: 29\n",
      "batch 499, loss: 0.0002, instance_loss: 0.6306, weighted_loss: 0.1893, label: 1, bag_size: 69\n",
      "batch 519, loss: 0.0000, instance_loss: 0.2299, weighted_loss: 0.0690, label: 1, bag_size: 99\n",
      "batch 539, loss: 0.1428, instance_loss: 0.9012, weighted_loss: 0.3703, label: 0, bag_size: 53\n",
      "batch 559, loss: 0.0209, instance_loss: 0.6623, weighted_loss: 0.2133, label: 1, bag_size: 45\n",
      "batch 579, loss: 0.9654, instance_loss: 0.3357, weighted_loss: 0.7765, label: 0, bag_size: 28\n",
      "batch 599, loss: 0.0001, instance_loss: 0.6184, weighted_loss: 0.1856, label: 1, bag_size: 107\n",
      "batch 619, loss: 2.0578, instance_loss: 1.8577, weighted_loss: 1.9978, label: 0, bag_size: 45\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0864, weighted_loss: 0.0259, label: 0, bag_size: 78\n",
      "batch 659, loss: 0.0048, instance_loss: 1.0221, weighted_loss: 0.3100, label: 0, bag_size: 83\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9202682563338301: correct 9880/10736\n",
      "class 1 clustering acc 0.4606929955290611: correct 2473/5368\n",
      "Epoch: 6, train_loss: 0.5429, train_clustering_loss:  0.7647, train_error: 0.1937\n",
      "class 0: acc 0.7963525835866262, correct 262/329\n",
      "class 1: acc 0.8157894736842105, correct 279/342\n",
      "\n",
      "Val Set, val_loss: 3.1668, val_error: 0.5119, auc: 0.9186\n",
      "class 0 clustering acc 0.8563988095238095: correct 1151/1344\n",
      "class 1 clustering acc 0.40773809523809523: correct 274/672\n",
      "class 0: acc 0.0, correct 0/43\n",
      "class 1: acc 1.0, correct 41/41\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 2.5299, instance_loss: 1.0895, weighted_loss: 2.0978, label: 0, bag_size: 43\n",
      "batch 39, loss: 0.0038, instance_loss: 0.6532, weighted_loss: 0.1986, label: 1, bag_size: 53\n",
      "batch 59, loss: 0.8182, instance_loss: 0.6679, weighted_loss: 0.7731, label: 0, bag_size: 46\n",
      "batch 79, loss: 0.0017, instance_loss: 0.5173, weighted_loss: 0.1564, label: 1, bag_size: 29\n",
      "batch 99, loss: 0.0470, instance_loss: 0.5059, weighted_loss: 0.1847, label: 1, bag_size: 56\n",
      "batch 119, loss: 0.0001, instance_loss: 0.4573, weighted_loss: 0.1373, label: 1, bag_size: 39\n",
      "batch 139, loss: 0.0155, instance_loss: 0.2888, weighted_loss: 0.0975, label: 0, bag_size: 51\n",
      "batch 159, loss: 0.0083, instance_loss: 0.1107, weighted_loss: 0.0390, label: 0, bag_size: 67\n",
      "batch 179, loss: 0.0006, instance_loss: 0.0536, weighted_loss: 0.0165, label: 0, bag_size: 71\n",
      "batch 199, loss: 0.4514, instance_loss: 0.2084, weighted_loss: 0.3785, label: 1, bag_size: 45\n",
      "batch 219, loss: 0.0016, instance_loss: 0.0479, weighted_loss: 0.0155, label: 1, bag_size: 21\n",
      "batch 239, loss: 0.1347, instance_loss: 0.2954, weighted_loss: 0.1829, label: 1, bag_size: 41\n",
      "batch 259, loss: 0.0957, instance_loss: 0.0457, weighted_loss: 0.0807, label: 1, bag_size: 28\n",
      "batch 279, loss: 0.0464, instance_loss: 0.8375, weighted_loss: 0.2837, label: 0, bag_size: 81\n",
      "batch 299, loss: 0.2995, instance_loss: 0.6986, weighted_loss: 0.4192, label: 1, bag_size: 62\n",
      "batch 319, loss: 0.1224, instance_loss: 0.6268, weighted_loss: 0.2737, label: 1, bag_size: 19\n",
      "batch 339, loss: 0.0383, instance_loss: 0.6668, weighted_loss: 0.2269, label: 1, bag_size: 84\n",
      "batch 359, loss: 0.7888, instance_loss: 1.7745, weighted_loss: 1.0845, label: 1, bag_size: 70\n",
      "batch 379, loss: 0.0186, instance_loss: 0.6040, weighted_loss: 0.1942, label: 1, bag_size: 99\n",
      "batch 399, loss: 0.0269, instance_loss: 0.7632, weighted_loss: 0.2478, label: 1, bag_size: 53\n",
      "batch 419, loss: 0.0012, instance_loss: 0.0762, weighted_loss: 0.0237, label: 1, bag_size: 47\n",
      "batch 439, loss: 0.0013, instance_loss: 0.2690, weighted_loss: 0.0816, label: 1, bag_size: 83\n",
      "batch 459, loss: 0.0030, instance_loss: 0.0477, weighted_loss: 0.0164, label: 0, bag_size: 55\n",
      "batch 479, loss: 0.6645, instance_loss: 0.9170, weighted_loss: 0.7403, label: 1, bag_size: 52\n",
      "batch 499, loss: 6.0060, instance_loss: 4.5796, weighted_loss: 5.5781, label: 0, bag_size: 73\n",
      "batch 519, loss: 0.9018, instance_loss: 0.7856, weighted_loss: 0.8669, label: 1, bag_size: 62\n",
      "batch 539, loss: 0.6049, instance_loss: 0.4786, weighted_loss: 0.5670, label: 1, bag_size: 52\n",
      "batch 559, loss: 0.0130, instance_loss: 0.0415, weighted_loss: 0.0216, label: 0, bag_size: 61\n",
      "batch 579, loss: 0.0446, instance_loss: 0.2121, weighted_loss: 0.0949, label: 1, bag_size: 86\n",
      "batch 599, loss: 0.0032, instance_loss: 0.4616, weighted_loss: 0.1407, label: 1, bag_size: 45\n",
      "batch 619, loss: 1.0240, instance_loss: 1.0263, weighted_loss: 1.0247, label: 0, bag_size: 28\n",
      "batch 639, loss: 0.0033, instance_loss: 0.1976, weighted_loss: 0.0616, label: 0, bag_size: 90\n",
      "batch 659, loss: 4.6800, instance_loss: 0.6291, weighted_loss: 3.4648, label: 0, bag_size: 48\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9323770491803278: correct 10010/10736\n",
      "class 1 clustering acc 0.6333830104321908: correct 3400/5368\n",
      "Epoch: 7, train_loss: 0.4119, train_clustering_loss:  0.5888, train_error: 0.1505\n",
      "class 0: acc 0.8388059701492537, correct 281/335\n",
      "class 1: acc 0.8601190476190477, correct 289/336\n",
      "\n",
      "Val Set, val_loss: 0.3744, val_error: 0.1190, auc: 0.9472\n",
      "class 0 clustering acc 0.9635416666666666: correct 1295/1344\n",
      "class 1 clustering acc 0.49107142857142855: correct 330/672\n",
      "class 0: acc 0.9069767441860465, correct 39/43\n",
      "class 1: acc 0.8536585365853658, correct 35/41\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0126, instance_loss: 0.2621, weighted_loss: 0.0875, label: 0, bag_size: 55\n",
      "batch 39, loss: 0.0427, instance_loss: 0.6613, weighted_loss: 0.2282, label: 1, bag_size: 45\n",
      "batch 59, loss: 0.3063, instance_loss: 0.7613, weighted_loss: 0.4428, label: 1, bag_size: 63\n",
      "batch 79, loss: 2.6051, instance_loss: 3.5960, weighted_loss: 2.9024, label: 0, bag_size: 79\n",
      "batch 99, loss: 0.8658, instance_loss: 0.7119, weighted_loss: 0.8197, label: 0, bag_size: 26\n",
      "batch 119, loss: 0.3484, instance_loss: 0.3462, weighted_loss: 0.3477, label: 1, bag_size: 56\n",
      "batch 139, loss: 0.0000, instance_loss: 0.5200, weighted_loss: 0.1560, label: 0, bag_size: 114\n",
      "batch 159, loss: 0.0447, instance_loss: 0.4363, weighted_loss: 0.1622, label: 0, bag_size: 58\n",
      "batch 179, loss: 0.0015, instance_loss: 0.0252, weighted_loss: 0.0086, label: 1, bag_size: 81\n",
      "batch 199, loss: 0.0007, instance_loss: 0.5374, weighted_loss: 0.1617, label: 0, bag_size: 33\n",
      "batch 219, loss: 0.1995, instance_loss: 0.7151, weighted_loss: 0.3542, label: 0, bag_size: 62\n",
      "batch 239, loss: 0.1110, instance_loss: 0.5695, weighted_loss: 0.2486, label: 0, bag_size: 85\n",
      "batch 259, loss: 0.0000, instance_loss: 0.4442, weighted_loss: 0.1333, label: 0, bag_size: 49\n",
      "batch 279, loss: 1.6669, instance_loss: 0.3393, weighted_loss: 1.2686, label: 0, bag_size: 35\n",
      "batch 299, loss: 0.0000, instance_loss: 0.1007, weighted_loss: 0.0302, label: 0, bag_size: 97\n",
      "batch 319, loss: 0.0069, instance_loss: 0.0668, weighted_loss: 0.0249, label: 0, bag_size: 77\n",
      "batch 339, loss: 0.0110, instance_loss: 0.7628, weighted_loss: 0.2365, label: 1, bag_size: 120\n",
      "batch 359, loss: 0.0011, instance_loss: 0.4890, weighted_loss: 0.1475, label: 0, bag_size: 110\n",
      "batch 379, loss: 0.0023, instance_loss: 0.2804, weighted_loss: 0.0857, label: 0, bag_size: 89\n",
      "batch 399, loss: 1.8208, instance_loss: 0.3792, weighted_loss: 1.3883, label: 1, bag_size: 111\n",
      "batch 419, loss: 0.0008, instance_loss: 0.3947, weighted_loss: 0.1190, label: 1, bag_size: 84\n",
      "batch 439, loss: 0.0033, instance_loss: 0.5009, weighted_loss: 0.1526, label: 1, bag_size: 70\n",
      "batch 459, loss: 0.1846, instance_loss: 0.4717, weighted_loss: 0.2707, label: 0, bag_size: 25\n",
      "batch 479, loss: 0.0384, instance_loss: 0.6053, weighted_loss: 0.2085, label: 1, bag_size: 16\n",
      "batch 499, loss: 0.0234, instance_loss: 0.4733, weighted_loss: 0.1584, label: 0, bag_size: 46\n",
      "batch 519, loss: 6.5964, instance_loss: 3.4907, weighted_loss: 5.6647, label: 1, bag_size: 38\n",
      "batch 539, loss: 0.0023, instance_loss: 0.4793, weighted_loss: 0.1454, label: 1, bag_size: 103\n",
      "batch 559, loss: 3.0247, instance_loss: 1.9934, weighted_loss: 2.7153, label: 1, bag_size: 97\n",
      "batch 579, loss: 3.6905, instance_loss: 1.1511, weighted_loss: 2.9287, label: 1, bag_size: 13\n",
      "batch 599, loss: 0.3950, instance_loss: 0.7727, weighted_loss: 0.5083, label: 1, bag_size: 72\n",
      "batch 619, loss: 0.1708, instance_loss: 0.7172, weighted_loss: 0.3347, label: 0, bag_size: 51\n",
      "batch 639, loss: 0.4976, instance_loss: 1.0138, weighted_loss: 0.6524, label: 0, bag_size: 28\n",
      "batch 659, loss: 0.0000, instance_loss: 0.7796, weighted_loss: 0.2339, label: 1, bag_size: 32\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9241803278688525: correct 9922/10736\n",
      "class 1 clustering acc 0.5407973174366617: correct 2903/5368\n",
      "Epoch: 8, train_loss: 0.5269, train_clustering_loss:  0.7026, train_error: 0.1773\n",
      "class 0: acc 0.8221574344023324, correct 282/343\n",
      "class 1: acc 0.823170731707317, correct 270/328\n",
      "\n",
      "Val Set, val_loss: 0.6752, val_error: 0.2738, auc: 0.9495\n",
      "class 0 clustering acc 0.9761904761904762: correct 1312/1344\n",
      "class 1 clustering acc 0.3854166666666667: correct 259/672\n",
      "class 0: acc 0.9767441860465116, correct 42/43\n",
      "class 1: acc 0.4634146341463415, correct 19/41\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2769, instance_loss: 0.8000, weighted_loss: 0.4338, label: 0, bag_size: 65\n",
      "batch 39, loss: 0.0005, instance_loss: 0.1195, weighted_loss: 0.0362, label: 0, bag_size: 67\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0476, weighted_loss: 0.0143, label: 0, bag_size: 25\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0442, weighted_loss: 0.0133, label: 0, bag_size: 27\n",
      "batch 99, loss: 0.0053, instance_loss: 0.1055, weighted_loss: 0.0353, label: 1, bag_size: 38\n",
      "batch 119, loss: 0.0142, instance_loss: 0.0815, weighted_loss: 0.0344, label: 1, bag_size: 31\n",
      "batch 139, loss: 0.0002, instance_loss: 0.1034, weighted_loss: 0.0312, label: 1, bag_size: 85\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0864, weighted_loss: 0.0259, label: 1, bag_size: 62\n",
      "batch 179, loss: 1.9025, instance_loss: 4.0813, weighted_loss: 2.5562, label: 1, bag_size: 126\n",
      "batch 199, loss: 0.6525, instance_loss: 1.0820, weighted_loss: 0.7814, label: 1, bag_size: 86\n",
      "batch 219, loss: 0.0004, instance_loss: 0.0469, weighted_loss: 0.0144, label: 0, bag_size: 118\n",
      "batch 239, loss: 0.0074, instance_loss: 0.0978, weighted_loss: 0.0345, label: 1, bag_size: 119\n",
      "batch 259, loss: 0.0431, instance_loss: 0.5071, weighted_loss: 0.1823, label: 1, bag_size: 53\n",
      "batch 279, loss: 0.0416, instance_loss: 0.9887, weighted_loss: 0.3257, label: 1, bag_size: 95\n",
      "batch 299, loss: 0.0062, instance_loss: 0.0161, weighted_loss: 0.0092, label: 0, bag_size: 26\n",
      "batch 319, loss: 0.0036, instance_loss: 0.7176, weighted_loss: 0.2178, label: 1, bag_size: 92\n",
      "batch 339, loss: 0.2960, instance_loss: 0.8939, weighted_loss: 0.4754, label: 0, bag_size: 49\n",
      "batch 359, loss: 0.4766, instance_loss: 1.7133, weighted_loss: 0.8476, label: 0, bag_size: 77\n",
      "batch 379, loss: 0.8356, instance_loss: 0.6471, weighted_loss: 0.7790, label: 0, bag_size: 65\n",
      "batch 399, loss: 0.0047, instance_loss: 0.7409, weighted_loss: 0.2255, label: 0, bag_size: 95\n",
      "batch 419, loss: 0.0071, instance_loss: 0.8040, weighted_loss: 0.2462, label: 0, bag_size: 49\n",
      "batch 439, loss: 0.0084, instance_loss: 0.7419, weighted_loss: 0.2284, label: 0, bag_size: 55\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0668, weighted_loss: 0.0200, label: 1, bag_size: 35\n",
      "batch 479, loss: 0.4573, instance_loss: 0.7346, weighted_loss: 0.5405, label: 0, bag_size: 81\n",
      "batch 499, loss: 0.0213, instance_loss: 0.0904, weighted_loss: 0.0420, label: 1, bag_size: 41\n",
      "batch 519, loss: 0.0008, instance_loss: 0.7314, weighted_loss: 0.2200, label: 1, bag_size: 53\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0151, weighted_loss: 0.0046, label: 1, bag_size: 44\n",
      "batch 559, loss: 0.0291, instance_loss: 0.2704, weighted_loss: 0.1015, label: 0, bag_size: 110\n",
      "batch 579, loss: 0.0003, instance_loss: 0.2181, weighted_loss: 0.0656, label: 0, bag_size: 46\n",
      "batch 599, loss: 0.2465, instance_loss: 3.3253, weighted_loss: 1.1702, label: 0, bag_size: 43\n",
      "batch 619, loss: 0.1780, instance_loss: 1.0473, weighted_loss: 0.4388, label: 1, bag_size: 48\n",
      "batch 639, loss: 0.0091, instance_loss: 1.2456, weighted_loss: 0.3801, label: 1, bag_size: 53\n",
      "batch 659, loss: 0.0401, instance_loss: 0.6260, weighted_loss: 0.2159, label: 1, bag_size: 72\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9429023845007451: correct 10123/10736\n",
      "class 1 clustering acc 0.6492175856929955: correct 3485/5368\n",
      "Epoch: 9, train_loss: 0.4556, train_clustering_loss:  0.5711, train_error: 0.1550\n",
      "class 0: acc 0.844311377245509, correct 282/334\n",
      "class 1: acc 0.8456973293768546, correct 285/337\n",
      "\n",
      "Val Set, val_loss: 0.8020, val_error: 0.2143, auc: 0.9302\n",
      "class 0 clustering acc 0.8787202380952381: correct 1181/1344\n",
      "class 1 clustering acc 0.5133928571428571: correct 345/672\n",
      "class 0: acc 0.6511627906976745, correct 28/43\n",
      "class 1: acc 0.926829268292683, correct 38/41\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.0790, instance_loss: 0.9891, weighted_loss: 1.0520, label: 0, bag_size: 36\n",
      "batch 39, loss: 0.0093, instance_loss: 0.2920, weighted_loss: 0.0941, label: 1, bag_size: 41\n",
      "batch 59, loss: 0.0038, instance_loss: 0.1285, weighted_loss: 0.0412, label: 1, bag_size: 84\n",
      "batch 79, loss: 0.4133, instance_loss: 0.2223, weighted_loss: 0.3560, label: 0, bag_size: 87\n",
      "batch 99, loss: 0.4985, instance_loss: 1.2566, weighted_loss: 0.7260, label: 1, bag_size: 64\n",
      "batch 119, loss: 0.0467, instance_loss: 0.1129, weighted_loss: 0.0666, label: 0, bag_size: 66\n",
      "batch 139, loss: 1.5658, instance_loss: 0.6592, weighted_loss: 1.2938, label: 1, bag_size: 116\n",
      "batch 159, loss: 0.6620, instance_loss: 0.3982, weighted_loss: 0.5828, label: 0, bag_size: 92\n",
      "batch 179, loss: 0.4652, instance_loss: 0.0935, weighted_loss: 0.3537, label: 1, bag_size: 48\n",
      "batch 199, loss: 0.0352, instance_loss: 0.0823, weighted_loss: 0.0493, label: 1, bag_size: 88\n",
      "batch 219, loss: 0.0067, instance_loss: 0.0261, weighted_loss: 0.0125, label: 1, bag_size: 74\n",
      "batch 239, loss: 0.1294, instance_loss: 1.2292, weighted_loss: 0.4594, label: 0, bag_size: 78\n",
      "batch 259, loss: 0.0008, instance_loss: 0.0323, weighted_loss: 0.0102, label: 0, bag_size: 106\n",
      "batch 279, loss: 0.0033, instance_loss: 0.0127, weighted_loss: 0.0061, label: 0, bag_size: 91\n",
      "batch 299, loss: 0.1149, instance_loss: 0.0440, weighted_loss: 0.0936, label: 1, bag_size: 31\n",
      "batch 319, loss: 0.0007, instance_loss: 0.0135, weighted_loss: 0.0045, label: 1, bag_size: 47\n",
      "batch 339, loss: 0.0002, instance_loss: 0.0203, weighted_loss: 0.0062, label: 0, bag_size: 50\n",
      "batch 359, loss: 0.0002, instance_loss: 0.1282, weighted_loss: 0.0386, label: 0, bag_size: 83\n",
      "batch 379, loss: 1.4889, instance_loss: 1.9680, weighted_loss: 1.6326, label: 0, bag_size: 73\n",
      "batch 399, loss: 0.0383, instance_loss: 0.0962, weighted_loss: 0.0557, label: 1, bag_size: 30\n",
      "batch 419, loss: 0.3981, instance_loss: 0.2514, weighted_loss: 0.3541, label: 0, bag_size: 43\n",
      "batch 439, loss: 0.0742, instance_loss: 0.2064, weighted_loss: 0.1139, label: 0, bag_size: 83\n",
      "batch 459, loss: 0.0095, instance_loss: 0.0381, weighted_loss: 0.0181, label: 0, bag_size: 61\n",
      "batch 479, loss: 0.5301, instance_loss: 1.1837, weighted_loss: 0.7262, label: 1, bag_size: 64\n",
      "batch 499, loss: 0.8077, instance_loss: 0.1264, weighted_loss: 0.6033, label: 1, bag_size: 48\n",
      "batch 519, loss: 2.7933, instance_loss: 0.9201, weighted_loss: 2.2313, label: 1, bag_size: 42\n",
      "batch 539, loss: 0.0002, instance_loss: 0.0064, weighted_loss: 0.0020, label: 0, bag_size: 39\n",
      "batch 559, loss: 0.0122, instance_loss: 0.0132, weighted_loss: 0.0125, label: 0, bag_size: 40\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0837, weighted_loss: 0.0251, label: 0, bag_size: 27\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 31\n",
      "batch 619, loss: 2.0206, instance_loss: 0.6713, weighted_loss: 1.6158, label: 0, bag_size: 67\n",
      "batch 639, loss: 0.0005, instance_loss: 0.0163, weighted_loss: 0.0053, label: 1, bag_size: 69\n",
      "batch 659, loss: 0.0035, instance_loss: 0.2794, weighted_loss: 0.0863, label: 0, bag_size: 31\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9565946348733234: correct 10270/10736\n",
      "class 1 clustering acc 0.7947093889716841: correct 4266/5368\n",
      "Epoch: 10, train_loss: 0.3787, train_clustering_loss:  0.3901, train_error: 0.1282\n",
      "class 0: acc 0.8691860465116279, correct 299/344\n",
      "class 1: acc 0.8746177370030581, correct 286/327\n",
      "\n",
      "Val Set, val_loss: 0.3802, val_error: 0.1190, auc: 0.9229\n",
      "class 0 clustering acc 0.96875: correct 1302/1344\n",
      "class 1 clustering acc 0.7083333333333334: correct 476/672\n",
      "class 0: acc 0.813953488372093, correct 35/43\n",
      "class 1: acc 0.9512195121951219, correct 39/41\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0015, instance_loss: 0.0162, weighted_loss: 0.0060, label: 0, bag_size: 39\n",
      "batch 39, loss: 0.3309, instance_loss: 0.9984, weighted_loss: 0.5311, label: 0, bag_size: 81\n",
      "batch 59, loss: 0.0631, instance_loss: 0.0906, weighted_loss: 0.0713, label: 1, bag_size: 75\n",
      "batch 79, loss: 0.0129, instance_loss: 0.0788, weighted_loss: 0.0327, label: 0, bag_size: 112\n",
      "batch 99, loss: 0.0559, instance_loss: 0.2836, weighted_loss: 0.1243, label: 1, bag_size: 17\n",
      "batch 119, loss: 0.0012, instance_loss: 0.0031, weighted_loss: 0.0017, label: 0, bag_size: 86\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0529, weighted_loss: 0.0160, label: 0, bag_size: 64\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0327, weighted_loss: 0.0099, label: 1, bag_size: 31\n",
      "batch 179, loss: 1.4171, instance_loss: 2.2882, weighted_loss: 1.6785, label: 1, bag_size: 73\n",
      "batch 199, loss: 0.0278, instance_loss: 0.7380, weighted_loss: 0.2409, label: 0, bag_size: 86\n",
      "batch 219, loss: 0.0000, instance_loss: 0.5167, weighted_loss: 0.1550, label: 1, bag_size: 90\n",
      "batch 239, loss: 0.0070, instance_loss: 0.1040, weighted_loss: 0.0361, label: 0, bag_size: 112\n",
      "batch 259, loss: 1.7701, instance_loss: 1.7446, weighted_loss: 1.7624, label: 1, bag_size: 32\n",
      "batch 279, loss: 0.0809, instance_loss: 0.6432, weighted_loss: 0.2496, label: 0, bag_size: 122\n",
      "batch 299, loss: 0.0012, instance_loss: 0.0894, weighted_loss: 0.0277, label: 0, bag_size: 62\n",
      "batch 319, loss: 0.0385, instance_loss: 0.2784, weighted_loss: 0.1104, label: 1, bag_size: 52\n",
      "batch 339, loss: 0.0018, instance_loss: 0.2082, weighted_loss: 0.0637, label: 1, bag_size: 89\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0330, weighted_loss: 0.0099, label: 1, bag_size: 37\n",
      "batch 379, loss: 0.0640, instance_loss: 0.4416, weighted_loss: 0.1773, label: 1, bag_size: 63\n",
      "batch 399, loss: 0.0010, instance_loss: 0.4060, weighted_loss: 0.1225, label: 1, bag_size: 31\n",
      "batch 419, loss: 1.8776, instance_loss: 0.3332, weighted_loss: 1.4143, label: 1, bag_size: 29\n",
      "batch 439, loss: 0.0004, instance_loss: 0.0919, weighted_loss: 0.0278, label: 0, bag_size: 46\n",
      "batch 459, loss: 0.0065, instance_loss: 0.0140, weighted_loss: 0.0088, label: 1, bag_size: 89\n",
      "batch 479, loss: 0.0009, instance_loss: 0.1763, weighted_loss: 0.0535, label: 0, bag_size: 63\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0184, weighted_loss: 0.0056, label: 0, bag_size: 81\n",
      "batch 519, loss: 0.0120, instance_loss: 0.0787, weighted_loss: 0.0320, label: 0, bag_size: 108\n",
      "batch 539, loss: 0.0003, instance_loss: 0.4551, weighted_loss: 0.1367, label: 1, bag_size: 32\n",
      "batch 559, loss: 0.0142, instance_loss: 0.0156, weighted_loss: 0.0146, label: 0, bag_size: 18\n",
      "batch 579, loss: 0.9047, instance_loss: 0.8712, weighted_loss: 0.8947, label: 0, bag_size: 51\n",
      "batch 599, loss: 0.0058, instance_loss: 0.1791, weighted_loss: 0.0578, label: 1, bag_size: 127\n",
      "batch 619, loss: 0.0980, instance_loss: 0.1124, weighted_loss: 0.1023, label: 1, bag_size: 22\n",
      "batch 639, loss: 0.7182, instance_loss: 1.3835, weighted_loss: 0.9178, label: 0, bag_size: 73\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0184, weighted_loss: 0.0055, label: 1, bag_size: 41\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9474664679582713: correct 10172/10736\n",
      "class 1 clustering acc 0.7155365126676602: correct 3841/5368\n",
      "Epoch: 11, train_loss: 0.4965, train_clustering_loss:  0.4915, train_error: 0.1744\n",
      "class 0: acc 0.822289156626506, correct 273/332\n",
      "class 1: acc 0.8289085545722714, correct 281/339\n",
      "\n",
      "Val Set, val_loss: 0.6744, val_error: 0.1667, auc: 0.9138\n",
      "class 0 clustering acc 0.9352678571428571: correct 1257/1344\n",
      "class 1 clustering acc 0.7321428571428571: correct 492/672\n",
      "class 0: acc 0.7209302325581395, correct 31/43\n",
      "class 1: acc 0.9512195121951219, correct 39/41\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0039, instance_loss: 0.0037, weighted_loss: 0.0038, label: 1, bag_size: 36\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0110, weighted_loss: 0.0033, label: 0, bag_size: 51\n",
      "batch 59, loss: 0.0003, instance_loss: 0.0016, weighted_loss: 0.0007, label: 1, bag_size: 65\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 42\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 79\n",
      "batch 119, loss: 2.2837, instance_loss: 0.0960, weighted_loss: 1.6274, label: 0, bag_size: 48\n",
      "batch 139, loss: 0.0568, instance_loss: 0.3911, weighted_loss: 0.1571, label: 0, bag_size: 95\n",
      "batch 159, loss: 0.0016, instance_loss: 0.0036, weighted_loss: 0.0022, label: 1, bag_size: 69\n",
      "batch 179, loss: 0.0014, instance_loss: 0.4804, weighted_loss: 0.1451, label: 0, bag_size: 29\n",
      "batch 199, loss: 0.0007, instance_loss: 0.0014, weighted_loss: 0.0009, label: 1, bag_size: 71\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0546, weighted_loss: 0.0164, label: 0, bag_size: 87\n",
      "batch 239, loss: 0.0215, instance_loss: 0.1404, weighted_loss: 0.0571, label: 1, bag_size: 39\n",
      "batch 259, loss: 0.0771, instance_loss: 1.5855, weighted_loss: 0.5296, label: 1, bag_size: 51\n",
      "batch 279, loss: 1.5956, instance_loss: 1.2413, weighted_loss: 1.4893, label: 1, bag_size: 61\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0250, weighted_loss: 0.0075, label: 0, bag_size: 65\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0021, weighted_loss: 0.0007, label: 0, bag_size: 80\n",
      "batch 339, loss: 0.1060, instance_loss: 0.0245, weighted_loss: 0.0815, label: 1, bag_size: 44\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0908, weighted_loss: 0.0273, label: 0, bag_size: 65\n",
      "batch 379, loss: 0.0001, instance_loss: 0.0050, weighted_loss: 0.0016, label: 0, bag_size: 85\n",
      "batch 399, loss: 0.3689, instance_loss: 0.1903, weighted_loss: 0.3153, label: 0, bag_size: 88\n",
      "batch 419, loss: 0.0015, instance_loss: 0.0039, weighted_loss: 0.0022, label: 1, bag_size: 68\n",
      "batch 439, loss: 4.1627, instance_loss: 3.5210, weighted_loss: 3.9702, label: 0, bag_size: 79\n",
      "batch 459, loss: 0.0114, instance_loss: 0.0635, weighted_loss: 0.0270, label: 0, bag_size: 24\n",
      "batch 479, loss: 0.0076, instance_loss: 0.0069, weighted_loss: 0.0074, label: 0, bag_size: 116\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0054, weighted_loss: 0.0017, label: 0, bag_size: 61\n",
      "batch 519, loss: 0.4131, instance_loss: 0.2193, weighted_loss: 0.3550, label: 0, bag_size: 77\n",
      "batch 539, loss: 0.2101, instance_loss: 0.2972, weighted_loss: 0.2362, label: 0, bag_size: 31\n",
      "batch 559, loss: 0.0016, instance_loss: 0.0058, weighted_loss: 0.0029, label: 0, bag_size: 79\n",
      "batch 579, loss: 0.0076, instance_loss: 0.0563, weighted_loss: 0.0222, label: 1, bag_size: 23\n",
      "batch 599, loss: 0.0160, instance_loss: 0.0133, weighted_loss: 0.0152, label: 0, bag_size: 67\n",
      "batch 619, loss: 1.2347, instance_loss: 3.0746, weighted_loss: 1.7866, label: 1, bag_size: 24\n",
      "batch 639, loss: 0.2022, instance_loss: 0.4043, weighted_loss: 0.2628, label: 0, bag_size: 87\n",
      "batch 659, loss: 0.0014, instance_loss: 0.0037, weighted_loss: 0.0021, label: 0, bag_size: 38\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.967306259314456: correct 10385/10736\n",
      "class 1 clustering acc 0.8500372578241431: correct 4563/5368\n",
      "Epoch: 12, train_loss: 0.2580, train_clustering_loss:  0.2925, train_error: 0.1028\n",
      "class 0: acc 0.9020172910662824, correct 313/347\n",
      "class 1: acc 0.8919753086419753, correct 289/324\n",
      "\n",
      "Val Set, val_loss: 0.4391, val_error: 0.1429, auc: 0.9246\n",
      "class 0 clustering acc 0.9650297619047619: correct 1297/1344\n",
      "class 1 clustering acc 0.8258928571428571: correct 555/672\n",
      "class 0: acc 0.8604651162790697, correct 37/43\n",
      "class 1: acc 0.8536585365853658, correct 35/41\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0129, instance_loss: 0.0244, weighted_loss: 0.0163, label: 1, bag_size: 78\n",
      "batch 39, loss: 0.0042, instance_loss: 0.1675, weighted_loss: 0.0532, label: 0, bag_size: 66\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0109, weighted_loss: 0.0033, label: 0, bag_size: 63\n",
      "batch 79, loss: 0.0003, instance_loss: 0.0244, weighted_loss: 0.0075, label: 1, bag_size: 123\n",
      "batch 99, loss: 0.1840, instance_loss: 0.2605, weighted_loss: 0.2069, label: 1, bag_size: 38\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0117, weighted_loss: 0.0035, label: 1, bag_size: 75\n",
      "batch 139, loss: 0.0002, instance_loss: 0.0906, weighted_loss: 0.0273, label: 1, bag_size: 71\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0171, weighted_loss: 0.0051, label: 0, bag_size: 77\n",
      "batch 179, loss: 0.0006, instance_loss: 0.0218, weighted_loss: 0.0070, label: 0, bag_size: 41\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0078, weighted_loss: 0.0023, label: 1, bag_size: 69\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0083, weighted_loss: 0.0025, label: 1, bag_size: 94\n",
      "batch 239, loss: 0.0515, instance_loss: 0.8136, weighted_loss: 0.2801, label: 0, bag_size: 51\n",
      "batch 259, loss: 0.0043, instance_loss: 0.0763, weighted_loss: 0.0259, label: 1, bag_size: 68\n",
      "batch 279, loss: 0.0001, instance_loss: 0.1496, weighted_loss: 0.0449, label: 0, bag_size: 88\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0057, weighted_loss: 0.0017, label: 0, bag_size: 57\n",
      "batch 319, loss: 0.5521, instance_loss: 1.1489, weighted_loss: 0.7312, label: 0, bag_size: 37\n",
      "batch 339, loss: 0.0801, instance_loss: 0.2164, weighted_loss: 0.1210, label: 1, bag_size: 53\n",
      "batch 359, loss: 0.0007, instance_loss: 0.0256, weighted_loss: 0.0081, label: 0, bag_size: 38\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0062, weighted_loss: 0.0018, label: 1, bag_size: 31\n",
      "batch 399, loss: 0.0022, instance_loss: 0.2560, weighted_loss: 0.0784, label: 1, bag_size: 100\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0022, weighted_loss: 0.0007, label: 1, bag_size: 85\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 1, bag_size: 78\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0005, label: 1, bag_size: 83\n",
      "batch 479, loss: 0.0012, instance_loss: 0.1722, weighted_loss: 0.0525, label: 0, bag_size: 114\n",
      "batch 499, loss: 0.0030, instance_loss: 0.1899, weighted_loss: 0.0591, label: 1, bag_size: 39\n",
      "batch 519, loss: 0.0000, instance_loss: 0.1460, weighted_loss: 0.0438, label: 0, bag_size: 37\n",
      "batch 539, loss: 0.0050, instance_loss: 0.7203, weighted_loss: 0.2196, label: 0, bag_size: 36\n",
      "batch 559, loss: 0.0042, instance_loss: 0.4665, weighted_loss: 0.1429, label: 1, bag_size: 85\n",
      "batch 579, loss: 0.0003, instance_loss: 0.0717, weighted_loss: 0.0217, label: 0, bag_size: 92\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0196, weighted_loss: 0.0059, label: 0, bag_size: 24\n",
      "batch 619, loss: 0.0271, instance_loss: 0.0402, weighted_loss: 0.0311, label: 0, bag_size: 50\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0146, weighted_loss: 0.0044, label: 0, bag_size: 73\n",
      "batch 659, loss: 0.0001, instance_loss: 1.5895, weighted_loss: 0.4769, label: 1, bag_size: 80\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9573397913561847: correct 10278/10736\n",
      "class 1 clustering acc 0.7704918032786885: correct 4136/5368\n",
      "Epoch: 13, train_loss: 0.4560, train_clustering_loss:  0.4132, train_error: 0.1401\n",
      "class 0: acc 0.8456591639871383, correct 263/311\n",
      "class 1: acc 0.8722222222222222, correct 314/360\n",
      "\n",
      "Val Set, val_loss: 0.6753, val_error: 0.1786, auc: 0.9370\n",
      "class 0 clustering acc 0.890625: correct 1197/1344\n",
      "class 1 clustering acc 0.7023809523809523: correct 472/672\n",
      "class 0: acc 0.6976744186046512, correct 30/43\n",
      "class 1: acc 0.9512195121951219, correct 39/41\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.0167, weighted_loss: 0.0051, label: 0, bag_size: 83\n",
      "batch 39, loss: 0.0776, instance_loss: 0.0091, weighted_loss: 0.0570, label: 0, bag_size: 24\n",
      "batch 59, loss: 0.0063, instance_loss: 0.0373, weighted_loss: 0.0156, label: 0, bag_size: 103\n",
      "batch 79, loss: 0.0210, instance_loss: 0.1564, weighted_loss: 0.0616, label: 0, bag_size: 93\n",
      "batch 99, loss: 0.0000, instance_loss: 0.4592, weighted_loss: 0.1378, label: 1, bag_size: 74\n",
      "batch 119, loss: 0.0198, instance_loss: 0.0383, weighted_loss: 0.0253, label: 1, bag_size: 123\n",
      "batch 139, loss: 0.0001, instance_loss: 0.6042, weighted_loss: 0.1813, label: 1, bag_size: 40\n",
      "batch 159, loss: 0.0000, instance_loss: 0.5651, weighted_loss: 0.1695, label: 1, bag_size: 123\n",
      "batch 179, loss: 0.1273, instance_loss: 0.0235, weighted_loss: 0.0961, label: 0, bag_size: 98\n",
      "batch 199, loss: 0.2882, instance_loss: 0.9495, weighted_loss: 0.4866, label: 1, bag_size: 82\n",
      "batch 219, loss: 0.0154, instance_loss: 0.6626, weighted_loss: 0.2096, label: 0, bag_size: 26\n",
      "batch 239, loss: 0.0531, instance_loss: 0.4007, weighted_loss: 0.1573, label: 0, bag_size: 51\n",
      "batch 259, loss: 0.0033, instance_loss: 0.6583, weighted_loss: 0.1998, label: 1, bag_size: 38\n",
      "batch 279, loss: 4.1528, instance_loss: 1.5477, weighted_loss: 3.3712, label: 1, bag_size: 33\n",
      "batch 299, loss: 2.1246, instance_loss: 3.0790, weighted_loss: 2.4109, label: 0, bag_size: 33\n",
      "batch 319, loss: 0.0609, instance_loss: 0.4686, weighted_loss: 0.1832, label: 1, bag_size: 31\n",
      "batch 339, loss: 1.1348, instance_loss: 1.0529, weighted_loss: 1.1102, label: 0, bag_size: 17\n",
      "batch 359, loss: 0.0038, instance_loss: 0.1074, weighted_loss: 0.0349, label: 0, bag_size: 73\n",
      "batch 379, loss: 0.0355, instance_loss: 0.5981, weighted_loss: 0.2043, label: 0, bag_size: 28\n",
      "batch 399, loss: 0.0016, instance_loss: 0.2945, weighted_loss: 0.0895, label: 0, bag_size: 65\n",
      "batch 419, loss: 0.0060, instance_loss: 0.1162, weighted_loss: 0.0390, label: 0, bag_size: 13\n",
      "batch 439, loss: 0.0731, instance_loss: 0.1117, weighted_loss: 0.0847, label: 1, bag_size: 60\n",
      "batch 459, loss: 0.1216, instance_loss: 0.2461, weighted_loss: 0.1590, label: 0, bag_size: 74\n",
      "batch 479, loss: 0.0018, instance_loss: 1.7221, weighted_loss: 0.5179, label: 1, bag_size: 60\n",
      "batch 499, loss: 1.6827, instance_loss: 2.1671, weighted_loss: 1.8280, label: 1, bag_size: 76\n",
      "batch 519, loss: 0.0716, instance_loss: 0.1182, weighted_loss: 0.0855, label: 0, bag_size: 14\n",
      "batch 539, loss: 0.0323, instance_loss: 0.7235, weighted_loss: 0.2396, label: 1, bag_size: 17\n",
      "batch 559, loss: 5.1671, instance_loss: 3.0757, weighted_loss: 4.5396, label: 0, bag_size: 48\n",
      "batch 579, loss: 0.1924, instance_loss: 1.4520, weighted_loss: 0.5703, label: 1, bag_size: 61\n",
      "batch 599, loss: 0.0356, instance_loss: 0.0722, weighted_loss: 0.0466, label: 0, bag_size: 67\n",
      "batch 619, loss: 0.0014, instance_loss: 0.0161, weighted_loss: 0.0058, label: 1, bag_size: 110\n",
      "batch 639, loss: 0.0004, instance_loss: 0.1987, weighted_loss: 0.0599, label: 1, bag_size: 82\n",
      "batch 659, loss: 0.0037, instance_loss: 0.1463, weighted_loss: 0.0465, label: 0, bag_size: 104\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9470007451564829: correct 10167/10736\n",
      "class 1 clustering acc 0.7183308494783904: correct 3856/5368\n",
      "Epoch: 14, train_loss: 0.4470, train_clustering_loss:  0.4798, train_error: 0.1297\n",
      "class 0: acc 0.8709677419354839, correct 297/341\n",
      "class 1: acc 0.8696969696969697, correct 287/330\n",
      "\n",
      "Val Set, val_loss: 0.5003, val_error: 0.1786, auc: 0.9444\n",
      "class 0 clustering acc 0.9598214285714286: correct 1290/1344\n",
      "class 1 clustering acc 0.8392857142857143: correct 564/672\n",
      "class 0: acc 0.9534883720930233, correct 41/43\n",
      "class 1: acc 0.6829268292682927, correct 28/41\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0011, instance_loss: 0.0091, weighted_loss: 0.0035, label: 0, bag_size: 63\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0062, weighted_loss: 0.0019, label: 0, bag_size: 97\n",
      "batch 59, loss: 0.2371, instance_loss: 0.0919, weighted_loss: 0.1935, label: 1, bag_size: 50\n",
      "batch 79, loss: 0.0005, instance_loss: 0.0221, weighted_loss: 0.0070, label: 0, bag_size: 79\n",
      "batch 99, loss: 0.0105, instance_loss: 0.0342, weighted_loss: 0.0176, label: 1, bag_size: 31\n",
      "batch 119, loss: 0.0013, instance_loss: 0.0073, weighted_loss: 0.0031, label: 1, bag_size: 95\n",
      "batch 139, loss: 0.0045, instance_loss: 0.0206, weighted_loss: 0.0093, label: 0, bag_size: 38\n",
      "batch 159, loss: 0.0448, instance_loss: 0.0703, weighted_loss: 0.0525, label: 1, bag_size: 52\n",
      "batch 179, loss: 1.4809, instance_loss: 2.2160, weighted_loss: 1.7014, label: 0, bag_size: 35\n",
      "batch 199, loss: 0.0057, instance_loss: 0.0519, weighted_loss: 0.0195, label: 1, bag_size: 31\n",
      "batch 219, loss: 0.0004, instance_loss: 0.0251, weighted_loss: 0.0078, label: 0, bag_size: 51\n",
      "batch 239, loss: 0.0071, instance_loss: 0.0138, weighted_loss: 0.0091, label: 1, bag_size: 83\n",
      "batch 259, loss: 1.1683, instance_loss: 1.2432, weighted_loss: 1.1907, label: 0, bag_size: 110\n",
      "batch 279, loss: 0.8340, instance_loss: 0.2988, weighted_loss: 0.6734, label: 0, bag_size: 68\n",
      "batch 299, loss: 0.0091, instance_loss: 0.0292, weighted_loss: 0.0151, label: 1, bag_size: 75\n",
      "batch 319, loss: 1.9324, instance_loss: 0.9858, weighted_loss: 1.6484, label: 1, bag_size: 79\n",
      "batch 339, loss: 0.0004, instance_loss: 0.0121, weighted_loss: 0.0039, label: 1, bag_size: 20\n",
      "batch 359, loss: 0.2282, instance_loss: 0.0440, weighted_loss: 0.1730, label: 0, bag_size: 122\n",
      "batch 379, loss: 0.0013, instance_loss: 0.0356, weighted_loss: 0.0116, label: 0, bag_size: 49\n",
      "batch 399, loss: 0.0027, instance_loss: 0.0224, weighted_loss: 0.0086, label: 1, bag_size: 32\n",
      "batch 419, loss: 0.0648, instance_loss: 0.0899, weighted_loss: 0.0723, label: 0, bag_size: 73\n",
      "batch 439, loss: 0.0005, instance_loss: 0.0083, weighted_loss: 0.0028, label: 1, bag_size: 46\n",
      "batch 459, loss: 0.0021, instance_loss: 0.0073, weighted_loss: 0.0037, label: 1, bag_size: 21\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0192, weighted_loss: 0.0059, label: 0, bag_size: 91\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 1, bag_size: 70\n",
      "batch 519, loss: 0.2538, instance_loss: 0.0673, weighted_loss: 0.1978, label: 1, bag_size: 35\n",
      "batch 539, loss: 8.3597, instance_loss: 2.8999, weighted_loss: 6.7217, label: 0, bag_size: 44\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0101, weighted_loss: 0.0030, label: 1, bag_size: 60\n",
      "batch 579, loss: 0.0361, instance_loss: 0.0277, weighted_loss: 0.0336, label: 0, bag_size: 25\n",
      "batch 599, loss: 0.0191, instance_loss: 0.0264, weighted_loss: 0.0213, label: 0, bag_size: 67\n",
      "batch 619, loss: 0.2010, instance_loss: 0.3504, weighted_loss: 0.2458, label: 1, bag_size: 19\n",
      "batch 639, loss: 0.0071, instance_loss: 0.0274, weighted_loss: 0.0132, label: 0, bag_size: 43\n",
      "batch 659, loss: 0.0451, instance_loss: 0.0365, weighted_loss: 0.0425, label: 0, bag_size: 30\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9632078986587184: correct 10341/10736\n",
      "class 1 clustering acc 0.8256333830104322: correct 4432/5368\n",
      "Epoch: 15, train_loss: 0.3507, train_clustering_loss:  0.3226, train_error: 0.1297\n",
      "class 0: acc 0.8727272727272727, correct 288/330\n",
      "class 1: acc 0.8680351906158358, correct 296/341\n",
      "\n",
      "Val Set, val_loss: 0.3708, val_error: 0.1429, auc: 0.9353\n",
      "class 0 clustering acc 0.9449404761904762: correct 1270/1344\n",
      "class 1 clustering acc 0.8348214285714286: correct 561/672\n",
      "class 0: acc 0.8372093023255814, correct 36/43\n",
      "class 1: acc 0.8780487804878049, correct 36/41\n",
      "EarlyStopping counter: 12 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.8494, instance_loss: 0.6737, weighted_loss: 0.7967, label: 1, bag_size: 50\n",
      "batch 39, loss: 0.0072, instance_loss: 0.3168, weighted_loss: 0.1001, label: 1, bag_size: 78\n",
      "batch 59, loss: 0.0415, instance_loss: 0.1251, weighted_loss: 0.0666, label: 1, bag_size: 35\n",
      "batch 79, loss: 1.1748, instance_loss: 0.4594, weighted_loss: 0.9602, label: 1, bag_size: 51\n",
      "batch 99, loss: 0.3842, instance_loss: 0.0063, weighted_loss: 0.2709, label: 1, bag_size: 30\n",
      "batch 119, loss: 0.0447, instance_loss: 0.2570, weighted_loss: 0.1084, label: 1, bag_size: 51\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0181, weighted_loss: 0.0055, label: 0, bag_size: 93\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 0, bag_size: 28\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 30\n",
      "batch 199, loss: 0.0017, instance_loss: 0.0119, weighted_loss: 0.0048, label: 1, bag_size: 54\n",
      "batch 219, loss: 0.0864, instance_loss: 0.0273, weighted_loss: 0.0686, label: 0, bag_size: 118\n",
      "batch 239, loss: 0.0630, instance_loss: 0.1003, weighted_loss: 0.0742, label: 0, bag_size: 41\n",
      "batch 259, loss: 3.0640, instance_loss: 2.1983, weighted_loss: 2.8043, label: 1, bag_size: 67\n",
      "batch 279, loss: 0.0004, instance_loss: 0.0045, weighted_loss: 0.0016, label: 1, bag_size: 107\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0026, weighted_loss: 0.0008, label: 1, bag_size: 85\n",
      "batch 319, loss: 0.0006, instance_loss: 0.0031, weighted_loss: 0.0013, label: 0, bag_size: 88\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0070, weighted_loss: 0.0021, label: 1, bag_size: 32\n",
      "batch 359, loss: 0.0011, instance_loss: 0.0138, weighted_loss: 0.0049, label: 1, bag_size: 103\n",
      "batch 379, loss: 0.0167, instance_loss: 0.0149, weighted_loss: 0.0162, label: 0, bag_size: 101\n",
      "batch 399, loss: 1.0232, instance_loss: 1.2650, weighted_loss: 1.0957, label: 0, bag_size: 93\n",
      "batch 419, loss: 0.1255, instance_loss: 1.3224, weighted_loss: 0.4846, label: 1, bag_size: 51\n",
      "batch 439, loss: 4.9159, instance_loss: 5.0797, weighted_loss: 4.9650, label: 1, bag_size: 24\n",
      "batch 459, loss: 0.2683, instance_loss: 0.8171, weighted_loss: 0.4329, label: 0, bag_size: 48\n",
      "batch 479, loss: 0.0354, instance_loss: 1.9775, weighted_loss: 0.6180, label: 1, bag_size: 21\n",
      "batch 499, loss: 0.0847, instance_loss: 2.4770, weighted_loss: 0.8023, label: 1, bag_size: 23\n",
      "batch 519, loss: 0.0001, instance_loss: 0.1816, weighted_loss: 0.0546, label: 0, bag_size: 79\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0983, weighted_loss: 0.0297, label: 0, bag_size: 19\n",
      "batch 559, loss: 2.1133, instance_loss: 1.5562, weighted_loss: 1.9462, label: 1, bag_size: 30\n",
      "batch 579, loss: 0.0001, instance_loss: 0.3313, weighted_loss: 0.0995, label: 0, bag_size: 40\n",
      "batch 599, loss: 0.2264, instance_loss: 0.5106, weighted_loss: 0.3117, label: 1, bag_size: 65\n",
      "batch 619, loss: 0.0000, instance_loss: 0.2786, weighted_loss: 0.0836, label: 0, bag_size: 49\n",
      "batch 639, loss: 0.0008, instance_loss: 0.5409, weighted_loss: 0.1628, label: 1, bag_size: 86\n",
      "batch 659, loss: 0.0005, instance_loss: 0.3239, weighted_loss: 0.0975, label: 0, bag_size: 27\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.959295827123696: correct 10299/10736\n",
      "class 1 clustering acc 0.7511177347242921: correct 4032/5368\n",
      "Epoch: 16, train_loss: 0.3399, train_clustering_loss:  0.3908, train_error: 0.1133\n",
      "class 0: acc 0.8795180722891566, correct 292/332\n",
      "class 1: acc 0.8938053097345132, correct 303/339\n",
      "\n",
      "Val Set, val_loss: 0.4598, val_error: 0.1786, auc: 0.9013\n",
      "class 0 clustering acc 0.8898809523809523: correct 1196/1344\n",
      "class 1 clustering acc 0.47023809523809523: correct 316/672\n",
      "class 0: acc 0.813953488372093, correct 35/43\n",
      "class 1: acc 0.8292682926829268, correct 34/41\n",
      "EarlyStopping counter: 13 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3901, instance_loss: 0.1980, weighted_loss: 0.3324, label: 0, bag_size: 77\n",
      "batch 39, loss: 0.0006, instance_loss: 0.4855, weighted_loss: 0.1461, label: 1, bag_size: 29\n",
      "batch 59, loss: 0.0155, instance_loss: 0.6493, weighted_loss: 0.2056, label: 1, bag_size: 51\n",
      "batch 79, loss: 0.0002, instance_loss: 0.3732, weighted_loss: 0.1121, label: 0, bag_size: 63\n",
      "batch 99, loss: 0.0001, instance_loss: 0.2951, weighted_loss: 0.0886, label: 0, bag_size: 29\n",
      "batch 119, loss: 0.0134, instance_loss: 0.2720, weighted_loss: 0.0910, label: 1, bag_size: 40\n",
      "batch 139, loss: 0.0000, instance_loss: 0.1645, weighted_loss: 0.0494, label: 1, bag_size: 30\n",
      "batch 159, loss: 6.3524, instance_loss: 3.4702, weighted_loss: 5.4877, label: 1, bag_size: 67\n",
      "batch 179, loss: 0.0033, instance_loss: 0.1440, weighted_loss: 0.0455, label: 1, bag_size: 31\n",
      "batch 199, loss: 0.0665, instance_loss: 0.8490, weighted_loss: 0.3013, label: 0, bag_size: 17\n",
      "batch 219, loss: 0.3350, instance_loss: 0.2396, weighted_loss: 0.3064, label: 1, bag_size: 126\n",
      "batch 239, loss: 0.0107, instance_loss: 0.5103, weighted_loss: 0.1606, label: 0, bag_size: 47\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0315, weighted_loss: 0.0095, label: 1, bag_size: 67\n",
      "batch 279, loss: 0.2904, instance_loss: 1.0538, weighted_loss: 0.5194, label: 0, bag_size: 28\n",
      "batch 299, loss: 0.0619, instance_loss: 0.7873, weighted_loss: 0.2795, label: 1, bag_size: 69\n",
      "batch 319, loss: 0.0001, instance_loss: 0.3265, weighted_loss: 0.0980, label: 0, bag_size: 64\n",
      "batch 339, loss: 0.0000, instance_loss: 0.7035, weighted_loss: 0.2110, label: 0, bag_size: 91\n",
      "batch 359, loss: 0.0001, instance_loss: 0.4301, weighted_loss: 0.1291, label: 0, bag_size: 72\n",
      "batch 379, loss: 0.0004, instance_loss: 0.0200, weighted_loss: 0.0063, label: 1, bag_size: 84\n",
      "batch 399, loss: 0.0855, instance_loss: 0.9711, weighted_loss: 0.3512, label: 0, bag_size: 33\n",
      "batch 419, loss: 0.4389, instance_loss: 0.3256, weighted_loss: 0.4049, label: 1, bag_size: 56\n",
      "batch 439, loss: 0.0006, instance_loss: 0.0037, weighted_loss: 0.0016, label: 1, bag_size: 62\n",
      "batch 459, loss: 0.0000, instance_loss: 0.3105, weighted_loss: 0.0932, label: 0, bag_size: 27\n",
      "batch 479, loss: 0.0120, instance_loss: 0.0642, weighted_loss: 0.0277, label: 1, bag_size: 16\n",
      "batch 499, loss: 0.0011, instance_loss: 0.0062, weighted_loss: 0.0026, label: 1, bag_size: 76\n",
      "batch 519, loss: 0.0040, instance_loss: 0.0579, weighted_loss: 0.0202, label: 1, bag_size: 26\n",
      "batch 539, loss: 0.1861, instance_loss: 0.7274, weighted_loss: 0.3485, label: 0, bag_size: 93\n",
      "batch 559, loss: 0.0000, instance_loss: 0.1248, weighted_loss: 0.0375, label: 0, bag_size: 80\n",
      "batch 579, loss: 0.0033, instance_loss: 0.0829, weighted_loss: 0.0272, label: 0, bag_size: 78\n",
      "batch 599, loss: 0.0056, instance_loss: 0.0831, weighted_loss: 0.0288, label: 0, bag_size: 61\n",
      "batch 619, loss: 0.1971, instance_loss: 0.9838, weighted_loss: 0.4331, label: 0, bag_size: 55\n",
      "batch 639, loss: 0.0003, instance_loss: 0.3304, weighted_loss: 0.0993, label: 0, bag_size: 107\n",
      "batch 659, loss: 0.2160, instance_loss: 0.1706, weighted_loss: 0.2024, label: 0, bag_size: 94\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.938338301043219: correct 10074/10736\n",
      "class 1 clustering acc 0.6747391952309985: correct 3622/5368\n",
      "Epoch: 17, train_loss: 0.4350, train_clustering_loss:  0.5148, train_error: 0.1356\n",
      "class 0: acc 0.8647058823529412, correct 294/340\n",
      "class 1: acc 0.8640483383685801, correct 286/331\n",
      "\n",
      "Val Set, val_loss: 0.5770, val_error: 0.2976, auc: 0.8718\n",
      "class 0 clustering acc 0.9724702380952381: correct 1307/1344\n",
      "class 1 clustering acc 0.7619047619047619: correct 512/672\n",
      "class 0: acc 0.8837209302325582, correct 38/43\n",
      "class 1: acc 0.5121951219512195, correct 21/41\n",
      "EarlyStopping counter: 14 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0043, instance_loss: 0.0754, weighted_loss: 0.0257, label: 1, bag_size: 60\n",
      "batch 39, loss: 0.0013, instance_loss: 0.1456, weighted_loss: 0.0446, label: 0, bag_size: 110\n",
      "batch 59, loss: 1.1158, instance_loss: 0.3779, weighted_loss: 0.8945, label: 0, bag_size: 66\n",
      "batch 79, loss: 0.8258, instance_loss: 0.3326, weighted_loss: 0.6778, label: 1, bag_size: 80\n",
      "batch 99, loss: 2.1054, instance_loss: 0.4796, weighted_loss: 1.6177, label: 0, bag_size: 79\n",
      "batch 119, loss: 0.0002, instance_loss: 0.3412, weighted_loss: 0.1025, label: 1, bag_size: 100\n",
      "batch 139, loss: 0.0000, instance_loss: 0.7218, weighted_loss: 0.2166, label: 0, bag_size: 89\n",
      "batch 159, loss: 0.0000, instance_loss: 0.1543, weighted_loss: 0.0463, label: 1, bag_size: 62\n",
      "batch 179, loss: 6.9401, instance_loss: 3.6746, weighted_loss: 5.9605, label: 0, bag_size: 22\n",
      "batch 199, loss: 0.0939, instance_loss: 0.0342, weighted_loss: 0.0760, label: 1, bag_size: 95\n",
      "batch 219, loss: 6.0744, instance_loss: 1.7929, weighted_loss: 4.7900, label: 0, bag_size: 43\n",
      "batch 239, loss: 0.0064, instance_loss: 0.5582, weighted_loss: 0.1719, label: 0, bag_size: 31\n",
      "batch 259, loss: 0.0761, instance_loss: 0.9087, weighted_loss: 0.3259, label: 0, bag_size: 87\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0641, weighted_loss: 0.0192, label: 1, bag_size: 62\n",
      "batch 299, loss: 0.5806, instance_loss: 0.5188, weighted_loss: 0.5621, label: 0, bag_size: 43\n",
      "batch 319, loss: 0.0205, instance_loss: 0.0498, weighted_loss: 0.0293, label: 1, bag_size: 31\n",
      "batch 339, loss: 0.0048, instance_loss: 0.1819, weighted_loss: 0.0579, label: 1, bag_size: 87\n",
      "batch 359, loss: 0.0149, instance_loss: 0.5673, weighted_loss: 0.1806, label: 0, bag_size: 83\n",
      "batch 379, loss: 0.0039, instance_loss: 0.0509, weighted_loss: 0.0180, label: 1, bag_size: 30\n",
      "batch 399, loss: 0.1110, instance_loss: 0.0781, weighted_loss: 0.1012, label: 1, bag_size: 54\n",
      "batch 419, loss: 0.0005, instance_loss: 0.0360, weighted_loss: 0.0111, label: 1, bag_size: 68\n",
      "batch 439, loss: 0.0004, instance_loss: 0.3978, weighted_loss: 0.1196, label: 0, bag_size: 66\n",
      "batch 459, loss: 0.0029, instance_loss: 0.0171, weighted_loss: 0.0071, label: 1, bag_size: 24\n",
      "batch 479, loss: 0.0173, instance_loss: 0.3052, weighted_loss: 0.1037, label: 0, bag_size: 39\n",
      "batch 499, loss: 0.0191, instance_loss: 0.0524, weighted_loss: 0.0291, label: 1, bag_size: 28\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0129, weighted_loss: 0.0040, label: 1, bag_size: 35\n",
      "batch 539, loss: 0.0090, instance_loss: 0.0786, weighted_loss: 0.0299, label: 1, bag_size: 67\n",
      "batch 559, loss: 0.0056, instance_loss: 0.0205, weighted_loss: 0.0100, label: 1, bag_size: 56\n",
      "batch 579, loss: 0.1702, instance_loss: 0.2813, weighted_loss: 0.2035, label: 0, bag_size: 68\n",
      "batch 599, loss: 0.0110, instance_loss: 0.1916, weighted_loss: 0.0651, label: 0, bag_size: 94\n",
      "batch 619, loss: 0.0594, instance_loss: 2.3689, weighted_loss: 0.7523, label: 0, bag_size: 33\n",
      "batch 639, loss: 0.0002, instance_loss: 0.0539, weighted_loss: 0.0163, label: 1, bag_size: 44\n",
      "batch 659, loss: 2.1695, instance_loss: 0.7516, weighted_loss: 1.7442, label: 1, bag_size: 83\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9413189269746647: correct 10106/10736\n",
      "class 1 clustering acc 0.7065946348733234: correct 3793/5368\n",
      "Epoch: 18, train_loss: 0.5687, train_clustering_loss:  0.5322, train_error: 0.1759\n",
      "class 0: acc 0.7993197278911565, correct 235/294\n",
      "class 1: acc 0.843501326259947, correct 318/377\n",
      "\n",
      "Val Set, val_loss: 2.2593, val_error: 0.3452, auc: 0.9410\n",
      "class 0 clustering acc 0.8563988095238095: correct 1151/1344\n",
      "class 1 clustering acc 0.49255952380952384: correct 331/672\n",
      "class 0: acc 0.32558139534883723, correct 14/43\n",
      "class 1: acc 1.0, correct 41/41\n",
      "EarlyStopping counter: 15 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0231, instance_loss: 0.1544, weighted_loss: 0.0625, label: 1, bag_size: 77\n",
      "batch 39, loss: 1.1771, instance_loss: 0.7699, weighted_loss: 1.0549, label: 1, bag_size: 77\n",
      "batch 59, loss: 0.1790, instance_loss: 0.1035, weighted_loss: 0.1563, label: 1, bag_size: 20\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0559, weighted_loss: 0.0168, label: 0, bag_size: 114\n",
      "batch 99, loss: 0.7066, instance_loss: 0.7689, weighted_loss: 0.7253, label: 1, bag_size: 62\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0758, weighted_loss: 0.0227, label: 1, bag_size: 109\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0150, weighted_loss: 0.0045, label: 1, bag_size: 90\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0754, weighted_loss: 0.0226, label: 0, bag_size: 37\n",
      "batch 179, loss: 0.0187, instance_loss: 0.1017, weighted_loss: 0.0436, label: 0, bag_size: 14\n",
      "batch 199, loss: 0.0040, instance_loss: 0.3202, weighted_loss: 0.0989, label: 1, bag_size: 31\n",
      "batch 219, loss: 0.0006, instance_loss: 0.1300, weighted_loss: 0.0394, label: 0, bag_size: 29\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0562, weighted_loss: 0.0169, label: 1, bag_size: 72\n",
      "batch 259, loss: 2.4246, instance_loss: 1.1833, weighted_loss: 2.0522, label: 0, bag_size: 20\n",
      "batch 279, loss: 0.0479, instance_loss: 0.1584, weighted_loss: 0.0810, label: 1, bag_size: 61\n",
      "batch 299, loss: 0.0003, instance_loss: 0.0190, weighted_loss: 0.0059, label: 1, bag_size: 44\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0708, weighted_loss: 0.0213, label: 0, bag_size: 88\n",
      "batch 339, loss: 0.0018, instance_loss: 0.0414, weighted_loss: 0.0137, label: 0, bag_size: 24\n",
      "batch 359, loss: 2.4382, instance_loss: 2.5378, weighted_loss: 2.4681, label: 1, bag_size: 35\n",
      "batch 379, loss: 0.0342, instance_loss: 0.0598, weighted_loss: 0.0419, label: 1, bag_size: 80\n",
      "batch 399, loss: 0.0021, instance_loss: 0.0402, weighted_loss: 0.0135, label: 0, bag_size: 88\n",
      "batch 419, loss: 0.0000, instance_loss: 0.1893, weighted_loss: 0.0568, label: 0, bag_size: 65\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0171, weighted_loss: 0.0051, label: 1, bag_size: 19\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0059, weighted_loss: 0.0018, label: 1, bag_size: 41\n",
      "batch 479, loss: 0.0053, instance_loss: 0.1607, weighted_loss: 0.0519, label: 1, bag_size: 95\n",
      "batch 499, loss: 0.0040, instance_loss: 0.1040, weighted_loss: 0.0340, label: 0, bag_size: 72\n",
      "batch 519, loss: 0.3984, instance_loss: 0.1213, weighted_loss: 0.3153, label: 1, bag_size: 42\n",
      "batch 539, loss: 0.0005, instance_loss: 0.1315, weighted_loss: 0.0398, label: 0, bag_size: 95\n",
      "batch 559, loss: 0.0020, instance_loss: 0.1703, weighted_loss: 0.0525, label: 0, bag_size: 93\n",
      "batch 579, loss: 0.0059, instance_loss: 0.0312, weighted_loss: 0.0135, label: 1, bag_size: 58\n",
      "batch 599, loss: 0.0031, instance_loss: 0.3529, weighted_loss: 0.1080, label: 0, bag_size: 37\n",
      "batch 619, loss: 0.0170, instance_loss: 0.0574, weighted_loss: 0.0291, label: 1, bag_size: 76\n",
      "batch 639, loss: 0.0002, instance_loss: 0.0035, weighted_loss: 0.0012, label: 1, bag_size: 57\n",
      "batch 659, loss: 0.0007, instance_loss: 0.0097, weighted_loss: 0.0034, label: 1, bag_size: 38\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9676788375558867: correct 10389/10736\n",
      "class 1 clustering acc 0.787816691505216: correct 4229/5368\n",
      "Epoch: 19, train_loss: 0.4619, train_clustering_loss:  0.3785, train_error: 0.1252\n",
      "class 0: acc 0.8640776699029126, correct 267/309\n",
      "class 1: acc 0.8839779005524862, correct 320/362\n",
      "\n",
      "Val Set, val_loss: 0.5882, val_error: 0.1667, auc: 0.9410\n",
      "class 0 clustering acc 0.9471726190476191: correct 1273/1344\n",
      "class 1 clustering acc 0.6666666666666666: correct 448/672\n",
      "class 0: acc 0.7209302325581395, correct 31/43\n",
      "class 1: acc 0.9512195121951219, correct 39/41\n",
      "EarlyStopping counter: 16 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0666, instance_loss: 0.0639, weighted_loss: 0.0658, label: 1, bag_size: 31\n",
      "batch 39, loss: 0.0034, instance_loss: 0.0181, weighted_loss: 0.0078, label: 0, bag_size: 31\n",
      "batch 59, loss: 0.0007, instance_loss: 0.0019, weighted_loss: 0.0010, label: 1, bag_size: 90\n",
      "batch 79, loss: 0.0044, instance_loss: 0.0240, weighted_loss: 0.0103, label: 1, bag_size: 17\n",
      "batch 99, loss: 0.0005, instance_loss: 0.0015, weighted_loss: 0.0008, label: 1, bag_size: 35\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 1, bag_size: 98\n",
      "batch 139, loss: 0.0000, instance_loss: 0.1893, weighted_loss: 0.0568, label: 1, bag_size: 98\n",
      "batch 159, loss: 0.0149, instance_loss: 0.0915, weighted_loss: 0.0379, label: 0, bag_size: 43\n",
      "batch 179, loss: 0.0253, instance_loss: 0.0122, weighted_loss: 0.0214, label: 1, bag_size: 36\n",
      "batch 199, loss: 0.8677, instance_loss: 0.6829, weighted_loss: 0.8123, label: 1, bag_size: 60\n",
      "batch 219, loss: 0.1168, instance_loss: 0.3058, weighted_loss: 0.1735, label: 1, bag_size: 57\n",
      "batch 239, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 30\n",
      "batch 259, loss: 0.0000, instance_loss: 0.3255, weighted_loss: 0.0976, label: 0, bag_size: 101\n",
      "batch 279, loss: 0.0001, instance_loss: 0.1217, weighted_loss: 0.0366, label: 0, bag_size: 80\n",
      "batch 299, loss: 0.0004, instance_loss: 0.1792, weighted_loss: 0.0540, label: 0, bag_size: 56\n",
      "batch 319, loss: 0.1525, instance_loss: 0.0607, weighted_loss: 0.1249, label: 0, bag_size: 40\n",
      "batch 339, loss: 1.8999, instance_loss: 0.7498, weighted_loss: 1.5549, label: 1, bag_size: 45\n",
      "batch 359, loss: 0.0027, instance_loss: 0.0077, weighted_loss: 0.0042, label: 1, bag_size: 52\n",
      "batch 379, loss: 0.0685, instance_loss: 0.0628, weighted_loss: 0.0668, label: 0, bag_size: 50\n",
      "batch 399, loss: 0.0025, instance_loss: 0.2139, weighted_loss: 0.0659, label: 0, bag_size: 18\n",
      "batch 419, loss: 0.0081, instance_loss: 0.1993, weighted_loss: 0.0654, label: 0, bag_size: 114\n",
      "batch 439, loss: 2.2175, instance_loss: 1.2603, weighted_loss: 1.9304, label: 1, bag_size: 78\n",
      "batch 459, loss: 1.0056, instance_loss: 1.8250, weighted_loss: 1.2514, label: 0, bag_size: 75\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0035, weighted_loss: 0.0012, label: 1, bag_size: 100\n",
      "batch 499, loss: 0.0043, instance_loss: 0.1192, weighted_loss: 0.0388, label: 0, bag_size: 65\n",
      "batch 519, loss: 0.0312, instance_loss: 0.8525, weighted_loss: 0.2776, label: 1, bag_size: 53\n",
      "batch 539, loss: 0.0061, instance_loss: 0.2128, weighted_loss: 0.0681, label: 0, bag_size: 107\n",
      "batch 559, loss: 0.0012, instance_loss: 0.1143, weighted_loss: 0.0352, label: 0, bag_size: 24\n",
      "batch 579, loss: 0.0011, instance_loss: 0.0174, weighted_loss: 0.0060, label: 0, bag_size: 80\n",
      "batch 599, loss: 0.0106, instance_loss: 0.0367, weighted_loss: 0.0185, label: 1, bag_size: 23\n",
      "batch 619, loss: 0.0002, instance_loss: 0.1177, weighted_loss: 0.0355, label: 0, bag_size: 61\n",
      "batch 639, loss: 0.0026, instance_loss: 0.2503, weighted_loss: 0.0769, label: 0, bag_size: 106\n",
      "batch 659, loss: 1.5738, instance_loss: 2.5015, weighted_loss: 1.8521, label: 1, bag_size: 38\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9683308494783904: correct 10396/10736\n",
      "class 1 clustering acc 0.7986214605067065: correct 4287/5368\n",
      "Epoch: 20, train_loss: 0.3176, train_clustering_loss:  0.3458, train_error: 0.1133\n",
      "class 0: acc 0.8878504672897196, correct 285/321\n",
      "class 1: acc 0.8857142857142857, correct 310/350\n",
      "\n",
      "Val Set, val_loss: 0.7363, val_error: 0.2262, auc: 0.9104\n",
      "class 0 clustering acc 0.9553571428571429: correct 1284/1344\n",
      "class 1 clustering acc 0.7157738095238095: correct 481/672\n",
      "class 0: acc 0.9767441860465116, correct 42/43\n",
      "class 1: acc 0.5609756097560976, correct 23/41\n",
      "EarlyStopping counter: 17 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0035, instance_loss: 0.0834, weighted_loss: 0.0275, label: 1, bag_size: 63\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0718, weighted_loss: 0.0215, label: 0, bag_size: 80\n",
      "batch 59, loss: 0.0351, instance_loss: 0.2202, weighted_loss: 0.0906, label: 1, bag_size: 35\n",
      "batch 79, loss: 0.0052, instance_loss: 0.0772, weighted_loss: 0.0268, label: 1, bag_size: 78\n",
      "batch 99, loss: 0.0012, instance_loss: 0.1086, weighted_loss: 0.0335, label: 0, bag_size: 102\n",
      "batch 119, loss: 0.0000, instance_loss: 0.1172, weighted_loss: 0.0352, label: 0, bag_size: 45\n",
      "batch 139, loss: 0.0015, instance_loss: 0.3660, weighted_loss: 0.1108, label: 1, bag_size: 83\n",
      "batch 159, loss: 0.5091, instance_loss: 0.6248, weighted_loss: 0.5438, label: 1, bag_size: 51\n",
      "batch 179, loss: 0.0000, instance_loss: 0.2938, weighted_loss: 0.0882, label: 0, bag_size: 31\n",
      "batch 199, loss: 0.0003, instance_loss: 2.0818, weighted_loss: 0.6248, label: 0, bag_size: 65\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0362, weighted_loss: 0.0110, label: 1, bag_size: 31\n",
      "batch 239, loss: 0.1887, instance_loss: 0.2036, weighted_loss: 0.1932, label: 1, bag_size: 18\n",
      "batch 259, loss: 0.0429, instance_loss: 0.0917, weighted_loss: 0.0575, label: 1, bag_size: 48\n",
      "batch 279, loss: 0.2960, instance_loss: 0.1553, weighted_loss: 0.2538, label: 1, bag_size: 56\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0575, weighted_loss: 0.0173, label: 0, bag_size: 13\n",
      "batch 319, loss: 0.0004, instance_loss: 0.0103, weighted_loss: 0.0034, label: 1, bag_size: 34\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0129, weighted_loss: 0.0039, label: 0, bag_size: 25\n",
      "batch 359, loss: 0.0036, instance_loss: 0.0473, weighted_loss: 0.0167, label: 1, bag_size: 42\n",
      "batch 379, loss: 0.0037, instance_loss: 0.0178, weighted_loss: 0.0080, label: 0, bag_size: 49\n",
      "batch 399, loss: 0.0004, instance_loss: 0.0082, weighted_loss: 0.0027, label: 0, bag_size: 73\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0156, weighted_loss: 0.0047, label: 0, bag_size: 27\n",
      "batch 439, loss: 2.0693, instance_loss: 0.2425, weighted_loss: 1.5213, label: 1, bag_size: 52\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0071, weighted_loss: 0.0021, label: 1, bag_size: 88\n",
      "batch 479, loss: 0.0002, instance_loss: 0.2515, weighted_loss: 0.0756, label: 1, bag_size: 15\n",
      "batch 499, loss: 1.1403, instance_loss: 0.8069, weighted_loss: 1.0403, label: 1, bag_size: 28\n",
      "batch 519, loss: 0.0606, instance_loss: 0.2555, weighted_loss: 0.1191, label: 0, bag_size: 21\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0041, weighted_loss: 0.0012, label: 1, bag_size: 68\n",
      "batch 559, loss: 0.0595, instance_loss: 0.1603, weighted_loss: 0.0898, label: 0, bag_size: 35\n",
      "batch 579, loss: 0.0217, instance_loss: 0.0282, weighted_loss: 0.0237, label: 0, bag_size: 30\n",
      "batch 599, loss: 0.0032, instance_loss: 0.0103, weighted_loss: 0.0053, label: 0, bag_size: 88\n",
      "batch 619, loss: 0.0007, instance_loss: 0.0007, weighted_loss: 0.0007, label: 1, bag_size: 36\n",
      "batch 639, loss: 0.1940, instance_loss: 0.1491, weighted_loss: 0.1806, label: 1, bag_size: 76\n",
      "batch 659, loss: 0.0034, instance_loss: 0.0401, weighted_loss: 0.0144, label: 0, bag_size: 27\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9603204172876304: correct 10310/10736\n",
      "class 1 clustering acc 0.8163189269746647: correct 4382/5368\n",
      "Epoch: 21, train_loss: 0.3594, train_clustering_loss:  0.3647, train_error: 0.1073\n",
      "class 0: acc 0.8945868945868946, correct 314/351\n",
      "class 1: acc 0.890625, correct 285/320\n",
      "\n",
      "Val Set, val_loss: 0.4082, val_error: 0.1429, auc: 0.9348\n",
      "class 0 clustering acc 0.9479166666666666: correct 1274/1344\n",
      "class 1 clustering acc 0.7842261904761905: correct 527/672\n",
      "class 0: acc 0.7906976744186046, correct 34/43\n",
      "class 1: acc 0.926829268292683, correct 38/41\n",
      "EarlyStopping counter: 18 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0410, instance_loss: 0.0973, weighted_loss: 0.0579, label: 1, bag_size: 56\n",
      "batch 39, loss: 0.0004, instance_loss: 0.0041, weighted_loss: 0.0015, label: 1, bag_size: 102\n",
      "batch 59, loss: 0.0013, instance_loss: 0.0100, weighted_loss: 0.0039, label: 0, bag_size: 96\n",
      "batch 79, loss: 0.8774, instance_loss: 1.2371, weighted_loss: 0.9854, label: 0, bag_size: 27\n",
      "batch 99, loss: 0.3584, instance_loss: 0.2441, weighted_loss: 0.3241, label: 0, bag_size: 68\n",
      "batch 119, loss: 0.5710, instance_loss: 0.6310, weighted_loss: 0.5890, label: 1, bag_size: 100\n",
      "batch 139, loss: 0.0057, instance_loss: 0.1641, weighted_loss: 0.0532, label: 0, bag_size: 58\n",
      "batch 159, loss: 0.0002, instance_loss: 0.0092, weighted_loss: 0.0029, label: 1, bag_size: 31\n",
      "batch 179, loss: 0.0008, instance_loss: 0.0038, weighted_loss: 0.0017, label: 1, bag_size: 38\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0051, weighted_loss: 0.0015, label: 0, bag_size: 31\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0095, weighted_loss: 0.0028, label: 0, bag_size: 86\n",
      "batch 239, loss: 0.5947, instance_loss: 0.1979, weighted_loss: 0.4756, label: 0, bag_size: 48\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0362, weighted_loss: 0.0109, label: 0, bag_size: 101\n",
      "batch 279, loss: 0.0008, instance_loss: 0.0691, weighted_loss: 0.0213, label: 1, bag_size: 83\n",
      "batch 299, loss: 0.2640, instance_loss: 0.0766, weighted_loss: 0.2078, label: 1, bag_size: 136\n",
      "batch 319, loss: 0.0094, instance_loss: 0.0442, weighted_loss: 0.0198, label: 1, bag_size: 95\n",
      "batch 339, loss: 0.0000, instance_loss: 0.2114, weighted_loss: 0.0634, label: 0, bag_size: 27\n",
      "batch 359, loss: 1.2279, instance_loss: 0.4642, weighted_loss: 0.9988, label: 1, bag_size: 99\n",
      "batch 379, loss: 0.0011, instance_loss: 0.1432, weighted_loss: 0.0437, label: 1, bag_size: 98\n",
      "batch 399, loss: 0.0005, instance_loss: 0.0187, weighted_loss: 0.0060, label: 0, bag_size: 46\n",
      "batch 419, loss: 0.0043, instance_loss: 0.3486, weighted_loss: 0.1076, label: 1, bag_size: 100\n",
      "batch 439, loss: 0.0114, instance_loss: 0.0143, weighted_loss: 0.0123, label: 1, bag_size: 27\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 1, bag_size: 85\n",
      "batch 479, loss: 2.8732, instance_loss: 0.7662, weighted_loss: 2.2411, label: 0, bag_size: 53\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0058, weighted_loss: 0.0018, label: 1, bag_size: 31\n",
      "batch 519, loss: 0.0307, instance_loss: 0.5097, weighted_loss: 0.1744, label: 0, bag_size: 74\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0179, weighted_loss: 0.0055, label: 0, bag_size: 64\n",
      "batch 559, loss: 1.8256, instance_loss: 0.4841, weighted_loss: 1.4231, label: 0, bag_size: 73\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0196, weighted_loss: 0.0059, label: 0, bag_size: 101\n",
      "batch 599, loss: 0.5570, instance_loss: 1.1867, weighted_loss: 0.7459, label: 0, bag_size: 18\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0654, weighted_loss: 0.0196, label: 1, bag_size: 59\n",
      "batch 639, loss: 0.0001, instance_loss: 0.6002, weighted_loss: 0.1802, label: 0, bag_size: 53\n",
      "batch 659, loss: 0.0005, instance_loss: 1.6558, weighted_loss: 0.4971, label: 1, bag_size: 70\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9594821162444114: correct 10301/10736\n",
      "class 1 clustering acc 0.8081222056631893: correct 4338/5368\n",
      "Epoch: 22, train_loss: 0.5262, train_clustering_loss:  0.3686, train_error: 0.1118\n",
      "class 0: acc 0.8810289389067524, correct 274/311\n",
      "class 1: acc 0.8944444444444445, correct 322/360\n",
      "\n",
      "Val Set, val_loss: 0.4424, val_error: 0.1667, auc: 0.9217\n",
      "class 0 clustering acc 0.9196428571428571: correct 1236/1344\n",
      "class 1 clustering acc 0.6532738095238095: correct 439/672\n",
      "class 0: acc 0.8604651162790697, correct 37/43\n",
      "class 1: acc 0.8048780487804879, correct 33/41\n",
      "EarlyStopping counter: 19 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.1282, weighted_loss: 0.0385, label: 1, bag_size: 97\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0390, weighted_loss: 0.0117, label: 1, bag_size: 51\n",
      "batch 59, loss: 0.0010, instance_loss: 0.1251, weighted_loss: 0.0382, label: 1, bag_size: 74\n",
      "batch 79, loss: 3.1532, instance_loss: 0.7904, weighted_loss: 2.4444, label: 0, bag_size: 63\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0207, weighted_loss: 0.0062, label: 1, bag_size: 56\n",
      "batch 119, loss: 0.0019, instance_loss: 0.0503, weighted_loss: 0.0164, label: 1, bag_size: 101\n",
      "batch 139, loss: 0.4583, instance_loss: 0.3332, weighted_loss: 0.4207, label: 0, bag_size: 73\n",
      "batch 159, loss: 0.0013, instance_loss: 0.0606, weighted_loss: 0.0191, label: 1, bag_size: 26\n",
      "batch 179, loss: 0.0003, instance_loss: 0.0524, weighted_loss: 0.0159, label: 1, bag_size: 51\n",
      "batch 199, loss: 0.0008, instance_loss: 0.7371, weighted_loss: 0.2217, label: 1, bag_size: 46\n",
      "batch 219, loss: 2.4512, instance_loss: 0.0674, weighted_loss: 1.7361, label: 1, bag_size: 119\n",
      "batch 239, loss: 0.0000, instance_loss: 0.3844, weighted_loss: 0.1153, label: 0, bag_size: 118\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0286, weighted_loss: 0.0086, label: 0, bag_size: 30\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0363, weighted_loss: 0.0109, label: 1, bag_size: 40\n",
      "batch 299, loss: 8.6721, instance_loss: 0.9608, weighted_loss: 6.3587, label: 1, bag_size: 44\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0439, weighted_loss: 0.0132, label: 0, bag_size: 79\n",
      "batch 339, loss: 0.2087, instance_loss: 0.2272, weighted_loss: 0.2142, label: 1, bag_size: 118\n",
      "batch 359, loss: 7.5152, instance_loss: 2.0389, weighted_loss: 5.8724, label: 0, bag_size: 94\n",
      "batch 379, loss: 0.0022, instance_loss: 0.1261, weighted_loss: 0.0393, label: 1, bag_size: 68\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0382, weighted_loss: 0.0115, label: 0, bag_size: 25\n",
      "batch 419, loss: 0.0001, instance_loss: 0.0233, weighted_loss: 0.0070, label: 0, bag_size: 112\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0128, weighted_loss: 0.0038, label: 0, bag_size: 63\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0330, weighted_loss: 0.0099, label: 1, bag_size: 56\n",
      "batch 479, loss: 9.2002, instance_loss: 3.3902, weighted_loss: 7.4572, label: 0, bag_size: 28\n",
      "batch 499, loss: 0.0043, instance_loss: 0.2144, weighted_loss: 0.0673, label: 0, bag_size: 75\n",
      "batch 519, loss: 0.0006, instance_loss: 0.0623, weighted_loss: 0.0191, label: 1, bag_size: 87\n",
      "batch 539, loss: 0.1138, instance_loss: 0.0024, weighted_loss: 0.0804, label: 1, bag_size: 41\n",
      "batch 559, loss: 0.0002, instance_loss: 0.0326, weighted_loss: 0.0099, label: 0, bag_size: 75\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 44\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0039, weighted_loss: 0.0012, label: 0, bag_size: 80\n",
      "batch 619, loss: 3.1023, instance_loss: 1.7595, weighted_loss: 2.6994, label: 0, bag_size: 61\n",
      "batch 639, loss: 0.0033, instance_loss: 0.0129, weighted_loss: 0.0062, label: 1, bag_size: 62\n",
      "batch 659, loss: 0.0023, instance_loss: 0.0251, weighted_loss: 0.0091, label: 0, bag_size: 36\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9578986587183308: correct 10284/10736\n",
      "class 1 clustering acc 0.7818554396423248: correct 4197/5368\n",
      "Epoch: 23, train_loss: 0.6036, train_clustering_loss:  0.3882, train_error: 0.1207\n",
      "class 0: acc 0.8832335329341318, correct 295/334\n",
      "class 1: acc 0.8753709198813057, correct 295/337\n",
      "\n",
      "Val Set, val_loss: 0.3942, val_error: 0.1667, auc: 0.9410\n",
      "class 0 clustering acc 0.9605654761904762: correct 1291/1344\n",
      "class 1 clustering acc 0.7247023809523809: correct 487/672\n",
      "class 0: acc 0.7209302325581395, correct 31/43\n",
      "class 1: acc 0.9512195121951219, correct 39/41\n",
      "EarlyStopping counter: 20 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 2.0934, instance_loss: 0.0950, weighted_loss: 1.4939, label: 1, bag_size: 102\n",
      "batch 39, loss: 0.0226, instance_loss: 0.0347, weighted_loss: 0.0262, label: 0, bag_size: 62\n",
      "batch 59, loss: 0.1102, instance_loss: 0.1309, weighted_loss: 0.1164, label: 0, bag_size: 114\n",
      "batch 79, loss: 1.2358, instance_loss: 1.9286, weighted_loss: 1.4437, label: 1, bag_size: 29\n",
      "batch 99, loss: 0.0009, instance_loss: 0.0017, weighted_loss: 0.0011, label: 0, bag_size: 116\n",
      "batch 119, loss: 0.0066, instance_loss: 0.0130, weighted_loss: 0.0085, label: 0, bag_size: 42\n",
      "batch 139, loss: 0.0003, instance_loss: 0.0036, weighted_loss: 0.0013, label: 0, bag_size: 97\n",
      "batch 159, loss: 0.0023, instance_loss: 0.0045, weighted_loss: 0.0030, label: 1, bag_size: 123\n",
      "batch 179, loss: 0.0193, instance_loss: 0.0269, weighted_loss: 0.0216, label: 0, bag_size: 82\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0127, weighted_loss: 0.0038, label: 0, bag_size: 86\n",
      "batch 219, loss: 3.0407, instance_loss: 2.7097, weighted_loss: 2.9414, label: 0, bag_size: 93\n",
      "batch 239, loss: 0.0002, instance_loss: 0.0081, weighted_loss: 0.0025, label: 0, bag_size: 66\n",
      "batch 259, loss: 0.0027, instance_loss: 0.0603, weighted_loss: 0.0200, label: 0, bag_size: 78\n",
      "batch 279, loss: 0.0168, instance_loss: 0.0831, weighted_loss: 0.0367, label: 0, bag_size: 91\n",
      "batch 299, loss: 0.0828, instance_loss: 0.1457, weighted_loss: 0.1017, label: 1, bag_size: 41\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0576, weighted_loss: 0.0173, label: 1, bag_size: 88\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0427, weighted_loss: 0.0129, label: 0, bag_size: 66\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0130, weighted_loss: 0.0039, label: 1, bag_size: 83\n",
      "batch 379, loss: 0.0279, instance_loss: 0.0257, weighted_loss: 0.0272, label: 1, bag_size: 71\n",
      "batch 399, loss: 0.0003, instance_loss: 0.0039, weighted_loss: 0.0014, label: 0, bag_size: 72\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0122, weighted_loss: 0.0037, label: 0, bag_size: 74\n",
      "batch 439, loss: 0.0004, instance_loss: 0.0310, weighted_loss: 0.0096, label: 0, bag_size: 89\n",
      "batch 459, loss: 0.0341, instance_loss: 0.1341, weighted_loss: 0.0641, label: 1, bag_size: 118\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0026, weighted_loss: 0.0008, label: 0, bag_size: 65\n",
      "batch 499, loss: 0.0013, instance_loss: 0.0082, weighted_loss: 0.0034, label: 1, bag_size: 92\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0075, weighted_loss: 0.0022, label: 0, bag_size: 80\n",
      "batch 539, loss: 0.0115, instance_loss: 0.0587, weighted_loss: 0.0257, label: 0, bag_size: 35\n",
      "batch 559, loss: 0.1790, instance_loss: 0.2796, weighted_loss: 0.2092, label: 0, bag_size: 98\n",
      "batch 579, loss: 0.0044, instance_loss: 0.0602, weighted_loss: 0.0211, label: 1, bag_size: 64\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0160, weighted_loss: 0.0048, label: 0, bag_size: 87\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0023, weighted_loss: 0.0007, label: 0, bag_size: 110\n",
      "batch 639, loss: 4.9677, instance_loss: 1.5690, weighted_loss: 3.9481, label: 0, bag_size: 87\n",
      "batch 659, loss: 2.0863, instance_loss: 1.9803, weighted_loss: 2.0545, label: 0, bag_size: 17\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9665611028315947: correct 10377/10736\n",
      "class 1 clustering acc 0.8254470938897168: correct 4431/5368\n",
      "Epoch: 24, train_loss: 0.2947, train_clustering_loss:  0.3154, train_error: 0.1103\n",
      "class 0: acc 0.8950276243093923, correct 324/362\n",
      "class 1: acc 0.883495145631068, correct 273/309\n",
      "\n",
      "Val Set, val_loss: 0.4051, val_error: 0.1071, auc: 0.9308\n",
      "class 0 clustering acc 0.9553571428571429: correct 1284/1344\n",
      "class 1 clustering acc 0.7767857142857143: correct 522/672\n",
      "class 0: acc 0.9302325581395349, correct 40/43\n",
      "class 1: acc 0.8536585365853658, correct 35/41\n",
      "EarlyStopping counter: 21 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0267, instance_loss: 1.0863, weighted_loss: 0.3446, label: 0, bag_size: 44\n",
      "batch 39, loss: 0.4210, instance_loss: 0.0462, weighted_loss: 0.3086, label: 0, bag_size: 29\n",
      "batch 59, loss: 0.0002, instance_loss: 0.0093, weighted_loss: 0.0029, label: 1, bag_size: 86\n",
      "batch 79, loss: 0.0001, instance_loss: 0.0056, weighted_loss: 0.0018, label: 1, bag_size: 18\n",
      "batch 99, loss: 0.0004, instance_loss: 0.0007, weighted_loss: 0.0005, label: 1, bag_size: 75\n",
      "batch 119, loss: 0.0205, instance_loss: 0.0143, weighted_loss: 0.0186, label: 0, bag_size: 90\n",
      "batch 139, loss: 0.3493, instance_loss: 0.6888, weighted_loss: 0.4511, label: 0, bag_size: 28\n",
      "batch 159, loss: 0.0040, instance_loss: 0.0066, weighted_loss: 0.0048, label: 1, bag_size: 80\n",
      "batch 179, loss: 0.0008, instance_loss: 0.0079, weighted_loss: 0.0030, label: 0, bag_size: 85\n",
      "batch 199, loss: 0.1477, instance_loss: 0.9198, weighted_loss: 0.3793, label: 1, bag_size: 64\n",
      "batch 219, loss: 0.0071, instance_loss: 0.0064, weighted_loss: 0.0069, label: 1, bag_size: 83\n",
      "batch 239, loss: 0.0246, instance_loss: 0.0279, weighted_loss: 0.0256, label: 1, bag_size: 62\n",
      "batch 259, loss: 0.0155, instance_loss: 0.1198, weighted_loss: 0.0468, label: 0, bag_size: 50\n",
      "batch 279, loss: 0.0054, instance_loss: 0.8890, weighted_loss: 0.2705, label: 0, bag_size: 60\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0088, weighted_loss: 0.0027, label: 0, bag_size: 65\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0267, weighted_loss: 0.0080, label: 0, bag_size: 45\n",
      "batch 339, loss: 0.0010, instance_loss: 0.0451, weighted_loss: 0.0142, label: 1, bag_size: 119\n",
      "batch 359, loss: 0.0002, instance_loss: 0.0028, weighted_loss: 0.0010, label: 0, bag_size: 97\n",
      "batch 379, loss: 0.0205, instance_loss: 0.0107, weighted_loss: 0.0175, label: 0, bag_size: 82\n",
      "batch 399, loss: 0.0019, instance_loss: 0.1637, weighted_loss: 0.0504, label: 1, bag_size: 38\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0055, weighted_loss: 0.0017, label: 0, bag_size: 118\n",
      "batch 439, loss: 0.0117, instance_loss: 0.0034, weighted_loss: 0.0092, label: 0, bag_size: 88\n",
      "batch 459, loss: 0.0121, instance_loss: 0.0104, weighted_loss: 0.0116, label: 1, bag_size: 21\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0106, weighted_loss: 0.0032, label: 1, bag_size: 19\n",
      "batch 499, loss: 0.0003, instance_loss: 0.0164, weighted_loss: 0.0051, label: 0, bag_size: 41\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0065, weighted_loss: 0.0020, label: 0, bag_size: 89\n",
      "batch 539, loss: 0.0220, instance_loss: 0.1669, weighted_loss: 0.0655, label: 1, bag_size: 123\n",
      "batch 559, loss: 0.0006, instance_loss: 0.0080, weighted_loss: 0.0028, label: 0, bag_size: 107\n",
      "batch 579, loss: 0.1753, instance_loss: 1.3259, weighted_loss: 0.5205, label: 1, bag_size: 35\n",
      "batch 599, loss: 0.0041, instance_loss: 0.0515, weighted_loss: 0.0183, label: 0, bag_size: 58\n",
      "batch 619, loss: 0.0016, instance_loss: 0.0049, weighted_loss: 0.0026, label: 0, bag_size: 114\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 41\n",
      "batch 659, loss: 0.0070, instance_loss: 0.0221, weighted_loss: 0.0115, label: 1, bag_size: 100\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9725223546944859: correct 10441/10736\n",
      "class 1 clustering acc 0.8630774962742176: correct 4633/5368\n",
      "Epoch: 25, train_loss: 0.2984, train_clustering_loss:  0.2600, train_error: 0.0939\n",
      "class 0: acc 0.9069767441860465, correct 312/344\n",
      "class 1: acc 0.9051987767584098, correct 296/327\n",
      "\n",
      "Val Set, val_loss: 1.2276, val_error: 0.2143, auc: 0.9501\n",
      "class 0 clustering acc 0.9226190476190477: correct 1240/1344\n",
      "class 1 clustering acc 0.7857142857142857: correct 528/672\n",
      "class 0: acc 0.627906976744186, correct 27/43\n",
      "class 1: acc 0.9512195121951219, correct 39/41\n",
      "EarlyStopping counter: 22 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0023, weighted_loss: 0.0007, label: 1, bag_size: 32\n",
      "batch 39, loss: 0.0003, instance_loss: 0.0077, weighted_loss: 0.0025, label: 1, bag_size: 84\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 0, bag_size: 37\n",
      "batch 79, loss: 0.0047, instance_loss: 0.0660, weighted_loss: 0.0231, label: 1, bag_size: 64\n",
      "batch 99, loss: 0.0344, instance_loss: 0.0227, weighted_loss: 0.0309, label: 0, bag_size: 48\n",
      "batch 119, loss: 0.0153, instance_loss: 0.0120, weighted_loss: 0.0143, label: 1, bag_size: 69\n",
      "batch 139, loss: 0.0014, instance_loss: 0.0069, weighted_loss: 0.0030, label: 1, bag_size: 17\n",
      "batch 159, loss: 0.0011, instance_loss: 0.0390, weighted_loss: 0.0125, label: 0, bag_size: 57\n",
      "batch 179, loss: 0.0002, instance_loss: 0.2490, weighted_loss: 0.0748, label: 1, bag_size: 23\n",
      "batch 199, loss: 0.0018, instance_loss: 0.0377, weighted_loss: 0.0126, label: 1, bag_size: 58\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0180, weighted_loss: 0.0054, label: 0, bag_size: 24\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0124, weighted_loss: 0.0037, label: 1, bag_size: 19\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0142, weighted_loss: 0.0043, label: 1, bag_size: 83\n",
      "batch 279, loss: 0.0000, instance_loss: 0.1260, weighted_loss: 0.0378, label: 0, bag_size: 25\n",
      "batch 299, loss: 0.0027, instance_loss: 0.0123, weighted_loss: 0.0056, label: 1, bag_size: 80\n",
      "batch 319, loss: 0.0002, instance_loss: 0.0148, weighted_loss: 0.0045, label: 1, bag_size: 23\n",
      "batch 339, loss: 3.0236, instance_loss: 0.8191, weighted_loss: 2.3622, label: 1, bag_size: 99\n",
      "batch 359, loss: 0.0302, instance_loss: 0.6117, weighted_loss: 0.2046, label: 1, bag_size: 36\n",
      "batch 379, loss: 0.0047, instance_loss: 0.0242, weighted_loss: 0.0106, label: 1, bag_size: 78\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0067, weighted_loss: 0.0020, label: 0, bag_size: 41\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0044, weighted_loss: 0.0013, label: 0, bag_size: 14\n",
      "batch 439, loss: 0.0689, instance_loss: 0.0739, weighted_loss: 0.0704, label: 0, bag_size: 50\n",
      "batch 459, loss: 6.0764, instance_loss: 2.1395, weighted_loss: 4.8953, label: 1, bag_size: 35\n",
      "batch 479, loss: 0.0029, instance_loss: 0.0009, weighted_loss: 0.0023, label: 0, bag_size: 55\n",
      "batch 499, loss: 0.0096, instance_loss: 0.0112, weighted_loss: 0.0101, label: 1, bag_size: 53\n",
      "batch 519, loss: 0.0034, instance_loss: 0.0345, weighted_loss: 0.0127, label: 0, bag_size: 78\n",
      "batch 539, loss: 0.0441, instance_loss: 0.0364, weighted_loss: 0.0418, label: 0, bag_size: 18\n",
      "batch 559, loss: 0.1172, instance_loss: 0.4543, weighted_loss: 0.2183, label: 1, bag_size: 87\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0076, weighted_loss: 0.0023, label: 1, bag_size: 35\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0076, weighted_loss: 0.0023, label: 0, bag_size: 42\n",
      "batch 619, loss: 0.0004, instance_loss: 0.0513, weighted_loss: 0.0157, label: 0, bag_size: 85\n",
      "batch 639, loss: 0.0003, instance_loss: 0.0023, weighted_loss: 0.0009, label: 1, bag_size: 53\n",
      "batch 659, loss: 0.0003, instance_loss: 0.0122, weighted_loss: 0.0039, label: 0, bag_size: 50\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9681445603576752: correct 10394/10736\n",
      "class 1 clustering acc 0.8537630402384501: correct 4583/5368\n",
      "Epoch: 26, train_loss: 0.3093, train_clustering_loss:  0.2808, train_error: 0.0939\n",
      "class 0: acc 0.9006024096385542, correct 299/332\n",
      "class 1: acc 0.911504424778761, correct 309/339\n",
      "\n",
      "Val Set, val_loss: 0.8543, val_error: 0.1548, auc: 0.9334\n",
      "class 0 clustering acc 0.9248511904761905: correct 1243/1344\n",
      "class 1 clustering acc 0.7485119047619048: correct 503/672\n",
      "class 0: acc 0.7441860465116279, correct 32/43\n",
      "class 1: acc 0.9512195121951219, correct 39/41\n",
      "EarlyStopping counter: 23 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 8.9720, instance_loss: 2.8410, weighted_loss: 7.1327, label: 1, bag_size: 61\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0016, weighted_loss: 0.0005, label: 0, bag_size: 89\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 1, bag_size: 76\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0041, weighted_loss: 0.0012, label: 1, bag_size: 18\n",
      "batch 99, loss: 0.0044, instance_loss: 0.0782, weighted_loss: 0.0266, label: 0, bag_size: 50\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 31\n",
      "batch 139, loss: 0.0025, instance_loss: 4.4089, weighted_loss: 1.3244, label: 0, bag_size: 92\n",
      "batch 159, loss: 0.0000, instance_loss: 0.3992, weighted_loss: 0.1198, label: 0, bag_size: 107\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0149, weighted_loss: 0.0045, label: 0, bag_size: 46\n",
      "batch 199, loss: 34.2028, instance_loss: 5.4416, weighted_loss: 25.5744, label: 0, bag_size: 69\n",
      "batch 219, loss: 0.0209, instance_loss: 0.6477, weighted_loss: 0.2090, label: 0, bag_size: 73\n",
      "batch 239, loss: 2.9892, instance_loss: 1.3574, weighted_loss: 2.4997, label: 0, bag_size: 44\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0005, label: 0, bag_size: 19\n",
      "batch 279, loss: 2.8283, instance_loss: 0.6613, weighted_loss: 2.1782, label: 0, bag_size: 18\n",
      "batch 299, loss: 0.0002, instance_loss: 0.0061, weighted_loss: 0.0019, label: 1, bag_size: 95\n",
      "batch 319, loss: 2.8600, instance_loss: 0.5492, weighted_loss: 2.1668, label: 1, bag_size: 116\n",
      "batch 339, loss: 0.0424, instance_loss: 0.5060, weighted_loss: 0.1815, label: 1, bag_size: 74\n",
      "batch 359, loss: 0.0004, instance_loss: 0.1107, weighted_loss: 0.0335, label: 1, bag_size: 86\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0065, weighted_loss: 0.0019, label: 0, bag_size: 101\n",
      "batch 399, loss: 8.3370, instance_loss: 1.9430, weighted_loss: 6.4188, label: 0, bag_size: 37\n",
      "batch 419, loss: 0.2424, instance_loss: 0.1778, weighted_loss: 0.2230, label: 0, bag_size: 73\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0029, weighted_loss: 0.0009, label: 1, bag_size: 87\n",
      "batch 459, loss: 0.2853, instance_loss: 0.0558, weighted_loss: 0.2164, label: 1, bag_size: 72\n",
      "batch 479, loss: 9.8990, instance_loss: 9.2045, weighted_loss: 9.6907, label: 1, bag_size: 84\n",
      "batch 499, loss: 0.0000, instance_loss: 0.3455, weighted_loss: 0.1036, label: 0, bag_size: 81\n",
      "batch 519, loss: 4.8706, instance_loss: 0.7272, weighted_loss: 3.6275, label: 1, bag_size: 41\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0023, weighted_loss: 0.0007, label: 0, bag_size: 88\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0036, weighted_loss: 0.0011, label: 0, bag_size: 98\n",
      "batch 579, loss: 0.0007, instance_loss: 0.0179, weighted_loss: 0.0058, label: 0, bag_size: 132\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 51\n",
      "batch 619, loss: 0.0005, instance_loss: 0.0279, weighted_loss: 0.0087, label: 1, bag_size: 41\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0196, weighted_loss: 0.0059, label: 1, bag_size: 76\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0115, weighted_loss: 0.0034, label: 0, bag_size: 27\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9666542473919523: correct 10378/10736\n",
      "class 1 clustering acc 0.8509687034277198: correct 4568/5368\n",
      "Epoch: 27, train_loss: 0.4447, train_clustering_loss:  0.3222, train_error: 0.0999\n",
      "class 0: acc 0.8985074626865671, correct 301/335\n",
      "class 1: acc 0.9017857142857143, correct 303/336\n",
      "\n",
      "Val Set, val_loss: 0.5631, val_error: 0.1190, auc: 0.9263\n",
      "class 0 clustering acc 0.9471726190476191: correct 1273/1344\n",
      "class 1 clustering acc 0.8005952380952381: correct 538/672\n",
      "class 0: acc 0.8372093023255814, correct 36/43\n",
      "class 1: acc 0.926829268292683, correct 38/41\n",
      "EarlyStopping counter: 24 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0031, weighted_loss: 0.0011, label: 0, bag_size: 26\n",
      "batch 39, loss: 0.0004, instance_loss: 0.0112, weighted_loss: 0.0037, label: 0, bag_size: 42\n",
      "batch 59, loss: 0.3450, instance_loss: 0.6987, weighted_loss: 0.4511, label: 0, bag_size: 55\n",
      "batch 79, loss: 0.0036, instance_loss: 0.0351, weighted_loss: 0.0130, label: 0, bag_size: 96\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0074, weighted_loss: 0.0022, label: 0, bag_size: 67\n",
      "batch 119, loss: 0.0043, instance_loss: 0.0265, weighted_loss: 0.0110, label: 0, bag_size: 77\n",
      "batch 139, loss: 0.3224, instance_loss: 0.4995, weighted_loss: 0.3756, label: 1, bag_size: 85\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 0, bag_size: 77\n",
      "batch 179, loss: 0.0357, instance_loss: 0.4534, weighted_loss: 0.1610, label: 0, bag_size: 46\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 35\n",
      "batch 219, loss: 6.4211, instance_loss: 3.0415, weighted_loss: 5.4073, label: 0, bag_size: 48\n",
      "batch 239, loss: 1.4697, instance_loss: 2.6662, weighted_loss: 1.8286, label: 0, bag_size: 79\n",
      "batch 259, loss: 0.0021, instance_loss: 0.3104, weighted_loss: 0.0946, label: 1, bag_size: 60\n",
      "batch 279, loss: 0.0433, instance_loss: 0.4971, weighted_loss: 0.1794, label: 0, bag_size: 81\n",
      "batch 299, loss: 0.2981, instance_loss: 0.4754, weighted_loss: 0.3513, label: 0, bag_size: 103\n",
      "batch 319, loss: 0.0562, instance_loss: 0.5527, weighted_loss: 0.2051, label: 1, bag_size: 57\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0006, label: 1, bag_size: 39\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 0, bag_size: 77\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 86\n",
      "batch 399, loss: 0.0039, instance_loss: 0.0132, weighted_loss: 0.0067, label: 0, bag_size: 78\n",
      "batch 419, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 126\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0095, weighted_loss: 0.0029, label: 0, bag_size: 45\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 1, bag_size: 112\n",
      "batch 479, loss: 0.0141, instance_loss: 0.0791, weighted_loss: 0.0336, label: 1, bag_size: 92\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0108, weighted_loss: 0.0032, label: 1, bag_size: 126\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 79\n",
      "batch 539, loss: 0.0004, instance_loss: 0.0001, weighted_loss: 0.0003, label: 1, bag_size: 31\n",
      "batch 559, loss: 0.1364, instance_loss: 0.0536, weighted_loss: 0.1116, label: 0, bag_size: 18\n",
      "batch 579, loss: 0.0009, instance_loss: 0.0003, weighted_loss: 0.0007, label: 1, bag_size: 80\n",
      "batch 599, loss: 0.0447, instance_loss: 0.0721, weighted_loss: 0.0529, label: 0, bag_size: 77\n",
      "batch 619, loss: 1.0053, instance_loss: 0.4169, weighted_loss: 0.8288, label: 1, bag_size: 100\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0103, weighted_loss: 0.0031, label: 0, bag_size: 102\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0005, label: 0, bag_size: 78\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9745715350223547: correct 10463/10736\n",
      "class 1 clustering acc 0.8641952309985097: correct 4639/5368\n",
      "Epoch: 28, train_loss: 0.2733, train_clustering_loss:  0.2606, train_error: 0.0984\n",
      "class 0: acc 0.8988439306358381, correct 311/346\n",
      "class 1: acc 0.9046153846153846, correct 294/325\n",
      "\n",
      "Val Set, val_loss: 0.7213, val_error: 0.1667, auc: 0.9376\n",
      "class 0 clustering acc 0.9352678571428571: correct 1257/1344\n",
      "class 1 clustering acc 0.7916666666666666: correct 532/672\n",
      "class 0: acc 0.7209302325581395, correct 31/43\n",
      "class 1: acc 0.9512195121951219, correct 39/41\n",
      "EarlyStopping counter: 25 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1645, instance_loss: 0.1117, weighted_loss: 0.1487, label: 0, bag_size: 87\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 76\n",
      "batch 59, loss: 0.0361, instance_loss: 0.0357, weighted_loss: 0.0360, label: 0, bag_size: 69\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 0, bag_size: 67\n",
      "batch 99, loss: 0.0004, instance_loss: 0.0025, weighted_loss: 0.0010, label: 1, bag_size: 28\n",
      "batch 119, loss: 0.1194, instance_loss: 0.0742, weighted_loss: 0.1058, label: 1, bag_size: 81\n",
      "batch 139, loss: 0.0140, instance_loss: 0.0340, weighted_loss: 0.0200, label: 0, bag_size: 90\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0005, label: 1, bag_size: 68\n",
      "batch 179, loss: 0.0266, instance_loss: 0.0834, weighted_loss: 0.0437, label: 1, bag_size: 38\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0021, weighted_loss: 0.0007, label: 0, bag_size: 26\n",
      "batch 219, loss: 3.9750, instance_loss: 1.7960, weighted_loss: 3.3213, label: 0, bag_size: 54\n",
      "batch 239, loss: 4.4641, instance_loss: 4.7745, weighted_loss: 4.5572, label: 1, bag_size: 67\n",
      "batch 259, loss: 0.0078, instance_loss: 0.0168, weighted_loss: 0.0105, label: 1, bag_size: 107\n",
      "batch 279, loss: 0.0005, instance_loss: 0.0025, weighted_loss: 0.0011, label: 1, bag_size: 26\n",
      "batch 299, loss: 0.0042, instance_loss: 0.0376, weighted_loss: 0.0142, label: 1, bag_size: 64\n",
      "batch 319, loss: 0.0140, instance_loss: 0.2854, weighted_loss: 0.0955, label: 0, bag_size: 132\n",
      "batch 339, loss: 0.0191, instance_loss: 0.2317, weighted_loss: 0.0829, label: 1, bag_size: 102\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 86\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0028, weighted_loss: 0.0009, label: 0, bag_size: 49\n",
      "batch 399, loss: 4.3891, instance_loss: 3.0240, weighted_loss: 3.9795, label: 0, bag_size: 89\n",
      "batch 419, loss: 0.0009, instance_loss: 0.0232, weighted_loss: 0.0076, label: 0, bag_size: 33\n",
      "batch 439, loss: 0.0095, instance_loss: 0.0181, weighted_loss: 0.0121, label: 0, bag_size: 78\n",
      "batch 459, loss: 0.0063, instance_loss: 0.0005, weighted_loss: 0.0045, label: 0, bag_size: 97\n",
      "batch 479, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 38\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 25\n",
      "batch 519, loss: 0.0636, instance_loss: 0.3369, weighted_loss: 0.1456, label: 1, bag_size: 77\n",
      "batch 539, loss: 0.0005, instance_loss: 0.0008, weighted_loss: 0.0006, label: 1, bag_size: 56\n",
      "batch 559, loss: 1.9133, instance_loss: 0.2374, weighted_loss: 1.4105, label: 1, bag_size: 100\n",
      "batch 579, loss: 0.0105, instance_loss: 0.0415, weighted_loss: 0.0198, label: 1, bag_size: 28\n",
      "batch 599, loss: 0.0002, instance_loss: 0.0026, weighted_loss: 0.0009, label: 1, bag_size: 84\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 67\n",
      "batch 639, loss: 0.0026, instance_loss: 0.0096, weighted_loss: 0.0047, label: 1, bag_size: 74\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9767138599105812: correct 10486/10736\n",
      "class 1 clustering acc 0.8809612518628912: correct 4729/5368\n",
      "Epoch: 29, train_loss: 0.2645, train_clustering_loss:  0.2406, train_error: 0.0924\n",
      "class 0: acc 0.9017857142857143, correct 303/336\n",
      "class 1: acc 0.9134328358208955, correct 306/335\n",
      "\n",
      "Val Set, val_loss: 0.3560, val_error: 0.1190, auc: 0.9461\n",
      "class 0 clustering acc 0.9642857142857143: correct 1296/1344\n",
      "class 1 clustering acc 0.8020833333333334: correct 539/672\n",
      "class 0: acc 0.8837209302325582, correct 38/43\n",
      "class 1: acc 0.8780487804878049, correct 36/41\n",
      "Validation loss decreased (0.361845 --> 0.355989).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0007, instance_loss: 0.0005, weighted_loss: 0.0006, label: 1, bag_size: 153\n",
      "batch 39, loss: 0.4157, instance_loss: 0.0922, weighted_loss: 0.3187, label: 0, bag_size: 82\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 108\n",
      "batch 79, loss: 0.0003, instance_loss: 0.0011, weighted_loss: 0.0005, label: 0, bag_size: 88\n",
      "batch 99, loss: 0.0002, instance_loss: 0.0009, weighted_loss: 0.0004, label: 1, bag_size: 123\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 78\n",
      "batch 139, loss: 0.0063, instance_loss: 0.0199, weighted_loss: 0.0104, label: 0, bag_size: 26\n",
      "batch 159, loss: 0.0031, instance_loss: 0.0135, weighted_loss: 0.0062, label: 1, bag_size: 14\n",
      "batch 179, loss: 0.2930, instance_loss: 0.6898, weighted_loss: 0.4121, label: 0, bag_size: 89\n",
      "batch 199, loss: 0.0218, instance_loss: 0.5009, weighted_loss: 0.1656, label: 1, bag_size: 82\n",
      "batch 219, loss: 0.4590, instance_loss: 0.2363, weighted_loss: 0.3922, label: 0, bag_size: 90\n",
      "batch 239, loss: 0.0032, instance_loss: 0.0201, weighted_loss: 0.0083, label: 0, bag_size: 33\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 29\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 102\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 1, bag_size: 83\n",
      "batch 319, loss: 2.2685, instance_loss: 0.0092, weighted_loss: 1.5907, label: 0, bag_size: 82\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 0, bag_size: 61\n",
      "batch 359, loss: 0.0004, instance_loss: 0.0423, weighted_loss: 0.0130, label: 1, bag_size: 50\n",
      "batch 379, loss: 0.0018, instance_loss: 0.0061, weighted_loss: 0.0031, label: 0, bag_size: 76\n",
      "batch 399, loss: 0.0001, instance_loss: 0.1097, weighted_loss: 0.0330, label: 0, bag_size: 48\n",
      "batch 419, loss: 0.0895, instance_loss: 0.3688, weighted_loss: 0.1733, label: 0, bag_size: 25\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0048, weighted_loss: 0.0015, label: 1, bag_size: 99\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0062, weighted_loss: 0.0020, label: 1, bag_size: 67\n",
      "batch 479, loss: 3.6548, instance_loss: 4.7821, weighted_loss: 3.9930, label: 1, bag_size: 96\n",
      "batch 499, loss: 0.1877, instance_loss: 0.0867, weighted_loss: 0.1574, label: 0, bag_size: 108\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0054, weighted_loss: 0.0016, label: 0, bag_size: 86\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0128, weighted_loss: 0.0040, label: 0, bag_size: 58\n",
      "batch 559, loss: 0.7825, instance_loss: 1.0065, weighted_loss: 0.8497, label: 1, bag_size: 91\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0039, weighted_loss: 0.0012, label: 1, bag_size: 98\n",
      "batch 599, loss: 0.1726, instance_loss: 0.9131, weighted_loss: 0.3947, label: 1, bag_size: 30\n",
      "batch 619, loss: 0.0009, instance_loss: 0.0071, weighted_loss: 0.0027, label: 1, bag_size: 57\n",
      "batch 639, loss: 0.0056, instance_loss: 0.0995, weighted_loss: 0.0337, label: 1, bag_size: 126\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 88\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.978204172876304: correct 10502/10736\n",
      "class 1 clustering acc 0.8869225037257824: correct 4761/5368\n",
      "Epoch: 30, train_loss: 0.3063, train_clustering_loss:  0.2312, train_error: 0.0879\n",
      "class 0: acc 0.9217391304347826, correct 318/345\n",
      "class 1: acc 0.901840490797546, correct 294/326\n",
      "\n",
      "Val Set, val_loss: 0.7095, val_error: 0.1548, auc: 0.9376\n",
      "class 0 clustering acc 0.9494047619047619: correct 1276/1344\n",
      "class 1 clustering acc 0.8050595238095238: correct 541/672\n",
      "class 0: acc 0.9302325581395349, correct 40/43\n",
      "class 1: acc 0.7560975609756098, correct 31/41\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0094, instance_loss: 0.0115, weighted_loss: 0.0101, label: 1, bag_size: 80\n",
      "batch 39, loss: 0.0507, instance_loss: 0.8425, weighted_loss: 0.2883, label: 1, bag_size: 38\n",
      "batch 59, loss: 0.0004, instance_loss: 0.0212, weighted_loss: 0.0066, label: 1, bag_size: 128\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 0, bag_size: 65\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 1, bag_size: 18\n",
      "batch 119, loss: 0.3321, instance_loss: 0.4758, weighted_loss: 0.3752, label: 0, bag_size: 35\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0018, weighted_loss: 0.0006, label: 0, bag_size: 24\n",
      "batch 159, loss: 0.0002, instance_loss: 0.0025, weighted_loss: 0.0009, label: 1, bag_size: 112\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 97\n",
      "batch 199, loss: 0.0026, instance_loss: 0.0061, weighted_loss: 0.0037, label: 1, bag_size: 29\n",
      "batch 219, loss: 0.0005, instance_loss: 0.0806, weighted_loss: 0.0245, label: 1, bag_size: 23\n",
      "batch 239, loss: 0.0002, instance_loss: 0.0062, weighted_loss: 0.0020, label: 1, bag_size: 16\n",
      "batch 259, loss: 0.0021, instance_loss: 0.0314, weighted_loss: 0.0109, label: 0, bag_size: 53\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 42\n",
      "batch 299, loss: 0.0004, instance_loss: 0.0409, weighted_loss: 0.0126, label: 1, bag_size: 53\n",
      "batch 319, loss: 0.0010, instance_loss: 0.0234, weighted_loss: 0.0077, label: 0, bag_size: 17\n",
      "batch 339, loss: 0.5925, instance_loss: 0.4931, weighted_loss: 0.5627, label: 1, bag_size: 57\n",
      "batch 359, loss: 0.1143, instance_loss: 0.1302, weighted_loss: 0.1191, label: 0, bag_size: 35\n",
      "batch 379, loss: 5.1096, instance_loss: 4.0063, weighted_loss: 4.7786, label: 0, bag_size: 93\n",
      "batch 399, loss: 0.0645, instance_loss: 0.4628, weighted_loss: 0.1840, label: 1, bag_size: 56\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0021, weighted_loss: 0.0006, label: 0, bag_size: 88\n",
      "batch 439, loss: 0.0038, instance_loss: 0.0115, weighted_loss: 0.0061, label: 1, bag_size: 23\n",
      "batch 459, loss: 0.0182, instance_loss: 0.3982, weighted_loss: 0.1322, label: 1, bag_size: 50\n",
      "batch 479, loss: 0.0498, instance_loss: 0.1643, weighted_loss: 0.0842, label: 1, bag_size: 92\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0063, weighted_loss: 0.0019, label: 1, bag_size: 30\n",
      "batch 519, loss: 0.0153, instance_loss: 0.1874, weighted_loss: 0.0669, label: 1, bag_size: 72\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0022, weighted_loss: 0.0007, label: 0, bag_size: 101\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 0, bag_size: 71\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 25\n",
      "batch 599, loss: 0.3462, instance_loss: 2.5820, weighted_loss: 1.0170, label: 0, bag_size: 60\n",
      "batch 619, loss: 0.0987, instance_loss: 0.3298, weighted_loss: 0.1680, label: 1, bag_size: 51\n",
      "batch 639, loss: 0.0600, instance_loss: 0.9987, weighted_loss: 0.3416, label: 0, bag_size: 27\n",
      "batch 659, loss: 0.0451, instance_loss: 0.0035, weighted_loss: 0.0326, label: 0, bag_size: 25\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9764344262295082: correct 10483/10736\n",
      "class 1 clustering acc 0.8897168405365127: correct 4776/5368\n",
      "Epoch: 31, train_loss: 0.2562, train_clustering_loss:  0.2246, train_error: 0.0760\n",
      "class 0: acc 0.9279279279279279, correct 309/333\n",
      "class 1: acc 0.9201183431952663, correct 311/338\n",
      "\n",
      "Val Set, val_loss: 0.6874, val_error: 0.2024, auc: 0.9268\n",
      "class 0 clustering acc 0.9322916666666666: correct 1253/1344\n",
      "class 1 clustering acc 0.7604166666666666: correct 511/672\n",
      "class 0: acc 0.7209302325581395, correct 31/43\n",
      "class 1: acc 0.8780487804878049, correct 36/41\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0129, instance_loss: 0.1345, weighted_loss: 0.0494, label: 0, bag_size: 35\n",
      "batch 39, loss: 1.1634, instance_loss: 0.1725, weighted_loss: 0.8661, label: 0, bag_size: 22\n",
      "batch 59, loss: 1.0407, instance_loss: 0.2808, weighted_loss: 0.8127, label: 1, bag_size: 92\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 97\n",
      "batch 99, loss: 8.5995, instance_loss: 4.7046, weighted_loss: 7.4310, label: 1, bag_size: 54\n",
      "batch 119, loss: 0.5357, instance_loss: 1.9957, weighted_loss: 0.9737, label: 0, bag_size: 37\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 77\n",
      "batch 159, loss: 0.0071, instance_loss: 0.0094, weighted_loss: 0.0078, label: 0, bag_size: 41\n",
      "batch 179, loss: 0.0033, instance_loss: 0.0314, weighted_loss: 0.0118, label: 0, bag_size: 74\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 85\n",
      "batch 219, loss: 0.0005, instance_loss: 0.0038, weighted_loss: 0.0015, label: 1, bag_size: 31\n",
      "batch 239, loss: 0.0036, instance_loss: 0.0047, weighted_loss: 0.0039, label: 1, bag_size: 53\n",
      "batch 259, loss: 0.1847, instance_loss: 0.5144, weighted_loss: 0.2836, label: 1, bag_size: 35\n",
      "batch 279, loss: 0.0125, instance_loss: 0.0022, weighted_loss: 0.0094, label: 0, bag_size: 110\n",
      "batch 299, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 71\n",
      "batch 319, loss: 0.0002, instance_loss: 0.0002, weighted_loss: 0.0002, label: 0, bag_size: 102\n",
      "batch 339, loss: 0.0012, instance_loss: 0.0020, weighted_loss: 0.0014, label: 0, bag_size: 48\n",
      "batch 359, loss: 0.2926, instance_loss: 0.1200, weighted_loss: 0.2408, label: 0, bag_size: 28\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0004, label: 0, bag_size: 85\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0021, weighted_loss: 0.0006, label: 0, bag_size: 83\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0046, weighted_loss: 0.0014, label: 0, bag_size: 47\n",
      "batch 439, loss: 0.0002, instance_loss: 0.0048, weighted_loss: 0.0016, label: 1, bag_size: 17\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0021, weighted_loss: 0.0006, label: 1, bag_size: 36\n",
      "batch 479, loss: 0.0071, instance_loss: 0.0076, weighted_loss: 0.0072, label: 1, bag_size: 28\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0059, weighted_loss: 0.0019, label: 1, bag_size: 96\n",
      "batch 519, loss: 1.3485, instance_loss: 0.3638, weighted_loss: 1.0531, label: 1, bag_size: 64\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0069, weighted_loss: 0.0021, label: 0, bag_size: 43\n",
      "batch 559, loss: 0.0002, instance_loss: 0.0015, weighted_loss: 0.0006, label: 0, bag_size: 65\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 29\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 42\n",
      "batch 619, loss: 4.0627, instance_loss: 2.1954, weighted_loss: 3.5025, label: 1, bag_size: 98\n",
      "batch 639, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 65\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 39\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9784836065573771: correct 10505/10736\n",
      "class 1 clustering acc 0.8928837555886736: correct 4793/5368\n",
      "Epoch: 32, train_loss: 0.2049, train_clustering_loss:  0.2045, train_error: 0.0700\n",
      "class 0: acc 0.9283582089552239, correct 311/335\n",
      "class 1: acc 0.9315476190476191, correct 313/336\n",
      "\n",
      "Val Set, val_loss: 0.5501, val_error: 0.1190, auc: 0.9297\n",
      "class 0 clustering acc 0.9538690476190477: correct 1282/1344\n",
      "class 1 clustering acc 0.8452380952380952: correct 568/672\n",
      "class 0: acc 0.8604651162790697, correct 37/43\n",
      "class 1: acc 0.9024390243902439, correct 37/41\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0024, instance_loss: 0.0044, weighted_loss: 0.0030, label: 1, bag_size: 81\n",
      "batch 39, loss: 0.3363, instance_loss: 1.3162, weighted_loss: 0.6303, label: 1, bag_size: 98\n",
      "batch 59, loss: 0.0001, instance_loss: 0.0024, weighted_loss: 0.0008, label: 0, bag_size: 74\n",
      "batch 79, loss: 0.5520, instance_loss: 2.2188, weighted_loss: 1.0520, label: 0, bag_size: 79\n",
      "batch 99, loss: 0.0017, instance_loss: 0.1040, weighted_loss: 0.0324, label: 1, bag_size: 76\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 27\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 20\n",
      "batch 159, loss: 0.0735, instance_loss: 0.6061, weighted_loss: 0.2333, label: 0, bag_size: 77\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 29\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0083, weighted_loss: 0.0025, label: 0, bag_size: 38\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0043, weighted_loss: 0.0013, label: 1, bag_size: 45\n",
      "batch 239, loss: 0.0221, instance_loss: 0.1307, weighted_loss: 0.0547, label: 0, bag_size: 79\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0103, weighted_loss: 0.0031, label: 0, bag_size: 64\n",
      "batch 279, loss: 0.0066, instance_loss: 0.0507, weighted_loss: 0.0198, label: 0, bag_size: 53\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0055, weighted_loss: 0.0017, label: 0, bag_size: 95\n",
      "batch 319, loss: 4.0408, instance_loss: 2.9744, weighted_loss: 3.7208, label: 1, bag_size: 84\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 57\n",
      "batch 359, loss: 0.0029, instance_loss: 0.0015, weighted_loss: 0.0025, label: 1, bag_size: 76\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0289, weighted_loss: 0.0087, label: 0, bag_size: 81\n",
      "batch 399, loss: 0.2281, instance_loss: 0.0321, weighted_loss: 0.1693, label: 0, bag_size: 98\n",
      "batch 419, loss: 0.0007, instance_loss: 0.0218, weighted_loss: 0.0070, label: 1, bag_size: 94\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0268, weighted_loss: 0.0081, label: 0, bag_size: 87\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 25\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 37\n",
      "batch 499, loss: 0.0977, instance_loss: 0.0562, weighted_loss: 0.0852, label: 0, bag_size: 26\n",
      "batch 519, loss: 0.0109, instance_loss: 0.1671, weighted_loss: 0.0578, label: 0, bag_size: 94\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0354, weighted_loss: 0.0109, label: 1, bag_size: 89\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 80\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0388, weighted_loss: 0.0117, label: 1, bag_size: 61\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0940, weighted_loss: 0.0282, label: 1, bag_size: 24\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0067, weighted_loss: 0.0020, label: 1, bag_size: 24\n",
      "batch 639, loss: 0.0002, instance_loss: 0.0496, weighted_loss: 0.0150, label: 0, bag_size: 30\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 71\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9756892697466468: correct 10475/10736\n",
      "class 1 clustering acc 0.896795827123696: correct 4814/5368\n",
      "Epoch: 33, train_loss: 0.3030, train_clustering_loss:  0.2284, train_error: 0.0775\n",
      "class 0: acc 0.9262536873156342, correct 314/339\n",
      "class 1: acc 0.9186746987951807, correct 305/332\n",
      "\n",
      "Val Set, val_loss: 0.8421, val_error: 0.1667, auc: 0.9268\n",
      "class 0 clustering acc 0.9479166666666666: correct 1274/1344\n",
      "class 1 clustering acc 0.8005952380952381: correct 538/672\n",
      "class 0: acc 0.7674418604651163, correct 33/43\n",
      "class 1: acc 0.9024390243902439, correct 37/41\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0075, instance_loss: 0.0714, weighted_loss: 0.0267, label: 0, bag_size: 79\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 93\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0005, label: 1, bag_size: 92\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0038, weighted_loss: 0.0011, label: 0, bag_size: 36\n",
      "batch 99, loss: 2.5147, instance_loss: 0.0096, weighted_loss: 1.7632, label: 1, bag_size: 60\n",
      "batch 119, loss: 0.0033, instance_loss: 0.0445, weighted_loss: 0.0156, label: 1, bag_size: 83\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 65\n",
      "batch 159, loss: 0.1058, instance_loss: 0.2540, weighted_loss: 0.1503, label: 0, bag_size: 62\n",
      "batch 179, loss: 0.0008, instance_loss: 0.0016, weighted_loss: 0.0010, label: 1, bag_size: 32\n",
      "batch 199, loss: 3.3559, instance_loss: 0.8258, weighted_loss: 2.5968, label: 1, bag_size: 46\n",
      "batch 219, loss: 0.0009, instance_loss: 0.5728, weighted_loss: 0.1724, label: 0, bag_size: 28\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 95\n",
      "batch 259, loss: 0.0056, instance_loss: 0.0243, weighted_loss: 0.0112, label: 0, bag_size: 82\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 81\n",
      "batch 299, loss: 0.1520, instance_loss: 0.3000, weighted_loss: 0.1964, label: 0, bag_size: 77\n",
      "batch 319, loss: 0.0000, instance_loss: 0.1752, weighted_loss: 0.0526, label: 0, bag_size: 25\n",
      "batch 339, loss: 0.0077, instance_loss: 0.1656, weighted_loss: 0.0550, label: 1, bag_size: 91\n",
      "batch 359, loss: 0.0011, instance_loss: 0.0060, weighted_loss: 0.0026, label: 1, bag_size: 88\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0154, weighted_loss: 0.0046, label: 1, bag_size: 17\n",
      "batch 399, loss: 0.4924, instance_loss: 0.0175, weighted_loss: 0.3499, label: 0, bag_size: 88\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0030, weighted_loss: 0.0009, label: 1, bag_size: 104\n",
      "batch 439, loss: 0.0198, instance_loss: 0.3683, weighted_loss: 0.1244, label: 1, bag_size: 54\n",
      "batch 459, loss: 10.3713, instance_loss: 6.2301, weighted_loss: 9.1290, label: 1, bag_size: 13\n",
      "batch 479, loss: 0.0046, instance_loss: 1.6348, weighted_loss: 0.4936, label: 1, bag_size: 21\n",
      "batch 499, loss: 0.0000, instance_loss: 1.5614, weighted_loss: 0.4684, label: 1, bag_size: 30\n",
      "batch 519, loss: 0.0006, instance_loss: 0.0023, weighted_loss: 0.0011, label: 1, bag_size: 32\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 77\n",
      "batch 559, loss: 4.8813, instance_loss: 2.3528, weighted_loss: 4.1228, label: 0, bag_size: 35\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0029, weighted_loss: 0.0009, label: 1, bag_size: 81\n",
      "batch 599, loss: 1.3239, instance_loss: 0.3406, weighted_loss: 1.0289, label: 1, bag_size: 67\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 96\n",
      "batch 639, loss: 0.0016, instance_loss: 0.6147, weighted_loss: 0.1855, label: 0, bag_size: 40\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 65\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9614381520119225: correct 10322/10736\n",
      "class 1 clustering acc 0.8226527570789866: correct 4416/5368\n",
      "Epoch: 34, train_loss: 0.5258, train_clustering_loss:  0.3618, train_error: 0.1133\n",
      "class 0: acc 0.8901734104046243, correct 308/346\n",
      "class 1: acc 0.8830769230769231, correct 287/325\n",
      "\n",
      "Val Set, val_loss: 1.1736, val_error: 0.1786, auc: 0.9393\n",
      "class 0 clustering acc 0.9657738095238095: correct 1298/1344\n",
      "class 1 clustering acc 0.84375: correct 567/672\n",
      "class 0: acc 0.6744186046511628, correct 29/43\n",
      "class 1: acc 0.975609756097561, correct 40/41\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.0516, weighted_loss: 0.0155, label: 1, bag_size: 17\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0037, weighted_loss: 0.0011, label: 1, bag_size: 123\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0025, weighted_loss: 0.0008, label: 1, bag_size: 26\n",
      "batch 79, loss: 0.0401, instance_loss: 0.1317, weighted_loss: 0.0676, label: 0, bag_size: 82\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0088, weighted_loss: 0.0027, label: 1, bag_size: 109\n",
      "batch 119, loss: 0.0003, instance_loss: 0.0338, weighted_loss: 0.0103, label: 0, bag_size: 78\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0037, weighted_loss: 0.0011, label: 0, bag_size: 65\n",
      "batch 159, loss: 0.0795, instance_loss: 0.3556, weighted_loss: 0.1623, label: 0, bag_size: 81\n",
      "batch 179, loss: 3.0026, instance_loss: 0.3995, weighted_loss: 2.2217, label: 1, bag_size: 24\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 110\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 26\n",
      "batch 239, loss: 0.2924, instance_loss: 1.1356, weighted_loss: 0.5453, label: 1, bag_size: 29\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0025, weighted_loss: 0.0008, label: 0, bag_size: 71\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0007, weighted_loss: 0.0003, label: 0, bag_size: 67\n",
      "batch 299, loss: 0.0091, instance_loss: 0.2078, weighted_loss: 0.0687, label: 1, bag_size: 77\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 89\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0046, weighted_loss: 0.0014, label: 0, bag_size: 73\n",
      "batch 359, loss: 0.0003, instance_loss: 0.0044, weighted_loss: 0.0015, label: 0, bag_size: 50\n",
      "batch 379, loss: 0.0143, instance_loss: 0.0440, weighted_loss: 0.0232, label: 1, bag_size: 95\n",
      "batch 399, loss: 0.0029, instance_loss: 0.2342, weighted_loss: 0.0723, label: 0, bag_size: 28\n",
      "batch 419, loss: 0.0976, instance_loss: 0.0918, weighted_loss: 0.0958, label: 0, bag_size: 67\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 63\n",
      "batch 459, loss: 0.0019, instance_loss: 0.0043, weighted_loss: 0.0026, label: 0, bag_size: 66\n",
      "batch 479, loss: 0.0014, instance_loss: 0.0087, weighted_loss: 0.0036, label: 1, bag_size: 67\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 25\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 25\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 44\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 31\n",
      "batch 579, loss: 0.0007, instance_loss: 0.0003, weighted_loss: 0.0006, label: 1, bag_size: 83\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 88\n",
      "batch 619, loss: 0.0108, instance_loss: 0.0000, weighted_loss: 0.0076, label: 0, bag_size: 64\n",
      "batch 639, loss: 0.0021, instance_loss: 0.0012, weighted_loss: 0.0019, label: 1, bag_size: 71\n",
      "batch 659, loss: 2.2665, instance_loss: 0.0135, weighted_loss: 1.5906, label: 0, bag_size: 29\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9755029806259314: correct 10473/10736\n",
      "class 1 clustering acc 0.8854321907600596: correct 4753/5368\n",
      "Epoch: 35, train_loss: 0.2322, train_clustering_loss:  0.2188, train_error: 0.0775\n",
      "class 0: acc 0.9138461538461539, correct 297/325\n",
      "class 1: acc 0.930635838150289, correct 322/346\n",
      "\n",
      "Val Set, val_loss: 1.1513, val_error: 0.2262, auc: 0.9461\n",
      "class 0 clustering acc 0.9308035714285714: correct 1251/1344\n",
      "class 1 clustering acc 0.8735119047619048: correct 587/672\n",
      "class 0: acc 0.9534883720930233, correct 41/43\n",
      "class 1: acc 0.5853658536585366, correct 24/41\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3522, instance_loss: 0.3217, weighted_loss: 0.3430, label: 0, bag_size: 43\n",
      "batch 39, loss: 2.6402, instance_loss: 4.0085, weighted_loss: 3.0506, label: 1, bag_size: 126\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0068, weighted_loss: 0.0020, label: 0, bag_size: 50\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0125, weighted_loss: 0.0037, label: 1, bag_size: 27\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 108\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0421, weighted_loss: 0.0126, label: 1, bag_size: 85\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0381, weighted_loss: 0.0114, label: 0, bag_size: 85\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0869, weighted_loss: 0.0261, label: 1, bag_size: 41\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0020, weighted_loss: 0.0006, label: 1, bag_size: 71\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0158, weighted_loss: 0.0047, label: 1, bag_size: 17\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0448, weighted_loss: 0.0134, label: 1, bag_size: 23\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0005, label: 1, bag_size: 101\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0524, weighted_loss: 0.0158, label: 1, bag_size: 82\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 70\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0076, weighted_loss: 0.0023, label: 1, bag_size: 76\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0128, weighted_loss: 0.0038, label: 1, bag_size: 32\n",
      "batch 339, loss: 0.0020, instance_loss: 0.1837, weighted_loss: 0.0565, label: 1, bag_size: 118\n",
      "batch 359, loss: 0.0043, instance_loss: 0.3473, weighted_loss: 0.1072, label: 1, bag_size: 53\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 37\n",
      "batch 399, loss: 0.0011, instance_loss: 0.6479, weighted_loss: 0.1951, label: 0, bag_size: 118\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 71\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 68\n",
      "batch 459, loss: 1.6517, instance_loss: 1.0401, weighted_loss: 1.4682, label: 1, bag_size: 43\n",
      "batch 479, loss: 4.5154, instance_loss: 1.0994, weighted_loss: 3.4906, label: 1, bag_size: 31\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 61\n",
      "batch 519, loss: 0.0011, instance_loss: 0.0002, weighted_loss: 0.0009, label: 0, bag_size: 116\n",
      "batch 539, loss: 0.0176, instance_loss: 0.0949, weighted_loss: 0.0408, label: 0, bag_size: 64\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 35\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0056, weighted_loss: 0.0017, label: 0, bag_size: 89\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 97\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 97\n",
      "batch 639, loss: 0.8471, instance_loss: 0.4772, weighted_loss: 0.7361, label: 0, bag_size: 75\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 31\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9687034277198212: correct 10400/10736\n",
      "class 1 clustering acc 0.860655737704918: correct 4620/5368\n",
      "Epoch: 36, train_loss: 0.4141, train_clustering_loss:  0.3000, train_error: 0.0909\n",
      "class 0: acc 0.907185628742515, correct 303/334\n",
      "class 1: acc 0.9109792284866469, correct 307/337\n",
      "\n",
      "Val Set, val_loss: 1.2786, val_error: 0.2024, auc: 0.9399\n",
      "class 0 clustering acc 0.9270833333333334: correct 1246/1344\n",
      "class 1 clustering acc 0.7708333333333334: correct 518/672\n",
      "class 0: acc 0.6046511627906976, correct 26/43\n",
      "class 1: acc 1.0, correct 41/41\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0037, instance_loss: 0.0265, weighted_loss: 0.0105, label: 1, bag_size: 31\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0023, weighted_loss: 0.0007, label: 1, bag_size: 58\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0036, weighted_loss: 0.0011, label: 0, bag_size: 66\n",
      "batch 79, loss: 0.0003, instance_loss: 0.0115, weighted_loss: 0.0036, label: 0, bag_size: 110\n",
      "batch 99, loss: 0.0030, instance_loss: 0.2364, weighted_loss: 0.0730, label: 1, bag_size: 24\n",
      "batch 119, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 88\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0041, weighted_loss: 0.0013, label: 0, bag_size: 85\n",
      "batch 159, loss: 0.0213, instance_loss: 0.0193, weighted_loss: 0.0207, label: 0, bag_size: 82\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0000, label: 0, bag_size: 49\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 99\n",
      "batch 219, loss: 0.4763, instance_loss: 0.6095, weighted_loss: 0.5162, label: 0, bag_size: 41\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0036, weighted_loss: 0.0011, label: 0, bag_size: 26\n",
      "batch 259, loss: 0.6308, instance_loss: 1.0500, weighted_loss: 0.7565, label: 0, bag_size: 90\n",
      "batch 279, loss: 0.0004, instance_loss: 0.0713, weighted_loss: 0.0217, label: 1, bag_size: 56\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 30\n",
      "batch 319, loss: 0.0018, instance_loss: 0.2316, weighted_loss: 0.0707, label: 1, bag_size: 53\n",
      "batch 339, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 91\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 32\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 102\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0032, weighted_loss: 0.0010, label: 1, bag_size: 75\n",
      "batch 419, loss: 0.0271, instance_loss: 0.0123, weighted_loss: 0.0227, label: 0, bag_size: 48\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 45\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 22\n",
      "batch 479, loss: 0.0006, instance_loss: 0.0141, weighted_loss: 0.0047, label: 1, bag_size: 38\n",
      "batch 499, loss: 0.0629, instance_loss: 0.2418, weighted_loss: 0.1166, label: 1, bag_size: 53\n",
      "batch 519, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 74\n",
      "batch 539, loss: 0.1218, instance_loss: 0.0332, weighted_loss: 0.0952, label: 1, bag_size: 91\n",
      "batch 559, loss: 0.0168, instance_loss: 1.2396, weighted_loss: 0.3837, label: 0, bag_size: 92\n",
      "batch 579, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 39\n",
      "batch 599, loss: 0.0021, instance_loss: 0.0013, weighted_loss: 0.0019, label: 0, bag_size: 64\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 42\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 28\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 0, bag_size: 107\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9785767511177347: correct 10506/10736\n",
      "class 1 clustering acc 0.8880402384500745: correct 4767/5368\n",
      "Epoch: 37, train_loss: 0.2160, train_clustering_loss:  0.2001, train_error: 0.0760\n",
      "class 0: acc 0.922360248447205, correct 297/322\n",
      "class 1: acc 0.9255014326647565, correct 323/349\n",
      "\n",
      "Val Set, val_loss: 0.6844, val_error: 0.2143, auc: 0.9518\n",
      "class 0 clustering acc 0.9516369047619048: correct 1279/1344\n",
      "class 1 clustering acc 0.7544642857142857: correct 507/672\n",
      "class 0: acc 0.9767441860465116, correct 42/43\n",
      "class 1: acc 0.5853658536585366, correct 24/41\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0535, instance_loss: 0.1311, weighted_loss: 0.0768, label: 1, bag_size: 23\n",
      "batch 39, loss: 0.0570, instance_loss: 0.0000, weighted_loss: 0.0399, label: 1, bag_size: 31\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 0, bag_size: 53\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 58\n",
      "batch 99, loss: 0.0008, instance_loss: 0.0319, weighted_loss: 0.0101, label: 1, bag_size: 119\n",
      "batch 119, loss: 0.0467, instance_loss: 0.3541, weighted_loss: 0.1389, label: 0, bag_size: 41\n",
      "batch 139, loss: 0.0024, instance_loss: 0.0493, weighted_loss: 0.0165, label: 1, bag_size: 85\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 61\n",
      "batch 179, loss: 0.0619, instance_loss: 0.5222, weighted_loss: 0.2000, label: 1, bag_size: 76\n",
      "batch 199, loss: 0.0330, instance_loss: 0.0785, weighted_loss: 0.0466, label: 0, bag_size: 51\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0036, weighted_loss: 0.0011, label: 0, bag_size: 90\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 69\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 77\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0017, weighted_loss: 0.0005, label: 1, bag_size: 33\n",
      "batch 299, loss: 0.1221, instance_loss: 0.3074, weighted_loss: 0.1777, label: 1, bag_size: 118\n",
      "batch 319, loss: 0.0095, instance_loss: 0.0647, weighted_loss: 0.0261, label: 0, bag_size: 39\n",
      "batch 339, loss: 0.0783, instance_loss: 0.0862, weighted_loss: 0.0807, label: 1, bag_size: 95\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 88\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 63\n",
      "batch 399, loss: 0.0002, instance_loss: 0.0090, weighted_loss: 0.0029, label: 1, bag_size: 31\n",
      "batch 419, loss: 0.9482, instance_loss: 0.2533, weighted_loss: 0.7397, label: 0, bag_size: 67\n",
      "batch 439, loss: 0.0156, instance_loss: 0.0363, weighted_loss: 0.0218, label: 1, bag_size: 52\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 49\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 118\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0001, label: 0, bag_size: 83\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 63\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 82\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0018, weighted_loss: 0.0006, label: 1, bag_size: 60\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0004, label: 1, bag_size: 100\n",
      "batch 599, loss: 0.0060, instance_loss: 0.0000, weighted_loss: 0.0042, label: 0, bag_size: 53\n",
      "batch 619, loss: 0.0016, instance_loss: 0.0014, weighted_loss: 0.0015, label: 0, bag_size: 28\n",
      "batch 639, loss: 0.0459, instance_loss: 0.1216, weighted_loss: 0.0686, label: 1, bag_size: 30\n",
      "batch 659, loss: 0.4700, instance_loss: 0.7402, weighted_loss: 0.5511, label: 1, bag_size: 53\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.977645305514158: correct 10496/10736\n",
      "class 1 clustering acc 0.8872950819672131: correct 4763/5368\n",
      "Epoch: 38, train_loss: 0.2349, train_clustering_loss:  0.2083, train_error: 0.0805\n",
      "class 0: acc 0.9164265129682997, correct 318/347\n",
      "class 1: acc 0.9228395061728395, correct 299/324\n",
      "\n",
      "Val Set, val_loss: 0.5879, val_error: 0.1310, auc: 0.9461\n",
      "class 0 clustering acc 0.9464285714285714: correct 1272/1344\n",
      "class 1 clustering acc 0.8333333333333334: correct 560/672\n",
      "class 0: acc 0.7674418604651163, correct 33/43\n",
      "class 1: acc 0.975609756097561, correct 40/41\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0130, instance_loss: 0.0140, weighted_loss: 0.0133, label: 1, bag_size: 62\n",
      "batch 39, loss: 0.0010, instance_loss: 0.0007, weighted_loss: 0.0009, label: 0, bag_size: 19\n",
      "batch 59, loss: 0.0027, instance_loss: 0.0144, weighted_loss: 0.0062, label: 0, bag_size: 78\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 71\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 88\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0021, weighted_loss: 0.0007, label: 1, bag_size: 39\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 47\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 35\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 63\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0034, weighted_loss: 0.0010, label: 1, bag_size: 41\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 36\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 112\n",
      "batch 259, loss: 0.0025, instance_loss: 0.0001, weighted_loss: 0.0018, label: 0, bag_size: 84\n",
      "batch 279, loss: 0.3053, instance_loss: 1.6876, weighted_loss: 0.7200, label: 1, bag_size: 63\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 97\n",
      "batch 319, loss: 0.4262, instance_loss: 0.5635, weighted_loss: 0.4674, label: 0, bag_size: 34\n",
      "batch 339, loss: 0.0002, instance_loss: 0.0096, weighted_loss: 0.0030, label: 1, bag_size: 84\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0001, label: 0, bag_size: 65\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 61\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 65\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 65\n",
      "batch 439, loss: 0.1701, instance_loss: 0.1095, weighted_loss: 0.1519, label: 0, bag_size: 17\n",
      "batch 459, loss: 0.0458, instance_loss: 0.0185, weighted_loss: 0.0376, label: 1, bag_size: 84\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 13\n",
      "batch 499, loss: 0.0004, instance_loss: 0.0038, weighted_loss: 0.0014, label: 1, bag_size: 60\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 96\n",
      "batch 539, loss: 0.0039, instance_loss: 0.0034, weighted_loss: 0.0037, label: 0, bag_size: 27\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 114\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 84\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 51\n",
      "batch 619, loss: 0.0048, instance_loss: 0.0143, weighted_loss: 0.0077, label: 1, bag_size: 92\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 28\n",
      "batch 659, loss: 0.2272, instance_loss: 0.6777, weighted_loss: 0.3623, label: 1, bag_size: 77\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9858420268256334: correct 10584/10736\n",
      "class 1 clustering acc 0.9292101341281669: correct 4988/5368\n",
      "Epoch: 39, train_loss: 0.1277, train_clustering_loss:  0.1376, train_error: 0.0402\n",
      "class 0: acc 0.9603399433427762, correct 339/353\n",
      "class 1: acc 0.9591194968553459, correct 305/318\n",
      "\n",
      "Val Set, val_loss: 0.5669, val_error: 0.1429, auc: 0.9427\n",
      "class 0 clustering acc 0.9486607142857143: correct 1275/1344\n",
      "class 1 clustering acc 0.8273809523809523: correct 556/672\n",
      "class 0: acc 0.8837209302325582, correct 38/43\n",
      "class 1: acc 0.8292682926829268, correct 34/41\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0037, instance_loss: 0.0729, weighted_loss: 0.0245, label: 1, bag_size: 70\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0041, weighted_loss: 0.0013, label: 1, bag_size: 84\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 33\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 68\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 56\n",
      "batch 119, loss: 0.0064, instance_loss: 0.0139, weighted_loss: 0.0087, label: 1, bag_size: 94\n",
      "batch 139, loss: 0.7187, instance_loss: 0.4480, weighted_loss: 0.6375, label: 1, bag_size: 59\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 58\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 77\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 85\n",
      "batch 219, loss: 0.0060, instance_loss: 0.0004, weighted_loss: 0.0043, label: 1, bag_size: 40\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 40\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 37\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 83\n",
      "batch 299, loss: 0.0057, instance_loss: 0.0042, weighted_loss: 0.0052, label: 1, bag_size: 53\n",
      "batch 319, loss: 0.0008, instance_loss: 0.0384, weighted_loss: 0.0120, label: 1, bag_size: 26\n",
      "batch 339, loss: 0.0034, instance_loss: 0.0120, weighted_loss: 0.0059, label: 1, bag_size: 53\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 67\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0378, weighted_loss: 0.0113, label: 0, bag_size: 19\n",
      "batch 399, loss: 0.0007, instance_loss: 0.0051, weighted_loss: 0.0020, label: 1, bag_size: 39\n",
      "batch 419, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 82\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 87\n",
      "batch 459, loss: 0.0259, instance_loss: 0.0035, weighted_loss: 0.0192, label: 1, bag_size: 38\n",
      "batch 479, loss: 0.0134, instance_loss: 0.0004, weighted_loss: 0.0095, label: 0, bag_size: 75\n",
      "batch 499, loss: 0.0021, instance_loss: 0.0046, weighted_loss: 0.0029, label: 1, bag_size: 86\n",
      "batch 519, loss: 0.0002, instance_loss: 0.0060, weighted_loss: 0.0019, label: 1, bag_size: 24\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 24\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 74\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0024, weighted_loss: 0.0007, label: 1, bag_size: 31\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 38\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 88\n",
      "batch 639, loss: 0.0003, instance_loss: 0.2954, weighted_loss: 0.0888, label: 0, bag_size: 58\n",
      "batch 659, loss: 0.0003, instance_loss: 0.0156, weighted_loss: 0.0049, label: 1, bag_size: 74\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9838859910581222: correct 10563/10736\n",
      "class 1 clustering acc 0.9239940387481371: correct 4960/5368\n",
      "Epoch: 40, train_loss: 0.1438, train_clustering_loss:  0.1429, train_error: 0.0462\n",
      "class 0: acc 0.9572649572649573, correct 336/351\n",
      "class 1: acc 0.95, correct 304/320\n",
      "\n",
      "Val Set, val_loss: 0.7342, val_error: 0.1429, auc: 0.9393\n",
      "class 0 clustering acc 0.9546130952380952: correct 1283/1344\n",
      "class 1 clustering acc 0.8556547619047619: correct 575/672\n",
      "class 0: acc 0.8837209302325582, correct 38/43\n",
      "class 1: acc 0.8292682926829268, correct 34/41\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 89\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 41\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0057, weighted_loss: 0.0017, label: 1, bag_size: 20\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 1, bag_size: 51\n",
      "batch 99, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 85\n",
      "batch 119, loss: 0.0028, instance_loss: 0.0208, weighted_loss: 0.0082, label: 1, bag_size: 70\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 57\n",
      "batch 159, loss: 0.2794, instance_loss: 0.0654, weighted_loss: 0.2152, label: 1, bag_size: 30\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0004, label: 1, bag_size: 21\n",
      "batch 199, loss: 0.0300, instance_loss: 0.2646, weighted_loss: 0.1004, label: 0, bag_size: 31\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0498, weighted_loss: 0.0149, label: 1, bag_size: 101\n",
      "batch 239, loss: 0.0004, instance_loss: 0.0168, weighted_loss: 0.0053, label: 0, bag_size: 66\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0023, weighted_loss: 0.0008, label: 1, bag_size: 83\n",
      "batch 279, loss: 1.6337, instance_loss: 1.9007, weighted_loss: 1.7138, label: 1, bag_size: 102\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 65\n",
      "batch 319, loss: 0.0549, instance_loss: 0.8841, weighted_loss: 0.3036, label: 1, bag_size: 80\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 88\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 107\n",
      "batch 379, loss: 0.0001, instance_loss: 0.0206, weighted_loss: 0.0062, label: 1, bag_size: 26\n",
      "batch 399, loss: 0.0002, instance_loss: 0.4703, weighted_loss: 0.1412, label: 0, bag_size: 81\n",
      "batch 419, loss: 0.0003, instance_loss: 0.0081, weighted_loss: 0.0026, label: 0, bag_size: 49\n",
      "batch 439, loss: 0.0901, instance_loss: 0.0478, weighted_loss: 0.0774, label: 0, bag_size: 62\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0064, weighted_loss: 0.0020, label: 0, bag_size: 50\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 84\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 1, bag_size: 19\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 36\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 72\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0177, weighted_loss: 0.0053, label: 1, bag_size: 83\n",
      "batch 579, loss: 10.1650, instance_loss: 1.6678, weighted_loss: 7.6158, label: 0, bag_size: 27\n",
      "batch 599, loss: 0.0000, instance_loss: 0.9839, weighted_loss: 0.2952, label: 1, bag_size: 36\n",
      "batch 619, loss: 0.0000, instance_loss: 0.1265, weighted_loss: 0.0379, label: 0, bag_size: 58\n",
      "batch 639, loss: 5.2678, instance_loss: 0.3551, weighted_loss: 3.7940, label: 0, bag_size: 25\n",
      "batch 659, loss: 0.0001, instance_loss: 0.1182, weighted_loss: 0.0355, label: 1, bag_size: 31\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9673994038748137: correct 10386/10736\n",
      "class 1 clustering acc 0.8621460506706409: correct 4628/5368\n",
      "Epoch: 41, train_loss: 0.5189, train_clustering_loss:  0.2966, train_error: 0.0924\n",
      "class 0: acc 0.9113149847094801, correct 298/327\n",
      "class 1: acc 0.9040697674418605, correct 311/344\n",
      "\n",
      "Val Set, val_loss: 2.2273, val_error: 0.2500, auc: 0.9178\n",
      "class 0 clustering acc 0.8497023809523809: correct 1142/1344\n",
      "class 1 clustering acc 0.4955357142857143: correct 333/672\n",
      "class 0: acc 0.5116279069767442, correct 22/43\n",
      "class 1: acc 1.0, correct 41/41\n",
      "EarlyStopping counter: 12 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.3808, weighted_loss: 0.1143, label: 1, bag_size: 65\n",
      "batch 39, loss: 0.0695, instance_loss: 0.4531, weighted_loss: 0.1846, label: 1, bag_size: 72\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0355, weighted_loss: 0.0107, label: 0, bag_size: 37\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0108, weighted_loss: 0.0032, label: 0, bag_size: 47\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0877, weighted_loss: 0.0263, label: 1, bag_size: 14\n",
      "batch 119, loss: 0.0027, instance_loss: 0.0894, weighted_loss: 0.0287, label: 1, bag_size: 49\n",
      "batch 139, loss: 2.1130, instance_loss: 0.0692, weighted_loss: 1.4998, label: 0, bag_size: 35\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 1, bag_size: 86\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0284, weighted_loss: 0.0085, label: 1, bag_size: 24\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0044, weighted_loss: 0.0013, label: 0, bag_size: 54\n",
      "batch 219, loss: 0.0000, instance_loss: 0.5142, weighted_loss: 0.1543, label: 1, bag_size: 32\n",
      "batch 239, loss: 0.0130, instance_loss: 0.3744, weighted_loss: 0.1214, label: 0, bag_size: 77\n",
      "batch 259, loss: 0.0069, instance_loss: 0.1656, weighted_loss: 0.0545, label: 0, bag_size: 78\n",
      "batch 279, loss: 0.0993, instance_loss: 1.5512, weighted_loss: 0.5349, label: 0, bag_size: 66\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 116\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 1, bag_size: 100\n",
      "batch 339, loss: 2.6283, instance_loss: 0.3043, weighted_loss: 1.9311, label: 1, bag_size: 67\n",
      "batch 359, loss: 0.0042, instance_loss: 0.5921, weighted_loss: 0.1806, label: 1, bag_size: 84\n",
      "batch 379, loss: 0.0001, instance_loss: 0.0122, weighted_loss: 0.0037, label: 0, bag_size: 61\n",
      "batch 399, loss: 0.0042, instance_loss: 1.4557, weighted_loss: 0.4397, label: 1, bag_size: 31\n",
      "batch 419, loss: 0.0077, instance_loss: 0.0789, weighted_loss: 0.0291, label: 1, bag_size: 36\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0016, weighted_loss: 0.0005, label: 0, bag_size: 89\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0436, weighted_loss: 0.0131, label: 0, bag_size: 49\n",
      "batch 479, loss: 0.0006, instance_loss: 0.0951, weighted_loss: 0.0289, label: 0, bag_size: 35\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0369, weighted_loss: 0.0112, label: 1, bag_size: 153\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0065, weighted_loss: 0.0019, label: 1, bag_size: 97\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0051, weighted_loss: 0.0015, label: 0, bag_size: 62\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 66\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0176, weighted_loss: 0.0053, label: 1, bag_size: 87\n",
      "batch 599, loss: 0.0060, instance_loss: 0.4406, weighted_loss: 0.1363, label: 0, bag_size: 79\n",
      "batch 619, loss: 0.0203, instance_loss: 0.2360, weighted_loss: 0.0850, label: 0, bag_size: 30\n",
      "batch 639, loss: 3.6566, instance_loss: 0.3544, weighted_loss: 2.6659, label: 0, bag_size: 78\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 0, bag_size: 63\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9576192250372578: correct 10281/10736\n",
      "class 1 clustering acc 0.8047690014903129: correct 4320/5368\n",
      "Epoch: 42, train_loss: 0.4404, train_clustering_loss:  0.3850, train_error: 0.0909\n",
      "class 0: acc 0.9032258064516129, correct 308/341\n",
      "class 1: acc 0.9151515151515152, correct 302/330\n",
      "\n",
      "Val Set, val_loss: 0.6532, val_error: 0.1310, auc: 0.9393\n",
      "class 0 clustering acc 0.9486607142857143: correct 1275/1344\n",
      "class 1 clustering acc 0.875: correct 588/672\n",
      "class 0: acc 0.9069767441860465, correct 39/43\n",
      "class 1: acc 0.8292682926829268, correct 34/41\n",
      "EarlyStopping counter: 13 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 49\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0349, weighted_loss: 0.0105, label: 0, bag_size: 83\n",
      "batch 59, loss: 0.2361, instance_loss: 0.0586, weighted_loss: 0.1829, label: 0, bag_size: 64\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 29\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 49\n",
      "batch 119, loss: 0.0108, instance_loss: 0.0086, weighted_loss: 0.0101, label: 1, bag_size: 87\n",
      "batch 139, loss: 0.0000, instance_loss: 0.4830, weighted_loss: 0.1449, label: 1, bag_size: 45\n",
      "batch 159, loss: 0.0003, instance_loss: 0.0118, weighted_loss: 0.0038, label: 1, bag_size: 56\n",
      "batch 179, loss: 3.5743, instance_loss: 3.0061, weighted_loss: 3.4038, label: 0, bag_size: 29\n",
      "batch 199, loss: 0.0145, instance_loss: 0.0028, weighted_loss: 0.0110, label: 1, bag_size: 76\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0579, weighted_loss: 0.0174, label: 1, bag_size: 72\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0081, weighted_loss: 0.0024, label: 0, bag_size: 67\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0421, weighted_loss: 0.0127, label: 1, bag_size: 33\n",
      "batch 279, loss: 0.5583, instance_loss: 3.5404, weighted_loss: 1.4529, label: 0, bag_size: 81\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0095, weighted_loss: 0.0029, label: 0, bag_size: 110\n",
      "batch 319, loss: 0.0008, instance_loss: 0.0208, weighted_loss: 0.0068, label: 0, bag_size: 35\n",
      "batch 339, loss: 0.0014, instance_loss: 0.0104, weighted_loss: 0.0041, label: 1, bag_size: 25\n",
      "batch 359, loss: 0.2041, instance_loss: 1.8795, weighted_loss: 0.7067, label: 1, bag_size: 17\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0040, weighted_loss: 0.0012, label: 1, bag_size: 53\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0362, weighted_loss: 0.0109, label: 0, bag_size: 65\n",
      "batch 419, loss: 1.1477, instance_loss: 0.1415, weighted_loss: 0.8459, label: 0, bag_size: 95\n",
      "batch 439, loss: 0.0000, instance_loss: 0.2344, weighted_loss: 0.0703, label: 1, bag_size: 87\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0027, weighted_loss: 0.0008, label: 0, bag_size: 113\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 63\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0434, weighted_loss: 0.0130, label: 0, bag_size: 104\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0020, weighted_loss: 0.0006, label: 1, bag_size: 74\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0053, weighted_loss: 0.0016, label: 0, bag_size: 87\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0071, weighted_loss: 0.0021, label: 1, bag_size: 80\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 96\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0109, weighted_loss: 0.0033, label: 0, bag_size: 28\n",
      "batch 619, loss: 1.8306, instance_loss: 1.0472, weighted_loss: 1.5956, label: 1, bag_size: 21\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 58\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 61\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9712183308494784: correct 10427/10736\n",
      "class 1 clustering acc 0.8658718330849479: correct 4648/5368\n",
      "Epoch: 43, train_loss: 0.3688, train_clustering_loss:  0.2837, train_error: 0.0864\n",
      "class 0: acc 0.9203296703296703, correct 335/364\n",
      "class 1: acc 0.9055374592833876, correct 278/307\n",
      "\n",
      "Val Set, val_loss: 1.0496, val_error: 0.1786, auc: 0.9419\n",
      "class 0 clustering acc 0.9665178571428571: correct 1299/1344\n",
      "class 1 clustering acc 0.8616071428571429: correct 579/672\n",
      "class 0: acc 0.7441860465116279, correct 32/43\n",
      "class 1: acc 0.9024390243902439, correct 37/41\n",
      "EarlyStopping counter: 14 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.1710, weighted_loss: 0.0513, label: 0, bag_size: 77\n",
      "batch 39, loss: 0.0004, instance_loss: 0.7453, weighted_loss: 0.2239, label: 0, bag_size: 101\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0028, weighted_loss: 0.0008, label: 0, bag_size: 67\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 108\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 88\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0023, weighted_loss: 0.0007, label: 1, bag_size: 44\n",
      "batch 139, loss: 0.7379, instance_loss: 0.0411, weighted_loss: 0.5289, label: 1, bag_size: 22\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0001, label: 1, bag_size: 97\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0109, weighted_loss: 0.0033, label: 1, bag_size: 23\n",
      "batch 199, loss: 0.5177, instance_loss: 0.5950, weighted_loss: 0.5409, label: 0, bag_size: 78\n",
      "batch 219, loss: 0.0264, instance_loss: 0.1670, weighted_loss: 0.0686, label: 1, bag_size: 32\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0109, weighted_loss: 0.0033, label: 1, bag_size: 75\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 115\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0028, weighted_loss: 0.0008, label: 0, bag_size: 20\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 108\n",
      "batch 319, loss: 3.9577, instance_loss: 2.0649, weighted_loss: 3.3899, label: 0, bag_size: 29\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 49\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0414, weighted_loss: 0.0124, label: 1, bag_size: 85\n",
      "batch 379, loss: 0.5258, instance_loss: 1.5692, weighted_loss: 0.8388, label: 1, bag_size: 40\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 58\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0044, weighted_loss: 0.0013, label: 1, bag_size: 31\n",
      "batch 439, loss: 0.0286, instance_loss: 0.2950, weighted_loss: 0.1086, label: 0, bag_size: 35\n",
      "batch 459, loss: 0.0003, instance_loss: 0.0235, weighted_loss: 0.0073, label: 1, bag_size: 91\n",
      "batch 479, loss: 5.7616, instance_loss: 1.5253, weighted_loss: 4.4907, label: 0, bag_size: 59\n",
      "batch 499, loss: 0.0195, instance_loss: 0.6051, weighted_loss: 0.1952, label: 0, bag_size: 29\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0034, weighted_loss: 0.0010, label: 1, bag_size: 32\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0021, weighted_loss: 0.0006, label: 1, bag_size: 84\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0056, weighted_loss: 0.0017, label: 0, bag_size: 101\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0014, weighted_loss: 0.0005, label: 0, bag_size: 28\n",
      "batch 599, loss: 0.0296, instance_loss: 0.7821, weighted_loss: 0.2554, label: 1, bag_size: 67\n",
      "batch 619, loss: 0.0000, instance_loss: 0.1034, weighted_loss: 0.0310, label: 1, bag_size: 40\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 74\n",
      "batch 659, loss: 0.0000, instance_loss: 0.8949, weighted_loss: 0.2685, label: 0, bag_size: 122\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9619038748137109: correct 10327/10736\n",
      "class 1 clustering acc 0.8397913561847988: correct 4508/5368\n",
      "Epoch: 44, train_loss: 0.4889, train_clustering_loss:  0.2995, train_error: 0.1148\n",
      "class 0: acc 0.8844984802431611, correct 291/329\n",
      "class 1: acc 0.8859649122807017, correct 303/342\n",
      "\n",
      "Val Set, val_loss: 0.8850, val_error: 0.1429, auc: 0.9365\n",
      "class 0 clustering acc 0.8407738095238095: correct 1130/1344\n",
      "class 1 clustering acc 0.49702380952380953: correct 334/672\n",
      "class 0: acc 0.9302325581395349, correct 40/43\n",
      "class 1: acc 0.7804878048780488, correct 32/41\n",
      "EarlyStopping counter: 15 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 2.8112, instance_loss: 1.4996, weighted_loss: 2.4177, label: 1, bag_size: 53\n",
      "batch 39, loss: 0.0002, instance_loss: 0.9101, weighted_loss: 0.2732, label: 1, bag_size: 22\n",
      "batch 59, loss: 0.0000, instance_loss: 0.5765, weighted_loss: 0.1730, label: 0, bag_size: 18\n",
      "batch 79, loss: 4.9431, instance_loss: 1.2719, weighted_loss: 3.8417, label: 0, bag_size: 96\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0821, weighted_loss: 0.0246, label: 1, bag_size: 31\n",
      "batch 119, loss: 0.0093, instance_loss: 0.3121, weighted_loss: 0.1002, label: 1, bag_size: 30\n",
      "batch 139, loss: 6.8584, instance_loss: 1.2168, weighted_loss: 5.1660, label: 0, bag_size: 34\n",
      "batch 159, loss: 0.0000, instance_loss: 0.2749, weighted_loss: 0.0825, label: 0, bag_size: 25\n",
      "batch 179, loss: 0.0000, instance_loss: 0.2842, weighted_loss: 0.0853, label: 0, bag_size: 53\n",
      "batch 199, loss: 0.0000, instance_loss: 0.5295, weighted_loss: 0.1588, label: 1, bag_size: 32\n",
      "batch 219, loss: 0.0000, instance_loss: 0.4042, weighted_loss: 0.1213, label: 1, bag_size: 153\n",
      "batch 239, loss: 0.7269, instance_loss: 2.3996, weighted_loss: 1.2287, label: 1, bag_size: 67\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0961, weighted_loss: 0.0288, label: 0, bag_size: 81\n",
      "batch 279, loss: 0.0002, instance_loss: 0.1363, weighted_loss: 0.0411, label: 0, bag_size: 29\n",
      "batch 299, loss: 0.0008, instance_loss: 1.0610, weighted_loss: 0.3189, label: 1, bag_size: 38\n",
      "batch 319, loss: 0.0279, instance_loss: 0.5638, weighted_loss: 0.1886, label: 1, bag_size: 16\n",
      "batch 339, loss: 0.0003, instance_loss: 0.1901, weighted_loss: 0.0572, label: 0, bag_size: 31\n",
      "batch 359, loss: 0.0003, instance_loss: 0.3429, weighted_loss: 0.1031, label: 1, bag_size: 31\n",
      "batch 379, loss: 0.0126, instance_loss: 0.1094, weighted_loss: 0.0417, label: 1, bag_size: 96\n",
      "batch 399, loss: 0.0155, instance_loss: 0.0213, weighted_loss: 0.0172, label: 0, bag_size: 110\n",
      "batch 419, loss: 0.0001, instance_loss: 0.0281, weighted_loss: 0.0085, label: 0, bag_size: 79\n",
      "batch 439, loss: 0.0006, instance_loss: 0.5469, weighted_loss: 0.1645, label: 1, bag_size: 61\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0479, weighted_loss: 0.0144, label: 1, bag_size: 72\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 116\n",
      "batch 499, loss: 0.1201, instance_loss: 1.3772, weighted_loss: 0.4972, label: 1, bag_size: 100\n",
      "batch 519, loss: 0.0066, instance_loss: 0.1505, weighted_loss: 0.0498, label: 1, bag_size: 87\n",
      "batch 539, loss: 0.0002, instance_loss: 0.0440, weighted_loss: 0.0134, label: 0, bag_size: 57\n",
      "batch 559, loss: 0.0053, instance_loss: 0.1266, weighted_loss: 0.0417, label: 1, bag_size: 33\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0040, weighted_loss: 0.0012, label: 1, bag_size: 112\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0057, weighted_loss: 0.0017, label: 0, bag_size: 86\n",
      "batch 619, loss: 0.0046, instance_loss: 0.8371, weighted_loss: 0.2544, label: 1, bag_size: 67\n",
      "batch 639, loss: 1.8691, instance_loss: 2.2797, weighted_loss: 1.9923, label: 0, bag_size: 25\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0131, weighted_loss: 0.0040, label: 0, bag_size: 66\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.930327868852459: correct 9988/10736\n",
      "class 1 clustering acc 0.7555886736214605: correct 4056/5368\n",
      "Epoch: 45, train_loss: 0.3403, train_clustering_loss:  0.4127, train_error: 0.0790\n",
      "class 0: acc 0.9186046511627907, correct 316/344\n",
      "class 1: acc 0.9235474006116208, correct 302/327\n",
      "\n",
      "Val Set, val_loss: 0.7234, val_error: 0.1905, auc: 0.9223\n",
      "class 0 clustering acc 0.9561011904761905: correct 1285/1344\n",
      "class 1 clustering acc 0.8184523809523809: correct 550/672\n",
      "class 0: acc 0.9069767441860465, correct 39/43\n",
      "class 1: acc 0.7073170731707317, correct 29/41\n",
      "EarlyStopping counter: 16 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0011, instance_loss: 0.0116, weighted_loss: 0.0042, label: 1, bag_size: 77\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 86\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 70\n",
      "batch 79, loss: 0.0004, instance_loss: 0.0600, weighted_loss: 0.0183, label: 0, bag_size: 110\n",
      "batch 99, loss: 0.0020, instance_loss: 0.0483, weighted_loss: 0.0159, label: 0, bag_size: 35\n",
      "batch 119, loss: 0.0267, instance_loss: 0.2146, weighted_loss: 0.0831, label: 1, bag_size: 23\n",
      "batch 139, loss: 0.0002, instance_loss: 0.0216, weighted_loss: 0.0066, label: 1, bag_size: 51\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0068, weighted_loss: 0.0021, label: 1, bag_size: 38\n",
      "batch 179, loss: 0.0003, instance_loss: 0.0037, weighted_loss: 0.0013, label: 1, bag_size: 64\n",
      "batch 199, loss: 0.0003, instance_loss: 0.0007, weighted_loss: 0.0004, label: 0, bag_size: 78\n",
      "batch 219, loss: 0.0262, instance_loss: 0.0651, weighted_loss: 0.0379, label: 1, bag_size: 43\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 51\n",
      "batch 259, loss: 1.3526, instance_loss: 1.4631, weighted_loss: 1.3858, label: 0, bag_size: 69\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 1, bag_size: 35\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 75\n",
      "batch 319, loss: 0.0004, instance_loss: 0.0016, weighted_loss: 0.0007, label: 0, bag_size: 50\n",
      "batch 339, loss: 0.0074, instance_loss: 0.2304, weighted_loss: 0.0743, label: 1, bag_size: 38\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 0, bag_size: 78\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 79\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0047, weighted_loss: 0.0014, label: 0, bag_size: 108\n",
      "batch 419, loss: 0.0027, instance_loss: 0.0066, weighted_loss: 0.0039, label: 0, bag_size: 72\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0162, weighted_loss: 0.0049, label: 1, bag_size: 96\n",
      "batch 459, loss: 0.0299, instance_loss: 0.0396, weighted_loss: 0.0328, label: 1, bag_size: 31\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 104\n",
      "batch 499, loss: 0.0202, instance_loss: 0.0002, weighted_loss: 0.0142, label: 1, bag_size: 40\n",
      "batch 519, loss: 0.0911, instance_loss: 0.1879, weighted_loss: 0.1201, label: 0, bag_size: 39\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0021, weighted_loss: 0.0006, label: 1, bag_size: 32\n",
      "batch 559, loss: 0.0011, instance_loss: 0.0276, weighted_loss: 0.0090, label: 0, bag_size: 61\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 23\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0078, weighted_loss: 0.0023, label: 1, bag_size: 26\n",
      "batch 619, loss: 0.2187, instance_loss: 3.0176, weighted_loss: 1.0584, label: 0, bag_size: 45\n",
      "batch 639, loss: 0.0005, instance_loss: 0.0012, weighted_loss: 0.0007, label: 0, bag_size: 18\n",
      "batch 659, loss: 0.0007, instance_loss: 0.1097, weighted_loss: 0.0334, label: 1, bag_size: 26\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.985655737704918: correct 10582/10736\n",
      "class 1 clustering acc 0.9336810730253353: correct 5012/5368\n",
      "Epoch: 46, train_loss: 0.1191, train_clustering_loss:  0.1486, train_error: 0.0387\n",
      "class 0: acc 0.963302752293578, correct 315/327\n",
      "class 1: acc 0.9593023255813954, correct 330/344\n",
      "\n",
      "Val Set, val_loss: 0.5893, val_error: 0.1667, auc: 0.9240\n",
      "class 0 clustering acc 0.953125: correct 1281/1344\n",
      "class 1 clustering acc 0.8556547619047619: correct 575/672\n",
      "class 0: acc 0.8604651162790697, correct 37/43\n",
      "class 1: acc 0.8048780487804879, correct 33/41\n",
      "EarlyStopping counter: 17 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 39\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 49\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 55\n",
      "batch 79, loss: 0.0335, instance_loss: 0.0069, weighted_loss: 0.0255, label: 1, bag_size: 35\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0210, weighted_loss: 0.0063, label: 0, bag_size: 28\n",
      "batch 119, loss: 0.1160, instance_loss: 0.4140, weighted_loss: 0.2054, label: 1, bag_size: 62\n",
      "batch 139, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 101\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 29\n",
      "batch 179, loss: 0.0005, instance_loss: 0.0019, weighted_loss: 0.0009, label: 0, bag_size: 78\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 82\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 101\n",
      "batch 239, loss: 0.0189, instance_loss: 0.2648, weighted_loss: 0.0927, label: 0, bag_size: 36\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 57\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0042, weighted_loss: 0.0013, label: 0, bag_size: 90\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 80\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 95\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 111\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 74\n",
      "batch 379, loss: 0.0183, instance_loss: 0.0855, weighted_loss: 0.0385, label: 1, bag_size: 87\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 93\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 72\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 1, bag_size: 84\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 76\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0051, weighted_loss: 0.0016, label: 1, bag_size: 31\n",
      "batch 499, loss: 1.8748, instance_loss: 1.5112, weighted_loss: 1.7658, label: 1, bag_size: 63\n",
      "batch 519, loss: 0.0005, instance_loss: 0.0795, weighted_loss: 0.0242, label: 0, bag_size: 35\n",
      "batch 539, loss: 0.0013, instance_loss: 0.0338, weighted_loss: 0.0110, label: 0, bag_size: 29\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0021, weighted_loss: 0.0007, label: 0, bag_size: 92\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0005, label: 1, bag_size: 28\n",
      "batch 599, loss: 0.0121, instance_loss: 0.4359, weighted_loss: 0.1392, label: 1, bag_size: 92\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 53\n",
      "batch 639, loss: 0.0261, instance_loss: 0.2923, weighted_loss: 0.1059, label: 1, bag_size: 58\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 105\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9849105812220567: correct 10574/10736\n",
      "class 1 clustering acc 0.9370342771982116: correct 5030/5368\n",
      "Epoch: 47, train_loss: 0.1463, train_clustering_loss:  0.1367, train_error: 0.0417\n",
      "class 0: acc 0.9574468085106383, correct 315/329\n",
      "class 1: acc 0.9590643274853801, correct 328/342\n",
      "\n",
      "Val Set, val_loss: 1.1629, val_error: 0.1786, auc: 0.9396\n",
      "class 0 clustering acc 0.9278273809523809: correct 1247/1344\n",
      "class 1 clustering acc 0.8229166666666666: correct 553/672\n",
      "class 0: acc 0.6976744186046512, correct 30/43\n",
      "class 1: acc 0.9512195121951219, correct 39/41\n",
      "EarlyStopping counter: 18 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0867, instance_loss: 0.2388, weighted_loss: 0.1323, label: 0, bag_size: 48\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 65\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0058, weighted_loss: 0.0017, label: 0, bag_size: 49\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 114\n",
      "batch 99, loss: 0.0018, instance_loss: 0.0040, weighted_loss: 0.0025, label: 0, bag_size: 58\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 1, bag_size: 32\n",
      "batch 139, loss: 0.0050, instance_loss: 0.0185, weighted_loss: 0.0091, label: 1, bag_size: 20\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0026, weighted_loss: 0.0008, label: 0, bag_size: 98\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0109, weighted_loss: 0.0033, label: 1, bag_size: 68\n",
      "batch 199, loss: 0.0000, instance_loss: 0.9601, weighted_loss: 0.2880, label: 0, bag_size: 40\n",
      "batch 219, loss: 0.0000, instance_loss: 0.8453, weighted_loss: 0.2536, label: 0, bag_size: 79\n",
      "batch 239, loss: 14.6010, instance_loss: 1.5447, weighted_loss: 10.6841, label: 1, bag_size: 41\n",
      "batch 259, loss: 0.0000, instance_loss: 0.6755, weighted_loss: 0.2027, label: 1, bag_size: 72\n",
      "batch 279, loss: 20.0113, instance_loss: 1.7255, weighted_loss: 14.5255, label: 0, bag_size: 68\n",
      "batch 299, loss: 0.1568, instance_loss: 1.4082, weighted_loss: 0.5322, label: 1, bag_size: 83\n",
      "batch 319, loss: 2.3389, instance_loss: 0.5668, weighted_loss: 1.8073, label: 0, bag_size: 81\n",
      "batch 339, loss: 0.0000, instance_loss: 0.7136, weighted_loss: 0.2141, label: 0, bag_size: 63\n",
      "batch 359, loss: 0.0000, instance_loss: 0.6654, weighted_loss: 0.1996, label: 1, bag_size: 123\n",
      "batch 379, loss: 0.0003, instance_loss: 0.9097, weighted_loss: 0.2732, label: 0, bag_size: 78\n",
      "batch 399, loss: 0.0000, instance_loss: 0.6130, weighted_loss: 0.1839, label: 1, bag_size: 24\n",
      "batch 419, loss: 0.0000, instance_loss: 0.5668, weighted_loss: 0.1701, label: 1, bag_size: 59\n",
      "batch 439, loss: 0.0000, instance_loss: 0.6777, weighted_loss: 0.2033, label: 1, bag_size: 84\n",
      "batch 459, loss: 0.0000, instance_loss: 0.5699, weighted_loss: 0.1710, label: 1, bag_size: 45\n",
      "batch 479, loss: 0.0051, instance_loss: 0.6374, weighted_loss: 0.1948, label: 0, bag_size: 96\n",
      "batch 499, loss: 2.4485, instance_loss: 0.6495, weighted_loss: 1.9088, label: 0, bag_size: 21\n",
      "batch 519, loss: 0.0000, instance_loss: 0.8370, weighted_loss: 0.2511, label: 1, bag_size: 25\n",
      "batch 539, loss: 0.0000, instance_loss: 0.4997, weighted_loss: 0.1499, label: 1, bag_size: 86\n",
      "batch 559, loss: 0.0000, instance_loss: 1.2100, weighted_loss: 0.3630, label: 1, bag_size: 50\n",
      "batch 579, loss: 10.1264, instance_loss: 2.5932, weighted_loss: 7.8664, label: 1, bag_size: 121\n",
      "batch 599, loss: 0.0000, instance_loss: 0.8905, weighted_loss: 0.2672, label: 0, bag_size: 57\n",
      "batch 619, loss: 0.0000, instance_loss: 0.4631, weighted_loss: 0.1389, label: 0, bag_size: 85\n",
      "batch 639, loss: 0.0000, instance_loss: 0.6807, weighted_loss: 0.2042, label: 0, bag_size: 33\n",
      "batch 659, loss: 0.0000, instance_loss: 0.4153, weighted_loss: 0.1246, label: 0, bag_size: 88\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.8844076005961252: correct 9495/10736\n",
      "class 1 clustering acc 0.6011549925484352: correct 3227/5368\n",
      "Epoch: 48, train_loss: 1.3714, train_clustering_loss:  0.6749, train_error: 0.1311\n",
      "class 0: acc 0.8775510204081632, correct 301/343\n",
      "class 1: acc 0.8597560975609756, correct 282/328\n",
      "\n",
      "Val Set, val_loss: 1.3524, val_error: 0.1190, auc: 0.9376\n",
      "class 0 clustering acc 0.9620535714285714: correct 1293/1344\n",
      "class 1 clustering acc 0.625: correct 420/672\n",
      "class 0: acc 0.813953488372093, correct 35/43\n",
      "class 1: acc 0.9512195121951219, correct 39/41\n",
      "EarlyStopping counter: 19 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.3303, weighted_loss: 0.0991, label: 1, bag_size: 24\n",
      "batch 39, loss: 0.0000, instance_loss: 0.4335, weighted_loss: 0.1301, label: 1, bag_size: 14\n",
      "batch 59, loss: 0.0000, instance_loss: 1.1165, weighted_loss: 0.3350, label: 0, bag_size: 99\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0872, weighted_loss: 0.0262, label: 0, bag_size: 58\n",
      "batch 99, loss: 0.0000, instance_loss: 0.6741, weighted_loss: 0.2022, label: 1, bag_size: 90\n",
      "batch 119, loss: 0.0000, instance_loss: 0.5899, weighted_loss: 0.1770, label: 1, bag_size: 75\n",
      "batch 139, loss: 0.0000, instance_loss: 0.1491, weighted_loss: 0.0447, label: 0, bag_size: 65\n",
      "batch 159, loss: 0.0000, instance_loss: 0.5141, weighted_loss: 0.1542, label: 1, bag_size: 126\n",
      "batch 179, loss: 0.0000, instance_loss: 0.4341, weighted_loss: 0.1302, label: 1, bag_size: 81\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0904, weighted_loss: 0.0271, label: 0, bag_size: 109\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0137, weighted_loss: 0.0041, label: 0, bag_size: 75\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0643, weighted_loss: 0.0193, label: 0, bag_size: 27\n",
      "batch 259, loss: 0.0215, instance_loss: 1.0233, weighted_loss: 0.3221, label: 0, bag_size: 24\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0297, weighted_loss: 0.0089, label: 0, bag_size: 110\n",
      "batch 299, loss: 0.0012, instance_loss: 0.4468, weighted_loss: 0.1349, label: 0, bag_size: 78\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0169, weighted_loss: 0.0051, label: 1, bag_size: 89\n",
      "batch 339, loss: 0.0000, instance_loss: 0.1192, weighted_loss: 0.0358, label: 1, bag_size: 60\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0273, weighted_loss: 0.0082, label: 1, bag_size: 73\n",
      "batch 379, loss: 13.0949, instance_loss: 2.6512, weighted_loss: 9.9618, label: 1, bag_size: 96\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0883, weighted_loss: 0.0265, label: 1, bag_size: 71\n",
      "batch 419, loss: 0.0013, instance_loss: 0.3691, weighted_loss: 0.1117, label: 0, bag_size: 118\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0064, weighted_loss: 0.0019, label: 1, bag_size: 69\n",
      "batch 459, loss: 0.0367, instance_loss: 0.0261, weighted_loss: 0.0335, label: 1, bag_size: 47\n",
      "batch 479, loss: 3.4532, instance_loss: 1.4682, weighted_loss: 2.8577, label: 0, bag_size: 73\n",
      "batch 499, loss: 0.0288, instance_loss: 0.3199, weighted_loss: 0.1161, label: 1, bag_size: 24\n",
      "batch 519, loss: 0.1707, instance_loss: 0.4338, weighted_loss: 0.2496, label: 0, bag_size: 94\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0209, weighted_loss: 0.0063, label: 0, bag_size: 103\n",
      "batch 559, loss: 0.0005, instance_loss: 0.0062, weighted_loss: 0.0022, label: 1, bag_size: 68\n",
      "batch 579, loss: 0.0000, instance_loss: 0.7563, weighted_loss: 0.2269, label: 0, bag_size: 65\n",
      "batch 599, loss: 0.0000, instance_loss: 0.4585, weighted_loss: 0.1375, label: 0, bag_size: 67\n",
      "batch 619, loss: 0.0021, instance_loss: 0.3737, weighted_loss: 0.1136, label: 0, bag_size: 101\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0686, weighted_loss: 0.0206, label: 1, bag_size: 30\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0259, weighted_loss: 0.0078, label: 0, bag_size: 53\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9466281669150521: correct 10163/10736\n",
      "class 1 clustering acc 0.8111028315946349: correct 4354/5368\n",
      "Epoch: 49, train_loss: 0.5407, train_clustering_loss:  0.3684, train_error: 0.0864\n",
      "class 0: acc 0.9009584664536742, correct 282/313\n",
      "class 1: acc 0.9245810055865922, correct 331/358\n",
      "\n",
      "Val Set, val_loss: 0.6807, val_error: 0.1190, auc: 0.9467\n",
      "class 0 clustering acc 0.9375: correct 1260/1344\n",
      "class 1 clustering acc 0.8497023809523809: correct 571/672\n",
      "class 0: acc 0.8604651162790697, correct 37/43\n",
      "class 1: acc 0.9024390243902439, correct 37/41\n",
      "EarlyStopping counter: 20 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0804, weighted_loss: 0.0241, label: 0, bag_size: 46\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0112, weighted_loss: 0.0034, label: 0, bag_size: 79\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 33\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0171, weighted_loss: 0.0051, label: 0, bag_size: 29\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0137, weighted_loss: 0.0041, label: 1, bag_size: 43\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0281, weighted_loss: 0.0085, label: 1, bag_size: 31\n",
      "batch 139, loss: 2.1196, instance_loss: 0.0797, weighted_loss: 1.5076, label: 1, bag_size: 69\n",
      "batch 159, loss: 0.0889, instance_loss: 0.6350, weighted_loss: 0.2527, label: 0, bag_size: 28\n",
      "batch 179, loss: 0.0000, instance_loss: 0.1860, weighted_loss: 0.0558, label: 1, bag_size: 42\n",
      "batch 199, loss: 0.0752, instance_loss: 0.0057, weighted_loss: 0.0544, label: 1, bag_size: 48\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0423, weighted_loss: 0.0127, label: 1, bag_size: 109\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0603, weighted_loss: 0.0181, label: 1, bag_size: 31\n",
      "batch 259, loss: 0.0000, instance_loss: 0.3687, weighted_loss: 0.1106, label: 0, bag_size: 28\n",
      "batch 279, loss: 0.0002, instance_loss: 2.7638, weighted_loss: 0.8293, label: 1, bag_size: 58\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0026, weighted_loss: 0.0008, label: 0, bag_size: 38\n",
      "batch 319, loss: 0.0075, instance_loss: 0.1620, weighted_loss: 0.0539, label: 1, bag_size: 115\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0035, weighted_loss: 0.0011, label: 1, bag_size: 107\n",
      "batch 359, loss: 0.0031, instance_loss: 0.0033, weighted_loss: 0.0031, label: 0, bag_size: 19\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 30\n",
      "batch 399, loss: 0.0000, instance_loss: 0.1565, weighted_loss: 0.0469, label: 0, bag_size: 30\n",
      "batch 419, loss: 0.0039, instance_loss: 0.0042, weighted_loss: 0.0040, label: 0, bag_size: 58\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0944, weighted_loss: 0.0283, label: 0, bag_size: 73\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0318, weighted_loss: 0.0095, label: 0, bag_size: 118\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0485, weighted_loss: 0.0145, label: 0, bag_size: 97\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0073, weighted_loss: 0.0022, label: 1, bag_size: 69\n",
      "batch 519, loss: 0.0037, instance_loss: 0.7267, weighted_loss: 0.2206, label: 1, bag_size: 58\n",
      "batch 539, loss: 0.0000, instance_loss: 0.1836, weighted_loss: 0.0551, label: 0, bag_size: 18\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0072, weighted_loss: 0.0022, label: 1, bag_size: 31\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 0, bag_size: 102\n",
      "batch 599, loss: 0.1837, instance_loss: 0.7475, weighted_loss: 0.3528, label: 1, bag_size: 60\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0032, weighted_loss: 0.0010, label: 1, bag_size: 60\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0264, weighted_loss: 0.0079, label: 1, bag_size: 61\n",
      "batch 659, loss: 0.5849, instance_loss: 0.1764, weighted_loss: 0.4623, label: 0, bag_size: 74\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9637667660208644: correct 10347/10736\n",
      "class 1 clustering acc 0.8584202682563339: correct 4608/5368\n",
      "Epoch: 50, train_loss: 0.3304, train_clustering_loss:  0.3206, train_error: 0.0760\n",
      "class 0: acc 0.9285714285714286, correct 325/350\n",
      "class 1: acc 0.9190031152647975, correct 295/321\n",
      "\n",
      "Val Set, val_loss: 0.9520, val_error: 0.1190, auc: 0.9461\n",
      "class 0 clustering acc 0.8050595238095238: correct 1082/1344\n",
      "class 1 clustering acc 0.6502976190476191: correct 437/672\n",
      "class 0: acc 0.9069767441860465, correct 39/43\n",
      "class 1: acc 0.8536585365853658, correct 35/41\n",
      "EarlyStopping counter: 21 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 2.5615, instance_loss: 1.1461, weighted_loss: 2.1369, label: 0, bag_size: 77\n",
      "batch 39, loss: 0.0003, instance_loss: 3.6618, weighted_loss: 1.0988, label: 0, bag_size: 81\n",
      "batch 59, loss: 0.0000, instance_loss: 0.7268, weighted_loss: 0.2180, label: 0, bag_size: 27\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0382, weighted_loss: 0.0115, label: 1, bag_size: 99\n",
      "batch 99, loss: 0.0558, instance_loss: 0.6169, weighted_loss: 0.2242, label: 0, bag_size: 45\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0201, weighted_loss: 0.0060, label: 1, bag_size: 69\n",
      "batch 139, loss: 0.0000, instance_loss: 0.1169, weighted_loss: 0.0351, label: 1, bag_size: 51\n",
      "batch 159, loss: 0.0021, instance_loss: 2.1607, weighted_loss: 0.6497, label: 0, bag_size: 81\n",
      "batch 179, loss: 0.0000, instance_loss: 0.9584, weighted_loss: 0.2875, label: 0, bag_size: 33\n",
      "batch 199, loss: 0.0000, instance_loss: 0.2584, weighted_loss: 0.0775, label: 0, bag_size: 87\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0705, weighted_loss: 0.0211, label: 1, bag_size: 84\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 1, bag_size: 95\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0494, weighted_loss: 0.0148, label: 1, bag_size: 109\n",
      "batch 279, loss: 8.2108, instance_loss: 2.7104, weighted_loss: 6.5607, label: 1, bag_size: 43\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0671, weighted_loss: 0.0201, label: 0, bag_size: 72\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0111, weighted_loss: 0.0033, label: 1, bag_size: 78\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0137, weighted_loss: 0.0041, label: 0, bag_size: 97\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0043, weighted_loss: 0.0013, label: 1, bag_size: 29\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0046, weighted_loss: 0.0014, label: 1, bag_size: 126\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0040, weighted_loss: 0.0012, label: 1, bag_size: 86\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 86\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0127, weighted_loss: 0.0038, label: 1, bag_size: 85\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0517, weighted_loss: 0.0156, label: 0, bag_size: 27\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 0, bag_size: 91\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 0, bag_size: 113\n",
      "batch 519, loss: 0.0204, instance_loss: 0.4254, weighted_loss: 0.1419, label: 1, bag_size: 121\n",
      "batch 539, loss: 0.0000, instance_loss: 0.7390, weighted_loss: 0.2217, label: 1, bag_size: 13\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0052, weighted_loss: 0.0015, label: 0, bag_size: 55\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0065, weighted_loss: 0.0020, label: 0, bag_size: 79\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0304, weighted_loss: 0.0091, label: 1, bag_size: 51\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0069, weighted_loss: 0.0021, label: 1, bag_size: 81\n",
      "batch 639, loss: 0.6106, instance_loss: 0.1585, weighted_loss: 0.4750, label: 1, bag_size: 98\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0348, weighted_loss: 0.0104, label: 1, bag_size: 18\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9718703427719821: correct 10434/10736\n",
      "class 1 clustering acc 0.8710879284649776: correct 4676/5368\n",
      "Epoch: 51, train_loss: 0.2774, train_clustering_loss:  0.3001, train_error: 0.0447\n",
      "class 0: acc 0.9535603715170279, correct 308/323\n",
      "class 1: acc 0.9568965517241379, correct 333/348\n",
      "\n",
      "Val Set, val_loss: 0.8769, val_error: 0.0952, auc: 0.9436\n",
      "class 0 clustering acc 0.9516369047619048: correct 1279/1344\n",
      "class 1 clustering acc 0.8511904761904762: correct 572/672\n",
      "class 0: acc 0.8604651162790697, correct 37/43\n",
      "class 1: acc 0.9512195121951219, correct 39/41\n",
      "EarlyStopping counter: 22 out of 20\n",
      "Early stopping\n",
      "Val error: 0.1190, ROC AUC: 0.9461\n",
      "Test error: 0.1644, ROC AUC: 0.9379\n",
      "class 0: acc 0.8787878787878788, correct 29/33\n",
      "class 1: acc 0.8, correct 32/40\n",
      "\n",
      "Training Fold 2!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 660 samples\n",
      "Validating on 84 samples\n",
      "Testing on 84 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Setting tau to 1.0\n",
      "Done!\n",
      "MCBAT_SB(\n",
      "  (instance_loss_fn): SmoothTop1SVM()\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (transformer_low): TransformerEncoder_FLASHAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FlashAttention(\n",
      "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
      "            (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer_high): TransformerEncoder_FLASHAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FlashAttention(\n",
      "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
      "            (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attention): Attn_Net_Gated(\n",
      "    (attention_a): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_b): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (attention_V2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (attention_U2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (attention_weights2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Total number of parameters: 8408073\n",
      "Total number of trainable parameters: 8408073\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "660.0\n",
      "2\n",
      "321\n",
      "339\n",
      "##################################################\n",
      "Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 6.0311, instance_loss: 1.4234, weighted_loss: 4.6488, label: 0, bag_size: 40\n",
      "batch 39, loss: 0.1567, instance_loss: 0.6978, weighted_loss: 0.3190, label: 0, bag_size: 41\n",
      "batch 59, loss: 2.5152, instance_loss: 0.8968, weighted_loss: 2.0297, label: 1, bag_size: 53\n",
      "batch 79, loss: 0.0812, instance_loss: 1.0802, weighted_loss: 0.3809, label: 1, bag_size: 83\n",
      "batch 99, loss: 2.9236, instance_loss: 0.9834, weighted_loss: 2.3415, label: 0, bag_size: 28\n",
      "batch 119, loss: 4.7634, instance_loss: 1.0679, weighted_loss: 3.6548, label: 0, bag_size: 35\n",
      "batch 139, loss: 0.4600, instance_loss: 1.1400, weighted_loss: 0.6640, label: 0, bag_size: 86\n",
      "batch 159, loss: 2.8882, instance_loss: 1.4679, weighted_loss: 2.4621, label: 0, bag_size: 101\n",
      "batch 179, loss: 1.7260, instance_loss: 1.5429, weighted_loss: 1.6710, label: 0, bag_size: 28\n",
      "batch 199, loss: 0.0874, instance_loss: 1.0007, weighted_loss: 0.3614, label: 1, bag_size: 39\n",
      "batch 219, loss: 2.0893, instance_loss: 1.1031, weighted_loss: 1.7935, label: 0, bag_size: 132\n",
      "batch 239, loss: 0.5491, instance_loss: 1.7621, weighted_loss: 0.9130, label: 1, bag_size: 77\n",
      "batch 259, loss: 0.4106, instance_loss: 0.7594, weighted_loss: 0.5152, label: 1, bag_size: 23\n",
      "batch 279, loss: 0.0676, instance_loss: 0.8279, weighted_loss: 0.2957, label: 1, bag_size: 69\n",
      "batch 299, loss: 0.5864, instance_loss: 1.3435, weighted_loss: 0.8135, label: 0, bag_size: 32\n",
      "batch 319, loss: 1.2192, instance_loss: 0.7730, weighted_loss: 1.0853, label: 1, bag_size: 51\n",
      "batch 339, loss: 1.9179, instance_loss: 0.8756, weighted_loss: 1.6052, label: 0, bag_size: 47\n",
      "batch 359, loss: 0.9156, instance_loss: 1.2472, weighted_loss: 1.0151, label: 1, bag_size: 92\n",
      "batch 379, loss: 0.4937, instance_loss: 0.8216, weighted_loss: 0.5921, label: 1, bag_size: 51\n",
      "batch 399, loss: 2.6381, instance_loss: 0.7190, weighted_loss: 2.0624, label: 0, bag_size: 28\n",
      "batch 419, loss: 0.5917, instance_loss: 0.9190, weighted_loss: 0.6899, label: 0, bag_size: 77\n",
      "batch 439, loss: 0.2808, instance_loss: 0.7570, weighted_loss: 0.4237, label: 0, bag_size: 30\n",
      "batch 459, loss: 0.2840, instance_loss: 0.7854, weighted_loss: 0.4344, label: 1, bag_size: 17\n",
      "batch 479, loss: 2.9108, instance_loss: 1.7998, weighted_loss: 2.5775, label: 0, bag_size: 36\n",
      "batch 499, loss: 0.1819, instance_loss: 1.3693, weighted_loss: 0.5382, label: 1, bag_size: 99\n",
      "batch 519, loss: 0.6632, instance_loss: 1.0544, weighted_loss: 0.7805, label: 1, bag_size: 111\n",
      "batch 539, loss: 1.6502, instance_loss: 0.7354, weighted_loss: 1.3758, label: 1, bag_size: 44\n",
      "batch 559, loss: 0.1009, instance_loss: 1.0171, weighted_loss: 0.3758, label: 0, bag_size: 96\n",
      "batch 579, loss: 2.6034, instance_loss: 0.9491, weighted_loss: 2.1071, label: 1, bag_size: 63\n",
      "batch 599, loss: 1.7965, instance_loss: 1.3244, weighted_loss: 1.6549, label: 1, bag_size: 107\n",
      "batch 619, loss: 2.5649, instance_loss: 1.3131, weighted_loss: 2.1893, label: 1, bag_size: 88\n",
      "batch 639, loss: 0.6046, instance_loss: 1.0324, weighted_loss: 0.7329, label: 0, bag_size: 102\n",
      "batch 659, loss: 0.8220, instance_loss: 0.9090, weighted_loss: 0.8481, label: 0, bag_size: 38\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9775568181818182: correct 10323/10560\n",
      "class 1 clustering acc 0.04261363636363636: correct 225/5280\n",
      "Epoch: 0, train_loss: 1.0514, train_clustering_loss:  1.0097, train_error: 0.4258\n",
      "class 0: acc 0.5792682926829268, correct 190/328\n",
      "class 1: acc 0.5692771084337349, correct 189/332\n",
      "\n",
      "Val Set, val_loss: 1.2100, val_error: 0.4524, auc: 0.7385\n",
      "class 0 clustering acc 0.9970238095238095: correct 1340/1344\n",
      "class 1 clustering acc 0.03273809523809524: correct 22/672\n",
      "class 0: acc 0.02564102564102564, correct 1/39\n",
      "class 1: acc 1.0, correct 45/45\n",
      "Validation loss decreased (inf --> 1.209983).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7414, instance_loss: 0.9473, weighted_loss: 0.8031, label: 1, bag_size: 112\n",
      "batch 39, loss: 0.0150, instance_loss: 0.7916, weighted_loss: 0.2480, label: 1, bag_size: 76\n",
      "batch 59, loss: 0.8141, instance_loss: 0.7012, weighted_loss: 0.7802, label: 0, bag_size: 56\n",
      "batch 79, loss: 0.3756, instance_loss: 0.8714, weighted_loss: 0.5243, label: 0, bag_size: 28\n",
      "batch 99, loss: 0.1046, instance_loss: 1.0440, weighted_loss: 0.3864, label: 0, bag_size: 126\n",
      "batch 119, loss: 0.4100, instance_loss: 0.9224, weighted_loss: 0.5637, label: 1, bag_size: 65\n",
      "batch 139, loss: 0.4635, instance_loss: 1.0620, weighted_loss: 0.6430, label: 1, bag_size: 64\n",
      "batch 159, loss: 0.1546, instance_loss: 1.5583, weighted_loss: 0.5757, label: 1, bag_size: 38\n",
      "batch 179, loss: 0.2780, instance_loss: 0.6094, weighted_loss: 0.3774, label: 1, bag_size: 68\n",
      "batch 199, loss: 0.1947, instance_loss: 0.8927, weighted_loss: 0.4041, label: 1, bag_size: 98\n",
      "batch 219, loss: 0.0368, instance_loss: 0.7747, weighted_loss: 0.2582, label: 0, bag_size: 49\n",
      "batch 239, loss: 2.3393, instance_loss: 0.8007, weighted_loss: 1.8777, label: 0, bag_size: 93\n",
      "batch 259, loss: 1.4766, instance_loss: 1.4580, weighted_loss: 1.4710, label: 1, bag_size: 78\n",
      "batch 279, loss: 0.2550, instance_loss: 0.8852, weighted_loss: 0.4441, label: 0, bag_size: 42\n",
      "batch 299, loss: 0.0110, instance_loss: 0.6687, weighted_loss: 0.2083, label: 0, bag_size: 64\n",
      "batch 319, loss: 0.8266, instance_loss: 1.0837, weighted_loss: 0.9037, label: 0, bag_size: 48\n",
      "batch 339, loss: 0.0362, instance_loss: 1.2983, weighted_loss: 0.4148, label: 1, bag_size: 30\n",
      "batch 359, loss: 0.1496, instance_loss: 0.7519, weighted_loss: 0.3303, label: 1, bag_size: 72\n",
      "batch 379, loss: 0.6887, instance_loss: 1.0136, weighted_loss: 0.7861, label: 1, bag_size: 76\n",
      "batch 399, loss: 0.0352, instance_loss: 0.9547, weighted_loss: 0.3110, label: 0, bag_size: 65\n",
      "batch 419, loss: 0.0003, instance_loss: 0.6898, weighted_loss: 0.2071, label: 0, bag_size: 87\n",
      "batch 439, loss: 0.0183, instance_loss: 1.0190, weighted_loss: 0.3185, label: 0, bag_size: 90\n",
      "batch 459, loss: 0.0001, instance_loss: 0.7720, weighted_loss: 0.2317, label: 0, bag_size: 40\n",
      "batch 479, loss: 1.1057, instance_loss: 0.8774, weighted_loss: 1.0372, label: 1, bag_size: 41\n",
      "batch 499, loss: 0.6138, instance_loss: 0.8717, weighted_loss: 0.6912, label: 1, bag_size: 24\n",
      "batch 519, loss: 0.0036, instance_loss: 0.6713, weighted_loss: 0.2039, label: 1, bag_size: 56\n",
      "batch 539, loss: 0.3879, instance_loss: 0.6844, weighted_loss: 0.4768, label: 1, bag_size: 121\n",
      "batch 559, loss: 1.2626, instance_loss: 1.0183, weighted_loss: 1.1893, label: 0, bag_size: 48\n",
      "batch 579, loss: 1.3887, instance_loss: 1.0433, weighted_loss: 1.2851, label: 0, bag_size: 67\n",
      "batch 599, loss: 4.7034, instance_loss: 1.6960, weighted_loss: 3.8012, label: 0, bag_size: 40\n",
      "batch 619, loss: 0.0977, instance_loss: 0.6509, weighted_loss: 0.2636, label: 1, bag_size: 44\n",
      "batch 639, loss: 0.1090, instance_loss: 0.6482, weighted_loss: 0.2708, label: 0, bag_size: 31\n",
      "batch 659, loss: 0.9950, instance_loss: 1.0740, weighted_loss: 1.0187, label: 0, bag_size: 55\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9733901515151515: correct 10279/10560\n",
      "class 1 clustering acc 0.07537878787878788: correct 398/5280\n",
      "Epoch: 1, train_loss: 0.7420, train_clustering_loss:  0.9552, train_error: 0.3439\n",
      "class 0: acc 0.6417445482866043, correct 206/321\n",
      "class 1: acc 0.6696165191740413, correct 227/339\n",
      "\n",
      "Val Set, val_loss: 0.6609, val_error: 0.3690, auc: 0.8011\n",
      "class 0 clustering acc 0.9955357142857143: correct 1338/1344\n",
      "class 1 clustering acc 0.03273809523809524: correct 22/672\n",
      "class 0: acc 0.28205128205128205, correct 11/39\n",
      "class 1: acc 0.9333333333333333, correct 42/45\n",
      "Validation loss decreased (1.209983 --> 0.660862).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 1.3895, instance_loss: 0.9374, weighted_loss: 1.2539, label: 1, bag_size: 84\n",
      "batch 39, loss: 0.0260, instance_loss: 0.6930, weighted_loss: 0.2261, label: 0, bag_size: 81\n",
      "batch 59, loss: 0.2697, instance_loss: 0.7899, weighted_loss: 0.4258, label: 1, bag_size: 61\n",
      "batch 79, loss: 3.4043, instance_loss: 1.1396, weighted_loss: 2.7249, label: 0, bag_size: 35\n",
      "batch 99, loss: 4.2671, instance_loss: 0.6416, weighted_loss: 3.1795, label: 1, bag_size: 69\n",
      "batch 119, loss: 1.0788, instance_loss: 1.3717, weighted_loss: 1.1666, label: 1, bag_size: 76\n",
      "batch 139, loss: 0.8884, instance_loss: 0.9996, weighted_loss: 0.9218, label: 0, bag_size: 89\n",
      "batch 159, loss: 0.0661, instance_loss: 0.5712, weighted_loss: 0.2176, label: 1, bag_size: 91\n",
      "batch 179, loss: 0.3460, instance_loss: 0.8565, weighted_loss: 0.4991, label: 0, bag_size: 31\n",
      "batch 199, loss: 1.0645, instance_loss: 1.4703, weighted_loss: 1.1862, label: 1, bag_size: 30\n",
      "batch 219, loss: 0.1024, instance_loss: 0.6192, weighted_loss: 0.2574, label: 0, bag_size: 101\n",
      "batch 239, loss: 0.0004, instance_loss: 0.5146, weighted_loss: 0.1547, label: 1, bag_size: 99\n",
      "batch 259, loss: 0.0383, instance_loss: 0.6175, weighted_loss: 0.2120, label: 0, bag_size: 51\n",
      "batch 279, loss: 1.8602, instance_loss: 1.3025, weighted_loss: 1.6929, label: 0, bag_size: 95\n",
      "batch 299, loss: 0.6836, instance_loss: 1.0356, weighted_loss: 0.7892, label: 1, bag_size: 100\n",
      "batch 319, loss: 1.0287, instance_loss: 1.0195, weighted_loss: 1.0260, label: 1, bag_size: 60\n",
      "batch 339, loss: 1.3617, instance_loss: 0.7644, weighted_loss: 1.1825, label: 0, bag_size: 96\n",
      "batch 359, loss: 0.2786, instance_loss: 0.7438, weighted_loss: 0.4182, label: 1, bag_size: 77\n",
      "batch 379, loss: 0.4341, instance_loss: 0.7900, weighted_loss: 0.5409, label: 0, bag_size: 91\n",
      "batch 399, loss: 1.5572, instance_loss: 1.3301, weighted_loss: 1.4890, label: 0, bag_size: 62\n",
      "batch 419, loss: 0.0392, instance_loss: 0.8523, weighted_loss: 0.2831, label: 1, bag_size: 31\n",
      "batch 439, loss: 3.2134, instance_loss: 1.0465, weighted_loss: 2.5633, label: 1, bag_size: 24\n",
      "batch 459, loss: 0.9822, instance_loss: 1.1133, weighted_loss: 1.0215, label: 0, bag_size: 69\n",
      "batch 479, loss: 1.6321, instance_loss: 1.2394, weighted_loss: 1.5143, label: 0, bag_size: 36\n",
      "batch 499, loss: 0.0063, instance_loss: 0.7216, weighted_loss: 0.2209, label: 1, bag_size: 75\n",
      "batch 519, loss: 0.0087, instance_loss: 0.6512, weighted_loss: 0.2015, label: 1, bag_size: 88\n",
      "batch 539, loss: 0.3361, instance_loss: 0.8442, weighted_loss: 0.4885, label: 0, bag_size: 106\n",
      "batch 559, loss: 0.0021, instance_loss: 0.6054, weighted_loss: 0.1831, label: 0, bag_size: 26\n",
      "batch 579, loss: 4.0551, instance_loss: 1.6416, weighted_loss: 3.3310, label: 0, bag_size: 66\n",
      "batch 599, loss: 0.0692, instance_loss: 0.6284, weighted_loss: 0.2369, label: 1, bag_size: 47\n",
      "batch 619, loss: 2.1299, instance_loss: 0.9245, weighted_loss: 1.7683, label: 1, bag_size: 28\n",
      "batch 639, loss: 1.4664, instance_loss: 0.6618, weighted_loss: 1.2250, label: 0, bag_size: 35\n",
      "batch 659, loss: 2.0540, instance_loss: 1.4486, weighted_loss: 1.8724, label: 0, bag_size: 79\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9631628787878788: correct 10171/10560\n",
      "class 1 clustering acc 0.14261363636363636: correct 753/5280\n",
      "Epoch: 2, train_loss: 0.7288, train_clustering_loss:  0.9122, train_error: 0.3242\n",
      "class 0: acc 0.6476190476190476, correct 204/315\n",
      "class 1: acc 0.7014492753623188, correct 242/345\n",
      "\n",
      "Val Set, val_loss: 0.6397, val_error: 0.3095, auc: 0.7966\n",
      "class 0 clustering acc 1.0: correct 1344/1344\n",
      "class 1 clustering acc 0.0: correct 0/672\n",
      "class 0: acc 0.4358974358974359, correct 17/39\n",
      "class 1: acc 0.9111111111111111, correct 41/45\n",
      "Validation loss decreased (0.660862 --> 0.639660).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0511, instance_loss: 0.7087, weighted_loss: 0.2484, label: 1, bag_size: 69\n",
      "batch 39, loss: 0.2711, instance_loss: 0.8942, weighted_loss: 0.4580, label: 1, bag_size: 84\n",
      "batch 59, loss: 0.0369, instance_loss: 0.5473, weighted_loss: 0.1900, label: 1, bag_size: 95\n",
      "batch 79, loss: 0.0023, instance_loss: 0.4489, weighted_loss: 0.1363, label: 1, bag_size: 41\n",
      "batch 99, loss: 0.0180, instance_loss: 0.6294, weighted_loss: 0.2014, label: 0, bag_size: 46\n",
      "batch 119, loss: 0.0402, instance_loss: 0.5133, weighted_loss: 0.1821, label: 1, bag_size: 96\n",
      "batch 139, loss: 0.8591, instance_loss: 0.7780, weighted_loss: 0.8348, label: 0, bag_size: 96\n",
      "batch 159, loss: 0.0539, instance_loss: 0.5170, weighted_loss: 0.1928, label: 1, bag_size: 136\n",
      "batch 179, loss: 0.2235, instance_loss: 0.8088, weighted_loss: 0.3991, label: 0, bag_size: 36\n",
      "batch 199, loss: 0.0289, instance_loss: 0.5607, weighted_loss: 0.1884, label: 0, bag_size: 65\n",
      "batch 219, loss: 2.1728, instance_loss: 1.2832, weighted_loss: 1.9059, label: 0, bag_size: 102\n",
      "batch 239, loss: 0.3920, instance_loss: 1.4750, weighted_loss: 0.7169, label: 1, bag_size: 22\n",
      "batch 259, loss: 0.0364, instance_loss: 0.4821, weighted_loss: 0.1701, label: 1, bag_size: 76\n",
      "batch 279, loss: 0.3476, instance_loss: 0.7762, weighted_loss: 0.4762, label: 1, bag_size: 76\n",
      "batch 299, loss: 0.3546, instance_loss: 1.1882, weighted_loss: 0.6047, label: 1, bag_size: 64\n",
      "batch 319, loss: 0.0021, instance_loss: 0.6361, weighted_loss: 0.1923, label: 1, bag_size: 87\n",
      "batch 339, loss: 0.3872, instance_loss: 0.5400, weighted_loss: 0.4331, label: 0, bag_size: 70\n",
      "batch 359, loss: 0.0563, instance_loss: 0.8641, weighted_loss: 0.2986, label: 1, bag_size: 20\n",
      "batch 379, loss: 0.1237, instance_loss: 0.3798, weighted_loss: 0.2005, label: 0, bag_size: 28\n",
      "batch 399, loss: 0.5069, instance_loss: 1.4339, weighted_loss: 0.7850, label: 0, bag_size: 18\n",
      "batch 419, loss: 0.2621, instance_loss: 0.4011, weighted_loss: 0.3038, label: 0, bag_size: 88\n",
      "batch 439, loss: 1.6369, instance_loss: 1.3385, weighted_loss: 1.5474, label: 1, bag_size: 53\n",
      "batch 459, loss: 0.0483, instance_loss: 0.3488, weighted_loss: 0.1385, label: 0, bag_size: 75\n",
      "batch 479, loss: 1.0200, instance_loss: 1.3453, weighted_loss: 1.1176, label: 1, bag_size: 104\n",
      "batch 499, loss: 0.0022, instance_loss: 0.0801, weighted_loss: 0.0256, label: 0, bag_size: 36\n",
      "batch 519, loss: 0.6625, instance_loss: 0.9271, weighted_loss: 0.7418, label: 0, bag_size: 13\n",
      "batch 539, loss: 0.6206, instance_loss: 0.8673, weighted_loss: 0.6946, label: 1, bag_size: 53\n",
      "batch 559, loss: 0.3394, instance_loss: 1.2543, weighted_loss: 0.6138, label: 1, bag_size: 96\n",
      "batch 579, loss: 0.2873, instance_loss: 0.8229, weighted_loss: 0.4480, label: 1, bag_size: 23\n",
      "batch 599, loss: 0.1232, instance_loss: 0.5618, weighted_loss: 0.2548, label: 0, bag_size: 65\n",
      "batch 619, loss: 1.2627, instance_loss: 0.9453, weighted_loss: 1.1675, label: 1, bag_size: 102\n",
      "batch 639, loss: 0.0565, instance_loss: 0.8985, weighted_loss: 0.3091, label: 1, bag_size: 94\n",
      "batch 659, loss: 0.0880, instance_loss: 0.7967, weighted_loss: 0.3007, label: 0, bag_size: 25\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9521780303030303: correct 10055/10560\n",
      "class 1 clustering acc 0.3234848484848485: correct 1708/5280\n",
      "Epoch: 3, train_loss: 0.5516, train_clustering_loss:  0.8046, train_error: 0.2364\n",
      "class 0: acc 0.7613293051359517, correct 252/331\n",
      "class 1: acc 0.7659574468085106, correct 252/329\n",
      "\n",
      "Val Set, val_loss: 0.6823, val_error: 0.3690, auc: 0.8838\n",
      "class 0 clustering acc 0.9449404761904762: correct 1270/1344\n",
      "class 1 clustering acc 0.26785714285714285: correct 180/672\n",
      "class 0: acc 1.0, correct 39/39\n",
      "class 1: acc 0.3111111111111111, correct 14/45\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0323, instance_loss: 0.0867, weighted_loss: 0.0486, label: 0, bag_size: 35\n",
      "batch 39, loss: 0.0419, instance_loss: 0.3508, weighted_loss: 0.1346, label: 0, bag_size: 38\n",
      "batch 59, loss: 0.0740, instance_loss: 0.1623, weighted_loss: 0.1005, label: 0, bag_size: 30\n",
      "batch 79, loss: 0.0083, instance_loss: 0.1551, weighted_loss: 0.0523, label: 0, bag_size: 75\n",
      "batch 99, loss: 0.0055, instance_loss: 0.3078, weighted_loss: 0.0962, label: 0, bag_size: 77\n",
      "batch 119, loss: 0.0043, instance_loss: 0.2978, weighted_loss: 0.0923, label: 1, bag_size: 26\n",
      "batch 139, loss: 2.9529, instance_loss: 1.0380, weighted_loss: 2.3784, label: 0, bag_size: 23\n",
      "batch 159, loss: 0.0719, instance_loss: 0.7335, weighted_loss: 0.2704, label: 1, bag_size: 82\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0949, weighted_loss: 0.0285, label: 0, bag_size: 79\n",
      "batch 199, loss: 0.3087, instance_loss: 0.6010, weighted_loss: 0.3964, label: 0, bag_size: 88\n",
      "batch 219, loss: 0.3010, instance_loss: 1.2645, weighted_loss: 0.5900, label: 1, bag_size: 24\n",
      "batch 239, loss: 0.8918, instance_loss: 1.2364, weighted_loss: 0.9952, label: 1, bag_size: 87\n",
      "batch 259, loss: 0.0284, instance_loss: 0.1449, weighted_loss: 0.0634, label: 0, bag_size: 35\n",
      "batch 279, loss: 0.1228, instance_loss: 0.9275, weighted_loss: 0.3642, label: 1, bag_size: 127\n",
      "batch 299, loss: 0.0057, instance_loss: 0.1647, weighted_loss: 0.0534, label: 0, bag_size: 113\n",
      "batch 319, loss: 0.4968, instance_loss: 0.2313, weighted_loss: 0.4172, label: 0, bag_size: 88\n",
      "batch 339, loss: 1.0290, instance_loss: 1.4759, weighted_loss: 1.1631, label: 1, bag_size: 94\n",
      "batch 359, loss: 0.0064, instance_loss: 0.1931, weighted_loss: 0.0624, label: 1, bag_size: 82\n",
      "batch 379, loss: 0.0009, instance_loss: 0.3153, weighted_loss: 0.0952, label: 1, bag_size: 30\n",
      "batch 399, loss: 0.0109, instance_loss: 0.1096, weighted_loss: 0.0405, label: 1, bag_size: 77\n",
      "batch 419, loss: 0.0001, instance_loss: 0.0751, weighted_loss: 0.0226, label: 0, bag_size: 67\n",
      "batch 439, loss: 0.4328, instance_loss: 0.5689, weighted_loss: 0.4736, label: 1, bag_size: 48\n",
      "batch 459, loss: 0.1311, instance_loss: 0.2628, weighted_loss: 0.1706, label: 1, bag_size: 24\n",
      "batch 479, loss: 0.0099, instance_loss: 0.0340, weighted_loss: 0.0171, label: 0, bag_size: 96\n",
      "batch 499, loss: 1.3565, instance_loss: 2.2083, weighted_loss: 1.6121, label: 0, bag_size: 99\n",
      "batch 519, loss: 0.1222, instance_loss: 0.2380, weighted_loss: 0.1570, label: 0, bag_size: 83\n",
      "batch 539, loss: 0.0026, instance_loss: 0.4230, weighted_loss: 0.1287, label: 1, bag_size: 58\n",
      "batch 559, loss: 3.7726, instance_loss: 1.5256, weighted_loss: 3.0985, label: 0, bag_size: 77\n",
      "batch 579, loss: 0.0277, instance_loss: 0.4391, weighted_loss: 0.1511, label: 0, bag_size: 88\n",
      "batch 599, loss: 1.3875, instance_loss: 1.5899, weighted_loss: 1.4482, label: 0, bag_size: 69\n",
      "batch 619, loss: 0.0291, instance_loss: 0.1821, weighted_loss: 0.0750, label: 0, bag_size: 37\n",
      "batch 639, loss: 0.0043, instance_loss: 0.1068, weighted_loss: 0.0351, label: 1, bag_size: 89\n",
      "batch 659, loss: 0.5234, instance_loss: 0.8439, weighted_loss: 0.6196, label: 1, bag_size: 56\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9464962121212122: correct 9995/10560\n",
      "class 1 clustering acc 0.5604166666666667: correct 2959/5280\n",
      "Epoch: 4, train_loss: 0.5137, train_clustering_loss:  0.6359, train_error: 0.2045\n",
      "class 0: acc 0.765079365079365, correct 241/315\n",
      "class 1: acc 0.8231884057971014, correct 284/345\n",
      "\n",
      "Val Set, val_loss: 1.3531, val_error: 0.4524, auc: 0.8758\n",
      "class 0 clustering acc 0.7254464285714286: correct 975/1344\n",
      "class 1 clustering acc 0.4375: correct 294/672\n",
      "class 0: acc 1.0, correct 39/39\n",
      "class 1: acc 0.15555555555555556, correct 7/45\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 5.2750, instance_loss: 3.1162, weighted_loss: 4.6273, label: 0, bag_size: 31\n",
      "batch 39, loss: 0.7038, instance_loss: 1.2826, weighted_loss: 0.8774, label: 1, bag_size: 24\n",
      "batch 59, loss: 0.0136, instance_loss: 0.3468, weighted_loss: 0.1135, label: 1, bag_size: 82\n",
      "batch 79, loss: 0.0072, instance_loss: 0.0150, weighted_loss: 0.0095, label: 0, bag_size: 81\n",
      "batch 99, loss: 0.0521, instance_loss: 0.2337, weighted_loss: 0.1066, label: 0, bag_size: 48\n",
      "batch 119, loss: 0.3005, instance_loss: 0.7392, weighted_loss: 0.4321, label: 0, bag_size: 58\n",
      "batch 139, loss: 0.9996, instance_loss: 1.0665, weighted_loss: 1.0196, label: 0, bag_size: 39\n",
      "batch 159, loss: 0.0048, instance_loss: 0.0521, weighted_loss: 0.0190, label: 0, bag_size: 35\n",
      "batch 179, loss: 0.4610, instance_loss: 1.3761, weighted_loss: 0.7355, label: 0, bag_size: 78\n",
      "batch 199, loss: 0.0431, instance_loss: 0.2798, weighted_loss: 0.1141, label: 0, bag_size: 28\n",
      "batch 219, loss: 0.0050, instance_loss: 0.0450, weighted_loss: 0.0170, label: 0, bag_size: 95\n",
      "batch 239, loss: 5.9617, instance_loss: 1.1278, weighted_loss: 4.5116, label: 0, bag_size: 68\n",
      "batch 259, loss: 0.1949, instance_loss: 0.5158, weighted_loss: 0.2912, label: 1, bag_size: 19\n",
      "batch 279, loss: 1.9908, instance_loss: 0.0924, weighted_loss: 1.4213, label: 1, bag_size: 122\n",
      "batch 299, loss: 0.0086, instance_loss: 0.3072, weighted_loss: 0.0982, label: 0, bag_size: 13\n",
      "batch 319, loss: 0.0855, instance_loss: 0.4772, weighted_loss: 0.2030, label: 1, bag_size: 22\n",
      "batch 339, loss: 0.0580, instance_loss: 0.4740, weighted_loss: 0.1828, label: 1, bag_size: 80\n",
      "batch 359, loss: 0.0226, instance_loss: 0.0668, weighted_loss: 0.0358, label: 0, bag_size: 116\n",
      "batch 379, loss: 0.0146, instance_loss: 0.3634, weighted_loss: 0.1193, label: 0, bag_size: 88\n",
      "batch 399, loss: 0.7008, instance_loss: 0.4005, weighted_loss: 0.6107, label: 1, bag_size: 47\n",
      "batch 419, loss: 0.2614, instance_loss: 0.5960, weighted_loss: 0.3618, label: 1, bag_size: 33\n",
      "batch 439, loss: 0.0028, instance_loss: 0.4650, weighted_loss: 0.1414, label: 1, bag_size: 14\n",
      "batch 459, loss: 0.2554, instance_loss: 0.6172, weighted_loss: 0.3639, label: 0, bag_size: 78\n",
      "batch 479, loss: 1.6777, instance_loss: 0.7924, weighted_loss: 1.4121, label: 1, bag_size: 51\n",
      "batch 499, loss: 0.0423, instance_loss: 0.9018, weighted_loss: 0.3002, label: 1, bag_size: 24\n",
      "batch 519, loss: 0.0265, instance_loss: 0.0846, weighted_loss: 0.0439, label: 0, bag_size: 25\n",
      "batch 539, loss: 1.4676, instance_loss: 1.8083, weighted_loss: 1.5698, label: 0, bag_size: 66\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0618, weighted_loss: 0.0186, label: 0, bag_size: 57\n",
      "batch 579, loss: 0.0000, instance_loss: 0.1308, weighted_loss: 0.0393, label: 1, bag_size: 107\n",
      "batch 599, loss: 0.0184, instance_loss: 0.3193, weighted_loss: 0.1087, label: 1, bag_size: 72\n",
      "batch 619, loss: 1.3025, instance_loss: 1.7476, weighted_loss: 1.4360, label: 0, bag_size: 44\n",
      "batch 639, loss: 0.0014, instance_loss: 0.0991, weighted_loss: 0.0307, label: 0, bag_size: 50\n",
      "batch 659, loss: 0.0186, instance_loss: 0.4818, weighted_loss: 0.1575, label: 1, bag_size: 88\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9487689393939394: correct 10019/10560\n",
      "class 1 clustering acc 0.5606060606060606: correct 2960/5280\n",
      "Epoch: 5, train_loss: 0.5623, train_clustering_loss:  0.6496, train_error: 0.2091\n",
      "class 0: acc 0.7763578274760383, correct 243/313\n",
      "class 1: acc 0.8040345821325648, correct 279/347\n",
      "\n",
      "Val Set, val_loss: 0.5537, val_error: 0.2262, auc: 0.8934\n",
      "class 0 clustering acc 0.8690476190476191: correct 1168/1344\n",
      "class 1 clustering acc 0.40029761904761907: correct 269/672\n",
      "class 0: acc 0.8717948717948718, correct 34/39\n",
      "class 1: acc 0.6888888888888889, correct 31/45\n",
      "Validation loss decreased (0.639660 --> 0.553690).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0106, instance_loss: 0.6883, weighted_loss: 0.2139, label: 1, bag_size: 88\n",
      "batch 39, loss: 0.0019, instance_loss: 0.0510, weighted_loss: 0.0166, label: 0, bag_size: 85\n",
      "batch 59, loss: 0.0345, instance_loss: 0.4599, weighted_loss: 0.1621, label: 0, bag_size: 18\n",
      "batch 79, loss: 0.0154, instance_loss: 0.3951, weighted_loss: 0.1293, label: 1, bag_size: 115\n",
      "batch 99, loss: 0.0040, instance_loss: 0.0407, weighted_loss: 0.0150, label: 0, bag_size: 26\n",
      "batch 119, loss: 0.2469, instance_loss: 0.0175, weighted_loss: 0.1781, label: 0, bag_size: 20\n",
      "batch 139, loss: 0.2050, instance_loss: 0.5133, weighted_loss: 0.2975, label: 1, bag_size: 136\n",
      "batch 159, loss: 0.0083, instance_loss: 0.2025, weighted_loss: 0.0666, label: 0, bag_size: 85\n",
      "batch 179, loss: 0.0749, instance_loss: 0.9241, weighted_loss: 0.3297, label: 0, bag_size: 42\n",
      "batch 199, loss: 0.0061, instance_loss: 0.8835, weighted_loss: 0.2693, label: 0, bag_size: 74\n",
      "batch 219, loss: 0.0000, instance_loss: 0.5238, weighted_loss: 0.1571, label: 1, bag_size: 81\n",
      "batch 239, loss: 0.2345, instance_loss: 0.6468, weighted_loss: 0.3582, label: 1, bag_size: 30\n",
      "batch 259, loss: 1.3131, instance_loss: 0.4695, weighted_loss: 1.0601, label: 0, bag_size: 23\n",
      "batch 279, loss: 0.0000, instance_loss: 0.6290, weighted_loss: 0.1887, label: 1, bag_size: 72\n",
      "batch 299, loss: 0.0005, instance_loss: 0.0630, weighted_loss: 0.0192, label: 0, bag_size: 30\n",
      "batch 319, loss: 0.2356, instance_loss: 0.5220, weighted_loss: 0.3215, label: 0, bag_size: 28\n",
      "batch 339, loss: 0.5850, instance_loss: 0.4507, weighted_loss: 0.5447, label: 0, bag_size: 78\n",
      "batch 359, loss: 0.0000, instance_loss: 0.2246, weighted_loss: 0.0674, label: 0, bag_size: 77\n",
      "batch 379, loss: 0.0116, instance_loss: 0.9965, weighted_loss: 0.3071, label: 0, bag_size: 36\n",
      "batch 399, loss: 0.0241, instance_loss: 0.7061, weighted_loss: 0.2287, label: 0, bag_size: 33\n",
      "batch 419, loss: 0.0073, instance_loss: 0.5728, weighted_loss: 0.1769, label: 1, bag_size: 153\n",
      "batch 439, loss: 0.0154, instance_loss: 0.6363, weighted_loss: 0.2017, label: 1, bag_size: 24\n",
      "batch 459, loss: 0.4452, instance_loss: 1.4291, weighted_loss: 0.7404, label: 1, bag_size: 53\n",
      "batch 479, loss: 0.0002, instance_loss: 0.7020, weighted_loss: 0.2107, label: 0, bag_size: 79\n",
      "batch 499, loss: 0.0053, instance_loss: 0.5670, weighted_loss: 0.1738, label: 1, bag_size: 36\n",
      "batch 519, loss: 0.2097, instance_loss: 0.6226, weighted_loss: 0.3336, label: 0, bag_size: 62\n",
      "batch 539, loss: 0.0118, instance_loss: 0.7325, weighted_loss: 0.2280, label: 1, bag_size: 98\n",
      "batch 559, loss: 0.0004, instance_loss: 0.0253, weighted_loss: 0.0079, label: 0, bag_size: 63\n",
      "batch 579, loss: 0.0272, instance_loss: 1.3011, weighted_loss: 0.4094, label: 0, bag_size: 79\n",
      "batch 599, loss: 0.0289, instance_loss: 0.0631, weighted_loss: 0.0392, label: 0, bag_size: 50\n",
      "batch 619, loss: 0.0038, instance_loss: 0.5780, weighted_loss: 0.1761, label: 1, bag_size: 76\n",
      "batch 639, loss: 0.2168, instance_loss: 0.0266, weighted_loss: 0.1598, label: 0, bag_size: 45\n",
      "batch 659, loss: 1.5637, instance_loss: 0.5264, weighted_loss: 1.2525, label: 0, bag_size: 36\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.915625: correct 9669/10560\n",
      "class 1 clustering acc 0.49242424242424243: correct 2600/5280\n",
      "Epoch: 6, train_loss: 0.6376, train_clustering_loss:  0.7204, train_error: 0.1970\n",
      "class 0: acc 0.8151515151515152, correct 269/330\n",
      "class 1: acc 0.7909090909090909, correct 261/330\n",
      "\n",
      "Val Set, val_loss: 0.4936, val_error: 0.2262, auc: 0.8838\n",
      "class 0 clustering acc 0.9330357142857143: correct 1254/1344\n",
      "class 1 clustering acc 0.13988095238095238: correct 94/672\n",
      "class 0: acc 0.6666666666666666, correct 26/39\n",
      "class 1: acc 0.8666666666666667, correct 39/45\n",
      "Validation loss decreased (0.553690 --> 0.493556).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0938, instance_loss: 0.7061, weighted_loss: 0.2775, label: 1, bag_size: 102\n",
      "batch 39, loss: 0.2273, instance_loss: 0.7522, weighted_loss: 0.3848, label: 0, bag_size: 29\n",
      "batch 59, loss: 1.2597, instance_loss: 1.2826, weighted_loss: 1.2666, label: 1, bag_size: 40\n",
      "batch 79, loss: 0.0000, instance_loss: 0.7340, weighted_loss: 0.2202, label: 1, bag_size: 83\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0451, weighted_loss: 0.0135, label: 0, bag_size: 106\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0213, weighted_loss: 0.0064, label: 0, bag_size: 50\n",
      "batch 139, loss: 0.0000, instance_loss: 0.5791, weighted_loss: 0.1738, label: 1, bag_size: 92\n",
      "batch 159, loss: 0.0506, instance_loss: 0.1054, weighted_loss: 0.0670, label: 0, bag_size: 81\n",
      "batch 179, loss: 0.0015, instance_loss: 0.5153, weighted_loss: 0.1556, label: 1, bag_size: 96\n",
      "batch 199, loss: 0.0036, instance_loss: 0.2410, weighted_loss: 0.0748, label: 0, bag_size: 42\n",
      "batch 219, loss: 0.1674, instance_loss: 1.5220, weighted_loss: 0.5737, label: 0, bag_size: 38\n",
      "batch 239, loss: 0.1129, instance_loss: 0.6176, weighted_loss: 0.2643, label: 1, bag_size: 95\n",
      "batch 259, loss: 0.0494, instance_loss: 0.4826, weighted_loss: 0.1794, label: 1, bag_size: 60\n",
      "batch 279, loss: 0.0002, instance_loss: 0.7883, weighted_loss: 0.2366, label: 1, bag_size: 36\n",
      "batch 299, loss: 0.3675, instance_loss: 1.1313, weighted_loss: 0.5967, label: 0, bag_size: 107\n",
      "batch 319, loss: 0.1424, instance_loss: 0.6999, weighted_loss: 0.3097, label: 0, bag_size: 61\n",
      "batch 339, loss: 0.0002, instance_loss: 0.1832, weighted_loss: 0.0551, label: 0, bag_size: 96\n",
      "batch 359, loss: 0.2209, instance_loss: 0.0512, weighted_loss: 0.1700, label: 0, bag_size: 78\n",
      "batch 379, loss: 0.1651, instance_loss: 0.2765, weighted_loss: 0.1985, label: 0, bag_size: 102\n",
      "batch 399, loss: 0.4277, instance_loss: 1.1670, weighted_loss: 0.6495, label: 1, bag_size: 62\n",
      "batch 419, loss: 0.0005, instance_loss: 0.4815, weighted_loss: 0.1448, label: 1, bag_size: 69\n",
      "batch 439, loss: 0.8401, instance_loss: 0.1566, weighted_loss: 0.6350, label: 0, bag_size: 12\n",
      "batch 459, loss: 2.3724, instance_loss: 0.5763, weighted_loss: 1.8335, label: 0, bag_size: 25\n",
      "batch 479, loss: 0.0438, instance_loss: 0.6118, weighted_loss: 0.2142, label: 1, bag_size: 62\n",
      "batch 499, loss: 0.3376, instance_loss: 0.1797, weighted_loss: 0.2902, label: 0, bag_size: 66\n",
      "batch 519, loss: 0.1100, instance_loss: 0.5522, weighted_loss: 0.2427, label: 1, bag_size: 100\n",
      "batch 539, loss: 0.2976, instance_loss: 0.8918, weighted_loss: 0.4759, label: 0, bag_size: 102\n",
      "batch 559, loss: 0.0000, instance_loss: 0.7193, weighted_loss: 0.2158, label: 1, bag_size: 19\n",
      "batch 579, loss: 1.1177, instance_loss: 1.7305, weighted_loss: 1.3016, label: 1, bag_size: 67\n",
      "batch 599, loss: 2.3998, instance_loss: 1.5417, weighted_loss: 2.1424, label: 0, bag_size: 29\n",
      "batch 619, loss: 0.0089, instance_loss: 0.6608, weighted_loss: 0.2045, label: 1, bag_size: 82\n",
      "batch 639, loss: 0.0238, instance_loss: 0.5396, weighted_loss: 0.1786, label: 0, bag_size: 104\n",
      "batch 659, loss: 1.5429, instance_loss: 0.8529, weighted_loss: 1.3359, label: 0, bag_size: 55\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9341856060606061: correct 9865/10560\n",
      "class 1 clustering acc 0.5263257575757576: correct 2779/5280\n",
      "Epoch: 7, train_loss: 0.4649, train_clustering_loss:  0.6703, train_error: 0.1773\n",
      "class 0: acc 0.8148148148148148, correct 264/324\n",
      "class 1: acc 0.8303571428571429, correct 279/336\n",
      "\n",
      "Val Set, val_loss: 1.0810, val_error: 0.3095, auc: 0.8513\n",
      "class 0 clustering acc 0.7953869047619048: correct 1069/1344\n",
      "class 1 clustering acc 0.4419642857142857: correct 297/672\n",
      "class 0: acc 0.48717948717948717, correct 19/39\n",
      "class 1: acc 0.8666666666666667, correct 39/45\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0235, weighted_loss: 0.0071, label: 0, bag_size: 38\n",
      "batch 39, loss: 0.7030, instance_loss: 0.4653, weighted_loss: 0.6317, label: 0, bag_size: 48\n",
      "batch 59, loss: 1.2453, instance_loss: 1.1087, weighted_loss: 1.2043, label: 1, bag_size: 88\n",
      "batch 79, loss: 0.0182, instance_loss: 0.5175, weighted_loss: 0.1680, label: 1, bag_size: 120\n",
      "batch 99, loss: 0.0005, instance_loss: 0.7659, weighted_loss: 0.2301, label: 1, bag_size: 81\n",
      "batch 119, loss: 0.3610, instance_loss: 0.4664, weighted_loss: 0.3926, label: 1, bag_size: 84\n",
      "batch 139, loss: 0.0013, instance_loss: 0.5429, weighted_loss: 0.1638, label: 1, bag_size: 83\n",
      "batch 159, loss: 0.0269, instance_loss: 0.4585, weighted_loss: 0.1564, label: 1, bag_size: 87\n",
      "batch 179, loss: 0.0219, instance_loss: 0.5697, weighted_loss: 0.1863, label: 1, bag_size: 14\n",
      "batch 199, loss: 0.0015, instance_loss: 0.0255, weighted_loss: 0.0087, label: 0, bag_size: 21\n",
      "batch 219, loss: 1.0004, instance_loss: 0.3186, weighted_loss: 0.7959, label: 0, bag_size: 67\n",
      "batch 239, loss: 0.0076, instance_loss: 0.0219, weighted_loss: 0.0119, label: 0, bag_size: 66\n",
      "batch 259, loss: 0.0304, instance_loss: 0.9839, weighted_loss: 0.3165, label: 0, bag_size: 66\n",
      "batch 279, loss: 0.0746, instance_loss: 0.0738, weighted_loss: 0.0743, label: 0, bag_size: 94\n",
      "batch 299, loss: 0.0000, instance_loss: 0.4636, weighted_loss: 0.1391, label: 0, bag_size: 31\n",
      "batch 319, loss: 0.4880, instance_loss: 0.6164, weighted_loss: 0.5265, label: 1, bag_size: 43\n",
      "batch 339, loss: 0.0054, instance_loss: 0.4242, weighted_loss: 0.1310, label: 1, bag_size: 21\n",
      "batch 359, loss: 0.0014, instance_loss: 0.4301, weighted_loss: 0.1300, label: 1, bag_size: 85\n",
      "batch 379, loss: 0.0606, instance_loss: 0.1771, weighted_loss: 0.0955, label: 0, bag_size: 28\n",
      "batch 399, loss: 0.0348, instance_loss: 0.5231, weighted_loss: 0.1813, label: 1, bag_size: 33\n",
      "batch 419, loss: 0.0000, instance_loss: 0.1351, weighted_loss: 0.0405, label: 0, bag_size: 93\n",
      "batch 439, loss: 0.0007, instance_loss: 0.2250, weighted_loss: 0.0680, label: 0, bag_size: 24\n",
      "batch 459, loss: 0.0001, instance_loss: 0.5271, weighted_loss: 0.1582, label: 1, bag_size: 91\n",
      "batch 479, loss: 0.0098, instance_loss: 0.0532, weighted_loss: 0.0229, label: 0, bag_size: 45\n",
      "batch 499, loss: 0.3537, instance_loss: 1.0874, weighted_loss: 0.5738, label: 0, bag_size: 54\n",
      "batch 519, loss: 0.0176, instance_loss: 0.0120, weighted_loss: 0.0159, label: 0, bag_size: 48\n",
      "batch 539, loss: 0.0118, instance_loss: 0.2309, weighted_loss: 0.0775, label: 0, bag_size: 118\n",
      "batch 559, loss: 0.1925, instance_loss: 0.5450, weighted_loss: 0.2982, label: 0, bag_size: 37\n",
      "batch 579, loss: 0.0000, instance_loss: 0.3421, weighted_loss: 0.1027, label: 1, bag_size: 35\n",
      "batch 599, loss: 0.0088, instance_loss: 0.3579, weighted_loss: 0.1135, label: 1, bag_size: 40\n",
      "batch 619, loss: 0.6907, instance_loss: 0.0464, weighted_loss: 0.4974, label: 0, bag_size: 80\n",
      "batch 639, loss: 0.0001, instance_loss: 0.3415, weighted_loss: 0.1025, label: 1, bag_size: 69\n",
      "batch 659, loss: 0.0604, instance_loss: 0.7961, weighted_loss: 0.2811, label: 0, bag_size: 54\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9132575757575757: correct 9644/10560\n",
      "class 1 clustering acc 0.6160984848484848: correct 3253/5280\n",
      "Epoch: 8, train_loss: 0.5149, train_clustering_loss:  0.6256, train_error: 0.1561\n",
      "class 0: acc 0.8313609467455622, correct 281/338\n",
      "class 1: acc 0.8571428571428571, correct 276/322\n",
      "\n",
      "Val Set, val_loss: 1.1525, val_error: 0.2857, auc: 0.8678\n",
      "class 0 clustering acc 0.7864583333333334: correct 1057/1344\n",
      "class 1 clustering acc 0.4836309523809524: correct 325/672\n",
      "class 0: acc 0.9487179487179487, correct 37/39\n",
      "class 1: acc 0.5111111111111111, correct 23/45\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0003, instance_loss: 0.0260, weighted_loss: 0.0080, label: 0, bag_size: 24\n",
      "batch 39, loss: 0.0003, instance_loss: 0.2839, weighted_loss: 0.0854, label: 1, bag_size: 34\n",
      "batch 59, loss: 0.0000, instance_loss: 0.3159, weighted_loss: 0.0948, label: 1, bag_size: 128\n",
      "batch 79, loss: 0.0187, instance_loss: 0.9400, weighted_loss: 0.2951, label: 1, bag_size: 23\n",
      "batch 99, loss: 0.0073, instance_loss: 0.4041, weighted_loss: 0.1263, label: 1, bag_size: 28\n",
      "batch 119, loss: 1.3515, instance_loss: 0.2755, weighted_loss: 1.0287, label: 0, bag_size: 49\n",
      "batch 139, loss: 0.8432, instance_loss: 0.3641, weighted_loss: 0.6994, label: 1, bag_size: 46\n",
      "batch 159, loss: 9.5909, instance_loss: 0.9414, weighted_loss: 6.9960, label: 1, bag_size: 89\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0374, weighted_loss: 0.0112, label: 0, bag_size: 113\n",
      "batch 199, loss: 0.0003, instance_loss: 0.5329, weighted_loss: 0.1601, label: 1, bag_size: 85\n",
      "batch 219, loss: 0.0984, instance_loss: 0.6488, weighted_loss: 0.2635, label: 0, bag_size: 18\n",
      "batch 239, loss: 0.5375, instance_loss: 0.3664, weighted_loss: 0.4862, label: 0, bag_size: 57\n",
      "batch 259, loss: 0.0601, instance_loss: 0.0508, weighted_loss: 0.0573, label: 1, bag_size: 88\n",
      "batch 279, loss: 2.0362, instance_loss: 4.0933, weighted_loss: 2.6533, label: 1, bag_size: 38\n",
      "batch 299, loss: 0.0086, instance_loss: 0.8641, weighted_loss: 0.2653, label: 1, bag_size: 78\n",
      "batch 319, loss: 0.0007, instance_loss: 0.2237, weighted_loss: 0.0676, label: 1, bag_size: 99\n",
      "batch 339, loss: 0.7777, instance_loss: 0.2916, weighted_loss: 0.6319, label: 0, bag_size: 79\n",
      "batch 359, loss: 0.6626, instance_loss: 1.2915, weighted_loss: 0.8513, label: 1, bag_size: 29\n",
      "batch 379, loss: 1.2288, instance_loss: 0.2261, weighted_loss: 0.9280, label: 0, bag_size: 31\n",
      "batch 399, loss: 0.1741, instance_loss: 0.6773, weighted_loss: 0.3251, label: 0, bag_size: 55\n",
      "batch 419, loss: 2.4439, instance_loss: 2.6725, weighted_loss: 2.5125, label: 1, bag_size: 57\n",
      "batch 439, loss: 0.2454, instance_loss: 0.3702, weighted_loss: 0.2828, label: 0, bag_size: 89\n",
      "batch 459, loss: 0.0000, instance_loss: 0.4034, weighted_loss: 0.1210, label: 1, bag_size: 70\n",
      "batch 479, loss: 1.7585, instance_loss: 3.5203, weighted_loss: 2.2871, label: 1, bag_size: 38\n",
      "batch 499, loss: 1.2545, instance_loss: 1.6491, weighted_loss: 1.3729, label: 0, bag_size: 57\n",
      "batch 519, loss: 0.1502, instance_loss: 1.3598, weighted_loss: 0.5131, label: 1, bag_size: 21\n",
      "batch 539, loss: 0.0005, instance_loss: 0.0164, weighted_loss: 0.0053, label: 0, bag_size: 81\n",
      "batch 559, loss: 0.0029, instance_loss: 0.0321, weighted_loss: 0.0116, label: 0, bag_size: 112\n",
      "batch 579, loss: 0.8784, instance_loss: 0.3464, weighted_loss: 0.7188, label: 1, bag_size: 46\n",
      "batch 599, loss: 1.1874, instance_loss: 0.0101, weighted_loss: 0.8342, label: 0, bag_size: 101\n",
      "batch 619, loss: 0.8258, instance_loss: 0.1908, weighted_loss: 0.6353, label: 0, bag_size: 49\n",
      "batch 639, loss: 0.0010, instance_loss: 0.0378, weighted_loss: 0.0120, label: 0, bag_size: 102\n",
      "batch 659, loss: 0.0011, instance_loss: 0.0261, weighted_loss: 0.0086, label: 0, bag_size: 67\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9347537878787879: correct 9871/10560\n",
      "class 1 clustering acc 0.6837121212121212: correct 3610/5280\n",
      "Epoch: 9, train_loss: 0.6266, train_clustering_loss:  0.5636, train_error: 0.2000\n",
      "class 0: acc 0.8023952095808383, correct 268/334\n",
      "class 1: acc 0.7975460122699386, correct 260/326\n",
      "\n",
      "Val Set, val_loss: 0.6131, val_error: 0.2024, auc: 0.8798\n",
      "class 0 clustering acc 0.8199404761904762: correct 1102/1344\n",
      "class 1 clustering acc 0.6026785714285714: correct 405/672\n",
      "class 0: acc 0.8461538461538461, correct 33/39\n",
      "class 1: acc 0.7555555555555555, correct 34/45\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0115, instance_loss: 0.3504, weighted_loss: 0.1132, label: 1, bag_size: 16\n",
      "batch 39, loss: 0.0531, instance_loss: 0.0844, weighted_loss: 0.0625, label: 1, bag_size: 88\n",
      "batch 59, loss: 0.0437, instance_loss: 0.4863, weighted_loss: 0.1765, label: 0, bag_size: 25\n",
      "batch 79, loss: 1.0646, instance_loss: 0.4695, weighted_loss: 0.8861, label: 0, bag_size: 82\n",
      "batch 99, loss: 0.0154, instance_loss: 1.5450, weighted_loss: 0.4743, label: 1, bag_size: 51\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0260, weighted_loss: 0.0078, label: 1, bag_size: 136\n",
      "batch 139, loss: 2.2583, instance_loss: 0.7298, weighted_loss: 1.7997, label: 1, bag_size: 82\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0575, weighted_loss: 0.0172, label: 0, bag_size: 91\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0059, weighted_loss: 0.0018, label: 0, bag_size: 44\n",
      "batch 199, loss: 12.2537, instance_loss: 2.2520, weighted_loss: 9.2532, label: 0, bag_size: 78\n",
      "batch 219, loss: 3.8265, instance_loss: 0.9330, weighted_loss: 2.9584, label: 0, bag_size: 29\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0367, weighted_loss: 0.0110, label: 1, bag_size: 67\n",
      "batch 259, loss: 0.0003, instance_loss: 0.0802, weighted_loss: 0.0243, label: 1, bag_size: 31\n",
      "batch 279, loss: 0.0306, instance_loss: 0.2059, weighted_loss: 0.0832, label: 0, bag_size: 50\n",
      "batch 299, loss: 0.0016, instance_loss: 0.0574, weighted_loss: 0.0183, label: 1, bag_size: 104\n",
      "batch 319, loss: 0.0830, instance_loss: 1.5093, weighted_loss: 0.5109, label: 0, bag_size: 66\n",
      "batch 339, loss: 0.0284, instance_loss: 0.1560, weighted_loss: 0.0667, label: 1, bag_size: 84\n",
      "batch 359, loss: 0.0506, instance_loss: 0.1774, weighted_loss: 0.0886, label: 0, bag_size: 79\n",
      "batch 379, loss: 0.1249, instance_loss: 0.1359, weighted_loss: 0.1282, label: 0, bag_size: 64\n",
      "batch 399, loss: 0.4460, instance_loss: 0.1382, weighted_loss: 0.3537, label: 1, bag_size: 14\n",
      "batch 419, loss: 0.0017, instance_loss: 0.0202, weighted_loss: 0.0072, label: 0, bag_size: 27\n",
      "batch 439, loss: 3.2848, instance_loss: 1.0418, weighted_loss: 2.6119, label: 0, bag_size: 13\n",
      "batch 459, loss: 0.0183, instance_loss: 0.0697, weighted_loss: 0.0337, label: 0, bag_size: 39\n",
      "batch 479, loss: 0.0014, instance_loss: 0.0435, weighted_loss: 0.0140, label: 1, bag_size: 100\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0568, weighted_loss: 0.0171, label: 0, bag_size: 47\n",
      "batch 519, loss: 0.0044, instance_loss: 0.2997, weighted_loss: 0.0930, label: 1, bag_size: 21\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0674, weighted_loss: 0.0202, label: 1, bag_size: 26\n",
      "batch 559, loss: 0.0000, instance_loss: 0.1733, weighted_loss: 0.0520, label: 0, bag_size: 80\n",
      "batch 579, loss: 0.0089, instance_loss: 0.0438, weighted_loss: 0.0193, label: 1, bag_size: 62\n",
      "batch 599, loss: 0.0001, instance_loss: 0.2422, weighted_loss: 0.0727, label: 0, bag_size: 89\n",
      "batch 619, loss: 1.0345, instance_loss: 0.9447, weighted_loss: 1.0076, label: 1, bag_size: 16\n",
      "batch 639, loss: 0.1080, instance_loss: 1.5631, weighted_loss: 0.5445, label: 1, bag_size: 59\n",
      "batch 659, loss: 0.0159, instance_loss: 0.2269, weighted_loss: 0.0792, label: 0, bag_size: 80\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9408143939393939: correct 9935/10560\n",
      "class 1 clustering acc 0.7018939393939394: correct 3706/5280\n",
      "Epoch: 10, train_loss: 0.6427, train_clustering_loss:  0.5301, train_error: 0.1788\n",
      "class 0: acc 0.8206686930091185, correct 270/329\n",
      "class 1: acc 0.8217522658610272, correct 272/331\n",
      "\n",
      "Val Set, val_loss: 1.0547, val_error: 0.3214, auc: 0.8570\n",
      "class 0 clustering acc 0.8311011904761905: correct 1117/1344\n",
      "class 1 clustering acc 0.6607142857142857: correct 444/672\n",
      "class 0: acc 0.41025641025641024, correct 16/39\n",
      "class 1: acc 0.9111111111111111, correct 41/45\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1164, instance_loss: 0.2204, weighted_loss: 0.1476, label: 1, bag_size: 71\n",
      "batch 39, loss: 0.0009, instance_loss: 0.0207, weighted_loss: 0.0068, label: 0, bag_size: 35\n",
      "batch 59, loss: 0.0030, instance_loss: 0.0401, weighted_loss: 0.0141, label: 0, bag_size: 29\n",
      "batch 79, loss: 0.2830, instance_loss: 0.2956, weighted_loss: 0.2868, label: 0, bag_size: 85\n",
      "batch 99, loss: 0.6448, instance_loss: 1.4223, weighted_loss: 0.8780, label: 1, bag_size: 35\n",
      "batch 119, loss: 1.2893, instance_loss: 0.4869, weighted_loss: 1.0486, label: 1, bag_size: 68\n",
      "batch 139, loss: 1.9575, instance_loss: 2.0682, weighted_loss: 1.9907, label: 0, bag_size: 27\n",
      "batch 159, loss: 0.4606, instance_loss: 1.3047, weighted_loss: 0.7138, label: 0, bag_size: 61\n",
      "batch 179, loss: 0.0412, instance_loss: 0.0819, weighted_loss: 0.0534, label: 1, bag_size: 39\n",
      "batch 199, loss: 0.2456, instance_loss: 0.6001, weighted_loss: 0.3519, label: 0, bag_size: 34\n",
      "batch 219, loss: 0.9506, instance_loss: 0.5605, weighted_loss: 0.8336, label: 0, bag_size: 60\n",
      "batch 239, loss: 0.0409, instance_loss: 0.2260, weighted_loss: 0.0965, label: 0, bag_size: 78\n",
      "batch 259, loss: 0.0110, instance_loss: 0.0407, weighted_loss: 0.0199, label: 1, bag_size: 95\n",
      "batch 279, loss: 0.0446, instance_loss: 0.0714, weighted_loss: 0.0527, label: 0, bag_size: 126\n",
      "batch 299, loss: 0.0530, instance_loss: 0.1152, weighted_loss: 0.0716, label: 1, bag_size: 69\n",
      "batch 319, loss: 1.6420, instance_loss: 1.3736, weighted_loss: 1.5615, label: 0, bag_size: 45\n",
      "batch 339, loss: 0.2585, instance_loss: 0.6519, weighted_loss: 0.3765, label: 0, bag_size: 73\n",
      "batch 359, loss: 0.0092, instance_loss: 0.0139, weighted_loss: 0.0106, label: 1, bag_size: 24\n",
      "batch 379, loss: 0.2598, instance_loss: 1.0758, weighted_loss: 0.5046, label: 0, bag_size: 34\n",
      "batch 399, loss: 0.0024, instance_loss: 0.0258, weighted_loss: 0.0094, label: 0, bag_size: 108\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0983, weighted_loss: 0.0295, label: 1, bag_size: 22\n",
      "batch 439, loss: 0.0153, instance_loss: 0.0544, weighted_loss: 0.0271, label: 0, bag_size: 28\n",
      "batch 459, loss: 0.0007, instance_loss: 0.0457, weighted_loss: 0.0142, label: 1, bag_size: 110\n",
      "batch 479, loss: 0.0049, instance_loss: 0.0349, weighted_loss: 0.0139, label: 0, bag_size: 39\n",
      "batch 499, loss: 0.1461, instance_loss: 0.6548, weighted_loss: 0.2987, label: 1, bag_size: 40\n",
      "batch 519, loss: 0.0864, instance_loss: 0.0594, weighted_loss: 0.0783, label: 0, bag_size: 50\n",
      "batch 539, loss: 0.0189, instance_loss: 0.0158, weighted_loss: 0.0180, label: 1, bag_size: 121\n",
      "batch 559, loss: 0.2325, instance_loss: 0.3922, weighted_loss: 0.2804, label: 1, bag_size: 84\n",
      "batch 579, loss: 0.0954, instance_loss: 1.1327, weighted_loss: 0.4066, label: 1, bag_size: 84\n",
      "batch 599, loss: 0.2786, instance_loss: 0.4420, weighted_loss: 0.3276, label: 0, bag_size: 43\n",
      "batch 619, loss: 0.0007, instance_loss: 0.0883, weighted_loss: 0.0270, label: 0, bag_size: 57\n",
      "batch 639, loss: 0.0403, instance_loss: 0.0293, weighted_loss: 0.0370, label: 1, bag_size: 127\n",
      "batch 659, loss: 0.0495, instance_loss: 0.0831, weighted_loss: 0.0596, label: 1, bag_size: 91\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9519886363636364: correct 10053/10560\n",
      "class 1 clustering acc 0.7518939393939394: correct 3970/5280\n",
      "Epoch: 11, train_loss: 0.3708, train_clustering_loss:  0.4489, train_error: 0.1485\n",
      "class 0: acc 0.8626865671641791, correct 289/335\n",
      "class 1: acc 0.84, correct 273/325\n",
      "\n",
      "Val Set, val_loss: 0.6034, val_error: 0.2024, auc: 0.8815\n",
      "class 0 clustering acc 0.8586309523809523: correct 1154/1344\n",
      "class 1 clustering acc 0.6339285714285714: correct 426/672\n",
      "class 0: acc 0.7692307692307693, correct 30/39\n",
      "class 1: acc 0.8222222222222222, correct 37/45\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0896, instance_loss: 0.3168, weighted_loss: 0.1578, label: 0, bag_size: 71\n",
      "batch 39, loss: 0.0037, instance_loss: 0.2163, weighted_loss: 0.0675, label: 0, bag_size: 116\n",
      "batch 59, loss: 0.0050, instance_loss: 0.1070, weighted_loss: 0.0356, label: 1, bag_size: 78\n",
      "batch 79, loss: 0.0002, instance_loss: 0.1588, weighted_loss: 0.0478, label: 0, bag_size: 77\n",
      "batch 99, loss: 0.0151, instance_loss: 0.0265, weighted_loss: 0.0185, label: 1, bag_size: 48\n",
      "batch 119, loss: 0.0125, instance_loss: 0.1710, weighted_loss: 0.0601, label: 0, bag_size: 66\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0034, weighted_loss: 0.0011, label: 0, bag_size: 87\n",
      "batch 159, loss: 0.0212, instance_loss: 0.0259, weighted_loss: 0.0226, label: 1, bag_size: 91\n",
      "batch 179, loss: 0.0212, instance_loss: 0.0166, weighted_loss: 0.0198, label: 1, bag_size: 83\n",
      "batch 199, loss: 0.0000, instance_loss: 0.9438, weighted_loss: 0.2832, label: 0, bag_size: 27\n",
      "batch 219, loss: 0.0261, instance_loss: 0.0035, weighted_loss: 0.0193, label: 1, bag_size: 90\n",
      "batch 239, loss: 0.0879, instance_loss: 1.4277, weighted_loss: 0.4899, label: 1, bag_size: 53\n",
      "batch 259, loss: 0.4384, instance_loss: 2.0766, weighted_loss: 0.9298, label: 1, bag_size: 17\n",
      "batch 279, loss: 0.0009, instance_loss: 0.0245, weighted_loss: 0.0080, label: 0, bag_size: 78\n",
      "batch 299, loss: 0.1076, instance_loss: 0.1745, weighted_loss: 0.1276, label: 0, bag_size: 101\n",
      "batch 319, loss: 0.0013, instance_loss: 0.1541, weighted_loss: 0.0472, label: 1, bag_size: 118\n",
      "batch 339, loss: 1.8015, instance_loss: 0.5323, weighted_loss: 1.4208, label: 0, bag_size: 85\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0333, weighted_loss: 0.0100, label: 0, bag_size: 79\n",
      "batch 379, loss: 0.0001, instance_loss: 1.4394, weighted_loss: 0.4319, label: 0, bag_size: 51\n",
      "batch 399, loss: 0.1096, instance_loss: 1.1350, weighted_loss: 0.4173, label: 0, bag_size: 36\n",
      "batch 419, loss: 7.2027, instance_loss: 1.2411, weighted_loss: 5.4142, label: 1, bag_size: 92\n",
      "batch 439, loss: 0.1520, instance_loss: 0.1717, weighted_loss: 0.1579, label: 1, bag_size: 99\n",
      "batch 459, loss: 0.0016, instance_loss: 0.3700, weighted_loss: 0.1121, label: 1, bag_size: 33\n",
      "batch 479, loss: 0.0000, instance_loss: 0.2642, weighted_loss: 0.0793, label: 0, bag_size: 92\n",
      "batch 499, loss: 0.0024, instance_loss: 0.5273, weighted_loss: 0.1598, label: 0, bag_size: 54\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0694, weighted_loss: 0.0209, label: 1, bag_size: 76\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0040, weighted_loss: 0.0012, label: 0, bag_size: 35\n",
      "batch 559, loss: 0.0095, instance_loss: 0.4004, weighted_loss: 0.1268, label: 0, bag_size: 40\n",
      "batch 579, loss: 0.1191, instance_loss: 0.7123, weighted_loss: 0.2971, label: 0, bag_size: 103\n",
      "batch 599, loss: 0.0002, instance_loss: 0.0328, weighted_loss: 0.0100, label: 0, bag_size: 57\n",
      "batch 619, loss: 0.0007, instance_loss: 0.0154, weighted_loss: 0.0051, label: 0, bag_size: 86\n",
      "batch 639, loss: 0.0141, instance_loss: 0.1705, weighted_loss: 0.0610, label: 0, bag_size: 29\n",
      "batch 659, loss: 0.0003, instance_loss: 0.1794, weighted_loss: 0.0540, label: 0, bag_size: 113\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9331439393939394: correct 9854/10560\n",
      "class 1 clustering acc 0.7267045454545454: correct 3837/5280\n",
      "Epoch: 12, train_loss: 0.3701, train_clustering_loss:  0.4753, train_error: 0.1318\n",
      "class 0: acc 0.8725212464589235, correct 308/353\n",
      "class 1: acc 0.8631921824104235, correct 265/307\n",
      "\n",
      "Val Set, val_loss: 0.4771, val_error: 0.1786, auc: 0.9054\n",
      "class 0 clustering acc 0.7872023809523809: correct 1058/1344\n",
      "class 1 clustering acc 0.6889880952380952: correct 463/672\n",
      "class 0: acc 0.7948717948717948, correct 31/39\n",
      "class 1: acc 0.8444444444444444, correct 38/45\n",
      "Validation loss decreased (0.493556 --> 0.477106).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 1.1737, instance_loss: 1.2656, weighted_loss: 1.2013, label: 1, bag_size: 28\n",
      "batch 39, loss: 0.0016, instance_loss: 0.2758, weighted_loss: 0.0839, label: 0, bag_size: 68\n",
      "batch 59, loss: 0.0054, instance_loss: 0.5542, weighted_loss: 0.1700, label: 1, bag_size: 89\n",
      "batch 79, loss: 0.0000, instance_loss: 0.4067, weighted_loss: 0.1220, label: 0, bag_size: 79\n",
      "batch 99, loss: 0.0004, instance_loss: 0.4222, weighted_loss: 0.1269, label: 1, bag_size: 23\n",
      "batch 119, loss: 0.0355, instance_loss: 0.0988, weighted_loss: 0.0545, label: 1, bag_size: 94\n",
      "batch 139, loss: 0.0228, instance_loss: 0.3809, weighted_loss: 0.1303, label: 1, bag_size: 92\n",
      "batch 159, loss: 0.0039, instance_loss: 0.0456, weighted_loss: 0.0164, label: 1, bag_size: 85\n",
      "batch 179, loss: 0.0011, instance_loss: 0.3854, weighted_loss: 0.1164, label: 0, bag_size: 85\n",
      "batch 199, loss: 0.1173, instance_loss: 0.5192, weighted_loss: 0.2379, label: 1, bag_size: 62\n",
      "batch 219, loss: 0.0020, instance_loss: 0.1159, weighted_loss: 0.0362, label: 1, bag_size: 13\n",
      "batch 239, loss: 0.0122, instance_loss: 0.0912, weighted_loss: 0.0359, label: 1, bag_size: 31\n",
      "batch 259, loss: 0.0000, instance_loss: 0.1245, weighted_loss: 0.0373, label: 1, bag_size: 41\n",
      "batch 279, loss: 0.0043, instance_loss: 0.3997, weighted_loss: 0.1229, label: 1, bag_size: 100\n",
      "batch 299, loss: 0.0045, instance_loss: 0.2726, weighted_loss: 0.0849, label: 1, bag_size: 32\n",
      "batch 319, loss: 1.1121, instance_loss: 0.5465, weighted_loss: 0.9424, label: 0, bag_size: 87\n",
      "batch 339, loss: 0.0091, instance_loss: 0.0889, weighted_loss: 0.0330, label: 1, bag_size: 107\n",
      "batch 359, loss: 0.3978, instance_loss: 0.2951, weighted_loss: 0.3670, label: 0, bag_size: 13\n",
      "batch 379, loss: 0.0060, instance_loss: 0.3457, weighted_loss: 0.1079, label: 1, bag_size: 30\n",
      "batch 399, loss: 0.0002, instance_loss: 0.0633, weighted_loss: 0.0191, label: 1, bag_size: 91\n",
      "batch 419, loss: 0.0114, instance_loss: 0.0202, weighted_loss: 0.0141, label: 1, bag_size: 85\n",
      "batch 439, loss: 0.0003, instance_loss: 0.0226, weighted_loss: 0.0070, label: 1, bag_size: 88\n",
      "batch 459, loss: 0.0517, instance_loss: 1.8555, weighted_loss: 0.5929, label: 1, bag_size: 17\n",
      "batch 479, loss: 0.0116, instance_loss: 0.1062, weighted_loss: 0.0400, label: 1, bag_size: 81\n",
      "batch 499, loss: 0.0520, instance_loss: 1.3643, weighted_loss: 0.4457, label: 1, bag_size: 21\n",
      "batch 519, loss: 0.0013, instance_loss: 0.0401, weighted_loss: 0.0129, label: 1, bag_size: 44\n",
      "batch 539, loss: 0.0465, instance_loss: 0.0864, weighted_loss: 0.0584, label: 1, bag_size: 41\n",
      "batch 559, loss: 0.0324, instance_loss: 0.0138, weighted_loss: 0.0268, label: 1, bag_size: 62\n",
      "batch 579, loss: 1.5008, instance_loss: 3.0722, weighted_loss: 1.9722, label: 0, bag_size: 22\n",
      "batch 599, loss: 0.4260, instance_loss: 0.9629, weighted_loss: 0.5871, label: 0, bag_size: 28\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0318, weighted_loss: 0.0096, label: 1, bag_size: 91\n",
      "batch 639, loss: 0.0451, instance_loss: 0.0152, weighted_loss: 0.0362, label: 1, bag_size: 23\n",
      "batch 659, loss: 0.2072, instance_loss: 0.0451, weighted_loss: 0.1586, label: 1, bag_size: 51\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.931155303030303: correct 9833/10560\n",
      "class 1 clustering acc 0.762689393939394: correct 4027/5280\n",
      "Epoch: 13, train_loss: 0.3149, train_clustering_loss:  0.4555, train_error: 0.1242\n",
      "class 0: acc 0.8605442176870748, correct 253/294\n",
      "class 1: acc 0.8879781420765027, correct 325/366\n",
      "\n",
      "Val Set, val_loss: 1.0111, val_error: 0.2619, auc: 0.8900\n",
      "class 0 clustering acc 0.7589285714285714: correct 1020/1344\n",
      "class 1 clustering acc 0.7232142857142857: correct 486/672\n",
      "class 0: acc 0.9743589743589743, correct 38/39\n",
      "class 1: acc 0.5333333333333333, correct 24/45\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0300, instance_loss: 0.1647, weighted_loss: 0.0704, label: 0, bag_size: 37\n",
      "batch 39, loss: 0.0428, instance_loss: 0.1903, weighted_loss: 0.0871, label: 1, bag_size: 78\n",
      "batch 59, loss: 0.6214, instance_loss: 1.0242, weighted_loss: 0.7422, label: 1, bag_size: 38\n",
      "batch 79, loss: 0.0196, instance_loss: 0.0237, weighted_loss: 0.0208, label: 1, bag_size: 62\n",
      "batch 99, loss: 0.9972, instance_loss: 0.3471, weighted_loss: 0.8021, label: 1, bag_size: 83\n",
      "batch 119, loss: 0.7228, instance_loss: 0.5557, weighted_loss: 0.6727, label: 0, bag_size: 48\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0063, weighted_loss: 0.0019, label: 0, bag_size: 111\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0090, weighted_loss: 0.0027, label: 0, bag_size: 73\n",
      "batch 179, loss: 0.0081, instance_loss: 0.0088, weighted_loss: 0.0083, label: 1, bag_size: 21\n",
      "batch 199, loss: 0.0016, instance_loss: 0.0010, weighted_loss: 0.0014, label: 1, bag_size: 44\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0398, weighted_loss: 0.0121, label: 1, bag_size: 84\n",
      "batch 239, loss: 0.0002, instance_loss: 0.1348, weighted_loss: 0.0406, label: 0, bag_size: 38\n",
      "batch 259, loss: 0.0077, instance_loss: 0.0210, weighted_loss: 0.0117, label: 0, bag_size: 28\n",
      "batch 279, loss: 0.0001, instance_loss: 0.1589, weighted_loss: 0.0477, label: 0, bag_size: 64\n",
      "batch 299, loss: 0.0147, instance_loss: 0.1961, weighted_loss: 0.0691, label: 0, bag_size: 87\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0060, weighted_loss: 0.0018, label: 0, bag_size: 97\n",
      "batch 339, loss: 0.0000, instance_loss: 0.1838, weighted_loss: 0.0552, label: 0, bag_size: 25\n",
      "batch 359, loss: 0.0045, instance_loss: 0.0211, weighted_loss: 0.0095, label: 1, bag_size: 104\n",
      "batch 379, loss: 0.0369, instance_loss: 0.0252, weighted_loss: 0.0334, label: 0, bag_size: 66\n",
      "batch 399, loss: 0.0041, instance_loss: 0.0550, weighted_loss: 0.0194, label: 1, bag_size: 16\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0029, weighted_loss: 0.0009, label: 1, bag_size: 97\n",
      "batch 439, loss: 0.0019, instance_loss: 0.0114, weighted_loss: 0.0048, label: 1, bag_size: 86\n",
      "batch 459, loss: 0.0000, instance_loss: 0.1494, weighted_loss: 0.0449, label: 1, bag_size: 14\n",
      "batch 479, loss: 1.3563, instance_loss: 2.1500, weighted_loss: 1.5944, label: 1, bag_size: 87\n",
      "batch 499, loss: 0.0030, instance_loss: 0.1326, weighted_loss: 0.0418, label: 0, bag_size: 88\n",
      "batch 519, loss: 0.0752, instance_loss: 0.7637, weighted_loss: 0.2818, label: 1, bag_size: 32\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0204, weighted_loss: 0.0061, label: 1, bag_size: 70\n",
      "batch 559, loss: 0.0008, instance_loss: 0.0093, weighted_loss: 0.0034, label: 1, bag_size: 51\n",
      "batch 579, loss: 0.1184, instance_loss: 0.2486, weighted_loss: 0.1575, label: 0, bag_size: 88\n",
      "batch 599, loss: 0.0192, instance_loss: 0.1141, weighted_loss: 0.0476, label: 1, bag_size: 48\n",
      "batch 619, loss: 1.9414, instance_loss: 1.5016, weighted_loss: 1.8095, label: 1, bag_size: 72\n",
      "batch 639, loss: 0.0000, instance_loss: 0.2204, weighted_loss: 0.0661, label: 0, bag_size: 74\n",
      "batch 659, loss: 0.0062, instance_loss: 0.0892, weighted_loss: 0.0311, label: 1, bag_size: 43\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9616477272727273: correct 10155/10560\n",
      "class 1 clustering acc 0.8456439393939394: correct 4465/5280\n",
      "Epoch: 14, train_loss: 0.2592, train_clustering_loss:  0.3143, train_error: 0.0909\n",
      "class 0: acc 0.908256880733945, correct 297/327\n",
      "class 1: acc 0.9099099099099099, correct 303/333\n",
      "\n",
      "Val Set, val_loss: 1.0088, val_error: 0.2024, auc: 0.8758\n",
      "class 0 clustering acc 0.9077380952380952: correct 1220/1344\n",
      "class 1 clustering acc 0.3601190476190476: correct 242/672\n",
      "class 0: acc 0.7948717948717948, correct 31/39\n",
      "class 1: acc 0.8, correct 36/45\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 1.3670, weighted_loss: 0.4103, label: 1, bag_size: 77\n",
      "batch 39, loss: 0.0008, instance_loss: 0.4521, weighted_loss: 0.1362, label: 1, bag_size: 53\n",
      "batch 59, loss: 0.8591, instance_loss: 0.0107, weighted_loss: 0.6046, label: 1, bag_size: 99\n",
      "batch 79, loss: 0.0011, instance_loss: 0.0907, weighted_loss: 0.0280, label: 0, bag_size: 49\n",
      "batch 99, loss: 0.0000, instance_loss: 0.1028, weighted_loss: 0.0308, label: 1, bag_size: 30\n",
      "batch 119, loss: 0.0000, instance_loss: 0.1450, weighted_loss: 0.0435, label: 1, bag_size: 40\n",
      "batch 139, loss: 0.0433, instance_loss: 1.0729, weighted_loss: 0.3522, label: 1, bag_size: 64\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0244, weighted_loss: 0.0074, label: 1, bag_size: 85\n",
      "batch 179, loss: 0.0189, instance_loss: 0.0969, weighted_loss: 0.0423, label: 1, bag_size: 53\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0142, weighted_loss: 0.0043, label: 0, bag_size: 96\n",
      "batch 219, loss: 0.0051, instance_loss: 0.3697, weighted_loss: 0.1145, label: 0, bag_size: 75\n",
      "batch 239, loss: 0.0046, instance_loss: 0.1661, weighted_loss: 0.0531, label: 0, bag_size: 108\n",
      "batch 259, loss: 0.0111, instance_loss: 0.0211, weighted_loss: 0.0141, label: 1, bag_size: 99\n",
      "batch 279, loss: 0.0001, instance_loss: 0.3150, weighted_loss: 0.0945, label: 0, bag_size: 72\n",
      "batch 299, loss: 0.0085, instance_loss: 0.0565, weighted_loss: 0.0229, label: 1, bag_size: 89\n",
      "batch 319, loss: 1.8924, instance_loss: 2.5953, weighted_loss: 2.1033, label: 1, bag_size: 17\n",
      "batch 339, loss: 0.0041, instance_loss: 0.0346, weighted_loss: 0.0132, label: 1, bag_size: 82\n",
      "batch 359, loss: 0.0031, instance_loss: 0.2384, weighted_loss: 0.0737, label: 1, bag_size: 45\n",
      "batch 379, loss: 0.0001, instance_loss: 0.0768, weighted_loss: 0.0231, label: 1, bag_size: 44\n",
      "batch 399, loss: 0.4413, instance_loss: 0.1379, weighted_loss: 0.3503, label: 1, bag_size: 40\n",
      "batch 419, loss: 0.0000, instance_loss: 0.1867, weighted_loss: 0.0560, label: 1, bag_size: 68\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0092, weighted_loss: 0.0028, label: 0, bag_size: 108\n",
      "batch 459, loss: 0.0002, instance_loss: 0.0717, weighted_loss: 0.0216, label: 1, bag_size: 78\n",
      "batch 479, loss: 5.2833, instance_loss: 1.5584, weighted_loss: 4.1658, label: 0, bag_size: 78\n",
      "batch 499, loss: 0.0004, instance_loss: 0.1373, weighted_loss: 0.0415, label: 0, bag_size: 86\n",
      "batch 519, loss: 0.0422, instance_loss: 1.0666, weighted_loss: 0.3495, label: 0, bag_size: 54\n",
      "batch 539, loss: 0.0953, instance_loss: 0.5369, weighted_loss: 0.2277, label: 1, bag_size: 102\n",
      "batch 559, loss: 0.0006, instance_loss: 0.0999, weighted_loss: 0.0303, label: 0, bag_size: 79\n",
      "batch 579, loss: 0.0087, instance_loss: 0.0107, weighted_loss: 0.0093, label: 1, bag_size: 46\n",
      "batch 599, loss: 0.0060, instance_loss: 0.2024, weighted_loss: 0.0649, label: 1, bag_size: 72\n",
      "batch 619, loss: 1.7265, instance_loss: 1.7425, weighted_loss: 1.7313, label: 1, bag_size: 67\n",
      "batch 639, loss: 0.2769, instance_loss: 0.5139, weighted_loss: 0.3480, label: 1, bag_size: 51\n",
      "batch 659, loss: 0.0014, instance_loss: 0.5223, weighted_loss: 0.1577, label: 1, bag_size: 19\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9398674242424242: correct 9925/10560\n",
      "class 1 clustering acc 0.7412878787878788: correct 3914/5280\n",
      "Epoch: 15, train_loss: 0.5165, train_clustering_loss:  0.4785, train_error: 0.1348\n",
      "class 0: acc 0.8648648648648649, correct 288/333\n",
      "class 1: acc 0.8654434250764526, correct 283/327\n",
      "\n",
      "Val Set, val_loss: 0.9740, val_error: 0.2024, auc: 0.8849\n",
      "class 0 clustering acc 0.8616071428571429: correct 1158/1344\n",
      "class 1 clustering acc 0.6101190476190477: correct 410/672\n",
      "class 0: acc 0.8461538461538461, correct 33/39\n",
      "class 1: acc 0.7555555555555555, correct 34/45\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0003, instance_loss: 0.4399, weighted_loss: 0.1322, label: 1, bag_size: 98\n",
      "batch 39, loss: 0.9390, instance_loss: 2.4368, weighted_loss: 1.3883, label: 1, bag_size: 38\n",
      "batch 59, loss: 0.8391, instance_loss: 0.6657, weighted_loss: 0.7871, label: 0, bag_size: 35\n",
      "batch 79, loss: 0.0394, instance_loss: 0.4352, weighted_loss: 0.1581, label: 1, bag_size: 51\n",
      "batch 99, loss: 0.0000, instance_loss: 0.6901, weighted_loss: 0.2070, label: 1, bag_size: 40\n",
      "batch 119, loss: 0.0000, instance_loss: 0.4032, weighted_loss: 0.1210, label: 0, bag_size: 95\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0999, weighted_loss: 0.0300, label: 0, bag_size: 92\n",
      "batch 159, loss: 0.0000, instance_loss: 0.1561, weighted_loss: 0.0468, label: 0, bag_size: 85\n",
      "batch 179, loss: 0.8999, instance_loss: 0.4028, weighted_loss: 0.7508, label: 1, bag_size: 92\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0694, weighted_loss: 0.0208, label: 0, bag_size: 45\n",
      "batch 219, loss: 0.0186, instance_loss: 0.1317, weighted_loss: 0.0525, label: 0, bag_size: 29\n",
      "batch 239, loss: 0.0014, instance_loss: 0.2748, weighted_loss: 0.0834, label: 1, bag_size: 86\n",
      "batch 259, loss: 0.0222, instance_loss: 0.9834, weighted_loss: 0.3105, label: 0, bag_size: 54\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0883, weighted_loss: 0.0265, label: 0, bag_size: 93\n",
      "batch 299, loss: 0.2422, instance_loss: 0.0526, weighted_loss: 0.1853, label: 1, bag_size: 28\n",
      "batch 319, loss: 0.1139, instance_loss: 0.2193, weighted_loss: 0.1455, label: 1, bag_size: 85\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0293, weighted_loss: 0.0089, label: 1, bag_size: 80\n",
      "batch 359, loss: 1.3973, instance_loss: 0.3212, weighted_loss: 1.0745, label: 0, bag_size: 44\n",
      "batch 379, loss: 6.9942, instance_loss: 5.5034, weighted_loss: 6.5470, label: 0, bag_size: 32\n",
      "batch 399, loss: 1.8311, instance_loss: 1.3289, weighted_loss: 1.6805, label: 1, bag_size: 54\n",
      "batch 419, loss: 0.0002, instance_loss: 0.0688, weighted_loss: 0.0208, label: 0, bag_size: 85\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0101, weighted_loss: 0.0030, label: 1, bag_size: 111\n",
      "batch 459, loss: 0.0012, instance_loss: 0.0130, weighted_loss: 0.0047, label: 0, bag_size: 65\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0198, weighted_loss: 0.0060, label: 1, bag_size: 68\n",
      "batch 499, loss: 0.2411, instance_loss: 0.4498, weighted_loss: 0.3037, label: 0, bag_size: 46\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0045, weighted_loss: 0.0014, label: 0, bag_size: 29\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0057, weighted_loss: 0.0017, label: 1, bag_size: 33\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0081, weighted_loss: 0.0025, label: 1, bag_size: 47\n",
      "batch 579, loss: 0.0515, instance_loss: 0.0467, weighted_loss: 0.0501, label: 1, bag_size: 77\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0320, weighted_loss: 0.0097, label: 0, bag_size: 39\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0032, weighted_loss: 0.0010, label: 1, bag_size: 32\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0023, weighted_loss: 0.0007, label: 1, bag_size: 111\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 73\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9421401515151515: correct 9949/10560\n",
      "class 1 clustering acc 0.7571969696969697: correct 3998/5280\n",
      "Epoch: 16, train_loss: 0.3478, train_clustering_loss:  0.4284, train_error: 0.1000\n",
      "class 0: acc 0.901840490797546, correct 294/326\n",
      "class 1: acc 0.8982035928143712, correct 300/334\n",
      "\n",
      "Val Set, val_loss: 0.9669, val_error: 0.2619, auc: 0.8997\n",
      "class 0 clustering acc 0.8943452380952381: correct 1202/1344\n",
      "class 1 clustering acc 0.6755952380952381: correct 454/672\n",
      "class 0: acc 0.5384615384615384, correct 21/39\n",
      "class 1: acc 0.9111111111111111, correct 41/45\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4085, instance_loss: 0.2249, weighted_loss: 0.3534, label: 0, bag_size: 28\n",
      "batch 39, loss: 0.8790, instance_loss: 0.0242, weighted_loss: 0.6226, label: 0, bag_size: 28\n",
      "batch 59, loss: 0.1696, instance_loss: 0.0196, weighted_loss: 0.1246, label: 1, bag_size: 102\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0031, weighted_loss: 0.0010, label: 0, bag_size: 65\n",
      "batch 99, loss: 0.0024, instance_loss: 0.0111, weighted_loss: 0.0050, label: 0, bag_size: 102\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 61\n",
      "batch 139, loss: 0.0774, instance_loss: 0.8255, weighted_loss: 0.3018, label: 0, bag_size: 78\n",
      "batch 159, loss: 0.0516, instance_loss: 0.2487, weighted_loss: 0.1107, label: 1, bag_size: 77\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0722, weighted_loss: 0.0217, label: 0, bag_size: 61\n",
      "batch 199, loss: 0.4629, instance_loss: 0.1993, weighted_loss: 0.3838, label: 1, bag_size: 88\n",
      "batch 219, loss: 0.0006, instance_loss: 0.0066, weighted_loss: 0.0024, label: 1, bag_size: 97\n",
      "batch 239, loss: 0.0007, instance_loss: 0.0046, weighted_loss: 0.0019, label: 1, bag_size: 85\n",
      "batch 259, loss: 0.5163, instance_loss: 2.0493, weighted_loss: 0.9762, label: 1, bag_size: 121\n",
      "batch 279, loss: 0.0028, instance_loss: 0.0242, weighted_loss: 0.0093, label: 0, bag_size: 92\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0041, weighted_loss: 0.0013, label: 1, bag_size: 40\n",
      "batch 319, loss: 0.0969, instance_loss: 0.0496, weighted_loss: 0.0827, label: 0, bag_size: 71\n",
      "batch 339, loss: 0.0035, instance_loss: 0.0048, weighted_loss: 0.0039, label: 1, bag_size: 84\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0040, weighted_loss: 0.0012, label: 0, bag_size: 25\n",
      "batch 379, loss: 0.8374, instance_loss: 0.5698, weighted_loss: 0.7572, label: 1, bag_size: 21\n",
      "batch 399, loss: 0.0003, instance_loss: 0.0205, weighted_loss: 0.0064, label: 0, bag_size: 95\n",
      "batch 419, loss: 0.0176, instance_loss: 0.1138, weighted_loss: 0.0465, label: 0, bag_size: 102\n",
      "batch 439, loss: 0.0425, instance_loss: 0.0829, weighted_loss: 0.0546, label: 0, bag_size: 88\n",
      "batch 459, loss: 0.0000, instance_loss: 0.2120, weighted_loss: 0.0636, label: 0, bag_size: 27\n",
      "batch 479, loss: 0.0016, instance_loss: 0.0042, weighted_loss: 0.0024, label: 1, bag_size: 85\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0035, weighted_loss: 0.0011, label: 0, bag_size: 126\n",
      "batch 519, loss: 0.5315, instance_loss: 0.3977, weighted_loss: 0.4913, label: 0, bag_size: 24\n",
      "batch 539, loss: 0.8034, instance_loss: 0.0776, weighted_loss: 0.5857, label: 1, bag_size: 45\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 85\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0112, weighted_loss: 0.0034, label: 1, bag_size: 81\n",
      "batch 599, loss: 3.2998, instance_loss: 4.1243, weighted_loss: 3.5472, label: 1, bag_size: 63\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0078, weighted_loss: 0.0023, label: 1, bag_size: 48\n",
      "batch 639, loss: 2.6541, instance_loss: 0.5279, weighted_loss: 2.0163, label: 1, bag_size: 67\n",
      "batch 659, loss: 0.0201, instance_loss: 0.1226, weighted_loss: 0.0508, label: 1, bag_size: 46\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9684659090909091: correct 10227/10560\n",
      "class 1 clustering acc 0.8554924242424242: correct 4517/5280\n",
      "Epoch: 17, train_loss: 0.2614, train_clustering_loss:  0.2883, train_error: 0.1030\n",
      "class 0: acc 0.890625, correct 285/320\n",
      "class 1: acc 0.9029411764705882, correct 307/340\n",
      "\n",
      "Val Set, val_loss: 1.3669, val_error: 0.2500, auc: 0.8946\n",
      "class 0 clustering acc 0.9092261904761905: correct 1222/1344\n",
      "class 1 clustering acc 0.7440476190476191: correct 500/672\n",
      "class 0: acc 0.9743589743589743, correct 38/39\n",
      "class 1: acc 0.5555555555555556, correct 25/45\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0021, weighted_loss: 0.0006, label: 1, bag_size: 99\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0038, weighted_loss: 0.0012, label: 0, bag_size: 87\n",
      "batch 59, loss: 0.1015, instance_loss: 0.1944, weighted_loss: 0.1294, label: 0, bag_size: 87\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0090, weighted_loss: 0.0027, label: 1, bag_size: 98\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0038, weighted_loss: 0.0012, label: 0, bag_size: 53\n",
      "batch 119, loss: 0.0006, instance_loss: 0.2364, weighted_loss: 0.0713, label: 1, bag_size: 87\n",
      "batch 139, loss: 1.6061, instance_loss: 3.8687, weighted_loss: 2.2849, label: 0, bag_size: 73\n",
      "batch 159, loss: 0.0056, instance_loss: 0.7754, weighted_loss: 0.2365, label: 1, bag_size: 62\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0001, label: 0, bag_size: 36\n",
      "batch 199, loss: 0.0003, instance_loss: 0.0319, weighted_loss: 0.0098, label: 1, bag_size: 23\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0013, weighted_loss: 0.0005, label: 1, bag_size: 52\n",
      "batch 239, loss: 0.0869, instance_loss: 0.0244, weighted_loss: 0.0681, label: 1, bag_size: 23\n",
      "batch 259, loss: 2.3660, instance_loss: 2.2335, weighted_loss: 2.3262, label: 0, bag_size: 27\n",
      "batch 279, loss: 0.9364, instance_loss: 0.6593, weighted_loss: 0.8532, label: 0, bag_size: 28\n",
      "batch 299, loss: 0.3017, instance_loss: 1.0087, weighted_loss: 0.5138, label: 1, bag_size: 29\n",
      "batch 319, loss: 0.5642, instance_loss: 0.0546, weighted_loss: 0.4113, label: 1, bag_size: 79\n",
      "batch 339, loss: 0.0004, instance_loss: 0.0251, weighted_loss: 0.0078, label: 1, bag_size: 21\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0059, weighted_loss: 0.0018, label: 0, bag_size: 65\n",
      "batch 379, loss: 0.1244, instance_loss: 0.0152, weighted_loss: 0.0917, label: 1, bag_size: 92\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0127, weighted_loss: 0.0038, label: 1, bag_size: 107\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 1, bag_size: 73\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 42\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 0, bag_size: 62\n",
      "batch 479, loss: 0.2497, instance_loss: 0.9506, weighted_loss: 0.4600, label: 1, bag_size: 24\n",
      "batch 499, loss: 0.0002, instance_loss: 0.0110, weighted_loss: 0.0035, label: 0, bag_size: 94\n",
      "batch 519, loss: 0.0002, instance_loss: 0.0008, weighted_loss: 0.0004, label: 1, bag_size: 51\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0021, weighted_loss: 0.0006, label: 1, bag_size: 80\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0439, weighted_loss: 0.0132, label: 1, bag_size: 136\n",
      "batch 579, loss: 0.0277, instance_loss: 0.1353, weighted_loss: 0.0600, label: 0, bag_size: 57\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0058, weighted_loss: 0.0018, label: 1, bag_size: 21\n",
      "batch 619, loss: 0.0727, instance_loss: 0.3329, weighted_loss: 0.1508, label: 0, bag_size: 46\n",
      "batch 639, loss: 0.0074, instance_loss: 0.0398, weighted_loss: 0.0171, label: 0, bag_size: 73\n",
      "batch 659, loss: 0.0001, instance_loss: 0.0050, weighted_loss: 0.0015, label: 0, bag_size: 25\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9696022727272727: correct 10239/10560\n",
      "class 1 clustering acc 0.8363636363636363: correct 4416/5280\n",
      "Epoch: 18, train_loss: 0.4059, train_clustering_loss:  0.3172, train_error: 0.1212\n",
      "class 0: acc 0.8685897435897436, correct 271/312\n",
      "class 1: acc 0.8879310344827587, correct 309/348\n",
      "\n",
      "Val Set, val_loss: 1.7527, val_error: 0.3095, auc: 0.8604\n",
      "class 0 clustering acc 0.9047619047619048: correct 1216/1344\n",
      "class 1 clustering acc 0.7038690476190477: correct 473/672\n",
      "class 0: acc 0.46153846153846156, correct 18/39\n",
      "class 1: acc 0.8888888888888888, correct 40/45\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0186, weighted_loss: 0.0056, label: 0, bag_size: 90\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0150, weighted_loss: 0.0045, label: 1, bag_size: 34\n",
      "batch 59, loss: 0.0122, instance_loss: 0.0959, weighted_loss: 0.0373, label: 1, bag_size: 107\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0025, weighted_loss: 0.0007, label: 1, bag_size: 89\n",
      "batch 99, loss: 0.0055, instance_loss: 0.0036, weighted_loss: 0.0050, label: 0, bag_size: 78\n",
      "batch 119, loss: 0.0002, instance_loss: 0.0130, weighted_loss: 0.0040, label: 1, bag_size: 48\n",
      "batch 139, loss: 0.0005, instance_loss: 0.0047, weighted_loss: 0.0017, label: 0, bag_size: 85\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0315, weighted_loss: 0.0095, label: 0, bag_size: 107\n",
      "batch 179, loss: 0.0021, instance_loss: 0.0428, weighted_loss: 0.0143, label: 1, bag_size: 88\n",
      "batch 199, loss: 1.7525, instance_loss: 2.4298, weighted_loss: 1.9557, label: 1, bag_size: 51\n",
      "batch 219, loss: 0.0046, instance_loss: 0.0322, weighted_loss: 0.0129, label: 0, bag_size: 52\n",
      "batch 239, loss: 0.0061, instance_loss: 0.0542, weighted_loss: 0.0205, label: 1, bag_size: 44\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 0, bag_size: 83\n",
      "batch 279, loss: 0.0032, instance_loss: 0.0078, weighted_loss: 0.0046, label: 1, bag_size: 75\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0020, weighted_loss: 0.0006, label: 0, bag_size: 86\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0121, weighted_loss: 0.0037, label: 1, bag_size: 31\n",
      "batch 339, loss: 0.0456, instance_loss: 0.0541, weighted_loss: 0.0482, label: 1, bag_size: 85\n",
      "batch 359, loss: 0.0041, instance_loss: 0.0628, weighted_loss: 0.0217, label: 1, bag_size: 92\n",
      "batch 379, loss: 2.3484, instance_loss: 0.7170, weighted_loss: 1.8590, label: 1, bag_size: 53\n",
      "batch 399, loss: 0.0673, instance_loss: 0.5478, weighted_loss: 0.2114, label: 1, bag_size: 69\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 1, bag_size: 76\n",
      "batch 439, loss: 0.0082, instance_loss: 0.0319, weighted_loss: 0.0153, label: 1, bag_size: 118\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 95\n",
      "batch 479, loss: 0.0010, instance_loss: 0.0021, weighted_loss: 0.0013, label: 1, bag_size: 65\n",
      "batch 499, loss: 1.1904, instance_loss: 0.4111, weighted_loss: 0.9566, label: 1, bag_size: 84\n",
      "batch 519, loss: 2.1019, instance_loss: 2.4097, weighted_loss: 2.1943, label: 0, bag_size: 48\n",
      "batch 539, loss: 0.2116, instance_loss: 0.1699, weighted_loss: 0.1991, label: 1, bag_size: 77\n",
      "batch 559, loss: 0.0024, instance_loss: 0.0142, weighted_loss: 0.0060, label: 1, bag_size: 67\n",
      "batch 579, loss: 0.0005, instance_loss: 0.0023, weighted_loss: 0.0010, label: 0, bag_size: 86\n",
      "batch 599, loss: 0.0167, instance_loss: 0.0166, weighted_loss: 0.0167, label: 1, bag_size: 119\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 30\n",
      "batch 639, loss: 0.5623, instance_loss: 1.4319, weighted_loss: 0.8232, label: 0, bag_size: 92\n",
      "batch 659, loss: 0.7389, instance_loss: 1.2287, weighted_loss: 0.8858, label: 1, bag_size: 126\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9733901515151515: correct 10279/10560\n",
      "class 1 clustering acc 0.8829545454545454: correct 4662/5280\n",
      "Epoch: 19, train_loss: 0.2490, train_clustering_loss:  0.2477, train_error: 0.0894\n",
      "class 0: acc 0.8945578231292517, correct 263/294\n",
      "class 1: acc 0.9234972677595629, correct 338/366\n",
      "\n",
      "Val Set, val_loss: 0.8535, val_error: 0.1905, auc: 0.8906\n",
      "class 0 clustering acc 0.9248511904761905: correct 1243/1344\n",
      "class 1 clustering acc 0.7291666666666666: correct 490/672\n",
      "class 0: acc 0.8461538461538461, correct 33/39\n",
      "class 1: acc 0.7777777777777778, correct 35/45\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.8106, instance_loss: 2.0474, weighted_loss: 1.1817, label: 1, bag_size: 62\n",
      "batch 39, loss: 0.0001, instance_loss: 0.2219, weighted_loss: 0.0666, label: 1, bag_size: 41\n",
      "batch 59, loss: 0.0367, instance_loss: 0.0673, weighted_loss: 0.0458, label: 0, bag_size: 28\n",
      "batch 79, loss: 0.0578, instance_loss: 0.0123, weighted_loss: 0.0442, label: 1, bag_size: 58\n",
      "batch 99, loss: 0.3362, instance_loss: 1.0158, weighted_loss: 0.5401, label: 1, bag_size: 91\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0022, weighted_loss: 0.0007, label: 1, bag_size: 80\n",
      "batch 139, loss: 0.8260, instance_loss: 0.7320, weighted_loss: 0.7978, label: 0, bag_size: 48\n",
      "batch 159, loss: 0.0019, instance_loss: 0.0054, weighted_loss: 0.0029, label: 1, bag_size: 67\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0072, weighted_loss: 0.0021, label: 0, bag_size: 75\n",
      "batch 199, loss: 1.9502, instance_loss: 1.0627, weighted_loss: 1.6839, label: 0, bag_size: 95\n",
      "batch 219, loss: 2.6759, instance_loss: 0.0146, weighted_loss: 1.8775, label: 1, bag_size: 25\n",
      "batch 239, loss: 0.2963, instance_loss: 1.9106, weighted_loss: 0.7806, label: 1, bag_size: 24\n",
      "batch 259, loss: 1.1524, instance_loss: 2.8810, weighted_loss: 1.6710, label: 0, bag_size: 41\n",
      "batch 279, loss: 0.0043, instance_loss: 2.2758, weighted_loss: 0.6858, label: 1, bag_size: 17\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0910, weighted_loss: 0.0273, label: 1, bag_size: 32\n",
      "batch 319, loss: 0.1188, instance_loss: 0.0181, weighted_loss: 0.0886, label: 1, bag_size: 36\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0066, weighted_loss: 0.0020, label: 1, bag_size: 24\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0065, weighted_loss: 0.0020, label: 1, bag_size: 24\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0731, weighted_loss: 0.0219, label: 0, bag_size: 122\n",
      "batch 399, loss: 0.0001, instance_loss: 0.0070, weighted_loss: 0.0022, label: 1, bag_size: 85\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 1, bag_size: 80\n",
      "batch 439, loss: 0.0057, instance_loss: 0.5332, weighted_loss: 0.1639, label: 1, bag_size: 69\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0257, weighted_loss: 0.0077, label: 1, bag_size: 16\n",
      "batch 479, loss: 0.0038, instance_loss: 0.1106, weighted_loss: 0.0358, label: 0, bag_size: 107\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0096, weighted_loss: 0.0029, label: 0, bag_size: 31\n",
      "batch 519, loss: 0.0052, instance_loss: 0.0011, weighted_loss: 0.0040, label: 0, bag_size: 90\n",
      "batch 539, loss: 0.0508, instance_loss: 0.5092, weighted_loss: 0.1883, label: 0, bag_size: 99\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0046, weighted_loss: 0.0014, label: 1, bag_size: 98\n",
      "batch 579, loss: 8.3880, instance_loss: 0.9526, weighted_loss: 6.1574, label: 1, bag_size: 52\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0258, weighted_loss: 0.0077, label: 0, bag_size: 78\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 102\n",
      "batch 639, loss: 0.0000, instance_loss: 0.3113, weighted_loss: 0.0934, label: 0, bag_size: 28\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0076, weighted_loss: 0.0023, label: 1, bag_size: 97\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.959280303030303: correct 10130/10560\n",
      "class 1 clustering acc 0.7808712121212121: correct 4123/5280\n",
      "Epoch: 20, train_loss: 0.6642, train_clustering_loss:  0.3920, train_error: 0.1500\n",
      "class 0: acc 0.8463949843260188, correct 270/319\n",
      "class 1: acc 0.8533724340175953, correct 291/341\n",
      "\n",
      "Val Set, val_loss: 1.7031, val_error: 0.2024, auc: 0.8849\n",
      "class 0 clustering acc 0.8943452380952381: correct 1202/1344\n",
      "class 1 clustering acc 0.7633928571428571: correct 513/672\n",
      "class 0: acc 0.8974358974358975, correct 35/39\n",
      "class 1: acc 0.7111111111111111, correct 32/45\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6217, instance_loss: 0.4363, weighted_loss: 0.5661, label: 1, bag_size: 89\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0287, weighted_loss: 0.0086, label: 0, bag_size: 49\n",
      "batch 59, loss: 0.0011, instance_loss: 0.0013, weighted_loss: 0.0012, label: 1, bag_size: 95\n",
      "batch 79, loss: 0.0121, instance_loss: 1.3734, weighted_loss: 0.4205, label: 1, bag_size: 126\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 55\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 62\n",
      "batch 139, loss: 0.0012, instance_loss: 0.1808, weighted_loss: 0.0551, label: 0, bag_size: 96\n",
      "batch 159, loss: 0.0000, instance_loss: 0.5335, weighted_loss: 0.1600, label: 0, bag_size: 115\n",
      "batch 179, loss: 0.0097, instance_loss: 0.1894, weighted_loss: 0.0636, label: 0, bag_size: 64\n",
      "batch 199, loss: 0.0332, instance_loss: 0.0042, weighted_loss: 0.0245, label: 1, bag_size: 115\n",
      "batch 219, loss: 0.0852, instance_loss: 0.0665, weighted_loss: 0.0796, label: 0, bag_size: 58\n",
      "batch 239, loss: 3.1714, instance_loss: 1.1274, weighted_loss: 2.5582, label: 1, bag_size: 59\n",
      "batch 259, loss: 0.0192, instance_loss: 0.0345, weighted_loss: 0.0238, label: 1, bag_size: 30\n",
      "batch 279, loss: 0.0003, instance_loss: 0.0495, weighted_loss: 0.0150, label: 1, bag_size: 112\n",
      "batch 299, loss: 0.0031, instance_loss: 0.0312, weighted_loss: 0.0115, label: 1, bag_size: 76\n",
      "batch 319, loss: 0.0011, instance_loss: 0.0058, weighted_loss: 0.0025, label: 1, bag_size: 74\n",
      "batch 339, loss: 0.0264, instance_loss: 0.0644, weighted_loss: 0.0378, label: 1, bag_size: 24\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0034, weighted_loss: 0.0010, label: 0, bag_size: 78\n",
      "batch 379, loss: 0.0001, instance_loss: 0.0046, weighted_loss: 0.0015, label: 1, bag_size: 62\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 19\n",
      "batch 419, loss: 0.1411, instance_loss: 0.1231, weighted_loss: 0.1357, label: 1, bag_size: 31\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 35\n",
      "batch 459, loss: 0.0241, instance_loss: 0.1261, weighted_loss: 0.0547, label: 0, bag_size: 75\n",
      "batch 479, loss: 0.0029, instance_loss: 0.0567, weighted_loss: 0.0190, label: 1, bag_size: 102\n",
      "batch 499, loss: 0.6694, instance_loss: 0.0586, weighted_loss: 0.4862, label: 1, bag_size: 84\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 128\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 20\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 0, bag_size: 113\n",
      "batch 579, loss: 0.0052, instance_loss: 0.0081, weighted_loss: 0.0061, label: 1, bag_size: 31\n",
      "batch 599, loss: 0.0503, instance_loss: 1.1213, weighted_loss: 0.3716, label: 0, bag_size: 35\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0060, weighted_loss: 0.0018, label: 1, bag_size: 85\n",
      "batch 639, loss: 0.0085, instance_loss: 0.0107, weighted_loss: 0.0092, label: 1, bag_size: 79\n",
      "batch 659, loss: 0.0302, instance_loss: 0.5358, weighted_loss: 0.1819, label: 0, bag_size: 122\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9703598484848485: correct 10247/10560\n",
      "class 1 clustering acc 0.8481060606060606: correct 4478/5280\n",
      "Epoch: 21, train_loss: 0.3656, train_clustering_loss:  0.3071, train_error: 0.0955\n",
      "class 0: acc 0.8944099378881988, correct 288/322\n",
      "class 1: acc 0.9142011834319527, correct 309/338\n",
      "\n",
      "Val Set, val_loss: 0.6867, val_error: 0.2262, auc: 0.8724\n",
      "class 0 clustering acc 0.9196428571428571: correct 1236/1344\n",
      "class 1 clustering acc 0.7366071428571429: correct 495/672\n",
      "class 0: acc 0.7435897435897436, correct 29/39\n",
      "class 1: acc 0.8, correct 36/45\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0377, instance_loss: 0.8794, weighted_loss: 0.2902, label: 1, bag_size: 111\n",
      "batch 39, loss: 0.0109, instance_loss: 0.0138, weighted_loss: 0.0118, label: 1, bag_size: 78\n",
      "batch 59, loss: 0.1877, instance_loss: 0.5913, weighted_loss: 0.3088, label: 1, bag_size: 48\n",
      "batch 79, loss: 0.0205, instance_loss: 0.0878, weighted_loss: 0.0407, label: 0, bag_size: 94\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 114\n",
      "batch 119, loss: 0.0779, instance_loss: 1.1287, weighted_loss: 0.3931, label: 1, bag_size: 50\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0018, weighted_loss: 0.0006, label: 0, bag_size: 110\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 0, bag_size: 94\n",
      "batch 179, loss: 1.7079, instance_loss: 0.9204, weighted_loss: 1.4717, label: 1, bag_size: 59\n",
      "batch 199, loss: 0.0944, instance_loss: 0.0159, weighted_loss: 0.0708, label: 1, bag_size: 23\n",
      "batch 219, loss: 0.0018, instance_loss: 0.0060, weighted_loss: 0.0030, label: 0, bag_size: 102\n",
      "batch 239, loss: 15.1721, instance_loss: 3.9056, weighted_loss: 11.7921, label: 1, bag_size: 57\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0023, weighted_loss: 0.0007, label: 0, bag_size: 115\n",
      "batch 279, loss: 0.0000, instance_loss: 0.1495, weighted_loss: 0.0449, label: 0, bag_size: 93\n",
      "batch 299, loss: 0.0000, instance_loss: 2.3456, weighted_loss: 0.7037, label: 0, bag_size: 95\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0031, weighted_loss: 0.0009, label: 1, bag_size: 67\n",
      "batch 339, loss: 0.0000, instance_loss: 0.3644, weighted_loss: 0.1093, label: 1, bag_size: 17\n",
      "batch 359, loss: 0.0006, instance_loss: 0.4142, weighted_loss: 0.1247, label: 1, bag_size: 44\n",
      "batch 379, loss: 0.0034, instance_loss: 1.8131, weighted_loss: 0.5463, label: 1, bag_size: 21\n",
      "batch 399, loss: 0.0010, instance_loss: 0.0830, weighted_loss: 0.0256, label: 1, bag_size: 64\n",
      "batch 419, loss: 0.5806, instance_loss: 1.4592, weighted_loss: 0.8442, label: 0, bag_size: 35\n",
      "batch 439, loss: 0.1964, instance_loss: 0.5516, weighted_loss: 0.3030, label: 1, bag_size: 86\n",
      "batch 459, loss: 0.0000, instance_loss: 0.3391, weighted_loss: 0.1017, label: 1, bag_size: 74\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0663, weighted_loss: 0.0199, label: 1, bag_size: 95\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0099, weighted_loss: 0.0030, label: 0, bag_size: 58\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0245, weighted_loss: 0.0073, label: 1, bag_size: 101\n",
      "batch 539, loss: 0.0023, instance_loss: 0.1526, weighted_loss: 0.0474, label: 1, bag_size: 21\n",
      "batch 559, loss: 0.0005, instance_loss: 0.0429, weighted_loss: 0.0132, label: 0, bag_size: 59\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0558, weighted_loss: 0.0168, label: 1, bag_size: 112\n",
      "batch 599, loss: 0.0245, instance_loss: 0.1644, weighted_loss: 0.0665, label: 1, bag_size: 94\n",
      "batch 619, loss: 0.0002, instance_loss: 0.1960, weighted_loss: 0.0589, label: 1, bag_size: 29\n",
      "batch 639, loss: 0.7291, instance_loss: 0.1190, weighted_loss: 0.5461, label: 1, bag_size: 48\n",
      "batch 659, loss: 0.0035, instance_loss: 0.0080, weighted_loss: 0.0048, label: 0, bag_size: 67\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9565340909090909: correct 10101/10560\n",
      "class 1 clustering acc 0.7765151515151515: correct 4100/5280\n",
      "Epoch: 22, train_loss: 0.5441, train_clustering_loss:  0.4120, train_error: 0.1348\n",
      "class 0: acc 0.8776758409785933, correct 287/327\n",
      "class 1: acc 0.8528528528528528, correct 284/333\n",
      "\n",
      "Val Set, val_loss: 0.8793, val_error: 0.2381, auc: 0.9014\n",
      "class 0 clustering acc 0.9203869047619048: correct 1237/1344\n",
      "class 1 clustering acc 0.7157738095238095: correct 481/672\n",
      "class 0: acc 0.6153846153846154, correct 24/39\n",
      "class 1: acc 0.8888888888888888, correct 40/45\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0065, instance_loss: 0.0041, weighted_loss: 0.0057, label: 1, bag_size: 111\n",
      "batch 39, loss: 0.0005, instance_loss: 0.0187, weighted_loss: 0.0060, label: 1, bag_size: 71\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 72\n",
      "batch 79, loss: 0.0002, instance_loss: 0.3596, weighted_loss: 0.1080, label: 1, bag_size: 80\n",
      "batch 99, loss: 0.0221, instance_loss: 0.0456, weighted_loss: 0.0291, label: 0, bag_size: 74\n",
      "batch 119, loss: 0.0009, instance_loss: 0.0418, weighted_loss: 0.0132, label: 0, bag_size: 64\n",
      "batch 139, loss: 0.0013, instance_loss: 0.0107, weighted_loss: 0.0041, label: 0, bag_size: 31\n",
      "batch 159, loss: 0.2717, instance_loss: 1.2497, weighted_loss: 0.5651, label: 0, bag_size: 42\n",
      "batch 179, loss: 10.1155, instance_loss: 3.2257, weighted_loss: 8.0486, label: 0, bag_size: 77\n",
      "batch 199, loss: 0.2375, instance_loss: 0.3114, weighted_loss: 0.2597, label: 1, bag_size: 24\n",
      "batch 219, loss: 0.0003, instance_loss: 0.2810, weighted_loss: 0.0845, label: 0, bag_size: 53\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 81\n",
      "batch 259, loss: 0.0046, instance_loss: 0.0231, weighted_loss: 0.0101, label: 0, bag_size: 31\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 89\n",
      "batch 299, loss: 0.0059, instance_loss: 0.0076, weighted_loss: 0.0064, label: 0, bag_size: 79\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 0, bag_size: 115\n",
      "batch 339, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 44\n",
      "batch 359, loss: 0.0002, instance_loss: 0.0021, weighted_loss: 0.0008, label: 1, bag_size: 94\n",
      "batch 379, loss: 0.3541, instance_loss: 0.4306, weighted_loss: 0.3770, label: 1, bag_size: 28\n",
      "batch 399, loss: 0.0319, instance_loss: 0.0144, weighted_loss: 0.0266, label: 1, bag_size: 93\n",
      "batch 419, loss: 0.0005, instance_loss: 0.0032, weighted_loss: 0.0013, label: 0, bag_size: 18\n",
      "batch 439, loss: 0.0002, instance_loss: 0.0163, weighted_loss: 0.0050, label: 1, bag_size: 31\n",
      "batch 459, loss: 0.0283, instance_loss: 0.0596, weighted_loss: 0.0377, label: 1, bag_size: 57\n",
      "batch 479, loss: 1.4192, instance_loss: 1.0129, weighted_loss: 1.2973, label: 0, bag_size: 35\n",
      "batch 499, loss: 0.0003, instance_loss: 0.0102, weighted_loss: 0.0033, label: 1, bag_size: 24\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0092, weighted_loss: 0.0028, label: 1, bag_size: 41\n",
      "batch 539, loss: 0.0006, instance_loss: 0.0047, weighted_loss: 0.0018, label: 0, bag_size: 35\n",
      "batch 559, loss: 0.0002, instance_loss: 0.0117, weighted_loss: 0.0037, label: 0, bag_size: 88\n",
      "batch 579, loss: 0.0003, instance_loss: 0.0120, weighted_loss: 0.0038, label: 1, bag_size: 41\n",
      "batch 599, loss: 3.2866, instance_loss: 2.6383, weighted_loss: 3.0922, label: 1, bag_size: 59\n",
      "batch 619, loss: 0.1758, instance_loss: 0.4036, weighted_loss: 0.2441, label: 0, bag_size: 60\n",
      "batch 639, loss: 0.0002, instance_loss: 0.0207, weighted_loss: 0.0063, label: 1, bag_size: 33\n",
      "batch 659, loss: 0.0004, instance_loss: 0.0027, weighted_loss: 0.0011, label: 0, bag_size: 90\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9753787878787878: correct 10300/10560\n",
      "class 1 clustering acc 0.868560606060606: correct 4586/5280\n",
      "Epoch: 23, train_loss: 0.2491, train_clustering_loss:  0.2437, train_error: 0.0788\n",
      "class 0: acc 0.9131832797427653, correct 284/311\n",
      "class 1: acc 0.9283667621776505, correct 324/349\n",
      "\n",
      "Val Set, val_loss: 1.1541, val_error: 0.2024, auc: 0.8570\n",
      "class 0 clustering acc 0.9144345238095238: correct 1229/1344\n",
      "class 1 clustering acc 0.78125: correct 525/672\n",
      "class 0: acc 0.9743589743589743, correct 38/39\n",
      "class 1: acc 0.6444444444444445, correct 29/45\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.1650, weighted_loss: 0.0496, label: 1, bag_size: 24\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0226, weighted_loss: 0.0068, label: 0, bag_size: 27\n",
      "batch 59, loss: 0.0002, instance_loss: 0.0013, weighted_loss: 0.0005, label: 0, bag_size: 33\n",
      "batch 79, loss: 0.0013, instance_loss: 0.0138, weighted_loss: 0.0051, label: 1, bag_size: 72\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0015, weighted_loss: 0.0005, label: 0, bag_size: 24\n",
      "batch 119, loss: 1.5055, instance_loss: 0.8261, weighted_loss: 1.3017, label: 0, bag_size: 28\n",
      "batch 139, loss: 0.0030, instance_loss: 0.0040, weighted_loss: 0.0033, label: 0, bag_size: 37\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0076, weighted_loss: 0.0023, label: 1, bag_size: 56\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 33\n",
      "batch 199, loss: 0.0054, instance_loss: 0.0064, weighted_loss: 0.0057, label: 1, bag_size: 31\n",
      "batch 219, loss: 0.0833, instance_loss: 0.0243, weighted_loss: 0.0656, label: 0, bag_size: 79\n",
      "batch 239, loss: 0.0011, instance_loss: 0.0022, weighted_loss: 0.0014, label: 0, bag_size: 50\n",
      "batch 259, loss: 0.0005, instance_loss: 0.0025, weighted_loss: 0.0011, label: 0, bag_size: 95\n",
      "batch 279, loss: 0.0018, instance_loss: 0.0034, weighted_loss: 0.0023, label: 1, bag_size: 20\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 72\n",
      "batch 319, loss: 3.2344, instance_loss: 3.4001, weighted_loss: 3.2841, label: 1, bag_size: 60\n",
      "batch 339, loss: 0.0022, instance_loss: 0.0006, weighted_loss: 0.0017, label: 0, bag_size: 86\n",
      "batch 359, loss: 0.0692, instance_loss: 0.0384, weighted_loss: 0.0600, label: 1, bag_size: 87\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0060, weighted_loss: 0.0018, label: 0, bag_size: 87\n",
      "batch 399, loss: 0.0010, instance_loss: 0.4303, weighted_loss: 0.1298, label: 1, bag_size: 28\n",
      "batch 419, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 85\n",
      "batch 439, loss: 2.4589, instance_loss: 0.6416, weighted_loss: 1.9137, label: 1, bag_size: 25\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 93\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0031, weighted_loss: 0.0009, label: 1, bag_size: 84\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0031, weighted_loss: 0.0009, label: 0, bag_size: 49\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0020, weighted_loss: 0.0006, label: 0, bag_size: 64\n",
      "batch 539, loss: 0.0000, instance_loss: 0.2428, weighted_loss: 0.0728, label: 1, bag_size: 92\n",
      "batch 559, loss: 0.0010, instance_loss: 0.0170, weighted_loss: 0.0058, label: 0, bag_size: 126\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0055, weighted_loss: 0.0017, label: 1, bag_size: 31\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0142, weighted_loss: 0.0043, label: 1, bag_size: 21\n",
      "batch 619, loss: 0.0004, instance_loss: 0.0048, weighted_loss: 0.0017, label: 1, bag_size: 35\n",
      "batch 639, loss: 0.0084, instance_loss: 0.0258, weighted_loss: 0.0136, label: 1, bag_size: 41\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0037, weighted_loss: 0.0011, label: 0, bag_size: 62\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.975: correct 10296/10560\n",
      "class 1 clustering acc 0.8696969696969697: correct 4592/5280\n",
      "Epoch: 24, train_loss: 0.2745, train_clustering_loss:  0.2388, train_error: 0.0909\n",
      "class 0: acc 0.9195402298850575, correct 320/348\n",
      "class 1: acc 0.8974358974358975, correct 280/312\n",
      "\n",
      "Val Set, val_loss: 0.9718, val_error: 0.2143, auc: 0.8695\n",
      "class 0 clustering acc 0.90625: correct 1218/1344\n",
      "class 1 clustering acc 0.7589285714285714: correct 510/672\n",
      "class 0: acc 0.7435897435897436, correct 29/39\n",
      "class 1: acc 0.8222222222222222, correct 37/45\n",
      "EarlyStopping counter: 12 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.0007, weighted_loss: 0.0003, label: 0, bag_size: 27\n",
      "batch 39, loss: 4.5576, instance_loss: 1.3538, weighted_loss: 3.5964, label: 1, bag_size: 17\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 75\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 1, bag_size: 76\n",
      "batch 99, loss: 0.0063, instance_loss: 0.0444, weighted_loss: 0.0177, label: 1, bag_size: 69\n",
      "batch 119, loss: 0.0003, instance_loss: 0.0065, weighted_loss: 0.0021, label: 1, bag_size: 16\n",
      "batch 139, loss: 0.0083, instance_loss: 0.0401, weighted_loss: 0.0179, label: 0, bag_size: 21\n",
      "batch 159, loss: 0.0050, instance_loss: 0.0138, weighted_loss: 0.0076, label: 0, bag_size: 24\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0023, weighted_loss: 0.0007, label: 1, bag_size: 62\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0023, weighted_loss: 0.0007, label: 0, bag_size: 36\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 36\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0114, weighted_loss: 0.0034, label: 1, bag_size: 118\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0022, weighted_loss: 0.0007, label: 1, bag_size: 95\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0043, weighted_loss: 0.0013, label: 1, bag_size: 48\n",
      "batch 299, loss: 0.0006, instance_loss: 0.0003, weighted_loss: 0.0005, label: 1, bag_size: 107\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0009, weighted_loss: 0.0004, label: 0, bag_size: 95\n",
      "batch 339, loss: 0.2667, instance_loss: 0.0295, weighted_loss: 0.1955, label: 1, bag_size: 82\n",
      "batch 359, loss: 0.0035, instance_loss: 0.0494, weighted_loss: 0.0173, label: 0, bag_size: 99\n",
      "batch 379, loss: 0.0030, instance_loss: 0.0612, weighted_loss: 0.0205, label: 0, bag_size: 35\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 48\n",
      "batch 419, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 25\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0201, weighted_loss: 0.0061, label: 1, bag_size: 107\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 83\n",
      "batch 479, loss: 0.0007, instance_loss: 0.0005, weighted_loss: 0.0006, label: 0, bag_size: 47\n",
      "batch 499, loss: 0.0052, instance_loss: 0.0021, weighted_loss: 0.0043, label: 0, bag_size: 39\n",
      "batch 519, loss: 0.0563, instance_loss: 0.0496, weighted_loss: 0.0543, label: 0, bag_size: 49\n",
      "batch 539, loss: 0.0090, instance_loss: 0.1389, weighted_loss: 0.0480, label: 0, bag_size: 45\n",
      "batch 559, loss: 0.0016, instance_loss: 0.0084, weighted_loss: 0.0037, label: 1, bag_size: 82\n",
      "batch 579, loss: 0.0016, instance_loss: 0.1510, weighted_loss: 0.0464, label: 0, bag_size: 72\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0041, weighted_loss: 0.0012, label: 0, bag_size: 78\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0092, weighted_loss: 0.0028, label: 0, bag_size: 108\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0083, weighted_loss: 0.0025, label: 0, bag_size: 24\n",
      "batch 659, loss: 0.0001, instance_loss: 0.0017, weighted_loss: 0.0005, label: 0, bag_size: 81\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9753787878787878: correct 10300/10560\n",
      "class 1 clustering acc 0.884280303030303: correct 4669/5280\n",
      "Epoch: 25, train_loss: 0.3144, train_clustering_loss:  0.2329, train_error: 0.0939\n",
      "class 0: acc 0.9142857142857143, correct 320/350\n",
      "class 1: acc 0.896774193548387, correct 278/310\n",
      "\n",
      "Val Set, val_loss: 1.4720, val_error: 0.2262, auc: 0.8758\n",
      "class 0 clustering acc 0.9122023809523809: correct 1226/1344\n",
      "class 1 clustering acc 0.7589285714285714: correct 510/672\n",
      "class 0: acc 0.7435897435897436, correct 29/39\n",
      "class 1: acc 0.8, correct 36/45\n",
      "EarlyStopping counter: 13 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0017, weighted_loss: 0.0005, label: 0, bag_size: 20\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 37\n",
      "batch 59, loss: 0.0000, instance_loss: 1.5920, weighted_loss: 0.4776, label: 1, bag_size: 53\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0255, weighted_loss: 0.0077, label: 1, bag_size: 29\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0280, weighted_loss: 0.0084, label: 0, bag_size: 84\n",
      "batch 119, loss: 0.0027, instance_loss: 0.0148, weighted_loss: 0.0063, label: 1, bag_size: 58\n",
      "batch 139, loss: 0.0005, instance_loss: 0.0148, weighted_loss: 0.0048, label: 0, bag_size: 27\n",
      "batch 159, loss: 0.0287, instance_loss: 0.0141, weighted_loss: 0.0244, label: 0, bag_size: 47\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0033, weighted_loss: 0.0010, label: 1, bag_size: 110\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 90\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0018, weighted_loss: 0.0007, label: 0, bag_size: 67\n",
      "batch 239, loss: 0.0129, instance_loss: 0.8251, weighted_loss: 0.2566, label: 0, bag_size: 48\n",
      "batch 259, loss: 0.0012, instance_loss: 0.0075, weighted_loss: 0.0031, label: 1, bag_size: 94\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0109, weighted_loss: 0.0033, label: 0, bag_size: 42\n",
      "batch 299, loss: 0.0004, instance_loss: 0.0055, weighted_loss: 0.0020, label: 1, bag_size: 35\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 85\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 42\n",
      "batch 359, loss: 0.0006, instance_loss: 0.0412, weighted_loss: 0.0128, label: 0, bag_size: 33\n",
      "batch 379, loss: 0.0127, instance_loss: 0.0269, weighted_loss: 0.0170, label: 1, bag_size: 51\n",
      "batch 399, loss: 0.0001, instance_loss: 0.0095, weighted_loss: 0.0030, label: 1, bag_size: 119\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0511, weighted_loss: 0.0153, label: 0, bag_size: 80\n",
      "batch 439, loss: 0.0378, instance_loss: 0.2401, weighted_loss: 0.0985, label: 0, bag_size: 35\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0266, weighted_loss: 0.0080, label: 0, bag_size: 36\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0489, weighted_loss: 0.0147, label: 0, bag_size: 41\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 74\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0056, weighted_loss: 0.0017, label: 1, bag_size: 68\n",
      "batch 539, loss: 0.1820, instance_loss: 0.0634, weighted_loss: 0.1464, label: 1, bag_size: 18\n",
      "batch 559, loss: 0.0000, instance_loss: 0.1280, weighted_loss: 0.0384, label: 0, bag_size: 108\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0479, weighted_loss: 0.0144, label: 1, bag_size: 30\n",
      "batch 599, loss: 0.0000, instance_loss: 0.1914, weighted_loss: 0.0574, label: 0, bag_size: 66\n",
      "batch 619, loss: 12.6141, instance_loss: 0.0931, weighted_loss: 8.8578, label: 1, bag_size: 76\n",
      "batch 639, loss: 0.0027, instance_loss: 0.3563, weighted_loss: 0.1088, label: 0, bag_size: 31\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0649, weighted_loss: 0.0195, label: 0, bag_size: 51\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9664772727272727: correct 10206/10560\n",
      "class 1 clustering acc 0.8248106060606061: correct 4355/5280\n",
      "Epoch: 26, train_loss: 0.7194, train_clustering_loss:  0.3609, train_error: 0.1106\n",
      "class 0: acc 0.8931750741839762, correct 301/337\n",
      "class 1: acc 0.8854489164086687, correct 286/323\n",
      "\n",
      "Val Set, val_loss: 3.9596, val_error: 0.3452, auc: 0.8826\n",
      "class 0 clustering acc 0.9144345238095238: correct 1229/1344\n",
      "class 1 clustering acc 0.7202380952380952: correct 484/672\n",
      "class 0: acc 1.0, correct 39/39\n",
      "class 1: acc 0.35555555555555557, correct 16/45\n",
      "EarlyStopping counter: 14 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0019, weighted_loss: 0.0006, label: 1, bag_size: 58\n",
      "batch 39, loss: 0.0001, instance_loss: 0.2136, weighted_loss: 0.0641, label: 1, bag_size: 110\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0134, weighted_loss: 0.0040, label: 1, bag_size: 68\n",
      "batch 79, loss: 0.0007, instance_loss: 0.0148, weighted_loss: 0.0049, label: 1, bag_size: 84\n",
      "batch 99, loss: 0.0210, instance_loss: 0.9748, weighted_loss: 0.3071, label: 1, bag_size: 17\n",
      "batch 119, loss: 0.0016, instance_loss: 0.2636, weighted_loss: 0.0802, label: 0, bag_size: 68\n",
      "batch 139, loss: 3.0163, instance_loss: 0.4416, weighted_loss: 2.2439, label: 0, bag_size: 39\n",
      "batch 159, loss: 0.0122, instance_loss: 0.0399, weighted_loss: 0.0206, label: 1, bag_size: 79\n",
      "batch 179, loss: 2.2048, instance_loss: 0.2474, weighted_loss: 1.6176, label: 1, bag_size: 68\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 33\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 44\n",
      "batch 239, loss: 0.0000, instance_loss: 0.9754, weighted_loss: 0.2926, label: 1, bag_size: 74\n",
      "batch 259, loss: 5.2207, instance_loss: 1.4479, weighted_loss: 4.0889, label: 1, bag_size: 44\n",
      "batch 279, loss: 7.9225, instance_loss: 4.2661, weighted_loss: 6.8256, label: 0, bag_size: 18\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0027, weighted_loss: 0.0008, label: 1, bag_size: 53\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0527, weighted_loss: 0.0158, label: 0, bag_size: 37\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0062, weighted_loss: 0.0019, label: 0, bag_size: 50\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0038, weighted_loss: 0.0011, label: 0, bag_size: 49\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 47\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0376, weighted_loss: 0.0113, label: 1, bag_size: 84\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 0, bag_size: 81\n",
      "batch 439, loss: 0.0102, instance_loss: 0.0564, weighted_loss: 0.0240, label: 1, bag_size: 57\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 65\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0060, weighted_loss: 0.0018, label: 0, bag_size: 21\n",
      "batch 499, loss: 0.0003, instance_loss: 0.1345, weighted_loss: 0.0405, label: 1, bag_size: 23\n",
      "batch 519, loss: 0.0003, instance_loss: 0.0201, weighted_loss: 0.0062, label: 1, bag_size: 36\n",
      "batch 539, loss: 0.2991, instance_loss: 1.1232, weighted_loss: 0.5464, label: 1, bag_size: 33\n",
      "batch 559, loss: 0.0835, instance_loss: 0.0363, weighted_loss: 0.0693, label: 1, bag_size: 92\n",
      "batch 579, loss: 0.0050, instance_loss: 0.0129, weighted_loss: 0.0074, label: 1, bag_size: 48\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0265, weighted_loss: 0.0080, label: 0, bag_size: 115\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0111, weighted_loss: 0.0034, label: 0, bag_size: 47\n",
      "batch 639, loss: 2.7000, instance_loss: 2.7885, weighted_loss: 2.7265, label: 1, bag_size: 79\n",
      "batch 659, loss: 9.0506, instance_loss: 0.4412, weighted_loss: 6.4678, label: 1, bag_size: 104\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9693181818181819: correct 10236/10560\n",
      "class 1 clustering acc 0.8464015151515152: correct 4469/5280\n",
      "Epoch: 27, train_loss: 0.5217, train_clustering_loss:  0.2977, train_error: 0.1212\n",
      "class 0: acc 0.8835820895522388, correct 296/335\n",
      "class 1: acc 0.8738461538461538, correct 284/325\n",
      "\n",
      "Val Set, val_loss: 1.4490, val_error: 0.1905, auc: 0.8712\n",
      "class 0 clustering acc 0.9337797619047619: correct 1255/1344\n",
      "class 1 clustering acc 0.5208333333333334: correct 350/672\n",
      "class 0: acc 0.8461538461538461, correct 33/39\n",
      "class 1: acc 0.7777777777777778, correct 35/45\n",
      "EarlyStopping counter: 15 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0066, weighted_loss: 0.0020, label: 0, bag_size: 106\n",
      "batch 39, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 1, bag_size: 128\n",
      "batch 59, loss: 0.0000, instance_loss: 0.1345, weighted_loss: 0.0404, label: 0, bag_size: 93\n",
      "batch 79, loss: 0.0002, instance_loss: 0.0140, weighted_loss: 0.0043, label: 0, bag_size: 71\n",
      "batch 99, loss: 0.0080, instance_loss: 1.5839, weighted_loss: 0.4808, label: 1, bag_size: 25\n",
      "batch 119, loss: 0.9752, instance_loss: 0.2613, weighted_loss: 0.7610, label: 1, bag_size: 84\n",
      "batch 139, loss: 0.0199, instance_loss: 0.6898, weighted_loss: 0.2209, label: 1, bag_size: 98\n",
      "batch 159, loss: 0.0000, instance_loss: 0.1470, weighted_loss: 0.0441, label: 0, bag_size: 75\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0688, weighted_loss: 0.0206, label: 1, bag_size: 118\n",
      "batch 199, loss: 0.0056, instance_loss: 0.0800, weighted_loss: 0.0279, label: 1, bag_size: 38\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0039, weighted_loss: 0.0012, label: 0, bag_size: 24\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0037, weighted_loss: 0.0011, label: 1, bag_size: 95\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0031, weighted_loss: 0.0009, label: 1, bag_size: 30\n",
      "batch 279, loss: 1.0845, instance_loss: 2.0198, weighted_loss: 1.3651, label: 1, bag_size: 39\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0095, weighted_loss: 0.0028, label: 0, bag_size: 67\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0841, weighted_loss: 0.0252, label: 0, bag_size: 46\n",
      "batch 339, loss: 1.6620, instance_loss: 2.2871, weighted_loss: 1.8495, label: 1, bag_size: 28\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0024, weighted_loss: 0.0007, label: 0, bag_size: 55\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0319, weighted_loss: 0.0096, label: 1, bag_size: 72\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0000, label: 0, bag_size: 35\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0020, weighted_loss: 0.0006, label: 1, bag_size: 49\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0017, weighted_loss: 0.0005, label: 0, bag_size: 92\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0021, weighted_loss: 0.0006, label: 0, bag_size: 92\n",
      "batch 479, loss: 0.0000, instance_loss: 0.1362, weighted_loss: 0.0408, label: 1, bag_size: 58\n",
      "batch 499, loss: 0.0000, instance_loss: 1.1131, weighted_loss: 0.3339, label: 1, bag_size: 38\n",
      "batch 519, loss: 0.0183, instance_loss: 0.0110, weighted_loss: 0.0161, label: 1, bag_size: 68\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0029, weighted_loss: 0.0009, label: 1, bag_size: 31\n",
      "batch 559, loss: 0.0048, instance_loss: 0.0047, weighted_loss: 0.0048, label: 1, bag_size: 16\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0621, weighted_loss: 0.0186, label: 0, bag_size: 34\n",
      "batch 599, loss: 0.0000, instance_loss: 0.2564, weighted_loss: 0.0769, label: 1, bag_size: 38\n",
      "batch 619, loss: 0.0001, instance_loss: 0.1875, weighted_loss: 0.0563, label: 0, bag_size: 79\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0250, weighted_loss: 0.0075, label: 0, bag_size: 35\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0069, weighted_loss: 0.0021, label: 0, bag_size: 56\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9585227272727272: correct 10122/10560\n",
      "class 1 clustering acc 0.8393939393939394: correct 4432/5280\n",
      "Epoch: 28, train_loss: 0.6899, train_clustering_loss:  0.3362, train_error: 0.1242\n",
      "class 0: acc 0.8809523809523809, correct 296/336\n",
      "class 1: acc 0.8703703703703703, correct 282/324\n",
      "\n",
      "Val Set, val_loss: 3.5127, val_error: 0.3452, auc: 0.8798\n",
      "class 0 clustering acc 0.9174107142857143: correct 1233/1344\n",
      "class 1 clustering acc 0.5119047619047619: correct 344/672\n",
      "class 0: acc 0.9743589743589743, correct 38/39\n",
      "class 1: acc 0.37777777777777777, correct 17/45\n",
      "EarlyStopping counter: 16 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0287, weighted_loss: 0.0086, label: 0, bag_size: 103\n",
      "batch 39, loss: 0.0000, instance_loss: 0.2886, weighted_loss: 0.0866, label: 0, bag_size: 62\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0454, weighted_loss: 0.0136, label: 1, bag_size: 76\n",
      "batch 79, loss: 0.0000, instance_loss: 0.1654, weighted_loss: 0.0496, label: 0, bag_size: 95\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0130, weighted_loss: 0.0039, label: 1, bag_size: 50\n",
      "batch 119, loss: 1.3805, instance_loss: 1.3614, weighted_loss: 1.3748, label: 1, bag_size: 41\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0019, weighted_loss: 0.0006, label: 1, bag_size: 44\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0469, weighted_loss: 0.0141, label: 0, bag_size: 27\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0034, weighted_loss: 0.0010, label: 0, bag_size: 87\n",
      "batch 199, loss: 6.3678, instance_loss: 0.6232, weighted_loss: 4.6445, label: 0, bag_size: 28\n",
      "batch 219, loss: 1.3513, instance_loss: 0.2897, weighted_loss: 1.0328, label: 1, bag_size: 31\n",
      "batch 239, loss: 0.0005, instance_loss: 0.0121, weighted_loss: 0.0039, label: 1, bag_size: 80\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0048, weighted_loss: 0.0014, label: 0, bag_size: 88\n",
      "batch 279, loss: 0.0001, instance_loss: 0.2488, weighted_loss: 0.0747, label: 0, bag_size: 88\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 65\n",
      "batch 319, loss: 0.0606, instance_loss: 0.2021, weighted_loss: 0.1031, label: 0, bag_size: 78\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0121, weighted_loss: 0.0037, label: 0, bag_size: 112\n",
      "batch 359, loss: 11.8023, instance_loss: 4.1242, weighted_loss: 9.4989, label: 0, bag_size: 66\n",
      "batch 379, loss: 0.0234, instance_loss: 0.3546, weighted_loss: 0.1227, label: 0, bag_size: 66\n",
      "batch 399, loss: 0.0007, instance_loss: 0.0032, weighted_loss: 0.0014, label: 1, bag_size: 115\n",
      "batch 419, loss: 0.2246, instance_loss: 0.0843, weighted_loss: 0.1825, label: 1, bag_size: 53\n",
      "batch 439, loss: 0.0774, instance_loss: 0.2136, weighted_loss: 0.1183, label: 1, bag_size: 68\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 0, bag_size: 61\n",
      "batch 479, loss: 0.0005, instance_loss: 0.0181, weighted_loss: 0.0058, label: 1, bag_size: 67\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 26\n",
      "batch 519, loss: 0.0028, instance_loss: 0.0040, weighted_loss: 0.0032, label: 0, bag_size: 30\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0153, weighted_loss: 0.0046, label: 0, bag_size: 79\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 58\n",
      "batch 579, loss: 1.7101, instance_loss: 0.7918, weighted_loss: 1.4346, label: 0, bag_size: 89\n",
      "batch 599, loss: 0.0021, instance_loss: 0.0054, weighted_loss: 0.0031, label: 0, bag_size: 49\n",
      "batch 619, loss: 0.0362, instance_loss: 0.0179, weighted_loss: 0.0307, label: 1, bag_size: 82\n",
      "batch 639, loss: 0.0005, instance_loss: 0.0030, weighted_loss: 0.0012, label: 0, bag_size: 57\n",
      "batch 659, loss: 0.1812, instance_loss: 0.1122, weighted_loss: 0.1605, label: 1, bag_size: 91\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9736742424242424: correct 10282/10560\n",
      "class 1 clustering acc 0.884469696969697: correct 4670/5280\n",
      "Epoch: 29, train_loss: 0.5242, train_clustering_loss:  0.2453, train_error: 0.0864\n",
      "class 0: acc 0.9154078549848943, correct 303/331\n",
      "class 1: acc 0.9118541033434651, correct 300/329\n",
      "\n",
      "Val Set, val_loss: 1.2399, val_error: 0.2262, auc: 0.8803\n",
      "class 0 clustering acc 0.9263392857142857: correct 1245/1344\n",
      "class 1 clustering acc 0.7872023809523809: correct 529/672\n",
      "class 0: acc 0.9743589743589743, correct 38/39\n",
      "class 1: acc 0.6, correct 27/45\n",
      "EarlyStopping counter: 17 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0049, weighted_loss: 0.0015, label: 0, bag_size: 37\n",
      "batch 39, loss: 0.0014, instance_loss: 0.0027, weighted_loss: 0.0018, label: 0, bag_size: 46\n",
      "batch 59, loss: 0.1173, instance_loss: 0.1144, weighted_loss: 0.1164, label: 0, bag_size: 51\n",
      "batch 79, loss: 0.0002, instance_loss: 0.0032, weighted_loss: 0.0011, label: 1, bag_size: 57\n",
      "batch 99, loss: 0.0015, instance_loss: 0.0026, weighted_loss: 0.0019, label: 0, bag_size: 57\n",
      "batch 119, loss: 1.9531, instance_loss: 0.7397, weighted_loss: 1.5890, label: 1, bag_size: 41\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 42\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0005, label: 0, bag_size: 108\n",
      "batch 179, loss: 0.0011, instance_loss: 0.0187, weighted_loss: 0.0064, label: 0, bag_size: 66\n",
      "batch 199, loss: 0.0004, instance_loss: 0.0053, weighted_loss: 0.0019, label: 0, bag_size: 29\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0608, weighted_loss: 0.0184, label: 1, bag_size: 58\n",
      "batch 239, loss: 4.5342, instance_loss: 4.5451, weighted_loss: 4.5375, label: 1, bag_size: 96\n",
      "batch 259, loss: 0.0000, instance_loss: 0.8664, weighted_loss: 0.2599, label: 0, bag_size: 50\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 36\n",
      "batch 299, loss: 8.2124, instance_loss: 0.8452, weighted_loss: 6.0022, label: 1, bag_size: 79\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0895, weighted_loss: 0.0269, label: 1, bag_size: 62\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 30\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 26\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0031, weighted_loss: 0.0009, label: 0, bag_size: 128\n",
      "batch 399, loss: 0.0011, instance_loss: 0.2519, weighted_loss: 0.0763, label: 0, bag_size: 67\n",
      "batch 419, loss: 4.5369, instance_loss: 2.5801, weighted_loss: 3.9498, label: 1, bag_size: 63\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 76\n",
      "batch 459, loss: 0.1637, instance_loss: 0.4511, weighted_loss: 0.2499, label: 1, bag_size: 74\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 86\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 33\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0183, weighted_loss: 0.0055, label: 0, bag_size: 29\n",
      "batch 559, loss: 0.0027, instance_loss: 0.0052, weighted_loss: 0.0035, label: 0, bag_size: 49\n",
      "batch 579, loss: 0.8564, instance_loss: 0.5396, weighted_loss: 0.7614, label: 1, bag_size: 28\n",
      "batch 599, loss: 0.0037, instance_loss: 0.0062, weighted_loss: 0.0045, label: 1, bag_size: 60\n",
      "batch 619, loss: 0.1992, instance_loss: 0.0620, weighted_loss: 0.1581, label: 1, bag_size: 127\n",
      "batch 639, loss: 3.8937, instance_loss: 1.0843, weighted_loss: 3.0509, label: 0, bag_size: 78\n",
      "batch 659, loss: 0.0005, instance_loss: 0.2912, weighted_loss: 0.0877, label: 0, bag_size: 78\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9728219696969697: correct 10273/10560\n",
      "class 1 clustering acc 0.8698863636363636: correct 4593/5280\n",
      "Epoch: 30, train_loss: 0.5270, train_clustering_loss:  0.2576, train_error: 0.1061\n",
      "class 0: acc 0.8919753086419753, correct 289/324\n",
      "class 1: acc 0.8958333333333334, correct 301/336\n",
      "\n",
      "Val Set, val_loss: 1.0460, val_error: 0.1667, auc: 0.8906\n",
      "class 0 clustering acc 0.9241071428571429: correct 1242/1344\n",
      "class 1 clustering acc 0.7976190476190477: correct 536/672\n",
      "class 0: acc 0.8974358974358975, correct 35/39\n",
      "class 1: acc 0.7777777777777778, correct 35/45\n",
      "EarlyStopping counter: 18 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0009, instance_loss: 0.0721, weighted_loss: 0.0222, label: 1, bag_size: 94\n",
      "batch 39, loss: 0.0002, instance_loss: 0.0058, weighted_loss: 0.0018, label: 1, bag_size: 56\n",
      "batch 59, loss: 0.0002, instance_loss: 0.0032, weighted_loss: 0.0011, label: 1, bag_size: 30\n",
      "batch 79, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 71\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 89\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0088, weighted_loss: 0.0027, label: 1, bag_size: 21\n",
      "batch 139, loss: 2.8771, instance_loss: 2.1991, weighted_loss: 2.6737, label: 0, bag_size: 58\n",
      "batch 159, loss: 0.1027, instance_loss: 0.0579, weighted_loss: 0.0892, label: 0, bag_size: 39\n",
      "batch 179, loss: 1.1732, instance_loss: 2.0258, weighted_loss: 1.4289, label: 1, bag_size: 50\n",
      "batch 199, loss: 0.0002, instance_loss: 0.0052, weighted_loss: 0.0017, label: 0, bag_size: 29\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0022, weighted_loss: 0.0007, label: 0, bag_size: 46\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0058, weighted_loss: 0.0017, label: 1, bag_size: 87\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 89\n",
      "batch 279, loss: 0.0094, instance_loss: 0.0232, weighted_loss: 0.0135, label: 1, bag_size: 51\n",
      "batch 299, loss: 2.5530, instance_loss: 1.3069, weighted_loss: 2.1792, label: 0, bag_size: 41\n",
      "batch 319, loss: 0.0016, instance_loss: 0.0004, weighted_loss: 0.0013, label: 1, bag_size: 17\n",
      "batch 339, loss: 0.0009, instance_loss: 0.6425, weighted_loss: 0.1934, label: 0, bag_size: 35\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0682, weighted_loss: 0.0206, label: 1, bag_size: 38\n",
      "batch 379, loss: 0.2448, instance_loss: 0.0471, weighted_loss: 0.1855, label: 1, bag_size: 50\n",
      "batch 399, loss: 0.0044, instance_loss: 0.1472, weighted_loss: 0.0472, label: 1, bag_size: 21\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 36\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 64\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 61\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 107\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 30\n",
      "batch 519, loss: 0.0002, instance_loss: 0.0064, weighted_loss: 0.0020, label: 1, bag_size: 96\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 40\n",
      "batch 559, loss: 1.1159, instance_loss: 3.3944, weighted_loss: 1.7995, label: 0, bag_size: 42\n",
      "batch 579, loss: 0.0059, instance_loss: 0.0784, weighted_loss: 0.0276, label: 1, bag_size: 83\n",
      "batch 599, loss: 0.0006, instance_loss: 0.0646, weighted_loss: 0.0198, label: 1, bag_size: 91\n",
      "batch 619, loss: 0.0009, instance_loss: 0.0080, weighted_loss: 0.0030, label: 1, bag_size: 90\n",
      "batch 639, loss: 0.1212, instance_loss: 0.0447, weighted_loss: 0.0982, label: 1, bag_size: 84\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0023, weighted_loss: 0.0007, label: 0, bag_size: 29\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9791666666666666: correct 10340/10560\n",
      "class 1 clustering acc 0.8988636363636363: correct 4746/5280\n",
      "Epoch: 31, train_loss: 0.2216, train_clustering_loss:  0.1933, train_error: 0.0788\n",
      "class 0: acc 0.9212827988338192, correct 316/343\n",
      "class 1: acc 0.9211356466876972, correct 292/317\n",
      "\n",
      "Val Set, val_loss: 1.3191, val_error: 0.2024, auc: 0.8587\n",
      "class 0 clustering acc 0.9114583333333334: correct 1225/1344\n",
      "class 1 clustering acc 0.7708333333333334: correct 518/672\n",
      "class 0: acc 0.7692307692307693, correct 30/39\n",
      "class 1: acc 0.8222222222222222, correct 37/45\n",
      "EarlyStopping counter: 19 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0192, instance_loss: 0.0353, weighted_loss: 0.0240, label: 1, bag_size: 48\n",
      "batch 39, loss: 0.0091, instance_loss: 0.4798, weighted_loss: 0.1503, label: 0, bag_size: 35\n",
      "batch 59, loss: 0.0083, instance_loss: 0.1435, weighted_loss: 0.0489, label: 1, bag_size: 56\n",
      "batch 79, loss: 0.0220, instance_loss: 0.0409, weighted_loss: 0.0276, label: 1, bag_size: 64\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 75\n",
      "batch 119, loss: 0.4781, instance_loss: 0.1226, weighted_loss: 0.3714, label: 0, bag_size: 50\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 120\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 128\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0055, weighted_loss: 0.0017, label: 0, bag_size: 87\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 75\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 100\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 33\n",
      "batch 259, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 48\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 30\n",
      "batch 299, loss: 0.1417, instance_loss: 0.0000, weighted_loss: 0.0992, label: 0, bag_size: 31\n",
      "batch 319, loss: 0.0065, instance_loss: 0.0173, weighted_loss: 0.0098, label: 0, bag_size: 103\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0025, weighted_loss: 0.0008, label: 0, bag_size: 77\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 50\n",
      "batch 379, loss: 0.0012, instance_loss: 0.1021, weighted_loss: 0.0315, label: 0, bag_size: 58\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 101\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 89\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0070, weighted_loss: 0.0021, label: 0, bag_size: 63\n",
      "batch 459, loss: 0.0008, instance_loss: 0.0211, weighted_loss: 0.0069, label: 1, bag_size: 100\n",
      "batch 479, loss: 0.0001, instance_loss: 0.3005, weighted_loss: 0.0902, label: 1, bag_size: 111\n",
      "batch 499, loss: 0.0006, instance_loss: 0.0057, weighted_loss: 0.0022, label: 1, bag_size: 33\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 87\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 103\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 1, bag_size: 102\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 63\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 33\n",
      "batch 619, loss: 0.6622, instance_loss: 0.3823, weighted_loss: 0.5782, label: 0, bag_size: 94\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 31\n",
      "batch 659, loss: 0.0002, instance_loss: 0.0078, weighted_loss: 0.0024, label: 1, bag_size: 45\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9760416666666667: correct 10307/10560\n",
      "class 1 clustering acc 0.8831439393939394: correct 4663/5280\n",
      "Epoch: 32, train_loss: 0.2165, train_clustering_loss:  0.2164, train_error: 0.0712\n",
      "class 0: acc 0.9308176100628931, correct 296/318\n",
      "class 1: acc 0.9269005847953217, correct 317/342\n",
      "\n",
      "Val Set, val_loss: 1.4482, val_error: 0.2024, auc: 0.8496\n",
      "class 0 clustering acc 0.9010416666666666: correct 1211/1344\n",
      "class 1 clustering acc 0.7336309523809523: correct 493/672\n",
      "class 0: acc 0.8205128205128205, correct 32/39\n",
      "class 1: acc 0.7777777777777778, correct 35/45\n",
      "EarlyStopping counter: 20 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0038, instance_loss: 0.2797, weighted_loss: 0.0866, label: 0, bag_size: 93\n",
      "batch 39, loss: 0.0171, instance_loss: 0.7102, weighted_loss: 0.2250, label: 1, bag_size: 35\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 30\n",
      "batch 79, loss: 0.1373, instance_loss: 0.3111, weighted_loss: 0.1895, label: 1, bag_size: 75\n",
      "batch 99, loss: 0.0026, instance_loss: 0.0056, weighted_loss: 0.0035, label: 1, bag_size: 17\n",
      "batch 119, loss: 0.0164, instance_loss: 0.0041, weighted_loss: 0.0127, label: 1, bag_size: 76\n",
      "batch 139, loss: 2.0625, instance_loss: 0.6397, weighted_loss: 1.6357, label: 1, bag_size: 59\n",
      "batch 159, loss: 1.0288, instance_loss: 1.0296, weighted_loss: 1.0291, label: 1, bag_size: 84\n",
      "batch 179, loss: 0.5168, instance_loss: 1.9961, weighted_loss: 0.9606, label: 0, bag_size: 21\n",
      "batch 199, loss: 5.9011, instance_loss: 0.5828, weighted_loss: 4.3056, label: 0, bag_size: 45\n",
      "batch 219, loss: 1.2638, instance_loss: 0.3676, weighted_loss: 0.9950, label: 1, bag_size: 80\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0071, weighted_loss: 0.0021, label: 0, bag_size: 73\n",
      "batch 259, loss: 0.0230, instance_loss: 0.0284, weighted_loss: 0.0246, label: 1, bag_size: 67\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 97\n",
      "batch 299, loss: 3.2719, instance_loss: 1.9311, weighted_loss: 2.8697, label: 0, bag_size: 44\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 87\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0026, weighted_loss: 0.0009, label: 0, bag_size: 76\n",
      "batch 359, loss: 0.0006, instance_loss: 0.0003, weighted_loss: 0.0005, label: 1, bag_size: 85\n",
      "batch 379, loss: 0.0004, instance_loss: 0.0007, weighted_loss: 0.0005, label: 1, bag_size: 35\n",
      "batch 399, loss: 0.0308, instance_loss: 0.0093, weighted_loss: 0.0243, label: 1, bag_size: 99\n",
      "batch 419, loss: 0.0446, instance_loss: 0.3810, weighted_loss: 0.1455, label: 1, bag_size: 96\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 77\n",
      "batch 459, loss: 0.0039, instance_loss: 0.0425, weighted_loss: 0.0155, label: 1, bag_size: 46\n",
      "batch 479, loss: 0.0106, instance_loss: 0.0808, weighted_loss: 0.0317, label: 0, bag_size: 45\n",
      "batch 499, loss: 0.0179, instance_loss: 0.0653, weighted_loss: 0.0321, label: 0, bag_size: 54\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 58\n",
      "batch 539, loss: 0.1037, instance_loss: 1.6149, weighted_loss: 0.5571, label: 1, bag_size: 85\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 63\n",
      "batch 579, loss: 0.0299, instance_loss: 0.0385, weighted_loss: 0.0325, label: 1, bag_size: 72\n",
      "batch 599, loss: 0.0023, instance_loss: 0.7113, weighted_loss: 0.2150, label: 0, bag_size: 92\n",
      "batch 619, loss: 0.0041, instance_loss: 0.0000, weighted_loss: 0.0029, label: 0, bag_size: 51\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 84\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 42\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9817234848484848: correct 10367/10560\n",
      "class 1 clustering acc 0.9022727272727272: correct 4764/5280\n",
      "Epoch: 33, train_loss: 0.2103, train_clustering_loss:  0.1830, train_error: 0.0636\n",
      "class 0: acc 0.9429429429429429, correct 314/333\n",
      "class 1: acc 0.9296636085626911, correct 304/327\n",
      "\n",
      "Val Set, val_loss: 1.5648, val_error: 0.2381, auc: 0.8764\n",
      "class 0 clustering acc 0.9032738095238095: correct 1214/1344\n",
      "class 1 clustering acc 0.7589285714285714: correct 510/672\n",
      "class 0: acc 0.9743589743589743, correct 38/39\n",
      "class 1: acc 0.5777777777777777, correct 26/45\n",
      "EarlyStopping counter: 21 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0003, weighted_loss: 0.0002, label: 1, bag_size: 99\n",
      "batch 39, loss: 0.0051, instance_loss: 0.0099, weighted_loss: 0.0065, label: 1, bag_size: 21\n",
      "batch 59, loss: 0.1009, instance_loss: 0.0815, weighted_loss: 0.0951, label: 1, bag_size: 92\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 0, bag_size: 86\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0004, label: 0, bag_size: 108\n",
      "batch 119, loss: 0.0032, instance_loss: 0.0052, weighted_loss: 0.0038, label: 1, bag_size: 82\n",
      "batch 139, loss: 0.1179, instance_loss: 0.0176, weighted_loss: 0.0878, label: 1, bag_size: 76\n",
      "batch 159, loss: 0.0020, instance_loss: 0.0091, weighted_loss: 0.0041, label: 1, bag_size: 16\n",
      "batch 179, loss: 0.0002, instance_loss: 0.0085, weighted_loss: 0.0027, label: 0, bag_size: 48\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 31\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0016, weighted_loss: 0.0005, label: 0, bag_size: 64\n",
      "batch 239, loss: 0.0011, instance_loss: 0.9846, weighted_loss: 0.2962, label: 0, bag_size: 62\n",
      "batch 259, loss: 0.0013, instance_loss: 0.1754, weighted_loss: 0.0535, label: 1, bag_size: 50\n",
      "batch 279, loss: 0.0031, instance_loss: 0.2833, weighted_loss: 0.0872, label: 1, bag_size: 17\n",
      "batch 299, loss: 0.0295, instance_loss: 0.0384, weighted_loss: 0.0321, label: 1, bag_size: 77\n",
      "batch 319, loss: 0.0004, instance_loss: 0.0203, weighted_loss: 0.0064, label: 1, bag_size: 87\n",
      "batch 339, loss: 0.0006, instance_loss: 0.1516, weighted_loss: 0.0459, label: 0, bag_size: 103\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 103\n",
      "batch 379, loss: 0.7144, instance_loss: 0.4921, weighted_loss: 0.6477, label: 0, bag_size: 37\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 56\n",
      "batch 419, loss: 0.0085, instance_loss: 0.9209, weighted_loss: 0.2822, label: 1, bag_size: 69\n",
      "batch 439, loss: 0.0028, instance_loss: 0.0242, weighted_loss: 0.0092, label: 1, bag_size: 31\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 65\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0045, weighted_loss: 0.0013, label: 0, bag_size: 46\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 75\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0031, weighted_loss: 0.0009, label: 1, bag_size: 48\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 79\n",
      "batch 559, loss: 0.0000, instance_loss: 0.1461, weighted_loss: 0.0438, label: 1, bag_size: 62\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0331, weighted_loss: 0.0099, label: 1, bag_size: 102\n",
      "batch 599, loss: 0.0002, instance_loss: 0.0407, weighted_loss: 0.0124, label: 1, bag_size: 71\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0016, weighted_loss: 0.0005, label: 1, bag_size: 61\n",
      "batch 639, loss: 7.0852, instance_loss: 5.1188, weighted_loss: 6.4953, label: 1, bag_size: 31\n",
      "batch 659, loss: 0.8653, instance_loss: 0.3600, weighted_loss: 0.7137, label: 0, bag_size: 91\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9711174242424242: correct 10255/10560\n",
      "class 1 clustering acc 0.8727272727272727: correct 4608/5280\n",
      "Epoch: 34, train_loss: 0.3036, train_clustering_loss:  0.2565, train_error: 0.0652\n",
      "class 0: acc 0.9387755102040817, correct 322/343\n",
      "class 1: acc 0.9305993690851735, correct 295/317\n",
      "\n",
      "Val Set, val_loss: 1.5469, val_error: 0.2857, auc: 0.8704\n",
      "class 0 clustering acc 0.8794642857142857: correct 1182/1344\n",
      "class 1 clustering acc 0.7068452380952381: correct 475/672\n",
      "class 0: acc 0.5641025641025641, correct 22/39\n",
      "class 1: acc 0.8444444444444444, correct 38/45\n",
      "EarlyStopping counter: 22 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 0, bag_size: 79\n",
      "batch 39, loss: 0.0012, instance_loss: 0.0110, weighted_loss: 0.0041, label: 1, bag_size: 78\n",
      "batch 59, loss: 0.0168, instance_loss: 0.0175, weighted_loss: 0.0170, label: 0, bag_size: 95\n",
      "batch 79, loss: 0.0104, instance_loss: 0.0977, weighted_loss: 0.0366, label: 1, bag_size: 29\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0028, weighted_loss: 0.0009, label: 0, bag_size: 58\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 56\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 83\n",
      "batch 159, loss: 0.0242, instance_loss: 0.0020, weighted_loss: 0.0175, label: 0, bag_size: 75\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0013, weighted_loss: 0.0005, label: 1, bag_size: 69\n",
      "batch 199, loss: 0.0007, instance_loss: 0.0030, weighted_loss: 0.0014, label: 0, bag_size: 88\n",
      "batch 219, loss: 0.0000, instance_loss: 0.3307, weighted_loss: 0.0992, label: 0, bag_size: 164\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 20\n",
      "batch 259, loss: 0.0020, instance_loss: 0.0915, weighted_loss: 0.0288, label: 1, bag_size: 68\n",
      "batch 279, loss: 12.2736, instance_loss: 6.4710, weighted_loss: 10.5328, label: 0, bag_size: 73\n",
      "batch 299, loss: 0.0058, instance_loss: 0.0621, weighted_loss: 0.0227, label: 0, bag_size: 77\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 108\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 116\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0005, weighted_loss: 0.0002, label: 0, bag_size: 25\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0040, weighted_loss: 0.0012, label: 1, bag_size: 76\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 97\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 46\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 68\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 78\n",
      "batch 479, loss: 0.0003, instance_loss: 0.0108, weighted_loss: 0.0034, label: 1, bag_size: 38\n",
      "batch 499, loss: 0.0126, instance_loss: 0.0084, weighted_loss: 0.0113, label: 0, bag_size: 26\n",
      "batch 519, loss: 0.8017, instance_loss: 0.7162, weighted_loss: 0.7760, label: 1, bag_size: 42\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 0, bag_size: 44\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 61\n",
      "batch 579, loss: 0.0045, instance_loss: 0.0065, weighted_loss: 0.0051, label: 1, bag_size: 93\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 88\n",
      "batch 619, loss: 0.1075, instance_loss: 0.0346, weighted_loss: 0.0856, label: 0, bag_size: 112\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 88\n",
      "batch 659, loss: 1.1658, instance_loss: 0.3757, weighted_loss: 0.9287, label: 0, bag_size: 87\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9810606060606061: correct 10360/10560\n",
      "class 1 clustering acc 0.9130681818181818: correct 4821/5280\n",
      "Epoch: 35, train_loss: 0.1931, train_clustering_loss:  0.1787, train_error: 0.0530\n",
      "class 0: acc 0.948170731707317, correct 311/328\n",
      "class 1: acc 0.9457831325301205, correct 314/332\n",
      "\n",
      "Val Set, val_loss: 1.3644, val_error: 0.2143, auc: 0.8815\n",
      "class 0 clustering acc 0.9114583333333334: correct 1225/1344\n",
      "class 1 clustering acc 0.7291666666666666: correct 490/672\n",
      "class 0: acc 0.6410256410256411, correct 25/39\n",
      "class 1: acc 0.9111111111111111, correct 41/45\n",
      "EarlyStopping counter: 23 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0016, weighted_loss: 0.0005, label: 0, bag_size: 59\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0203, weighted_loss: 0.0061, label: 0, bag_size: 78\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 86\n",
      "batch 79, loss: 0.0152, instance_loss: 0.0134, weighted_loss: 0.0147, label: 0, bag_size: 58\n",
      "batch 99, loss: 0.0002, instance_loss: 0.2640, weighted_loss: 0.0793, label: 1, bag_size: 17\n",
      "batch 119, loss: 0.1147, instance_loss: 0.6472, weighted_loss: 0.2744, label: 0, bag_size: 28\n",
      "batch 139, loss: 0.1473, instance_loss: 0.1483, weighted_loss: 0.1476, label: 0, bag_size: 50\n",
      "batch 159, loss: 0.0086, instance_loss: 0.0120, weighted_loss: 0.0096, label: 1, bag_size: 48\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 84\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 98\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 76\n",
      "batch 239, loss: 0.0002, instance_loss: 0.0051, weighted_loss: 0.0016, label: 1, bag_size: 49\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 36\n",
      "batch 279, loss: 0.7265, instance_loss: 0.8219, weighted_loss: 0.7551, label: 1, bag_size: 111\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 0, bag_size: 111\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0112, weighted_loss: 0.0034, label: 0, bag_size: 43\n",
      "batch 339, loss: 2.4055, instance_loss: 1.8242, weighted_loss: 2.2311, label: 1, bag_size: 121\n",
      "batch 359, loss: 0.0018, instance_loss: 0.3547, weighted_loss: 0.1076, label: 1, bag_size: 68\n",
      "batch 379, loss: 0.1380, instance_loss: 0.0400, weighted_loss: 0.1086, label: 1, bag_size: 99\n",
      "batch 399, loss: 0.0003, instance_loss: 0.0375, weighted_loss: 0.0115, label: 0, bag_size: 88\n",
      "batch 419, loss: 0.0005, instance_loss: 0.0216, weighted_loss: 0.0068, label: 1, bag_size: 64\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 95\n",
      "batch 459, loss: 0.4262, instance_loss: 1.4642, weighted_loss: 0.7376, label: 1, bag_size: 35\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 95\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0020, weighted_loss: 0.0006, label: 0, bag_size: 79\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0052, weighted_loss: 0.0016, label: 0, bag_size: 102\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0150, weighted_loss: 0.0045, label: 1, bag_size: 20\n",
      "batch 559, loss: 0.0002, instance_loss: 0.0178, weighted_loss: 0.0055, label: 0, bag_size: 49\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 12\n",
      "batch 599, loss: 0.0059, instance_loss: 0.1457, weighted_loss: 0.0478, label: 0, bag_size: 45\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 51\n",
      "batch 639, loss: 0.0148, instance_loss: 0.0885, weighted_loss: 0.0369, label: 1, bag_size: 87\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0835, weighted_loss: 0.0250, label: 0, bag_size: 45\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9801136363636364: correct 10350/10560\n",
      "class 1 clustering acc 0.9130681818181818: correct 4821/5280\n",
      "Epoch: 36, train_loss: 0.2178, train_clustering_loss:  0.1829, train_error: 0.0652\n",
      "class 0: acc 0.9265175718849841, correct 290/313\n",
      "class 1: acc 0.9423631123919308, correct 327/347\n",
      "\n",
      "Val Set, val_loss: 1.1493, val_error: 0.2143, auc: 0.8752\n",
      "class 0 clustering acc 0.921875: correct 1239/1344\n",
      "class 1 clustering acc 0.65625: correct 441/672\n",
      "class 0: acc 0.8717948717948718, correct 34/39\n",
      "class 1: acc 0.7111111111111111, correct 32/45\n",
      "EarlyStopping counter: 24 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0033, weighted_loss: 0.0010, label: 1, bag_size: 81\n",
      "batch 39, loss: 5.1376, instance_loss: 1.9591, weighted_loss: 4.1841, label: 1, bag_size: 38\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 49\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 0, bag_size: 65\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 67\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0047, weighted_loss: 0.0014, label: 0, bag_size: 40\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0031, weighted_loss: 0.0010, label: 1, bag_size: 47\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0146, weighted_loss: 0.0044, label: 0, bag_size: 111\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0056, weighted_loss: 0.0017, label: 0, bag_size: 72\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0026, weighted_loss: 0.0008, label: 0, bag_size: 73\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 99\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 43\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 92\n",
      "batch 279, loss: 0.0027, instance_loss: 0.0006, weighted_loss: 0.0021, label: 0, bag_size: 31\n",
      "batch 299, loss: 0.0004, instance_loss: 0.0154, weighted_loss: 0.0049, label: 1, bag_size: 54\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0025, weighted_loss: 0.0007, label: 0, bag_size: 50\n",
      "batch 339, loss: 0.0002, instance_loss: 0.0154, weighted_loss: 0.0047, label: 0, bag_size: 75\n",
      "batch 359, loss: 0.0014, instance_loss: 0.0065, weighted_loss: 0.0030, label: 1, bag_size: 62\n",
      "batch 379, loss: 0.2248, instance_loss: 0.3355, weighted_loss: 0.2580, label: 1, bag_size: 42\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 1, bag_size: 122\n",
      "batch 419, loss: 0.0002, instance_loss: 0.0348, weighted_loss: 0.0106, label: 1, bag_size: 47\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0022, weighted_loss: 0.0008, label: 0, bag_size: 101\n",
      "batch 459, loss: 0.1119, instance_loss: 0.0239, weighted_loss: 0.0855, label: 0, bag_size: 51\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 46\n",
      "batch 499, loss: 1.2388, instance_loss: 1.9247, weighted_loss: 1.4446, label: 0, bag_size: 60\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 0, bag_size: 55\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 44\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0056, weighted_loss: 0.0017, label: 0, bag_size: 75\n",
      "batch 579, loss: 0.0001, instance_loss: 0.2699, weighted_loss: 0.0811, label: 0, bag_size: 46\n",
      "batch 599, loss: 0.0261, instance_loss: 0.0354, weighted_loss: 0.0289, label: 1, bag_size: 35\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 72\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 37\n",
      "batch 659, loss: 0.1015, instance_loss: 0.0602, weighted_loss: 0.0891, label: 1, bag_size: 79\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9798295454545455: correct 10347/10560\n",
      "class 1 clustering acc 0.8950757575757575: correct 4726/5280\n",
      "Epoch: 37, train_loss: 0.2491, train_clustering_loss:  0.2063, train_error: 0.0712\n",
      "class 0: acc 0.924924924924925, correct 308/333\n",
      "class 1: acc 0.9327217125382263, correct 305/327\n",
      "\n",
      "Val Set, val_loss: 1.3288, val_error: 0.1905, auc: 0.8479\n",
      "class 0 clustering acc 0.9114583333333334: correct 1225/1344\n",
      "class 1 clustering acc 0.7619047619047619: correct 512/672\n",
      "class 0: acc 0.8717948717948718, correct 34/39\n",
      "class 1: acc 0.7555555555555555, correct 34/45\n",
      "EarlyStopping counter: 25 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 1, bag_size: 107\n",
      "batch 39, loss: 0.0011, instance_loss: 0.0004, weighted_loss: 0.0009, label: 0, bag_size: 45\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 1, bag_size: 58\n",
      "batch 79, loss: 0.0003, instance_loss: 0.0056, weighted_loss: 0.0019, label: 0, bag_size: 29\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 65\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0039, weighted_loss: 0.0012, label: 1, bag_size: 62\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 22\n",
      "batch 159, loss: 0.0004, instance_loss: 0.0566, weighted_loss: 0.0172, label: 1, bag_size: 77\n",
      "batch 179, loss: 0.2872, instance_loss: 0.9549, weighted_loss: 0.4875, label: 0, bag_size: 53\n",
      "batch 199, loss: 1.9791, instance_loss: 1.0379, weighted_loss: 1.6967, label: 0, bag_size: 75\n",
      "batch 219, loss: 0.0069, instance_loss: 0.3463, weighted_loss: 0.1087, label: 0, bag_size: 25\n",
      "batch 239, loss: 0.0001, instance_loss: 0.0183, weighted_loss: 0.0055, label: 1, bag_size: 88\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0017, weighted_loss: 0.0005, label: 0, bag_size: 36\n",
      "batch 279, loss: 0.0002, instance_loss: 0.0032, weighted_loss: 0.0011, label: 1, bag_size: 36\n",
      "batch 299, loss: 0.0003, instance_loss: 0.0002, weighted_loss: 0.0002, label: 1, bag_size: 87\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 19\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 57\n",
      "batch 359, loss: 0.0233, instance_loss: 0.0459, weighted_loss: 0.0301, label: 0, bag_size: 39\n",
      "batch 379, loss: 0.0108, instance_loss: 0.0188, weighted_loss: 0.0132, label: 1, bag_size: 78\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 67\n",
      "batch 419, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 83\n",
      "batch 439, loss: 0.0091, instance_loss: 0.0161, weighted_loss: 0.0112, label: 0, bag_size: 28\n",
      "batch 459, loss: 0.1213, instance_loss: 0.3194, weighted_loss: 0.1807, label: 1, bag_size: 67\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 42\n",
      "batch 499, loss: 0.0012, instance_loss: 0.0282, weighted_loss: 0.0093, label: 1, bag_size: 76\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 87\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0099, weighted_loss: 0.0032, label: 0, bag_size: 87\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0031, weighted_loss: 0.0010, label: 0, bag_size: 79\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 0, bag_size: 94\n",
      "batch 599, loss: 0.0566, instance_loss: 1.1198, weighted_loss: 0.3756, label: 1, bag_size: 57\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0084, weighted_loss: 0.0025, label: 1, bag_size: 71\n",
      "batch 639, loss: 0.0064, instance_loss: 0.0607, weighted_loss: 0.0227, label: 0, bag_size: 31\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 26\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9833333333333333: correct 10384/10560\n",
      "class 1 clustering acc 0.9106060606060606: correct 4808/5280\n",
      "Epoch: 38, train_loss: 0.1961, train_clustering_loss:  0.1812, train_error: 0.0515\n",
      "class 0: acc 0.9446153846153846, correct 307/325\n",
      "class 1: acc 0.9522388059701492, correct 319/335\n",
      "\n",
      "Val Set, val_loss: 1.2400, val_error: 0.1786, auc: 0.8561\n",
      "class 0 clustering acc 0.9196428571428571: correct 1236/1344\n",
      "class 1 clustering acc 0.7291666666666666: correct 490/672\n",
      "class 0: acc 0.8461538461538461, correct 33/39\n",
      "class 1: acc 0.8, correct 36/45\n",
      "EarlyStopping counter: 26 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 85\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 57\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 51\n",
      "batch 79, loss: 0.0001, instance_loss: 0.0012, weighted_loss: 0.0004, label: 1, bag_size: 43\n",
      "batch 99, loss: 0.0510, instance_loss: 0.7484, weighted_loss: 0.2602, label: 0, bag_size: 54\n",
      "batch 119, loss: 0.1178, instance_loss: 0.1104, weighted_loss: 0.1156, label: 1, bag_size: 96\n",
      "batch 139, loss: 0.0002, instance_loss: 0.0015, weighted_loss: 0.0006, label: 1, bag_size: 107\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 72\n",
      "batch 179, loss: 0.0494, instance_loss: 0.0298, weighted_loss: 0.0435, label: 1, bag_size: 53\n",
      "batch 199, loss: 0.0011, instance_loss: 0.0008, weighted_loss: 0.0010, label: 0, bag_size: 24\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0020, weighted_loss: 0.0006, label: 0, bag_size: 111\n",
      "batch 239, loss: 0.0008, instance_loss: 0.0022, weighted_loss: 0.0012, label: 0, bag_size: 37\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0030, weighted_loss: 0.0009, label: 0, bag_size: 89\n",
      "batch 279, loss: 0.0016, instance_loss: 0.0120, weighted_loss: 0.0047, label: 0, bag_size: 27\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 25\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0029, weighted_loss: 0.0009, label: 0, bag_size: 87\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 61\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0035, weighted_loss: 0.0011, label: 0, bag_size: 38\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0130, weighted_loss: 0.0039, label: 1, bag_size: 84\n",
      "batch 399, loss: 0.0093, instance_loss: 0.0192, weighted_loss: 0.0123, label: 1, bag_size: 18\n",
      "batch 419, loss: 1.0978, instance_loss: 0.5974, weighted_loss: 0.9477, label: 0, bag_size: 101\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 74\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 55\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0259, weighted_loss: 0.0078, label: 1, bag_size: 31\n",
      "batch 499, loss: 0.0034, instance_loss: 0.0001, weighted_loss: 0.0024, label: 0, bag_size: 19\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 38\n",
      "batch 539, loss: 0.0000, instance_loss: 0.1588, weighted_loss: 0.0477, label: 1, bag_size: 68\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0244, weighted_loss: 0.0073, label: 1, bag_size: 31\n",
      "batch 579, loss: 0.0654, instance_loss: 1.1352, weighted_loss: 0.3863, label: 0, bag_size: 51\n",
      "batch 599, loss: 11.6779, instance_loss: 0.0402, weighted_loss: 8.1865, label: 1, bag_size: 24\n",
      "batch 619, loss: 0.0001, instance_loss: 0.2041, weighted_loss: 0.0613, label: 1, bag_size: 84\n",
      "batch 639, loss: 2.8804, instance_loss: 3.6350, weighted_loss: 3.1068, label: 0, bag_size: 72\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 1, bag_size: 79\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.975284090909091: correct 10299/10560\n",
      "class 1 clustering acc 0.8803030303030303: correct 4648/5280\n",
      "Epoch: 39, train_loss: 0.5174, train_clustering_loss:  0.2486, train_error: 0.1000\n",
      "class 0: acc 0.9085545722713865, correct 308/339\n",
      "class 1: acc 0.8909657320872274, correct 286/321\n",
      "\n",
      "Val Set, val_loss: 1.8203, val_error: 0.2143, auc: 0.8476\n",
      "class 0 clustering acc 0.9151785714285714: correct 1230/1344\n",
      "class 1 clustering acc 0.7113095238095238: correct 478/672\n",
      "class 0: acc 0.6923076923076923, correct 27/39\n",
      "class 1: acc 0.8666666666666667, correct 39/45\n",
      "EarlyStopping counter: 27 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0687, weighted_loss: 0.0206, label: 1, bag_size: 45\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 92\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 17\n",
      "batch 79, loss: 0.0025, instance_loss: 0.0762, weighted_loss: 0.0246, label: 1, bag_size: 121\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 73\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 45\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 116\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 42\n",
      "batch 179, loss: 0.0142, instance_loss: 0.0013, weighted_loss: 0.0103, label: 0, bag_size: 39\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0019, weighted_loss: 0.0006, label: 0, bag_size: 29\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 58\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 77\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0026, weighted_loss: 0.0008, label: 1, bag_size: 91\n",
      "batch 279, loss: 0.0790, instance_loss: 0.0069, weighted_loss: 0.0574, label: 1, bag_size: 127\n",
      "batch 299, loss: 0.0081, instance_loss: 0.0032, weighted_loss: 0.0066, label: 1, bag_size: 94\n",
      "batch 319, loss: 0.0147, instance_loss: 0.2098, weighted_loss: 0.0732, label: 0, bag_size: 73\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 53\n",
      "batch 359, loss: 0.0092, instance_loss: 0.0054, weighted_loss: 0.0081, label: 0, bag_size: 69\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 164\n",
      "batch 399, loss: 0.0129, instance_loss: 0.0062, weighted_loss: 0.0109, label: 1, bag_size: 24\n",
      "batch 419, loss: 0.1230, instance_loss: 0.0147, weighted_loss: 0.0905, label: 0, bag_size: 48\n",
      "batch 439, loss: 0.0010, instance_loss: 0.0012, weighted_loss: 0.0010, label: 1, bag_size: 72\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0057, weighted_loss: 0.0017, label: 1, bag_size: 81\n",
      "batch 479, loss: 0.0131, instance_loss: 0.1405, weighted_loss: 0.0513, label: 1, bag_size: 38\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0061, weighted_loss: 0.0018, label: 1, bag_size: 81\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 20\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0044, weighted_loss: 0.0013, label: 0, bag_size: 90\n",
      "batch 559, loss: 2.7269, instance_loss: 0.7910, weighted_loss: 2.1461, label: 1, bag_size: 60\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 21\n",
      "batch 599, loss: 0.0383, instance_loss: 0.0910, weighted_loss: 0.0542, label: 0, bag_size: 88\n",
      "batch 619, loss: 3.9565, instance_loss: 1.4038, weighted_loss: 3.1907, label: 1, bag_size: 46\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 45\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 69\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9821022727272727: correct 10371/10560\n",
      "class 1 clustering acc 0.9051136363636364: correct 4779/5280\n",
      "Epoch: 40, train_loss: 0.2256, train_clustering_loss:  0.1955, train_error: 0.0636\n",
      "class 0: acc 0.9300291545189504, correct 319/343\n",
      "class 1: acc 0.943217665615142, correct 299/317\n",
      "\n",
      "Val Set, val_loss: 1.8349, val_error: 0.1667, auc: 0.8729\n",
      "class 0 clustering acc 0.9159226190476191: correct 1231/1344\n",
      "class 1 clustering acc 0.7752976190476191: correct 521/672\n",
      "class 0: acc 0.8205128205128205, correct 32/39\n",
      "class 1: acc 0.8444444444444444, correct 38/45\n",
      "EarlyStopping counter: 28 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 13.6161, instance_loss: 4.1896, weighted_loss: 10.7882, label: 0, bag_size: 40\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0463, weighted_loss: 0.0139, label: 0, bag_size: 22\n",
      "batch 59, loss: 1.2163, instance_loss: 1.0896, weighted_loss: 1.1783, label: 0, bag_size: 35\n",
      "batch 79, loss: 0.0001, instance_loss: 0.0199, weighted_loss: 0.0061, label: 1, bag_size: 110\n",
      "batch 99, loss: 3.3364, instance_loss: 1.5006, weighted_loss: 2.7856, label: 0, bag_size: 68\n",
      "batch 119, loss: 0.0010, instance_loss: 0.0239, weighted_loss: 0.0079, label: 1, bag_size: 76\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 77\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 23\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 21\n",
      "batch 199, loss: 0.0183, instance_loss: 0.0040, weighted_loss: 0.0140, label: 1, bag_size: 67\n",
      "batch 219, loss: 0.0043, instance_loss: 0.5774, weighted_loss: 0.1763, label: 0, bag_size: 25\n",
      "batch 239, loss: 3.9191, instance_loss: 1.1707, weighted_loss: 3.0946, label: 1, bag_size: 29\n",
      "batch 259, loss: 0.0000, instance_loss: 0.5918, weighted_loss: 0.1776, label: 1, bag_size: 42\n",
      "batch 279, loss: 0.0101, instance_loss: 0.5951, weighted_loss: 0.1856, label: 1, bag_size: 24\n",
      "batch 299, loss: 0.0245, instance_loss: 0.6076, weighted_loss: 0.1994, label: 0, bag_size: 39\n",
      "batch 319, loss: 0.0000, instance_loss: 0.5766, weighted_loss: 0.1730, label: 0, bag_size: 75\n",
      "batch 339, loss: 0.0023, instance_loss: 0.5458, weighted_loss: 0.1654, label: 1, bag_size: 25\n",
      "batch 359, loss: 0.0012, instance_loss: 0.5830, weighted_loss: 0.1757, label: 0, bag_size: 75\n",
      "batch 379, loss: 0.0000, instance_loss: 0.5678, weighted_loss: 0.1703, label: 1, bag_size: 33\n",
      "batch 399, loss: 0.0000, instance_loss: 0.4732, weighted_loss: 0.1420, label: 0, bag_size: 111\n",
      "batch 419, loss: 0.0000, instance_loss: 0.4165, weighted_loss: 0.1250, label: 1, bag_size: 72\n",
      "batch 439, loss: 0.0000, instance_loss: 0.8050, weighted_loss: 0.2415, label: 0, bag_size: 75\n",
      "batch 459, loss: 2.5088, instance_loss: 2.0412, weighted_loss: 2.3685, label: 1, bag_size: 54\n",
      "batch 479, loss: 0.0000, instance_loss: 0.6604, weighted_loss: 0.1981, label: 0, bag_size: 24\n",
      "batch 499, loss: 0.0422, instance_loss: 0.1826, weighted_loss: 0.0843, label: 1, bag_size: 77\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0392, weighted_loss: 0.0118, label: 1, bag_size: 127\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0763, weighted_loss: 0.0229, label: 1, bag_size: 69\n",
      "batch 559, loss: 0.0000, instance_loss: 0.1610, weighted_loss: 0.0483, label: 0, bag_size: 75\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0030, weighted_loss: 0.0009, label: 0, bag_size: 27\n",
      "batch 599, loss: 0.0002, instance_loss: 0.0201, weighted_loss: 0.0062, label: 1, bag_size: 22\n",
      "batch 619, loss: 0.0000, instance_loss: 0.1565, weighted_loss: 0.0470, label: 0, bag_size: 29\n",
      "batch 639, loss: 0.0000, instance_loss: 0.1751, weighted_loss: 0.0525, label: 1, bag_size: 31\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0039, weighted_loss: 0.0012, label: 0, bag_size: 67\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9164772727272728: correct 9678/10560\n",
      "class 1 clustering acc 0.7361742424242425: correct 3887/5280\n",
      "Epoch: 41, train_loss: 0.3342, train_clustering_loss:  0.4495, train_error: 0.1015\n",
      "class 0: acc 0.8956521739130435, correct 309/345\n",
      "class 1: acc 0.9015873015873016, correct 284/315\n",
      "\n",
      "Val Set, val_loss: 1.3340, val_error: 0.1905, auc: 0.8718\n",
      "class 0 clustering acc 0.9598214285714286: correct 1290/1344\n",
      "class 1 clustering acc 0.5044642857142857: correct 339/672\n",
      "class 0: acc 0.7692307692307693, correct 30/39\n",
      "class 1: acc 0.8444444444444444, correct 38/45\n",
      "EarlyStopping counter: 29 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0023, weighted_loss: 0.0007, label: 0, bag_size: 65\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 132\n",
      "batch 59, loss: 0.0659, instance_loss: 0.0173, weighted_loss: 0.0513, label: 0, bag_size: 63\n",
      "batch 79, loss: 9.6057, instance_loss: 3.1036, weighted_loss: 7.6550, label: 1, bag_size: 95\n",
      "batch 99, loss: 1.9332, instance_loss: 3.8906, weighted_loss: 2.5204, label: 0, bag_size: 90\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0064, weighted_loss: 0.0019, label: 0, bag_size: 57\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 83\n",
      "batch 159, loss: 0.0002, instance_loss: 0.0337, weighted_loss: 0.0102, label: 0, bag_size: 77\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 89\n",
      "batch 199, loss: 0.1182, instance_loss: 0.0204, weighted_loss: 0.0888, label: 0, bag_size: 78\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 39\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 68\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0052, weighted_loss: 0.0016, label: 0, bag_size: 78\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 0, bag_size: 96\n",
      "batch 299, loss: 0.1301, instance_loss: 1.1160, weighted_loss: 0.4259, label: 1, bag_size: 29\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 114\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 72\n",
      "batch 359, loss: 0.0003, instance_loss: 0.0021, weighted_loss: 0.0008, label: 0, bag_size: 26\n",
      "batch 379, loss: 0.0296, instance_loss: 0.0108, weighted_loss: 0.0239, label: 0, bag_size: 34\n",
      "batch 399, loss: 0.0099, instance_loss: 0.0131, weighted_loss: 0.0108, label: 0, bag_size: 101\n",
      "batch 419, loss: 0.0006, instance_loss: 0.0337, weighted_loss: 0.0105, label: 0, bag_size: 67\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0021, weighted_loss: 0.0006, label: 0, bag_size: 83\n",
      "batch 459, loss: 0.0218, instance_loss: 0.1347, weighted_loss: 0.0557, label: 0, bag_size: 51\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 21\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 68\n",
      "batch 519, loss: 0.4210, instance_loss: 0.2299, weighted_loss: 0.3637, label: 0, bag_size: 58\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 85\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 55\n",
      "batch 579, loss: 0.0000, instance_loss: 0.5221, weighted_loss: 0.1566, label: 0, bag_size: 90\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 45\n",
      "batch 619, loss: 0.0003, instance_loss: 0.0322, weighted_loss: 0.0099, label: 0, bag_size: 71\n",
      "batch 639, loss: 0.0028, instance_loss: 0.8058, weighted_loss: 0.2437, label: 1, bag_size: 59\n",
      "batch 659, loss: 0.0000, instance_loss: 0.1012, weighted_loss: 0.0304, label: 1, bag_size: 61\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9826704545454545: correct 10377/10560\n",
      "class 1 clustering acc 0.9285984848484848: correct 4903/5280\n",
      "Epoch: 42, train_loss: 0.4298, train_clustering_loss:  0.1549, train_error: 0.0742\n",
      "class 0: acc 0.9242424242424242, correct 305/330\n",
      "class 1: acc 0.9272727272727272, correct 306/330\n",
      "\n",
      "Val Set, val_loss: 11.3165, val_error: 0.4048, auc: 0.6764\n",
      "class 0 clustering acc 0.9345238095238095: correct 1256/1344\n",
      "class 1 clustering acc 0.47619047619047616: correct 320/672\n",
      "class 0: acc 0.1282051282051282, correct 5/39\n",
      "class 1: acc 1.0, correct 45/45\n",
      "EarlyStopping counter: 30 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 29.3378, instance_loss: 0.0141, weighted_loss: 20.5407, label: 1, bag_size: 126\n",
      "batch 39, loss: 3.9526, instance_loss: 0.6457, weighted_loss: 2.9605, label: 0, bag_size: 53\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0932, weighted_loss: 0.0280, label: 0, bag_size: 77\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0205, weighted_loss: 0.0061, label: 0, bag_size: 64\n",
      "batch 99, loss: 20.3115, instance_loss: 1.2329, weighted_loss: 14.5879, label: 0, bag_size: 81\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0086, weighted_loss: 0.0026, label: 1, bag_size: 83\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0122, weighted_loss: 0.0037, label: 1, bag_size: 76\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0305, weighted_loss: 0.0092, label: 1, bag_size: 31\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 59\n",
      "batch 199, loss: 0.0106, instance_loss: 0.7737, weighted_loss: 0.2395, label: 1, bag_size: 24\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0291, weighted_loss: 0.0087, label: 1, bag_size: 41\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0312, weighted_loss: 0.0094, label: 0, bag_size: 43\n",
      "batch 259, loss: 0.0009, instance_loss: 0.4050, weighted_loss: 0.1222, label: 0, bag_size: 61\n",
      "batch 279, loss: 2.9710, instance_loss: 0.8239, weighted_loss: 2.3269, label: 0, bag_size: 50\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 1, bag_size: 153\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 71\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 94\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0367, weighted_loss: 0.0110, label: 0, bag_size: 58\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 77\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 0, bag_size: 34\n",
      "batch 419, loss: 0.3886, instance_loss: 0.3116, weighted_loss: 0.3655, label: 0, bag_size: 30\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0241, weighted_loss: 0.0072, label: 0, bag_size: 76\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 13\n",
      "batch 479, loss: 0.0269, instance_loss: 0.0175, weighted_loss: 0.0241, label: 1, bag_size: 79\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0101, weighted_loss: 0.0030, label: 0, bag_size: 31\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0251, weighted_loss: 0.0075, label: 1, bag_size: 47\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0308, weighted_loss: 0.0092, label: 1, bag_size: 29\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 92\n",
      "batch 579, loss: 0.1128, instance_loss: 0.4371, weighted_loss: 0.2101, label: 1, bag_size: 56\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0021, weighted_loss: 0.0006, label: 0, bag_size: 71\n",
      "batch 619, loss: 0.0038, instance_loss: 0.0482, weighted_loss: 0.0171, label: 0, bag_size: 52\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 88\n",
      "batch 659, loss: 0.8108, instance_loss: 0.5914, weighted_loss: 0.7450, label: 0, bag_size: 79\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9554924242424242: correct 10090/10560\n",
      "class 1 clustering acc 0.809469696969697: correct 4274/5280\n",
      "Epoch: 43, train_loss: 1.0402, train_clustering_loss:  0.3936, train_error: 0.1258\n",
      "class 0: acc 0.8738461538461538, correct 284/325\n",
      "class 1: acc 0.8746268656716418, correct 293/335\n",
      "\n",
      "Val Set, val_loss: 1.4363, val_error: 0.1905, auc: 0.8650\n",
      "class 0 clustering acc 0.9077380952380952: correct 1220/1344\n",
      "class 1 clustering acc 0.7053571428571429: correct 474/672\n",
      "class 0: acc 0.7692307692307693, correct 30/39\n",
      "class 1: acc 0.8444444444444444, correct 38/45\n",
      "EarlyStopping counter: 31 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 51\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 45\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 31\n",
      "batch 79, loss: 0.1654, instance_loss: 0.6359, weighted_loss: 0.3065, label: 0, bag_size: 38\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 79\n",
      "batch 119, loss: 0.0765, instance_loss: 0.3596, weighted_loss: 0.1614, label: 0, bag_size: 43\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0065, weighted_loss: 0.0019, label: 1, bag_size: 41\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 37\n",
      "batch 179, loss: 0.0926, instance_loss: 0.0694, weighted_loss: 0.0856, label: 0, bag_size: 61\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 114\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 1, bag_size: 92\n",
      "batch 259, loss: 0.0029, instance_loss: 0.0379, weighted_loss: 0.0134, label: 0, bag_size: 40\n",
      "batch 279, loss: 0.0175, instance_loss: 1.0653, weighted_loss: 0.3319, label: 1, bag_size: 80\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 33\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0031, weighted_loss: 0.0009, label: 1, bag_size: 100\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0016, weighted_loss: 0.0005, label: 1, bag_size: 77\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 29\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 40\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 107\n",
      "batch 439, loss: 0.5502, instance_loss: 0.8413, weighted_loss: 0.6375, label: 0, bag_size: 68\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 44\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0075, weighted_loss: 0.0022, label: 1, bag_size: 88\n",
      "batch 499, loss: 0.0123, instance_loss: 0.0490, weighted_loss: 0.0233, label: 0, bag_size: 93\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 85\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 78\n",
      "batch 559, loss: 0.0205, instance_loss: 0.3968, weighted_loss: 0.1334, label: 1, bag_size: 60\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 37\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 23\n",
      "batch 619, loss: 0.6362, instance_loss: 0.3120, weighted_loss: 0.5390, label: 1, bag_size: 28\n",
      "batch 639, loss: 0.1977, instance_loss: 0.0476, weighted_loss: 0.1526, label: 0, bag_size: 66\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 110\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9825757575757575: correct 10376/10560\n",
      "class 1 clustering acc 0.9212121212121213: correct 4864/5280\n",
      "Epoch: 44, train_loss: 0.1847, train_clustering_loss:  0.1506, train_error: 0.0576\n",
      "class 0: acc 0.95, correct 342/360\n",
      "class 1: acc 0.9333333333333333, correct 280/300\n",
      "\n",
      "Val Set, val_loss: 1.6495, val_error: 0.2024, auc: 0.8809\n",
      "class 0 clustering acc 0.9032738095238095: correct 1214/1344\n",
      "class 1 clustering acc 0.7380952380952381: correct 496/672\n",
      "class 0: acc 0.6923076923076923, correct 27/39\n",
      "class 1: acc 0.8888888888888888, correct 40/45\n",
      "EarlyStopping counter: 32 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0056, instance_loss: 1.2501, weighted_loss: 0.3789, label: 1, bag_size: 35\n",
      "batch 39, loss: 0.2675, instance_loss: 0.0433, weighted_loss: 0.2003, label: 1, bag_size: 65\n",
      "batch 59, loss: 0.0002, instance_loss: 0.0040, weighted_loss: 0.0013, label: 0, bag_size: 37\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0670, weighted_loss: 0.0201, label: 1, bag_size: 38\n",
      "batch 99, loss: 2.6262, instance_loss: 2.0846, weighted_loss: 2.4637, label: 0, bag_size: 78\n",
      "batch 119, loss: 0.0008, instance_loss: 0.2372, weighted_loss: 0.0717, label: 0, bag_size: 46\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0003, label: 1, bag_size: 71\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 110\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 74\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 111\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 62\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 107\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 72\n",
      "batch 279, loss: 0.0341, instance_loss: 0.0767, weighted_loss: 0.0469, label: 0, bag_size: 18\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0003, label: 1, bag_size: 94\n",
      "batch 319, loss: 0.0001, instance_loss: 0.1003, weighted_loss: 0.0302, label: 1, bag_size: 38\n",
      "batch 339, loss: 0.0357, instance_loss: 0.0140, weighted_loss: 0.0292, label: 1, bag_size: 78\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 0, bag_size: 18\n",
      "batch 379, loss: 0.2802, instance_loss: 0.6010, weighted_loss: 0.3765, label: 0, bag_size: 90\n",
      "batch 399, loss: 0.0221, instance_loss: 0.0463, weighted_loss: 0.0294, label: 0, bag_size: 80\n",
      "batch 419, loss: 0.7115, instance_loss: 0.1612, weighted_loss: 0.5464, label: 1, bag_size: 69\n",
      "batch 439, loss: 0.0009, instance_loss: 0.0047, weighted_loss: 0.0020, label: 1, bag_size: 71\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 62\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0050, weighted_loss: 0.0015, label: 0, bag_size: 17\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 128\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0373, weighted_loss: 0.0112, label: 0, bag_size: 30\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 107\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0039, weighted_loss: 0.0012, label: 0, bag_size: 49\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 57\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 51\n",
      "batch 619, loss: 2.5733, instance_loss: 2.3914, weighted_loss: 2.5187, label: 0, bag_size: 89\n",
      "batch 639, loss: 0.0119, instance_loss: 0.0255, weighted_loss: 0.0160, label: 0, bag_size: 28\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 62\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9816287878787879: correct 10366/10560\n",
      "class 1 clustering acc 0.9130681818181818: correct 4821/5280\n",
      "Epoch: 45, train_loss: 0.2852, train_clustering_loss:  0.1732, train_error: 0.0697\n",
      "class 0: acc 0.9329268292682927, correct 306/328\n",
      "class 1: acc 0.927710843373494, correct 308/332\n",
      "\n",
      "Val Set, val_loss: 2.7787, val_error: 0.2143, auc: 0.8399\n",
      "class 0 clustering acc 0.8913690476190477: correct 1198/1344\n",
      "class 1 clustering acc 0.7321428571428571: correct 492/672\n",
      "class 0: acc 0.7692307692307693, correct 30/39\n",
      "class 1: acc 0.8, correct 36/45\n",
      "EarlyStopping counter: 33 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0013, instance_loss: 0.1333, weighted_loss: 0.0409, label: 0, bag_size: 14\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 68\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 58\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 75\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 87\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 122\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0025, weighted_loss: 0.0007, label: 1, bag_size: 56\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0054, weighted_loss: 0.0016, label: 0, bag_size: 55\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0027, weighted_loss: 0.0008, label: 0, bag_size: 51\n",
      "batch 199, loss: 0.0014, instance_loss: 0.0004, weighted_loss: 0.0011, label: 0, bag_size: 108\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 110\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0192, weighted_loss: 0.0058, label: 0, bag_size: 49\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 0, bag_size: 75\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 128\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0025, weighted_loss: 0.0008, label: 0, bag_size: 25\n",
      "batch 319, loss: 0.4717, instance_loss: 0.7824, weighted_loss: 0.5649, label: 1, bag_size: 16\n",
      "batch 339, loss: 9.6109, instance_loss: 1.4572, weighted_loss: 7.1648, label: 0, bag_size: 28\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0023, weighted_loss: 0.0007, label: 0, bag_size: 99\n",
      "batch 379, loss: 0.8101, instance_loss: 1.4723, weighted_loss: 1.0088, label: 0, bag_size: 25\n",
      "batch 399, loss: 0.0003, instance_loss: 0.0206, weighted_loss: 0.0064, label: 0, bag_size: 47\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0182, weighted_loss: 0.0055, label: 0, bag_size: 80\n",
      "batch 439, loss: 0.0022, instance_loss: 0.5248, weighted_loss: 0.1590, label: 1, bag_size: 21\n",
      "batch 459, loss: 0.2703, instance_loss: 0.0568, weighted_loss: 0.2062, label: 0, bag_size: 38\n",
      "batch 479, loss: 10.7345, instance_loss: 2.8741, weighted_loss: 8.3763, label: 1, bag_size: 63\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0031, weighted_loss: 0.0009, label: 0, bag_size: 25\n",
      "batch 519, loss: 7.7811, instance_loss: 0.8506, weighted_loss: 5.7019, label: 1, bag_size: 60\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0340, weighted_loss: 0.0102, label: 1, bag_size: 22\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0292, weighted_loss: 0.0088, label: 1, bag_size: 56\n",
      "batch 579, loss: 5.5891, instance_loss: 1.5363, weighted_loss: 4.3733, label: 1, bag_size: 53\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 37\n",
      "batch 619, loss: 5.5113, instance_loss: 1.2879, weighted_loss: 4.2443, label: 0, bag_size: 28\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0013, weighted_loss: 0.0005, label: 1, bag_size: 64\n",
      "batch 659, loss: 0.0310, instance_loss: 0.0332, weighted_loss: 0.0317, label: 0, bag_size: 75\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9770833333333333: correct 10318/10560\n",
      "class 1 clustering acc 0.8901515151515151: correct 4700/5280\n",
      "Epoch: 46, train_loss: 0.2775, train_clustering_loss:  0.2096, train_error: 0.0742\n",
      "class 0: acc 0.9221183800623053, correct 296/321\n",
      "class 1: acc 0.9292035398230089, correct 315/339\n",
      "\n",
      "Val Set, val_loss: 2.2338, val_error: 0.2738, auc: 0.8493\n",
      "class 0 clustering acc 0.8824404761904762: correct 1186/1344\n",
      "class 1 clustering acc 0.7127976190476191: correct 479/672\n",
      "class 0: acc 0.5641025641025641, correct 22/39\n",
      "class 1: acc 0.8666666666666667, correct 39/45\n",
      "EarlyStopping counter: 34 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 61\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0049, weighted_loss: 0.0015, label: 0, bag_size: 24\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 84\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0006, label: 0, bag_size: 56\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 13\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 52\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 76\n",
      "batch 159, loss: 0.0310, instance_loss: 0.2090, weighted_loss: 0.0844, label: 0, bag_size: 103\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 90\n",
      "batch 199, loss: 0.0005, instance_loss: 0.0018, weighted_loss: 0.0009, label: 1, bag_size: 80\n",
      "batch 219, loss: 0.9621, instance_loss: 0.0446, weighted_loss: 0.6869, label: 1, bag_size: 41\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 73\n",
      "batch 259, loss: 0.0003, instance_loss: 0.0047, weighted_loss: 0.0016, label: 1, bag_size: 84\n",
      "batch 279, loss: 0.0006, instance_loss: 0.0011, weighted_loss: 0.0008, label: 1, bag_size: 115\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 82\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0196, weighted_loss: 0.0059, label: 0, bag_size: 73\n",
      "batch 339, loss: 0.1278, instance_loss: 0.0378, weighted_loss: 0.1008, label: 1, bag_size: 51\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 78\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0257, weighted_loss: 0.0077, label: 0, bag_size: 90\n",
      "batch 399, loss: 0.0000, instance_loss: 0.1446, weighted_loss: 0.0434, label: 1, bag_size: 21\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0137, weighted_loss: 0.0041, label: 0, bag_size: 80\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0048, weighted_loss: 0.0015, label: 1, bag_size: 77\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0027, weighted_loss: 0.0008, label: 1, bag_size: 98\n",
      "batch 479, loss: 0.9435, instance_loss: 0.1951, weighted_loss: 0.7190, label: 1, bag_size: 31\n",
      "batch 499, loss: 1.6208, instance_loss: 3.0793, weighted_loss: 2.0583, label: 0, bag_size: 61\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0052, weighted_loss: 0.0016, label: 1, bag_size: 61\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0026, weighted_loss: 0.0008, label: 0, bag_size: 27\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0107, weighted_loss: 0.0032, label: 1, bag_size: 30\n",
      "batch 579, loss: 0.0001, instance_loss: 0.4152, weighted_loss: 0.1247, label: 1, bag_size: 119\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 37\n",
      "batch 619, loss: 2.7998, instance_loss: 1.2716, weighted_loss: 2.3413, label: 0, bag_size: 94\n",
      "batch 639, loss: 0.7129, instance_loss: 0.4201, weighted_loss: 0.6251, label: 1, bag_size: 51\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 85\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9794507575757576: correct 10343/10560\n",
      "class 1 clustering acc 0.906439393939394: correct 4786/5280\n",
      "Epoch: 47, train_loss: 0.4032, train_clustering_loss:  0.1900, train_error: 0.0879\n",
      "class 0: acc 0.9090909090909091, correct 300/330\n",
      "class 1: acc 0.9151515151515152, correct 302/330\n",
      "\n",
      "Val Set, val_loss: 3.2825, val_error: 0.3095, auc: 0.8359\n",
      "class 0 clustering acc 0.8958333333333334: correct 1204/1344\n",
      "class 1 clustering acc 0.7098214285714286: correct 477/672\n",
      "class 0: acc 0.4358974358974359, correct 17/39\n",
      "class 1: acc 0.9111111111111111, correct 41/45\n",
      "EarlyStopping counter: 35 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 13\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 20\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0038, weighted_loss: 0.0011, label: 0, bag_size: 29\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 65\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 115\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 36\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 76\n",
      "batch 159, loss: 0.0006, instance_loss: 0.0006, weighted_loss: 0.0006, label: 1, bag_size: 31\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 106\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 58\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0126, weighted_loss: 0.0039, label: 1, bag_size: 84\n",
      "batch 239, loss: 0.0009, instance_loss: 0.2835, weighted_loss: 0.0857, label: 1, bag_size: 78\n",
      "batch 259, loss: 0.0198, instance_loss: 0.1671, weighted_loss: 0.0640, label: 0, bag_size: 78\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 44\n",
      "batch 299, loss: 0.0005, instance_loss: 0.0532, weighted_loss: 0.0163, label: 0, bag_size: 42\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0155, weighted_loss: 0.0047, label: 1, bag_size: 48\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 26\n",
      "batch 359, loss: 0.0009, instance_loss: 0.0046, weighted_loss: 0.0020, label: 1, bag_size: 52\n",
      "batch 379, loss: 0.1994, instance_loss: 0.0794, weighted_loss: 0.1634, label: 1, bag_size: 84\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 37\n",
      "batch 419, loss: 0.0003, instance_loss: 0.0192, weighted_loss: 0.0059, label: 0, bag_size: 27\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 50\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 35\n",
      "batch 479, loss: 0.0195, instance_loss: 0.0133, weighted_loss: 0.0177, label: 0, bag_size: 28\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 55\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 38\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 0, bag_size: 86\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 84\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 32\n",
      "batch 599, loss: 0.0101, instance_loss: 0.0433, weighted_loss: 0.0201, label: 1, bag_size: 48\n",
      "batch 619, loss: 0.0885, instance_loss: 0.1834, weighted_loss: 0.1169, label: 1, bag_size: 51\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 45\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 14\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.984185606060606: correct 10393/10560\n",
      "class 1 clustering acc 0.9240530303030303: correct 4879/5280\n",
      "Epoch: 48, train_loss: 0.1610, train_clustering_loss:  0.1431, train_error: 0.0455\n",
      "class 0: acc 0.9507692307692308, correct 309/325\n",
      "class 1: acc 0.9582089552238806, correct 321/335\n",
      "\n",
      "Val Set, val_loss: 1.7114, val_error: 0.1786, auc: 0.8672\n",
      "class 0 clustering acc 0.9025297619047619: correct 1213/1344\n",
      "class 1 clustering acc 0.7767857142857143: correct 522/672\n",
      "class 0: acc 0.7948717948717948, correct 31/39\n",
      "class 1: acc 0.8444444444444444, correct 38/45\n",
      "EarlyStopping counter: 36 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 140\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 88\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 71\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 76\n",
      "batch 99, loss: 0.2450, instance_loss: 0.4356, weighted_loss: 0.3022, label: 0, bag_size: 28\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 72\n",
      "batch 139, loss: 0.0294, instance_loss: 0.0177, weighted_loss: 0.0259, label: 1, bag_size: 119\n",
      "batch 159, loss: 0.0009, instance_loss: 0.0339, weighted_loss: 0.0108, label: 0, bag_size: 37\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 87\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 78\n",
      "batch 219, loss: 0.0024, instance_loss: 0.0070, weighted_loss: 0.0038, label: 0, bag_size: 67\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 1, bag_size: 78\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0033, weighted_loss: 0.0010, label: 1, bag_size: 85\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0005, label: 1, bag_size: 34\n",
      "batch 299, loss: 0.0136, instance_loss: 0.0050, weighted_loss: 0.0110, label: 1, bag_size: 41\n",
      "batch 319, loss: 0.0002, instance_loss: 0.0088, weighted_loss: 0.0028, label: 1, bag_size: 104\n",
      "batch 339, loss: 0.0271, instance_loss: 0.2425, weighted_loss: 0.0917, label: 1, bag_size: 83\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0153, weighted_loss: 0.0046, label: 1, bag_size: 95\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 122\n",
      "batch 399, loss: 0.0148, instance_loss: 0.0741, weighted_loss: 0.0326, label: 1, bag_size: 17\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 85\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0172, weighted_loss: 0.0052, label: 0, bag_size: 94\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0046, weighted_loss: 0.0014, label: 0, bag_size: 101\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 89\n",
      "batch 499, loss: 19.5480, instance_loss: 0.9954, weighted_loss: 13.9822, label: 0, bag_size: 96\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0287, weighted_loss: 0.0086, label: 1, bag_size: 85\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 116\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0268, weighted_loss: 0.0080, label: 0, bag_size: 96\n",
      "batch 579, loss: 0.0000, instance_loss: 0.2773, weighted_loss: 0.0832, label: 1, bag_size: 30\n",
      "batch 599, loss: 0.0000, instance_loss: 0.1896, weighted_loss: 0.0569, label: 1, bag_size: 120\n",
      "batch 619, loss: 0.1750, instance_loss: 0.0001, weighted_loss: 0.1225, label: 1, bag_size: 48\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0137, weighted_loss: 0.0041, label: 1, bag_size: 38\n",
      "batch 659, loss: 54.8337, instance_loss: 1.1450, weighted_loss: 38.7271, label: 1, bag_size: 51\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9635416666666666: correct 10175/10560\n",
      "class 1 clustering acc 0.8583333333333333: correct 4532/5280\n",
      "Epoch: 49, train_loss: 0.8885, train_clustering_loss:  0.3456, train_error: 0.0955\n",
      "class 0: acc 0.90625, correct 290/320\n",
      "class 1: acc 0.9029411764705882, correct 307/340\n",
      "\n",
      "Val Set, val_loss: 8.1831, val_error: 0.2619, auc: 0.8342\n",
      "class 0 clustering acc 0.7864583333333334: correct 1057/1344\n",
      "class 1 clustering acc 0.5714285714285714: correct 384/672\n",
      "class 0: acc 1.0, correct 39/39\n",
      "class 1: acc 0.5111111111111111, correct 23/45\n",
      "EarlyStopping counter: 37 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0224, weighted_loss: 0.0067, label: 0, bag_size: 81\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0052, weighted_loss: 0.0016, label: 0, bag_size: 78\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0022, weighted_loss: 0.0006, label: 1, bag_size: 71\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 49\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 28\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0249, weighted_loss: 0.0075, label: 1, bag_size: 76\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0422, weighted_loss: 0.0127, label: 1, bag_size: 75\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0087, weighted_loss: 0.0026, label: 0, bag_size: 52\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0299, weighted_loss: 0.0090, label: 0, bag_size: 41\n",
      "batch 199, loss: 0.0090, instance_loss: 0.5007, weighted_loss: 0.1565, label: 0, bag_size: 17\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0293, weighted_loss: 0.0088, label: 0, bag_size: 86\n",
      "batch 239, loss: 0.0000, instance_loss: 0.8876, weighted_loss: 0.2663, label: 0, bag_size: 164\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0091, weighted_loss: 0.0027, label: 1, bag_size: 53\n",
      "batch 279, loss: 0.0000, instance_loss: 0.4908, weighted_loss: 0.1472, label: 0, bag_size: 49\n",
      "batch 299, loss: 0.0000, instance_loss: 0.4353, weighted_loss: 0.1306, label: 0, bag_size: 108\n",
      "batch 319, loss: 0.0000, instance_loss: 0.2183, weighted_loss: 0.0655, label: 0, bag_size: 96\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0120, weighted_loss: 0.0036, label: 0, bag_size: 103\n",
      "batch 359, loss: 0.0000, instance_loss: 0.4680, weighted_loss: 0.1404, label: 0, bag_size: 107\n",
      "batch 379, loss: 0.0001, instance_loss: 0.5020, weighted_loss: 0.1507, label: 0, bag_size: 96\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0240, weighted_loss: 0.0072, label: 0, bag_size: 67\n",
      "batch 419, loss: 0.0000, instance_loss: 1.6360, weighted_loss: 0.4908, label: 1, bag_size: 45\n",
      "batch 439, loss: 0.0000, instance_loss: 0.1195, weighted_loss: 0.0358, label: 1, bag_size: 97\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0472, weighted_loss: 0.0142, label: 1, bag_size: 24\n",
      "batch 479, loss: 8.1647, instance_loss: 0.1059, weighted_loss: 5.7471, label: 1, bag_size: 153\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 0, bag_size: 36\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 76\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 35\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0026, weighted_loss: 0.0008, label: 1, bag_size: 112\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0049, weighted_loss: 0.0015, label: 1, bag_size: 88\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0395, weighted_loss: 0.0118, label: 1, bag_size: 76\n",
      "batch 619, loss: 0.0001, instance_loss: 0.5354, weighted_loss: 0.1607, label: 1, bag_size: 74\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 94\n",
      "batch 659, loss: 0.0000, instance_loss: 0.4488, weighted_loss: 0.1346, label: 0, bag_size: 88\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9490530303030303: correct 10022/10560\n",
      "class 1 clustering acc 0.7712121212121212: correct 4072/5280\n",
      "Epoch: 50, train_loss: 1.7510, train_clustering_loss:  0.5173, train_error: 0.1258\n",
      "class 0: acc 0.8719512195121951, correct 286/328\n",
      "class 1: acc 0.8765060240963856, correct 291/332\n",
      "\n",
      "Val Set, val_loss: 4.2187, val_error: 0.2024, auc: 0.8165\n",
      "class 0 clustering acc 0.8928571428571429: correct 1200/1344\n",
      "class 1 clustering acc 0.6190476190476191: correct 416/672\n",
      "class 0: acc 0.7692307692307693, correct 30/39\n",
      "class 1: acc 0.8222222222222222, correct 37/45\n",
      "EarlyStopping counter: 38 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 26\n",
      "batch 39, loss: 0.0000, instance_loss: 0.3149, weighted_loss: 0.0945, label: 1, bag_size: 86\n",
      "batch 59, loss: 1.6028, instance_loss: 2.3981, weighted_loss: 1.8414, label: 1, bag_size: 31\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 37\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 1, bag_size: 76\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0053, weighted_loss: 0.0016, label: 1, bag_size: 32\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 51\n",
      "batch 159, loss: 0.0001, instance_loss: 0.1701, weighted_loss: 0.0511, label: 0, bag_size: 28\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 97\n",
      "batch 199, loss: 7.2876, instance_loss: 2.7426, weighted_loss: 5.9241, label: 0, bag_size: 42\n",
      "batch 219, loss: 0.0000, instance_loss: 0.3258, weighted_loss: 0.0977, label: 1, bag_size: 45\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0024, weighted_loss: 0.0007, label: 0, bag_size: 101\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0016, weighted_loss: 0.0005, label: 1, bag_size: 118\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 0, bag_size: 91\n",
      "batch 299, loss: 0.2539, instance_loss: 0.2197, weighted_loss: 0.2436, label: 1, bag_size: 95\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0194, weighted_loss: 0.0058, label: 0, bag_size: 67\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 78\n",
      "batch 359, loss: 0.0077, instance_loss: 2.5236, weighted_loss: 0.7624, label: 0, bag_size: 72\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0166, weighted_loss: 0.0050, label: 1, bag_size: 25\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 0, bag_size: 28\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0083, weighted_loss: 0.0025, label: 0, bag_size: 45\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 96\n",
      "batch 459, loss: 3.0257, instance_loss: 1.9642, weighted_loss: 2.7073, label: 1, bag_size: 63\n",
      "batch 479, loss: 0.0061, instance_loss: 0.0100, weighted_loss: 0.0073, label: 0, bag_size: 87\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0021, weighted_loss: 0.0006, label: 0, bag_size: 36\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0135, weighted_loss: 0.0041, label: 1, bag_size: 52\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 114\n",
      "batch 559, loss: 2.7969, instance_loss: 0.4832, weighted_loss: 2.1028, label: 0, bag_size: 75\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0016, weighted_loss: 0.0005, label: 1, bag_size: 23\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 37\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 0, bag_size: 79\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0731, weighted_loss: 0.0220, label: 0, bag_size: 41\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 80\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9644886363636364: correct 10185/10560\n",
      "class 1 clustering acc 0.8393939393939394: correct 4432/5280\n",
      "Epoch: 51, train_loss: 0.5778, train_clustering_loss:  0.3480, train_error: 0.0636\n",
      "class 0: acc 0.9343283582089552, correct 313/335\n",
      "class 1: acc 0.9384615384615385, correct 305/325\n",
      "\n",
      "Val Set, val_loss: 2.4763, val_error: 0.2143, auc: 0.8376\n",
      "class 0 clustering acc 0.9047619047619048: correct 1216/1344\n",
      "class 1 clustering acc 0.5863095238095238: correct 394/672\n",
      "class 0: acc 0.7692307692307693, correct 30/39\n",
      "class 1: acc 0.8, correct 36/45\n",
      "EarlyStopping counter: 39 out of 20\n",
      "Early stopping\n",
      "Val error: 0.1786, ROC AUC: 0.9054\n",
      "Test error: 0.0714, ROC AUC: 0.9818\n",
      "class 0: acc 0.8529411764705882, correct 29/34\n",
      "class 1: acc 0.98, correct 49/50\n",
      "\n",
      "Training Fold 3!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 660 samples\n",
      "Validating on 74 samples\n",
      "Testing on 94 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Setting tau to 1.0\n",
      "Done!\n",
      "MCBAT_SB(\n",
      "  (instance_loss_fn): SmoothTop1SVM()\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (transformer_low): TransformerEncoder_FLASHAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FlashAttention(\n",
      "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
      "            (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer_high): TransformerEncoder_FLASHAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FlashAttention(\n",
      "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
      "            (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attention): Attn_Net_Gated(\n",
      "    (attention_a): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_b): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (attention_V2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (attention_U2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (attention_weights2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Total number of parameters: 8408073\n",
      "Total number of trainable parameters: 8408073\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "660.0\n",
      "2\n",
      "319\n",
      "341\n",
      "##################################################\n",
      "Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 3.3166, instance_loss: 1.3514, weighted_loss: 2.7270, label: 0, bag_size: 29\n",
      "batch 39, loss: 0.2724, instance_loss: 0.6846, weighted_loss: 0.3960, label: 0, bag_size: 106\n",
      "batch 59, loss: 2.5315, instance_loss: 0.8598, weighted_loss: 2.0300, label: 1, bag_size: 38\n",
      "batch 79, loss: 0.1125, instance_loss: 1.1103, weighted_loss: 0.4118, label: 1, bag_size: 59\n",
      "batch 99, loss: 1.4694, instance_loss: 0.8525, weighted_loss: 1.2843, label: 0, bag_size: 65\n",
      "batch 119, loss: 2.5638, instance_loss: 1.1369, weighted_loss: 2.1357, label: 0, bag_size: 25\n",
      "batch 139, loss: 0.1780, instance_loss: 1.1454, weighted_loss: 0.4682, label: 0, bag_size: 94\n",
      "batch 159, loss: 2.1670, instance_loss: 1.5244, weighted_loss: 1.9742, label: 0, bag_size: 95\n",
      "batch 179, loss: 0.3151, instance_loss: 0.9301, weighted_loss: 0.4996, label: 0, bag_size: 28\n",
      "batch 199, loss: 3.3260, instance_loss: 1.2745, weighted_loss: 2.7105, label: 1, bag_size: 107\n",
      "batch 219, loss: 0.6270, instance_loss: 0.7589, weighted_loss: 0.6666, label: 0, bag_size: 72\n",
      "batch 239, loss: 0.1561, instance_loss: 1.5265, weighted_loss: 0.5673, label: 1, bag_size: 24\n",
      "batch 259, loss: 0.1793, instance_loss: 0.9107, weighted_loss: 0.3987, label: 1, bag_size: 81\n",
      "batch 279, loss: 0.3737, instance_loss: 0.8171, weighted_loss: 0.5067, label: 1, bag_size: 25\n",
      "batch 299, loss: 0.0071, instance_loss: 1.0524, weighted_loss: 0.3207, label: 0, bag_size: 33\n",
      "batch 319, loss: 1.9790, instance_loss: 0.9003, weighted_loss: 1.6554, label: 1, bag_size: 69\n",
      "batch 339, loss: 0.8049, instance_loss: 0.9135, weighted_loss: 0.8375, label: 0, bag_size: 38\n",
      "batch 359, loss: 0.0175, instance_loss: 1.2320, weighted_loss: 0.3818, label: 1, bag_size: 112\n",
      "batch 379, loss: 1.0427, instance_loss: 0.9304, weighted_loss: 1.0090, label: 1, bag_size: 69\n",
      "batch 399, loss: 1.5241, instance_loss: 0.8619, weighted_loss: 1.3254, label: 0, bag_size: 46\n",
      "batch 419, loss: 0.2059, instance_loss: 0.9421, weighted_loss: 0.4268, label: 0, bag_size: 50\n",
      "batch 439, loss: 0.5944, instance_loss: 0.8708, weighted_loss: 0.6773, label: 0, bag_size: 91\n",
      "batch 459, loss: 0.1998, instance_loss: 0.5587, weighted_loss: 0.3075, label: 1, bag_size: 17\n",
      "batch 479, loss: 3.7884, instance_loss: 1.5095, weighted_loss: 3.1047, label: 0, bag_size: 27\n",
      "batch 499, loss: 0.0693, instance_loss: 0.7150, weighted_loss: 0.2630, label: 1, bag_size: 53\n",
      "batch 519, loss: 1.5224, instance_loss: 1.0969, weighted_loss: 1.3948, label: 1, bag_size: 41\n",
      "batch 539, loss: 2.3910, instance_loss: 0.9515, weighted_loss: 1.9591, label: 1, bag_size: 24\n",
      "batch 559, loss: 0.1405, instance_loss: 1.1474, weighted_loss: 0.4425, label: 0, bag_size: 96\n",
      "batch 579, loss: 4.6525, instance_loss: 1.0360, weighted_loss: 3.5675, label: 1, bag_size: 63\n",
      "batch 599, loss: 0.2351, instance_loss: 0.7540, weighted_loss: 0.3908, label: 1, bag_size: 36\n",
      "batch 619, loss: 0.3020, instance_loss: 0.8308, weighted_loss: 0.4606, label: 1, bag_size: 88\n",
      "batch 639, loss: 1.2971, instance_loss: 1.4646, weighted_loss: 1.3473, label: 0, bag_size: 107\n",
      "batch 659, loss: 5.1641, instance_loss: 1.2441, weighted_loss: 3.9881, label: 0, bag_size: 110\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.978125: correct 10329/10560\n",
      "class 1 clustering acc 0.11458333333333333: correct 605/5280\n",
      "Epoch: 0, train_loss: 0.9639, train_clustering_loss:  0.9613, train_error: 0.3864\n",
      "class 0: acc 0.6097560975609756, correct 200/328\n",
      "class 1: acc 0.6174698795180723, correct 205/332\n",
      "\n",
      "Val Set, val_loss: 0.9674, val_error: 0.2973, auc: 0.8176\n",
      "class 0 clustering acc 0.9763513513513513: correct 1156/1184\n",
      "class 1 clustering acc 0.2719594594594595: correct 161/592\n",
      "class 0: acc 0.35294117647058826, correct 12/34\n",
      "class 1: acc 1.0, correct 40/40\n",
      "Validation loss decreased (inf --> 0.967417).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.8014, instance_loss: 0.9112, weighted_loss: 0.8343, label: 1, bag_size: 79\n",
      "batch 39, loss: 0.0485, instance_loss: 0.6986, weighted_loss: 0.2435, label: 1, bag_size: 76\n",
      "batch 59, loss: 1.1404, instance_loss: 0.8491, weighted_loss: 1.0530, label: 0, bag_size: 65\n",
      "batch 79, loss: 0.1766, instance_loss: 0.6542, weighted_loss: 0.3199, label: 0, bag_size: 38\n",
      "batch 99, loss: 0.0033, instance_loss: 1.1183, weighted_loss: 0.3378, label: 0, bag_size: 65\n",
      "batch 119, loss: 0.2050, instance_loss: 0.8803, weighted_loss: 0.4076, label: 1, bag_size: 82\n",
      "batch 139, loss: 0.0292, instance_loss: 0.7366, weighted_loss: 0.2414, label: 1, bag_size: 21\n",
      "batch 159, loss: 0.6566, instance_loss: 2.0000, weighted_loss: 1.0596, label: 1, bag_size: 38\n",
      "batch 179, loss: 0.1684, instance_loss: 0.7319, weighted_loss: 0.3375, label: 1, bag_size: 83\n",
      "batch 199, loss: 1.2327, instance_loss: 0.9184, weighted_loss: 1.1384, label: 1, bag_size: 78\n",
      "batch 219, loss: 0.0955, instance_loss: 0.9928, weighted_loss: 0.3647, label: 0, bag_size: 33\n",
      "batch 239, loss: 1.7328, instance_loss: 0.8146, weighted_loss: 1.4574, label: 0, bag_size: 50\n",
      "batch 259, loss: 1.0221, instance_loss: 1.1992, weighted_loss: 1.0752, label: 1, bag_size: 87\n",
      "batch 279, loss: 0.0189, instance_loss: 0.8960, weighted_loss: 0.2820, label: 0, bag_size: 25\n",
      "batch 299, loss: 0.4841, instance_loss: 0.6935, weighted_loss: 0.5469, label: 0, bag_size: 77\n",
      "batch 319, loss: 0.0036, instance_loss: 0.7200, weighted_loss: 0.2185, label: 0, bag_size: 88\n",
      "batch 339, loss: 1.2716, instance_loss: 0.9231, weighted_loss: 1.1671, label: 1, bag_size: 29\n",
      "batch 359, loss: 0.1536, instance_loss: 0.6061, weighted_loss: 0.2894, label: 1, bag_size: 86\n",
      "batch 379, loss: 0.0398, instance_loss: 0.5870, weighted_loss: 0.2040, label: 1, bag_size: 85\n",
      "batch 399, loss: 0.6036, instance_loss: 1.2857, weighted_loss: 0.8082, label: 0, bag_size: 18\n",
      "batch 419, loss: 0.1004, instance_loss: 0.7892, weighted_loss: 0.3070, label: 0, bag_size: 96\n",
      "batch 439, loss: 1.6898, instance_loss: 1.1731, weighted_loss: 1.5348, label: 0, bag_size: 90\n",
      "batch 459, loss: 0.0144, instance_loss: 0.6978, weighted_loss: 0.2194, label: 0, bag_size: 83\n",
      "batch 479, loss: 0.5528, instance_loss: 0.8675, weighted_loss: 0.6472, label: 1, bag_size: 96\n",
      "batch 499, loss: 0.4068, instance_loss: 0.6498, weighted_loss: 0.4797, label: 1, bag_size: 123\n",
      "batch 519, loss: 0.2683, instance_loss: 0.6890, weighted_loss: 0.3945, label: 1, bag_size: 74\n",
      "batch 539, loss: 0.5970, instance_loss: 0.6553, weighted_loss: 0.6145, label: 1, bag_size: 63\n",
      "batch 559, loss: 0.1433, instance_loss: 1.1732, weighted_loss: 0.4523, label: 0, bag_size: 32\n",
      "batch 579, loss: 0.6940, instance_loss: 1.2736, weighted_loss: 0.8679, label: 0, bag_size: 67\n",
      "batch 599, loss: 1.4988, instance_loss: 1.1175, weighted_loss: 1.3844, label: 0, bag_size: 26\n",
      "batch 619, loss: 0.1192, instance_loss: 0.5683, weighted_loss: 0.2539, label: 1, bag_size: 24\n",
      "batch 639, loss: 1.4725, instance_loss: 0.9671, weighted_loss: 1.3209, label: 0, bag_size: 73\n",
      "batch 659, loss: 1.1830, instance_loss: 0.7657, weighted_loss: 1.0578, label: 0, bag_size: 93\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9646780303030303: correct 10187/10560\n",
      "class 1 clustering acc 0.12556818181818183: correct 663/5280\n",
      "Epoch: 1, train_loss: 0.7103, train_clustering_loss:  0.9213, train_error: 0.3152\n",
      "class 0: acc 0.6697819314641744, correct 215/321\n",
      "class 1: acc 0.6991150442477876, correct 237/339\n",
      "\n",
      "Val Set, val_loss: 0.5623, val_error: 0.2568, auc: 0.8478\n",
      "class 0 clustering acc 0.9636824324324325: correct 1141/1184\n",
      "class 1 clustering acc 0.32432432432432434: correct 192/592\n",
      "class 0: acc 0.5, correct 17/34\n",
      "class 1: acc 0.95, correct 38/40\n",
      "Validation loss decreased (0.967417 --> 0.562252).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0018, instance_loss: 0.6516, weighted_loss: 0.1968, label: 1, bag_size: 31\n",
      "batch 39, loss: 0.5179, instance_loss: 0.6937, weighted_loss: 0.5707, label: 0, bag_size: 64\n",
      "batch 59, loss: 0.4414, instance_loss: 0.8414, weighted_loss: 0.5614, label: 1, bag_size: 69\n",
      "batch 79, loss: 0.0287, instance_loss: 0.7018, weighted_loss: 0.2307, label: 0, bag_size: 25\n",
      "batch 99, loss: 2.0154, instance_loss: 0.8627, weighted_loss: 1.6696, label: 1, bag_size: 89\n",
      "batch 119, loss: 1.0358, instance_loss: 1.0107, weighted_loss: 1.0283, label: 1, bag_size: 127\n",
      "batch 139, loss: 0.0046, instance_loss: 0.6513, weighted_loss: 0.1986, label: 0, bag_size: 108\n",
      "batch 159, loss: 0.0411, instance_loss: 0.6607, weighted_loss: 0.2270, label: 1, bag_size: 100\n",
      "batch 179, loss: 3.1373, instance_loss: 1.2411, weighted_loss: 2.5685, label: 0, bag_size: 25\n",
      "batch 199, loss: 1.8864, instance_loss: 1.0808, weighted_loss: 1.6447, label: 1, bag_size: 30\n",
      "batch 219, loss: 0.0517, instance_loss: 0.6577, weighted_loss: 0.2335, label: 0, bag_size: 95\n",
      "batch 239, loss: 0.0021, instance_loss: 0.4760, weighted_loss: 0.1443, label: 1, bag_size: 47\n",
      "batch 259, loss: 0.2151, instance_loss: 0.7479, weighted_loss: 0.3749, label: 0, bag_size: 59\n",
      "batch 279, loss: 0.0052, instance_loss: 0.5783, weighted_loss: 0.1772, label: 0, bag_size: 109\n",
      "batch 299, loss: 0.5416, instance_loss: 1.3509, weighted_loss: 0.7844, label: 1, bag_size: 96\n",
      "batch 319, loss: 0.1028, instance_loss: 0.6309, weighted_loss: 0.2613, label: 1, bag_size: 24\n",
      "batch 339, loss: 1.7210, instance_loss: 1.2273, weighted_loss: 1.5729, label: 0, bag_size: 88\n",
      "batch 359, loss: 0.1016, instance_loss: 0.5938, weighted_loss: 0.2492, label: 1, bag_size: 77\n",
      "batch 379, loss: 0.0127, instance_loss: 0.8457, weighted_loss: 0.2626, label: 0, bag_size: 69\n",
      "batch 399, loss: 1.0595, instance_loss: 1.2714, weighted_loss: 1.1231, label: 0, bag_size: 94\n",
      "batch 419, loss: 0.0046, instance_loss: 0.8266, weighted_loss: 0.2512, label: 1, bag_size: 56\n",
      "batch 439, loss: 0.0082, instance_loss: 0.6031, weighted_loss: 0.1866, label: 1, bag_size: 99\n",
      "batch 459, loss: 0.8224, instance_loss: 1.0361, weighted_loss: 0.8865, label: 0, bag_size: 114\n",
      "batch 479, loss: 0.3629, instance_loss: 0.7520, weighted_loss: 0.4796, label: 0, bag_size: 25\n",
      "batch 499, loss: 0.8585, instance_loss: 1.4201, weighted_loss: 1.0270, label: 1, bag_size: 45\n",
      "batch 519, loss: 0.1079, instance_loss: 0.4625, weighted_loss: 0.2142, label: 1, bag_size: 89\n",
      "batch 539, loss: 0.5478, instance_loss: 0.7378, weighted_loss: 0.6048, label: 0, bag_size: 69\n",
      "batch 559, loss: 1.7880, instance_loss: 2.0296, weighted_loss: 1.8605, label: 0, bag_size: 71\n",
      "batch 579, loss: 0.0115, instance_loss: 0.3445, weighted_loss: 0.1114, label: 0, bag_size: 78\n",
      "batch 599, loss: 1.0960, instance_loss: 1.2085, weighted_loss: 1.1298, label: 1, bag_size: 97\n",
      "batch 619, loss: 0.1529, instance_loss: 0.1999, weighted_loss: 0.1670, label: 1, bag_size: 29\n",
      "batch 639, loss: 0.2801, instance_loss: 0.3446, weighted_loss: 0.2995, label: 0, bag_size: 35\n",
      "batch 659, loss: 2.0693, instance_loss: 1.4463, weighted_loss: 1.8824, label: 0, bag_size: 51\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9454545454545454: correct 9984/10560\n",
      "class 1 clustering acc 0.2935606060606061: correct 1550/5280\n",
      "Epoch: 2, train_loss: 0.5580, train_clustering_loss:  0.8112, train_error: 0.2212\n",
      "class 0: acc 0.7523809523809524, correct 237/315\n",
      "class 1: acc 0.8028985507246377, correct 277/345\n",
      "\n",
      "Val Set, val_loss: 0.5342, val_error: 0.1757, auc: 0.8618\n",
      "class 0 clustering acc 0.9315878378378378: correct 1103/1184\n",
      "class 1 clustering acc 0.6638513513513513: correct 393/592\n",
      "class 0: acc 0.7941176470588235, correct 27/34\n",
      "class 1: acc 0.85, correct 34/40\n",
      "Validation loss decreased (0.562252 --> 0.534151).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 2.6985, instance_loss: 2.7383, weighted_loss: 2.7105, label: 1, bag_size: 89\n",
      "batch 39, loss: 0.1328, instance_loss: 0.4601, weighted_loss: 0.2310, label: 1, bag_size: 65\n",
      "batch 59, loss: 0.0007, instance_loss: 0.0161, weighted_loss: 0.0053, label: 1, bag_size: 83\n",
      "batch 79, loss: 0.0059, instance_loss: 0.0381, weighted_loss: 0.0155, label: 1, bag_size: 61\n",
      "batch 99, loss: 0.4058, instance_loss: 0.0837, weighted_loss: 0.3092, label: 0, bag_size: 37\n",
      "batch 119, loss: 2.0843, instance_loss: 1.5159, weighted_loss: 1.9138, label: 1, bag_size: 87\n",
      "batch 139, loss: 0.8515, instance_loss: 0.5466, weighted_loss: 0.7600, label: 0, bag_size: 27\n",
      "batch 159, loss: 0.0420, instance_loss: 0.4377, weighted_loss: 0.1607, label: 1, bag_size: 96\n",
      "batch 179, loss: 1.8276, instance_loss: 0.1934, weighted_loss: 1.3373, label: 0, bag_size: 83\n",
      "batch 199, loss: 0.0051, instance_loss: 0.0646, weighted_loss: 0.0230, label: 0, bag_size: 63\n",
      "batch 219, loss: 2.7573, instance_loss: 3.0892, weighted_loss: 2.8569, label: 0, bag_size: 35\n",
      "batch 239, loss: 1.4689, instance_loss: 0.4921, weighted_loss: 1.1759, label: 1, bag_size: 25\n",
      "batch 259, loss: 0.0629, instance_loss: 0.1450, weighted_loss: 0.0876, label: 1, bag_size: 32\n",
      "batch 279, loss: 1.9955, instance_loss: 1.7299, weighted_loss: 1.9158, label: 1, bag_size: 82\n",
      "batch 299, loss: 0.0057, instance_loss: 0.1961, weighted_loss: 0.0628, label: 1, bag_size: 81\n",
      "batch 319, loss: 0.0789, instance_loss: 0.2251, weighted_loss: 0.1227, label: 1, bag_size: 95\n",
      "batch 339, loss: 0.3215, instance_loss: 0.1800, weighted_loss: 0.2791, label: 0, bag_size: 96\n",
      "batch 359, loss: 0.0047, instance_loss: 0.0733, weighted_loss: 0.0253, label: 1, bag_size: 28\n",
      "batch 379, loss: 0.2412, instance_loss: 0.2189, weighted_loss: 0.2345, label: 0, bag_size: 65\n",
      "batch 399, loss: 0.1884, instance_loss: 0.9831, weighted_loss: 0.4268, label: 0, bag_size: 73\n",
      "batch 419, loss: 0.2123, instance_loss: 1.3208, weighted_loss: 0.5448, label: 0, bag_size: 29\n",
      "batch 439, loss: 2.6232, instance_loss: 3.4961, weighted_loss: 2.8850, label: 1, bag_size: 38\n",
      "batch 459, loss: 0.0594, instance_loss: 0.5775, weighted_loss: 0.2148, label: 0, bag_size: 29\n",
      "batch 479, loss: 0.1255, instance_loss: 1.0227, weighted_loss: 0.3947, label: 1, bag_size: 92\n",
      "batch 499, loss: 0.2696, instance_loss: 1.2666, weighted_loss: 0.5687, label: 0, bag_size: 79\n",
      "batch 519, loss: 0.1152, instance_loss: 1.0000, weighted_loss: 0.3806, label: 0, bag_size: 26\n",
      "batch 539, loss: 0.0003, instance_loss: 0.3812, weighted_loss: 0.1146, label: 1, bag_size: 30\n",
      "batch 559, loss: 0.3754, instance_loss: 0.5671, weighted_loss: 0.4329, label: 1, bag_size: 18\n",
      "batch 579, loss: 0.0590, instance_loss: 1.0797, weighted_loss: 0.3652, label: 1, bag_size: 72\n",
      "batch 599, loss: 1.8570, instance_loss: 2.7092, weighted_loss: 2.1127, label: 0, bag_size: 63\n",
      "batch 619, loss: 0.0459, instance_loss: 0.3584, weighted_loss: 0.1397, label: 1, bag_size: 45\n",
      "batch 639, loss: 0.2430, instance_loss: 0.4673, weighted_loss: 0.3103, label: 1, bag_size: 14\n",
      "batch 659, loss: 1.2735, instance_loss: 1.6839, weighted_loss: 1.3966, label: 0, bag_size: 27\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9319128787878788: correct 9841/10560\n",
      "class 1 clustering acc 0.5696969696969697: correct 3008/5280\n",
      "Epoch: 3, train_loss: 0.6250, train_clustering_loss:  0.6610, train_error: 0.2333\n",
      "class 0: acc 0.7583081570996979, correct 251/331\n",
      "class 1: acc 0.7750759878419453, correct 255/329\n",
      "\n",
      "Val Set, val_loss: 0.5221, val_error: 0.1622, auc: 0.8904\n",
      "class 0 clustering acc 0.9400337837837838: correct 1113/1184\n",
      "class 1 clustering acc 0.44932432432432434: correct 266/592\n",
      "class 0: acc 0.8235294117647058, correct 28/34\n",
      "class 1: acc 0.85, correct 34/40\n",
      "Validation loss decreased (0.534151 --> 0.522059).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 3.4284, instance_loss: 1.6558, weighted_loss: 2.8966, label: 0, bag_size: 43\n",
      "batch 39, loss: 0.0997, instance_loss: 0.7335, weighted_loss: 0.2899, label: 0, bag_size: 103\n",
      "batch 59, loss: 1.3148, instance_loss: 1.0463, weighted_loss: 1.2342, label: 0, bag_size: 30\n",
      "batch 79, loss: 0.3393, instance_loss: 0.9483, weighted_loss: 0.5220, label: 0, bag_size: 29\n",
      "batch 99, loss: 0.0077, instance_loss: 0.3675, weighted_loss: 0.1156, label: 0, bag_size: 66\n",
      "batch 119, loss: 0.0123, instance_loss: 0.2653, weighted_loss: 0.0882, label: 1, bag_size: 100\n",
      "batch 139, loss: 3.0963, instance_loss: 1.7527, weighted_loss: 2.6932, label: 0, bag_size: 90\n",
      "batch 159, loss: 1.7549, instance_loss: 0.4715, weighted_loss: 1.3699, label: 1, bag_size: 40\n",
      "batch 179, loss: 0.2609, instance_loss: 0.2289, weighted_loss: 0.2513, label: 0, bag_size: 40\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0501, weighted_loss: 0.0151, label: 0, bag_size: 25\n",
      "batch 219, loss: 0.6239, instance_loss: 2.2143, weighted_loss: 1.1010, label: 1, bag_size: 55\n",
      "batch 239, loss: 0.5465, instance_loss: 0.8360, weighted_loss: 0.6333, label: 1, bag_size: 87\n",
      "batch 259, loss: 4.2452, instance_loss: 1.4747, weighted_loss: 3.4140, label: 0, bag_size: 43\n",
      "batch 279, loss: 0.2596, instance_loss: 2.2752, weighted_loss: 0.8643, label: 1, bag_size: 59\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0730, weighted_loss: 0.0219, label: 0, bag_size: 107\n",
      "batch 319, loss: 0.0253, instance_loss: 0.1212, weighted_loss: 0.0541, label: 0, bag_size: 86\n",
      "batch 339, loss: 0.0105, instance_loss: 0.0921, weighted_loss: 0.0350, label: 1, bag_size: 68\n",
      "batch 359, loss: 0.0018, instance_loss: 0.0939, weighted_loss: 0.0294, label: 1, bag_size: 62\n",
      "batch 379, loss: 0.1029, instance_loss: 0.1689, weighted_loss: 0.1227, label: 1, bag_size: 16\n",
      "batch 399, loss: 0.3997, instance_loss: 0.4462, weighted_loss: 0.4136, label: 1, bag_size: 77\n",
      "batch 419, loss: 0.0410, instance_loss: 0.0892, weighted_loss: 0.0554, label: 0, bag_size: 67\n",
      "batch 439, loss: 0.6437, instance_loss: 1.9358, weighted_loss: 1.0313, label: 1, bag_size: 31\n",
      "batch 459, loss: 0.2914, instance_loss: 1.0123, weighted_loss: 0.5077, label: 1, bag_size: 67\n",
      "batch 479, loss: 0.0001, instance_loss: 0.6294, weighted_loss: 0.1888, label: 0, bag_size: 116\n",
      "batch 499, loss: 0.0064, instance_loss: 1.0093, weighted_loss: 0.3073, label: 0, bag_size: 54\n",
      "batch 519, loss: 0.9866, instance_loss: 1.4222, weighted_loss: 1.1173, label: 0, bag_size: 99\n",
      "batch 539, loss: 0.0202, instance_loss: 0.7706, weighted_loss: 0.2453, label: 1, bag_size: 49\n",
      "batch 559, loss: 0.0019, instance_loss: 0.6648, weighted_loss: 0.2007, label: 0, bag_size: 87\n",
      "batch 579, loss: 2.3349, instance_loss: 1.4460, weighted_loss: 2.0683, label: 0, bag_size: 79\n",
      "batch 599, loss: 0.5976, instance_loss: 0.9915, weighted_loss: 0.7158, label: 0, bag_size: 114\n",
      "batch 619, loss: 0.0005, instance_loss: 0.6584, weighted_loss: 0.1978, label: 0, bag_size: 67\n",
      "batch 639, loss: 0.2403, instance_loss: 0.9104, weighted_loss: 0.4413, label: 1, bag_size: 83\n",
      "batch 659, loss: 0.1897, instance_loss: 0.7330, weighted_loss: 0.3527, label: 1, bag_size: 73\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9196969696969697: correct 9712/10560\n",
      "class 1 clustering acc 0.4420454545454545: correct 2334/5280\n",
      "Epoch: 4, train_loss: 0.6477, train_clustering_loss:  0.7672, train_error: 0.2288\n",
      "class 0: acc 0.7555555555555555, correct 238/315\n",
      "class 1: acc 0.7855072463768116, correct 271/345\n",
      "\n",
      "Val Set, val_loss: 0.6961, val_error: 0.2838, auc: 0.8610\n",
      "class 0 clustering acc 0.9476351351351351: correct 1122/1184\n",
      "class 1 clustering acc 0.18074324324324326: correct 107/592\n",
      "class 0: acc 0.8823529411764706, correct 30/34\n",
      "class 1: acc 0.575, correct 23/40\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 5.1902, instance_loss: 0.9579, weighted_loss: 3.9205, label: 0, bag_size: 25\n",
      "batch 39, loss: 1.2478, instance_loss: 0.8047, weighted_loss: 1.1148, label: 1, bag_size: 84\n",
      "batch 59, loss: 0.0039, instance_loss: 0.6604, weighted_loss: 0.2009, label: 1, bag_size: 126\n",
      "batch 79, loss: 0.0396, instance_loss: 0.6107, weighted_loss: 0.2109, label: 0, bag_size: 64\n",
      "batch 99, loss: 0.0057, instance_loss: 0.4891, weighted_loss: 0.1507, label: 0, bag_size: 33\n",
      "batch 119, loss: 1.7137, instance_loss: 1.9047, weighted_loss: 1.7710, label: 0, bag_size: 71\n",
      "batch 139, loss: 0.0011, instance_loss: 0.5471, weighted_loss: 0.1649, label: 0, bag_size: 39\n",
      "batch 159, loss: 1.8635, instance_loss: 0.8830, weighted_loss: 1.5693, label: 0, bag_size: 48\n",
      "batch 179, loss: 1.0059, instance_loss: 1.2138, weighted_loss: 1.0682, label: 0, bag_size: 61\n",
      "batch 199, loss: 0.1433, instance_loss: 0.9797, weighted_loss: 0.3942, label: 0, bag_size: 49\n",
      "batch 219, loss: 0.5292, instance_loss: 0.8849, weighted_loss: 0.6359, label: 0, bag_size: 75\n",
      "batch 239, loss: 2.3393, instance_loss: 1.4427, weighted_loss: 2.0704, label: 0, bag_size: 73\n",
      "batch 259, loss: 0.0004, instance_loss: 0.4921, weighted_loss: 0.1479, label: 1, bag_size: 35\n",
      "batch 279, loss: 0.0405, instance_loss: 0.9892, weighted_loss: 0.3251, label: 1, bag_size: 76\n",
      "batch 299, loss: 3.5992, instance_loss: 1.8999, weighted_loss: 3.0894, label: 0, bag_size: 26\n",
      "batch 319, loss: 0.3220, instance_loss: 0.9839, weighted_loss: 0.5205, label: 1, bag_size: 17\n",
      "batch 339, loss: 0.0046, instance_loss: 0.6861, weighted_loss: 0.2090, label: 1, bag_size: 67\n",
      "batch 359, loss: 0.0086, instance_loss: 0.8320, weighted_loss: 0.2556, label: 0, bag_size: 84\n",
      "batch 379, loss: 2.5035, instance_loss: 1.5959, weighted_loss: 2.2312, label: 0, bag_size: 86\n",
      "batch 399, loss: 0.0022, instance_loss: 0.6289, weighted_loss: 0.1902, label: 1, bag_size: 97\n",
      "batch 419, loss: 2.2989, instance_loss: 0.8804, weighted_loss: 1.8734, label: 1, bag_size: 102\n",
      "batch 439, loss: 0.0002, instance_loss: 0.3455, weighted_loss: 0.1038, label: 1, bag_size: 39\n",
      "batch 459, loss: 0.0004, instance_loss: 0.8639, weighted_loss: 0.2594, label: 0, bag_size: 78\n",
      "batch 479, loss: 0.0316, instance_loss: 0.7388, weighted_loss: 0.2437, label: 1, bag_size: 84\n",
      "batch 499, loss: 0.5094, instance_loss: 0.7242, weighted_loss: 0.5739, label: 1, bag_size: 67\n",
      "batch 519, loss: 0.0000, instance_loss: 0.5402, weighted_loss: 0.1621, label: 0, bag_size: 75\n",
      "batch 539, loss: 0.0002, instance_loss: 0.3055, weighted_loss: 0.0918, label: 0, bag_size: 78\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0520, weighted_loss: 0.0156, label: 0, bag_size: 57\n",
      "batch 579, loss: 0.0477, instance_loss: 0.5773, weighted_loss: 0.2066, label: 1, bag_size: 35\n",
      "batch 599, loss: 0.0014, instance_loss: 0.1598, weighted_loss: 0.0489, label: 1, bag_size: 25\n",
      "batch 619, loss: 0.2338, instance_loss: 0.3177, weighted_loss: 0.2590, label: 0, bag_size: 44\n",
      "batch 639, loss: 5.7821, instance_loss: 0.9510, weighted_loss: 4.3327, label: 0, bag_size: 78\n",
      "batch 659, loss: 0.0008, instance_loss: 0.0557, weighted_loss: 0.0173, label: 1, bag_size: 89\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9513257575757575: correct 10046/10560\n",
      "class 1 clustering acc 0.2596590909090909: correct 1371/5280\n",
      "Epoch: 5, train_loss: 0.7407, train_clustering_loss:  0.8403, train_error: 0.2409\n",
      "class 0: acc 0.731629392971246, correct 229/313\n",
      "class 1: acc 0.7838616714697406, correct 272/347\n",
      "\n",
      "Val Set, val_loss: 0.8362, val_error: 0.2432, auc: 0.8713\n",
      "class 0 clustering acc 0.8648648648648649: correct 1024/1184\n",
      "class 1 clustering acc 0.6013513513513513: correct 356/592\n",
      "class 0: acc 0.5, correct 17/34\n",
      "class 1: acc 0.975, correct 39/40\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 2.0401, instance_loss: 2.2167, weighted_loss: 2.0931, label: 1, bag_size: 119\n",
      "batch 39, loss: 0.0004, instance_loss: 0.2366, weighted_loss: 0.0713, label: 0, bag_size: 81\n",
      "batch 59, loss: 0.0124, instance_loss: 0.3522, weighted_loss: 0.1143, label: 0, bag_size: 73\n",
      "batch 79, loss: 0.0073, instance_loss: 0.0825, weighted_loss: 0.0299, label: 1, bag_size: 80\n",
      "batch 99, loss: 0.1011, instance_loss: 0.9586, weighted_loss: 0.3583, label: 0, bag_size: 71\n",
      "batch 119, loss: 0.8846, instance_loss: 0.6029, weighted_loss: 0.8001, label: 0, bag_size: 28\n",
      "batch 139, loss: 0.0045, instance_loss: 0.1434, weighted_loss: 0.0462, label: 1, bag_size: 118\n",
      "batch 159, loss: 0.0311, instance_loss: 0.1527, weighted_loss: 0.0676, label: 0, bag_size: 90\n",
      "batch 179, loss: 0.0001, instance_loss: 0.8518, weighted_loss: 0.2556, label: 0, bag_size: 42\n",
      "batch 199, loss: 0.0008, instance_loss: 0.3754, weighted_loss: 0.1132, label: 0, bag_size: 74\n",
      "batch 219, loss: 0.0035, instance_loss: 0.1554, weighted_loss: 0.0491, label: 1, bag_size: 83\n",
      "batch 239, loss: 0.0687, instance_loss: 0.6439, weighted_loss: 0.2412, label: 1, bag_size: 97\n",
      "batch 259, loss: 1.7538, instance_loss: 0.7232, weighted_loss: 1.4446, label: 0, bag_size: 90\n",
      "batch 279, loss: 0.2611, instance_loss: 0.3840, weighted_loss: 0.2980, label: 1, bag_size: 93\n",
      "batch 299, loss: 0.0056, instance_loss: 0.1544, weighted_loss: 0.0502, label: 0, bag_size: 91\n",
      "batch 319, loss: 0.0088, instance_loss: 0.4421, weighted_loss: 0.1388, label: 0, bag_size: 49\n",
      "batch 339, loss: 0.0246, instance_loss: 0.6462, weighted_loss: 0.2111, label: 0, bag_size: 78\n",
      "batch 359, loss: 0.0004, instance_loss: 0.2624, weighted_loss: 0.0790, label: 0, bag_size: 46\n",
      "batch 379, loss: 0.0003, instance_loss: 1.4642, weighted_loss: 0.4395, label: 0, bag_size: 49\n",
      "batch 399, loss: 0.0001, instance_loss: 0.1723, weighted_loss: 0.0517, label: 0, bag_size: 37\n",
      "batch 419, loss: 0.0260, instance_loss: 0.2692, weighted_loss: 0.0989, label: 1, bag_size: 153\n",
      "batch 439, loss: 0.0151, instance_loss: 0.0575, weighted_loss: 0.0278, label: 1, bag_size: 67\n",
      "batch 459, loss: 0.7330, instance_loss: 0.8521, weighted_loss: 0.7687, label: 1, bag_size: 86\n",
      "batch 479, loss: 0.0712, instance_loss: 0.8992, weighted_loss: 0.3196, label: 0, bag_size: 79\n",
      "batch 499, loss: 1.3643, instance_loss: 0.8477, weighted_loss: 1.2094, label: 1, bag_size: 108\n",
      "batch 519, loss: 0.1134, instance_loss: 0.1693, weighted_loss: 0.1302, label: 0, bag_size: 12\n",
      "batch 539, loss: 0.0061, instance_loss: 0.6195, weighted_loss: 0.1901, label: 1, bag_size: 31\n",
      "batch 559, loss: 5.7545, instance_loss: 0.5858, weighted_loss: 4.2039, label: 0, bag_size: 132\n",
      "batch 579, loss: 0.0000, instance_loss: 0.2615, weighted_loss: 0.0785, label: 0, bag_size: 91\n",
      "batch 599, loss: 0.2171, instance_loss: 0.6248, weighted_loss: 0.3394, label: 0, bag_size: 52\n",
      "batch 619, loss: 0.0303, instance_loss: 0.3403, weighted_loss: 0.1233, label: 1, bag_size: 32\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0261, weighted_loss: 0.0079, label: 0, bag_size: 42\n",
      "batch 659, loss: 2.5642, instance_loss: 1.4712, weighted_loss: 2.2363, label: 0, bag_size: 36\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9267045454545455: correct 9786/10560\n",
      "class 1 clustering acc 0.5401515151515152: correct 2852/5280\n",
      "Epoch: 6, train_loss: 0.6667, train_clustering_loss:  0.6754, train_error: 0.2303\n",
      "class 0: acc 0.7666666666666667, correct 253/330\n",
      "class 1: acc 0.7727272727272727, correct 255/330\n",
      "\n",
      "Val Set, val_loss: 0.5267, val_error: 0.2027, auc: 0.8934\n",
      "class 0 clustering acc 0.9341216216216216: correct 1106/1184\n",
      "class 1 clustering acc 0.6841216216216216: correct 405/592\n",
      "class 0: acc 0.6764705882352942, correct 23/34\n",
      "class 1: acc 0.9, correct 36/40\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0477, weighted_loss: 0.0143, label: 1, bag_size: 25\n",
      "batch 39, loss: 0.0002, instance_loss: 0.0714, weighted_loss: 0.0215, label: 0, bag_size: 103\n",
      "batch 59, loss: 0.0124, instance_loss: 0.1927, weighted_loss: 0.0665, label: 1, bag_size: 95\n",
      "batch 79, loss: 0.0622, instance_loss: 1.2536, weighted_loss: 0.4196, label: 1, bag_size: 59\n",
      "batch 99, loss: 0.0359, instance_loss: 0.0801, weighted_loss: 0.0492, label: 0, bag_size: 28\n",
      "batch 119, loss: 7.3852, instance_loss: 2.8892, weighted_loss: 6.0364, label: 0, bag_size: 78\n",
      "batch 139, loss: 0.0089, instance_loss: 0.1038, weighted_loss: 0.0374, label: 1, bag_size: 112\n",
      "batch 159, loss: 0.0222, instance_loss: 0.1206, weighted_loss: 0.0517, label: 0, bag_size: 30\n",
      "batch 179, loss: 0.0028, instance_loss: 0.3519, weighted_loss: 0.1075, label: 1, bag_size: 99\n",
      "batch 199, loss: 0.0087, instance_loss: 0.0295, weighted_loss: 0.0150, label: 0, bag_size: 42\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0144, weighted_loss: 0.0045, label: 0, bag_size: 65\n",
      "batch 239, loss: 0.0040, instance_loss: 0.0326, weighted_loss: 0.0126, label: 1, bag_size: 120\n",
      "batch 259, loss: 0.0079, instance_loss: 0.2842, weighted_loss: 0.0908, label: 1, bag_size: 94\n",
      "batch 279, loss: 0.3482, instance_loss: 0.3056, weighted_loss: 0.3354, label: 1, bag_size: 108\n",
      "batch 299, loss: 1.3495, instance_loss: 0.9633, weighted_loss: 1.2336, label: 0, bag_size: 31\n",
      "batch 319, loss: 4.3318, instance_loss: 2.4133, weighted_loss: 3.7562, label: 0, bag_size: 72\n",
      "batch 339, loss: 0.7003, instance_loss: 0.3856, weighted_loss: 0.6059, label: 0, bag_size: 28\n",
      "batch 359, loss: 0.4075, instance_loss: 0.3892, weighted_loss: 0.4020, label: 0, bag_size: 77\n",
      "batch 379, loss: 0.5977, instance_loss: 0.8676, weighted_loss: 0.6787, label: 0, bag_size: 31\n",
      "batch 399, loss: 0.9180, instance_loss: 0.2970, weighted_loss: 0.7317, label: 1, bag_size: 100\n",
      "batch 419, loss: 0.1379, instance_loss: 1.4334, weighted_loss: 0.5265, label: 1, bag_size: 42\n",
      "batch 439, loss: 0.0187, instance_loss: 0.1021, weighted_loss: 0.0437, label: 0, bag_size: 46\n",
      "batch 459, loss: 0.0003, instance_loss: 0.0277, weighted_loss: 0.0085, label: 0, bag_size: 75\n",
      "batch 479, loss: 0.0005, instance_loss: 0.0182, weighted_loss: 0.0058, label: 1, bag_size: 83\n",
      "batch 499, loss: 1.2462, instance_loss: 0.8801, weighted_loss: 1.1364, label: 0, bag_size: 88\n",
      "batch 519, loss: 1.3939, instance_loss: 0.3238, weighted_loss: 1.0729, label: 1, bag_size: 84\n",
      "batch 539, loss: 0.3386, instance_loss: 0.1161, weighted_loss: 0.2719, label: 0, bag_size: 31\n",
      "batch 559, loss: 0.1085, instance_loss: 0.6578, weighted_loss: 0.2733, label: 1, bag_size: 62\n",
      "batch 579, loss: 0.0405, instance_loss: 0.0363, weighted_loss: 0.0393, label: 1, bag_size: 84\n",
      "batch 599, loss: 0.0158, instance_loss: 0.0653, weighted_loss: 0.0306, label: 0, bag_size: 83\n",
      "batch 619, loss: 0.5662, instance_loss: 0.4722, weighted_loss: 0.5380, label: 1, bag_size: 40\n",
      "batch 639, loss: 0.3725, instance_loss: 0.3827, weighted_loss: 0.3755, label: 0, bag_size: 79\n",
      "batch 659, loss: 0.0034, instance_loss: 0.0229, weighted_loss: 0.0093, label: 0, bag_size: 36\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9421401515151515: correct 9949/10560\n",
      "class 1 clustering acc 0.7142045454545455: correct 3771/5280\n",
      "Epoch: 7, train_loss: 0.4901, train_clustering_loss:  0.5176, train_error: 0.2000\n",
      "class 0: acc 0.7808641975308642, correct 253/324\n",
      "class 1: acc 0.8184523809523809, correct 275/336\n",
      "\n",
      "Val Set, val_loss: 0.6273, val_error: 0.2432, auc: 0.8838\n",
      "class 0 clustering acc 0.9206081081081081: correct 1090/1184\n",
      "class 1 clustering acc 0.7331081081081081: correct 434/592\n",
      "class 0: acc 0.5294117647058824, correct 18/34\n",
      "class 1: acc 0.95, correct 38/40\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0569, instance_loss: 0.2677, weighted_loss: 0.1201, label: 0, bag_size: 103\n",
      "batch 39, loss: 0.5795, instance_loss: 1.1832, weighted_loss: 0.7606, label: 0, bag_size: 32\n",
      "batch 59, loss: 3.9868, instance_loss: 1.6951, weighted_loss: 3.2993, label: 1, bag_size: 88\n",
      "batch 79, loss: 0.0180, instance_loss: 0.0190, weighted_loss: 0.0183, label: 1, bag_size: 82\n",
      "batch 99, loss: 0.0029, instance_loss: 0.0189, weighted_loss: 0.0077, label: 1, bag_size: 98\n",
      "batch 119, loss: 0.3506, instance_loss: 0.1956, weighted_loss: 0.3041, label: 1, bag_size: 65\n",
      "batch 139, loss: 1.3261, instance_loss: 2.4906, weighted_loss: 1.6755, label: 1, bag_size: 59\n",
      "batch 159, loss: 0.9784, instance_loss: 0.8030, weighted_loss: 0.9258, label: 1, bag_size: 51\n",
      "batch 179, loss: 0.0006, instance_loss: 0.2717, weighted_loss: 0.0819, label: 1, bag_size: 56\n",
      "batch 199, loss: 2.0560, instance_loss: 0.5176, weighted_loss: 1.5945, label: 0, bag_size: 35\n",
      "batch 219, loss: 0.3075, instance_loss: 0.5006, weighted_loss: 0.3654, label: 0, bag_size: 82\n",
      "batch 239, loss: 0.0036, instance_loss: 0.0245, weighted_loss: 0.0098, label: 0, bag_size: 106\n",
      "batch 259, loss: 2.6000, instance_loss: 2.3352, weighted_loss: 2.5205, label: 0, bag_size: 88\n",
      "batch 279, loss: 0.0000, instance_loss: 0.1313, weighted_loss: 0.0394, label: 0, bag_size: 88\n",
      "batch 299, loss: 0.0018, instance_loss: 0.0896, weighted_loss: 0.0281, label: 0, bag_size: 25\n",
      "batch 319, loss: 1.3440, instance_loss: 0.0625, weighted_loss: 0.9595, label: 1, bag_size: 61\n",
      "batch 339, loss: 0.0238, instance_loss: 0.3316, weighted_loss: 0.1161, label: 1, bag_size: 32\n",
      "batch 359, loss: 0.0402, instance_loss: 1.3067, weighted_loss: 0.4201, label: 1, bag_size: 36\n",
      "batch 379, loss: 0.0010, instance_loss: 0.6370, weighted_loss: 0.1918, label: 0, bag_size: 65\n",
      "batch 399, loss: 0.0715, instance_loss: 0.7683, weighted_loss: 0.2805, label: 1, bag_size: 62\n",
      "batch 419, loss: 0.1350, instance_loss: 0.7194, weighted_loss: 0.3103, label: 0, bag_size: 50\n",
      "batch 439, loss: 1.5007, instance_loss: 1.0578, weighted_loss: 1.3678, label: 0, bag_size: 93\n",
      "batch 459, loss: 0.0016, instance_loss: 0.6608, weighted_loss: 0.1993, label: 1, bag_size: 70\n",
      "batch 479, loss: 0.0194, instance_loss: 0.6488, weighted_loss: 0.2082, label: 0, bag_size: 42\n",
      "batch 499, loss: 0.6159, instance_loss: 0.8815, weighted_loss: 0.6956, label: 0, bag_size: 90\n",
      "batch 519, loss: 0.0051, instance_loss: 0.6153, weighted_loss: 0.1882, label: 0, bag_size: 65\n",
      "batch 539, loss: 0.2196, instance_loss: 0.7218, weighted_loss: 0.3702, label: 0, bag_size: 95\n",
      "batch 559, loss: 0.0407, instance_loss: 0.5965, weighted_loss: 0.2074, label: 0, bag_size: 94\n",
      "batch 579, loss: 0.0030, instance_loss: 0.5188, weighted_loss: 0.1577, label: 1, bag_size: 112\n",
      "batch 599, loss: 0.0184, instance_loss: 0.8338, weighted_loss: 0.2630, label: 1, bag_size: 83\n",
      "batch 619, loss: 0.4819, instance_loss: 0.9517, weighted_loss: 0.6228, label: 0, bag_size: 98\n",
      "batch 639, loss: 0.1445, instance_loss: 1.7497, weighted_loss: 0.6261, label: 1, bag_size: 89\n",
      "batch 659, loss: 0.0003, instance_loss: 0.6633, weighted_loss: 0.1992, label: 0, bag_size: 118\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.915435606060606: correct 9667/10560\n",
      "class 1 clustering acc 0.4960227272727273: correct 2619/5280\n",
      "Epoch: 8, train_loss: 0.5397, train_clustering_loss:  0.6899, train_error: 0.2091\n",
      "class 0: acc 0.8017751479289941, correct 271/338\n",
      "class 1: acc 0.7795031055900621, correct 251/322\n",
      "\n",
      "Val Set, val_loss: 0.8130, val_error: 0.2162, auc: 0.8713\n",
      "class 0 clustering acc 0.9298986486486487: correct 1101/1184\n",
      "class 1 clustering acc 0.21114864864864866: correct 125/592\n",
      "class 0: acc 0.7352941176470589, correct 25/34\n",
      "class 1: acc 0.825, correct 33/40\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 2.5155, instance_loss: 0.7773, weighted_loss: 1.9941, label: 0, bag_size: 89\n",
      "batch 39, loss: 0.7276, instance_loss: 0.9196, weighted_loss: 0.7852, label: 1, bag_size: 43\n",
      "batch 59, loss: 0.0327, instance_loss: 0.7044, weighted_loss: 0.2342, label: 1, bag_size: 30\n",
      "batch 79, loss: 0.0045, instance_loss: 0.4812, weighted_loss: 0.1475, label: 1, bag_size: 23\n",
      "batch 99, loss: 3.4634, instance_loss: 2.1552, weighted_loss: 3.0709, label: 1, bag_size: 22\n",
      "batch 119, loss: 0.0017, instance_loss: 0.4294, weighted_loss: 0.1300, label: 0, bag_size: 93\n",
      "batch 139, loss: 1.0009, instance_loss: 0.6890, weighted_loss: 0.9073, label: 1, bag_size: 46\n",
      "batch 159, loss: 0.0072, instance_loss: 0.4962, weighted_loss: 0.1539, label: 1, bag_size: 76\n",
      "batch 179, loss: 0.0029, instance_loss: 0.4691, weighted_loss: 0.1427, label: 0, bag_size: 107\n",
      "batch 199, loss: 0.0122, instance_loss: 0.5184, weighted_loss: 0.1640, label: 1, bag_size: 78\n",
      "batch 219, loss: 0.0003, instance_loss: 0.4552, weighted_loss: 0.1368, label: 0, bag_size: 88\n",
      "batch 239, loss: 0.4264, instance_loss: 1.0995, weighted_loss: 0.6283, label: 0, bag_size: 41\n",
      "batch 259, loss: 0.0202, instance_loss: 0.5507, weighted_loss: 0.1793, label: 1, bag_size: 88\n",
      "batch 279, loss: 0.1044, instance_loss: 0.5661, weighted_loss: 0.2429, label: 1, bag_size: 20\n",
      "batch 299, loss: 0.0019, instance_loss: 0.7357, weighted_loss: 0.2221, label: 1, bag_size: 79\n",
      "batch 319, loss: 0.0696, instance_loss: 0.5613, weighted_loss: 0.2171, label: 1, bag_size: 53\n",
      "batch 339, loss: 0.0190, instance_loss: 1.2455, weighted_loss: 0.3870, label: 0, bag_size: 79\n",
      "batch 359, loss: 1.3861, instance_loss: 0.8027, weighted_loss: 1.2111, label: 1, bag_size: 46\n",
      "batch 379, loss: 0.2944, instance_loss: 0.6510, weighted_loss: 0.4014, label: 0, bag_size: 50\n",
      "batch 399, loss: 0.0006, instance_loss: 0.1069, weighted_loss: 0.0325, label: 0, bag_size: 36\n",
      "batch 419, loss: 0.0006, instance_loss: 0.4402, weighted_loss: 0.1325, label: 1, bag_size: 85\n",
      "batch 439, loss: 0.0027, instance_loss: 0.1796, weighted_loss: 0.0557, label: 0, bag_size: 66\n",
      "batch 459, loss: 0.0068, instance_loss: 0.3813, weighted_loss: 0.1191, label: 1, bag_size: 51\n",
      "batch 479, loss: 0.0016, instance_loss: 0.3752, weighted_loss: 0.1137, label: 1, bag_size: 20\n",
      "batch 499, loss: 0.9143, instance_loss: 1.1753, weighted_loss: 0.9926, label: 0, bag_size: 41\n",
      "batch 519, loss: 0.0524, instance_loss: 0.4355, weighted_loss: 0.1674, label: 1, bag_size: 77\n",
      "batch 539, loss: 0.1150, instance_loss: 0.0584, weighted_loss: 0.0980, label: 0, bag_size: 30\n",
      "batch 559, loss: 0.0006, instance_loss: 0.2057, weighted_loss: 0.0621, label: 0, bag_size: 103\n",
      "batch 579, loss: 0.7566, instance_loss: 0.3975, weighted_loss: 0.6489, label: 1, bag_size: 46\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0492, weighted_loss: 0.0148, label: 0, bag_size: 101\n",
      "batch 619, loss: 0.0004, instance_loss: 0.0474, weighted_loss: 0.0145, label: 0, bag_size: 93\n",
      "batch 639, loss: 0.2210, instance_loss: 0.2024, weighted_loss: 0.2154, label: 0, bag_size: 107\n",
      "batch 659, loss: 1.4580, instance_loss: 0.6033, weighted_loss: 1.2016, label: 0, bag_size: 101\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9223484848484849: correct 9740/10560\n",
      "class 1 clustering acc 0.5234848484848484: correct 2764/5280\n",
      "Epoch: 9, train_loss: 0.4341, train_clustering_loss:  0.6607, train_error: 0.1591\n",
      "class 0: acc 0.8413173652694611, correct 281/334\n",
      "class 1: acc 0.8404907975460123, correct 274/326\n",
      "\n",
      "Val Set, val_loss: 0.7271, val_error: 0.2162, auc: 0.8882\n",
      "class 0 clustering acc 0.7778716216216216: correct 921/1184\n",
      "class 1 clustering acc 0.5557432432432432: correct 329/592\n",
      "class 0: acc 0.5588235294117647, correct 19/34\n",
      "class 1: acc 0.975, correct 39/40\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0802, instance_loss: 0.2239, weighted_loss: 0.1233, label: 1, bag_size: 34\n",
      "batch 39, loss: 0.2988, instance_loss: 0.6818, weighted_loss: 0.4137, label: 1, bag_size: 62\n",
      "batch 59, loss: 0.0995, instance_loss: 0.1702, weighted_loss: 0.1207, label: 0, bag_size: 27\n",
      "batch 79, loss: 1.7679, instance_loss: 2.9440, weighted_loss: 2.1208, label: 0, bag_size: 54\n",
      "batch 99, loss: 0.0062, instance_loss: 0.3857, weighted_loss: 0.1200, label: 1, bag_size: 69\n",
      "batch 119, loss: 0.7406, instance_loss: 0.7818, weighted_loss: 0.7530, label: 1, bag_size: 96\n",
      "batch 139, loss: 1.4034, instance_loss: 0.5358, weighted_loss: 1.1431, label: 1, bag_size: 44\n",
      "batch 159, loss: 0.0004, instance_loss: 0.2638, weighted_loss: 0.0794, label: 0, bag_size: 79\n",
      "batch 179, loss: 0.0018, instance_loss: 0.1702, weighted_loss: 0.0523, label: 0, bag_size: 44\n",
      "batch 199, loss: 0.0018, instance_loss: 0.0342, weighted_loss: 0.0116, label: 0, bag_size: 81\n",
      "batch 219, loss: 0.0064, instance_loss: 0.0160, weighted_loss: 0.0093, label: 0, bag_size: 87\n",
      "batch 239, loss: 0.0209, instance_loss: 0.4253, weighted_loss: 0.1422, label: 1, bag_size: 84\n",
      "batch 259, loss: 0.0004, instance_loss: 0.4005, weighted_loss: 0.1204, label: 1, bag_size: 56\n",
      "batch 279, loss: 4.8655, instance_loss: 1.8891, weighted_loss: 3.9726, label: 0, bag_size: 78\n",
      "batch 299, loss: 0.0073, instance_loss: 0.3481, weighted_loss: 0.1095, label: 1, bag_size: 92\n",
      "batch 319, loss: 0.0023, instance_loss: 0.0835, weighted_loss: 0.0267, label: 0, bag_size: 31\n",
      "batch 339, loss: 0.6830, instance_loss: 1.0668, weighted_loss: 0.7982, label: 1, bag_size: 42\n",
      "batch 359, loss: 0.0057, instance_loss: 0.0421, weighted_loss: 0.0166, label: 0, bag_size: 91\n",
      "batch 379, loss: 0.0001, instance_loss: 0.1253, weighted_loss: 0.0376, label: 0, bag_size: 64\n",
      "batch 399, loss: 0.7861, instance_loss: 1.9681, weighted_loss: 1.1407, label: 1, bag_size: 68\n",
      "batch 419, loss: 0.0086, instance_loss: 0.0138, weighted_loss: 0.0102, label: 0, bag_size: 106\n",
      "batch 439, loss: 0.0007, instance_loss: 0.0666, weighted_loss: 0.0205, label: 0, bag_size: 96\n",
      "batch 459, loss: 0.1849, instance_loss: 0.2276, weighted_loss: 0.1977, label: 0, bag_size: 36\n",
      "batch 479, loss: 0.0109, instance_loss: 1.2601, weighted_loss: 0.3857, label: 1, bag_size: 53\n",
      "batch 499, loss: 0.0017, instance_loss: 0.0781, weighted_loss: 0.0246, label: 0, bag_size: 38\n",
      "batch 519, loss: 0.0237, instance_loss: 0.4008, weighted_loss: 0.1369, label: 1, bag_size: 77\n",
      "batch 539, loss: 0.1387, instance_loss: 0.6928, weighted_loss: 0.3049, label: 1, bag_size: 62\n",
      "batch 559, loss: 0.1908, instance_loss: 2.0893, weighted_loss: 0.7604, label: 0, bag_size: 98\n",
      "batch 579, loss: 0.0186, instance_loss: 0.2760, weighted_loss: 0.0959, label: 1, bag_size: 40\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0307, weighted_loss: 0.0093, label: 0, bag_size: 101\n",
      "batch 619, loss: 0.8027, instance_loss: 0.8699, weighted_loss: 0.8228, label: 1, bag_size: 34\n",
      "batch 639, loss: 0.8170, instance_loss: 0.8509, weighted_loss: 0.8272, label: 1, bag_size: 24\n",
      "batch 659, loss: 0.1422, instance_loss: 0.0535, weighted_loss: 0.1156, label: 0, bag_size: 67\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9444128787878788: correct 9973/10560\n",
      "class 1 clustering acc 0.6049242424242425: correct 3194/5280\n",
      "Epoch: 10, train_loss: 0.4518, train_clustering_loss:  0.6179, train_error: 0.1712\n",
      "class 0: acc 0.8206686930091185, correct 270/329\n",
      "class 1: acc 0.8368580060422961, correct 277/331\n",
      "\n",
      "Val Set, val_loss: 0.3288, val_error: 0.1622, auc: 0.9287\n",
      "class 0 clustering acc 0.7778716216216216: correct 921/1184\n",
      "class 1 clustering acc 0.4391891891891892: correct 260/592\n",
      "class 0: acc 0.7941176470588235, correct 27/34\n",
      "class 1: acc 0.875, correct 35/40\n",
      "Validation loss decreased (0.522059 --> 0.328846).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0015, instance_loss: 0.2818, weighted_loss: 0.0856, label: 1, bag_size: 29\n",
      "batch 39, loss: 0.0010, instance_loss: 0.0803, weighted_loss: 0.0248, label: 0, bag_size: 48\n",
      "batch 59, loss: 0.1883, instance_loss: 0.0722, weighted_loss: 0.1535, label: 0, bag_size: 58\n",
      "batch 79, loss: 4.2592, instance_loss: 3.2044, weighted_loss: 3.9428, label: 0, bag_size: 32\n",
      "batch 99, loss: 0.4083, instance_loss: 0.8573, weighted_loss: 0.5430, label: 1, bag_size: 35\n",
      "batch 119, loss: 0.0014, instance_loss: 0.2194, weighted_loss: 0.0668, label: 1, bag_size: 25\n",
      "batch 139, loss: 0.0845, instance_loss: 0.3411, weighted_loss: 0.1615, label: 0, bag_size: 88\n",
      "batch 159, loss: 0.2560, instance_loss: 1.4641, weighted_loss: 0.6184, label: 0, bag_size: 72\n",
      "batch 179, loss: 0.0006, instance_loss: 0.4468, weighted_loss: 0.1345, label: 1, bag_size: 85\n",
      "batch 199, loss: 0.0584, instance_loss: 0.7610, weighted_loss: 0.2692, label: 0, bag_size: 77\n",
      "batch 219, loss: 0.1063, instance_loss: 0.3293, weighted_loss: 0.1732, label: 0, bag_size: 71\n",
      "batch 239, loss: 0.7632, instance_loss: 0.9487, weighted_loss: 0.8189, label: 0, bag_size: 78\n",
      "batch 259, loss: 4.3092, instance_loss: 2.2641, weighted_loss: 3.6956, label: 1, bag_size: 116\n",
      "batch 279, loss: 0.0002, instance_loss: 0.8434, weighted_loss: 0.2532, label: 0, bag_size: 65\n",
      "batch 299, loss: 0.3006, instance_loss: 0.9675, weighted_loss: 0.5006, label: 1, bag_size: 35\n",
      "batch 319, loss: 0.9992, instance_loss: 1.5672, weighted_loss: 1.1696, label: 0, bag_size: 45\n",
      "batch 339, loss: 0.8514, instance_loss: 0.6668, weighted_loss: 0.7960, label: 0, bag_size: 58\n",
      "batch 359, loss: 0.1204, instance_loss: 0.2481, weighted_loss: 0.1587, label: 1, bag_size: 67\n",
      "batch 379, loss: 1.9651, instance_loss: 0.2482, weighted_loss: 1.4500, label: 0, bag_size: 34\n",
      "batch 399, loss: 0.0000, instance_loss: 0.5009, weighted_loss: 0.1503, label: 0, bag_size: 76\n",
      "batch 419, loss: 0.9411, instance_loss: 0.3509, weighted_loss: 0.7640, label: 1, bag_size: 25\n",
      "batch 439, loss: 0.0077, instance_loss: 0.1306, weighted_loss: 0.0446, label: 0, bag_size: 65\n",
      "batch 459, loss: 0.0004, instance_loss: 0.3107, weighted_loss: 0.0935, label: 1, bag_size: 72\n",
      "batch 479, loss: 0.0069, instance_loss: 0.2874, weighted_loss: 0.0911, label: 0, bag_size: 49\n",
      "batch 499, loss: 2.0242, instance_loss: 0.9173, weighted_loss: 1.6921, label: 1, bag_size: 64\n",
      "batch 519, loss: 1.7488, instance_loss: 1.5432, weighted_loss: 1.6872, label: 0, bag_size: 50\n",
      "batch 539, loss: 0.2461, instance_loss: 0.2460, weighted_loss: 0.2461, label: 1, bag_size: 63\n",
      "batch 559, loss: 0.0258, instance_loss: 0.4207, weighted_loss: 0.1443, label: 1, bag_size: 42\n",
      "batch 579, loss: 0.0003, instance_loss: 0.0834, weighted_loss: 0.0252, label: 1, bag_size: 31\n",
      "batch 599, loss: 0.1042, instance_loss: 0.0599, weighted_loss: 0.0909, label: 0, bag_size: 83\n",
      "batch 619, loss: 0.2645, instance_loss: 2.5381, weighted_loss: 0.9466, label: 0, bag_size: 41\n",
      "batch 639, loss: 0.1975, instance_loss: 0.3796, weighted_loss: 0.2522, label: 1, bag_size: 59\n",
      "batch 659, loss: 0.0014, instance_loss: 0.2809, weighted_loss: 0.0853, label: 1, bag_size: 100\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9403409090909091: correct 9930/10560\n",
      "class 1 clustering acc 0.5789772727272727: correct 3057/5280\n",
      "Epoch: 11, train_loss: 0.4907, train_clustering_loss:  0.6471, train_error: 0.1773\n",
      "class 0: acc 0.8208955223880597, correct 275/335\n",
      "class 1: acc 0.8246153846153846, correct 268/325\n",
      "\n",
      "Val Set, val_loss: 0.5086, val_error: 0.1892, auc: 0.9088\n",
      "class 0 clustering acc 0.7432432432432432: correct 880/1184\n",
      "class 1 clustering acc 0.49155405405405406: correct 291/592\n",
      "class 0: acc 0.6764705882352942, correct 23/34\n",
      "class 1: acc 0.925, correct 37/40\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0621, instance_loss: 0.4955, weighted_loss: 0.1921, label: 0, bag_size: 79\n",
      "batch 39, loss: 0.0005, instance_loss: 1.9573, weighted_loss: 0.5875, label: 0, bag_size: 103\n",
      "batch 59, loss: 0.4157, instance_loss: 0.1628, weighted_loss: 0.3398, label: 1, bag_size: 80\n",
      "batch 79, loss: 0.1405, instance_loss: 0.3451, weighted_loss: 0.2019, label: 0, bag_size: 46\n",
      "batch 99, loss: 0.0371, instance_loss: 0.0393, weighted_loss: 0.0377, label: 1, bag_size: 33\n",
      "batch 119, loss: 0.4224, instance_loss: 0.0623, weighted_loss: 0.3143, label: 0, bag_size: 88\n",
      "batch 139, loss: 12.0563, instance_loss: 0.9964, weighted_loss: 8.7383, label: 0, bag_size: 96\n",
      "batch 159, loss: 2.3631, instance_loss: 0.1667, weighted_loss: 1.7042, label: 1, bag_size: 70\n",
      "batch 179, loss: 1.1653, instance_loss: 0.3313, weighted_loss: 0.9151, label: 1, bag_size: 21\n",
      "batch 199, loss: 0.0223, instance_loss: 0.7874, weighted_loss: 0.2518, label: 0, bag_size: 66\n",
      "batch 219, loss: 0.3635, instance_loss: 1.3922, weighted_loss: 0.6721, label: 1, bag_size: 52\n",
      "batch 239, loss: 0.0060, instance_loss: 1.1386, weighted_loss: 0.3458, label: 1, bag_size: 19\n",
      "batch 259, loss: 1.6458, instance_loss: 1.5374, weighted_loss: 1.6132, label: 1, bag_size: 17\n",
      "batch 279, loss: 0.0309, instance_loss: 0.6410, weighted_loss: 0.2139, label: 0, bag_size: 114\n",
      "batch 299, loss: 0.2778, instance_loss: 1.7538, weighted_loss: 0.7206, label: 0, bag_size: 48\n",
      "batch 319, loss: 0.4100, instance_loss: 0.6552, weighted_loss: 0.4836, label: 1, bag_size: 18\n",
      "batch 339, loss: 1.1954, instance_loss: 1.9272, weighted_loss: 1.4149, label: 0, bag_size: 32\n",
      "batch 359, loss: 0.7280, instance_loss: 0.1421, weighted_loss: 0.5522, label: 0, bag_size: 40\n",
      "batch 379, loss: 0.3722, instance_loss: 0.7119, weighted_loss: 0.4741, label: 0, bag_size: 59\n",
      "batch 399, loss: 3.3960, instance_loss: 1.3275, weighted_loss: 2.7755, label: 0, bag_size: 36\n",
      "batch 419, loss: 0.8548, instance_loss: 1.0715, weighted_loss: 0.9198, label: 1, bag_size: 65\n",
      "batch 439, loss: 0.2278, instance_loss: 0.7634, weighted_loss: 0.3885, label: 1, bag_size: 17\n",
      "batch 459, loss: 0.4423, instance_loss: 1.5974, weighted_loss: 0.7888, label: 1, bag_size: 20\n",
      "batch 479, loss: 0.0338, instance_loss: 0.2080, weighted_loss: 0.0861, label: 0, bag_size: 50\n",
      "batch 499, loss: 0.0001, instance_loss: 0.1166, weighted_loss: 0.0350, label: 0, bag_size: 118\n",
      "batch 519, loss: 0.0335, instance_loss: 0.5896, weighted_loss: 0.2003, label: 1, bag_size: 76\n",
      "batch 539, loss: 0.1592, instance_loss: 0.8932, weighted_loss: 0.3794, label: 0, bag_size: 48\n",
      "batch 559, loss: 0.0169, instance_loss: 0.1381, weighted_loss: 0.0532, label: 0, bag_size: 83\n",
      "batch 579, loss: 0.2778, instance_loss: 0.4344, weighted_loss: 0.3248, label: 0, bag_size: 23\n",
      "batch 599, loss: 2.2895, instance_loss: 1.7075, weighted_loss: 2.1149, label: 0, bag_size: 57\n",
      "batch 619, loss: 2.4818, instance_loss: 0.7396, weighted_loss: 1.9592, label: 0, bag_size: 66\n",
      "batch 639, loss: 0.0010, instance_loss: 0.0156, weighted_loss: 0.0053, label: 0, bag_size: 77\n",
      "batch 659, loss: 0.0040, instance_loss: 0.0414, weighted_loss: 0.0152, label: 0, bag_size: 107\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9379734848484849: correct 9905/10560\n",
      "class 1 clustering acc 0.4712121212121212: correct 2488/5280\n",
      "Epoch: 12, train_loss: 0.7704, train_clustering_loss:  0.7226, train_error: 0.2652\n",
      "class 0: acc 0.7478753541076487, correct 264/353\n",
      "class 1: acc 0.7198697068403909, correct 221/307\n",
      "\n",
      "Val Set, val_loss: 1.0728, val_error: 0.2703, auc: 0.8838\n",
      "class 0 clustering acc 0.6351351351351351: correct 752/1184\n",
      "class 1 clustering acc 0.5675675675675675: correct 336/592\n",
      "class 0: acc 0.4411764705882353, correct 15/34\n",
      "class 1: acc 0.975, correct 39/40\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0171, instance_loss: 0.0595, weighted_loss: 0.0298, label: 1, bag_size: 94\n",
      "batch 39, loss: 0.0114, instance_loss: 0.0115, weighted_loss: 0.0115, label: 0, bag_size: 49\n",
      "batch 59, loss: 0.0020, instance_loss: 0.0202, weighted_loss: 0.0074, label: 1, bag_size: 76\n",
      "batch 79, loss: 0.4870, instance_loss: 0.4514, weighted_loss: 0.4763, label: 0, bag_size: 63\n",
      "batch 99, loss: 0.0017, instance_loss: 0.0370, weighted_loss: 0.0123, label: 1, bag_size: 72\n",
      "batch 119, loss: 0.0035, instance_loss: 0.0509, weighted_loss: 0.0177, label: 1, bag_size: 73\n",
      "batch 139, loss: 0.7469, instance_loss: 0.7890, weighted_loss: 0.7595, label: 1, bag_size: 57\n",
      "batch 159, loss: 0.1957, instance_loss: 0.2360, weighted_loss: 0.2078, label: 1, bag_size: 48\n",
      "batch 179, loss: 0.0062, instance_loss: 0.2454, weighted_loss: 0.0780, label: 0, bag_size: 85\n",
      "batch 199, loss: 0.0038, instance_loss: 0.0691, weighted_loss: 0.0234, label: 1, bag_size: 83\n",
      "batch 219, loss: 0.0030, instance_loss: 0.0412, weighted_loss: 0.0145, label: 1, bag_size: 67\n",
      "batch 239, loss: 0.1772, instance_loss: 0.1020, weighted_loss: 0.1547, label: 1, bag_size: 47\n",
      "batch 259, loss: 0.0024, instance_loss: 0.0261, weighted_loss: 0.0095, label: 1, bag_size: 69\n",
      "batch 279, loss: 1.3469, instance_loss: 1.5688, weighted_loss: 1.4135, label: 1, bag_size: 53\n",
      "batch 299, loss: 0.0688, instance_loss: 0.5338, weighted_loss: 0.2083, label: 1, bag_size: 85\n",
      "batch 319, loss: 0.0059, instance_loss: 0.0662, weighted_loss: 0.0240, label: 0, bag_size: 63\n",
      "batch 339, loss: 0.1947, instance_loss: 0.2117, weighted_loss: 0.1998, label: 1, bag_size: 49\n",
      "batch 359, loss: 0.1433, instance_loss: 0.5272, weighted_loss: 0.2584, label: 0, bag_size: 26\n",
      "batch 379, loss: 0.7591, instance_loss: 0.6783, weighted_loss: 0.7348, label: 1, bag_size: 97\n",
      "batch 399, loss: 0.4604, instance_loss: 0.4910, weighted_loss: 0.4696, label: 1, bag_size: 67\n",
      "batch 419, loss: 0.2864, instance_loss: 0.5166, weighted_loss: 0.3555, label: 1, bag_size: 78\n",
      "batch 439, loss: 0.7062, instance_loss: 1.1515, weighted_loss: 0.8398, label: 1, bag_size: 39\n",
      "batch 459, loss: 0.1401, instance_loss: 0.2552, weighted_loss: 0.1746, label: 1, bag_size: 17\n",
      "batch 479, loss: 0.2687, instance_loss: 0.1742, weighted_loss: 0.2403, label: 1, bag_size: 34\n",
      "batch 499, loss: 0.0502, instance_loss: 0.9363, weighted_loss: 0.3160, label: 1, bag_size: 77\n",
      "batch 519, loss: 0.1226, instance_loss: 0.1742, weighted_loss: 0.1381, label: 1, bag_size: 68\n",
      "batch 539, loss: 0.1556, instance_loss: 0.1577, weighted_loss: 0.1563, label: 1, bag_size: 61\n",
      "batch 559, loss: 0.2219, instance_loss: 0.5814, weighted_loss: 0.3297, label: 1, bag_size: 60\n",
      "batch 579, loss: 0.0767, instance_loss: 0.2558, weighted_loss: 0.1304, label: 0, bag_size: 74\n",
      "batch 599, loss: 0.1564, instance_loss: 0.1402, weighted_loss: 0.1515, label: 0, bag_size: 38\n",
      "batch 619, loss: 0.0005, instance_loss: 0.2151, weighted_loss: 0.0649, label: 1, bag_size: 70\n",
      "batch 639, loss: 0.0017, instance_loss: 0.1512, weighted_loss: 0.0465, label: 1, bag_size: 81\n",
      "batch 659, loss: 0.1392, instance_loss: 0.7444, weighted_loss: 0.3207, label: 1, bag_size: 35\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9515151515151515: correct 10048/10560\n",
      "class 1 clustering acc 0.6996212121212121: correct 3694/5280\n",
      "Epoch: 13, train_loss: 0.4438, train_clustering_loss:  0.4982, train_error: 0.1788\n",
      "class 0: acc 0.7789115646258503, correct 229/294\n",
      "class 1: acc 0.855191256830601, correct 313/366\n",
      "\n",
      "Val Set, val_loss: 0.5803, val_error: 0.2568, auc: 0.8801\n",
      "class 0 clustering acc 0.668918918918919: correct 792/1184\n",
      "class 1 clustering acc 0.581081081081081: correct 344/592\n",
      "class 0: acc 0.7352941176470589, correct 25/34\n",
      "class 1: acc 0.75, correct 30/40\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1916, instance_loss: 0.0753, weighted_loss: 0.1567, label: 0, bag_size: 94\n",
      "batch 39, loss: 0.0052, instance_loss: 0.0443, weighted_loss: 0.0169, label: 1, bag_size: 79\n",
      "batch 59, loss: 0.0257, instance_loss: 0.2392, weighted_loss: 0.0898, label: 1, bag_size: 53\n",
      "batch 79, loss: 0.0082, instance_loss: 0.0654, weighted_loss: 0.0253, label: 1, bag_size: 60\n",
      "batch 99, loss: 0.3487, instance_loss: 0.2377, weighted_loss: 0.3154, label: 1, bag_size: 61\n",
      "batch 119, loss: 0.1446, instance_loss: 0.6680, weighted_loss: 0.3017, label: 0, bag_size: 31\n",
      "batch 139, loss: 3.8964, instance_loss: 2.8941, weighted_loss: 3.5957, label: 0, bag_size: 31\n",
      "batch 159, loss: 0.2482, instance_loss: 0.7763, weighted_loss: 0.4066, label: 0, bag_size: 38\n",
      "batch 179, loss: 0.2707, instance_loss: 0.5817, weighted_loss: 0.3640, label: 1, bag_size: 86\n",
      "batch 199, loss: 0.6482, instance_loss: 0.2946, weighted_loss: 0.5421, label: 1, bag_size: 20\n",
      "batch 219, loss: 0.1356, instance_loss: 0.3228, weighted_loss: 0.1918, label: 1, bag_size: 41\n",
      "batch 239, loss: 0.0120, instance_loss: 0.0178, weighted_loss: 0.0138, label: 0, bag_size: 58\n",
      "batch 259, loss: 1.1845, instance_loss: 0.6026, weighted_loss: 1.0099, label: 0, bag_size: 28\n",
      "batch 279, loss: 0.0008, instance_loss: 0.0155, weighted_loss: 0.0052, label: 0, bag_size: 64\n",
      "batch 299, loss: 0.0012, instance_loss: 0.0087, weighted_loss: 0.0034, label: 0, bag_size: 72\n",
      "batch 319, loss: 0.1816, instance_loss: 0.1868, weighted_loss: 0.1832, label: 0, bag_size: 75\n",
      "batch 339, loss: 0.0053, instance_loss: 0.0430, weighted_loss: 0.0166, label: 0, bag_size: 47\n",
      "batch 359, loss: 0.0058, instance_loss: 0.0180, weighted_loss: 0.0094, label: 1, bag_size: 92\n",
      "batch 379, loss: 0.0183, instance_loss: 0.1274, weighted_loss: 0.0511, label: 0, bag_size: 28\n",
      "batch 399, loss: 0.0127, instance_loss: 0.1724, weighted_loss: 0.0606, label: 1, bag_size: 62\n",
      "batch 419, loss: 0.0038, instance_loss: 0.0252, weighted_loss: 0.0102, label: 1, bag_size: 65\n",
      "batch 439, loss: 0.5003, instance_loss: 0.7623, weighted_loss: 0.5789, label: 1, bag_size: 52\n",
      "batch 459, loss: 0.1047, instance_loss: 0.1007, weighted_loss: 0.1035, label: 1, bag_size: 68\n",
      "batch 479, loss: 0.0038, instance_loss: 0.6582, weighted_loss: 0.2001, label: 1, bag_size: 84\n",
      "batch 499, loss: 0.0005, instance_loss: 0.9566, weighted_loss: 0.2874, label: 0, bag_size: 110\n",
      "batch 519, loss: 0.0005, instance_loss: 0.9078, weighted_loss: 0.2727, label: 1, bag_size: 31\n",
      "batch 539, loss: 0.0117, instance_loss: 0.6671, weighted_loss: 0.2083, label: 1, bag_size: 76\n",
      "batch 559, loss: 0.7991, instance_loss: 0.2353, weighted_loss: 0.6300, label: 1, bag_size: 93\n",
      "batch 579, loss: 0.0805, instance_loss: 0.0938, weighted_loss: 0.0845, label: 0, bag_size: 88\n",
      "batch 599, loss: 0.0044, instance_loss: 0.0448, weighted_loss: 0.0166, label: 1, bag_size: 31\n",
      "batch 619, loss: 3.6730, instance_loss: 2.1411, weighted_loss: 3.2134, label: 1, bag_size: 93\n",
      "batch 639, loss: 0.0132, instance_loss: 0.3095, weighted_loss: 0.1021, label: 0, bag_size: 35\n",
      "batch 659, loss: 0.0002, instance_loss: 0.2414, weighted_loss: 0.0726, label: 1, bag_size: 58\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9517992424242424: correct 10051/10560\n",
      "class 1 clustering acc 0.7232954545454545: correct 3819/5280\n",
      "Epoch: 14, train_loss: 0.4691, train_clustering_loss:  0.4579, train_error: 0.1758\n",
      "class 0: acc 0.8165137614678899, correct 267/327\n",
      "class 1: acc 0.8318318318318318, correct 277/333\n",
      "\n",
      "Val Set, val_loss: 0.8651, val_error: 0.2432, auc: 0.9000\n",
      "class 0 clustering acc 0.612331081081081: correct 725/1184\n",
      "class 1 clustering acc 0.6503378378378378: correct 385/592\n",
      "class 0: acc 0.5294117647058824, correct 18/34\n",
      "class 1: acc 0.95, correct 38/40\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2355, instance_loss: 0.6844, weighted_loss: 0.3702, label: 1, bag_size: 77\n",
      "batch 39, loss: 0.0161, instance_loss: 0.0210, weighted_loss: 0.0176, label: 1, bag_size: 107\n",
      "batch 59, loss: 0.0052, instance_loss: 0.0694, weighted_loss: 0.0245, label: 1, bag_size: 53\n",
      "batch 79, loss: 0.0851, instance_loss: 0.0709, weighted_loss: 0.0809, label: 0, bag_size: 92\n",
      "batch 99, loss: 0.0210, instance_loss: 0.0756, weighted_loss: 0.0374, label: 1, bag_size: 30\n",
      "batch 119, loss: 2.4571, instance_loss: 1.2168, weighted_loss: 2.0850, label: 1, bag_size: 95\n",
      "batch 139, loss: 0.1301, instance_loss: 0.1146, weighted_loss: 0.1255, label: 1, bag_size: 127\n",
      "batch 159, loss: 0.0308, instance_loss: 0.0926, weighted_loss: 0.0494, label: 1, bag_size: 78\n",
      "batch 179, loss: 0.0081, instance_loss: 0.0436, weighted_loss: 0.0188, label: 1, bag_size: 30\n",
      "batch 199, loss: 0.0591, instance_loss: 0.0412, weighted_loss: 0.0537, label: 0, bag_size: 28\n",
      "batch 219, loss: 0.0012, instance_loss: 0.0964, weighted_loss: 0.0298, label: 0, bag_size: 73\n",
      "batch 239, loss: 3.8636, instance_loss: 2.5773, weighted_loss: 3.4777, label: 0, bag_size: 68\n",
      "batch 259, loss: 1.8914, instance_loss: 3.8960, weighted_loss: 2.4928, label: 1, bag_size: 17\n",
      "batch 279, loss: 0.0004, instance_loss: 0.0235, weighted_loss: 0.0073, label: 0, bag_size: 72\n",
      "batch 299, loss: 0.0002, instance_loss: 0.5258, weighted_loss: 0.1579, label: 1, bag_size: 74\n",
      "batch 319, loss: 0.0798, instance_loss: 0.1167, weighted_loss: 0.0909, label: 1, bag_size: 123\n",
      "batch 339, loss: 0.2539, instance_loss: 0.5094, weighted_loss: 0.3306, label: 1, bag_size: 35\n",
      "batch 359, loss: 0.0382, instance_loss: 0.0689, weighted_loss: 0.0474, label: 1, bag_size: 56\n",
      "batch 379, loss: 0.1483, instance_loss: 0.2109, weighted_loss: 0.1671, label: 1, bag_size: 20\n",
      "batch 399, loss: 0.3621, instance_loss: 0.5933, weighted_loss: 0.4315, label: 1, bag_size: 95\n",
      "batch 419, loss: 0.1082, instance_loss: 0.1170, weighted_loss: 0.1108, label: 1, bag_size: 48\n",
      "batch 439, loss: 0.2591, instance_loss: 0.5878, weighted_loss: 0.3577, label: 0, bag_size: 68\n",
      "batch 459, loss: 0.0324, instance_loss: 0.1875, weighted_loss: 0.0789, label: 1, bag_size: 80\n",
      "batch 479, loss: 1.0367, instance_loss: 0.8537, weighted_loss: 0.9818, label: 0, bag_size: 34\n",
      "batch 499, loss: 0.9733, instance_loss: 0.5431, weighted_loss: 0.8442, label: 0, bag_size: 66\n",
      "batch 519, loss: 0.0002, instance_loss: 0.0041, weighted_loss: 0.0014, label: 0, bag_size: 36\n",
      "batch 539, loss: 0.0020, instance_loss: 0.4004, weighted_loss: 0.1216, label: 1, bag_size: 45\n",
      "batch 559, loss: 0.0166, instance_loss: 0.0147, weighted_loss: 0.0160, label: 0, bag_size: 40\n",
      "batch 579, loss: 0.0385, instance_loss: 0.3178, weighted_loss: 0.1223, label: 1, bag_size: 40\n",
      "batch 599, loss: 0.0614, instance_loss: 0.2288, weighted_loss: 0.1116, label: 1, bag_size: 56\n",
      "batch 619, loss: 0.2638, instance_loss: 0.1711, weighted_loss: 0.2360, label: 1, bag_size: 72\n",
      "batch 639, loss: 0.1463, instance_loss: 0.2209, weighted_loss: 0.1687, label: 1, bag_size: 93\n",
      "batch 659, loss: 0.4584, instance_loss: 1.2135, weighted_loss: 0.6850, label: 1, bag_size: 35\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9448863636363637: correct 9978/10560\n",
      "class 1 clustering acc 0.7702651515151515: correct 4067/5280\n",
      "Epoch: 15, train_loss: 0.3948, train_clustering_loss:  0.4384, train_error: 0.1561\n",
      "class 0: acc 0.8228228228228228, correct 274/333\n",
      "class 1: acc 0.8654434250764526, correct 283/327\n",
      "\n",
      "Val Set, val_loss: 0.4952, val_error: 0.2838, auc: 0.8765\n",
      "class 0 clustering acc 0.6891891891891891: correct 816/1184\n",
      "class 1 clustering acc 0.5945945945945946: correct 352/592\n",
      "class 0: acc 0.7941176470588235, correct 27/34\n",
      "class 1: acc 0.65, correct 26/40\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3049, instance_loss: 0.1424, weighted_loss: 0.2562, label: 1, bag_size: 78\n",
      "batch 39, loss: 0.0012, instance_loss: 0.1625, weighted_loss: 0.0496, label: 1, bag_size: 20\n",
      "batch 59, loss: 0.0279, instance_loss: 0.2029, weighted_loss: 0.0804, label: 0, bag_size: 91\n",
      "batch 79, loss: 0.0001, instance_loss: 0.0665, weighted_loss: 0.0201, label: 1, bag_size: 47\n",
      "batch 99, loss: 0.0000, instance_loss: 0.1621, weighted_loss: 0.0486, label: 1, bag_size: 28\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0416, weighted_loss: 0.0125, label: 0, bag_size: 75\n",
      "batch 139, loss: 0.0621, instance_loss: 1.2464, weighted_loss: 0.4174, label: 0, bag_size: 50\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0049, weighted_loss: 0.0015, label: 0, bag_size: 27\n",
      "batch 179, loss: 0.0002, instance_loss: 0.3224, weighted_loss: 0.0968, label: 1, bag_size: 65\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0152, weighted_loss: 0.0046, label: 0, bag_size: 89\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0043, weighted_loss: 0.0013, label: 0, bag_size: 40\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0313, weighted_loss: 0.0094, label: 1, bag_size: 52\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0125, weighted_loss: 0.0038, label: 0, bag_size: 36\n",
      "batch 279, loss: 0.0000, instance_loss: 0.2332, weighted_loss: 0.0700, label: 0, bag_size: 51\n",
      "batch 299, loss: 1.2334, instance_loss: 2.5080, weighted_loss: 1.6158, label: 1, bag_size: 22\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0113, weighted_loss: 0.0034, label: 1, bag_size: 41\n",
      "batch 339, loss: 0.0039, instance_loss: 2.4522, weighted_loss: 0.7384, label: 1, bag_size: 99\n",
      "batch 359, loss: 0.0000, instance_loss: 0.1539, weighted_loss: 0.0462, label: 0, bag_size: 55\n",
      "batch 379, loss: 0.0574, instance_loss: 1.3587, weighted_loss: 0.4478, label: 0, bag_size: 32\n",
      "batch 399, loss: 0.0360, instance_loss: 0.0816, weighted_loss: 0.0497, label: 1, bag_size: 35\n",
      "batch 419, loss: 0.0002, instance_loss: 0.3451, weighted_loss: 0.1037, label: 0, bag_size: 87\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0112, weighted_loss: 0.0034, label: 1, bag_size: 41\n",
      "batch 459, loss: 0.0323, instance_loss: 0.2600, weighted_loss: 0.1006, label: 0, bag_size: 18\n",
      "batch 479, loss: 0.4322, instance_loss: 0.1359, weighted_loss: 0.3433, label: 1, bag_size: 23\n",
      "batch 499, loss: 0.0015, instance_loss: 0.3314, weighted_loss: 0.1004, label: 0, bag_size: 35\n",
      "batch 519, loss: 0.0003, instance_loss: 0.3137, weighted_loss: 0.0943, label: 0, bag_size: 80\n",
      "batch 539, loss: 0.0376, instance_loss: 0.6229, weighted_loss: 0.2131, label: 1, bag_size: 102\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0319, weighted_loss: 0.0096, label: 1, bag_size: 81\n",
      "batch 579, loss: 0.1489, instance_loss: 0.1601, weighted_loss: 0.1523, label: 1, bag_size: 20\n",
      "batch 599, loss: 0.0905, instance_loss: 1.7981, weighted_loss: 0.6028, label: 0, bag_size: 36\n",
      "batch 619, loss: 2.1046, instance_loss: 1.2242, weighted_loss: 1.8405, label: 1, bag_size: 126\n",
      "batch 639, loss: 0.4423, instance_loss: 0.4839, weighted_loss: 0.4548, label: 1, bag_size: 41\n",
      "batch 659, loss: 4.0148, instance_loss: 0.8506, weighted_loss: 3.0656, label: 1, bag_size: 96\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9236742424242425: correct 9754/10560\n",
      "class 1 clustering acc 0.6547348484848485: correct 3457/5280\n",
      "Epoch: 16, train_loss: 0.8619, train_clustering_loss:  0.6027, train_error: 0.1833\n",
      "class 0: acc 0.8006134969325154, correct 261/326\n",
      "class 1: acc 0.8323353293413174, correct 278/334\n",
      "\n",
      "Val Set, val_loss: 0.5545, val_error: 0.2027, auc: 0.8993\n",
      "class 0 clustering acc 0.7390202702702703: correct 875/1184\n",
      "class 1 clustering acc 0.535472972972973: correct 317/592\n",
      "class 0: acc 0.7352941176470589, correct 25/34\n",
      "class 1: acc 0.85, correct 34/40\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0137, instance_loss: 0.4777, weighted_loss: 0.1529, label: 0, bag_size: 126\n",
      "batch 39, loss: 3.4765, instance_loss: 0.6958, weighted_loss: 2.6423, label: 0, bag_size: 28\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0301, weighted_loss: 0.0090, label: 1, bag_size: 25\n",
      "batch 79, loss: 0.0001, instance_loss: 0.3649, weighted_loss: 0.1095, label: 0, bag_size: 74\n",
      "batch 99, loss: 0.0000, instance_loss: 0.4842, weighted_loss: 0.1453, label: 0, bag_size: 107\n",
      "batch 119, loss: 0.0005, instance_loss: 0.2154, weighted_loss: 0.0649, label: 1, bag_size: 69\n",
      "batch 139, loss: 0.0042, instance_loss: 0.5677, weighted_loss: 0.1733, label: 0, bag_size: 81\n",
      "batch 159, loss: 1.2205, instance_loss: 1.4359, weighted_loss: 1.2851, label: 1, bag_size: 77\n",
      "batch 179, loss: 0.0000, instance_loss: 0.6238, weighted_loss: 0.1872, label: 0, bag_size: 51\n",
      "batch 199, loss: 0.2159, instance_loss: 0.4997, weighted_loss: 0.3011, label: 1, bag_size: 88\n",
      "batch 219, loss: 0.0000, instance_loss: 0.9826, weighted_loss: 0.2948, label: 1, bag_size: 51\n",
      "batch 239, loss: 0.1112, instance_loss: 0.6418, weighted_loss: 0.2704, label: 1, bag_size: 78\n",
      "batch 259, loss: 0.0008, instance_loss: 0.1984, weighted_loss: 0.0601, label: 1, bag_size: 75\n",
      "batch 279, loss: 0.0000, instance_loss: 0.3163, weighted_loss: 0.0949, label: 0, bag_size: 93\n",
      "batch 299, loss: 0.9216, instance_loss: 0.7715, weighted_loss: 0.8766, label: 1, bag_size: 95\n",
      "batch 319, loss: 0.0000, instance_loss: 0.2705, weighted_loss: 0.0812, label: 0, bag_size: 35\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0456, weighted_loss: 0.0137, label: 1, bag_size: 107\n",
      "batch 359, loss: 2.2504, instance_loss: 1.2736, weighted_loss: 1.9574, label: 0, bag_size: 27\n",
      "batch 379, loss: 0.0799, instance_loss: 0.0646, weighted_loss: 0.0753, label: 1, bag_size: 107\n",
      "batch 399, loss: 0.0001, instance_loss: 0.3406, weighted_loss: 0.1023, label: 0, bag_size: 21\n",
      "batch 419, loss: 0.0058, instance_loss: 0.4183, weighted_loss: 0.1296, label: 0, bag_size: 35\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0209, weighted_loss: 0.0063, label: 0, bag_size: 36\n",
      "batch 459, loss: 0.0023, instance_loss: 0.7773, weighted_loss: 0.2348, label: 0, bag_size: 66\n",
      "batch 479, loss: 0.0000, instance_loss: 0.7512, weighted_loss: 0.2253, label: 1, bag_size: 41\n",
      "batch 499, loss: 0.0000, instance_loss: 0.5847, weighted_loss: 0.1754, label: 0, bag_size: 65\n",
      "batch 519, loss: 0.0000, instance_loss: 0.3313, weighted_loss: 0.0994, label: 0, bag_size: 83\n",
      "batch 539, loss: 3.2903, instance_loss: 0.5154, weighted_loss: 2.4578, label: 1, bag_size: 85\n",
      "batch 559, loss: 6.6507, instance_loss: 1.3331, weighted_loss: 5.0554, label: 1, bag_size: 52\n",
      "batch 579, loss: 0.0034, instance_loss: 0.0763, weighted_loss: 0.0253, label: 1, bag_size: 98\n",
      "batch 599, loss: 0.0583, instance_loss: 1.6461, weighted_loss: 0.5347, label: 1, bag_size: 63\n",
      "batch 619, loss: 0.0115, instance_loss: 0.8621, weighted_loss: 0.2667, label: 1, bag_size: 21\n",
      "batch 639, loss: 0.0000, instance_loss: 0.5679, weighted_loss: 0.1704, label: 1, bag_size: 76\n",
      "batch 659, loss: 0.0000, instance_loss: 0.3958, weighted_loss: 0.1187, label: 1, bag_size: 46\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9195075757575758: correct 9710/10560\n",
      "class 1 clustering acc 0.6386363636363637: correct 3372/5280\n",
      "Epoch: 17, train_loss: 0.5154, train_clustering_loss:  0.5782, train_error: 0.1409\n",
      "class 0: acc 0.85625, correct 274/320\n",
      "class 1: acc 0.861764705882353, correct 293/340\n",
      "\n",
      "Val Set, val_loss: 0.7966, val_error: 0.1486, auc: 0.9287\n",
      "class 0 clustering acc 0.8859797297297297: correct 1049/1184\n",
      "class 1 clustering acc 0.48986486486486486: correct 290/592\n",
      "class 0: acc 0.7647058823529411, correct 26/34\n",
      "class 1: acc 0.925, correct 37/40\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0016, instance_loss: 0.7141, weighted_loss: 0.2153, label: 1, bag_size: 53\n",
      "batch 39, loss: 0.0000, instance_loss: 0.2301, weighted_loss: 0.0690, label: 0, bag_size: 77\n",
      "batch 59, loss: 0.0000, instance_loss: 0.4732, weighted_loss: 0.1420, label: 0, bag_size: 77\n",
      "batch 79, loss: 0.2403, instance_loss: 1.9705, weighted_loss: 0.7594, label: 1, bag_size: 31\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0384, weighted_loss: 0.0115, label: 0, bag_size: 65\n",
      "batch 119, loss: 0.0203, instance_loss: 0.6909, weighted_loss: 0.2215, label: 1, bag_size: 13\n",
      "batch 139, loss: 0.0775, instance_loss: 0.9920, weighted_loss: 0.3518, label: 0, bag_size: 73\n",
      "batch 159, loss: 0.0006, instance_loss: 0.2028, weighted_loss: 0.0612, label: 1, bag_size: 39\n",
      "batch 179, loss: 3.9631, instance_loss: 2.4058, weighted_loss: 3.4959, label: 0, bag_size: 79\n",
      "batch 199, loss: 0.7631, instance_loss: 0.4187, weighted_loss: 0.6598, label: 1, bag_size: 23\n",
      "batch 219, loss: 0.0009, instance_loss: 0.0124, weighted_loss: 0.0043, label: 1, bag_size: 62\n",
      "batch 239, loss: 0.1222, instance_loss: 0.1320, weighted_loss: 0.1251, label: 1, bag_size: 23\n",
      "batch 259, loss: 0.4561, instance_loss: 0.4424, weighted_loss: 0.4520, label: 0, bag_size: 75\n",
      "batch 279, loss: 0.0043, instance_loss: 0.2834, weighted_loss: 0.0880, label: 0, bag_size: 65\n",
      "batch 299, loss: 0.3635, instance_loss: 2.2800, weighted_loss: 0.9385, label: 1, bag_size: 46\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0326, weighted_loss: 0.0098, label: 1, bag_size: 109\n",
      "batch 339, loss: 0.0095, instance_loss: 0.1113, weighted_loss: 0.0401, label: 1, bag_size: 85\n",
      "batch 359, loss: 0.0012, instance_loss: 0.1905, weighted_loss: 0.0580, label: 0, bag_size: 97\n",
      "batch 379, loss: 0.0005, instance_loss: 0.1916, weighted_loss: 0.0578, label: 1, bag_size: 65\n",
      "batch 399, loss: 0.0721, instance_loss: 0.0422, weighted_loss: 0.0631, label: 1, bag_size: 36\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0158, weighted_loss: 0.0048, label: 1, bag_size: 83\n",
      "batch 439, loss: 0.0004, instance_loss: 0.1318, weighted_loss: 0.0398, label: 0, bag_size: 33\n",
      "batch 459, loss: 0.0000, instance_loss: 0.1784, weighted_loss: 0.0535, label: 0, bag_size: 94\n",
      "batch 479, loss: 0.5103, instance_loss: 0.9472, weighted_loss: 0.6414, label: 1, bag_size: 55\n",
      "batch 499, loss: 0.0773, instance_loss: 0.6220, weighted_loss: 0.2407, label: 0, bag_size: 36\n",
      "batch 519, loss: 0.0183, instance_loss: 0.0585, weighted_loss: 0.0303, label: 1, bag_size: 69\n",
      "batch 539, loss: 0.8793, instance_loss: 0.2474, weighted_loss: 0.6898, label: 1, bag_size: 36\n",
      "batch 559, loss: 0.2582, instance_loss: 0.3546, weighted_loss: 0.2871, label: 1, bag_size: 118\n",
      "batch 579, loss: 8.1482, instance_loss: 3.5338, weighted_loss: 6.7639, label: 0, bag_size: 41\n",
      "batch 599, loss: 0.0056, instance_loss: 0.0709, weighted_loss: 0.0252, label: 1, bag_size: 85\n",
      "batch 619, loss: 0.0853, instance_loss: 0.1249, weighted_loss: 0.0972, label: 0, bag_size: 35\n",
      "batch 639, loss: 4.6462, instance_loss: 2.7050, weighted_loss: 4.0638, label: 0, bag_size: 69\n",
      "batch 659, loss: 0.0126, instance_loss: 0.1707, weighted_loss: 0.0600, label: 0, bag_size: 75\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9306818181818182: correct 9828/10560\n",
      "class 1 clustering acc 0.7005681818181818: correct 3699/5280\n",
      "Epoch: 18, train_loss: 0.5193, train_clustering_loss:  0.5408, train_error: 0.1667\n",
      "class 0: acc 0.8141025641025641, correct 254/312\n",
      "class 1: acc 0.8505747126436781, correct 296/348\n",
      "\n",
      "Val Set, val_loss: 0.5623, val_error: 0.2027, auc: 0.9088\n",
      "class 0 clustering acc 0.7576013513513513: correct 897/1184\n",
      "class 1 clustering acc 0.5472972972972973: correct 324/592\n",
      "class 0: acc 0.6470588235294118, correct 22/34\n",
      "class 1: acc 0.925, correct 37/40\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0015, instance_loss: 0.2571, weighted_loss: 0.0782, label: 0, bag_size: 90\n",
      "batch 39, loss: 0.0326, instance_loss: 0.2511, weighted_loss: 0.0982, label: 1, bag_size: 32\n",
      "batch 59, loss: 0.0016, instance_loss: 0.0137, weighted_loss: 0.0053, label: 1, bag_size: 107\n",
      "batch 79, loss: 0.3818, instance_loss: 0.4079, weighted_loss: 0.3896, label: 1, bag_size: 83\n",
      "batch 99, loss: 0.1589, instance_loss: 0.7258, weighted_loss: 0.3290, label: 0, bag_size: 61\n",
      "batch 119, loss: 0.1019, instance_loss: 0.4318, weighted_loss: 0.2008, label: 1, bag_size: 21\n",
      "batch 139, loss: 0.0063, instance_loss: 0.3071, weighted_loss: 0.0965, label: 0, bag_size: 90\n",
      "batch 159, loss: 0.0726, instance_loss: 0.1477, weighted_loss: 0.0951, label: 0, bag_size: 49\n",
      "batch 179, loss: 0.0388, instance_loss: 0.0190, weighted_loss: 0.0329, label: 1, bag_size: 88\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0117, weighted_loss: 0.0035, label: 1, bag_size: 114\n",
      "batch 219, loss: 1.5550, instance_loss: 1.5462, weighted_loss: 1.5524, label: 0, bag_size: 95\n",
      "batch 239, loss: 1.6299, instance_loss: 1.2053, weighted_loss: 1.5025, label: 1, bag_size: 24\n",
      "batch 259, loss: 0.8939, instance_loss: 0.6217, weighted_loss: 0.8122, label: 0, bag_size: 28\n",
      "batch 279, loss: 0.0244, instance_loss: 0.0348, weighted_loss: 0.0275, label: 1, bag_size: 31\n",
      "batch 299, loss: 0.0001, instance_loss: 0.2414, weighted_loss: 0.0725, label: 0, bag_size: 45\n",
      "batch 319, loss: 0.4523, instance_loss: 0.7926, weighted_loss: 0.5544, label: 1, bag_size: 28\n",
      "batch 339, loss: 0.0544, instance_loss: 0.0762, weighted_loss: 0.0609, label: 1, bag_size: 84\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0051, weighted_loss: 0.0015, label: 1, bag_size: 112\n",
      "batch 379, loss: 0.1851, instance_loss: 0.5905, weighted_loss: 0.3067, label: 1, bag_size: 20\n",
      "batch 399, loss: 0.0046, instance_loss: 0.6815, weighted_loss: 0.2076, label: 1, bag_size: 67\n",
      "batch 419, loss: 0.1018, instance_loss: 0.2183, weighted_loss: 0.1367, label: 1, bag_size: 32\n",
      "batch 439, loss: 0.2394, instance_loss: 0.6444, weighted_loss: 0.3609, label: 1, bag_size: 18\n",
      "batch 459, loss: 0.0005, instance_loss: 0.0606, weighted_loss: 0.0185, label: 1, bag_size: 120\n",
      "batch 479, loss: 0.0011, instance_loss: 0.2478, weighted_loss: 0.0751, label: 1, bag_size: 58\n",
      "batch 499, loss: 0.0083, instance_loss: 1.7143, weighted_loss: 0.5201, label: 1, bag_size: 65\n",
      "batch 519, loss: 0.0000, instance_loss: 0.3498, weighted_loss: 0.1050, label: 0, bag_size: 88\n",
      "batch 539, loss: 2.8776, instance_loss: 1.3690, weighted_loss: 2.4250, label: 1, bag_size: 77\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0091, weighted_loss: 0.0027, label: 1, bag_size: 120\n",
      "batch 579, loss: 0.0002, instance_loss: 0.2746, weighted_loss: 0.0825, label: 0, bag_size: 45\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0379, weighted_loss: 0.0114, label: 1, bag_size: 68\n",
      "batch 619, loss: 1.6686, instance_loss: 0.6035, weighted_loss: 1.3491, label: 1, bag_size: 16\n",
      "batch 639, loss: 5.2330, instance_loss: 1.2946, weighted_loss: 4.0515, label: 0, bag_size: 37\n",
      "batch 659, loss: 0.1571, instance_loss: 0.1555, weighted_loss: 0.1566, label: 1, bag_size: 86\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9278409090909091: correct 9798/10560\n",
      "class 1 clustering acc 0.7261363636363637: correct 3834/5280\n",
      "Epoch: 19, train_loss: 0.4342, train_clustering_loss:  0.4941, train_error: 0.1394\n",
      "class 0: acc 0.8299319727891157, correct 244/294\n",
      "class 1: acc 0.8852459016393442, correct 324/366\n",
      "\n",
      "Val Set, val_loss: 0.5289, val_error: 0.2568, auc: 0.8765\n",
      "class 0 clustering acc 0.6959459459459459: correct 824/1184\n",
      "class 1 clustering acc 0.6216216216216216: correct 368/592\n",
      "class 0: acc 0.8529411764705882, correct 29/34\n",
      "class 1: acc 0.65, correct 26/40\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6423, instance_loss: 1.9239, weighted_loss: 1.0268, label: 1, bag_size: 67\n",
      "batch 39, loss: 0.0606, instance_loss: 0.2258, weighted_loss: 0.1102, label: 1, bag_size: 69\n",
      "batch 59, loss: 0.9593, instance_loss: 0.6332, weighted_loss: 0.8615, label: 0, bag_size: 38\n",
      "batch 79, loss: 0.0000, instance_loss: 0.1304, weighted_loss: 0.0391, label: 1, bag_size: 49\n",
      "batch 99, loss: 0.0030, instance_loss: 0.1944, weighted_loss: 0.0604, label: 1, bag_size: 100\n",
      "batch 119, loss: 0.0002, instance_loss: 0.0604, weighted_loss: 0.0182, label: 1, bag_size: 47\n",
      "batch 139, loss: 0.0000, instance_loss: 0.1470, weighted_loss: 0.0441, label: 0, bag_size: 65\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0102, weighted_loss: 0.0031, label: 1, bag_size: 63\n",
      "batch 179, loss: 0.0004, instance_loss: 0.2408, weighted_loss: 0.0725, label: 0, bag_size: 83\n",
      "batch 199, loss: 0.0002, instance_loss: 0.2626, weighted_loss: 0.0789, label: 0, bag_size: 109\n",
      "batch 219, loss: 0.1775, instance_loss: 0.2007, weighted_loss: 0.1844, label: 1, bag_size: 73\n",
      "batch 239, loss: 2.0858, instance_loss: 1.2857, weighted_loss: 1.8458, label: 1, bag_size: 88\n",
      "batch 259, loss: 0.0005, instance_loss: 0.1047, weighted_loss: 0.0317, label: 0, bag_size: 26\n",
      "batch 279, loss: 0.2072, instance_loss: 0.1113, weighted_loss: 0.1784, label: 1, bag_size: 17\n",
      "batch 299, loss: 0.3429, instance_loss: 0.3820, weighted_loss: 0.3547, label: 1, bag_size: 126\n",
      "batch 319, loss: 0.0001, instance_loss: 0.1906, weighted_loss: 0.0573, label: 1, bag_size: 122\n",
      "batch 339, loss: 0.3753, instance_loss: 0.2158, weighted_loss: 0.3275, label: 1, bag_size: 24\n",
      "batch 359, loss: 0.0038, instance_loss: 0.0109, weighted_loss: 0.0059, label: 1, bag_size: 24\n",
      "batch 379, loss: 0.0003, instance_loss: 0.4308, weighted_loss: 0.1294, label: 0, bag_size: 29\n",
      "batch 399, loss: 0.4022, instance_loss: 0.8685, weighted_loss: 0.5421, label: 1, bag_size: 36\n",
      "batch 419, loss: 0.0002, instance_loss: 0.2817, weighted_loss: 0.0847, label: 1, bag_size: 99\n",
      "batch 439, loss: 0.0026, instance_loss: 0.0063, weighted_loss: 0.0037, label: 1, bag_size: 95\n",
      "batch 459, loss: 0.0092, instance_loss: 0.0250, weighted_loss: 0.0140, label: 1, bag_size: 91\n",
      "batch 479, loss: 0.0015, instance_loss: 0.0274, weighted_loss: 0.0093, label: 0, bag_size: 50\n",
      "batch 499, loss: 0.2948, instance_loss: 0.5856, weighted_loss: 0.3821, label: 0, bag_size: 73\n",
      "batch 519, loss: 0.0059, instance_loss: 0.0133, weighted_loss: 0.0081, label: 0, bag_size: 90\n",
      "batch 539, loss: 0.0487, instance_loss: 0.3086, weighted_loss: 0.1267, label: 0, bag_size: 54\n",
      "batch 559, loss: 0.0381, instance_loss: 0.0658, weighted_loss: 0.0464, label: 1, bag_size: 17\n",
      "batch 579, loss: 0.0081, instance_loss: 0.0231, weighted_loss: 0.0126, label: 1, bag_size: 61\n",
      "batch 599, loss: 0.0000, instance_loss: 0.1046, weighted_loss: 0.0314, label: 0, bag_size: 73\n",
      "batch 619, loss: 0.0367, instance_loss: 0.1163, weighted_loss: 0.0606, label: 0, bag_size: 31\n",
      "batch 639, loss: 0.0000, instance_loss: 0.1292, weighted_loss: 0.0388, label: 0, bag_size: 49\n",
      "batch 659, loss: 0.3448, instance_loss: 0.8588, weighted_loss: 0.4990, label: 1, bag_size: 51\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9413825757575758: correct 9941/10560\n",
      "class 1 clustering acc 0.7880681818181818: correct 4161/5280\n",
      "Epoch: 20, train_loss: 0.2946, train_clustering_loss:  0.4053, train_error: 0.1182\n",
      "class 0: acc 0.8589341692789969, correct 274/319\n",
      "class 1: acc 0.9032258064516129, correct 308/341\n",
      "\n",
      "Val Set, val_loss: 0.5513, val_error: 0.2027, auc: 0.9081\n",
      "class 0 clustering acc 0.6359797297297297: correct 753/1184\n",
      "class 1 clustering acc 0.6570945945945946: correct 389/592\n",
      "class 0: acc 0.8529411764705882, correct 29/34\n",
      "class 1: acc 0.75, correct 30/40\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0123, instance_loss: 0.0549, weighted_loss: 0.0251, label: 1, bag_size: 76\n",
      "batch 39, loss: 0.0039, instance_loss: 0.1896, weighted_loss: 0.0596, label: 0, bag_size: 63\n",
      "batch 59, loss: 0.0451, instance_loss: 0.6991, weighted_loss: 0.2413, label: 1, bag_size: 116\n",
      "batch 79, loss: 0.0005, instance_loss: 0.0121, weighted_loss: 0.0040, label: 1, bag_size: 86\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0853, weighted_loss: 0.0256, label: 0, bag_size: 103\n",
      "batch 119, loss: 0.0183, instance_loss: 0.4490, weighted_loss: 0.1475, label: 1, bag_size: 27\n",
      "batch 139, loss: 1.4598, instance_loss: 0.8199, weighted_loss: 1.2678, label: 0, bag_size: 27\n",
      "batch 159, loss: 0.0177, instance_loss: 0.1205, weighted_loss: 0.0485, label: 0, bag_size: 86\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0982, weighted_loss: 0.0295, label: 0, bag_size: 77\n",
      "batch 199, loss: 1.8547, instance_loss: 3.2812, weighted_loss: 2.2826, label: 1, bag_size: 115\n",
      "batch 219, loss: 0.1875, instance_loss: 3.1011, weighted_loss: 1.0615, label: 0, bag_size: 35\n",
      "batch 239, loss: 0.0376, instance_loss: 0.5037, weighted_loss: 0.1774, label: 1, bag_size: 42\n",
      "batch 259, loss: 0.0186, instance_loss: 0.0185, weighted_loss: 0.0185, label: 1, bag_size: 29\n",
      "batch 279, loss: 0.0031, instance_loss: 0.0327, weighted_loss: 0.0119, label: 1, bag_size: 79\n",
      "batch 299, loss: 0.0002, instance_loss: 0.0043, weighted_loss: 0.0014, label: 1, bag_size: 32\n",
      "batch 319, loss: 1.8263, instance_loss: 0.7600, weighted_loss: 1.5064, label: 1, bag_size: 15\n",
      "batch 339, loss: 0.2429, instance_loss: 1.1564, weighted_loss: 0.5170, label: 1, bag_size: 88\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 0, bag_size: 87\n",
      "batch 379, loss: 0.0060, instance_loss: 0.0343, weighted_loss: 0.0145, label: 1, bag_size: 60\n",
      "batch 399, loss: 0.0219, instance_loss: 0.0160, weighted_loss: 0.0201, label: 1, bag_size: 62\n",
      "batch 419, loss: 0.4139, instance_loss: 1.1383, weighted_loss: 0.6312, label: 1, bag_size: 28\n",
      "batch 439, loss: 1.0288, instance_loss: 1.0633, weighted_loss: 1.0392, label: 0, bag_size: 48\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0056, weighted_loss: 0.0017, label: 0, bag_size: 75\n",
      "batch 479, loss: 0.0000, instance_loss: 0.1892, weighted_loss: 0.0568, label: 1, bag_size: 45\n",
      "batch 499, loss: 0.0000, instance_loss: 0.1696, weighted_loss: 0.0509, label: 1, bag_size: 107\n",
      "batch 519, loss: 0.0852, instance_loss: 0.4209, weighted_loss: 0.1859, label: 1, bag_size: 65\n",
      "batch 539, loss: 0.0001, instance_loss: 0.1042, weighted_loss: 0.0314, label: 0, bag_size: 28\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0070, weighted_loss: 0.0021, label: 0, bag_size: 107\n",
      "batch 579, loss: 0.0000, instance_loss: 0.1822, weighted_loss: 0.0547, label: 1, bag_size: 120\n",
      "batch 599, loss: 0.0000, instance_loss: 0.3861, weighted_loss: 0.1158, label: 0, bag_size: 25\n",
      "batch 619, loss: 0.1549, instance_loss: 1.4273, weighted_loss: 0.5366, label: 1, bag_size: 52\n",
      "batch 639, loss: 0.0002, instance_loss: 0.0771, weighted_loss: 0.0233, label: 1, bag_size: 109\n",
      "batch 659, loss: 0.0000, instance_loss: 0.1512, weighted_loss: 0.0454, label: 0, bag_size: 29\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9555871212121212: correct 10091/10560\n",
      "class 1 clustering acc 0.812310606060606: correct 4289/5280\n",
      "Epoch: 21, train_loss: 0.2302, train_clustering_loss:  0.3611, train_error: 0.0833\n",
      "class 0: acc 0.906832298136646, correct 292/322\n",
      "class 1: acc 0.9260355029585798, correct 313/338\n",
      "\n",
      "Val Set, val_loss: 0.8909, val_error: 0.2297, auc: 0.8978\n",
      "class 0 clustering acc 0.6528716216216216: correct 773/1184\n",
      "class 1 clustering acc 0.7094594594594594: correct 420/592\n",
      "class 0: acc 0.8823529411764706, correct 30/34\n",
      "class 1: acc 0.675, correct 27/40\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.4100, weighted_loss: 0.1231, label: 1, bag_size: 41\n",
      "batch 39, loss: 0.0000, instance_loss: 1.7316, weighted_loss: 0.5195, label: 1, bag_size: 26\n",
      "batch 59, loss: 0.0001, instance_loss: 0.1835, weighted_loss: 0.0551, label: 1, bag_size: 31\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0831, weighted_loss: 0.0249, label: 0, bag_size: 88\n",
      "batch 99, loss: 0.0153, instance_loss: 0.4843, weighted_loss: 0.1560, label: 0, bag_size: 29\n",
      "batch 119, loss: 0.0046, instance_loss: 0.3998, weighted_loss: 0.1232, label: 1, bag_size: 44\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0385, weighted_loss: 0.0116, label: 0, bag_size: 91\n",
      "batch 159, loss: 0.4269, instance_loss: 0.6445, weighted_loss: 0.4922, label: 0, bag_size: 36\n",
      "batch 179, loss: 0.2730, instance_loss: 0.5205, weighted_loss: 0.3472, label: 1, bag_size: 42\n",
      "batch 199, loss: 0.0563, instance_loss: 0.5308, weighted_loss: 0.1986, label: 1, bag_size: 23\n",
      "batch 219, loss: 0.0018, instance_loss: 0.1215, weighted_loss: 0.0377, label: 0, bag_size: 107\n",
      "batch 239, loss: 0.3109, instance_loss: 0.5531, weighted_loss: 0.3836, label: 1, bag_size: 48\n",
      "batch 259, loss: 0.2086, instance_loss: 0.3133, weighted_loss: 0.2400, label: 0, bag_size: 86\n",
      "batch 279, loss: 0.0249, instance_loss: 0.2795, weighted_loss: 0.1013, label: 0, bag_size: 79\n",
      "batch 299, loss: 0.0002, instance_loss: 0.1579, weighted_loss: 0.0475, label: 0, bag_size: 109\n",
      "batch 319, loss: 0.0007, instance_loss: 0.1512, weighted_loss: 0.0458, label: 1, bag_size: 84\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0046, weighted_loss: 0.0014, label: 1, bag_size: 26\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0073, weighted_loss: 0.0022, label: 1, bag_size: 100\n",
      "batch 379, loss: 0.0251, instance_loss: 1.0806, weighted_loss: 0.3417, label: 1, bag_size: 77\n",
      "batch 399, loss: 0.0049, instance_loss: 0.4687, weighted_loss: 0.1440, label: 1, bag_size: 59\n",
      "batch 419, loss: 0.0031, instance_loss: 0.9954, weighted_loss: 0.3008, label: 0, bag_size: 35\n",
      "batch 439, loss: 0.8915, instance_loss: 0.4451, weighted_loss: 0.7576, label: 1, bag_size: 41\n",
      "batch 459, loss: 0.0012, instance_loss: 0.5007, weighted_loss: 0.1510, label: 1, bag_size: 15\n",
      "batch 479, loss: 0.0024, instance_loss: 0.3609, weighted_loss: 0.1100, label: 1, bag_size: 116\n",
      "batch 499, loss: 0.3571, instance_loss: 2.1203, weighted_loss: 0.8860, label: 0, bag_size: 104\n",
      "batch 519, loss: 0.0000, instance_loss: 0.2075, weighted_loss: 0.0623, label: 1, bag_size: 101\n",
      "batch 539, loss: 0.0000, instance_loss: 0.2417, weighted_loss: 0.0725, label: 1, bag_size: 85\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0279, weighted_loss: 0.0085, label: 0, bag_size: 108\n",
      "batch 579, loss: 0.0005, instance_loss: 0.0295, weighted_loss: 0.0092, label: 1, bag_size: 79\n",
      "batch 599, loss: 0.0210, instance_loss: 0.2522, weighted_loss: 0.0903, label: 1, bag_size: 14\n",
      "batch 619, loss: 0.0187, instance_loss: 0.0402, weighted_loss: 0.0252, label: 1, bag_size: 23\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0106, weighted_loss: 0.0032, label: 1, bag_size: 33\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0372, weighted_loss: 0.0112, label: 0, bag_size: 58\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9430871212121212: correct 9959/10560\n",
      "class 1 clustering acc 0.7471590909090909: correct 3945/5280\n",
      "Epoch: 22, train_loss: 0.3045, train_clustering_loss:  0.4434, train_error: 0.1030\n",
      "class 0: acc 0.8929663608562691, correct 292/327\n",
      "class 1: acc 0.9009009009009009, correct 300/333\n",
      "\n",
      "Val Set, val_loss: 0.8399, val_error: 0.2297, auc: 0.9294\n",
      "class 0 clustering acc 0.6579391891891891: correct 779/1184\n",
      "class 1 clustering acc 0.660472972972973: correct 391/592\n",
      "class 0: acc 0.8823529411764706, correct 30/34\n",
      "class 1: acc 0.675, correct 27/40\n",
      "EarlyStopping counter: 12 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0037, instance_loss: 0.1707, weighted_loss: 0.0538, label: 1, bag_size: 41\n",
      "batch 39, loss: 0.0006, instance_loss: 0.0107, weighted_loss: 0.0036, label: 1, bag_size: 95\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0212, weighted_loss: 0.0064, label: 0, bag_size: 72\n",
      "batch 79, loss: 2.4378, instance_loss: 0.3821, weighted_loss: 1.8211, label: 1, bag_size: 67\n",
      "batch 99, loss: 0.6078, instance_loss: 1.1828, weighted_loss: 0.7803, label: 0, bag_size: 74\n",
      "batch 119, loss: 2.3662, instance_loss: 1.2614, weighted_loss: 2.0348, label: 0, bag_size: 66\n",
      "batch 139, loss: 0.7813, instance_loss: 2.4224, weighted_loss: 1.2737, label: 0, bag_size: 69\n",
      "batch 159, loss: 0.0002, instance_loss: 0.0075, weighted_loss: 0.0024, label: 0, bag_size: 66\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0033, weighted_loss: 0.0010, label: 0, bag_size: 87\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0042, weighted_loss: 0.0013, label: 1, bag_size: 76\n",
      "batch 219, loss: 0.1908, instance_loss: 0.3591, weighted_loss: 0.2413, label: 0, bag_size: 43\n",
      "batch 239, loss: 0.2964, instance_loss: 1.8225, weighted_loss: 0.7542, label: 1, bag_size: 100\n",
      "batch 259, loss: 0.0058, instance_loss: 0.0686, weighted_loss: 0.0246, label: 0, bag_size: 25\n",
      "batch 279, loss: 0.0016, instance_loss: 0.0172, weighted_loss: 0.0063, label: 0, bag_size: 101\n",
      "batch 299, loss: 0.6415, instance_loss: 1.7706, weighted_loss: 0.9803, label: 0, bag_size: 41\n",
      "batch 319, loss: 0.0044, instance_loss: 0.0424, weighted_loss: 0.0158, label: 0, bag_size: 86\n",
      "batch 339, loss: 0.0144, instance_loss: 0.0251, weighted_loss: 0.0176, label: 0, bag_size: 44\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0469, weighted_loss: 0.0141, label: 1, bag_size: 73\n",
      "batch 379, loss: 0.2134, instance_loss: 0.2408, weighted_loss: 0.2216, label: 1, bag_size: 22\n",
      "batch 399, loss: 0.0002, instance_loss: 0.1218, weighted_loss: 0.0367, label: 1, bag_size: 25\n",
      "batch 419, loss: 0.0315, instance_loss: 0.0597, weighted_loss: 0.0400, label: 0, bag_size: 73\n",
      "batch 439, loss: 0.0069, instance_loss: 0.1324, weighted_loss: 0.0446, label: 1, bag_size: 121\n",
      "batch 459, loss: 0.0027, instance_loss: 0.0619, weighted_loss: 0.0204, label: 1, bag_size: 25\n",
      "batch 479, loss: 0.0056, instance_loss: 0.0746, weighted_loss: 0.0263, label: 0, bag_size: 35\n",
      "batch 499, loss: 0.0588, instance_loss: 0.1293, weighted_loss: 0.0800, label: 1, bag_size: 123\n",
      "batch 519, loss: 0.2062, instance_loss: 0.2075, weighted_loss: 0.2066, label: 1, bag_size: 96\n",
      "batch 539, loss: 0.0305, instance_loss: 0.0258, weighted_loss: 0.0291, label: 0, bag_size: 75\n",
      "batch 559, loss: 0.0380, instance_loss: 0.1559, weighted_loss: 0.0734, label: 0, bag_size: 35\n",
      "batch 579, loss: 0.7913, instance_loss: 3.2722, weighted_loss: 1.5356, label: 1, bag_size: 69\n",
      "batch 599, loss: 0.0783, instance_loss: 0.3989, weighted_loss: 0.1745, label: 1, bag_size: 42\n",
      "batch 619, loss: 0.1005, instance_loss: 0.1153, weighted_loss: 0.1050, label: 0, bag_size: 71\n",
      "batch 639, loss: 0.0000, instance_loss: 0.2361, weighted_loss: 0.0708, label: 1, bag_size: 62\n",
      "batch 659, loss: 0.0000, instance_loss: 0.1064, weighted_loss: 0.0319, label: 0, bag_size: 80\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9631628787878788: correct 10171/10560\n",
      "class 1 clustering acc 0.8388257575757576: correct 4429/5280\n",
      "Epoch: 23, train_loss: 0.2205, train_clustering_loss:  0.3069, train_error: 0.0773\n",
      "class 0: acc 0.9131832797427653, correct 284/311\n",
      "class 1: acc 0.9312320916905444, correct 325/349\n",
      "\n",
      "Val Set, val_loss: 1.6702, val_error: 0.3784, auc: 0.9140\n",
      "class 0 clustering acc 0.8353040540540541: correct 989/1184\n",
      "class 1 clustering acc 0.4510135135135135: correct 267/592\n",
      "class 0: acc 0.9705882352941176, correct 33/34\n",
      "class 1: acc 0.325, correct 13/40\n",
      "EarlyStopping counter: 13 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0133, instance_loss: 0.2154, weighted_loss: 0.0739, label: 1, bag_size: 99\n",
      "batch 39, loss: 0.0002, instance_loss: 0.1433, weighted_loss: 0.0431, label: 0, bag_size: 66\n",
      "batch 59, loss: 0.0380, instance_loss: 0.1834, weighted_loss: 0.0816, label: 0, bag_size: 20\n",
      "batch 79, loss: 0.0011, instance_loss: 0.1918, weighted_loss: 0.0583, label: 1, bag_size: 62\n",
      "batch 99, loss: 0.0562, instance_loss: 0.0927, weighted_loss: 0.0671, label: 0, bag_size: 93\n",
      "batch 119, loss: 0.5205, instance_loss: 1.9121, weighted_loss: 0.9379, label: 0, bag_size: 46\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0468, weighted_loss: 0.0141, label: 0, bag_size: 85\n",
      "batch 159, loss: 0.0182, instance_loss: 0.3994, weighted_loss: 0.1326, label: 1, bag_size: 62\n",
      "batch 179, loss: 0.0000, instance_loss: 0.3422, weighted_loss: 0.1027, label: 0, bag_size: 37\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0815, weighted_loss: 0.0244, label: 1, bag_size: 111\n",
      "batch 219, loss: 0.0318, instance_loss: 0.0343, weighted_loss: 0.0325, label: 0, bag_size: 40\n",
      "batch 239, loss: 0.0008, instance_loss: 0.2620, weighted_loss: 0.0791, label: 0, bag_size: 41\n",
      "batch 259, loss: 0.0521, instance_loss: 0.6900, weighted_loss: 0.2435, label: 0, bag_size: 94\n",
      "batch 279, loss: 2.1740, instance_loss: 0.4915, weighted_loss: 1.6693, label: 1, bag_size: 31\n",
      "batch 299, loss: 0.0000, instance_loss: 0.1362, weighted_loss: 0.0409, label: 1, bag_size: 56\n",
      "batch 319, loss: 2.6492, instance_loss: 2.4300, weighted_loss: 2.5835, label: 1, bag_size: 28\n",
      "batch 339, loss: 0.0011, instance_loss: 0.5301, weighted_loss: 0.1598, label: 0, bag_size: 45\n",
      "batch 359, loss: 4.3405, instance_loss: 1.3575, weighted_loss: 3.4456, label: 1, bag_size: 84\n",
      "batch 379, loss: 0.0028, instance_loss: 0.0663, weighted_loss: 0.0219, label: 0, bag_size: 103\n",
      "batch 399, loss: 0.0018, instance_loss: 0.0119, weighted_loss: 0.0048, label: 1, bag_size: 29\n",
      "batch 419, loss: 0.0080, instance_loss: 1.0673, weighted_loss: 0.3258, label: 1, bag_size: 84\n",
      "batch 439, loss: 0.3777, instance_loss: 0.6532, weighted_loss: 0.4604, label: 1, bag_size: 94\n",
      "batch 459, loss: 0.0034, instance_loss: 0.0850, weighted_loss: 0.0279, label: 0, bag_size: 50\n",
      "batch 479, loss: 0.0005, instance_loss: 0.0730, weighted_loss: 0.0223, label: 1, bag_size: 51\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0243, weighted_loss: 0.0073, label: 0, bag_size: 33\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0943, weighted_loss: 0.0283, label: 0, bag_size: 64\n",
      "batch 539, loss: 0.0004, instance_loss: 0.0066, weighted_loss: 0.0023, label: 1, bag_size: 39\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0026, weighted_loss: 0.0008, label: 0, bag_size: 65\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0216, weighted_loss: 0.0066, label: 1, bag_size: 56\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0339, weighted_loss: 0.0102, label: 1, bag_size: 85\n",
      "batch 619, loss: 0.0002, instance_loss: 0.0534, weighted_loss: 0.0161, label: 1, bag_size: 112\n",
      "batch 639, loss: 0.0298, instance_loss: 0.1462, weighted_loss: 0.0647, label: 1, bag_size: 69\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0390, weighted_loss: 0.0117, label: 0, bag_size: 94\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.959280303030303: correct 10130/10560\n",
      "class 1 clustering acc 0.8071969696969697: correct 4262/5280\n",
      "Epoch: 24, train_loss: 0.2965, train_clustering_loss:  0.3672, train_error: 0.1152\n",
      "class 0: acc 0.8936781609195402, correct 311/348\n",
      "class 1: acc 0.875, correct 273/312\n",
      "\n",
      "Val Set, val_loss: 0.5441, val_error: 0.2432, auc: 0.8750\n",
      "class 0 clustering acc 0.96875: correct 1147/1184\n",
      "class 1 clustering acc 0.36655405405405406: correct 217/592\n",
      "class 0: acc 0.8235294117647058, correct 28/34\n",
      "class 1: acc 0.7, correct 28/40\n",
      "EarlyStopping counter: 14 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.0252, weighted_loss: 0.0076, label: 0, bag_size: 91\n",
      "batch 39, loss: 0.0004, instance_loss: 0.1058, weighted_loss: 0.0320, label: 1, bag_size: 32\n",
      "batch 59, loss: 0.0033, instance_loss: 0.0395, weighted_loss: 0.0142, label: 0, bag_size: 83\n",
      "batch 79, loss: 0.0693, instance_loss: 0.0876, weighted_loss: 0.0748, label: 1, bag_size: 32\n",
      "batch 99, loss: 0.4192, instance_loss: 0.8647, weighted_loss: 0.5529, label: 1, bag_size: 28\n",
      "batch 119, loss: 0.6192, instance_loss: 0.2436, weighted_loss: 0.5065, label: 1, bag_size: 91\n",
      "batch 139, loss: 0.1158, instance_loss: 0.1321, weighted_loss: 0.1207, label: 0, bag_size: 26\n",
      "batch 159, loss: 0.0004, instance_loss: 0.0509, weighted_loss: 0.0155, label: 0, bag_size: 83\n",
      "batch 179, loss: 0.0002, instance_loss: 0.0329, weighted_loss: 0.0101, label: 1, bag_size: 27\n",
      "batch 199, loss: 0.0004, instance_loss: 0.0246, weighted_loss: 0.0076, label: 0, bag_size: 47\n",
      "batch 219, loss: 0.0403, instance_loss: 0.0738, weighted_loss: 0.0504, label: 0, bag_size: 25\n",
      "batch 239, loss: 0.0051, instance_loss: 0.0404, weighted_loss: 0.0157, label: 1, bag_size: 26\n",
      "batch 259, loss: 0.2723, instance_loss: 0.1451, weighted_loss: 0.2342, label: 1, bag_size: 60\n",
      "batch 279, loss: 0.0077, instance_loss: 0.0188, weighted_loss: 0.0110, label: 1, bag_size: 33\n",
      "batch 299, loss: 0.0382, instance_loss: 0.2892, weighted_loss: 0.1135, label: 1, bag_size: 53\n",
      "batch 319, loss: 0.0006, instance_loss: 0.0410, weighted_loss: 0.0127, label: 0, bag_size: 95\n",
      "batch 339, loss: 0.0106, instance_loss: 0.0329, weighted_loss: 0.0173, label: 1, bag_size: 40\n",
      "batch 359, loss: 0.0037, instance_loss: 0.0256, weighted_loss: 0.0103, label: 0, bag_size: 54\n",
      "batch 379, loss: 0.3163, instance_loss: 0.2536, weighted_loss: 0.2975, label: 0, bag_size: 35\n",
      "batch 399, loss: 0.0157, instance_loss: 0.1541, weighted_loss: 0.0572, label: 1, bag_size: 86\n",
      "batch 419, loss: 0.4598, instance_loss: 0.5892, weighted_loss: 0.4986, label: 0, bag_size: 61\n",
      "batch 439, loss: 0.0966, instance_loss: 0.0489, weighted_loss: 0.0823, label: 1, bag_size: 49\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0042, weighted_loss: 0.0013, label: 0, bag_size: 93\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0943, weighted_loss: 0.0283, label: 0, bag_size: 89\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0264, weighted_loss: 0.0080, label: 0, bag_size: 49\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0123, weighted_loss: 0.0037, label: 0, bag_size: 49\n",
      "batch 539, loss: 0.0009, instance_loss: 0.0533, weighted_loss: 0.0166, label: 0, bag_size: 107\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0623, weighted_loss: 0.0188, label: 1, bag_size: 126\n",
      "batch 579, loss: 0.0081, instance_loss: 0.0214, weighted_loss: 0.0121, label: 0, bag_size: 37\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0085, weighted_loss: 0.0026, label: 0, bag_size: 73\n",
      "batch 619, loss: 0.1285, instance_loss: 0.1530, weighted_loss: 0.1359, label: 0, bag_size: 68\n",
      "batch 639, loss: 0.0135, instance_loss: 0.0326, weighted_loss: 0.0192, label: 0, bag_size: 83\n",
      "batch 659, loss: 0.0067, instance_loss: 0.1601, weighted_loss: 0.0527, label: 0, bag_size: 64\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9692234848484849: correct 10235/10560\n",
      "class 1 clustering acc 0.8242424242424242: correct 4352/5280\n",
      "Epoch: 25, train_loss: 0.2554, train_clustering_loss:  0.3461, train_error: 0.0924\n",
      "class 0: acc 0.9114285714285715, correct 319/350\n",
      "class 1: acc 0.9032258064516129, correct 280/310\n",
      "\n",
      "Val Set, val_loss: 0.7739, val_error: 0.2297, auc: 0.8794\n",
      "class 0 clustering acc 0.7989864864864865: correct 946/1184\n",
      "class 1 clustering acc 0.5287162162162162: correct 313/592\n",
      "class 0: acc 0.7941176470588235, correct 27/34\n",
      "class 1: acc 0.75, correct 30/40\n",
      "EarlyStopping counter: 15 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0009, instance_loss: 0.0129, weighted_loss: 0.0045, label: 0, bag_size: 28\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 0, bag_size: 37\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0770, weighted_loss: 0.0231, label: 1, bag_size: 86\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 60\n",
      "batch 99, loss: 0.0375, instance_loss: 0.0708, weighted_loss: 0.0475, label: 0, bag_size: 45\n",
      "batch 119, loss: 0.7428, instance_loss: 1.4787, weighted_loss: 0.9636, label: 1, bag_size: 100\n",
      "batch 139, loss: 0.0014, instance_loss: 0.0153, weighted_loss: 0.0055, label: 0, bag_size: 91\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0045, weighted_loss: 0.0013, label: 0, bag_size: 45\n",
      "batch 179, loss: 0.3843, instance_loss: 0.5865, weighted_loss: 0.4450, label: 1, bag_size: 81\n",
      "batch 199, loss: 0.1080, instance_loss: 0.2938, weighted_loss: 0.1637, label: 1, bag_size: 52\n",
      "batch 219, loss: 0.0295, instance_loss: 0.2629, weighted_loss: 0.0995, label: 0, bag_size: 67\n",
      "batch 239, loss: 0.0037, instance_loss: 0.0950, weighted_loss: 0.0311, label: 0, bag_size: 32\n",
      "batch 259, loss: 0.0020, instance_loss: 0.0351, weighted_loss: 0.0120, label: 1, bag_size: 31\n",
      "batch 279, loss: 0.0461, instance_loss: 0.9568, weighted_loss: 0.3193, label: 0, bag_size: 25\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0320, weighted_loss: 0.0096, label: 1, bag_size: 84\n",
      "batch 319, loss: 0.0005, instance_loss: 0.0032, weighted_loss: 0.0013, label: 0, bag_size: 90\n",
      "batch 339, loss: 11.3877, instance_loss: 1.0323, weighted_loss: 8.2811, label: 0, bag_size: 33\n",
      "batch 359, loss: 2.4359, instance_loss: 2.8566, weighted_loss: 2.5621, label: 0, bag_size: 20\n",
      "batch 379, loss: 0.0643, instance_loss: 0.3087, weighted_loss: 0.1376, label: 1, bag_size: 47\n",
      "batch 399, loss: 0.0199, instance_loss: 0.0408, weighted_loss: 0.0262, label: 1, bag_size: 68\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0166, weighted_loss: 0.0050, label: 0, bag_size: 67\n",
      "batch 439, loss: 0.6671, instance_loss: 0.7155, weighted_loss: 0.6816, label: 0, bag_size: 35\n",
      "batch 459, loss: 0.0002, instance_loss: 0.0352, weighted_loss: 0.0107, label: 0, bag_size: 25\n",
      "batch 479, loss: 0.0006, instance_loss: 0.0772, weighted_loss: 0.0236, label: 0, bag_size: 50\n",
      "batch 499, loss: 0.0002, instance_loss: 0.0113, weighted_loss: 0.0036, label: 0, bag_size: 35\n",
      "batch 519, loss: 0.0017, instance_loss: 0.0865, weighted_loss: 0.0271, label: 1, bag_size: 48\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0987, weighted_loss: 0.0297, label: 1, bag_size: 16\n",
      "batch 559, loss: 0.0070, instance_loss: 0.0236, weighted_loss: 0.0119, label: 0, bag_size: 76\n",
      "batch 579, loss: 0.0002, instance_loss: 0.0188, weighted_loss: 0.0058, label: 1, bag_size: 97\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0277, weighted_loss: 0.0083, label: 0, bag_size: 88\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0049, weighted_loss: 0.0015, label: 1, bag_size: 32\n",
      "batch 639, loss: 0.0535, instance_loss: 0.0713, weighted_loss: 0.0589, label: 0, bag_size: 50\n",
      "batch 659, loss: 0.0009, instance_loss: 0.3575, weighted_loss: 0.1079, label: 0, bag_size: 51\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9692234848484849: correct 10235/10560\n",
      "class 1 clustering acc 0.8206439393939394: correct 4333/5280\n",
      "Epoch: 26, train_loss: 0.3122, train_clustering_loss:  0.3276, train_error: 0.1015\n",
      "class 0: acc 0.8991097922848664, correct 303/337\n",
      "class 1: acc 0.8978328173374613, correct 290/323\n",
      "\n",
      "Val Set, val_loss: 0.9045, val_error: 0.2432, auc: 0.9029\n",
      "class 0 clustering acc 0.7711148648648649: correct 913/1184\n",
      "class 1 clustering acc 0.5591216216216216: correct 331/592\n",
      "class 0: acc 0.8529411764705882, correct 29/34\n",
      "class 1: acc 0.675, correct 27/40\n",
      "EarlyStopping counter: 16 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2146, instance_loss: 0.6052, weighted_loss: 0.3318, label: 1, bag_size: 40\n",
      "batch 39, loss: 0.0000, instance_loss: 0.1738, weighted_loss: 0.0521, label: 1, bag_size: 72\n",
      "batch 59, loss: 0.0156, instance_loss: 0.3081, weighted_loss: 0.1033, label: 1, bag_size: 60\n",
      "batch 79, loss: 0.0057, instance_loss: 0.0204, weighted_loss: 0.0101, label: 1, bag_size: 51\n",
      "batch 99, loss: 0.0035, instance_loss: 0.2067, weighted_loss: 0.0645, label: 1, bag_size: 84\n",
      "batch 119, loss: 0.0098, instance_loss: 0.4697, weighted_loss: 0.1478, label: 0, bag_size: 18\n",
      "batch 139, loss: 0.0005, instance_loss: 0.0130, weighted_loss: 0.0042, label: 0, bag_size: 39\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0034, weighted_loss: 0.0010, label: 1, bag_size: 52\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0018, weighted_loss: 0.0006, label: 1, bag_size: 25\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0207, weighted_loss: 0.0062, label: 1, bag_size: 62\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0060, weighted_loss: 0.0018, label: 0, bag_size: 67\n",
      "batch 239, loss: 0.0219, instance_loss: 0.1479, weighted_loss: 0.0597, label: 1, bag_size: 76\n",
      "batch 259, loss: 0.0000, instance_loss: 0.4508, weighted_loss: 0.1353, label: 1, bag_size: 24\n",
      "batch 279, loss: 0.0000, instance_loss: 0.2858, weighted_loss: 0.0857, label: 0, bag_size: 38\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0295, weighted_loss: 0.0089, label: 1, bag_size: 107\n",
      "batch 319, loss: 0.0031, instance_loss: 0.5099, weighted_loss: 0.1552, label: 0, bag_size: 37\n",
      "batch 339, loss: 2.0013, instance_loss: 1.0577, weighted_loss: 1.7182, label: 0, bag_size: 52\n",
      "batch 359, loss: 0.0630, instance_loss: 0.0513, weighted_loss: 0.0595, label: 0, bag_size: 63\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0098, weighted_loss: 0.0030, label: 0, bag_size: 89\n",
      "batch 399, loss: 0.0266, instance_loss: 0.5649, weighted_loss: 0.1881, label: 1, bag_size: 91\n",
      "batch 419, loss: 0.8118, instance_loss: 0.1861, weighted_loss: 0.6241, label: 0, bag_size: 30\n",
      "batch 439, loss: 0.0004, instance_loss: 0.4086, weighted_loss: 0.1228, label: 1, bag_size: 42\n",
      "batch 459, loss: 0.0534, instance_loss: 1.3876, weighted_loss: 0.4537, label: 1, bag_size: 91\n",
      "batch 479, loss: 0.4376, instance_loss: 0.5579, weighted_loss: 0.4737, label: 0, bag_size: 44\n",
      "batch 499, loss: 0.0045, instance_loss: 0.0396, weighted_loss: 0.0150, label: 1, bag_size: 64\n",
      "batch 519, loss: 0.0002, instance_loss: 0.0354, weighted_loss: 0.0108, label: 1, bag_size: 108\n",
      "batch 539, loss: 0.0050, instance_loss: 0.0944, weighted_loss: 0.0318, label: 1, bag_size: 54\n",
      "batch 559, loss: 0.0078, instance_loss: 0.4085, weighted_loss: 0.1281, label: 1, bag_size: 112\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0468, weighted_loss: 0.0140, label: 1, bag_size: 33\n",
      "batch 599, loss: 0.0126, instance_loss: 0.0728, weighted_loss: 0.0307, label: 0, bag_size: 86\n",
      "batch 619, loss: 0.0007, instance_loss: 0.0597, weighted_loss: 0.0184, label: 0, bag_size: 38\n",
      "batch 639, loss: 0.0002, instance_loss: 0.0181, weighted_loss: 0.0056, label: 1, bag_size: 100\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0331, weighted_loss: 0.0099, label: 1, bag_size: 92\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9655303030303031: correct 10196/10560\n",
      "class 1 clustering acc 0.7854166666666667: correct 4147/5280\n",
      "Epoch: 27, train_loss: 0.4676, train_clustering_loss:  0.3906, train_error: 0.1242\n",
      "class 0: acc 0.8776119402985074, correct 294/335\n",
      "class 1: acc 0.8738461538461538, correct 284/325\n",
      "\n",
      "Val Set, val_loss: 0.5810, val_error: 0.1622, auc: 0.9213\n",
      "class 0 clustering acc 0.7947635135135135: correct 941/1184\n",
      "class 1 clustering acc 0.7331081081081081: correct 434/592\n",
      "class 0: acc 0.7647058823529411, correct 26/34\n",
      "class 1: acc 0.9, correct 36/40\n",
      "EarlyStopping counter: 17 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0182, instance_loss: 0.0170, weighted_loss: 0.0178, label: 0, bag_size: 69\n",
      "batch 39, loss: 0.4732, instance_loss: 0.2942, weighted_loss: 0.4195, label: 1, bag_size: 30\n",
      "batch 59, loss: 0.0583, instance_loss: 0.1474, weighted_loss: 0.0851, label: 0, bag_size: 51\n",
      "batch 79, loss: 0.0002, instance_loss: 0.0034, weighted_loss: 0.0011, label: 0, bag_size: 35\n",
      "batch 99, loss: 0.0222, instance_loss: 0.0606, weighted_loss: 0.0337, label: 1, bag_size: 31\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0303, weighted_loss: 0.0091, label: 1, bag_size: 91\n",
      "batch 139, loss: 3.2400, instance_loss: 2.1597, weighted_loss: 2.9159, label: 1, bag_size: 31\n",
      "batch 159, loss: 0.0031, instance_loss: 0.0075, weighted_loss: 0.0044, label: 0, bag_size: 73\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0276, weighted_loss: 0.0083, label: 1, bag_size: 26\n",
      "batch 199, loss: 0.3123, instance_loss: 0.1471, weighted_loss: 0.2627, label: 1, bag_size: 77\n",
      "batch 219, loss: 0.0093, instance_loss: 0.0295, weighted_loss: 0.0153, label: 0, bag_size: 89\n",
      "batch 239, loss: 0.0019, instance_loss: 0.0123, weighted_loss: 0.0050, label: 1, bag_size: 120\n",
      "batch 259, loss: 0.0049, instance_loss: 0.1194, weighted_loss: 0.0392, label: 1, bag_size: 45\n",
      "batch 279, loss: 0.0204, instance_loss: 0.0823, weighted_loss: 0.0390, label: 1, bag_size: 23\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0346, weighted_loss: 0.0105, label: 0, bag_size: 67\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0098, weighted_loss: 0.0030, label: 0, bag_size: 118\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0080, weighted_loss: 0.0025, label: 1, bag_size: 94\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0005, label: 0, bag_size: 93\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0107, weighted_loss: 0.0032, label: 1, bag_size: 21\n",
      "batch 399, loss: 0.0398, instance_loss: 0.0244, weighted_loss: 0.0352, label: 0, bag_size: 35\n",
      "batch 419, loss: 0.2858, instance_loss: 0.3621, weighted_loss: 0.3087, label: 1, bag_size: 50\n",
      "batch 439, loss: 0.1746, instance_loss: 0.4139, weighted_loss: 0.2464, label: 0, bag_size: 50\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0005, label: 0, bag_size: 93\n",
      "batch 479, loss: 0.2434, instance_loss: 0.9697, weighted_loss: 0.4613, label: 1, bag_size: 38\n",
      "batch 499, loss: 0.0384, instance_loss: 0.0653, weighted_loss: 0.0465, label: 1, bag_size: 20\n",
      "batch 519, loss: 0.0084, instance_loss: 0.1188, weighted_loss: 0.0416, label: 1, bag_size: 43\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 1, bag_size: 73\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0109, weighted_loss: 0.0033, label: 1, bag_size: 57\n",
      "batch 579, loss: 0.0394, instance_loss: 0.0844, weighted_loss: 0.0529, label: 0, bag_size: 18\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0027, weighted_loss: 0.0008, label: 1, bag_size: 20\n",
      "batch 619, loss: 0.1036, instance_loss: 0.0282, weighted_loss: 0.0810, label: 0, bag_size: 40\n",
      "batch 639, loss: 0.0571, instance_loss: 0.3849, weighted_loss: 0.1554, label: 0, bag_size: 43\n",
      "batch 659, loss: 0.0056, instance_loss: 0.2537, weighted_loss: 0.0800, label: 0, bag_size: 72\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9730113636363636: correct 10275/10560\n",
      "class 1 clustering acc 0.8615530303030303: correct 4549/5280\n",
      "Epoch: 28, train_loss: 0.2440, train_clustering_loss:  0.2851, train_error: 0.0788\n",
      "class 0: acc 0.9136904761904762, correct 307/336\n",
      "class 1: acc 0.9290123456790124, correct 301/324\n",
      "\n",
      "Val Set, val_loss: 0.6556, val_error: 0.2432, auc: 0.8926\n",
      "class 0 clustering acc 0.7643581081081081: correct 905/1184\n",
      "class 1 clustering acc 0.6790540540540541: correct 402/592\n",
      "class 0: acc 0.7941176470588235, correct 27/34\n",
      "class 1: acc 0.725, correct 29/40\n",
      "EarlyStopping counter: 18 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0156, instance_loss: 0.0106, weighted_loss: 0.0141, label: 0, bag_size: 50\n",
      "batch 39, loss: 0.0002, instance_loss: 0.0129, weighted_loss: 0.0040, label: 0, bag_size: 12\n",
      "batch 59, loss: 0.0001, instance_loss: 0.0036, weighted_loss: 0.0012, label: 1, bag_size: 32\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0223, weighted_loss: 0.0067, label: 0, bag_size: 115\n",
      "batch 99, loss: 1.3972, instance_loss: 0.3642, weighted_loss: 1.0873, label: 1, bag_size: 44\n",
      "batch 119, loss: 0.0006, instance_loss: 0.0158, weighted_loss: 0.0051, label: 1, bag_size: 95\n",
      "batch 139, loss: 0.0489, instance_loss: 0.4310, weighted_loss: 0.1636, label: 1, bag_size: 20\n",
      "batch 159, loss: 0.6675, instance_loss: 0.9967, weighted_loss: 0.7663, label: 0, bag_size: 88\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0093, weighted_loss: 0.0028, label: 0, bag_size: 78\n",
      "batch 199, loss: 0.0320, instance_loss: 0.0664, weighted_loss: 0.0423, label: 0, bag_size: 28\n",
      "batch 219, loss: 0.2957, instance_loss: 0.8581, weighted_loss: 0.4644, label: 1, bag_size: 28\n",
      "batch 239, loss: 0.2603, instance_loss: 0.1903, weighted_loss: 0.2393, label: 1, bag_size: 67\n",
      "batch 259, loss: 0.0395, instance_loss: 0.0253, weighted_loss: 0.0353, label: 0, bag_size: 45\n",
      "batch 279, loss: 0.0009, instance_loss: 0.0087, weighted_loss: 0.0032, label: 0, bag_size: 50\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0168, weighted_loss: 0.0050, label: 0, bag_size: 85\n",
      "batch 319, loss: 0.0066, instance_loss: 0.2362, weighted_loss: 0.0755, label: 0, bag_size: 61\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 103\n",
      "batch 359, loss: 0.0055, instance_loss: 0.0197, weighted_loss: 0.0098, label: 0, bag_size: 78\n",
      "batch 379, loss: 0.0004, instance_loss: 0.0233, weighted_loss: 0.0073, label: 0, bag_size: 78\n",
      "batch 399, loss: 0.0001, instance_loss: 0.0034, weighted_loss: 0.0011, label: 1, bag_size: 80\n",
      "batch 419, loss: 0.0031, instance_loss: 0.0072, weighted_loss: 0.0044, label: 1, bag_size: 38\n",
      "batch 439, loss: 7.7212, instance_loss: 1.5626, weighted_loss: 5.8737, label: 1, bag_size: 43\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0095, weighted_loss: 0.0028, label: 0, bag_size: 51\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0338, weighted_loss: 0.0102, label: 1, bag_size: 120\n",
      "batch 499, loss: 0.0000, instance_loss: 0.4277, weighted_loss: 0.1283, label: 0, bag_size: 102\n",
      "batch 519, loss: 0.0000, instance_loss: 0.4021, weighted_loss: 0.1206, label: 0, bag_size: 30\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0837, weighted_loss: 0.0251, label: 0, bag_size: 71\n",
      "batch 559, loss: 0.0003, instance_loss: 1.5347, weighted_loss: 0.4606, label: 0, bag_size: 35\n",
      "batch 579, loss: 0.0000, instance_loss: 1.4422, weighted_loss: 0.4327, label: 0, bag_size: 66\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0606, weighted_loss: 0.0182, label: 0, bag_size: 92\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0653, weighted_loss: 0.0196, label: 1, bag_size: 40\n",
      "batch 639, loss: 48.1749, instance_loss: 5.2135, weighted_loss: 35.2865, label: 0, bag_size: 28\n",
      "batch 659, loss: 17.5613, instance_loss: 2.2100, weighted_loss: 12.9559, label: 1, bag_size: 67\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9632575757575758: correct 10172/10560\n",
      "class 1 clustering acc 0.8329545454545455: correct 4398/5280\n",
      "Epoch: 29, train_loss: 0.9348, train_clustering_loss:  0.3554, train_error: 0.1000\n",
      "class 0: acc 0.8942598187311178, correct 296/331\n",
      "class 1: acc 0.9057750759878419, correct 298/329\n",
      "\n",
      "Val Set, val_loss: 4.2682, val_error: 0.3649, auc: 0.9257\n",
      "class 0 clustering acc 0.8353040540540541: correct 989/1184\n",
      "class 1 clustering acc 0.6097972972972973: correct 361/592\n",
      "class 0: acc 0.9705882352941176, correct 33/34\n",
      "class 1: acc 0.35, correct 14/40\n",
      "EarlyStopping counter: 19 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0842, instance_loss: 0.0294, weighted_loss: 0.0678, label: 0, bag_size: 94\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0288, weighted_loss: 0.0086, label: 0, bag_size: 106\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0965, weighted_loss: 0.0290, label: 0, bag_size: 98\n",
      "batch 79, loss: 0.0000, instance_loss: 0.1319, weighted_loss: 0.0396, label: 1, bag_size: 20\n",
      "batch 99, loss: 6.3077, instance_loss: 0.5454, weighted_loss: 4.5790, label: 0, bag_size: 41\n",
      "batch 119, loss: 1.1007, instance_loss: 0.1919, weighted_loss: 0.8281, label: 1, bag_size: 21\n",
      "batch 139, loss: 0.0000, instance_loss: 0.2405, weighted_loss: 0.0721, label: 0, bag_size: 25\n",
      "batch 159, loss: 0.0007, instance_loss: 1.4701, weighted_loss: 0.4415, label: 0, bag_size: 14\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0223, weighted_loss: 0.0068, label: 0, bag_size: 40\n",
      "batch 199, loss: 0.0007, instance_loss: 0.2895, weighted_loss: 0.0874, label: 0, bag_size: 126\n",
      "batch 219, loss: 1.3565, instance_loss: 0.2654, weighted_loss: 1.0292, label: 1, bag_size: 83\n",
      "batch 239, loss: 0.0010, instance_loss: 0.7144, weighted_loss: 0.2150, label: 1, bag_size: 47\n",
      "batch 259, loss: 0.0917, instance_loss: 0.5545, weighted_loss: 0.2305, label: 0, bag_size: 50\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0177, weighted_loss: 0.0053, label: 1, bag_size: 108\n",
      "batch 299, loss: 0.0012, instance_loss: 0.0348, weighted_loss: 0.0113, label: 1, bag_size: 100\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0019, weighted_loss: 0.0006, label: 1, bag_size: 40\n",
      "batch 339, loss: 0.0087, instance_loss: 0.0204, weighted_loss: 0.0122, label: 1, bag_size: 65\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0093, weighted_loss: 0.0028, label: 1, bag_size: 136\n",
      "batch 379, loss: 0.0514, instance_loss: 0.2305, weighted_loss: 0.1051, label: 0, bag_size: 61\n",
      "batch 399, loss: 0.0055, instance_loss: 0.0251, weighted_loss: 0.0114, label: 0, bag_size: 73\n",
      "batch 419, loss: 2.3359, instance_loss: 0.6625, weighted_loss: 1.8339, label: 1, bag_size: 63\n",
      "batch 439, loss: 0.0009, instance_loss: 0.0220, weighted_loss: 0.0072, label: 1, bag_size: 59\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0075, weighted_loss: 0.0023, label: 1, bag_size: 69\n",
      "batch 479, loss: 0.0940, instance_loss: 0.0635, weighted_loss: 0.0849, label: 0, bag_size: 35\n",
      "batch 499, loss: 0.0022, instance_loss: 0.0208, weighted_loss: 0.0078, label: 0, bag_size: 66\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0046, weighted_loss: 0.0014, label: 0, bag_size: 33\n",
      "batch 539, loss: 0.0707, instance_loss: 0.0412, weighted_loss: 0.0618, label: 0, bag_size: 103\n",
      "batch 559, loss: 0.0060, instance_loss: 0.0607, weighted_loss: 0.0224, label: 0, bag_size: 64\n",
      "batch 579, loss: 0.0002, instance_loss: 0.0061, weighted_loss: 0.0019, label: 1, bag_size: 77\n",
      "batch 599, loss: 0.1549, instance_loss: 0.7654, weighted_loss: 0.3380, label: 1, bag_size: 59\n",
      "batch 619, loss: 0.0024, instance_loss: 0.0055, weighted_loss: 0.0033, label: 1, bag_size: 44\n",
      "batch 639, loss: 0.1243, instance_loss: 0.3488, weighted_loss: 0.1916, label: 0, bag_size: 78\n",
      "batch 659, loss: 0.0008, instance_loss: 0.0043, weighted_loss: 0.0018, label: 0, bag_size: 61\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9640151515151515: correct 10180/10560\n",
      "class 1 clustering acc 0.8308712121212121: correct 4387/5280\n",
      "Epoch: 30, train_loss: 0.6123, train_clustering_loss:  0.3464, train_error: 0.1121\n",
      "class 0: acc 0.8858024691358025, correct 287/324\n",
      "class 1: acc 0.8898809523809523, correct 299/336\n",
      "\n",
      "Val Set, val_loss: 0.8210, val_error: 0.2162, auc: 0.9044\n",
      "class 0 clustering acc 0.950168918918919: correct 1125/1184\n",
      "class 1 clustering acc 0.6993243243243243: correct 414/592\n",
      "class 0: acc 0.9117647058823529, correct 31/34\n",
      "class 1: acc 0.675, correct 27/40\n",
      "EarlyStopping counter: 20 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0102, instance_loss: 0.0697, weighted_loss: 0.0280, label: 1, bag_size: 14\n",
      "batch 39, loss: 0.0004, instance_loss: 0.0342, weighted_loss: 0.0105, label: 1, bag_size: 91\n",
      "batch 59, loss: 3.1392, instance_loss: 0.0743, weighted_loss: 2.2197, label: 1, bag_size: 65\n",
      "batch 79, loss: 0.0001, instance_loss: 0.0022, weighted_loss: 0.0007, label: 0, bag_size: 35\n",
      "batch 99, loss: 0.0002, instance_loss: 0.0491, weighted_loss: 0.0149, label: 0, bag_size: 66\n",
      "batch 119, loss: 0.0003, instance_loss: 0.0044, weighted_loss: 0.0015, label: 1, bag_size: 85\n",
      "batch 139, loss: 0.0000, instance_loss: 0.1710, weighted_loss: 0.0513, label: 0, bag_size: 27\n",
      "batch 159, loss: 3.1327, instance_loss: 1.7018, weighted_loss: 2.7034, label: 0, bag_size: 54\n",
      "batch 179, loss: 0.0003, instance_loss: 0.0578, weighted_loss: 0.0175, label: 1, bag_size: 89\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0273, weighted_loss: 0.0082, label: 0, bag_size: 78\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 118\n",
      "batch 239, loss: 0.0189, instance_loss: 0.0730, weighted_loss: 0.0352, label: 1, bag_size: 84\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0023, weighted_loss: 0.0007, label: 0, bag_size: 101\n",
      "batch 279, loss: 1.4491, instance_loss: 0.2800, weighted_loss: 1.0984, label: 1, bag_size: 82\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 114\n",
      "batch 319, loss: 0.0424, instance_loss: 0.1480, weighted_loss: 0.0741, label: 1, bag_size: 123\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0024, weighted_loss: 0.0007, label: 0, bag_size: 25\n",
      "batch 359, loss: 0.0394, instance_loss: 0.0145, weighted_loss: 0.0319, label: 1, bag_size: 53\n",
      "batch 379, loss: 0.0023, instance_loss: 0.0120, weighted_loss: 0.0052, label: 1, bag_size: 89\n",
      "batch 399, loss: 0.4686, instance_loss: 0.1004, weighted_loss: 0.3581, label: 1, bag_size: 82\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 62\n",
      "batch 439, loss: 0.0003, instance_loss: 0.0029, weighted_loss: 0.0011, label: 0, bag_size: 42\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0121, weighted_loss: 0.0037, label: 1, bag_size: 14\n",
      "batch 479, loss: 0.6235, instance_loss: 0.2305, weighted_loss: 0.5056, label: 1, bag_size: 76\n",
      "batch 499, loss: 0.0002, instance_loss: 0.0001, weighted_loss: 0.0002, label: 0, bag_size: 30\n",
      "batch 519, loss: 0.3329, instance_loss: 0.6138, weighted_loss: 0.4172, label: 1, bag_size: 96\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0024, weighted_loss: 0.0007, label: 0, bag_size: 83\n",
      "batch 559, loss: 0.0002, instance_loss: 0.0151, weighted_loss: 0.0046, label: 0, bag_size: 54\n",
      "batch 579, loss: 0.0180, instance_loss: 0.1061, weighted_loss: 0.0444, label: 1, bag_size: 60\n",
      "batch 599, loss: 0.0003, instance_loss: 0.0060, weighted_loss: 0.0020, label: 1, bag_size: 70\n",
      "batch 619, loss: 0.0281, instance_loss: 0.6524, weighted_loss: 0.2154, label: 1, bag_size: 41\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 1, bag_size: 107\n",
      "batch 659, loss: 0.0145, instance_loss: 0.0054, weighted_loss: 0.0117, label: 0, bag_size: 29\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9651515151515152: correct 10192/10560\n",
      "class 1 clustering acc 0.8378787878787879: correct 4424/5280\n",
      "Epoch: 31, train_loss: 0.4335, train_clustering_loss:  0.3282, train_error: 0.1379\n",
      "class 0: acc 0.8629737609329446, correct 296/343\n",
      "class 1: acc 0.861198738170347, correct 273/317\n",
      "\n",
      "Val Set, val_loss: 0.6991, val_error: 0.2027, auc: 0.9096\n",
      "class 0 clustering acc 0.9341216216216216: correct 1106/1184\n",
      "class 1 clustering acc 0.7668918918918919: correct 454/592\n",
      "class 0: acc 0.6470588235294118, correct 22/34\n",
      "class 1: acc 0.925, correct 37/40\n",
      "EarlyStopping counter: 21 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.0607, instance_loss: 0.4783, weighted_loss: 0.8860, label: 1, bag_size: 38\n",
      "batch 39, loss: 1.2893, instance_loss: 0.3035, weighted_loss: 0.9935, label: 0, bag_size: 29\n",
      "batch 59, loss: 0.0001, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 84\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 85\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0068, weighted_loss: 0.0021, label: 0, bag_size: 75\n",
      "batch 119, loss: 2.2205, instance_loss: 2.1131, weighted_loss: 2.1883, label: 0, bag_size: 89\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 35\n",
      "batch 159, loss: 0.0326, instance_loss: 0.0226, weighted_loss: 0.0296, label: 1, bag_size: 30\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 0, bag_size: 78\n",
      "batch 199, loss: 1.5662, instance_loss: 3.0752, weighted_loss: 2.0189, label: 1, bag_size: 24\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0346, weighted_loss: 0.0104, label: 1, bag_size: 40\n",
      "batch 239, loss: 0.0041, instance_loss: 0.4676, weighted_loss: 0.1431, label: 1, bag_size: 51\n",
      "batch 259, loss: 2.2168, instance_loss: 0.0543, weighted_loss: 1.5680, label: 1, bag_size: 31\n",
      "batch 279, loss: 3.2163, instance_loss: 1.3117, weighted_loss: 2.6449, label: 1, bag_size: 25\n",
      "batch 299, loss: 0.0902, instance_loss: 0.0353, weighted_loss: 0.0738, label: 0, bag_size: 73\n",
      "batch 319, loss: 0.0065, instance_loss: 0.0645, weighted_loss: 0.0239, label: 0, bag_size: 21\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 0, bag_size: 13\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0057, weighted_loss: 0.0017, label: 1, bag_size: 89\n",
      "batch 379, loss: 0.0001, instance_loss: 0.0812, weighted_loss: 0.0244, label: 0, bag_size: 31\n",
      "batch 399, loss: 0.0241, instance_loss: 0.4332, weighted_loss: 0.1468, label: 0, bag_size: 110\n",
      "batch 419, loss: 0.0028, instance_loss: 0.1257, weighted_loss: 0.0397, label: 1, bag_size: 65\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 77\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 80\n",
      "batch 479, loss: 0.0045, instance_loss: 0.0011, weighted_loss: 0.0035, label: 1, bag_size: 42\n",
      "batch 499, loss: 3.1144, instance_loss: 1.5561, weighted_loss: 2.6469, label: 1, bag_size: 102\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 63\n",
      "batch 539, loss: 0.0140, instance_loss: 0.0790, weighted_loss: 0.0335, label: 0, bag_size: 23\n",
      "batch 559, loss: 0.0051, instance_loss: 0.0435, weighted_loss: 0.0166, label: 1, bag_size: 90\n",
      "batch 579, loss: 0.0187, instance_loss: 0.0226, weighted_loss: 0.0199, label: 1, bag_size: 14\n",
      "batch 599, loss: 0.0056, instance_loss: 0.0130, weighted_loss: 0.0078, label: 1, bag_size: 51\n",
      "batch 619, loss: 0.0000, instance_loss: 1.8845, weighted_loss: 0.5653, label: 0, bag_size: 88\n",
      "batch 639, loss: 0.0000, instance_loss: 0.1552, weighted_loss: 0.0466, label: 1, bag_size: 57\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0071, weighted_loss: 0.0021, label: 1, bag_size: 58\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.962594696969697: correct 10165/10560\n",
      "class 1 clustering acc 0.8261363636363637: correct 4362/5280\n",
      "Epoch: 32, train_loss: 0.4427, train_clustering_loss:  0.3551, train_error: 0.1258\n",
      "class 0: acc 0.8553459119496856, correct 272/318\n",
      "class 1: acc 0.8918128654970761, correct 305/342\n",
      "\n",
      "Val Set, val_loss: 2.3071, val_error: 0.2973, auc: 0.9107\n",
      "class 0 clustering acc 0.9400337837837838: correct 1113/1184\n",
      "class 1 clustering acc 0.6773648648648649: correct 401/592\n",
      "class 0: acc 0.35294117647058826, correct 12/34\n",
      "class 1: acc 1.0, correct 40/40\n",
      "EarlyStopping counter: 22 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 1.4272, weighted_loss: 0.4282, label: 0, bag_size: 60\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0540, weighted_loss: 0.0162, label: 1, bag_size: 112\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0054, weighted_loss: 0.0016, label: 1, bag_size: 45\n",
      "batch 79, loss: 7.0665, instance_loss: 0.1101, weighted_loss: 4.9795, label: 1, bag_size: 53\n",
      "batch 99, loss: 0.0058, instance_loss: 0.2042, weighted_loss: 0.0653, label: 1, bag_size: 123\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0349, weighted_loss: 0.0105, label: 1, bag_size: 32\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0333, weighted_loss: 0.0100, label: 1, bag_size: 71\n",
      "batch 159, loss: 0.0135, instance_loss: 0.1166, weighted_loss: 0.0444, label: 1, bag_size: 39\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 70\n",
      "batch 199, loss: 0.4503, instance_loss: 1.0618, weighted_loss: 0.6338, label: 0, bag_size: 45\n",
      "batch 219, loss: 2.8146, instance_loss: 1.1586, weighted_loss: 2.3178, label: 1, bag_size: 67\n",
      "batch 239, loss: 0.0002, instance_loss: 0.2376, weighted_loss: 0.0714, label: 0, bag_size: 69\n",
      "batch 259, loss: 0.0000, instance_loss: 0.1455, weighted_loss: 0.0436, label: 1, bag_size: 41\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 67\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0456, weighted_loss: 0.0137, label: 0, bag_size: 55\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0021, weighted_loss: 0.0006, label: 0, bag_size: 103\n",
      "batch 339, loss: 0.0040, instance_loss: 0.0791, weighted_loss: 0.0266, label: 0, bag_size: 98\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 1, bag_size: 61\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0487, weighted_loss: 0.0146, label: 1, bag_size: 112\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0069, weighted_loss: 0.0021, label: 1, bag_size: 81\n",
      "batch 419, loss: 0.0346, instance_loss: 0.0064, weighted_loss: 0.0262, label: 1, bag_size: 47\n",
      "batch 439, loss: 0.0023, instance_loss: 0.0060, weighted_loss: 0.0034, label: 1, bag_size: 49\n",
      "batch 459, loss: 0.0097, instance_loss: 0.0494, weighted_loss: 0.0216, label: 1, bag_size: 46\n",
      "batch 479, loss: 0.0064, instance_loss: 0.0475, weighted_loss: 0.0187, label: 0, bag_size: 31\n",
      "batch 499, loss: 2.4041, instance_loss: 0.1662, weighted_loss: 1.7328, label: 0, bag_size: 78\n",
      "batch 519, loss: 21.9753, instance_loss: 6.6574, weighted_loss: 17.3800, label: 0, bag_size: 35\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0035, weighted_loss: 0.0012, label: 1, bag_size: 30\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 21\n",
      "batch 579, loss: 0.0017, instance_loss: 0.0536, weighted_loss: 0.0173, label: 1, bag_size: 62\n",
      "batch 599, loss: 0.2213, instance_loss: 0.3576, weighted_loss: 0.2622, label: 0, bag_size: 37\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0210, weighted_loss: 0.0063, label: 0, bag_size: 65\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0181, weighted_loss: 0.0054, label: 0, bag_size: 45\n",
      "batch 659, loss: 0.0008, instance_loss: 0.0003, weighted_loss: 0.0007, label: 0, bag_size: 33\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9652462121212121: correct 10193/10560\n",
      "class 1 clustering acc 0.8409090909090909: correct 4440/5280\n",
      "Epoch: 33, train_loss: 0.5853, train_clustering_loss:  0.3324, train_error: 0.1227\n",
      "class 0: acc 0.8768768768768769, correct 292/333\n",
      "class 1: acc 0.8776758409785933, correct 287/327\n",
      "\n",
      "Val Set, val_loss: 1.4881, val_error: 0.2027, auc: 0.8809\n",
      "class 0 clustering acc 0.9265202702702703: correct 1097/1184\n",
      "class 1 clustering acc 0.6891891891891891: correct 408/592\n",
      "class 0: acc 0.6764705882352942, correct 23/34\n",
      "class 1: acc 0.9, correct 36/40\n",
      "EarlyStopping counter: 23 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0068, weighted_loss: 0.0021, label: 1, bag_size: 86\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 1, bag_size: 107\n",
      "batch 59, loss: 0.0010, instance_loss: 0.0130, weighted_loss: 0.0046, label: 1, bag_size: 39\n",
      "batch 79, loss: 0.0025, instance_loss: 0.0035, weighted_loss: 0.0028, label: 0, bag_size: 94\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 116\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 29\n",
      "batch 139, loss: 0.1775, instance_loss: 0.2376, weighted_loss: 0.1955, label: 1, bag_size: 62\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0022, weighted_loss: 0.0007, label: 1, bag_size: 34\n",
      "batch 179, loss: 0.5479, instance_loss: 1.1954, weighted_loss: 0.7422, label: 0, bag_size: 110\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0410, weighted_loss: 0.0123, label: 0, bag_size: 25\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0016, weighted_loss: 0.0005, label: 0, bag_size: 64\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 94\n",
      "batch 259, loss: 1.2063, instance_loss: 0.4700, weighted_loss: 0.9854, label: 1, bag_size: 89\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0029, weighted_loss: 0.0009, label: 1, bag_size: 32\n",
      "batch 299, loss: 0.9373, instance_loss: 0.1711, weighted_loss: 0.7075, label: 1, bag_size: 77\n",
      "batch 319, loss: 2.1841, instance_loss: 0.9443, weighted_loss: 1.8122, label: 1, bag_size: 83\n",
      "batch 339, loss: 0.9813, instance_loss: 1.0771, weighted_loss: 1.0100, label: 0, bag_size: 103\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 98\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0101, weighted_loss: 0.0030, label: 0, bag_size: 88\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 91\n",
      "batch 419, loss: 0.0621, instance_loss: 0.5650, weighted_loss: 0.2130, label: 1, bag_size: 67\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 56\n",
      "batch 459, loss: 6.1787, instance_loss: 0.3114, weighted_loss: 4.4185, label: 0, bag_size: 18\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 37\n",
      "batch 499, loss: 0.0065, instance_loss: 0.1978, weighted_loss: 0.0639, label: 1, bag_size: 53\n",
      "batch 519, loss: 0.0001, instance_loss: 1.1565, weighted_loss: 0.3470, label: 1, bag_size: 21\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0088, weighted_loss: 0.0027, label: 0, bag_size: 63\n",
      "batch 559, loss: 0.5644, instance_loss: 1.2560, weighted_loss: 0.7719, label: 1, bag_size: 53\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 1, bag_size: 45\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0072, weighted_loss: 0.0021, label: 1, bag_size: 95\n",
      "batch 619, loss: 0.0019, instance_loss: 0.2515, weighted_loss: 0.0768, label: 1, bag_size: 94\n",
      "batch 639, loss: 0.0000, instance_loss: 0.1252, weighted_loss: 0.0376, label: 1, bag_size: 56\n",
      "batch 659, loss: 0.0000, instance_loss: 0.8604, weighted_loss: 0.2581, label: 0, bag_size: 39\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.96875: correct 10230/10560\n",
      "class 1 clustering acc 0.8443181818181819: correct 4458/5280\n",
      "Epoch: 34, train_loss: 0.6461, train_clustering_loss:  0.3135, train_error: 0.1227\n",
      "class 0: acc 0.8717201166180758, correct 299/343\n",
      "class 1: acc 0.8832807570977917, correct 280/317\n",
      "\n",
      "Val Set, val_loss: 2.4705, val_error: 0.2568, auc: 0.8971\n",
      "class 0 clustering acc 0.8826013513513513: correct 1045/1184\n",
      "class 1 clustering acc 0.6993243243243243: correct 414/592\n",
      "class 0: acc 0.8235294117647058, correct 28/34\n",
      "class 1: acc 0.675, correct 27/40\n",
      "EarlyStopping counter: 24 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0235, instance_loss: 0.0074, weighted_loss: 0.0187, label: 0, bag_size: 41\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0055, weighted_loss: 0.0016, label: 1, bag_size: 80\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0072, weighted_loss: 0.0022, label: 0, bag_size: 115\n",
      "batch 79, loss: 0.1006, instance_loss: 1.4787, weighted_loss: 0.5140, label: 1, bag_size: 23\n",
      "batch 99, loss: 0.0000, instance_loss: 0.1132, weighted_loss: 0.0340, label: 0, bag_size: 71\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0326, weighted_loss: 0.0098, label: 1, bag_size: 78\n",
      "batch 139, loss: 0.0008, instance_loss: 0.1654, weighted_loss: 0.0502, label: 0, bag_size: 28\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0005, label: 0, bag_size: 73\n",
      "batch 179, loss: 0.0055, instance_loss: 0.1570, weighted_loss: 0.0509, label: 1, bag_size: 94\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0060, weighted_loss: 0.0018, label: 0, bag_size: 88\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0030, weighted_loss: 0.0009, label: 0, bag_size: 164\n",
      "batch 239, loss: 0.0011, instance_loss: 0.0061, weighted_loss: 0.0026, label: 1, bag_size: 39\n",
      "batch 259, loss: 0.0004, instance_loss: 0.0900, weighted_loss: 0.0273, label: 1, bag_size: 83\n",
      "batch 279, loss: 0.0736, instance_loss: 0.0954, weighted_loss: 0.0802, label: 0, bag_size: 73\n",
      "batch 299, loss: 0.0138, instance_loss: 0.0022, weighted_loss: 0.0103, label: 0, bag_size: 70\n",
      "batch 319, loss: 7.5190, instance_loss: 3.1188, weighted_loss: 6.1990, label: 0, bag_size: 14\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0099, weighted_loss: 0.0030, label: 0, bag_size: 84\n",
      "batch 359, loss: 0.0035, instance_loss: 1.3761, weighted_loss: 0.4153, label: 0, bag_size: 27\n",
      "batch 379, loss: 1.1759, instance_loss: 0.6608, weighted_loss: 1.0214, label: 1, bag_size: 67\n",
      "batch 399, loss: 2.0733, instance_loss: 1.3433, weighted_loss: 1.8543, label: 0, bag_size: 81\n",
      "batch 419, loss: 0.1138, instance_loss: 0.0801, weighted_loss: 0.1037, label: 1, bag_size: 46\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0056, weighted_loss: 0.0017, label: 0, bag_size: 49\n",
      "batch 459, loss: 0.0162, instance_loss: 0.0015, weighted_loss: 0.0118, label: 1, bag_size: 80\n",
      "batch 479, loss: 0.0012, instance_loss: 0.0209, weighted_loss: 0.0071, label: 1, bag_size: 77\n",
      "batch 499, loss: 0.7158, instance_loss: 1.1283, weighted_loss: 0.8395, label: 0, bag_size: 18\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0027, weighted_loss: 0.0008, label: 1, bag_size: 48\n",
      "batch 539, loss: 0.0156, instance_loss: 0.1811, weighted_loss: 0.0653, label: 0, bag_size: 44\n",
      "batch 559, loss: 0.0108, instance_loss: 0.0135, weighted_loss: 0.0116, label: 0, bag_size: 51\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 1, bag_size: 25\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0005, label: 0, bag_size: 86\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0106, weighted_loss: 0.0032, label: 0, bag_size: 87\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 103\n",
      "batch 659, loss: 0.0001, instance_loss: 0.0009, weighted_loss: 0.0004, label: 0, bag_size: 72\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9765151515151516: correct 10312/10560\n",
      "class 1 clustering acc 0.865719696969697: correct 4571/5280\n",
      "Epoch: 35, train_loss: 0.4024, train_clustering_loss:  0.2686, train_error: 0.0939\n",
      "class 0: acc 0.8902439024390244, correct 292/328\n",
      "class 1: acc 0.9216867469879518, correct 306/332\n",
      "\n",
      "Val Set, val_loss: 0.7662, val_error: 0.1892, auc: 0.8868\n",
      "class 0 clustering acc 0.8994932432432432: correct 1065/1184\n",
      "class 1 clustering acc 0.7618243243243243: correct 451/592\n",
      "class 0: acc 0.8235294117647058, correct 28/34\n",
      "class 1: acc 0.8, correct 32/40\n",
      "EarlyStopping counter: 25 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 108\n",
      "batch 39, loss: 0.0078, instance_loss: 0.0142, weighted_loss: 0.0097, label: 0, bag_size: 77\n",
      "batch 59, loss: 0.0113, instance_loss: 0.0097, weighted_loss: 0.0108, label: 0, bag_size: 66\n",
      "batch 79, loss: 0.0006, instance_loss: 0.1825, weighted_loss: 0.0552, label: 0, bag_size: 31\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0170, weighted_loss: 0.0051, label: 1, bag_size: 107\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 0, bag_size: 126\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 0, bag_size: 30\n",
      "batch 159, loss: 0.0009, instance_loss: 0.0192, weighted_loss: 0.0064, label: 1, bag_size: 31\n",
      "batch 179, loss: 0.0537, instance_loss: 0.0380, weighted_loss: 0.0489, label: 1, bag_size: 42\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 78\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 116\n",
      "batch 239, loss: 2.6026, instance_loss: 0.3556, weighted_loss: 1.9285, label: 1, bag_size: 50\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 1, bag_size: 108\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0196, weighted_loss: 0.0059, label: 1, bag_size: 42\n",
      "batch 299, loss: 0.0061, instance_loss: 0.2282, weighted_loss: 0.0728, label: 0, bag_size: 31\n",
      "batch 319, loss: 0.0935, instance_loss: 0.1031, weighted_loss: 0.0964, label: 0, bag_size: 54\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0079, weighted_loss: 0.0024, label: 1, bag_size: 75\n",
      "batch 359, loss: 0.0196, instance_loss: 0.0733, weighted_loss: 0.0357, label: 1, bag_size: 48\n",
      "batch 379, loss: 0.0064, instance_loss: 0.1644, weighted_loss: 0.0538, label: 1, bag_size: 86\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 36\n",
      "batch 419, loss: 0.0000, instance_loss: 0.1421, weighted_loss: 0.0426, label: 1, bag_size: 85\n",
      "batch 439, loss: 8.4300, instance_loss: 0.9758, weighted_loss: 6.1937, label: 0, bag_size: 36\n",
      "batch 459, loss: 1.0000, instance_loss: 0.7437, weighted_loss: 0.9231, label: 1, bag_size: 52\n",
      "batch 479, loss: 0.0123, instance_loss: 0.0753, weighted_loss: 0.0312, label: 1, bag_size: 116\n",
      "batch 499, loss: 0.0014, instance_loss: 0.1152, weighted_loss: 0.0355, label: 0, bag_size: 91\n",
      "batch 519, loss: 0.0001, instance_loss: 0.2635, weighted_loss: 0.0791, label: 0, bag_size: 31\n",
      "batch 539, loss: 12.5386, instance_loss: 0.5427, weighted_loss: 8.9398, label: 1, bag_size: 31\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 93\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0566, weighted_loss: 0.0170, label: 0, bag_size: 46\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0947, weighted_loss: 0.0284, label: 0, bag_size: 102\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0321, weighted_loss: 0.0096, label: 0, bag_size: 59\n",
      "batch 639, loss: 0.0001, instance_loss: 0.1575, weighted_loss: 0.0473, label: 1, bag_size: 84\n",
      "batch 659, loss: 4.4375, instance_loss: 0.5207, weighted_loss: 3.2624, label: 0, bag_size: 31\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9632575757575758: correct 10172/10560\n",
      "class 1 clustering acc 0.8244318181818182: correct 4353/5280\n",
      "Epoch: 36, train_loss: 0.8571, train_clustering_loss:  0.3660, train_error: 0.1545\n",
      "class 0: acc 0.8338658146964856, correct 261/313\n",
      "class 1: acc 0.8559077809798271, correct 297/347\n",
      "\n",
      "Val Set, val_loss: 2.5758, val_error: 0.2432, auc: 0.8790\n",
      "class 0 clustering acc 0.9459459459459459: correct 1120/1184\n",
      "class 1 clustering acc 0.6368243243243243: correct 377/592\n",
      "class 0: acc 0.5, correct 17/34\n",
      "class 1: acc 0.975, correct 39/40\n",
      "EarlyStopping counter: 26 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0115, instance_loss: 0.5668, weighted_loss: 0.1781, label: 1, bag_size: 52\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0215, weighted_loss: 0.0065, label: 1, bag_size: 80\n",
      "batch 59, loss: 0.0002, instance_loss: 0.0175, weighted_loss: 0.0053, label: 0, bag_size: 92\n",
      "batch 79, loss: 0.0012, instance_loss: 0.0037, weighted_loss: 0.0019, label: 0, bag_size: 92\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0046, weighted_loss: 0.0014, label: 0, bag_size: 46\n",
      "batch 119, loss: 0.0019, instance_loss: 0.0619, weighted_loss: 0.0199, label: 0, bag_size: 66\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0193, weighted_loss: 0.0058, label: 1, bag_size: 97\n",
      "batch 159, loss: 0.8050, instance_loss: 1.2155, weighted_loss: 0.9281, label: 0, bag_size: 31\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0792, weighted_loss: 0.0238, label: 0, bag_size: 78\n",
      "batch 199, loss: 0.1540, instance_loss: 0.6677, weighted_loss: 0.3081, label: 0, bag_size: 114\n",
      "batch 219, loss: 0.0009, instance_loss: 0.2460, weighted_loss: 0.0744, label: 1, bag_size: 47\n",
      "batch 239, loss: 0.0002, instance_loss: 0.0472, weighted_loss: 0.0143, label: 1, bag_size: 85\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0044, weighted_loss: 0.0013, label: 0, bag_size: 93\n",
      "batch 279, loss: 0.0962, instance_loss: 1.0925, weighted_loss: 0.3951, label: 0, bag_size: 69\n",
      "batch 299, loss: 0.1946, instance_loss: 0.2069, weighted_loss: 0.1983, label: 1, bag_size: 35\n",
      "batch 319, loss: 3.4908, instance_loss: 1.6373, weighted_loss: 2.9348, label: 0, bag_size: 78\n",
      "batch 339, loss: 3.3079, instance_loss: 0.5133, weighted_loss: 2.4695, label: 0, bag_size: 29\n",
      "batch 359, loss: 0.8097, instance_loss: 0.9069, weighted_loss: 0.8388, label: 1, bag_size: 60\n",
      "batch 379, loss: 0.0597, instance_loss: 0.0447, weighted_loss: 0.0552, label: 1, bag_size: 39\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0034, weighted_loss: 0.0010, label: 1, bag_size: 122\n",
      "batch 419, loss: 0.0003, instance_loss: 0.0168, weighted_loss: 0.0052, label: 1, bag_size: 40\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0055, weighted_loss: 0.0016, label: 0, bag_size: 95\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0039, weighted_loss: 0.0012, label: 0, bag_size: 65\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0034, weighted_loss: 0.0010, label: 0, bag_size: 37\n",
      "batch 499, loss: 0.0264, instance_loss: 0.0097, weighted_loss: 0.0214, label: 0, bag_size: 71\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 103\n",
      "batch 539, loss: 0.5314, instance_loss: 1.4477, weighted_loss: 0.8063, label: 1, bag_size: 51\n",
      "batch 559, loss: 0.0651, instance_loss: 0.4589, weighted_loss: 0.1832, label: 0, bag_size: 75\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0000, label: 0, bag_size: 106\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0054, weighted_loss: 0.0016, label: 1, bag_size: 84\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 72\n",
      "batch 639, loss: 0.0423, instance_loss: 0.1228, weighted_loss: 0.0665, label: 1, bag_size: 30\n",
      "batch 659, loss: 0.0297, instance_loss: 0.0299, weighted_loss: 0.0297, label: 1, bag_size: 52\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9672348484848485: correct 10214/10560\n",
      "class 1 clustering acc 0.8212121212121212: correct 4336/5280\n",
      "Epoch: 37, train_loss: 0.4928, train_clustering_loss:  0.3323, train_error: 0.1227\n",
      "class 0: acc 0.8768768768768769, correct 292/333\n",
      "class 1: acc 0.8776758409785933, correct 287/327\n",
      "\n",
      "Val Set, val_loss: 0.7990, val_error: 0.2027, auc: 0.9022\n",
      "class 0 clustering acc 0.9298986486486487: correct 1101/1184\n",
      "class 1 clustering acc 0.7668918918918919: correct 454/592\n",
      "class 0: acc 0.8235294117647058, correct 28/34\n",
      "class 1: acc 0.775, correct 31/40\n",
      "EarlyStopping counter: 27 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0243, instance_loss: 0.0889, weighted_loss: 0.0436, label: 1, bag_size: 49\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0003, weighted_loss: 0.0002, label: 0, bag_size: 42\n",
      "batch 59, loss: 0.0216, instance_loss: 0.0071, weighted_loss: 0.0173, label: 1, bag_size: 96\n",
      "batch 79, loss: 1.8175, instance_loss: 1.2553, weighted_loss: 1.6488, label: 0, bag_size: 103\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0183, weighted_loss: 0.0055, label: 0, bag_size: 74\n",
      "batch 119, loss: 0.0383, instance_loss: 0.0423, weighted_loss: 0.0395, label: 1, bag_size: 56\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0257, weighted_loss: 0.0078, label: 1, bag_size: 25\n",
      "batch 159, loss: 0.0038, instance_loss: 0.1152, weighted_loss: 0.0372, label: 1, bag_size: 49\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0022, weighted_loss: 0.0006, label: 0, bag_size: 65\n",
      "batch 199, loss: 0.4190, instance_loss: 0.8311, weighted_loss: 0.5427, label: 0, bag_size: 75\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 110\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0058, weighted_loss: 0.0018, label: 1, bag_size: 39\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0028, weighted_loss: 0.0009, label: 0, bag_size: 83\n",
      "batch 279, loss: 0.0388, instance_loss: 0.0930, weighted_loss: 0.0551, label: 1, bag_size: 36\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0146, weighted_loss: 0.0044, label: 1, bag_size: 83\n",
      "batch 319, loss: 0.8649, instance_loss: 2.1244, weighted_loss: 1.2427, label: 1, bag_size: 17\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0597, weighted_loss: 0.0179, label: 0, bag_size: 57\n",
      "batch 359, loss: 0.5158, instance_loss: 1.1526, weighted_loss: 0.7068, label: 0, bag_size: 54\n",
      "batch 379, loss: 0.0060, instance_loss: 0.1122, weighted_loss: 0.0378, label: 1, bag_size: 87\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0028, weighted_loss: 0.0008, label: 0, bag_size: 67\n",
      "batch 419, loss: 0.0001, instance_loss: 0.1046, weighted_loss: 0.0315, label: 0, bag_size: 99\n",
      "batch 439, loss: 0.0000, instance_loss: 0.2290, weighted_loss: 0.0687, label: 0, bag_size: 46\n",
      "batch 459, loss: 0.0025, instance_loss: 0.0157, weighted_loss: 0.0065, label: 1, bag_size: 63\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0020, weighted_loss: 0.0006, label: 0, bag_size: 42\n",
      "batch 499, loss: 0.0064, instance_loss: 0.0257, weighted_loss: 0.0122, label: 1, bag_size: 76\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 72\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0150, weighted_loss: 0.0045, label: 0, bag_size: 72\n",
      "batch 559, loss: 0.0003, instance_loss: 0.0870, weighted_loss: 0.0263, label: 0, bag_size: 40\n",
      "batch 579, loss: 3.7214, instance_loss: 1.9728, weighted_loss: 3.1968, label: 0, bag_size: 36\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0180, weighted_loss: 0.0055, label: 1, bag_size: 85\n",
      "batch 619, loss: 0.0382, instance_loss: 0.7216, weighted_loss: 0.2432, label: 1, bag_size: 105\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 25\n",
      "batch 659, loss: 0.0000, instance_loss: 0.1190, weighted_loss: 0.0357, label: 0, bag_size: 57\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9627840909090909: correct 10167/10560\n",
      "class 1 clustering acc 0.803219696969697: correct 4241/5280\n",
      "Epoch: 38, train_loss: 0.6412, train_clustering_loss:  0.3572, train_error: 0.1591\n",
      "class 0: acc 0.8369230769230769, correct 272/325\n",
      "class 1: acc 0.844776119402985, correct 283/335\n",
      "\n",
      "Val Set, val_loss: 0.9601, val_error: 0.2162, auc: 0.8838\n",
      "class 0 clustering acc 0.9121621621621622: correct 1080/1184\n",
      "class 1 clustering acc 0.777027027027027: correct 460/592\n",
      "class 0: acc 0.7647058823529411, correct 26/34\n",
      "class 1: acc 0.8, correct 32/40\n",
      "EarlyStopping counter: 28 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0053, weighted_loss: 0.0016, label: 1, bag_size: 61\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 57\n",
      "batch 59, loss: 0.3836, instance_loss: 0.3017, weighted_loss: 0.3590, label: 0, bag_size: 51\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 85\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0375, weighted_loss: 0.0112, label: 0, bag_size: 78\n",
      "batch 119, loss: 0.0026, instance_loss: 0.0016, weighted_loss: 0.0023, label: 1, bag_size: 96\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0001, label: 1, bag_size: 36\n",
      "batch 159, loss: 0.1750, instance_loss: 0.7756, weighted_loss: 0.3552, label: 1, bag_size: 62\n",
      "batch 179, loss: 0.4565, instance_loss: 0.6582, weighted_loss: 0.5170, label: 1, bag_size: 38\n",
      "batch 199, loss: 3.0367, instance_loss: 2.3480, weighted_loss: 2.8301, label: 0, bag_size: 83\n",
      "batch 219, loss: 0.0021, instance_loss: 0.7126, weighted_loss: 0.2152, label: 0, bag_size: 31\n",
      "batch 239, loss: 0.0001, instance_loss: 0.7799, weighted_loss: 0.2341, label: 0, bag_size: 85\n",
      "batch 259, loss: 0.0154, instance_loss: 0.0011, weighted_loss: 0.0111, label: 0, bag_size: 101\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0095, weighted_loss: 0.0029, label: 0, bag_size: 91\n",
      "batch 299, loss: 0.0002, instance_loss: 0.0048, weighted_loss: 0.0016, label: 0, bag_size: 47\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 0, bag_size: 72\n",
      "batch 339, loss: 3.1708, instance_loss: 0.7276, weighted_loss: 2.4378, label: 0, bag_size: 51\n",
      "batch 359, loss: 0.1998, instance_loss: 0.2588, weighted_loss: 0.2175, label: 0, bag_size: 103\n",
      "batch 379, loss: 0.0001, instance_loss: 0.0041, weighted_loss: 0.0013, label: 1, bag_size: 51\n",
      "batch 399, loss: 0.0000, instance_loss: 0.1266, weighted_loss: 0.0380, label: 1, bag_size: 74\n",
      "batch 419, loss: 0.0019, instance_loss: 1.3339, weighted_loss: 0.4015, label: 0, bag_size: 110\n",
      "batch 439, loss: 0.0357, instance_loss: 0.2085, weighted_loss: 0.0875, label: 1, bag_size: 76\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 93\n",
      "batch 479, loss: 9.0737, instance_loss: 3.3043, weighted_loss: 7.3429, label: 1, bag_size: 28\n",
      "batch 499, loss: 0.0269, instance_loss: 2.6883, weighted_loss: 0.8253, label: 0, bag_size: 29\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 65\n",
      "batch 539, loss: 0.0077, instance_loss: 0.0644, weighted_loss: 0.0247, label: 1, bag_size: 23\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 1, bag_size: 105\n",
      "batch 579, loss: 0.0019, instance_loss: 0.2181, weighted_loss: 0.0668, label: 0, bag_size: 51\n",
      "batch 599, loss: 2.0306, instance_loss: 1.0448, weighted_loss: 1.7349, label: 1, bag_size: 88\n",
      "batch 619, loss: 0.6834, instance_loss: 0.1591, weighted_loss: 0.5261, label: 1, bag_size: 75\n",
      "batch 639, loss: 0.0004, instance_loss: 0.0015, weighted_loss: 0.0008, label: 0, bag_size: 92\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0026, weighted_loss: 0.0008, label: 1, bag_size: 52\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.956060606060606: correct 10096/10560\n",
      "class 1 clustering acc 0.7895833333333333: correct 4169/5280\n",
      "Epoch: 39, train_loss: 0.5436, train_clustering_loss:  0.4208, train_error: 0.1470\n",
      "class 0: acc 0.8643067846607669, correct 293/339\n",
      "class 1: acc 0.8411214953271028, correct 270/321\n",
      "\n",
      "Val Set, val_loss: 0.7798, val_error: 0.2027, auc: 0.9015\n",
      "class 0 clustering acc 0.8597972972972973: correct 1018/1184\n",
      "class 1 clustering acc 0.7533783783783784: correct 446/592\n",
      "class 0: acc 0.7647058823529411, correct 26/34\n",
      "class 1: acc 0.825, correct 33/40\n",
      "EarlyStopping counter: 29 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0166, instance_loss: 0.0451, weighted_loss: 0.0251, label: 1, bag_size: 85\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0023, weighted_loss: 0.0007, label: 0, bag_size: 50\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0403, weighted_loss: 0.0121, label: 0, bag_size: 17\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0114, weighted_loss: 0.0034, label: 1, bag_size: 75\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0345, weighted_loss: 0.0104, label: 1, bag_size: 83\n",
      "batch 119, loss: 0.0002, instance_loss: 0.0234, weighted_loss: 0.0072, label: 1, bag_size: 85\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0072, weighted_loss: 0.0022, label: 0, bag_size: 84\n",
      "batch 159, loss: 0.1243, instance_loss: 1.1252, weighted_loss: 0.4246, label: 0, bag_size: 33\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0034, weighted_loss: 0.0010, label: 0, bag_size: 36\n",
      "batch 199, loss: 0.2077, instance_loss: 0.1073, weighted_loss: 0.1776, label: 0, bag_size: 103\n",
      "batch 219, loss: 0.0067, instance_loss: 0.0757, weighted_loss: 0.0274, label: 0, bag_size: 104\n",
      "batch 239, loss: 0.0072, instance_loss: 0.0636, weighted_loss: 0.0241, label: 0, bag_size: 50\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0022, weighted_loss: 0.0007, label: 1, bag_size: 70\n",
      "batch 279, loss: 0.0078, instance_loss: 0.3368, weighted_loss: 0.1065, label: 1, bag_size: 59\n",
      "batch 299, loss: 0.0115, instance_loss: 0.0059, weighted_loss: 0.0098, label: 1, bag_size: 14\n",
      "batch 319, loss: 0.1060, instance_loss: 0.1676, weighted_loss: 0.1245, label: 0, bag_size: 58\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 65\n",
      "batch 359, loss: 0.0087, instance_loss: 0.1072, weighted_loss: 0.0382, label: 0, bag_size: 58\n",
      "batch 379, loss: 1.4244, instance_loss: 1.0519, weighted_loss: 1.3127, label: 0, bag_size: 17\n",
      "batch 399, loss: 0.0002, instance_loss: 0.0054, weighted_loss: 0.0018, label: 1, bag_size: 76\n",
      "batch 419, loss: 0.0003, instance_loss: 0.0005, weighted_loss: 0.0004, label: 0, bag_size: 110\n",
      "batch 439, loss: 0.1639, instance_loss: 0.2268, weighted_loss: 0.1828, label: 1, bag_size: 86\n",
      "batch 459, loss: 0.0505, instance_loss: 0.3134, weighted_loss: 0.1294, label: 1, bag_size: 100\n",
      "batch 479, loss: 0.0038, instance_loss: 0.0115, weighted_loss: 0.0061, label: 1, bag_size: 29\n",
      "batch 499, loss: 0.0018, instance_loss: 0.8055, weighted_loss: 0.2429, label: 1, bag_size: 83\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0003, label: 1, bag_size: 39\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0131, weighted_loss: 0.0039, label: 0, bag_size: 90\n",
      "batch 559, loss: 2.3568, instance_loss: 1.2055, weighted_loss: 2.0114, label: 1, bag_size: 94\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0017, weighted_loss: 0.0005, label: 1, bag_size: 85\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0006, weighted_loss: 0.0003, label: 0, bag_size: 88\n",
      "batch 619, loss: 0.3355, instance_loss: 0.0658, weighted_loss: 0.2545, label: 1, bag_size: 46\n",
      "batch 639, loss: 0.0029, instance_loss: 0.0165, weighted_loss: 0.0070, label: 0, bag_size: 42\n",
      "batch 659, loss: 0.0009, instance_loss: 0.0178, weighted_loss: 0.0059, label: 1, bag_size: 25\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9720643939393939: correct 10265/10560\n",
      "class 1 clustering acc 0.8545454545454545: correct 4512/5280\n",
      "Epoch: 40, train_loss: 0.3054, train_clustering_loss:  0.2825, train_error: 0.1045\n",
      "class 0: acc 0.8950437317784257, correct 307/343\n",
      "class 1: acc 0.8958990536277602, correct 284/317\n",
      "\n",
      "Val Set, val_loss: 0.7614, val_error: 0.2162, auc: 0.8934\n",
      "class 0 clustering acc 0.9087837837837838: correct 1076/1184\n",
      "class 1 clustering acc 0.7398648648648649: correct 438/592\n",
      "class 0: acc 0.8529411764705882, correct 29/34\n",
      "class 1: acc 0.725, correct 29/40\n",
      "EarlyStopping counter: 30 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.0033, weighted_loss: 0.0011, label: 0, bag_size: 21\n",
      "batch 39, loss: 0.0008, instance_loss: 0.0060, weighted_loss: 0.0024, label: 0, bag_size: 74\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 25\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0033, weighted_loss: 0.0010, label: 1, bag_size: 81\n",
      "batch 99, loss: 0.0009, instance_loss: 0.0055, weighted_loss: 0.0023, label: 0, bag_size: 18\n",
      "batch 119, loss: 0.0084, instance_loss: 0.2174, weighted_loss: 0.0711, label: 1, bag_size: 35\n",
      "batch 139, loss: 0.0046, instance_loss: 0.0076, weighted_loss: 0.0055, label: 0, bag_size: 13\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0041, weighted_loss: 0.0013, label: 1, bag_size: 110\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0173, weighted_loss: 0.0052, label: 1, bag_size: 85\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 1, bag_size: 84\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 25\n",
      "batch 239, loss: 0.0021, instance_loss: 0.0704, weighted_loss: 0.0226, label: 1, bag_size: 40\n",
      "batch 259, loss: 0.0015, instance_loss: 0.0110, weighted_loss: 0.0043, label: 1, bag_size: 21\n",
      "batch 279, loss: 0.0016, instance_loss: 0.0090, weighted_loss: 0.0038, label: 1, bag_size: 88\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 39\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 57\n",
      "batch 339, loss: 0.7022, instance_loss: 0.0906, weighted_loss: 0.5187, label: 1, bag_size: 73\n",
      "batch 359, loss: 1.7410, instance_loss: 1.9800, weighted_loss: 1.8127, label: 0, bag_size: 164\n",
      "batch 379, loss: 0.0001, instance_loss: 0.3115, weighted_loss: 0.0935, label: 1, bag_size: 54\n",
      "batch 399, loss: 0.0052, instance_loss: 0.4966, weighted_loss: 0.1527, label: 0, bag_size: 61\n",
      "batch 419, loss: 0.0189, instance_loss: 0.1419, weighted_loss: 0.0558, label: 1, bag_size: 21\n",
      "batch 439, loss: 3.0277, instance_loss: 0.0513, weighted_loss: 2.1347, label: 0, bag_size: 83\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0401, weighted_loss: 0.0120, label: 1, bag_size: 35\n",
      "batch 479, loss: 0.0015, instance_loss: 0.3385, weighted_loss: 0.1026, label: 0, bag_size: 93\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0096, weighted_loss: 0.0029, label: 1, bag_size: 24\n",
      "batch 519, loss: 5.1413, instance_loss: 1.5779, weighted_loss: 4.0723, label: 1, bag_size: 44\n",
      "batch 539, loss: 0.1288, instance_loss: 0.0221, weighted_loss: 0.0968, label: 1, bag_size: 57\n",
      "batch 559, loss: 7.1305, instance_loss: 2.0407, weighted_loss: 5.6036, label: 0, bag_size: 57\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 38\n",
      "batch 599, loss: 0.0374, instance_loss: 0.0089, weighted_loss: 0.0289, label: 1, bag_size: 25\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0043, weighted_loss: 0.0013, label: 0, bag_size: 29\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 57\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 50\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9735795454545455: correct 10281/10560\n",
      "class 1 clustering acc 0.8948863636363636: correct 4725/5280\n",
      "Epoch: 41, train_loss: 0.3364, train_clustering_loss:  0.2411, train_error: 0.0894\n",
      "class 0: acc 0.9188405797101449, correct 317/345\n",
      "class 1: acc 0.9015873015873016, correct 284/315\n",
      "\n",
      "Val Set, val_loss: 1.2702, val_error: 0.1892, auc: 0.9099\n",
      "class 0 clustering acc 0.8614864864864865: correct 1020/1184\n",
      "class 1 clustering acc 0.8091216216216216: correct 479/592\n",
      "class 0: acc 0.8823529411764706, correct 30/34\n",
      "class 1: acc 0.75, correct 30/40\n",
      "EarlyStopping counter: 31 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 26\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 72\n",
      "batch 59, loss: 0.4429, instance_loss: 4.2504, weighted_loss: 1.5852, label: 0, bag_size: 25\n",
      "batch 79, loss: 0.0747, instance_loss: 0.1550, weighted_loss: 0.0988, label: 1, bag_size: 53\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 33\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 64\n",
      "batch 139, loss: 10.8300, instance_loss: 4.0563, weighted_loss: 8.7979, label: 1, bag_size: 59\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 77\n",
      "batch 179, loss: 0.0465, instance_loss: 0.1127, weighted_loss: 0.0664, label: 1, bag_size: 83\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 61\n",
      "batch 219, loss: 0.0199, instance_loss: 0.0368, weighted_loss: 0.0250, label: 0, bag_size: 49\n",
      "batch 239, loss: 0.0011, instance_loss: 0.0054, weighted_loss: 0.0024, label: 1, bag_size: 48\n",
      "batch 259, loss: 0.0004, instance_loss: 0.0795, weighted_loss: 0.0242, label: 0, bag_size: 77\n",
      "batch 279, loss: 0.0195, instance_loss: 0.0032, weighted_loss: 0.0146, label: 0, bag_size: 96\n",
      "batch 299, loss: 7.8761, instance_loss: 1.1761, weighted_loss: 5.8661, label: 1, bag_size: 23\n",
      "batch 319, loss: 0.5258, instance_loss: 1.8214, weighted_loss: 0.9145, label: 0, bag_size: 29\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 72\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0022, weighted_loss: 0.0007, label: 0, bag_size: 102\n",
      "batch 379, loss: 7.9890, instance_loss: 2.2050, weighted_loss: 6.2538, label: 0, bag_size: 18\n",
      "batch 399, loss: 5.9062, instance_loss: 2.9970, weighted_loss: 5.0335, label: 0, bag_size: 110\n",
      "batch 419, loss: 0.0054, instance_loss: 0.1099, weighted_loss: 0.0368, label: 0, bag_size: 79\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0040, weighted_loss: 0.0012, label: 0, bag_size: 28\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 65\n",
      "batch 479, loss: 0.0043, instance_loss: 0.0095, weighted_loss: 0.0059, label: 0, bag_size: 44\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 49\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0098, weighted_loss: 0.0030, label: 0, bag_size: 81\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 61\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0039, weighted_loss: 0.0012, label: 0, bag_size: 103\n",
      "batch 579, loss: 0.0485, instance_loss: 0.5955, weighted_loss: 0.2126, label: 0, bag_size: 33\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0025, weighted_loss: 0.0008, label: 1, bag_size: 85\n",
      "batch 619, loss: 0.0196, instance_loss: 0.0166, weighted_loss: 0.0187, label: 0, bag_size: 79\n",
      "batch 639, loss: 0.0555, instance_loss: 0.1717, weighted_loss: 0.0904, label: 1, bag_size: 42\n",
      "batch 659, loss: 0.0008, instance_loss: 0.0064, weighted_loss: 0.0025, label: 1, bag_size: 69\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9743371212121212: correct 10289/10560\n",
      "class 1 clustering acc 0.8710227272727272: correct 4599/5280\n",
      "Epoch: 42, train_loss: 0.3427, train_clustering_loss:  0.2427, train_error: 0.0864\n",
      "class 0: acc 0.9121212121212121, correct 301/330\n",
      "class 1: acc 0.9151515151515152, correct 302/330\n",
      "\n",
      "Val Set, val_loss: 0.7922, val_error: 0.1486, auc: 0.8919\n",
      "class 0 clustering acc 0.9121621621621622: correct 1080/1184\n",
      "class 1 clustering acc 0.7516891891891891: correct 445/592\n",
      "class 0: acc 0.7941176470588235, correct 27/34\n",
      "class 1: acc 0.9, correct 36/40\n",
      "EarlyStopping counter: 32 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0572, instance_loss: 0.0219, weighted_loss: 0.0467, label: 1, bag_size: 86\n",
      "batch 39, loss: 0.0091, instance_loss: 0.0513, weighted_loss: 0.0217, label: 0, bag_size: 53\n",
      "batch 59, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 50\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 78\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 33\n",
      "batch 119, loss: 0.0068, instance_loss: 0.1027, weighted_loss: 0.0356, label: 1, bag_size: 53\n",
      "batch 139, loss: 2.0062, instance_loss: 1.4367, weighted_loss: 1.8354, label: 1, bag_size: 82\n",
      "batch 159, loss: 0.1498, instance_loss: 0.2795, weighted_loss: 0.1887, label: 1, bag_size: 105\n",
      "batch 179, loss: 0.0102, instance_loss: 1.6290, weighted_loss: 0.4959, label: 0, bag_size: 98\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0315, weighted_loss: 0.0095, label: 1, bag_size: 79\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0131, weighted_loss: 0.0039, label: 1, bag_size: 84\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 83\n",
      "batch 259, loss: 0.0003, instance_loss: 0.8229, weighted_loss: 0.2471, label: 0, bag_size: 72\n",
      "batch 279, loss: 4.7771, instance_loss: 1.1531, weighted_loss: 3.6899, label: 0, bag_size: 41\n",
      "batch 299, loss: 0.0001, instance_loss: 0.2484, weighted_loss: 0.0746, label: 1, bag_size: 153\n",
      "batch 319, loss: 0.1294, instance_loss: 0.8735, weighted_loss: 0.3526, label: 1, bag_size: 62\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 43\n",
      "batch 359, loss: 0.0002, instance_loss: 0.0003, weighted_loss: 0.0002, label: 0, bag_size: 81\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0001, label: 1, bag_size: 77\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 77\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 30\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0019, weighted_loss: 0.0006, label: 0, bag_size: 116\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 96\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0021, weighted_loss: 0.0007, label: 1, bag_size: 95\n",
      "batch 499, loss: 0.0098, instance_loss: 0.0236, weighted_loss: 0.0139, label: 0, bag_size: 50\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0020, weighted_loss: 0.0006, label: 1, bag_size: 97\n",
      "batch 539, loss: 0.0083, instance_loss: 0.3882, weighted_loss: 0.1223, label: 1, bag_size: 23\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 93\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0724, weighted_loss: 0.0217, label: 1, bag_size: 84\n",
      "batch 599, loss: 0.0000, instance_loss: 0.6575, weighted_loss: 0.1972, label: 0, bag_size: 35\n",
      "batch 619, loss: 0.0000, instance_loss: 1.2456, weighted_loss: 0.3737, label: 0, bag_size: 95\n",
      "batch 639, loss: 2.0756, instance_loss: 0.0867, weighted_loss: 1.4789, label: 0, bag_size: 88\n",
      "batch 659, loss: 0.0000, instance_loss: 0.2128, weighted_loss: 0.0639, label: 0, bag_size: 79\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9653409090909091: correct 10194/10560\n",
      "class 1 clustering acc 0.843560606060606: correct 4454/5280\n",
      "Epoch: 43, train_loss: 0.4589, train_clustering_loss:  0.3375, train_error: 0.1136\n",
      "class 0: acc 0.8830769230769231, correct 287/325\n",
      "class 1: acc 0.8895522388059701, correct 298/335\n",
      "\n",
      "Val Set, val_loss: 3.0670, val_error: 0.1892, auc: 0.8868\n",
      "class 0 clustering acc 0.9484797297297297: correct 1123/1184\n",
      "class 1 clustering acc 0.7077702702702703: correct 419/592\n",
      "class 0: acc 0.7352941176470589, correct 25/34\n",
      "class 1: acc 0.875, correct 35/40\n",
      "EarlyStopping counter: 33 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0311, weighted_loss: 0.0093, label: 1, bag_size: 47\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0243, weighted_loss: 0.0073, label: 0, bag_size: 89\n",
      "batch 59, loss: 0.0000, instance_loss: 0.6844, weighted_loss: 0.2053, label: 0, bag_size: 25\n",
      "batch 79, loss: 0.1670, instance_loss: 0.8247, weighted_loss: 0.3643, label: 0, bag_size: 64\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0319, weighted_loss: 0.0096, label: 0, bag_size: 91\n",
      "batch 119, loss: 0.0010, instance_loss: 0.5703, weighted_loss: 0.1718, label: 0, bag_size: 54\n",
      "batch 139, loss: 28.5255, instance_loss: 0.9260, weighted_loss: 20.2457, label: 1, bag_size: 96\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 37\n",
      "batch 179, loss: 0.9532, instance_loss: 2.4933, weighted_loss: 1.4152, label: 0, bag_size: 72\n",
      "batch 199, loss: 3.6220, instance_loss: 0.1579, weighted_loss: 2.5828, label: 0, bag_size: 62\n",
      "batch 219, loss: 21.9117, instance_loss: 4.5356, weighted_loss: 16.6989, label: 0, bag_size: 29\n",
      "batch 239, loss: 6.5985, instance_loss: 0.5723, weighted_loss: 4.7906, label: 1, bag_size: 65\n",
      "batch 259, loss: 6.0244, instance_loss: 0.1214, weighted_loss: 4.2535, label: 0, bag_size: 26\n",
      "batch 279, loss: 0.0000, instance_loss: 1.2287, weighted_loss: 0.3686, label: 1, bag_size: 67\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0261, weighted_loss: 0.0078, label: 1, bag_size: 102\n",
      "batch 319, loss: 0.0000, instance_loss: 1.0557, weighted_loss: 0.3167, label: 1, bag_size: 84\n",
      "batch 339, loss: 0.2229, instance_loss: 0.3675, weighted_loss: 0.2663, label: 1, bag_size: 77\n",
      "batch 359, loss: 3.8248, instance_loss: 2.4701, weighted_loss: 3.4184, label: 0, bag_size: 29\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0020, weighted_loss: 0.0006, label: 0, bag_size: 40\n",
      "batch 399, loss: 20.5486, instance_loss: 0.8956, weighted_loss: 14.6527, label: 1, bag_size: 64\n",
      "batch 419, loss: 23.7451, instance_loss: 3.5680, weighted_loss: 17.6920, label: 1, bag_size: 35\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 18\n",
      "batch 459, loss: 0.0133, instance_loss: 0.0037, weighted_loss: 0.0104, label: 0, bag_size: 44\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0027, weighted_loss: 0.0008, label: 1, bag_size: 89\n",
      "batch 499, loss: 8.6306, instance_loss: 2.1032, weighted_loss: 6.6723, label: 0, bag_size: 60\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0005, label: 0, bag_size: 87\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0471, weighted_loss: 0.0141, label: 0, bag_size: 44\n",
      "batch 559, loss: 0.0007, instance_loss: 0.4780, weighted_loss: 0.1439, label: 1, bag_size: 24\n",
      "batch 579, loss: 0.0003, instance_loss: 0.0080, weighted_loss: 0.0026, label: 0, bag_size: 85\n",
      "batch 599, loss: 1.4790, instance_loss: 0.1638, weighted_loss: 1.0845, label: 1, bag_size: 81\n",
      "batch 619, loss: 0.5397, instance_loss: 0.2615, weighted_loss: 0.4562, label: 1, bag_size: 29\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0004, label: 0, bag_size: 40\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0067, weighted_loss: 0.0020, label: 0, bag_size: 21\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.959280303030303: correct 10130/10560\n",
      "class 1 clustering acc 0.8178030303030303: correct 4318/5280\n",
      "Epoch: 44, train_loss: 1.5225, train_clustering_loss:  0.3707, train_error: 0.1515\n",
      "class 0: acc 0.8583333333333333, correct 309/360\n",
      "class 1: acc 0.8366666666666667, correct 251/300\n",
      "\n",
      "Val Set, val_loss: 1.2143, val_error: 0.2297, auc: 0.8971\n",
      "class 0 clustering acc 0.8631756756756757: correct 1022/1184\n",
      "class 1 clustering acc 0.7246621621621622: correct 429/592\n",
      "class 0: acc 0.8529411764705882, correct 29/34\n",
      "class 1: acc 0.7, correct 28/40\n",
      "EarlyStopping counter: 34 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 1, bag_size: 52\n",
      "batch 39, loss: 4.9660, instance_loss: 3.6015, weighted_loss: 4.5566, label: 1, bag_size: 57\n",
      "batch 59, loss: 0.0013, instance_loss: 0.0498, weighted_loss: 0.0158, label: 0, bag_size: 85\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0032, weighted_loss: 0.0010, label: 1, bag_size: 88\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 81\n",
      "batch 119, loss: 0.0005, instance_loss: 0.0057, weighted_loss: 0.0020, label: 0, bag_size: 54\n",
      "batch 139, loss: 0.0467, instance_loss: 1.2882, weighted_loss: 0.4191, label: 1, bag_size: 62\n",
      "batch 159, loss: 0.8309, instance_loss: 0.7020, weighted_loss: 0.7922, label: 0, bag_size: 22\n",
      "batch 179, loss: 0.0014, instance_loss: 0.0680, weighted_loss: 0.0214, label: 0, bag_size: 74\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0517, weighted_loss: 0.0155, label: 1, bag_size: 41\n",
      "batch 219, loss: 6.1983, instance_loss: 0.5397, weighted_loss: 4.5007, label: 1, bag_size: 60\n",
      "batch 239, loss: 2.1128, instance_loss: 0.2831, weighted_loss: 1.5639, label: 0, bag_size: 49\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 25\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0046, weighted_loss: 0.0014, label: 0, bag_size: 116\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0092, weighted_loss: 0.0028, label: 1, bag_size: 31\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 20\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0064, weighted_loss: 0.0019, label: 1, bag_size: 87\n",
      "batch 359, loss: 1.0178, instance_loss: 1.3734, weighted_loss: 1.1245, label: 0, bag_size: 73\n",
      "batch 379, loss: 0.0067, instance_loss: 0.0444, weighted_loss: 0.0180, label: 0, bag_size: 33\n",
      "batch 399, loss: 0.0000, instance_loss: 0.1273, weighted_loss: 0.0382, label: 0, bag_size: 98\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0688, weighted_loss: 0.0206, label: 1, bag_size: 95\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0001, label: 1, bag_size: 29\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0072, weighted_loss: 0.0022, label: 1, bag_size: 100\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 37\n",
      "batch 499, loss: 0.0229, instance_loss: 0.9450, weighted_loss: 0.2996, label: 0, bag_size: 61\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 30\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 31\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0019, weighted_loss: 0.0007, label: 0, bag_size: 64\n",
      "batch 579, loss: 0.0011, instance_loss: 0.0997, weighted_loss: 0.0306, label: 0, bag_size: 28\n",
      "batch 599, loss: 0.0552, instance_loss: 0.1050, weighted_loss: 0.0701, label: 0, bag_size: 51\n",
      "batch 619, loss: 1.8751, instance_loss: 1.5829, weighted_loss: 1.7874, label: 0, bag_size: 66\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 0, bag_size: 46\n",
      "batch 659, loss: 0.0007, instance_loss: 0.0438, weighted_loss: 0.0137, label: 1, bag_size: 27\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9757575757575757: correct 10304/10560\n",
      "class 1 clustering acc 0.8875: correct 4686/5280\n",
      "Epoch: 45, train_loss: 0.3946, train_clustering_loss:  0.2241, train_error: 0.1000\n",
      "class 0: acc 0.899390243902439, correct 295/328\n",
      "class 1: acc 0.9006024096385542, correct 299/332\n",
      "\n",
      "Val Set, val_loss: 0.9027, val_error: 0.2027, auc: 0.9022\n",
      "class 0 clustering acc 0.8581081081081081: correct 1016/1184\n",
      "class 1 clustering acc 0.7297297297297297: correct 432/592\n",
      "class 0: acc 0.8235294117647058, correct 28/34\n",
      "class 1: acc 0.775, correct 31/40\n",
      "EarlyStopping counter: 35 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 4.4406, instance_loss: 2.6481, weighted_loss: 3.9029, label: 0, bag_size: 35\n",
      "batch 39, loss: 0.0000, instance_loss: 1.2818, weighted_loss: 0.3845, label: 1, bag_size: 23\n",
      "batch 59, loss: 0.4900, instance_loss: 0.6655, weighted_loss: 0.5426, label: 1, bag_size: 40\n",
      "batch 79, loss: 0.0240, instance_loss: 0.4330, weighted_loss: 0.1467, label: 1, bag_size: 24\n",
      "batch 99, loss: 0.0002, instance_loss: 0.0444, weighted_loss: 0.0135, label: 0, bag_size: 78\n",
      "batch 119, loss: 0.5082, instance_loss: 0.0215, weighted_loss: 0.3622, label: 0, bag_size: 122\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 1, bag_size: 84\n",
      "batch 159, loss: 0.0000, instance_loss: 0.4485, weighted_loss: 0.1345, label: 0, bag_size: 82\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 0, bag_size: 65\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0105, weighted_loss: 0.0032, label: 0, bag_size: 76\n",
      "batch 219, loss: 5.9759, instance_loss: 2.9676, weighted_loss: 5.0734, label: 0, bag_size: 60\n",
      "batch 239, loss: 0.0149, instance_loss: 0.0115, weighted_loss: 0.0139, label: 0, bag_size: 92\n",
      "batch 259, loss: 1.8660, instance_loss: 0.1529, weighted_loss: 1.3521, label: 0, bag_size: 29\n",
      "batch 279, loss: 0.0450, instance_loss: 0.0057, weighted_loss: 0.0332, label: 1, bag_size: 30\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 67\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0055, weighted_loss: 0.0017, label: 1, bag_size: 57\n",
      "batch 339, loss: 0.0032, instance_loss: 0.2896, weighted_loss: 0.0891, label: 0, bag_size: 38\n",
      "batch 359, loss: 0.0007, instance_loss: 0.0213, weighted_loss: 0.0069, label: 0, bag_size: 54\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 110\n",
      "batch 399, loss: 0.0003, instance_loss: 0.1682, weighted_loss: 0.0507, label: 0, bag_size: 90\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 67\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0011, weighted_loss: 0.0004, label: 1, bag_size: 77\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 103\n",
      "batch 479, loss: 0.0475, instance_loss: 1.9566, weighted_loss: 0.6202, label: 1, bag_size: 63\n",
      "batch 499, loss: 0.0008, instance_loss: 0.0260, weighted_loss: 0.0083, label: 0, bag_size: 27\n",
      "batch 519, loss: 0.0776, instance_loss: 0.6170, weighted_loss: 0.2394, label: 1, bag_size: 24\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0448, weighted_loss: 0.0134, label: 1, bag_size: 17\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0680, weighted_loss: 0.0204, label: 1, bag_size: 73\n",
      "batch 579, loss: 0.0003, instance_loss: 0.1565, weighted_loss: 0.0472, label: 1, bag_size: 38\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 37\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0227, weighted_loss: 0.0068, label: 0, bag_size: 28\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0113, weighted_loss: 0.0034, label: 1, bag_size: 85\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0036, weighted_loss: 0.0011, label: 0, bag_size: 77\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9608901515151516: correct 10147/10560\n",
      "class 1 clustering acc 0.8632575757575758: correct 4558/5280\n",
      "Epoch: 46, train_loss: 0.4458, train_clustering_loss:  0.2963, train_error: 0.0955\n",
      "class 0: acc 0.9034267912772586, correct 290/321\n",
      "class 1: acc 0.9056047197640118, correct 307/339\n",
      "\n",
      "Val Set, val_loss: 1.3706, val_error: 0.2027, auc: 0.8893\n",
      "class 0 clustering acc 0.808277027027027: correct 957/1184\n",
      "class 1 clustering acc 0.7162162162162162: correct 424/592\n",
      "class 0: acc 0.7647058823529411, correct 26/34\n",
      "class 1: acc 0.825, correct 33/40\n",
      "EarlyStopping counter: 36 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0040, weighted_loss: 0.0012, label: 1, bag_size: 53\n",
      "batch 39, loss: 0.0015, instance_loss: 0.7007, weighted_loss: 0.2113, label: 0, bag_size: 93\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0154, weighted_loss: 0.0046, label: 0, bag_size: 23\n",
      "batch 79, loss: 10.8799, instance_loss: 2.3929, weighted_loss: 8.3338, label: 0, bag_size: 65\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0475, weighted_loss: 0.0143, label: 0, bag_size: 96\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0250, weighted_loss: 0.0075, label: 0, bag_size: 95\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0168, weighted_loss: 0.0050, label: 1, bag_size: 32\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0017, weighted_loss: 0.0005, label: 0, bag_size: 21\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0400, weighted_loss: 0.0120, label: 0, bag_size: 90\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0091, weighted_loss: 0.0028, label: 1, bag_size: 109\n",
      "batch 219, loss: 0.0005, instance_loss: 0.0808, weighted_loss: 0.0246, label: 1, bag_size: 36\n",
      "batch 239, loss: 0.0012, instance_loss: 1.2939, weighted_loss: 0.3890, label: 1, bag_size: 96\n",
      "batch 259, loss: 0.9024, instance_loss: 0.1938, weighted_loss: 0.6899, label: 1, bag_size: 94\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0083, weighted_loss: 0.0025, label: 1, bag_size: 80\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0047, weighted_loss: 0.0014, label: 1, bag_size: 126\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0021, weighted_loss: 0.0006, label: 0, bag_size: 26\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 114\n",
      "batch 359, loss: 1.2513, instance_loss: 0.4388, weighted_loss: 1.0076, label: 0, bag_size: 78\n",
      "batch 379, loss: 9.4977, instance_loss: 0.6702, weighted_loss: 6.8494, label: 0, bag_size: 48\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0092, weighted_loss: 0.0028, label: 1, bag_size: 107\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0326, weighted_loss: 0.0098, label: 0, bag_size: 67\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0485, weighted_loss: 0.0145, label: 1, bag_size: 77\n",
      "batch 459, loss: 0.0543, instance_loss: 2.1230, weighted_loss: 0.6749, label: 1, bag_size: 31\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 60\n",
      "batch 499, loss: 0.0000, instance_loss: 0.3585, weighted_loss: 0.1075, label: 0, bag_size: 72\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 53\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0061, weighted_loss: 0.0018, label: 0, bag_size: 66\n",
      "batch 559, loss: 15.6993, instance_loss: 7.1985, weighted_loss: 13.1491, label: 1, bag_size: 25\n",
      "batch 579, loss: 0.2157, instance_loss: 0.0117, weighted_loss: 0.1545, label: 1, bag_size: 27\n",
      "batch 599, loss: 0.0000, instance_loss: 0.1717, weighted_loss: 0.0515, label: 0, bag_size: 37\n",
      "batch 619, loss: 3.2299, instance_loss: 2.4728, weighted_loss: 3.0028, label: 0, bag_size: 94\n",
      "batch 639, loss: 0.0123, instance_loss: 0.1776, weighted_loss: 0.0618, label: 1, bag_size: 47\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0027, weighted_loss: 0.0008, label: 1, bag_size: 38\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9660984848484848: correct 10202/10560\n",
      "class 1 clustering acc 0.8770833333333333: correct 4631/5280\n",
      "Epoch: 47, train_loss: 0.6102, train_clustering_loss:  0.2747, train_error: 0.1076\n",
      "class 0: acc 0.8878787878787879, correct 293/330\n",
      "class 1: acc 0.896969696969697, correct 296/330\n",
      "\n",
      "Val Set, val_loss: 1.1464, val_error: 0.2027, auc: 0.9217\n",
      "class 0 clustering acc 0.9349662162162162: correct 1107/1184\n",
      "class 1 clustering acc 0.7652027027027027: correct 453/592\n",
      "class 0: acc 0.8235294117647058, correct 28/34\n",
      "class 1: acc 0.775, correct 31/40\n",
      "EarlyStopping counter: 37 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0016, instance_loss: 0.0062, weighted_loss: 0.0030, label: 0, bag_size: 26\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0035, weighted_loss: 0.0010, label: 0, bag_size: 28\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 40\n",
      "batch 79, loss: 1.5878, instance_loss: 0.0970, weighted_loss: 1.1406, label: 1, bag_size: 57\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 115\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0062, weighted_loss: 0.0019, label: 0, bag_size: 49\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 1, bag_size: 82\n",
      "batch 159, loss: 0.0000, instance_loss: 0.1668, weighted_loss: 0.0501, label: 1, bag_size: 47\n",
      "batch 179, loss: 0.0069, instance_loss: 0.1037, weighted_loss: 0.0359, label: 0, bag_size: 69\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0050, weighted_loss: 0.0015, label: 1, bag_size: 40\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 65\n",
      "batch 239, loss: 0.0000, instance_loss: 0.2762, weighted_loss: 0.0829, label: 1, bag_size: 87\n",
      "batch 259, loss: 1.2438, instance_loss: 1.9519, weighted_loss: 1.4562, label: 0, bag_size: 78\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0017, weighted_loss: 0.0005, label: 0, bag_size: 44\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0163, weighted_loss: 0.0049, label: 0, bag_size: 66\n",
      "batch 319, loss: 16.9284, instance_loss: 1.3214, weighted_loss: 12.2463, label: 1, bag_size: 28\n",
      "batch 339, loss: 0.0149, instance_loss: 0.6077, weighted_loss: 0.1927, label: 1, bag_size: 100\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 32\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 107\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 30\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0213, weighted_loss: 0.0064, label: 0, bag_size: 91\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0025, weighted_loss: 0.0008, label: 0, bag_size: 52\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0019, weighted_loss: 0.0006, label: 1, bag_size: 33\n",
      "batch 479, loss: 0.0002, instance_loss: 0.0026, weighted_loss: 0.0009, label: 0, bag_size: 28\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 103\n",
      "batch 519, loss: 0.0000, instance_loss: 0.4089, weighted_loss: 0.1227, label: 0, bag_size: 110\n",
      "batch 539, loss: 0.4603, instance_loss: 1.4447, weighted_loss: 0.7556, label: 0, bag_size: 102\n",
      "batch 559, loss: 4.6178, instance_loss: 3.0262, weighted_loss: 4.1403, label: 1, bag_size: 42\n",
      "batch 579, loss: 0.0001, instance_loss: 0.4879, weighted_loss: 0.1464, label: 1, bag_size: 126\n",
      "batch 599, loss: 0.0017, instance_loss: 0.0792, weighted_loss: 0.0249, label: 1, bag_size: 31\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0149, weighted_loss: 0.0045, label: 1, bag_size: 84\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 42\n",
      "batch 659, loss: 0.0034, instance_loss: 0.2306, weighted_loss: 0.0716, label: 1, bag_size: 39\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.968560606060606: correct 10228/10560\n",
      "class 1 clustering acc 0.8626893939393939: correct 4555/5280\n",
      "Epoch: 48, train_loss: 0.4739, train_clustering_loss:  0.3273, train_error: 0.0955\n",
      "class 0: acc 0.8984615384615384, correct 292/325\n",
      "class 1: acc 0.9104477611940298, correct 305/335\n",
      "\n",
      "Val Set, val_loss: 1.3317, val_error: 0.2568, auc: 0.8926\n",
      "class 0 clustering acc 0.9054054054054054: correct 1072/1184\n",
      "class 1 clustering acc 0.6469594594594594: correct 383/592\n",
      "class 0: acc 0.8235294117647058, correct 28/34\n",
      "class 1: acc 0.675, correct 27/40\n",
      "EarlyStopping counter: 38 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0035, weighted_loss: 0.0011, label: 1, bag_size: 120\n",
      "batch 39, loss: 0.0022, instance_loss: 0.0218, weighted_loss: 0.0081, label: 0, bag_size: 25\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 35\n",
      "batch 79, loss: 0.0000, instance_loss: 0.2247, weighted_loss: 0.0674, label: 1, bag_size: 85\n",
      "batch 99, loss: 1.7632, instance_loss: 1.7068, weighted_loss: 1.7462, label: 0, bag_size: 46\n",
      "batch 119, loss: 0.0008, instance_loss: 0.0121, weighted_loss: 0.0042, label: 1, bag_size: 86\n",
      "batch 139, loss: 0.0238, instance_loss: 0.3029, weighted_loss: 0.1075, label: 1, bag_size: 68\n",
      "batch 159, loss: 0.0505, instance_loss: 0.0153, weighted_loss: 0.0399, label: 0, bag_size: 37\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0413, weighted_loss: 0.0124, label: 0, bag_size: 63\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 44\n",
      "batch 219, loss: 0.0000, instance_loss: 0.1012, weighted_loss: 0.0304, label: 0, bag_size: 79\n",
      "batch 239, loss: 0.0013, instance_loss: 0.0149, weighted_loss: 0.0054, label: 1, bag_size: 79\n",
      "batch 259, loss: 0.0000, instance_loss: 0.2047, weighted_loss: 0.0614, label: 1, bag_size: 38\n",
      "batch 279, loss: 0.0001, instance_loss: 0.5006, weighted_loss: 0.1503, label: 1, bag_size: 92\n",
      "batch 299, loss: 0.0000, instance_loss: 0.6066, weighted_loss: 0.1820, label: 1, bag_size: 36\n",
      "batch 319, loss: 0.0051, instance_loss: 0.4834, weighted_loss: 0.1486, label: 1, bag_size: 92\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0400, weighted_loss: 0.0120, label: 1, bag_size: 83\n",
      "batch 359, loss: 0.0334, instance_loss: 0.0385, weighted_loss: 0.0349, label: 1, bag_size: 60\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0743, weighted_loss: 0.0223, label: 0, bag_size: 122\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0784, weighted_loss: 0.0235, label: 1, bag_size: 32\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 85\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 88\n",
      "batch 459, loss: 0.0000, instance_loss: 0.2849, weighted_loss: 0.0855, label: 0, bag_size: 27\n",
      "batch 479, loss: 0.0013, instance_loss: 0.0254, weighted_loss: 0.0085, label: 1, bag_size: 24\n",
      "batch 499, loss: 0.0018, instance_loss: 0.0807, weighted_loss: 0.0255, label: 0, bag_size: 88\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 48\n",
      "batch 539, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 103\n",
      "batch 559, loss: 0.0007, instance_loss: 0.1501, weighted_loss: 0.0455, label: 0, bag_size: 96\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0179, weighted_loss: 0.0054, label: 1, bag_size: 45\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0036, weighted_loss: 0.0011, label: 1, bag_size: 88\n",
      "batch 619, loss: 0.0011, instance_loss: 0.0586, weighted_loss: 0.0184, label: 1, bag_size: 31\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0107, weighted_loss: 0.0033, label: 1, bag_size: 77\n",
      "batch 659, loss: 0.0001, instance_loss: 0.0039, weighted_loss: 0.0013, label: 1, bag_size: 69\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9677083333333333: correct 10219/10560\n",
      "class 1 clustering acc 0.8454545454545455: correct 4464/5280\n",
      "Epoch: 49, train_loss: 0.4310, train_clustering_loss:  0.3064, train_error: 0.0879\n",
      "class 0: acc 0.90625, correct 290/320\n",
      "class 1: acc 0.9176470588235294, correct 312/340\n",
      "\n",
      "Val Set, val_loss: 0.9172, val_error: 0.2162, auc: 0.8956\n",
      "class 0 clustering acc 0.9349662162162162: correct 1107/1184\n",
      "class 1 clustering acc 0.722972972972973: correct 428/592\n",
      "class 0: acc 0.7058823529411765, correct 24/34\n",
      "class 1: acc 0.85, correct 34/40\n",
      "EarlyStopping counter: 39 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0087, weighted_loss: 0.0026, label: 0, bag_size: 64\n",
      "batch 39, loss: 0.0029, instance_loss: 0.0012, weighted_loss: 0.0024, label: 0, bag_size: 34\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0145, weighted_loss: 0.0044, label: 1, bag_size: 105\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 33\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 49\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 76\n",
      "batch 139, loss: 0.0000, instance_loss: 0.2379, weighted_loss: 0.0714, label: 1, bag_size: 53\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0251, weighted_loss: 0.0075, label: 0, bag_size: 95\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 86\n",
      "batch 199, loss: 0.0070, instance_loss: 0.6070, weighted_loss: 0.1870, label: 0, bag_size: 17\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 45\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0496, weighted_loss: 0.0149, label: 0, bag_size: 164\n",
      "batch 259, loss: 0.2352, instance_loss: 0.0127, weighted_loss: 0.1684, label: 1, bag_size: 86\n",
      "batch 279, loss: 0.0044, instance_loss: 0.0048, weighted_loss: 0.0045, label: 0, bag_size: 92\n",
      "batch 299, loss: 0.0001, instance_loss: 0.1119, weighted_loss: 0.0336, label: 0, bag_size: 68\n",
      "batch 319, loss: 0.0459, instance_loss: 0.4192, weighted_loss: 0.1579, label: 0, bag_size: 28\n",
      "batch 339, loss: 0.0040, instance_loss: 0.0042, weighted_loss: 0.0041, label: 0, bag_size: 103\n",
      "batch 359, loss: 0.3450, instance_loss: 0.0142, weighted_loss: 0.2458, label: 0, bag_size: 50\n",
      "batch 379, loss: 0.0145, instance_loss: 0.0005, weighted_loss: 0.0103, label: 0, bag_size: 88\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 0, bag_size: 79\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0891, weighted_loss: 0.0267, label: 1, bag_size: 14\n",
      "batch 439, loss: 11.7047, instance_loss: 0.1143, weighted_loss: 8.2276, label: 1, bag_size: 65\n",
      "batch 459, loss: 0.0000, instance_loss: 0.1872, weighted_loss: 0.0562, label: 1, bag_size: 76\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0247, weighted_loss: 0.0074, label: 1, bag_size: 153\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 83\n",
      "batch 519, loss: 0.0000, instance_loss: 0.7200, weighted_loss: 0.2160, label: 1, bag_size: 76\n",
      "batch 539, loss: 0.0000, instance_loss: 0.2539, weighted_loss: 0.0762, label: 1, bag_size: 112\n",
      "batch 559, loss: 0.0000, instance_loss: 0.1723, weighted_loss: 0.0517, label: 1, bag_size: 35\n",
      "batch 579, loss: 0.0000, instance_loss: 0.1171, weighted_loss: 0.0351, label: 1, bag_size: 62\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0378, weighted_loss: 0.0113, label: 1, bag_size: 32\n",
      "batch 619, loss: 0.0002, instance_loss: 0.0175, weighted_loss: 0.0054, label: 1, bag_size: 76\n",
      "batch 639, loss: 0.0000, instance_loss: 1.0978, weighted_loss: 0.3294, label: 0, bag_size: 43\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 35\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9662878787878788: correct 10204/10560\n",
      "class 1 clustering acc 0.8583333333333333: correct 4532/5280\n",
      "Epoch: 50, train_loss: 0.3238, train_clustering_loss:  0.2678, train_error: 0.0924\n",
      "class 0: acc 0.9054878048780488, correct 297/328\n",
      "class 1: acc 0.9096385542168675, correct 302/332\n",
      "\n",
      "Val Set, val_loss: 1.0233, val_error: 0.2027, auc: 0.9132\n",
      "class 0 clustering acc 0.9265202702702703: correct 1097/1184\n",
      "class 1 clustering acc 0.7837837837837838: correct 464/592\n",
      "class 0: acc 0.7941176470588235, correct 27/34\n",
      "class 1: acc 0.8, correct 32/40\n",
      "EarlyStopping counter: 40 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0047, weighted_loss: 0.0014, label: 1, bag_size: 136\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0942, weighted_loss: 0.0283, label: 1, bag_size: 77\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 1, bag_size: 56\n",
      "batch 79, loss: 0.2049, instance_loss: 2.2139, weighted_loss: 0.8076, label: 0, bag_size: 48\n",
      "batch 99, loss: 2.0709, instance_loss: 0.8105, weighted_loss: 1.6928, label: 1, bag_size: 59\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 1, bag_size: 84\n",
      "batch 139, loss: 0.0023, instance_loss: 0.0743, weighted_loss: 0.0239, label: 0, bag_size: 51\n",
      "batch 159, loss: 0.0021, instance_loss: 0.1699, weighted_loss: 0.0524, label: 0, bag_size: 20\n",
      "batch 179, loss: 0.0569, instance_loss: 0.5431, weighted_loss: 0.2027, label: 1, bag_size: 51\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 0, bag_size: 66\n",
      "batch 219, loss: 0.0004, instance_loss: 0.0139, weighted_loss: 0.0044, label: 1, bag_size: 64\n",
      "batch 239, loss: 0.0737, instance_loss: 0.1752, weighted_loss: 0.1041, label: 0, bag_size: 110\n",
      "batch 259, loss: 0.0005, instance_loss: 0.0121, weighted_loss: 0.0040, label: 1, bag_size: 18\n",
      "batch 279, loss: 0.0003, instance_loss: 0.0067, weighted_loss: 0.0022, label: 0, bag_size: 49\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 83\n",
      "batch 319, loss: 0.0000, instance_loss: 0.3206, weighted_loss: 0.0962, label: 0, bag_size: 67\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 73\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 0, bag_size: 37\n",
      "batch 379, loss: 0.0001, instance_loss: 0.0989, weighted_loss: 0.0297, label: 1, bag_size: 73\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0248, weighted_loss: 0.0074, label: 0, bag_size: 84\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0025, weighted_loss: 0.0008, label: 0, bag_size: 89\n",
      "batch 439, loss: 0.0015, instance_loss: 0.9500, weighted_loss: 0.2860, label: 0, bag_size: 28\n",
      "batch 459, loss: 0.0038, instance_loss: 0.0419, weighted_loss: 0.0152, label: 1, bag_size: 63\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 0, bag_size: 72\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0422, weighted_loss: 0.0127, label: 0, bag_size: 83\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0125, weighted_loss: 0.0038, label: 1, bag_size: 115\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0149, weighted_loss: 0.0045, label: 1, bag_size: 53\n",
      "batch 559, loss: 0.0666, instance_loss: 0.2683, weighted_loss: 0.1271, label: 0, bag_size: 29\n",
      "batch 579, loss: 0.0008, instance_loss: 0.0414, weighted_loss: 0.0130, label: 1, bag_size: 110\n",
      "batch 599, loss: 0.0002, instance_loss: 0.0156, weighted_loss: 0.0048, label: 1, bag_size: 30\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0164, weighted_loss: 0.0050, label: 0, bag_size: 41\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 86\n",
      "batch 659, loss: 0.0003, instance_loss: 0.0034, weighted_loss: 0.0013, label: 1, bag_size: 36\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9723484848484848: correct 10268/10560\n",
      "class 1 clustering acc 0.8926136363636363: correct 4713/5280\n",
      "Epoch: 51, train_loss: 0.2990, train_clustering_loss:  0.2352, train_error: 0.0848\n",
      "class 0: acc 0.9134328358208955, correct 306/335\n",
      "class 1: acc 0.916923076923077, correct 298/325\n",
      "\n",
      "Val Set, val_loss: 0.8361, val_error: 0.1622, auc: 0.9162\n",
      "class 0 clustering acc 0.8775337837837838: correct 1039/1184\n",
      "class 1 clustering acc 0.7635135135135135: correct 452/592\n",
      "class 0: acc 0.7941176470588235, correct 27/34\n",
      "class 1: acc 0.875, correct 35/40\n",
      "EarlyStopping counter: 41 out of 20\n",
      "Early stopping\n",
      "Val error: 0.1622, ROC AUC: 0.9287\n",
      "Test error: 0.2660, ROC AUC: 0.8495\n",
      "class 0: acc 0.6585365853658537, correct 27/41\n",
      "class 1: acc 0.7924528301886793, correct 42/53\n",
      "\n",
      "Training Fold 4!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 650 samples\n",
      "Validating on 93 samples\n",
      "Testing on 85 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Setting tau to 1.0\n",
      "Done!\n",
      "MCBAT_SB(\n",
      "  (instance_loss_fn): SmoothTop1SVM()\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (transformer_low): TransformerEncoder_FLASHAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FlashAttention(\n",
      "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
      "            (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer_high): TransformerEncoder_FLASHAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FlashAttention(\n",
      "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_kv): Linear(in_features=512, out_features=1024, bias=False)\n",
      "            (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attention): Attn_Net_Gated(\n",
      "    (attention_a): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_b): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (attention_V2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (attention_U2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (attention_weights2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Total number of parameters: 8408073\n",
      "Total number of trainable parameters: 8408073\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "650.0\n",
      "2\n",
      "303\n",
      "347\n",
      "##################################################\n",
      "Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 8.0528, instance_loss: 1.3134, weighted_loss: 6.0310, label: 0, bag_size: 42\n",
      "batch 39, loss: 0.0898, instance_loss: 0.6508, weighted_loss: 0.2581, label: 0, bag_size: 57\n",
      "batch 59, loss: 2.3201, instance_loss: 0.9289, weighted_loss: 1.9027, label: 1, bag_size: 53\n",
      "batch 79, loss: 0.0814, instance_loss: 1.1329, weighted_loss: 0.3969, label: 1, bag_size: 70\n",
      "batch 99, loss: 1.7745, instance_loss: 0.9258, weighted_loss: 1.5199, label: 0, bag_size: 103\n",
      "batch 119, loss: 4.1255, instance_loss: 0.9408, weighted_loss: 3.1701, label: 0, bag_size: 78\n",
      "batch 139, loss: 0.1311, instance_loss: 0.9001, weighted_loss: 0.3618, label: 0, bag_size: 63\n",
      "batch 159, loss: 3.7246, instance_loss: 1.4748, weighted_loss: 3.0497, label: 0, bag_size: 43\n",
      "batch 179, loss: 1.3891, instance_loss: 1.2230, weighted_loss: 1.3393, label: 0, bag_size: 95\n",
      "batch 199, loss: 0.6490, instance_loss: 1.0844, weighted_loss: 0.7796, label: 1, bag_size: 83\n",
      "batch 219, loss: 0.7189, instance_loss: 0.8643, weighted_loss: 0.7625, label: 0, bag_size: 49\n",
      "batch 239, loss: 1.2332, instance_loss: 1.3476, weighted_loss: 1.2675, label: 1, bag_size: 46\n",
      "batch 259, loss: 0.1313, instance_loss: 0.7893, weighted_loss: 0.3287, label: 1, bag_size: 89\n",
      "batch 279, loss: 0.3279, instance_loss: 1.0127, weighted_loss: 0.5334, label: 1, bag_size: 76\n",
      "batch 299, loss: 0.2826, instance_loss: 1.3541, weighted_loss: 0.6040, label: 0, bag_size: 76\n",
      "batch 319, loss: 0.5229, instance_loss: 0.8199, weighted_loss: 0.6120, label: 1, bag_size: 79\n",
      "batch 339, loss: 3.4901, instance_loss: 1.0072, weighted_loss: 2.7452, label: 0, bag_size: 35\n",
      "batch 359, loss: 0.7312, instance_loss: 1.5292, weighted_loss: 0.9706, label: 1, bag_size: 84\n",
      "batch 379, loss: 0.6737, instance_loss: 0.8271, weighted_loss: 0.7197, label: 1, bag_size: 13\n",
      "batch 399, loss: 2.9347, instance_loss: 0.8257, weighted_loss: 2.3020, label: 0, bag_size: 28\n",
      "batch 419, loss: 0.1871, instance_loss: 1.1095, weighted_loss: 0.4638, label: 0, bag_size: 112\n",
      "batch 439, loss: 0.7919, instance_loss: 0.9629, weighted_loss: 0.8432, label: 0, bag_size: 25\n",
      "batch 459, loss: 0.2291, instance_loss: 0.8305, weighted_loss: 0.4095, label: 1, bag_size: 100\n",
      "batch 479, loss: 2.2178, instance_loss: 1.2295, weighted_loss: 1.9213, label: 0, bag_size: 38\n",
      "batch 499, loss: 0.1665, instance_loss: 1.0174, weighted_loss: 0.4218, label: 1, bag_size: 74\n",
      "batch 519, loss: 0.4152, instance_loss: 1.0831, weighted_loss: 0.6156, label: 1, bag_size: 56\n",
      "batch 539, loss: 1.3355, instance_loss: 0.9905, weighted_loss: 1.2320, label: 1, bag_size: 39\n",
      "batch 559, loss: 0.4482, instance_loss: 1.2617, weighted_loss: 0.6923, label: 0, bag_size: 96\n",
      "batch 579, loss: 1.5345, instance_loss: 0.9322, weighted_loss: 1.3538, label: 1, bag_size: 37\n",
      "batch 599, loss: 0.3761, instance_loss: 1.0939, weighted_loss: 0.5914, label: 1, bag_size: 61\n",
      "batch 619, loss: 2.2302, instance_loss: 1.4242, weighted_loss: 1.9884, label: 1, bag_size: 88\n",
      "batch 639, loss: 1.9566, instance_loss: 1.3758, weighted_loss: 1.7824, label: 0, bag_size: 18\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9810576923076924: correct 10203/10400\n",
      "class 1 clustering acc 0.036730769230769234: correct 191/5200\n",
      "Epoch: 0, train_loss: 1.0645, train_clustering_loss:  1.0090, train_error: 0.4662\n",
      "class 0: acc 0.5401234567901234, correct 175/324\n",
      "class 1: acc 0.5276073619631901, correct 172/326\n",
      "\n",
      "Val Set, val_loss: 0.5500, val_error: 0.3441, auc: 0.9089\n",
      "class 0 clustering acc 0.9973118279569892: correct 1484/1488\n",
      "class 1 clustering acc 0.08064516129032258: correct 60/744\n",
      "class 0: acc 0.9361702127659575, correct 44/47\n",
      "class 1: acc 0.3695652173913043, correct 17/46\n",
      "Validation loss decreased (inf --> 0.550013).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5063, instance_loss: 0.9531, weighted_loss: 0.6403, label: 0, bag_size: 97\n",
      "batch 39, loss: 0.0807, instance_loss: 0.9902, weighted_loss: 0.3535, label: 1, bag_size: 44\n",
      "batch 59, loss: 0.0322, instance_loss: 0.7741, weighted_loss: 0.2547, label: 0, bag_size: 107\n",
      "batch 79, loss: 0.5809, instance_loss: 0.7896, weighted_loss: 0.6435, label: 0, bag_size: 34\n",
      "batch 99, loss: 0.0267, instance_loss: 1.0327, weighted_loss: 0.3285, label: 1, bag_size: 95\n",
      "batch 119, loss: 1.0988, instance_loss: 0.9310, weighted_loss: 1.0485, label: 0, bag_size: 95\n",
      "batch 139, loss: 0.0979, instance_loss: 1.0051, weighted_loss: 0.3701, label: 0, bag_size: 77\n",
      "batch 159, loss: 2.2761, instance_loss: 1.1285, weighted_loss: 1.9318, label: 1, bag_size: 35\n",
      "batch 179, loss: 0.3265, instance_loss: 1.0574, weighted_loss: 0.5458, label: 0, bag_size: 72\n",
      "batch 199, loss: 0.7493, instance_loss: 0.7757, weighted_loss: 0.7572, label: 1, bag_size: 79\n",
      "batch 219, loss: 1.1746, instance_loss: 1.3857, weighted_loss: 1.2379, label: 1, bag_size: 43\n",
      "batch 239, loss: 0.6559, instance_loss: 0.8109, weighted_loss: 0.7024, label: 0, bag_size: 104\n",
      "batch 259, loss: 1.8496, instance_loss: 1.2197, weighted_loss: 1.6606, label: 1, bag_size: 78\n",
      "batch 279, loss: 0.8026, instance_loss: 0.6939, weighted_loss: 0.7700, label: 0, bag_size: 88\n",
      "batch 299, loss: 1.2912, instance_loss: 0.5723, weighted_loss: 1.0756, label: 1, bag_size: 85\n",
      "batch 319, loss: 0.1536, instance_loss: 0.2794, weighted_loss: 0.1913, label: 0, bag_size: 37\n",
      "batch 339, loss: 1.2401, instance_loss: 2.6638, weighted_loss: 1.6672, label: 1, bag_size: 62\n",
      "batch 359, loss: 0.7228, instance_loss: 0.8351, weighted_loss: 0.7565, label: 0, bag_size: 43\n",
      "batch 379, loss: 0.8651, instance_loss: 1.0838, weighted_loss: 0.9307, label: 0, bag_size: 86\n",
      "batch 399, loss: 0.4227, instance_loss: 0.8283, weighted_loss: 0.5444, label: 1, bag_size: 86\n",
      "batch 419, loss: 0.4046, instance_loss: 1.2395, weighted_loss: 0.6550, label: 0, bag_size: 92\n",
      "batch 439, loss: 0.2781, instance_loss: 0.6440, weighted_loss: 0.3879, label: 1, bag_size: 22\n",
      "batch 459, loss: 0.2266, instance_loss: 0.4311, weighted_loss: 0.2880, label: 1, bag_size: 38\n",
      "batch 479, loss: 0.3545, instance_loss: 0.8016, weighted_loss: 0.4886, label: 1, bag_size: 75\n",
      "batch 499, loss: 0.1329, instance_loss: 1.2563, weighted_loss: 0.4699, label: 0, bag_size: 25\n",
      "batch 519, loss: 0.0287, instance_loss: 0.6357, weighted_loss: 0.2108, label: 1, bag_size: 41\n",
      "batch 539, loss: 0.1439, instance_loss: 0.9439, weighted_loss: 0.3839, label: 1, bag_size: 46\n",
      "batch 559, loss: 0.2577, instance_loss: 0.6500, weighted_loss: 0.3754, label: 1, bag_size: 42\n",
      "batch 579, loss: 1.2934, instance_loss: 1.5230, weighted_loss: 1.3622, label: 0, bag_size: 41\n",
      "batch 599, loss: 0.2618, instance_loss: 0.4952, weighted_loss: 0.3318, label: 0, bag_size: 40\n",
      "batch 619, loss: 0.9986, instance_loss: 0.7778, weighted_loss: 0.9323, label: 1, bag_size: 35\n",
      "batch 639, loss: 1.5286, instance_loss: 1.0293, weighted_loss: 1.3788, label: 1, bag_size: 56\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9594230769230769: correct 9978/10400\n",
      "class 1 clustering acc 0.14826923076923076: correct 771/5200\n",
      "Epoch: 1, train_loss: 0.7816, train_clustering_loss:  0.9297, train_error: 0.3646\n",
      "class 0: acc 0.6230031948881789, correct 195/313\n",
      "class 1: acc 0.6468842729970327, correct 218/337\n",
      "\n",
      "Val Set, val_loss: 0.4046, val_error: 0.1398, auc: 0.9251\n",
      "class 0 clustering acc 0.8514784946236559: correct 1267/1488\n",
      "class 1 clustering acc 0.4852150537634409: correct 361/744\n",
      "class 0: acc 0.8297872340425532, correct 39/47\n",
      "class 1: acc 0.8913043478260869, correct 41/46\n",
      "Validation loss decreased (0.550013 --> 0.404573).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3558, instance_loss: 0.8807, weighted_loss: 0.5133, label: 0, bag_size: 78\n",
      "batch 39, loss: 2.3822, instance_loss: 0.8659, weighted_loss: 1.9273, label: 1, bag_size: 25\n",
      "batch 59, loss: 0.1701, instance_loss: 0.8674, weighted_loss: 0.3793, label: 0, bag_size: 65\n",
      "batch 79, loss: 1.7569, instance_loss: 1.0151, weighted_loss: 1.5343, label: 1, bag_size: 82\n",
      "batch 99, loss: 0.5209, instance_loss: 0.7656, weighted_loss: 0.5943, label: 0, bag_size: 78\n",
      "batch 119, loss: 2.6568, instance_loss: 0.7739, weighted_loss: 2.0919, label: 1, bag_size: 67\n",
      "batch 139, loss: 1.8834, instance_loss: 1.1523, weighted_loss: 1.6641, label: 1, bag_size: 31\n",
      "batch 159, loss: 0.1535, instance_loss: 0.8701, weighted_loss: 0.3685, label: 0, bag_size: 35\n",
      "batch 179, loss: 0.0188, instance_loss: 0.6154, weighted_loss: 0.1978, label: 1, bag_size: 91\n",
      "batch 199, loss: 0.1178, instance_loss: 0.9617, weighted_loss: 0.3710, label: 0, bag_size: 72\n",
      "batch 219, loss: 0.2946, instance_loss: 1.3613, weighted_loss: 0.6146, label: 1, bag_size: 30\n",
      "batch 239, loss: 0.7423, instance_loss: 1.0159, weighted_loss: 0.8244, label: 0, bag_size: 92\n",
      "batch 259, loss: 0.0033, instance_loss: 0.5371, weighted_loss: 0.1634, label: 1, bag_size: 26\n",
      "batch 279, loss: 0.3079, instance_loss: 0.8023, weighted_loss: 0.4562, label: 0, bag_size: 61\n",
      "batch 299, loss: 0.5502, instance_loss: 1.0657, weighted_loss: 0.7048, label: 0, bag_size: 50\n",
      "batch 319, loss: 0.1129, instance_loss: 0.9033, weighted_loss: 0.3500, label: 1, bag_size: 56\n",
      "batch 339, loss: 0.7038, instance_loss: 0.7716, weighted_loss: 0.7241, label: 1, bag_size: 121\n",
      "batch 359, loss: 0.0137, instance_loss: 1.0834, weighted_loss: 0.3346, label: 0, bag_size: 38\n",
      "batch 379, loss: 0.7891, instance_loss: 1.2434, weighted_loss: 0.9254, label: 1, bag_size: 24\n",
      "batch 399, loss: 4.1432, instance_loss: 1.8712, weighted_loss: 3.4616, label: 0, bag_size: 82\n",
      "batch 419, loss: 2.2574, instance_loss: 1.8195, weighted_loss: 2.1260, label: 0, bag_size: 41\n",
      "batch 439, loss: 0.0892, instance_loss: 1.0788, weighted_loss: 0.3861, label: 1, bag_size: 21\n",
      "batch 459, loss: 0.0245, instance_loss: 0.7904, weighted_loss: 0.2543, label: 1, bag_size: 53\n",
      "batch 479, loss: 1.0565, instance_loss: 1.1016, weighted_loss: 1.0700, label: 0, bag_size: 31\n",
      "batch 499, loss: 0.7433, instance_loss: 0.7879, weighted_loss: 0.7567, label: 0, bag_size: 89\n",
      "batch 519, loss: 0.7034, instance_loss: 1.7820, weighted_loss: 1.0270, label: 1, bag_size: 45\n",
      "batch 539, loss: 3.2441, instance_loss: 1.4486, weighted_loss: 2.7055, label: 1, bag_size: 51\n",
      "batch 559, loss: 0.0575, instance_loss: 0.8421, weighted_loss: 0.2929, label: 0, bag_size: 74\n",
      "batch 579, loss: 1.4169, instance_loss: 1.6902, weighted_loss: 1.4989, label: 0, bag_size: 66\n",
      "batch 599, loss: 0.2948, instance_loss: 1.3496, weighted_loss: 0.6113, label: 0, bag_size: 34\n",
      "batch 619, loss: 0.0150, instance_loss: 0.6731, weighted_loss: 0.2124, label: 1, bag_size: 88\n",
      "batch 639, loss: 0.0415, instance_loss: 0.6480, weighted_loss: 0.2234, label: 1, bag_size: 95\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9618269230769231: correct 10003/10400\n",
      "class 1 clustering acc 0.12288461538461538: correct 639/5200\n",
      "Epoch: 2, train_loss: 0.7191, train_clustering_loss:  0.9523, train_error: 0.3262\n",
      "class 0: acc 0.6493506493506493, correct 200/308\n",
      "class 1: acc 0.695906432748538, correct 238/342\n",
      "\n",
      "Val Set, val_loss: 0.3849, val_error: 0.1613, auc: 0.9241\n",
      "class 0 clustering acc 0.9375: correct 1395/1488\n",
      "class 1 clustering acc 0.06048387096774194: correct 45/744\n",
      "class 0: acc 0.851063829787234, correct 40/47\n",
      "class 1: acc 0.8260869565217391, correct 38/46\n",
      "Validation loss decreased (0.404573 --> 0.384925).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1553, instance_loss: 0.7537, weighted_loss: 0.3348, label: 0, bag_size: 65\n",
      "batch 39, loss: 1.1632, instance_loss: 1.1509, weighted_loss: 1.1595, label: 1, bag_size: 67\n",
      "batch 59, loss: 4.1214, instance_loss: 1.2747, weighted_loss: 3.2674, label: 1, bag_size: 99\n",
      "batch 79, loss: 1.8204, instance_loss: 0.8549, weighted_loss: 1.5307, label: 1, bag_size: 38\n",
      "batch 99, loss: 1.0123, instance_loss: 0.7110, weighted_loss: 0.9219, label: 1, bag_size: 47\n",
      "batch 119, loss: 0.0783, instance_loss: 0.7649, weighted_loss: 0.2842, label: 1, bag_size: 107\n",
      "batch 139, loss: 0.4507, instance_loss: 1.5453, weighted_loss: 0.7790, label: 1, bag_size: 67\n",
      "batch 159, loss: 0.0735, instance_loss: 0.7727, weighted_loss: 0.2833, label: 0, bag_size: 26\n",
      "batch 179, loss: 4.6082, instance_loss: 1.1715, weighted_loss: 3.5772, label: 0, bag_size: 24\n",
      "batch 199, loss: 0.1869, instance_loss: 0.9679, weighted_loss: 0.4212, label: 1, bag_size: 30\n",
      "batch 219, loss: 0.4147, instance_loss: 0.9865, weighted_loss: 0.5862, label: 0, bag_size: 18\n",
      "batch 239, loss: 0.0008, instance_loss: 0.7606, weighted_loss: 0.2288, label: 0, bag_size: 85\n",
      "batch 259, loss: 0.4689, instance_loss: 0.9957, weighted_loss: 0.6270, label: 0, bag_size: 48\n",
      "batch 279, loss: 0.0804, instance_loss: 1.0742, weighted_loss: 0.3785, label: 1, bag_size: 53\n",
      "batch 299, loss: 0.0133, instance_loss: 0.6795, weighted_loss: 0.2131, label: 0, bag_size: 107\n",
      "batch 319, loss: 0.0286, instance_loss: 0.7713, weighted_loss: 0.2515, label: 0, bag_size: 19\n",
      "batch 339, loss: 0.0169, instance_loss: 0.4434, weighted_loss: 0.1449, label: 1, bag_size: 76\n",
      "batch 359, loss: 0.1442, instance_loss: 0.8527, weighted_loss: 0.3567, label: 0, bag_size: 80\n",
      "batch 379, loss: 1.0363, instance_loss: 1.3425, weighted_loss: 1.1281, label: 0, bag_size: 18\n",
      "batch 399, loss: 0.2056, instance_loss: 0.7465, weighted_loss: 0.3679, label: 0, bag_size: 87\n",
      "batch 419, loss: 0.8152, instance_loss: 0.8957, weighted_loss: 0.8394, label: 1, bag_size: 31\n",
      "batch 439, loss: 0.6398, instance_loss: 1.1221, weighted_loss: 0.7845, label: 1, bag_size: 18\n",
      "batch 459, loss: 0.3178, instance_loss: 1.2915, weighted_loss: 0.6099, label: 1, bag_size: 40\n",
      "batch 479, loss: 0.8873, instance_loss: 1.1382, weighted_loss: 0.9626, label: 0, bag_size: 79\n",
      "batch 499, loss: 1.2671, instance_loss: 1.2309, weighted_loss: 1.2562, label: 1, bag_size: 20\n",
      "batch 519, loss: 1.8079, instance_loss: 0.9869, weighted_loss: 1.5616, label: 0, bag_size: 64\n",
      "batch 539, loss: 1.3450, instance_loss: 0.6961, weighted_loss: 1.1503, label: 0, bag_size: 34\n",
      "batch 559, loss: 0.0040, instance_loss: 0.5758, weighted_loss: 0.1755, label: 1, bag_size: 73\n",
      "batch 579, loss: 2.6964, instance_loss: 2.0071, weighted_loss: 2.4896, label: 0, bag_size: 56\n",
      "batch 599, loss: 0.3064, instance_loss: 0.5727, weighted_loss: 0.3863, label: 0, bag_size: 126\n",
      "batch 619, loss: 0.0606, instance_loss: 1.1848, weighted_loss: 0.3979, label: 1, bag_size: 53\n",
      "batch 639, loss: 0.1519, instance_loss: 1.3909, weighted_loss: 0.5236, label: 1, bag_size: 76\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.97125: correct 10101/10400\n",
      "class 1 clustering acc 0.13884615384615384: correct 722/5200\n",
      "Epoch: 3, train_loss: 0.6283, train_clustering_loss:  0.9084, train_error: 0.2708\n",
      "class 0: acc 0.7280966767371602, correct 241/331\n",
      "class 1: acc 0.7304075235109718, correct 233/319\n",
      "\n",
      "Val Set, val_loss: 0.4549, val_error: 0.2151, auc: 0.9251\n",
      "class 0 clustering acc 0.9879032258064516: correct 1470/1488\n",
      "class 1 clustering acc 0.40725806451612906: correct 303/744\n",
      "class 0: acc 0.6382978723404256, correct 30/47\n",
      "class 1: acc 0.9347826086956522, correct 43/46\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1330, instance_loss: 0.6426, weighted_loss: 0.2859, label: 1, bag_size: 52\n",
      "batch 39, loss: 0.8565, instance_loss: 0.4483, weighted_loss: 0.7340, label: 1, bag_size: 82\n",
      "batch 59, loss: 0.0001, instance_loss: 0.5052, weighted_loss: 0.1516, label: 0, bag_size: 35\n",
      "batch 79, loss: 0.6033, instance_loss: 1.9124, weighted_loss: 0.9960, label: 0, bag_size: 21\n",
      "batch 99, loss: 0.2137, instance_loss: 1.2165, weighted_loss: 0.5145, label: 0, bag_size: 44\n",
      "batch 119, loss: 0.4929, instance_loss: 0.6619, weighted_loss: 0.5436, label: 0, bag_size: 27\n",
      "batch 139, loss: 0.1981, instance_loss: 0.9258, weighted_loss: 0.4164, label: 0, bag_size: 110\n",
      "batch 159, loss: 0.0378, instance_loss: 0.5982, weighted_loss: 0.2059, label: 1, bag_size: 16\n",
      "batch 179, loss: 5.8556, instance_loss: 2.2657, weighted_loss: 4.7786, label: 0, bag_size: 18\n",
      "batch 199, loss: 1.6477, instance_loss: 1.3035, weighted_loss: 1.5444, label: 1, bag_size: 43\n",
      "batch 219, loss: 0.0000, instance_loss: 0.6124, weighted_loss: 0.1837, label: 0, bag_size: 43\n",
      "batch 239, loss: 0.1012, instance_loss: 0.6312, weighted_loss: 0.2602, label: 0, bag_size: 65\n",
      "batch 259, loss: 3.1438, instance_loss: 1.0410, weighted_loss: 2.5129, label: 1, bag_size: 96\n",
      "batch 279, loss: 0.2154, instance_loss: 1.1455, weighted_loss: 0.4944, label: 1, bag_size: 87\n",
      "batch 299, loss: 0.0914, instance_loss: 1.2426, weighted_loss: 0.4367, label: 0, bag_size: 35\n",
      "batch 319, loss: 0.3291, instance_loss: 1.3353, weighted_loss: 0.6310, label: 1, bag_size: 76\n",
      "batch 339, loss: 0.0003, instance_loss: 0.5029, weighted_loss: 0.1511, label: 0, bag_size: 113\n",
      "batch 359, loss: 0.0000, instance_loss: 0.4329, weighted_loss: 0.1299, label: 0, bag_size: 108\n",
      "batch 379, loss: 3.5979, instance_loss: 0.8430, weighted_loss: 2.7714, label: 1, bag_size: 68\n",
      "batch 399, loss: 0.0005, instance_loss: 0.2064, weighted_loss: 0.0623, label: 1, bag_size: 31\n",
      "batch 419, loss: 1.4681, instance_loss: 0.7320, weighted_loss: 1.2473, label: 1, bag_size: 16\n",
      "batch 439, loss: 0.0335, instance_loss: 0.6967, weighted_loss: 0.2324, label: 1, bag_size: 77\n",
      "batch 459, loss: 3.6392, instance_loss: 0.9164, weighted_loss: 2.8224, label: 0, bag_size: 27\n",
      "batch 479, loss: 0.0129, instance_loss: 0.4748, weighted_loss: 0.1515, label: 1, bag_size: 76\n",
      "batch 499, loss: 1.8084, instance_loss: 1.0507, weighted_loss: 1.5811, label: 1, bag_size: 99\n",
      "batch 519, loss: 0.0184, instance_loss: 0.5077, weighted_loss: 0.1652, label: 0, bag_size: 29\n",
      "batch 539, loss: 0.0714, instance_loss: 0.8571, weighted_loss: 0.3071, label: 0, bag_size: 79\n",
      "batch 559, loss: 0.1995, instance_loss: 0.7146, weighted_loss: 0.3540, label: 0, bag_size: 109\n",
      "batch 579, loss: 0.9079, instance_loss: 0.4884, weighted_loss: 0.7820, label: 1, bag_size: 52\n",
      "batch 599, loss: 0.0005, instance_loss: 0.4054, weighted_loss: 0.1220, label: 0, bag_size: 87\n",
      "batch 619, loss: 1.9731, instance_loss: 2.2113, weighted_loss: 2.0446, label: 0, bag_size: 65\n",
      "batch 639, loss: 0.0044, instance_loss: 0.5824, weighted_loss: 0.1778, label: 0, bag_size: 23\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9492307692307692: correct 9872/10400\n",
      "class 1 clustering acc 0.2525: correct 1313/5200\n",
      "Epoch: 4, train_loss: 0.7131, train_clustering_loss:  0.8981, train_error: 0.2631\n",
      "class 0: acc 0.7156862745098039, correct 219/306\n",
      "class 1: acc 0.7558139534883721, correct 260/344\n",
      "\n",
      "Val Set, val_loss: 0.4026, val_error: 0.1613, auc: 0.9598\n",
      "class 0 clustering acc 0.989247311827957: correct 1472/1488\n",
      "class 1 clustering acc 0.3924731182795699: correct 292/744\n",
      "class 0: acc 0.6808510638297872, correct 32/47\n",
      "class 1: acc 1.0, correct 46/46\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0658, instance_loss: 0.5951, weighted_loss: 0.2246, label: 1, bag_size: 82\n",
      "batch 39, loss: 0.0403, instance_loss: 0.5849, weighted_loss: 0.2037, label: 1, bag_size: 97\n",
      "batch 59, loss: 1.4196, instance_loss: 2.2600, weighted_loss: 1.6717, label: 1, bag_size: 76\n",
      "batch 79, loss: 0.5551, instance_loss: 0.7022, weighted_loss: 0.5992, label: 0, bag_size: 39\n",
      "batch 99, loss: 0.3575, instance_loss: 0.9684, weighted_loss: 0.5408, label: 1, bag_size: 85\n",
      "batch 119, loss: 0.1322, instance_loss: 0.6349, weighted_loss: 0.2830, label: 1, bag_size: 28\n",
      "batch 139, loss: 3.9223, instance_loss: 1.2111, weighted_loss: 3.1089, label: 1, bag_size: 56\n",
      "batch 159, loss: 0.2732, instance_loss: 0.8322, weighted_loss: 0.4409, label: 1, bag_size: 28\n",
      "batch 179, loss: 0.2700, instance_loss: 0.8090, weighted_loss: 0.4317, label: 0, bag_size: 67\n",
      "batch 199, loss: 0.0224, instance_loss: 0.6939, weighted_loss: 0.2238, label: 1, bag_size: 39\n",
      "batch 219, loss: 0.0382, instance_loss: 0.5934, weighted_loss: 0.2047, label: 1, bag_size: 47\n",
      "batch 239, loss: 0.0064, instance_loss: 0.3658, weighted_loss: 0.1142, label: 1, bag_size: 45\n",
      "batch 259, loss: 0.0520, instance_loss: 0.6086, weighted_loss: 0.2190, label: 1, bag_size: 46\n",
      "batch 279, loss: 0.0279, instance_loss: 0.9423, weighted_loss: 0.3023, label: 1, bag_size: 44\n",
      "batch 299, loss: 0.3825, instance_loss: 0.3595, weighted_loss: 0.3756, label: 1, bag_size: 88\n",
      "batch 319, loss: 1.4826, instance_loss: 0.5467, weighted_loss: 1.2018, label: 1, bag_size: 28\n",
      "batch 339, loss: 1.2841, instance_loss: 1.7386, weighted_loss: 1.4204, label: 1, bag_size: 73\n",
      "batch 359, loss: 2.3340, instance_loss: 1.1743, weighted_loss: 1.9861, label: 1, bag_size: 84\n",
      "batch 379, loss: 2.3882, instance_loss: 0.8528, weighted_loss: 1.9276, label: 1, bag_size: 83\n",
      "batch 399, loss: 0.0001, instance_loss: 0.3624, weighted_loss: 0.1088, label: 0, bag_size: 53\n",
      "batch 419, loss: 0.0001, instance_loss: 0.4375, weighted_loss: 0.1314, label: 1, bag_size: 72\n",
      "batch 439, loss: 0.0000, instance_loss: 0.2714, weighted_loss: 0.0814, label: 1, bag_size: 98\n",
      "batch 459, loss: 0.0139, instance_loss: 2.1133, weighted_loss: 0.6437, label: 0, bag_size: 64\n",
      "batch 479, loss: 0.0088, instance_loss: 0.4775, weighted_loss: 0.1494, label: 1, bag_size: 74\n",
      "batch 499, loss: 0.2396, instance_loss: 1.1817, weighted_loss: 0.5222, label: 1, bag_size: 45\n",
      "batch 519, loss: 2.9102, instance_loss: 2.1441, weighted_loss: 2.6804, label: 1, bag_size: 64\n",
      "batch 539, loss: 0.5137, instance_loss: 0.5779, weighted_loss: 0.5330, label: 1, bag_size: 63\n",
      "batch 559, loss: 0.2279, instance_loss: 0.3036, weighted_loss: 0.2506, label: 1, bag_size: 123\n",
      "batch 579, loss: 3.7774, instance_loss: 1.3944, weighted_loss: 3.0625, label: 0, bag_size: 54\n",
      "batch 599, loss: 0.2371, instance_loss: 0.4635, weighted_loss: 0.3050, label: 0, bag_size: 91\n",
      "batch 619, loss: 0.0343, instance_loss: 0.5657, weighted_loss: 0.1937, label: 1, bag_size: 72\n",
      "batch 639, loss: 0.4807, instance_loss: 0.8414, weighted_loss: 0.5889, label: 0, bag_size: 80\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9375961538461538: correct 9751/10400\n",
      "class 1 clustering acc 0.4048076923076923: correct 2105/5200\n",
      "Epoch: 5, train_loss: 0.7029, train_clustering_loss:  0.7987, train_error: 0.2831\n",
      "class 0: acc 0.7032258064516129, correct 218/310\n",
      "class 1: acc 0.7294117647058823, correct 248/340\n",
      "\n",
      "Val Set, val_loss: 0.3729, val_error: 0.1183, auc: 0.9325\n",
      "class 0 clustering acc 0.9811827956989247: correct 1460/1488\n",
      "class 1 clustering acc 0.37231182795698925: correct 277/744\n",
      "class 0: acc 0.9361702127659575, correct 44/47\n",
      "class 1: acc 0.8260869565217391, correct 38/46\n",
      "Validation loss decreased (0.384925 --> 0.372868).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2200, instance_loss: 0.4424, weighted_loss: 0.2868, label: 1, bag_size: 24\n",
      "batch 39, loss: 0.0881, instance_loss: 0.4715, weighted_loss: 0.2031, label: 0, bag_size: 126\n",
      "batch 59, loss: 0.0118, instance_loss: 0.2119, weighted_loss: 0.0718, label: 1, bag_size: 79\n",
      "batch 79, loss: 1.1115, instance_loss: 0.5291, weighted_loss: 0.9368, label: 1, bag_size: 81\n",
      "batch 99, loss: 0.0000, instance_loss: 0.2549, weighted_loss: 0.0765, label: 0, bag_size: 80\n",
      "batch 119, loss: 2.1963, instance_loss: 2.6506, weighted_loss: 2.3326, label: 0, bag_size: 18\n",
      "batch 139, loss: 0.0061, instance_loss: 1.4989, weighted_loss: 0.4539, label: 1, bag_size: 35\n",
      "batch 159, loss: 0.0001, instance_loss: 0.5405, weighted_loss: 0.1622, label: 0, bag_size: 66\n",
      "batch 179, loss: 0.2860, instance_loss: 0.2191, weighted_loss: 0.2659, label: 0, bag_size: 25\n",
      "batch 199, loss: 0.0005, instance_loss: 0.1776, weighted_loss: 0.0537, label: 1, bag_size: 56\n",
      "batch 219, loss: 0.0380, instance_loss: 1.2051, weighted_loss: 0.3881, label: 0, bag_size: 36\n",
      "batch 239, loss: 0.5955, instance_loss: 0.2941, weighted_loss: 0.5051, label: 0, bag_size: 83\n",
      "batch 259, loss: 0.0007, instance_loss: 0.1553, weighted_loss: 0.0470, label: 0, bag_size: 53\n",
      "batch 279, loss: 0.0005, instance_loss: 0.0987, weighted_loss: 0.0299, label: 1, bag_size: 99\n",
      "batch 299, loss: 0.5085, instance_loss: 2.5623, weighted_loss: 1.1247, label: 1, bag_size: 67\n",
      "batch 319, loss: 2.6029, instance_loss: 2.5459, weighted_loss: 2.5858, label: 0, bag_size: 18\n",
      "batch 339, loss: 2.4025, instance_loss: 1.2304, weighted_loss: 2.0509, label: 1, bag_size: 60\n",
      "batch 359, loss: 0.0014, instance_loss: 0.1087, weighted_loss: 0.0336, label: 0, bag_size: 25\n",
      "batch 379, loss: 1.4359, instance_loss: 0.5260, weighted_loss: 1.1629, label: 0, bag_size: 78\n",
      "batch 399, loss: 4.6793, instance_loss: 2.2591, weighted_loss: 3.9532, label: 0, bag_size: 28\n",
      "batch 419, loss: 0.1343, instance_loss: 0.1513, weighted_loss: 0.1394, label: 0, bag_size: 110\n",
      "batch 439, loss: 3.4113, instance_loss: 2.3905, weighted_loss: 3.1050, label: 0, bag_size: 17\n",
      "batch 459, loss: 0.9268, instance_loss: 1.9901, weighted_loss: 1.2457, label: 0, bag_size: 93\n",
      "batch 479, loss: 0.0147, instance_loss: 0.1345, weighted_loss: 0.0506, label: 1, bag_size: 153\n",
      "batch 499, loss: 7.0461, instance_loss: 1.1331, weighted_loss: 5.2722, label: 1, bag_size: 99\n",
      "batch 519, loss: 1.8159, instance_loss: 0.7672, weighted_loss: 1.5013, label: 1, bag_size: 86\n",
      "batch 539, loss: 0.0635, instance_loss: 1.0656, weighted_loss: 0.3641, label: 0, bag_size: 27\n",
      "batch 559, loss: 0.6585, instance_loss: 1.1628, weighted_loss: 0.8098, label: 1, bag_size: 36\n",
      "batch 579, loss: 0.3750, instance_loss: 0.6347, weighted_loss: 0.4529, label: 0, bag_size: 94\n",
      "batch 599, loss: 0.0078, instance_loss: 0.1422, weighted_loss: 0.0481, label: 1, bag_size: 32\n",
      "batch 619, loss: 1.3738, instance_loss: 3.4227, weighted_loss: 1.9885, label: 0, bag_size: 89\n",
      "batch 639, loss: 0.1392, instance_loss: 0.6295, weighted_loss: 0.2863, label: 0, bag_size: 49\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.938076923076923: correct 9756/10400\n",
      "class 1 clustering acc 0.5919230769230769: correct 3078/5200\n",
      "Epoch: 6, train_loss: 0.6217, train_clustering_loss:  0.6453, train_error: 0.2277\n",
      "class 0: acc 0.7728706624605678, correct 245/317\n",
      "class 1: acc 0.7717717717717718, correct 257/333\n",
      "\n",
      "Val Set, val_loss: 0.4895, val_error: 0.2151, auc: 0.9348\n",
      "class 0 clustering acc 0.8151881720430108: correct 1213/1488\n",
      "class 1 clustering acc 0.3978494623655914: correct 296/744\n",
      "class 0: acc 0.6382978723404256, correct 30/47\n",
      "class 1: acc 0.9347826086956522, correct 43/46\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4357, instance_loss: 0.2687, weighted_loss: 0.3856, label: 1, bag_size: 14\n",
      "batch 39, loss: 1.6790, instance_loss: 0.6049, weighted_loss: 1.3568, label: 1, bag_size: 24\n",
      "batch 59, loss: 0.3792, instance_loss: 1.6955, weighted_loss: 0.7741, label: 1, bag_size: 62\n",
      "batch 79, loss: 0.2532, instance_loss: 0.5334, weighted_loss: 0.3372, label: 0, bag_size: 96\n",
      "batch 99, loss: 0.6039, instance_loss: 0.5530, weighted_loss: 0.5886, label: 0, bag_size: 28\n",
      "batch 119, loss: 2.4338, instance_loss: 0.8851, weighted_loss: 1.9692, label: 1, bag_size: 36\n",
      "batch 139, loss: 0.3291, instance_loss: 1.7205, weighted_loss: 0.7465, label: 1, bag_size: 76\n",
      "batch 159, loss: 0.1184, instance_loss: 0.3491, weighted_loss: 0.1876, label: 1, bag_size: 17\n",
      "batch 179, loss: 0.0183, instance_loss: 0.4521, weighted_loss: 0.1484, label: 1, bag_size: 30\n",
      "batch 199, loss: 0.4137, instance_loss: 1.0244, weighted_loss: 0.5969, label: 0, bag_size: 50\n",
      "batch 219, loss: 0.0000, instance_loss: 0.1663, weighted_loss: 0.0499, label: 1, bag_size: 83\n",
      "batch 239, loss: 5.5712, instance_loss: 1.3472, weighted_loss: 4.3040, label: 1, bag_size: 54\n",
      "batch 259, loss: 0.0003, instance_loss: 0.3833, weighted_loss: 0.1152, label: 0, bag_size: 25\n",
      "batch 279, loss: 0.1702, instance_loss: 2.0386, weighted_loss: 0.7307, label: 0, bag_size: 19\n",
      "batch 299, loss: 0.0155, instance_loss: 0.2666, weighted_loss: 0.0908, label: 0, bag_size: 29\n",
      "batch 319, loss: 2.6152, instance_loss: 0.6380, weighted_loss: 2.0220, label: 1, bag_size: 73\n",
      "batch 339, loss: 0.0441, instance_loss: 0.2902, weighted_loss: 0.1179, label: 1, bag_size: 17\n",
      "batch 359, loss: 0.1028, instance_loss: 0.9792, weighted_loss: 0.3657, label: 0, bag_size: 28\n",
      "batch 379, loss: 0.0166, instance_loss: 0.3562, weighted_loss: 0.1185, label: 1, bag_size: 52\n",
      "batch 399, loss: 0.0606, instance_loss: 0.0608, weighted_loss: 0.0607, label: 1, bag_size: 95\n",
      "batch 419, loss: 0.0662, instance_loss: 0.8613, weighted_loss: 0.3048, label: 1, bag_size: 57\n",
      "batch 439, loss: 0.2931, instance_loss: 0.1181, weighted_loss: 0.2406, label: 1, bag_size: 78\n",
      "batch 459, loss: 0.0454, instance_loss: 0.2155, weighted_loss: 0.0964, label: 0, bag_size: 92\n",
      "batch 479, loss: 0.0040, instance_loss: 0.1732, weighted_loss: 0.0548, label: 0, bag_size: 72\n",
      "batch 499, loss: 0.0000, instance_loss: 0.1428, weighted_loss: 0.0428, label: 0, bag_size: 81\n",
      "batch 519, loss: 2.2874, instance_loss: 1.0124, weighted_loss: 1.9049, label: 0, bag_size: 42\n",
      "batch 539, loss: 0.0109, instance_loss: 0.0366, weighted_loss: 0.0186, label: 1, bag_size: 62\n",
      "batch 559, loss: 0.0180, instance_loss: 0.0907, weighted_loss: 0.0398, label: 1, bag_size: 87\n",
      "batch 579, loss: 0.0458, instance_loss: 0.0416, weighted_loss: 0.0445, label: 1, bag_size: 35\n",
      "batch 599, loss: 0.0062, instance_loss: 0.1858, weighted_loss: 0.0601, label: 0, bag_size: 45\n",
      "batch 619, loss: 0.9982, instance_loss: 1.3021, weighted_loss: 1.0894, label: 1, bag_size: 62\n",
      "batch 639, loss: 0.6282, instance_loss: 0.2360, weighted_loss: 0.5106, label: 0, bag_size: 57\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9450961538461539: correct 9829/10400\n",
      "class 1 clustering acc 0.5907692307692308: correct 3072/5200\n",
      "Epoch: 7, train_loss: 0.4881, train_clustering_loss:  0.6233, train_error: 0.1892\n",
      "class 0: acc 0.8012422360248447, correct 258/322\n",
      "class 1: acc 0.8201219512195121, correct 269/328\n",
      "\n",
      "Val Set, val_loss: 0.6197, val_error: 0.1505, auc: 0.9214\n",
      "class 0 clustering acc 0.793010752688172: correct 1180/1488\n",
      "class 1 clustering acc 0.5860215053763441: correct 436/744\n",
      "class 0: acc 0.8936170212765957, correct 42/47\n",
      "class 1: acc 0.8043478260869565, correct 37/46\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0004, instance_loss: 0.2079, weighted_loss: 0.0626, label: 0, bag_size: 63\n",
      "batch 39, loss: 0.0003, instance_loss: 0.3820, weighted_loss: 0.1148, label: 0, bag_size: 43\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0400, weighted_loss: 0.0120, label: 1, bag_size: 72\n",
      "batch 79, loss: 0.0032, instance_loss: 0.3361, weighted_loss: 0.1031, label: 0, bag_size: 116\n",
      "batch 99, loss: 0.0143, instance_loss: 0.5536, weighted_loss: 0.1761, label: 0, bag_size: 21\n",
      "batch 119, loss: 0.0006, instance_loss: 0.3859, weighted_loss: 0.1162, label: 0, bag_size: 108\n",
      "batch 139, loss: 0.4683, instance_loss: 2.9044, weighted_loss: 1.1991, label: 1, bag_size: 88\n",
      "batch 159, loss: 0.0091, instance_loss: 0.0295, weighted_loss: 0.0152, label: 1, bag_size: 76\n",
      "batch 179, loss: 0.0855, instance_loss: 0.1320, weighted_loss: 0.0994, label: 1, bag_size: 110\n",
      "batch 199, loss: 0.8445, instance_loss: 0.8020, weighted_loss: 0.8318, label: 1, bag_size: 48\n",
      "batch 219, loss: 0.6697, instance_loss: 1.7538, weighted_loss: 0.9949, label: 1, bag_size: 70\n",
      "batch 239, loss: 2.5631, instance_loss: 2.2058, weighted_loss: 2.4559, label: 1, bag_size: 78\n",
      "batch 259, loss: 1.3891, instance_loss: 0.4558, weighted_loss: 1.1091, label: 1, bag_size: 94\n",
      "batch 279, loss: 1.4607, instance_loss: 0.4253, weighted_loss: 1.1501, label: 0, bag_size: 69\n",
      "batch 299, loss: 0.0798, instance_loss: 0.7997, weighted_loss: 0.2958, label: 0, bag_size: 86\n",
      "batch 319, loss: 0.1000, instance_loss: 0.3976, weighted_loss: 0.1893, label: 0, bag_size: 122\n",
      "batch 339, loss: 0.0964, instance_loss: 0.4114, weighted_loss: 0.1909, label: 0, bag_size: 67\n",
      "batch 359, loss: 0.0021, instance_loss: 0.2393, weighted_loss: 0.0732, label: 0, bag_size: 107\n",
      "batch 379, loss: 0.0091, instance_loss: 0.6695, weighted_loss: 0.2072, label: 0, bag_size: 72\n",
      "batch 399, loss: 1.6771, instance_loss: 1.7293, weighted_loss: 1.6928, label: 1, bag_size: 18\n",
      "batch 419, loss: 2.6061, instance_loss: 1.5451, weighted_loss: 2.2878, label: 1, bag_size: 34\n",
      "batch 439, loss: 0.0119, instance_loss: 0.2565, weighted_loss: 0.0853, label: 1, bag_size: 85\n",
      "batch 459, loss: 0.0225, instance_loss: 0.8584, weighted_loss: 0.2733, label: 0, bag_size: 75\n",
      "batch 479, loss: 0.1390, instance_loss: 0.2475, weighted_loss: 0.1716, label: 1, bag_size: 44\n",
      "batch 499, loss: 0.0000, instance_loss: 0.4526, weighted_loss: 0.1358, label: 0, bag_size: 37\n",
      "batch 519, loss: 0.0180, instance_loss: 0.4616, weighted_loss: 0.1510, label: 0, bag_size: 28\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0599, weighted_loss: 0.0180, label: 1, bag_size: 70\n",
      "batch 559, loss: 0.0043, instance_loss: 1.6268, weighted_loss: 0.4910, label: 0, bag_size: 58\n",
      "batch 579, loss: 0.1794, instance_loss: 0.3477, weighted_loss: 0.2299, label: 0, bag_size: 49\n",
      "batch 599, loss: 0.0003, instance_loss: 0.4101, weighted_loss: 0.1232, label: 0, bag_size: 49\n",
      "batch 619, loss: 0.0492, instance_loss: 0.4991, weighted_loss: 0.1842, label: 0, bag_size: 24\n",
      "batch 639, loss: 0.0003, instance_loss: 0.5835, weighted_loss: 0.1753, label: 0, bag_size: 102\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9434615384615385: correct 9812/10400\n",
      "class 1 clustering acc 0.5725: correct 2977/5200\n",
      "Epoch: 8, train_loss: 0.4558, train_clustering_loss:  0.6138, train_error: 0.1862\n",
      "class 0: acc 0.8184523809523809, correct 275/336\n",
      "class 1: acc 0.8089171974522293, correct 254/314\n",
      "\n",
      "Val Set, val_loss: 0.4566, val_error: 0.1720, auc: 0.9140\n",
      "class 0 clustering acc 0.7936827956989247: correct 1181/1488\n",
      "class 1 clustering acc 0.5645161290322581: correct 420/744\n",
      "class 0: acc 0.8085106382978723, correct 38/47\n",
      "class 1: acc 0.8478260869565217, correct 39/46\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0029, instance_loss: 0.0189, weighted_loss: 0.0077, label: 1, bag_size: 87\n",
      "batch 39, loss: 0.0012, instance_loss: 0.3108, weighted_loss: 0.0941, label: 0, bag_size: 87\n",
      "batch 59, loss: 0.0008, instance_loss: 0.3753, weighted_loss: 0.1131, label: 0, bag_size: 62\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0626, weighted_loss: 0.0188, label: 1, bag_size: 107\n",
      "batch 99, loss: 1.4477, instance_loss: 1.3034, weighted_loss: 1.4044, label: 0, bag_size: 20\n",
      "batch 119, loss: 0.0069, instance_loss: 0.0202, weighted_loss: 0.0109, label: 1, bag_size: 95\n",
      "batch 139, loss: 0.1969, instance_loss: 0.3407, weighted_loss: 0.2400, label: 1, bag_size: 77\n",
      "batch 159, loss: 1.5434, instance_loss: 0.3598, weighted_loss: 1.1883, label: 0, bag_size: 39\n",
      "batch 179, loss: 0.0677, instance_loss: 0.6891, weighted_loss: 0.2541, label: 0, bag_size: 58\n",
      "batch 199, loss: 0.0001, instance_loss: 0.5322, weighted_loss: 0.1597, label: 0, bag_size: 87\n",
      "batch 219, loss: 0.0201, instance_loss: 0.4005, weighted_loss: 0.1342, label: 0, bag_size: 39\n",
      "batch 239, loss: 0.3351, instance_loss: 1.1391, weighted_loss: 0.5763, label: 0, bag_size: 20\n",
      "batch 259, loss: 0.2813, instance_loss: 1.8046, weighted_loss: 0.7383, label: 1, bag_size: 42\n",
      "batch 279, loss: 0.4130, instance_loss: 0.1481, weighted_loss: 0.3336, label: 1, bag_size: 23\n",
      "batch 299, loss: 2.4139, instance_loss: 1.5637, weighted_loss: 2.1588, label: 0, bag_size: 72\n",
      "batch 319, loss: 0.0002, instance_loss: 0.0457, weighted_loss: 0.0139, label: 1, bag_size: 84\n",
      "batch 339, loss: 0.0000, instance_loss: 0.1236, weighted_loss: 0.0371, label: 1, bag_size: 33\n",
      "batch 359, loss: 0.0074, instance_loss: 0.3163, weighted_loss: 0.1001, label: 0, bag_size: 42\n",
      "batch 379, loss: 0.0141, instance_loss: 0.2108, weighted_loss: 0.0731, label: 0, bag_size: 28\n",
      "batch 399, loss: 1.6787, instance_loss: 0.4517, weighted_loss: 1.3106, label: 1, bag_size: 23\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0421, weighted_loss: 0.0126, label: 1, bag_size: 41\n",
      "batch 439, loss: 0.0327, instance_loss: 0.3204, weighted_loss: 0.1190, label: 1, bag_size: 84\n",
      "batch 459, loss: 0.0202, instance_loss: 0.2925, weighted_loss: 0.1019, label: 0, bag_size: 110\n",
      "batch 479, loss: 0.0000, instance_loss: 0.8555, weighted_loss: 0.2567, label: 0, bag_size: 29\n",
      "batch 499, loss: 0.0000, instance_loss: 0.1487, weighted_loss: 0.0446, label: 1, bag_size: 74\n",
      "batch 519, loss: 0.0043, instance_loss: 0.3948, weighted_loss: 0.1215, label: 1, bag_size: 56\n",
      "batch 539, loss: 0.1384, instance_loss: 0.5528, weighted_loss: 0.2627, label: 1, bag_size: 62\n",
      "batch 559, loss: 0.0004, instance_loss: 0.0452, weighted_loss: 0.0138, label: 1, bag_size: 30\n",
      "batch 579, loss: 0.2683, instance_loss: 0.1793, weighted_loss: 0.2416, label: 0, bag_size: 71\n",
      "batch 599, loss: 0.0629, instance_loss: 0.2273, weighted_loss: 0.1122, label: 1, bag_size: 99\n",
      "batch 619, loss: 0.0009, instance_loss: 0.2219, weighted_loss: 0.0672, label: 0, bag_size: 44\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0176, weighted_loss: 0.0053, label: 1, bag_size: 88\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9398076923076923: correct 9774/10400\n",
      "class 1 clustering acc 0.6346153846153846: correct 3300/5200\n",
      "Epoch: 9, train_loss: 0.5126, train_clustering_loss:  0.5736, train_error: 0.1600\n",
      "class 0: acc 0.838006230529595, correct 269/321\n",
      "class 1: acc 0.8419452887537994, correct 277/329\n",
      "\n",
      "Val Set, val_loss: 0.4019, val_error: 0.1290, auc: 0.9473\n",
      "class 0 clustering acc 0.6384408602150538: correct 950/1488\n",
      "class 1 clustering acc 0.7688172043010753: correct 572/744\n",
      "class 0: acc 0.8936170212765957, correct 42/47\n",
      "class 1: acc 0.8478260869565217, correct 39/46\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4051, instance_loss: 1.6291, weighted_loss: 0.7723, label: 1, bag_size: 57\n",
      "batch 39, loss: 0.0102, instance_loss: 0.2329, weighted_loss: 0.0770, label: 0, bag_size: 83\n",
      "batch 59, loss: 4.0007, instance_loss: 2.0858, weighted_loss: 3.4262, label: 0, bag_size: 14\n",
      "batch 79, loss: 0.0082, instance_loss: 0.3078, weighted_loss: 0.0981, label: 1, bag_size: 51\n",
      "batch 99, loss: 0.0123, instance_loss: 0.6243, weighted_loss: 0.1959, label: 1, bag_size: 38\n",
      "batch 119, loss: 0.0003, instance_loss: 0.0427, weighted_loss: 0.0130, label: 1, bag_size: 19\n",
      "batch 139, loss: 1.2109, instance_loss: 1.0536, weighted_loss: 1.1637, label: 1, bag_size: 81\n",
      "batch 159, loss: 0.1389, instance_loss: 0.1644, weighted_loss: 0.1465, label: 0, bag_size: 74\n",
      "batch 179, loss: 0.0083, instance_loss: 0.2412, weighted_loss: 0.0782, label: 0, bag_size: 91\n",
      "batch 199, loss: 0.1609, instance_loss: 0.3433, weighted_loss: 0.2156, label: 1, bag_size: 13\n",
      "batch 219, loss: 0.0533, instance_loss: 0.4238, weighted_loss: 0.1644, label: 1, bag_size: 56\n",
      "batch 239, loss: 0.0998, instance_loss: 0.7094, weighted_loss: 0.2827, label: 1, bag_size: 20\n",
      "batch 259, loss: 3.0571, instance_loss: 1.7401, weighted_loss: 2.6620, label: 0, bag_size: 82\n",
      "batch 279, loss: 0.0391, instance_loss: 0.2549, weighted_loss: 0.1039, label: 0, bag_size: 29\n",
      "batch 299, loss: 0.5400, instance_loss: 0.8270, weighted_loss: 0.6261, label: 0, bag_size: 38\n",
      "batch 319, loss: 0.0001, instance_loss: 0.1566, weighted_loss: 0.0470, label: 0, bag_size: 75\n",
      "batch 339, loss: 0.0022, instance_loss: 0.0337, weighted_loss: 0.0117, label: 1, bag_size: 67\n",
      "batch 359, loss: 0.9868, instance_loss: 0.6833, weighted_loss: 0.8957, label: 1, bag_size: 31\n",
      "batch 379, loss: 0.0006, instance_loss: 0.1331, weighted_loss: 0.0403, label: 0, bag_size: 73\n",
      "batch 399, loss: 0.2248, instance_loss: 0.9336, weighted_loss: 0.4374, label: 1, bag_size: 60\n",
      "batch 419, loss: 0.0000, instance_loss: 0.1926, weighted_loss: 0.0578, label: 0, bag_size: 67\n",
      "batch 439, loss: 0.0009, instance_loss: 0.1526, weighted_loss: 0.0464, label: 1, bag_size: 84\n",
      "batch 459, loss: 0.0514, instance_loss: 0.4065, weighted_loss: 0.1579, label: 0, bag_size: 49\n",
      "batch 479, loss: 0.0011, instance_loss: 0.3334, weighted_loss: 0.1008, label: 0, bag_size: 21\n",
      "batch 499, loss: 0.5323, instance_loss: 0.3715, weighted_loss: 0.4841, label: 1, bag_size: 14\n",
      "batch 519, loss: 0.9780, instance_loss: 2.1606, weighted_loss: 1.3328, label: 0, bag_size: 35\n",
      "batch 539, loss: 0.0003, instance_loss: 0.2425, weighted_loss: 0.0730, label: 0, bag_size: 85\n",
      "batch 559, loss: 0.1332, instance_loss: 2.0098, weighted_loss: 0.6962, label: 0, bag_size: 75\n",
      "batch 579, loss: 0.2356, instance_loss: 0.8014, weighted_loss: 0.4053, label: 1, bag_size: 53\n",
      "batch 599, loss: 0.0015, instance_loss: 0.5176, weighted_loss: 0.1564, label: 0, bag_size: 35\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0349, weighted_loss: 0.0105, label: 1, bag_size: 30\n",
      "batch 639, loss: 0.0276, instance_loss: 0.3609, weighted_loss: 0.1276, label: 1, bag_size: 16\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.93375: correct 9711/10400\n",
      "class 1 clustering acc 0.6261538461538462: correct 3256/5200\n",
      "Epoch: 10, train_loss: 0.4969, train_clustering_loss:  0.5964, train_error: 0.1831\n",
      "class 0: acc 0.8128834355828221, correct 265/326\n",
      "class 1: acc 0.8209876543209876, correct 266/324\n",
      "\n",
      "Val Set, val_loss: 0.5872, val_error: 0.2043, auc: 0.9200\n",
      "class 0 clustering acc 0.6129032258064516: correct 912/1488\n",
      "class 1 clustering acc 0.5672043010752689: correct 422/744\n",
      "class 0: acc 0.6595744680851063, correct 31/47\n",
      "class 1: acc 0.9347826086956522, correct 43/46\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1703, instance_loss: 0.0357, weighted_loss: 0.1299, label: 1, bag_size: 19\n",
      "batch 39, loss: 0.0067, instance_loss: 0.0231, weighted_loss: 0.0117, label: 1, bag_size: 21\n",
      "batch 59, loss: 0.0004, instance_loss: 0.2549, weighted_loss: 0.0767, label: 0, bag_size: 102\n",
      "batch 79, loss: 0.2665, instance_loss: 1.5704, weighted_loss: 0.6576, label: 0, bag_size: 50\n",
      "batch 99, loss: 0.0003, instance_loss: 0.1026, weighted_loss: 0.0310, label: 0, bag_size: 51\n",
      "batch 119, loss: 0.0500, instance_loss: 0.6323, weighted_loss: 0.2247, label: 1, bag_size: 80\n",
      "batch 139, loss: 0.6618, instance_loss: 0.8962, weighted_loss: 0.7321, label: 0, bag_size: 90\n",
      "batch 159, loss: 3.2716, instance_loss: 1.3067, weighted_loss: 2.6821, label: 1, bag_size: 67\n",
      "batch 179, loss: 0.0110, instance_loss: 0.1968, weighted_loss: 0.0668, label: 0, bag_size: 101\n",
      "batch 199, loss: 0.0092, instance_loss: 0.5472, weighted_loss: 0.1706, label: 0, bag_size: 73\n",
      "batch 219, loss: 0.2014, instance_loss: 1.8363, weighted_loss: 0.6919, label: 0, bag_size: 81\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0542, weighted_loss: 0.0163, label: 0, bag_size: 95\n",
      "batch 259, loss: 0.0779, instance_loss: 0.5025, weighted_loss: 0.2053, label: 0, bag_size: 28\n",
      "batch 279, loss: 0.0390, instance_loss: 0.0205, weighted_loss: 0.0334, label: 1, bag_size: 89\n",
      "batch 299, loss: 0.0021, instance_loss: 1.2780, weighted_loss: 0.3849, label: 1, bag_size: 76\n",
      "batch 319, loss: 0.5242, instance_loss: 0.2896, weighted_loss: 0.4538, label: 0, bag_size: 32\n",
      "batch 339, loss: 0.0000, instance_loss: 0.1106, weighted_loss: 0.0332, label: 0, bag_size: 46\n",
      "batch 359, loss: 0.0024, instance_loss: 0.3144, weighted_loss: 0.0960, label: 1, bag_size: 99\n",
      "batch 379, loss: 0.0000, instance_loss: 0.2043, weighted_loss: 0.0613, label: 1, bag_size: 32\n",
      "batch 399, loss: 0.1160, instance_loss: 0.2231, weighted_loss: 0.1481, label: 1, bag_size: 51\n",
      "batch 419, loss: 2.5791, instance_loss: 2.6508, weighted_loss: 2.6006, label: 0, bag_size: 28\n",
      "batch 439, loss: 0.1890, instance_loss: 0.1263, weighted_loss: 0.1702, label: 0, bag_size: 41\n",
      "batch 459, loss: 0.0414, instance_loss: 0.5572, weighted_loss: 0.1962, label: 0, bag_size: 79\n",
      "batch 479, loss: 0.0045, instance_loss: 0.0395, weighted_loss: 0.0150, label: 0, bag_size: 29\n",
      "batch 499, loss: 0.0155, instance_loss: 0.1445, weighted_loss: 0.0542, label: 1, bag_size: 115\n",
      "batch 519, loss: 0.1820, instance_loss: 1.1882, weighted_loss: 0.4838, label: 0, bag_size: 29\n",
      "batch 539, loss: 0.2652, instance_loss: 0.1337, weighted_loss: 0.2257, label: 0, bag_size: 32\n",
      "batch 559, loss: 0.0144, instance_loss: 0.2775, weighted_loss: 0.0933, label: 0, bag_size: 80\n",
      "batch 579, loss: 0.3234, instance_loss: 0.0699, weighted_loss: 0.2474, label: 1, bag_size: 17\n",
      "batch 599, loss: 0.7386, instance_loss: 0.7517, weighted_loss: 0.7425, label: 1, bag_size: 21\n",
      "batch 619, loss: 0.0206, instance_loss: 0.2600, weighted_loss: 0.0924, label: 1, bag_size: 64\n",
      "batch 639, loss: 0.1856, instance_loss: 0.2343, weighted_loss: 0.2002, label: 1, bag_size: 136\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9442307692307692: correct 9820/10400\n",
      "class 1 clustering acc 0.7171153846153846: correct 3729/5200\n",
      "Epoch: 11, train_loss: 0.4282, train_clustering_loss:  0.4879, train_error: 0.1538\n",
      "class 0: acc 0.8546511627906976, correct 294/344\n",
      "class 1: acc 0.8366013071895425, correct 256/306\n",
      "\n",
      "Val Set, val_loss: 0.3176, val_error: 0.1613, auc: 0.9408\n",
      "class 0 clustering acc 0.6149193548387096: correct 915/1488\n",
      "class 1 clustering acc 0.760752688172043: correct 566/744\n",
      "class 0: acc 0.8297872340425532, correct 39/47\n",
      "class 1: acc 0.8478260869565217, correct 39/46\n",
      "Validation loss decreased (0.372868 --> 0.317603).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0388, instance_loss: 0.0487, weighted_loss: 0.0418, label: 1, bag_size: 21\n",
      "batch 39, loss: 2.6838, instance_loss: 3.1908, weighted_loss: 2.8359, label: 1, bag_size: 51\n",
      "batch 59, loss: 0.0486, instance_loss: 0.1595, weighted_loss: 0.0819, label: 0, bag_size: 66\n",
      "batch 79, loss: 0.1272, instance_loss: 0.6720, weighted_loss: 0.2906, label: 1, bag_size: 62\n",
      "batch 99, loss: 0.0537, instance_loss: 0.0804, weighted_loss: 0.0617, label: 1, bag_size: 28\n",
      "batch 119, loss: 0.0317, instance_loss: 0.2289, weighted_loss: 0.0908, label: 0, bag_size: 78\n",
      "batch 139, loss: 0.0053, instance_loss: 0.1694, weighted_loss: 0.0545, label: 0, bag_size: 88\n",
      "batch 159, loss: 0.0563, instance_loss: 0.1486, weighted_loss: 0.0840, label: 0, bag_size: 21\n",
      "batch 179, loss: 0.0865, instance_loss: 0.1509, weighted_loss: 0.1058, label: 1, bag_size: 86\n",
      "batch 199, loss: 0.7226, instance_loss: 0.1305, weighted_loss: 0.5450, label: 0, bag_size: 88\n",
      "batch 219, loss: 0.0471, instance_loss: 0.0681, weighted_loss: 0.0534, label: 1, bag_size: 61\n",
      "batch 239, loss: 0.0000, instance_loss: 0.2525, weighted_loss: 0.0758, label: 0, bag_size: 35\n",
      "batch 259, loss: 0.0000, instance_loss: 0.3620, weighted_loss: 0.1086, label: 0, bag_size: 78\n",
      "batch 279, loss: 0.0548, instance_loss: 0.2182, weighted_loss: 0.1038, label: 1, bag_size: 51\n",
      "batch 299, loss: 0.0148, instance_loss: 0.3924, weighted_loss: 0.1281, label: 1, bag_size: 83\n",
      "batch 319, loss: 0.9665, instance_loss: 0.8232, weighted_loss: 0.9235, label: 0, bag_size: 43\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0179, weighted_loss: 0.0054, label: 1, bag_size: 85\n",
      "batch 359, loss: 0.7571, instance_loss: 0.2850, weighted_loss: 0.6155, label: 1, bag_size: 24\n",
      "batch 379, loss: 0.0213, instance_loss: 0.5716, weighted_loss: 0.1864, label: 1, bag_size: 100\n",
      "batch 399, loss: 0.1065, instance_loss: 0.8886, weighted_loss: 0.3411, label: 0, bag_size: 19\n",
      "batch 419, loss: 1.9441, instance_loss: 0.6859, weighted_loss: 1.5666, label: 0, bag_size: 45\n",
      "batch 439, loss: 0.0101, instance_loss: 0.3616, weighted_loss: 0.1156, label: 1, bag_size: 96\n",
      "batch 459, loss: 0.7246, instance_loss: 1.5351, weighted_loss: 0.9677, label: 0, bag_size: 80\n",
      "batch 479, loss: 0.0013, instance_loss: 0.0734, weighted_loss: 0.0229, label: 0, bag_size: 31\n",
      "batch 499, loss: 0.0098, instance_loss: 0.2491, weighted_loss: 0.0816, label: 0, bag_size: 61\n",
      "batch 519, loss: 0.0000, instance_loss: 0.1568, weighted_loss: 0.0470, label: 0, bag_size: 118\n",
      "batch 539, loss: 0.0163, instance_loss: 0.2059, weighted_loss: 0.0732, label: 1, bag_size: 49\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0060, weighted_loss: 0.0019, label: 1, bag_size: 81\n",
      "batch 579, loss: 0.7470, instance_loss: 0.0483, weighted_loss: 0.5374, label: 1, bag_size: 82\n",
      "batch 599, loss: 0.0000, instance_loss: 0.1108, weighted_loss: 0.0332, label: 0, bag_size: 37\n",
      "batch 619, loss: 0.2097, instance_loss: 0.2883, weighted_loss: 0.2333, label: 0, bag_size: 49\n",
      "batch 639, loss: 0.0056, instance_loss: 0.0499, weighted_loss: 0.0189, label: 1, bag_size: 76\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9475961538461538: correct 9855/10400\n",
      "class 1 clustering acc 0.7386538461538461: correct 3841/5200\n",
      "Epoch: 12, train_loss: 0.3261, train_clustering_loss:  0.4516, train_error: 0.1400\n",
      "class 0: acc 0.8742331288343558, correct 285/326\n",
      "class 1: acc 0.845679012345679, correct 274/324\n",
      "\n",
      "Val Set, val_loss: 0.2686, val_error: 0.1505, auc: 0.9616\n",
      "class 0 clustering acc 0.7264784946236559: correct 1081/1488\n",
      "class 1 clustering acc 0.6935483870967742: correct 516/744\n",
      "class 0: acc 0.7872340425531915, correct 37/47\n",
      "class 1: acc 0.9130434782608695, correct 42/46\n",
      "Validation loss decreased (0.317603 --> 0.268616).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.0207, weighted_loss: 0.0063, label: 1, bag_size: 83\n",
      "batch 39, loss: 0.0299, instance_loss: 0.2561, weighted_loss: 0.0978, label: 0, bag_size: 47\n",
      "batch 59, loss: 0.0008, instance_loss: 0.0466, weighted_loss: 0.0145, label: 0, bag_size: 78\n",
      "batch 79, loss: 0.2906, instance_loss: 1.2917, weighted_loss: 0.5909, label: 0, bag_size: 98\n",
      "batch 99, loss: 0.0030, instance_loss: 0.1375, weighted_loss: 0.0434, label: 0, bag_size: 92\n",
      "batch 119, loss: 0.0000, instance_loss: 0.1972, weighted_loss: 0.0592, label: 0, bag_size: 78\n",
      "batch 139, loss: 1.2188, instance_loss: 0.5192, weighted_loss: 1.0089, label: 0, bag_size: 71\n",
      "batch 159, loss: 0.7782, instance_loss: 0.9731, weighted_loss: 0.8367, label: 0, bag_size: 41\n",
      "batch 179, loss: 0.0104, instance_loss: 0.0875, weighted_loss: 0.0335, label: 1, bag_size: 83\n",
      "batch 199, loss: 0.0275, instance_loss: 0.5639, weighted_loss: 0.1884, label: 0, bag_size: 17\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0043, weighted_loss: 0.0013, label: 1, bag_size: 86\n",
      "batch 239, loss: 1.0379, instance_loss: 1.4990, weighted_loss: 1.1763, label: 0, bag_size: 69\n",
      "batch 259, loss: 0.0802, instance_loss: 0.3187, weighted_loss: 0.1518, label: 0, bag_size: 13\n",
      "batch 279, loss: 4.1069, instance_loss: 2.7322, weighted_loss: 3.6945, label: 0, bag_size: 73\n",
      "batch 299, loss: 3.3821, instance_loss: 1.9641, weighted_loss: 2.9567, label: 0, bag_size: 28\n",
      "batch 319, loss: 0.2434, instance_loss: 2.3268, weighted_loss: 0.8684, label: 1, bag_size: 31\n",
      "batch 339, loss: 0.0156, instance_loss: 0.2392, weighted_loss: 0.0827, label: 1, bag_size: 25\n",
      "batch 359, loss: 0.0401, instance_loss: 0.1054, weighted_loss: 0.0597, label: 0, bag_size: 31\n",
      "batch 379, loss: 0.0000, instance_loss: 0.1713, weighted_loss: 0.0514, label: 0, bag_size: 110\n",
      "batch 399, loss: 0.0581, instance_loss: 0.0566, weighted_loss: 0.0577, label: 1, bag_size: 99\n",
      "batch 419, loss: 0.0088, instance_loss: 0.2014, weighted_loss: 0.0666, label: 0, bag_size: 79\n",
      "batch 439, loss: 1.9013, instance_loss: 0.7760, weighted_loss: 1.5637, label: 1, bag_size: 24\n",
      "batch 459, loss: 0.0027, instance_loss: 0.1452, weighted_loss: 0.0455, label: 0, bag_size: 115\n",
      "batch 479, loss: 0.0069, instance_loss: 0.0384, weighted_loss: 0.0164, label: 1, bag_size: 60\n",
      "batch 499, loss: 0.0079, instance_loss: 0.0153, weighted_loss: 0.0101, label: 1, bag_size: 51\n",
      "batch 519, loss: 0.4482, instance_loss: 0.5017, weighted_loss: 0.4643, label: 0, bag_size: 17\n",
      "batch 539, loss: 0.0099, instance_loss: 0.0604, weighted_loss: 0.0250, label: 1, bag_size: 75\n",
      "batch 559, loss: 0.0004, instance_loss: 0.0520, weighted_loss: 0.0159, label: 0, bag_size: 74\n",
      "batch 579, loss: 0.0420, instance_loss: 0.0808, weighted_loss: 0.0536, label: 0, bag_size: 28\n",
      "batch 599, loss: 0.6204, instance_loss: 0.8011, weighted_loss: 0.6746, label: 1, bag_size: 61\n",
      "batch 619, loss: 0.1088, instance_loss: 0.1888, weighted_loss: 0.1328, label: 0, bag_size: 31\n",
      "batch 639, loss: 0.7677, instance_loss: 0.5600, weighted_loss: 0.7054, label: 1, bag_size: 42\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9474038461538462: correct 9853/10400\n",
      "class 1 clustering acc 0.7428846153846154: correct 3863/5200\n",
      "Epoch: 13, train_loss: 0.3425, train_clustering_loss:  0.4600, train_error: 0.1277\n",
      "class 0: acc 0.8598726114649682, correct 270/314\n",
      "class 1: acc 0.8839285714285714, correct 297/336\n",
      "\n",
      "Val Set, val_loss: 0.4933, val_error: 0.2151, auc: 0.9172\n",
      "class 0 clustering acc 0.616263440860215: correct 917/1488\n",
      "class 1 clustering acc 0.6868279569892473: correct 511/744\n",
      "class 0: acc 0.723404255319149, correct 34/47\n",
      "class 1: acc 0.8478260869565217, correct 39/46\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0054, instance_loss: 0.0590, weighted_loss: 0.0215, label: 0, bag_size: 25\n",
      "batch 39, loss: 0.0175, instance_loss: 0.2292, weighted_loss: 0.0810, label: 0, bag_size: 73\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0058, weighted_loss: 0.0017, label: 1, bag_size: 74\n",
      "batch 79, loss: 0.0359, instance_loss: 0.0882, weighted_loss: 0.0516, label: 0, bag_size: 25\n",
      "batch 99, loss: 0.0231, instance_loss: 0.0373, weighted_loss: 0.0273, label: 1, bag_size: 89\n",
      "batch 119, loss: 0.4706, instance_loss: 0.3585, weighted_loss: 0.4370, label: 1, bag_size: 78\n",
      "batch 139, loss: 6.1014, instance_loss: 4.3507, weighted_loss: 5.5761, label: 1, bag_size: 98\n",
      "batch 159, loss: 0.0358, instance_loss: 0.0863, weighted_loss: 0.0509, label: 0, bag_size: 102\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0246, weighted_loss: 0.0074, label: 1, bag_size: 79\n",
      "batch 199, loss: 0.0009, instance_loss: 0.3391, weighted_loss: 0.1024, label: 1, bag_size: 108\n",
      "batch 219, loss: 0.0701, instance_loss: 0.0485, weighted_loss: 0.0637, label: 1, bag_size: 78\n",
      "batch 239, loss: 0.0001, instance_loss: 0.2510, weighted_loss: 0.0754, label: 1, bag_size: 32\n",
      "batch 259, loss: 0.0895, instance_loss: 0.2327, weighted_loss: 0.1325, label: 0, bag_size: 78\n",
      "batch 279, loss: 0.0403, instance_loss: 0.0615, weighted_loss: 0.0467, label: 0, bag_size: 57\n",
      "batch 299, loss: 4.7112, instance_loss: 2.5643, weighted_loss: 4.0671, label: 0, bag_size: 78\n",
      "batch 319, loss: 0.0713, instance_loss: 0.5694, weighted_loss: 0.2207, label: 1, bag_size: 62\n",
      "batch 339, loss: 0.0002, instance_loss: 0.0112, weighted_loss: 0.0035, label: 1, bag_size: 74\n",
      "batch 359, loss: 0.1107, instance_loss: 0.4608, weighted_loss: 0.2157, label: 1, bag_size: 41\n",
      "batch 379, loss: 1.5527, instance_loss: 0.2281, weighted_loss: 1.1553, label: 0, bag_size: 40\n",
      "batch 399, loss: 0.0004, instance_loss: 0.2629, weighted_loss: 0.0792, label: 0, bag_size: 95\n",
      "batch 419, loss: 0.0046, instance_loss: 0.0947, weighted_loss: 0.0316, label: 0, bag_size: 21\n",
      "batch 439, loss: 0.0623, instance_loss: 0.1461, weighted_loss: 0.0874, label: 0, bag_size: 110\n",
      "batch 459, loss: 0.0088, instance_loss: 0.4555, weighted_loss: 0.1428, label: 0, bag_size: 73\n",
      "batch 479, loss: 0.0089, instance_loss: 0.0295, weighted_loss: 0.0151, label: 0, bag_size: 26\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0074, weighted_loss: 0.0023, label: 1, bag_size: 60\n",
      "batch 519, loss: 0.0039, instance_loss: 0.0728, weighted_loss: 0.0246, label: 0, bag_size: 29\n",
      "batch 539, loss: 0.0352, instance_loss: 0.0779, weighted_loss: 0.0480, label: 1, bag_size: 72\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0043, weighted_loss: 0.0013, label: 1, bag_size: 75\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0394, weighted_loss: 0.0118, label: 1, bag_size: 39\n",
      "batch 599, loss: 0.9990, instance_loss: 0.3424, weighted_loss: 0.8020, label: 1, bag_size: 14\n",
      "batch 619, loss: 0.0016, instance_loss: 0.0283, weighted_loss: 0.0096, label: 1, bag_size: 84\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0294, weighted_loss: 0.0088, label: 0, bag_size: 65\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9561538461538461: correct 9944/10400\n",
      "class 1 clustering acc 0.7815384615384615: correct 4064/5200\n",
      "Epoch: 14, train_loss: 0.2706, train_clustering_loss:  0.4074, train_error: 0.1000\n",
      "class 0: acc 0.8984126984126984, correct 283/315\n",
      "class 1: acc 0.9014925373134328, correct 302/335\n",
      "\n",
      "Val Set, val_loss: 1.1887, val_error: 0.2366, auc: 0.9260\n",
      "class 0 clustering acc 0.7217741935483871: correct 1074/1488\n",
      "class 1 clustering acc 0.5551075268817204: correct 413/744\n",
      "class 0: acc 0.574468085106383, correct 27/47\n",
      "class 1: acc 0.9565217391304348, correct 44/46\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0434, instance_loss: 0.5161, weighted_loss: 0.1852, label: 0, bag_size: 50\n",
      "batch 39, loss: 0.0082, instance_loss: 0.9803, weighted_loss: 0.2999, label: 1, bag_size: 38\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0075, weighted_loss: 0.0022, label: 1, bag_size: 100\n",
      "batch 79, loss: 0.0002, instance_loss: 0.0072, weighted_loss: 0.0023, label: 1, bag_size: 24\n",
      "batch 99, loss: 0.0008, instance_loss: 0.0494, weighted_loss: 0.0154, label: 0, bag_size: 63\n",
      "batch 119, loss: 0.0809, instance_loss: 0.0421, weighted_loss: 0.0693, label: 0, bag_size: 69\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0075, weighted_loss: 0.0023, label: 0, bag_size: 45\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0223, weighted_loss: 0.0067, label: 0, bag_size: 73\n",
      "batch 179, loss: 6.7033, instance_loss: 1.2779, weighted_loss: 5.0757, label: 1, bag_size: 20\n",
      "batch 199, loss: 0.0018, instance_loss: 3.2482, weighted_loss: 0.9757, label: 0, bag_size: 102\n",
      "batch 219, loss: 0.0000, instance_loss: 0.3064, weighted_loss: 0.0919, label: 0, bag_size: 57\n",
      "batch 239, loss: 0.0008, instance_loss: 0.1028, weighted_loss: 0.0314, label: 0, bag_size: 61\n",
      "batch 259, loss: 1.7841, instance_loss: 0.0527, weighted_loss: 1.2646, label: 1, bag_size: 68\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0300, weighted_loss: 0.0090, label: 0, bag_size: 72\n",
      "batch 299, loss: 5.7374, instance_loss: 2.2627, weighted_loss: 4.6950, label: 0, bag_size: 103\n",
      "batch 319, loss: 0.0000, instance_loss: 1.2771, weighted_loss: 0.3832, label: 1, bag_size: 47\n",
      "batch 339, loss: 0.0000, instance_loss: 0.6740, weighted_loss: 0.2022, label: 0, bag_size: 61\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0914, weighted_loss: 0.0275, label: 0, bag_size: 76\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0199, weighted_loss: 0.0060, label: 0, bag_size: 97\n",
      "batch 399, loss: 0.0001, instance_loss: 0.2126, weighted_loss: 0.0638, label: 0, bag_size: 112\n",
      "batch 419, loss: 0.0000, instance_loss: 0.3896, weighted_loss: 0.1169, label: 1, bag_size: 58\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0745, weighted_loss: 0.0223, label: 0, bag_size: 43\n",
      "batch 459, loss: 1.0378, instance_loss: 0.4211, weighted_loss: 0.8528, label: 1, bag_size: 63\n",
      "batch 479, loss: 0.0008, instance_loss: 3.2109, weighted_loss: 0.9638, label: 0, bag_size: 88\n",
      "batch 499, loss: 0.0004, instance_loss: 1.5849, weighted_loss: 0.4757, label: 0, bag_size: 27\n",
      "batch 519, loss: 0.0000, instance_loss: 0.5282, weighted_loss: 0.1584, label: 1, bag_size: 61\n",
      "batch 539, loss: 0.0001, instance_loss: 0.2848, weighted_loss: 0.0855, label: 0, bag_size: 53\n",
      "batch 559, loss: 0.0057, instance_loss: 1.9486, weighted_loss: 0.5885, label: 0, bag_size: 44\n",
      "batch 579, loss: 0.0000, instance_loss: 0.5982, weighted_loss: 0.1795, label: 0, bag_size: 50\n",
      "batch 599, loss: 0.0000, instance_loss: 0.3115, weighted_loss: 0.0935, label: 0, bag_size: 90\n",
      "batch 619, loss: 0.0000, instance_loss: 0.3788, weighted_loss: 0.1136, label: 1, bag_size: 50\n",
      "batch 639, loss: 0.5605, instance_loss: 0.6072, weighted_loss: 0.5745, label: 0, bag_size: 69\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9255769230769231: correct 9626/10400\n",
      "class 1 clustering acc 0.5892307692307692: correct 3064/5200\n",
      "Epoch: 15, train_loss: 0.9879, train_clustering_loss:  0.6986, train_error: 0.1800\n",
      "class 0: acc 0.8087774294670846, correct 258/319\n",
      "class 1: acc 0.8308157099697885, correct 275/331\n",
      "\n",
      "Val Set, val_loss: 0.7106, val_error: 0.1613, auc: 0.9283\n",
      "class 0 clustering acc 0.9731182795698925: correct 1448/1488\n",
      "class 1 clustering acc 0.18548387096774194: correct 138/744\n",
      "class 0: acc 0.8936170212765957, correct 42/47\n",
      "class 1: acc 0.782608695652174, correct 36/46\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1514, instance_loss: 0.7970, weighted_loss: 0.3451, label: 0, bag_size: 25\n",
      "batch 39, loss: 0.0000, instance_loss: 0.5404, weighted_loss: 0.1621, label: 1, bag_size: 67\n",
      "batch 59, loss: 0.0000, instance_loss: 0.3870, weighted_loss: 0.1161, label: 1, bag_size: 97\n",
      "batch 79, loss: 0.0000, instance_loss: 1.6699, weighted_loss: 0.5010, label: 1, bag_size: 120\n",
      "batch 99, loss: 4.3074, instance_loss: 1.5802, weighted_loss: 3.4893, label: 1, bag_size: 34\n",
      "batch 119, loss: 0.0003, instance_loss: 0.5729, weighted_loss: 0.1721, label: 1, bag_size: 49\n",
      "batch 139, loss: 0.0002, instance_loss: 0.7898, weighted_loss: 0.2371, label: 1, bag_size: 93\n",
      "batch 159, loss: 0.5165, instance_loss: 0.5833, weighted_loss: 0.5366, label: 1, bag_size: 56\n",
      "batch 179, loss: 0.0002, instance_loss: 0.3255, weighted_loss: 0.0978, label: 1, bag_size: 30\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0203, weighted_loss: 0.0061, label: 1, bag_size: 20\n",
      "batch 219, loss: 0.0006, instance_loss: 0.3290, weighted_loss: 0.0992, label: 0, bag_size: 42\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0297, weighted_loss: 0.0089, label: 1, bag_size: 51\n",
      "batch 259, loss: 0.0028, instance_loss: 0.3497, weighted_loss: 0.1068, label: 1, bag_size: 20\n",
      "batch 279, loss: 0.0005, instance_loss: 0.1649, weighted_loss: 0.0498, label: 0, bag_size: 95\n",
      "batch 299, loss: 0.0000, instance_loss: 0.5215, weighted_loss: 0.1565, label: 0, bag_size: 51\n",
      "batch 319, loss: 5.4499, instance_loss: 0.2165, weighted_loss: 3.8799, label: 0, bag_size: 91\n",
      "batch 339, loss: 5.1417, instance_loss: 0.4785, weighted_loss: 3.7427, label: 1, bag_size: 49\n",
      "batch 359, loss: 0.0001, instance_loss: 0.1153, weighted_loss: 0.0347, label: 0, bag_size: 42\n",
      "batch 379, loss: 2.0794, instance_loss: 0.4189, weighted_loss: 1.5813, label: 0, bag_size: 62\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0277, weighted_loss: 0.0083, label: 1, bag_size: 39\n",
      "batch 419, loss: 8.2193, instance_loss: 2.9074, weighted_loss: 6.6257, label: 0, bag_size: 75\n",
      "batch 439, loss: 0.0329, instance_loss: 0.0994, weighted_loss: 0.0528, label: 0, bag_size: 91\n",
      "batch 459, loss: 0.0002, instance_loss: 0.0133, weighted_loss: 0.0041, label: 1, bag_size: 72\n",
      "batch 479, loss: 0.0011, instance_loss: 0.0242, weighted_loss: 0.0080, label: 1, bag_size: 63\n",
      "batch 499, loss: 0.0567, instance_loss: 0.6998, weighted_loss: 0.2496, label: 1, bag_size: 80\n",
      "batch 519, loss: 0.0435, instance_loss: 0.5336, weighted_loss: 0.1905, label: 0, bag_size: 30\n",
      "batch 539, loss: 0.1366, instance_loss: 0.4239, weighted_loss: 0.2228, label: 0, bag_size: 46\n",
      "batch 559, loss: 0.0566, instance_loss: 0.2603, weighted_loss: 0.1177, label: 1, bag_size: 54\n",
      "batch 579, loss: 1.3155, instance_loss: 1.0319, weighted_loss: 1.2305, label: 0, bag_size: 42\n",
      "batch 599, loss: 0.0010, instance_loss: 0.2444, weighted_loss: 0.0740, label: 1, bag_size: 56\n",
      "batch 619, loss: 0.7997, instance_loss: 2.1953, weighted_loss: 1.2184, label: 0, bag_size: 51\n",
      "batch 639, loss: 0.1585, instance_loss: 0.8839, weighted_loss: 0.3762, label: 1, bag_size: 39\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9343269230769231: correct 9717/10400\n",
      "class 1 clustering acc 0.6471153846153846: correct 3365/5200\n",
      "Epoch: 16, train_loss: 0.5271, train_clustering_loss:  0.5784, train_error: 0.1369\n",
      "class 0: acc 0.8575851393188855, correct 277/323\n",
      "class 1: acc 0.8685015290519877, correct 284/327\n",
      "\n",
      "Val Set, val_loss: 0.5847, val_error: 0.1505, auc: 0.9154\n",
      "class 0 clustering acc 0.6310483870967742: correct 939/1488\n",
      "class 1 clustering acc 0.6989247311827957: correct 520/744\n",
      "class 0: acc 0.8085106382978723, correct 38/47\n",
      "class 1: acc 0.8913043478260869, correct 41/46\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0307, weighted_loss: 0.0093, label: 1, bag_size: 41\n",
      "batch 39, loss: 0.0145, instance_loss: 0.1741, weighted_loss: 0.0624, label: 0, bag_size: 118\n",
      "batch 59, loss: 0.0028, instance_loss: 0.3515, weighted_loss: 0.1074, label: 1, bag_size: 77\n",
      "batch 79, loss: 0.0071, instance_loss: 0.2189, weighted_loss: 0.0706, label: 1, bag_size: 52\n",
      "batch 99, loss: 0.0365, instance_loss: 0.2237, weighted_loss: 0.0926, label: 1, bag_size: 100\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0700, weighted_loss: 0.0210, label: 0, bag_size: 73\n",
      "batch 139, loss: 0.0066, instance_loss: 0.4391, weighted_loss: 0.1364, label: 1, bag_size: 82\n",
      "batch 159, loss: 0.8423, instance_loss: 0.1555, weighted_loss: 0.6362, label: 1, bag_size: 52\n",
      "batch 179, loss: 0.0012, instance_loss: 0.3107, weighted_loss: 0.0940, label: 1, bag_size: 78\n",
      "batch 199, loss: 0.0101, instance_loss: 0.4063, weighted_loss: 0.1289, label: 0, bag_size: 62\n",
      "batch 219, loss: 0.0000, instance_loss: 0.2980, weighted_loss: 0.0894, label: 1, bag_size: 33\n",
      "batch 239, loss: 0.0076, instance_loss: 0.7247, weighted_loss: 0.2227, label: 0, bag_size: 87\n",
      "batch 259, loss: 0.0000, instance_loss: 1.0052, weighted_loss: 0.3016, label: 1, bag_size: 48\n",
      "batch 279, loss: 0.0002, instance_loss: 0.0689, weighted_loss: 0.0208, label: 1, bag_size: 88\n",
      "batch 299, loss: 0.0000, instance_loss: 0.3378, weighted_loss: 0.1014, label: 0, bag_size: 76\n",
      "batch 319, loss: 1.7737, instance_loss: 0.5102, weighted_loss: 1.3947, label: 0, bag_size: 26\n",
      "batch 339, loss: 0.0058, instance_loss: 0.4043, weighted_loss: 0.1253, label: 0, bag_size: 65\n",
      "batch 359, loss: 0.0000, instance_loss: 0.4973, weighted_loss: 0.1492, label: 1, bag_size: 31\n",
      "batch 379, loss: 0.0000, instance_loss: 0.5053, weighted_loss: 0.1516, label: 1, bag_size: 84\n",
      "batch 399, loss: 0.0102, instance_loss: 0.6407, weighted_loss: 0.1994, label: 1, bag_size: 84\n",
      "batch 419, loss: 0.0454, instance_loss: 1.2138, weighted_loss: 0.3960, label: 0, bag_size: 28\n",
      "batch 439, loss: 0.0016, instance_loss: 0.0572, weighted_loss: 0.0183, label: 0, bag_size: 44\n",
      "batch 459, loss: 0.0039, instance_loss: 1.1950, weighted_loss: 0.3612, label: 1, bag_size: 79\n",
      "batch 479, loss: 0.0187, instance_loss: 0.2989, weighted_loss: 0.1027, label: 0, bag_size: 33\n",
      "batch 499, loss: 1.1220, instance_loss: 0.3989, weighted_loss: 0.9051, label: 0, bag_size: 43\n",
      "batch 519, loss: 0.0000, instance_loss: 0.1251, weighted_loss: 0.0375, label: 0, bag_size: 118\n",
      "batch 539, loss: 1.1004, instance_loss: 2.3700, weighted_loss: 1.4812, label: 0, bag_size: 18\n",
      "batch 559, loss: 0.0113, instance_loss: 0.3410, weighted_loss: 0.1102, label: 0, bag_size: 18\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0629, weighted_loss: 0.0189, label: 0, bag_size: 65\n",
      "batch 599, loss: 0.0024, instance_loss: 0.1496, weighted_loss: 0.0466, label: 0, bag_size: 37\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0664, weighted_loss: 0.0199, label: 1, bag_size: 38\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0853, weighted_loss: 0.0256, label: 0, bag_size: 47\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9235576923076924: correct 9605/10400\n",
      "class 1 clustering acc 0.6638461538461539: correct 3452/5200\n",
      "Epoch: 17, train_loss: 0.3781, train_clustering_loss:  0.5710, train_error: 0.1200\n",
      "class 0: acc 0.8789808917197452, correct 276/314\n",
      "class 1: acc 0.8809523809523809, correct 296/336\n",
      "\n",
      "Val Set, val_loss: 0.5133, val_error: 0.1613, auc: 0.9348\n",
      "class 0 clustering acc 0.8272849462365591: correct 1231/1488\n",
      "class 1 clustering acc 0.6653225806451613: correct 495/744\n",
      "class 0: acc 0.8085106382978723, correct 38/47\n",
      "class 1: acc 0.8695652173913043, correct 40/46\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1627, instance_loss: 0.6397, weighted_loss: 0.3058, label: 0, bag_size: 34\n",
      "batch 39, loss: 0.0001, instance_loss: 0.1035, weighted_loss: 0.0311, label: 0, bag_size: 21\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0155, weighted_loss: 0.0047, label: 0, bag_size: 95\n",
      "batch 79, loss: 0.0002, instance_loss: 0.0393, weighted_loss: 0.0119, label: 1, bag_size: 44\n",
      "batch 99, loss: 5.5521, instance_loss: 3.0943, weighted_loss: 4.8148, label: 1, bag_size: 24\n",
      "batch 119, loss: 0.0024, instance_loss: 0.3755, weighted_loss: 0.1144, label: 1, bag_size: 44\n",
      "batch 139, loss: 0.5121, instance_loss: 0.5271, weighted_loss: 0.5166, label: 1, bag_size: 35\n",
      "batch 159, loss: 0.0816, instance_loss: 1.1609, weighted_loss: 0.4054, label: 0, bag_size: 39\n",
      "batch 179, loss: 0.0000, instance_loss: 0.5807, weighted_loss: 0.1742, label: 1, bag_size: 136\n",
      "batch 199, loss: 0.0000, instance_loss: 1.0482, weighted_loss: 0.3145, label: 1, bag_size: 74\n",
      "batch 219, loss: 0.0000, instance_loss: 0.7435, weighted_loss: 0.2231, label: 0, bag_size: 69\n",
      "batch 239, loss: 0.0000, instance_loss: 0.6846, weighted_loss: 0.2054, label: 0, bag_size: 69\n",
      "batch 259, loss: 0.0000, instance_loss: 1.0626, weighted_loss: 0.3188, label: 1, bag_size: 89\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0106, weighted_loss: 0.0032, label: 0, bag_size: 102\n",
      "batch 299, loss: 0.9007, instance_loss: 1.3964, weighted_loss: 1.0494, label: 1, bag_size: 85\n",
      "batch 319, loss: 7.0264, instance_loss: 1.3859, weighted_loss: 5.3343, label: 0, bag_size: 73\n",
      "batch 339, loss: 0.0127, instance_loss: 0.4167, weighted_loss: 0.1339, label: 1, bag_size: 52\n",
      "batch 359, loss: 0.0000, instance_loss: 0.2318, weighted_loss: 0.0695, label: 0, bag_size: 58\n",
      "batch 379, loss: 0.4148, instance_loss: 1.0259, weighted_loss: 0.5981, label: 1, bag_size: 31\n",
      "batch 399, loss: 0.0001, instance_loss: 0.4427, weighted_loss: 0.1329, label: 1, bag_size: 35\n",
      "batch 419, loss: 0.0000, instance_loss: 0.5818, weighted_loss: 0.1745, label: 1, bag_size: 83\n",
      "batch 439, loss: 0.0018, instance_loss: 0.0654, weighted_loss: 0.0208, label: 0, bag_size: 95\n",
      "batch 459, loss: 1.4955, instance_loss: 0.9022, weighted_loss: 1.3175, label: 0, bag_size: 103\n",
      "batch 479, loss: 0.0009, instance_loss: 0.4649, weighted_loss: 0.1401, label: 1, bag_size: 25\n",
      "batch 499, loss: 0.0000, instance_loss: 0.5530, weighted_loss: 0.1659, label: 1, bag_size: 109\n",
      "batch 519, loss: 0.0000, instance_loss: 1.3172, weighted_loss: 0.3952, label: 1, bag_size: 17\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0200, weighted_loss: 0.0060, label: 0, bag_size: 67\n",
      "batch 559, loss: 0.0004, instance_loss: 0.5190, weighted_loss: 0.1559, label: 1, bag_size: 49\n",
      "batch 579, loss: 0.4400, instance_loss: 0.5165, weighted_loss: 0.4630, label: 1, bag_size: 61\n",
      "batch 599, loss: 0.0000, instance_loss: 0.5925, weighted_loss: 0.1778, label: 1, bag_size: 73\n",
      "batch 619, loss: 0.0024, instance_loss: 0.3236, weighted_loss: 0.0988, label: 0, bag_size: 75\n",
      "batch 639, loss: 0.0020, instance_loss: 0.7169, weighted_loss: 0.2165, label: 0, bag_size: 89\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9191346153846154: correct 9559/10400\n",
      "class 1 clustering acc 0.5438461538461539: correct 2828/5200\n",
      "Epoch: 18, train_loss: 0.5754, train_clustering_loss:  0.6542, train_error: 0.1477\n",
      "class 0: acc 0.8501529051987767, correct 278/327\n",
      "class 1: acc 0.8544891640866873, correct 276/323\n",
      "\n",
      "Val Set, val_loss: 0.7994, val_error: 0.1613, auc: 0.9315\n",
      "class 0 clustering acc 0.5759408602150538: correct 857/1488\n",
      "class 1 clustering acc 0.7486559139784946: correct 557/744\n",
      "class 0: acc 0.9361702127659575, correct 44/47\n",
      "class 1: acc 0.7391304347826086, correct 34/46\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.8968, instance_loss: 2.3062, weighted_loss: 2.0196, label: 0, bag_size: 35\n",
      "batch 39, loss: 0.0535, instance_loss: 0.4298, weighted_loss: 0.1664, label: 1, bag_size: 38\n",
      "batch 59, loss: 0.0001, instance_loss: 0.3423, weighted_loss: 0.1028, label: 1, bag_size: 49\n",
      "batch 79, loss: 0.0001, instance_loss: 0.4405, weighted_loss: 0.1322, label: 1, bag_size: 39\n",
      "batch 99, loss: 1.9787, instance_loss: 0.7332, weighted_loss: 1.6050, label: 0, bag_size: 50\n",
      "batch 119, loss: 0.0000, instance_loss: 0.3664, weighted_loss: 0.1099, label: 1, bag_size: 68\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0156, weighted_loss: 0.0047, label: 0, bag_size: 88\n",
      "batch 159, loss: 0.0001, instance_loss: 0.4031, weighted_loss: 0.1210, label: 1, bag_size: 31\n",
      "batch 179, loss: 0.0004, instance_loss: 0.0238, weighted_loss: 0.0074, label: 0, bag_size: 93\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0047, weighted_loss: 0.0014, label: 0, bag_size: 78\n",
      "batch 219, loss: 0.0001, instance_loss: 0.2276, weighted_loss: 0.0684, label: 1, bag_size: 75\n",
      "batch 239, loss: 0.0001, instance_loss: 0.2610, weighted_loss: 0.0784, label: 1, bag_size: 102\n",
      "batch 259, loss: 0.0005, instance_loss: 0.0424, weighted_loss: 0.0131, label: 0, bag_size: 88\n",
      "batch 279, loss: 0.0817, instance_loss: 0.0672, weighted_loss: 0.0773, label: 0, bag_size: 19\n",
      "batch 299, loss: 0.0000, instance_loss: 0.2258, weighted_loss: 0.0678, label: 1, bag_size: 68\n",
      "batch 319, loss: 0.0000, instance_loss: 0.3796, weighted_loss: 0.1139, label: 1, bag_size: 61\n",
      "batch 339, loss: 0.1609, instance_loss: 0.1180, weighted_loss: 0.1481, label: 0, bag_size: 95\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0054, weighted_loss: 0.0017, label: 0, bag_size: 66\n",
      "batch 379, loss: 0.6307, instance_loss: 2.1100, weighted_loss: 1.0745, label: 0, bag_size: 35\n",
      "batch 399, loss: 0.0007, instance_loss: 0.2691, weighted_loss: 0.0812, label: 1, bag_size: 128\n",
      "batch 419, loss: 0.5356, instance_loss: 0.2656, weighted_loss: 0.4546, label: 1, bag_size: 32\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0027, weighted_loss: 0.0008, label: 0, bag_size: 43\n",
      "batch 459, loss: 0.0001, instance_loss: 0.5040, weighted_loss: 0.1513, label: 1, bag_size: 45\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 0, bag_size: 45\n",
      "batch 499, loss: 0.0002, instance_loss: 0.1966, weighted_loss: 0.0591, label: 1, bag_size: 21\n",
      "batch 519, loss: 2.1538, instance_loss: 2.9804, weighted_loss: 2.4018, label: 0, bag_size: 44\n",
      "batch 539, loss: 0.0075, instance_loss: 0.4309, weighted_loss: 0.1345, label: 1, bag_size: 23\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 0, bag_size: 116\n",
      "batch 579, loss: 0.9385, instance_loss: 0.9599, weighted_loss: 0.9449, label: 0, bag_size: 34\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0025, weighted_loss: 0.0007, label: 0, bag_size: 43\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0044, weighted_loss: 0.0013, label: 0, bag_size: 107\n",
      "batch 639, loss: 0.0506, instance_loss: 0.0175, weighted_loss: 0.0407, label: 0, bag_size: 79\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.948173076923077: correct 9861/10400\n",
      "class 1 clustering acc 0.7540384615384615: correct 3921/5200\n",
      "Epoch: 19, train_loss: 0.2607, train_clustering_loss:  0.4147, train_error: 0.0954\n",
      "class 0: acc 0.8754448398576512, correct 246/281\n",
      "class 1: acc 0.926829268292683, correct 342/369\n",
      "\n",
      "Val Set, val_loss: 0.4283, val_error: 0.1505, auc: 0.9565\n",
      "class 0 clustering acc 0.5772849462365591: correct 859/1488\n",
      "class 1 clustering acc 0.8051075268817204: correct 599/744\n",
      "class 0: acc 0.8297872340425532, correct 39/47\n",
      "class 1: acc 0.8695652173913043, correct 40/46\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0460, weighted_loss: 0.0138, label: 1, bag_size: 19\n",
      "batch 39, loss: 3.6706, instance_loss: 5.2215, weighted_loss: 4.1358, label: 1, bag_size: 67\n",
      "batch 59, loss: 0.0153, instance_loss: 0.2357, weighted_loss: 0.0814, label: 1, bag_size: 47\n",
      "batch 79, loss: 0.8355, instance_loss: 1.5945, weighted_loss: 1.0632, label: 1, bag_size: 73\n",
      "batch 99, loss: 0.1447, instance_loss: 0.5098, weighted_loss: 0.2542, label: 1, bag_size: 82\n",
      "batch 119, loss: 0.0127, instance_loss: 0.1594, weighted_loss: 0.0567, label: 0, bag_size: 75\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0901, weighted_loss: 0.0270, label: 0, bag_size: 113\n",
      "batch 159, loss: 0.0338, instance_loss: 0.6009, weighted_loss: 0.2039, label: 1, bag_size: 36\n",
      "batch 179, loss: 2.3686, instance_loss: 0.8597, weighted_loss: 1.9159, label: 1, bag_size: 76\n",
      "batch 199, loss: 0.7807, instance_loss: 1.0957, weighted_loss: 0.8752, label: 0, bag_size: 35\n",
      "batch 219, loss: 0.2317, instance_loss: 0.0961, weighted_loss: 0.1910, label: 1, bag_size: 105\n",
      "batch 239, loss: 0.0000, instance_loss: 0.1028, weighted_loss: 0.0308, label: 1, bag_size: 70\n",
      "batch 259, loss: 0.0040, instance_loss: 0.0259, weighted_loss: 0.0105, label: 0, bag_size: 63\n",
      "batch 279, loss: 0.0002, instance_loss: 0.0811, weighted_loss: 0.0245, label: 1, bag_size: 52\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0291, weighted_loss: 0.0087, label: 1, bag_size: 91\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0533, weighted_loss: 0.0160, label: 1, bag_size: 80\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0088, weighted_loss: 0.0026, label: 0, bag_size: 49\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0397, weighted_loss: 0.0119, label: 1, bag_size: 47\n",
      "batch 379, loss: 0.0003, instance_loss: 0.0018, weighted_loss: 0.0008, label: 0, bag_size: 110\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0193, weighted_loss: 0.0058, label: 0, bag_size: 50\n",
      "batch 419, loss: 2.7499, instance_loss: 0.6403, weighted_loss: 2.1170, label: 1, bag_size: 46\n",
      "batch 439, loss: 0.0306, instance_loss: 0.2635, weighted_loss: 0.1005, label: 1, bag_size: 99\n",
      "batch 459, loss: 0.0026, instance_loss: 0.2427, weighted_loss: 0.0746, label: 0, bag_size: 75\n",
      "batch 479, loss: 0.1424, instance_loss: 0.7025, weighted_loss: 0.3105, label: 1, bag_size: 100\n",
      "batch 499, loss: 0.5785, instance_loss: 0.1037, weighted_loss: 0.4361, label: 1, bag_size: 53\n",
      "batch 519, loss: 0.0589, instance_loss: 0.1566, weighted_loss: 0.0882, label: 1, bag_size: 115\n",
      "batch 539, loss: 0.0234, instance_loss: 2.0815, weighted_loss: 0.6409, label: 1, bag_size: 89\n",
      "batch 559, loss: 0.0000, instance_loss: 0.1753, weighted_loss: 0.0526, label: 1, bag_size: 98\n",
      "batch 579, loss: 2.3762, instance_loss: 0.2230, weighted_loss: 1.7302, label: 0, bag_size: 67\n",
      "batch 599, loss: 0.0002, instance_loss: 0.7418, weighted_loss: 0.2227, label: 1, bag_size: 85\n",
      "batch 619, loss: 0.0000, instance_loss: 0.1203, weighted_loss: 0.0361, label: 1, bag_size: 80\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0024, weighted_loss: 0.0007, label: 1, bag_size: 100\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9500961538461539: correct 9881/10400\n",
      "class 1 clustering acc 0.7563461538461539: correct 3933/5200\n",
      "Epoch: 20, train_loss: 0.5342, train_clustering_loss:  0.4597, train_error: 0.1369\n",
      "class 0: acc 0.8456375838926175, correct 252/298\n",
      "class 1: acc 0.8778409090909091, correct 309/352\n",
      "\n",
      "Val Set, val_loss: 0.7741, val_error: 0.1398, auc: 0.9433\n",
      "class 0 clustering acc 0.8991935483870968: correct 1338/1488\n",
      "class 1 clustering acc 0.7540322580645161: correct 561/744\n",
      "class 0: acc 0.7872340425531915, correct 37/47\n",
      "class 1: acc 0.9347826086956522, correct 43/46\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0498, weighted_loss: 0.0149, label: 0, bag_size: 61\n",
      "batch 39, loss: 0.0580, instance_loss: 0.1607, weighted_loss: 0.0888, label: 0, bag_size: 103\n",
      "batch 59, loss: 0.0214, instance_loss: 0.0010, weighted_loss: 0.0153, label: 1, bag_size: 84\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0083, weighted_loss: 0.0025, label: 1, bag_size: 42\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0073, weighted_loss: 0.0022, label: 1, bag_size: 31\n",
      "batch 119, loss: 0.0023, instance_loss: 0.0230, weighted_loss: 0.0085, label: 1, bag_size: 115\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0395, weighted_loss: 0.0119, label: 0, bag_size: 43\n",
      "batch 159, loss: 0.0019, instance_loss: 0.6906, weighted_loss: 0.2085, label: 1, bag_size: 53\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0561, weighted_loss: 0.0168, label: 0, bag_size: 35\n",
      "batch 199, loss: 0.0570, instance_loss: 0.6522, weighted_loss: 0.2356, label: 0, bag_size: 75\n",
      "batch 219, loss: 0.0312, instance_loss: 0.1414, weighted_loss: 0.0643, label: 0, bag_size: 74\n",
      "batch 239, loss: 0.0007, instance_loss: 0.1654, weighted_loss: 0.0501, label: 1, bag_size: 41\n",
      "batch 259, loss: 0.0335, instance_loss: 0.0250, weighted_loss: 0.0310, label: 1, bag_size: 89\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0241, weighted_loss: 0.0073, label: 0, bag_size: 36\n",
      "batch 299, loss: 0.0001, instance_loss: 0.6547, weighted_loss: 0.1965, label: 0, bag_size: 28\n",
      "batch 319, loss: 0.0151, instance_loss: 0.0251, weighted_loss: 0.0181, label: 0, bag_size: 54\n",
      "batch 339, loss: 0.0005, instance_loss: 0.0281, weighted_loss: 0.0088, label: 1, bag_size: 42\n",
      "batch 359, loss: 0.2212, instance_loss: 0.6338, weighted_loss: 0.3450, label: 1, bag_size: 57\n",
      "batch 379, loss: 0.0120, instance_loss: 0.0476, weighted_loss: 0.0226, label: 1, bag_size: 64\n",
      "batch 399, loss: 0.0001, instance_loss: 0.0301, weighted_loss: 0.0091, label: 0, bag_size: 88\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 1, bag_size: 85\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0370, weighted_loss: 0.0111, label: 1, bag_size: 30\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0244, weighted_loss: 0.0073, label: 1, bag_size: 98\n",
      "batch 479, loss: 0.0002, instance_loss: 0.0573, weighted_loss: 0.0173, label: 1, bag_size: 52\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0046, weighted_loss: 0.0014, label: 0, bag_size: 104\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0260, weighted_loss: 0.0078, label: 0, bag_size: 108\n",
      "batch 539, loss: 0.0373, instance_loss: 0.1629, weighted_loss: 0.0750, label: 1, bag_size: 53\n",
      "batch 559, loss: 0.0071, instance_loss: 0.0092, weighted_loss: 0.0077, label: 0, bag_size: 25\n",
      "batch 579, loss: 1.6600, instance_loss: 0.2178, weighted_loss: 1.2273, label: 1, bag_size: 116\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0135, weighted_loss: 0.0040, label: 0, bag_size: 27\n",
      "batch 619, loss: 0.0006, instance_loss: 0.0106, weighted_loss: 0.0036, label: 0, bag_size: 88\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0141, weighted_loss: 0.0043, label: 1, bag_size: 92\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9620192307692308: correct 10005/10400\n",
      "class 1 clustering acc 0.8211538461538461: correct 4270/5200\n",
      "Epoch: 21, train_loss: 0.3171, train_clustering_loss:  0.3307, train_error: 0.1031\n",
      "class 0: acc 0.8924050632911392, correct 282/316\n",
      "class 1: acc 0.9011976047904192, correct 301/334\n",
      "\n",
      "Val Set, val_loss: 0.7040, val_error: 0.1505, auc: 0.9302\n",
      "class 0 clustering acc 0.9321236559139785: correct 1387/1488\n",
      "class 1 clustering acc 0.7486559139784946: correct 557/744\n",
      "class 0: acc 0.8297872340425532, correct 39/47\n",
      "class 1: acc 0.8695652173913043, correct 40/46\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0487, weighted_loss: 0.0147, label: 1, bag_size: 41\n",
      "batch 39, loss: 0.0259, instance_loss: 0.0872, weighted_loss: 0.0443, label: 0, bag_size: 57\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0089, weighted_loss: 0.0027, label: 0, bag_size: 58\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0170, weighted_loss: 0.0051, label: 0, bag_size: 67\n",
      "batch 99, loss: 0.0123, instance_loss: 0.0115, weighted_loss: 0.0120, label: 1, bag_size: 97\n",
      "batch 119, loss: 0.0109, instance_loss: 0.0121, weighted_loss: 0.0113, label: 0, bag_size: 26\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0031, weighted_loss: 0.0009, label: 0, bag_size: 39\n",
      "batch 159, loss: 0.2796, instance_loss: 2.9114, weighted_loss: 1.0691, label: 0, bag_size: 54\n",
      "batch 179, loss: 0.0000, instance_loss: 0.2522, weighted_loss: 0.0757, label: 0, bag_size: 35\n",
      "batch 199, loss: 0.8214, instance_loss: 0.1333, weighted_loss: 0.6150, label: 1, bag_size: 70\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0041, weighted_loss: 0.0013, label: 0, bag_size: 83\n",
      "batch 239, loss: 0.0000, instance_loss: 0.4045, weighted_loss: 0.1214, label: 1, bag_size: 56\n",
      "batch 259, loss: 0.0401, instance_loss: 0.5689, weighted_loss: 0.1987, label: 1, bag_size: 17\n",
      "batch 279, loss: 0.3028, instance_loss: 1.5013, weighted_loss: 0.6623, label: 1, bag_size: 76\n",
      "batch 299, loss: 0.0316, instance_loss: 0.4338, weighted_loss: 0.1523, label: 0, bag_size: 107\n",
      "batch 319, loss: 0.0014, instance_loss: 0.1777, weighted_loss: 0.0543, label: 0, bag_size: 50\n",
      "batch 339, loss: 0.0177, instance_loss: 0.6004, weighted_loss: 0.1925, label: 1, bag_size: 64\n",
      "batch 359, loss: 0.0008, instance_loss: 0.0583, weighted_loss: 0.0181, label: 0, bag_size: 51\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0190, weighted_loss: 0.0057, label: 0, bag_size: 91\n",
      "batch 399, loss: 0.0395, instance_loss: 0.3755, weighted_loss: 0.1403, label: 1, bag_size: 92\n",
      "batch 419, loss: 0.4147, instance_loss: 0.5166, weighted_loss: 0.4452, label: 1, bag_size: 31\n",
      "batch 439, loss: 0.0271, instance_loss: 0.0787, weighted_loss: 0.0426, label: 0, bag_size: 18\n",
      "batch 459, loss: 0.0022, instance_loss: 0.6456, weighted_loss: 0.1952, label: 1, bag_size: 59\n",
      "batch 479, loss: 0.0407, instance_loss: 0.0159, weighted_loss: 0.0332, label: 0, bag_size: 78\n",
      "batch 499, loss: 0.0129, instance_loss: 0.2647, weighted_loss: 0.0885, label: 0, bag_size: 52\n",
      "batch 519, loss: 0.0076, instance_loss: 0.1129, weighted_loss: 0.0392, label: 0, bag_size: 50\n",
      "batch 539, loss: 1.7022, instance_loss: 0.6655, weighted_loss: 1.3912, label: 1, bag_size: 67\n",
      "batch 559, loss: 0.0000, instance_loss: 0.2453, weighted_loss: 0.0736, label: 1, bag_size: 45\n",
      "batch 579, loss: 0.0006, instance_loss: 0.1323, weighted_loss: 0.0401, label: 1, bag_size: 127\n",
      "batch 599, loss: 0.0000, instance_loss: 0.1486, weighted_loss: 0.0446, label: 1, bag_size: 30\n",
      "batch 619, loss: 0.0002, instance_loss: 0.3894, weighted_loss: 0.1170, label: 1, bag_size: 70\n",
      "batch 639, loss: 2.6840, instance_loss: 2.5127, weighted_loss: 2.6326, label: 0, bag_size: 40\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9511538461538461: correct 9892/10400\n",
      "class 1 clustering acc 0.7680769230769231: correct 3994/5200\n",
      "Epoch: 22, train_loss: 0.3440, train_clustering_loss:  0.4333, train_error: 0.0892\n",
      "class 0: acc 0.9123867069486404, correct 302/331\n",
      "class 1: acc 0.9090909090909091, correct 290/319\n",
      "\n",
      "Val Set, val_loss: 0.5448, val_error: 0.1290, auc: 0.9454\n",
      "class 0 clustering acc 0.6592741935483871: correct 981/1488\n",
      "class 1 clustering acc 0.7647849462365591: correct 569/744\n",
      "class 0: acc 0.9787234042553191, correct 46/47\n",
      "class 1: acc 0.7608695652173914, correct 35/46\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1086, instance_loss: 1.1074, weighted_loss: 0.4083, label: 0, bag_size: 53\n",
      "batch 39, loss: 0.1090, instance_loss: 0.8823, weighted_loss: 0.3410, label: 1, bag_size: 21\n",
      "batch 59, loss: 0.0001, instance_loss: 0.0961, weighted_loss: 0.0289, label: 1, bag_size: 95\n",
      "batch 79, loss: 0.0000, instance_loss: 0.1257, weighted_loss: 0.0377, label: 1, bag_size: 79\n",
      "batch 99, loss: 0.0024, instance_loss: 0.7209, weighted_loss: 0.2180, label: 1, bag_size: 70\n",
      "batch 119, loss: 0.0174, instance_loss: 0.3130, weighted_loss: 0.1061, label: 1, bag_size: 59\n",
      "batch 139, loss: 1.8723, instance_loss: 0.4661, weighted_loss: 1.4504, label: 0, bag_size: 88\n",
      "batch 159, loss: 0.0010, instance_loss: 0.1247, weighted_loss: 0.0381, label: 0, bag_size: 25\n",
      "batch 179, loss: 0.0000, instance_loss: 0.1255, weighted_loss: 0.0377, label: 1, bag_size: 35\n",
      "batch 199, loss: 0.0000, instance_loss: 0.2371, weighted_loss: 0.0711, label: 1, bag_size: 97\n",
      "batch 219, loss: 0.0000, instance_loss: 0.1464, weighted_loss: 0.0439, label: 0, bag_size: 75\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0162, weighted_loss: 0.0049, label: 1, bag_size: 37\n",
      "batch 259, loss: 0.0000, instance_loss: 0.2443, weighted_loss: 0.0733, label: 1, bag_size: 107\n",
      "batch 279, loss: 0.0006, instance_loss: 0.2478, weighted_loss: 0.0748, label: 0, bag_size: 48\n",
      "batch 299, loss: 0.0000, instance_loss: 0.3074, weighted_loss: 0.0922, label: 1, bag_size: 35\n",
      "batch 319, loss: 0.0000, instance_loss: 0.3041, weighted_loss: 0.0912, label: 1, bag_size: 37\n",
      "batch 339, loss: 3.3865, instance_loss: 0.8653, weighted_loss: 2.6301, label: 1, bag_size: 119\n",
      "batch 359, loss: 0.0000, instance_loss: 0.3046, weighted_loss: 0.0914, label: 1, bag_size: 127\n",
      "batch 379, loss: 0.0027, instance_loss: 0.7013, weighted_loss: 0.2123, label: 0, bag_size: 79\n",
      "batch 399, loss: 5.6336, instance_loss: 0.1248, weighted_loss: 3.9810, label: 0, bag_size: 62\n",
      "batch 419, loss: 0.0779, instance_loss: 0.9822, weighted_loss: 0.3492, label: 0, bag_size: 25\n",
      "batch 439, loss: 0.0006, instance_loss: 0.1114, weighted_loss: 0.0338, label: 1, bag_size: 85\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0166, weighted_loss: 0.0050, label: 0, bag_size: 80\n",
      "batch 479, loss: 2.2927, instance_loss: 0.0255, weighted_loss: 1.6125, label: 0, bag_size: 65\n",
      "batch 499, loss: 0.0093, instance_loss: 0.1621, weighted_loss: 0.0551, label: 1, bag_size: 24\n",
      "batch 519, loss: 0.0002, instance_loss: 0.5495, weighted_loss: 0.1650, label: 1, bag_size: 96\n",
      "batch 539, loss: 0.0125, instance_loss: 0.5419, weighted_loss: 0.1713, label: 1, bag_size: 86\n",
      "batch 559, loss: 9.8593, instance_loss: 0.4898, weighted_loss: 7.0485, label: 1, bag_size: 51\n",
      "batch 579, loss: 0.0349, instance_loss: 0.7676, weighted_loss: 0.2547, label: 1, bag_size: 85\n",
      "batch 599, loss: 0.0000, instance_loss: 0.1635, weighted_loss: 0.0490, label: 1, bag_size: 74\n",
      "batch 619, loss: 0.0004, instance_loss: 0.3014, weighted_loss: 0.0907, label: 1, bag_size: 77\n",
      "batch 639, loss: 0.0002, instance_loss: 0.1254, weighted_loss: 0.0377, label: 1, bag_size: 45\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9413461538461538: correct 9790/10400\n",
      "class 1 clustering acc 0.7313461538461539: correct 3803/5200\n",
      "Epoch: 23, train_loss: 0.6703, train_clustering_loss:  0.5277, train_error: 0.1338\n",
      "class 0: acc 0.8631921824104235, correct 265/307\n",
      "class 1: acc 0.8688046647230321, correct 298/343\n",
      "\n",
      "Val Set, val_loss: 1.0643, val_error: 0.1613, auc: 0.9265\n",
      "class 0 clustering acc 0.7157258064516129: correct 1065/1488\n",
      "class 1 clustering acc 0.7365591397849462: correct 548/744\n",
      "class 0: acc 0.9787234042553191, correct 46/47\n",
      "class 1: acc 0.6956521739130435, correct 32/46\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0746, instance_loss: 0.0947, weighted_loss: 0.0806, label: 0, bag_size: 107\n",
      "batch 39, loss: 0.0144, instance_loss: 0.2109, weighted_loss: 0.0734, label: 1, bag_size: 82\n",
      "batch 59, loss: 0.0272, instance_loss: 1.4003, weighted_loss: 0.4391, label: 1, bag_size: 99\n",
      "batch 79, loss: 0.0018, instance_loss: 0.4282, weighted_loss: 0.1297, label: 1, bag_size: 100\n",
      "batch 99, loss: 0.6665, instance_loss: 0.3278, weighted_loss: 0.5649, label: 1, bag_size: 81\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0751, weighted_loss: 0.0225, label: 0, bag_size: 42\n",
      "batch 139, loss: 3.6443, instance_loss: 1.2831, weighted_loss: 2.9360, label: 1, bag_size: 78\n",
      "batch 159, loss: 0.0000, instance_loss: 0.2924, weighted_loss: 0.0877, label: 0, bag_size: 86\n",
      "batch 179, loss: 3.4030, instance_loss: 2.5936, weighted_loss: 3.1602, label: 0, bag_size: 69\n",
      "batch 199, loss: 0.0236, instance_loss: 0.2891, weighted_loss: 0.1033, label: 1, bag_size: 15\n",
      "batch 219, loss: 0.0114, instance_loss: 0.2081, weighted_loss: 0.0704, label: 1, bag_size: 70\n",
      "batch 239, loss: 1.4691, instance_loss: 3.3383, weighted_loss: 2.0298, label: 1, bag_size: 65\n",
      "batch 259, loss: 0.0008, instance_loss: 0.5047, weighted_loss: 0.1520, label: 1, bag_size: 84\n",
      "batch 279, loss: 0.0008, instance_loss: 0.1536, weighted_loss: 0.0466, label: 0, bag_size: 43\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0086, weighted_loss: 0.0026, label: 0, bag_size: 86\n",
      "batch 319, loss: 0.0001, instance_loss: 0.1492, weighted_loss: 0.0448, label: 1, bag_size: 90\n",
      "batch 339, loss: 0.0019, instance_loss: 0.0483, weighted_loss: 0.0158, label: 0, bag_size: 28\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0672, weighted_loss: 0.0202, label: 0, bag_size: 83\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0345, weighted_loss: 0.0104, label: 0, bag_size: 51\n",
      "batch 399, loss: 0.0002, instance_loss: 0.1256, weighted_loss: 0.0378, label: 1, bag_size: 71\n",
      "batch 419, loss: 0.0003, instance_loss: 0.0820, weighted_loss: 0.0248, label: 0, bag_size: 79\n",
      "batch 439, loss: 0.2165, instance_loss: 1.2936, weighted_loss: 0.5396, label: 1, bag_size: 31\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0384, weighted_loss: 0.0115, label: 0, bag_size: 43\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0208, weighted_loss: 0.0062, label: 0, bag_size: 65\n",
      "batch 499, loss: 0.0016, instance_loss: 0.0800, weighted_loss: 0.0251, label: 0, bag_size: 54\n",
      "batch 519, loss: 0.0026, instance_loss: 0.0969, weighted_loss: 0.0309, label: 1, bag_size: 20\n",
      "batch 539, loss: 0.0008, instance_loss: 0.3403, weighted_loss: 0.1027, label: 1, bag_size: 32\n",
      "batch 559, loss: 0.0005, instance_loss: 1.4425, weighted_loss: 0.4331, label: 1, bag_size: 18\n",
      "batch 579, loss: 2.3690, instance_loss: 0.1240, weighted_loss: 1.6955, label: 0, bag_size: 31\n",
      "batch 599, loss: 3.1665, instance_loss: 1.2962, weighted_loss: 2.6054, label: 1, bag_size: 84\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0059, weighted_loss: 0.0018, label: 0, bag_size: 67\n",
      "batch 639, loss: 0.0000, instance_loss: 0.1740, weighted_loss: 0.0522, label: 1, bag_size: 95\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9416346153846153: correct 9793/10400\n",
      "class 1 clustering acc 0.7023076923076923: correct 3652/5200\n",
      "Epoch: 24, train_loss: 0.7339, train_clustering_loss:  0.5217, train_error: 0.1862\n",
      "class 0: acc 0.8154761904761905, correct 274/336\n",
      "class 1: acc 0.8121019108280255, correct 255/314\n",
      "\n",
      "Val Set, val_loss: 5.6739, val_error: 0.4194, auc: 0.9514\n",
      "class 0 clustering acc 0.9469086021505376: correct 1409/1488\n",
      "class 1 clustering acc 0.5456989247311828: correct 406/744\n",
      "class 0: acc 1.0, correct 47/47\n",
      "class 1: acc 0.15217391304347827, correct 7/46\n",
      "EarlyStopping counter: 12 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0004, instance_loss: 1.0899, weighted_loss: 0.3273, label: 0, bag_size: 33\n",
      "batch 39, loss: 0.5435, instance_loss: 1.1773, weighted_loss: 0.7336, label: 0, bag_size: 28\n",
      "batch 59, loss: 0.3361, instance_loss: 0.6506, weighted_loss: 0.4304, label: 0, bag_size: 28\n",
      "batch 79, loss: 8.6566, instance_loss: 2.4518, weighted_loss: 6.7952, label: 0, bag_size: 93\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0074, weighted_loss: 0.0022, label: 0, bag_size: 95\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0844, weighted_loss: 0.0253, label: 0, bag_size: 112\n",
      "batch 139, loss: 0.0390, instance_loss: 0.5643, weighted_loss: 0.1966, label: 1, bag_size: 29\n",
      "batch 159, loss: 0.0212, instance_loss: 0.0791, weighted_loss: 0.0385, label: 1, bag_size: 17\n",
      "batch 179, loss: 0.0176, instance_loss: 0.2495, weighted_loss: 0.0871, label: 1, bag_size: 33\n",
      "batch 199, loss: 1.5257, instance_loss: 0.1764, weighted_loss: 1.1209, label: 1, bag_size: 115\n",
      "batch 219, loss: 0.0168, instance_loss: 0.5784, weighted_loss: 0.1853, label: 0, bag_size: 46\n",
      "batch 239, loss: 0.1740, instance_loss: 0.1148, weighted_loss: 0.1563, label: 0, bag_size: 33\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0203, weighted_loss: 0.0061, label: 1, bag_size: 98\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0837, weighted_loss: 0.0252, label: 0, bag_size: 25\n",
      "batch 299, loss: 0.0007, instance_loss: 0.0227, weighted_loss: 0.0073, label: 1, bag_size: 95\n",
      "batch 319, loss: 0.0036, instance_loss: 0.0877, weighted_loss: 0.0288, label: 1, bag_size: 45\n",
      "batch 339, loss: 0.0002, instance_loss: 0.1401, weighted_loss: 0.0422, label: 1, bag_size: 85\n",
      "batch 359, loss: 1.7251, instance_loss: 0.1643, weighted_loss: 1.2569, label: 1, bag_size: 88\n",
      "batch 379, loss: 0.0007, instance_loss: 0.1585, weighted_loss: 0.0480, label: 0, bag_size: 54\n",
      "batch 399, loss: 0.0005, instance_loss: 0.3281, weighted_loss: 0.0988, label: 1, bag_size: 121\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0321, weighted_loss: 0.0096, label: 0, bag_size: 107\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0602, weighted_loss: 0.0181, label: 1, bag_size: 38\n",
      "batch 459, loss: 0.0004, instance_loss: 0.0421, weighted_loss: 0.0129, label: 0, bag_size: 73\n",
      "batch 479, loss: 0.0260, instance_loss: 0.0886, weighted_loss: 0.0448, label: 0, bag_size: 54\n",
      "batch 499, loss: 0.0145, instance_loss: 2.7349, weighted_loss: 0.8306, label: 1, bag_size: 35\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0314, weighted_loss: 0.0094, label: 0, bag_size: 65\n",
      "batch 539, loss: 3.0976, instance_loss: 2.7117, weighted_loss: 2.9819, label: 0, bag_size: 55\n",
      "batch 559, loss: 0.0012, instance_loss: 0.0152, weighted_loss: 0.0054, label: 1, bag_size: 38\n",
      "batch 579, loss: 0.0004, instance_loss: 0.3536, weighted_loss: 0.1064, label: 1, bag_size: 20\n",
      "batch 599, loss: 0.0003, instance_loss: 0.0495, weighted_loss: 0.0151, label: 0, bag_size: 24\n",
      "batch 619, loss: 1.0238, instance_loss: 0.3879, weighted_loss: 0.8330, label: 1, bag_size: 36\n",
      "batch 639, loss: 1.0477, instance_loss: 0.3972, weighted_loss: 0.8526, label: 0, bag_size: 98\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9411538461538461: correct 9788/10400\n",
      "class 1 clustering acc 0.729423076923077: correct 3793/5200\n",
      "Epoch: 25, train_loss: 1.0122, train_clustering_loss:  0.5125, train_error: 0.1954\n",
      "class 0: acc 0.815028901734104, correct 282/346\n",
      "class 1: acc 0.7927631578947368, correct 241/304\n",
      "\n",
      "Val Set, val_loss: 0.6957, val_error: 0.1935, auc: 0.9459\n",
      "class 0 clustering acc 0.9798387096774194: correct 1458/1488\n",
      "class 1 clustering acc 0.7674731182795699: correct 571/744\n",
      "class 0: acc 1.0, correct 47/47\n",
      "class 1: acc 0.6086956521739131, correct 28/46\n",
      "EarlyStopping counter: 13 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0238, weighted_loss: 0.0073, label: 0, bag_size: 55\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0125, weighted_loss: 0.0038, label: 0, bag_size: 29\n",
      "batch 59, loss: 0.2005, instance_loss: 1.0299, weighted_loss: 0.4493, label: 0, bag_size: 35\n",
      "batch 79, loss: 0.0001, instance_loss: 0.0067, weighted_loss: 0.0021, label: 1, bag_size: 78\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0042, weighted_loss: 0.0013, label: 0, bag_size: 95\n",
      "batch 119, loss: 0.0286, instance_loss: 0.0066, weighted_loss: 0.0220, label: 0, bag_size: 78\n",
      "batch 139, loss: 0.0321, instance_loss: 0.1097, weighted_loss: 0.0554, label: 1, bag_size: 30\n",
      "batch 159, loss: 0.0012, instance_loss: 0.2156, weighted_loss: 0.0655, label: 1, bag_size: 101\n",
      "batch 179, loss: 0.0557, instance_loss: 0.3260, weighted_loss: 0.1368, label: 1, bag_size: 100\n",
      "batch 199, loss: 0.0964, instance_loss: 0.2804, weighted_loss: 0.1516, label: 1, bag_size: 46\n",
      "batch 219, loss: 0.0131, instance_loss: 0.0326, weighted_loss: 0.0189, label: 1, bag_size: 78\n",
      "batch 239, loss: 0.0007, instance_loss: 0.0039, weighted_loss: 0.0016, label: 1, bag_size: 61\n",
      "batch 259, loss: 0.3226, instance_loss: 0.2673, weighted_loss: 0.3060, label: 1, bag_size: 62\n",
      "batch 279, loss: 0.0491, instance_loss: 0.0073, weighted_loss: 0.0365, label: 0, bag_size: 25\n",
      "batch 299, loss: 0.0002, instance_loss: 0.0019, weighted_loss: 0.0007, label: 0, bag_size: 39\n",
      "batch 319, loss: 0.0016, instance_loss: 0.0198, weighted_loss: 0.0071, label: 1, bag_size: 86\n",
      "batch 339, loss: 1.5380, instance_loss: 0.6794, weighted_loss: 1.2804, label: 1, bag_size: 30\n",
      "batch 359, loss: 0.0341, instance_loss: 0.0978, weighted_loss: 0.0532, label: 0, bag_size: 13\n",
      "batch 379, loss: 0.0008, instance_loss: 0.0154, weighted_loss: 0.0052, label: 1, bag_size: 85\n",
      "batch 399, loss: 0.0396, instance_loss: 0.0464, weighted_loss: 0.0416, label: 0, bag_size: 35\n",
      "batch 419, loss: 0.0068, instance_loss: 0.0059, weighted_loss: 0.0065, label: 0, bag_size: 25\n",
      "batch 439, loss: 0.0000, instance_loss: 0.1797, weighted_loss: 0.0539, label: 1, bag_size: 89\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0075, weighted_loss: 0.0023, label: 1, bag_size: 85\n",
      "batch 479, loss: 0.0002, instance_loss: 0.0095, weighted_loss: 0.0030, label: 0, bag_size: 25\n",
      "batch 499, loss: 0.0070, instance_loss: 0.0045, weighted_loss: 0.0062, label: 0, bag_size: 108\n",
      "batch 519, loss: 0.2568, instance_loss: 0.1901, weighted_loss: 0.2368, label: 1, bag_size: 97\n",
      "batch 539, loss: 5.8913, instance_loss: 2.3666, weighted_loss: 4.8339, label: 0, bag_size: 58\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0232, weighted_loss: 0.0070, label: 1, bag_size: 122\n",
      "batch 579, loss: 0.1173, instance_loss: 0.8144, weighted_loss: 0.3265, label: 0, bag_size: 36\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0033, weighted_loss: 0.0011, label: 0, bag_size: 83\n",
      "batch 619, loss: 0.0007, instance_loss: 0.0287, weighted_loss: 0.0091, label: 0, bag_size: 86\n",
      "batch 639, loss: 0.5232, instance_loss: 0.1251, weighted_loss: 0.4038, label: 1, bag_size: 51\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.965: correct 10036/10400\n",
      "class 1 clustering acc 0.833076923076923: correct 4332/5200\n",
      "Epoch: 26, train_loss: 0.4736, train_clustering_loss:  0.3324, train_error: 0.1338\n",
      "class 0: acc 0.8656716417910447, correct 290/335\n",
      "class 1: acc 0.8666666666666667, correct 273/315\n",
      "\n",
      "Val Set, val_loss: 0.2493, val_error: 0.1183, auc: 0.9727\n",
      "class 0 clustering acc 0.9637096774193549: correct 1434/1488\n",
      "class 1 clustering acc 0.8709677419354839: correct 648/744\n",
      "class 0: acc 0.8297872340425532, correct 39/47\n",
      "class 1: acc 0.9347826086956522, correct 43/46\n",
      "Validation loss decreased (0.268616 --> 0.249291).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0052, weighted_loss: 0.0016, label: 1, bag_size: 81\n",
      "batch 39, loss: 0.0034, instance_loss: 0.0117, weighted_loss: 0.0059, label: 0, bag_size: 61\n",
      "batch 59, loss: 2.9077, instance_loss: 0.7206, weighted_loss: 2.2516, label: 1, bag_size: 43\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0258, weighted_loss: 0.0078, label: 1, bag_size: 97\n",
      "batch 99, loss: 0.0002, instance_loss: 0.0310, weighted_loss: 0.0094, label: 1, bag_size: 25\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0132, weighted_loss: 0.0040, label: 1, bag_size: 49\n",
      "batch 139, loss: 0.0722, instance_loss: 0.1758, weighted_loss: 0.1032, label: 1, bag_size: 84\n",
      "batch 159, loss: 6.3881, instance_loss: 3.2639, weighted_loss: 5.4508, label: 0, bag_size: 82\n",
      "batch 179, loss: 0.0046, instance_loss: 0.2242, weighted_loss: 0.0705, label: 1, bag_size: 38\n",
      "batch 199, loss: 0.0183, instance_loss: 0.0233, weighted_loss: 0.0198, label: 0, bag_size: 62\n",
      "batch 219, loss: 0.3071, instance_loss: 0.7731, weighted_loss: 0.4469, label: 1, bag_size: 41\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 0, bag_size: 91\n",
      "batch 259, loss: 2.2163, instance_loss: 1.5301, weighted_loss: 2.0105, label: 0, bag_size: 40\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0134, weighted_loss: 0.0041, label: 0, bag_size: 126\n",
      "batch 299, loss: 0.0003, instance_loss: 0.0007, weighted_loss: 0.0005, label: 0, bag_size: 81\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0044, weighted_loss: 0.0013, label: 0, bag_size: 91\n",
      "batch 339, loss: 0.0010, instance_loss: 0.0064, weighted_loss: 0.0026, label: 0, bag_size: 67\n",
      "batch 359, loss: 0.0047, instance_loss: 0.0103, weighted_loss: 0.0064, label: 1, bag_size: 89\n",
      "batch 379, loss: 1.0758, instance_loss: 1.7068, weighted_loss: 1.2651, label: 0, bag_size: 78\n",
      "batch 399, loss: 0.0015, instance_loss: 0.0048, weighted_loss: 0.0025, label: 0, bag_size: 78\n",
      "batch 419, loss: 0.0064, instance_loss: 0.0213, weighted_loss: 0.0108, label: 0, bag_size: 48\n",
      "batch 439, loss: 0.0046, instance_loss: 0.0076, weighted_loss: 0.0055, label: 1, bag_size: 84\n",
      "batch 459, loss: 0.0156, instance_loss: 0.5959, weighted_loss: 0.1897, label: 1, bag_size: 85\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0031, weighted_loss: 0.0010, label: 1, bag_size: 79\n",
      "batch 499, loss: 3.2892, instance_loss: 2.7817, weighted_loss: 3.1370, label: 1, bag_size: 119\n",
      "batch 519, loss: 0.0076, instance_loss: 0.0394, weighted_loss: 0.0171, label: 0, bag_size: 78\n",
      "batch 539, loss: 0.0567, instance_loss: 0.4258, weighted_loss: 0.1674, label: 0, bag_size: 62\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 0, bag_size: 101\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 0, bag_size: 57\n",
      "batch 599, loss: 0.7758, instance_loss: 0.2752, weighted_loss: 0.6256, label: 1, bag_size: 36\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0005, label: 0, bag_size: 87\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0058, weighted_loss: 0.0018, label: 1, bag_size: 41\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9696153846153847: correct 10084/10400\n",
      "class 1 clustering acc 0.8630769230769231: correct 4488/5200\n",
      "Epoch: 27, train_loss: 0.3136, train_clustering_loss:  0.2789, train_error: 0.1185\n",
      "class 0: acc 0.8834355828220859, correct 288/326\n",
      "class 1: acc 0.8796296296296297, correct 285/324\n",
      "\n",
      "Val Set, val_loss: 0.4620, val_error: 0.2043, auc: 0.9399\n",
      "class 0 clustering acc 0.9818548387096774: correct 1461/1488\n",
      "class 1 clustering acc 0.739247311827957: correct 550/744\n",
      "class 0: acc 0.6808510638297872, correct 32/47\n",
      "class 1: acc 0.9130434782608695, correct 42/46\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0054, instance_loss: 0.0028, weighted_loss: 0.0046, label: 1, bag_size: 72\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0280, weighted_loss: 0.0084, label: 0, bag_size: 87\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 65\n",
      "batch 79, loss: 0.1199, instance_loss: 0.0106, weighted_loss: 0.0871, label: 0, bag_size: 51\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0005, label: 1, bag_size: 98\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 122\n",
      "batch 139, loss: 0.0007, instance_loss: 0.0642, weighted_loss: 0.0198, label: 0, bag_size: 42\n",
      "batch 159, loss: 0.0023, instance_loss: 0.0271, weighted_loss: 0.0098, label: 0, bag_size: 33\n",
      "batch 179, loss: 0.0251, instance_loss: 0.0118, weighted_loss: 0.0211, label: 1, bag_size: 42\n",
      "batch 199, loss: 0.0009, instance_loss: 0.0237, weighted_loss: 0.0077, label: 0, bag_size: 86\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0145, weighted_loss: 0.0044, label: 0, bag_size: 34\n",
      "batch 239, loss: 0.0000, instance_loss: 0.1289, weighted_loss: 0.0387, label: 1, bag_size: 85\n",
      "batch 259, loss: 0.0091, instance_loss: 0.1722, weighted_loss: 0.0580, label: 0, bag_size: 101\n",
      "batch 279, loss: 0.0038, instance_loss: 0.3432, weighted_loss: 0.1056, label: 1, bag_size: 51\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0206, weighted_loss: 0.0062, label: 0, bag_size: 74\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0509, weighted_loss: 0.0153, label: 1, bag_size: 30\n",
      "batch 339, loss: 0.0033, instance_loss: 0.1494, weighted_loss: 0.0471, label: 0, bag_size: 91\n",
      "batch 359, loss: 0.0012, instance_loss: 0.7508, weighted_loss: 0.2261, label: 0, bag_size: 88\n",
      "batch 379, loss: 0.0010, instance_loss: 0.0132, weighted_loss: 0.0046, label: 1, bag_size: 61\n",
      "batch 399, loss: 2.2668, instance_loss: 2.6610, weighted_loss: 2.3851, label: 1, bag_size: 84\n",
      "batch 419, loss: 0.0000, instance_loss: 0.4339, weighted_loss: 0.1302, label: 1, bag_size: 32\n",
      "batch 439, loss: 0.0000, instance_loss: 0.6664, weighted_loss: 0.1999, label: 0, bag_size: 62\n",
      "batch 459, loss: 0.0032, instance_loss: 0.2853, weighted_loss: 0.0879, label: 1, bag_size: 96\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0003, label: 1, bag_size: 72\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0016, weighted_loss: 0.0005, label: 0, bag_size: 24\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 107\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0023, weighted_loss: 0.0007, label: 1, bag_size: 75\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0001, label: 1, bag_size: 60\n",
      "batch 579, loss: 0.0014, instance_loss: 0.0132, weighted_loss: 0.0049, label: 0, bag_size: 25\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 0, bag_size: 49\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0098, weighted_loss: 0.0029, label: 1, bag_size: 38\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 0, bag_size: 109\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9667307692307693: correct 10054/10400\n",
      "class 1 clustering acc 0.8438461538461538: correct 4388/5200\n",
      "Epoch: 28, train_loss: 0.4632, train_clustering_loss:  0.3063, train_error: 0.1154\n",
      "class 0: acc 0.8810975609756098, correct 289/328\n",
      "class 1: acc 0.8881987577639752, correct 286/322\n",
      "\n",
      "Val Set, val_loss: 0.7166, val_error: 0.1398, auc: 0.9454\n",
      "class 0 clustering acc 0.9549731182795699: correct 1421/1488\n",
      "class 1 clustering acc 0.8373655913978495: correct 623/744\n",
      "class 0: acc 0.9787234042553191, correct 46/47\n",
      "class 1: acc 0.7391304347826086, correct 34/46\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.1240, weighted_loss: 0.0374, label: 0, bag_size: 21\n",
      "batch 39, loss: 0.1789, instance_loss: 0.4633, weighted_loss: 0.2642, label: 0, bag_size: 29\n",
      "batch 59, loss: 0.8419, instance_loss: 0.2916, weighted_loss: 0.6768, label: 0, bag_size: 35\n",
      "batch 79, loss: 2.6505, instance_loss: 0.9049, weighted_loss: 2.1268, label: 1, bag_size: 60\n",
      "batch 99, loss: 0.0003, instance_loss: 0.2994, weighted_loss: 0.0900, label: 0, bag_size: 44\n",
      "batch 119, loss: 0.0003, instance_loss: 0.3722, weighted_loss: 0.1119, label: 1, bag_size: 19\n",
      "batch 139, loss: 0.0055, instance_loss: 0.2521, weighted_loss: 0.0794, label: 1, bag_size: 74\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0105, weighted_loss: 0.0031, label: 1, bag_size: 120\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0574, weighted_loss: 0.0172, label: 1, bag_size: 77\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0070, weighted_loss: 0.0021, label: 1, bag_size: 85\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0022, weighted_loss: 0.0007, label: 1, bag_size: 62\n",
      "batch 239, loss: 0.0329, instance_loss: 0.6153, weighted_loss: 0.2076, label: 0, bag_size: 82\n",
      "batch 259, loss: 1.4632, instance_loss: 1.0338, weighted_loss: 1.3344, label: 1, bag_size: 59\n",
      "batch 279, loss: 0.0061, instance_loss: 1.0620, weighted_loss: 0.3229, label: 0, bag_size: 19\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0029, weighted_loss: 0.0009, label: 0, bag_size: 40\n",
      "batch 319, loss: 0.0059, instance_loss: 0.0143, weighted_loss: 0.0084, label: 0, bag_size: 88\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 107\n",
      "batch 359, loss: 0.6456, instance_loss: 0.4256, weighted_loss: 0.5796, label: 0, bag_size: 38\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0055, weighted_loss: 0.0017, label: 1, bag_size: 34\n",
      "batch 399, loss: 0.0023, instance_loss: 0.0983, weighted_loss: 0.0311, label: 0, bag_size: 18\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0035, weighted_loss: 0.0010, label: 0, bag_size: 31\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0106, weighted_loss: 0.0032, label: 0, bag_size: 43\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0306, weighted_loss: 0.0092, label: 0, bag_size: 91\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 32\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0092, weighted_loss: 0.0028, label: 0, bag_size: 97\n",
      "batch 519, loss: 0.0164, instance_loss: 0.2670, weighted_loss: 0.0916, label: 0, bag_size: 33\n",
      "batch 539, loss: 0.2526, instance_loss: 0.1957, weighted_loss: 0.2355, label: 1, bag_size: 28\n",
      "batch 559, loss: 0.3354, instance_loss: 1.4395, weighted_loss: 0.6666, label: 0, bag_size: 81\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0127, weighted_loss: 0.0038, label: 0, bag_size: 62\n",
      "batch 599, loss: 0.0705, instance_loss: 0.1219, weighted_loss: 0.0859, label: 0, bag_size: 67\n",
      "batch 619, loss: 0.0233, instance_loss: 0.0197, weighted_loss: 0.0223, label: 0, bag_size: 88\n",
      "batch 639, loss: 0.0006, instance_loss: 0.0270, weighted_loss: 0.0086, label: 1, bag_size: 51\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9572115384615385: correct 9955/10400\n",
      "class 1 clustering acc 0.816923076923077: correct 4248/5200\n",
      "Epoch: 29, train_loss: 0.4168, train_clustering_loss:  0.3322, train_error: 0.1108\n",
      "class 0: acc 0.8932926829268293, correct 293/328\n",
      "class 1: acc 0.8850931677018633, correct 285/322\n",
      "\n",
      "Val Set, val_loss: 0.3753, val_error: 0.0968, auc: 0.9551\n",
      "class 0 clustering acc 0.9603494623655914: correct 1429/1488\n",
      "class 1 clustering acc 0.8629032258064516: correct 642/744\n",
      "class 0: acc 0.9148936170212766, correct 43/47\n",
      "class 1: acc 0.8913043478260869, correct 41/46\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.3958, weighted_loss: 0.1188, label: 0, bag_size: 45\n",
      "batch 39, loss: 0.0066, instance_loss: 0.6122, weighted_loss: 0.1883, label: 1, bag_size: 56\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0017, weighted_loss: 0.0005, label: 0, bag_size: 29\n",
      "batch 79, loss: 0.0221, instance_loss: 0.0337, weighted_loss: 0.0256, label: 1, bag_size: 52\n",
      "batch 99, loss: 0.3423, instance_loss: 0.1243, weighted_loss: 0.2769, label: 1, bag_size: 29\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 63\n",
      "batch 139, loss: 0.0002, instance_loss: 0.0049, weighted_loss: 0.0016, label: 0, bag_size: 21\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0193, weighted_loss: 0.0059, label: 1, bag_size: 39\n",
      "batch 179, loss: 0.0019, instance_loss: 0.0804, weighted_loss: 0.0254, label: 0, bag_size: 29\n",
      "batch 199, loss: 0.0071, instance_loss: 0.0997, weighted_loss: 0.0349, label: 1, bag_size: 51\n",
      "batch 219, loss: 0.0023, instance_loss: 0.0023, weighted_loss: 0.0023, label: 0, bag_size: 88\n",
      "batch 239, loss: 1.7745, instance_loss: 0.0862, weighted_loss: 1.2680, label: 1, bag_size: 26\n",
      "batch 259, loss: 0.0730, instance_loss: 1.6247, weighted_loss: 0.5385, label: 1, bag_size: 70\n",
      "batch 279, loss: 3.1369, instance_loss: 2.3422, weighted_loss: 2.8985, label: 1, bag_size: 60\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 53\n",
      "batch 319, loss: 0.0004, instance_loss: 0.0119, weighted_loss: 0.0039, label: 0, bag_size: 107\n",
      "batch 339, loss: 0.0008, instance_loss: 0.0049, weighted_loss: 0.0020, label: 0, bag_size: 122\n",
      "batch 359, loss: 0.0164, instance_loss: 0.0014, weighted_loss: 0.0119, label: 0, bag_size: 29\n",
      "batch 379, loss: 2.0156, instance_loss: 0.5431, weighted_loss: 1.5739, label: 1, bag_size: 57\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 65\n",
      "batch 419, loss: 0.0004, instance_loss: 0.0132, weighted_loss: 0.0043, label: 1, bag_size: 41\n",
      "batch 439, loss: 0.2005, instance_loss: 0.0727, weighted_loss: 0.1622, label: 0, bag_size: 58\n",
      "batch 459, loss: 0.0631, instance_loss: 0.2073, weighted_loss: 0.1063, label: 0, bag_size: 41\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0309, weighted_loss: 0.0093, label: 0, bag_size: 66\n",
      "batch 499, loss: 0.0029, instance_loss: 0.0811, weighted_loss: 0.0264, label: 0, bag_size: 71\n",
      "batch 519, loss: 0.0028, instance_loss: 0.0136, weighted_loss: 0.0060, label: 1, bag_size: 17\n",
      "batch 539, loss: 0.0025, instance_loss: 0.0318, weighted_loss: 0.0113, label: 1, bag_size: 26\n",
      "batch 559, loss: 0.1882, instance_loss: 1.2869, weighted_loss: 0.5178, label: 0, bag_size: 73\n",
      "batch 579, loss: 0.3803, instance_loss: 0.3677, weighted_loss: 0.3766, label: 1, bag_size: 36\n",
      "batch 599, loss: 0.0002, instance_loss: 0.0045, weighted_loss: 0.0015, label: 1, bag_size: 127\n",
      "batch 619, loss: 0.0580, instance_loss: 0.0027, weighted_loss: 0.0414, label: 1, bag_size: 23\n",
      "batch 639, loss: 0.0045, instance_loss: 0.0030, weighted_loss: 0.0041, label: 1, bag_size: 75\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9718269230769231: correct 10107/10400\n",
      "class 1 clustering acc 0.8525: correct 4433/5200\n",
      "Epoch: 30, train_loss: 0.3588, train_clustering_loss:  0.2670, train_error: 0.1015\n",
      "class 0: acc 0.899390243902439, correct 295/328\n",
      "class 1: acc 0.8975155279503105, correct 289/322\n",
      "\n",
      "Val Set, val_loss: 0.6829, val_error: 0.1505, auc: 0.9514\n",
      "class 0 clustering acc 0.9529569892473119: correct 1418/1488\n",
      "class 1 clustering acc 0.8010752688172043: correct 596/744\n",
      "class 0: acc 1.0, correct 47/47\n",
      "class 1: acc 0.6956521739130435, correct 32/46\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.6122, instance_loss: 2.4612, weighted_loss: 1.8669, label: 1, bag_size: 54\n",
      "batch 39, loss: 2.8244, instance_loss: 0.9689, weighted_loss: 2.2677, label: 0, bag_size: 21\n",
      "batch 59, loss: 0.0790, instance_loss: 0.5749, weighted_loss: 0.2278, label: 1, bag_size: 76\n",
      "batch 79, loss: 0.0186, instance_loss: 0.1209, weighted_loss: 0.0493, label: 1, bag_size: 84\n",
      "batch 99, loss: 0.0193, instance_loss: 0.0278, weighted_loss: 0.0218, label: 0, bag_size: 66\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 109\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 24\n",
      "batch 159, loss: 7.8303, instance_loss: 4.6134, weighted_loss: 6.8652, label: 1, bag_size: 98\n",
      "batch 179, loss: 4.8499, instance_loss: 0.4832, weighted_loss: 3.5399, label: 1, bag_size: 84\n",
      "batch 199, loss: 0.0144, instance_loss: 0.1019, weighted_loss: 0.0407, label: 0, bag_size: 41\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0027, weighted_loss: 0.0008, label: 0, bag_size: 67\n",
      "batch 239, loss: 0.0159, instance_loss: 0.0039, weighted_loss: 0.0123, label: 1, bag_size: 123\n",
      "batch 259, loss: 0.0697, instance_loss: 0.0662, weighted_loss: 0.0686, label: 1, bag_size: 35\n",
      "batch 279, loss: 0.0162, instance_loss: 0.0106, weighted_loss: 0.0145, label: 1, bag_size: 65\n",
      "batch 299, loss: 0.0782, instance_loss: 0.0957, weighted_loss: 0.0835, label: 0, bag_size: 90\n",
      "batch 319, loss: 0.0056, instance_loss: 0.7364, weighted_loss: 0.2248, label: 0, bag_size: 89\n",
      "batch 339, loss: 0.0507, instance_loss: 0.0240, weighted_loss: 0.0427, label: 0, bag_size: 86\n",
      "batch 359, loss: 1.9566, instance_loss: 3.8830, weighted_loss: 2.5345, label: 0, bag_size: 31\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 65\n",
      "batch 399, loss: 2.6855, instance_loss: 3.1560, weighted_loss: 2.8267, label: 0, bag_size: 25\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 114\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0050, weighted_loss: 0.0015, label: 1, bag_size: 26\n",
      "batch 459, loss: 0.0024, instance_loss: 0.0562, weighted_loss: 0.0186, label: 0, bag_size: 24\n",
      "batch 479, loss: 0.0175, instance_loss: 0.1761, weighted_loss: 0.0651, label: 0, bag_size: 102\n",
      "batch 499, loss: 1.7407, instance_loss: 1.2927, weighted_loss: 1.6063, label: 1, bag_size: 35\n",
      "batch 519, loss: 0.0033, instance_loss: 0.0422, weighted_loss: 0.0150, label: 0, bag_size: 34\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0052, weighted_loss: 0.0016, label: 1, bag_size: 118\n",
      "batch 559, loss: 7.1185, instance_loss: 2.5516, weighted_loss: 5.7484, label: 0, bag_size: 27\n",
      "batch 579, loss: 0.8604, instance_loss: 0.8690, weighted_loss: 0.8630, label: 0, bag_size: 36\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0207, weighted_loss: 0.0062, label: 1, bag_size: 19\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0482, weighted_loss: 0.0145, label: 1, bag_size: 108\n",
      "batch 639, loss: 0.0331, instance_loss: 0.0167, weighted_loss: 0.0281, label: 1, bag_size: 73\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9670192307692308: correct 10057/10400\n",
      "class 1 clustering acc 0.8488461538461538: correct 4414/5200\n",
      "Epoch: 31, train_loss: 0.4490, train_clustering_loss:  0.3212, train_error: 0.1215\n",
      "class 0: acc 0.8772455089820359, correct 293/334\n",
      "class 1: acc 0.879746835443038, correct 278/316\n",
      "\n",
      "Val Set, val_loss: 0.3210, val_error: 0.0860, auc: 0.9718\n",
      "class 0 clustering acc 0.9811827956989247: correct 1460/1488\n",
      "class 1 clustering acc 0.8051075268817204: correct 599/744\n",
      "class 0: acc 1.0, correct 47/47\n",
      "class 1: acc 0.8260869565217391, correct 38/46\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1192, instance_loss: 0.9088, weighted_loss: 0.3561, label: 0, bag_size: 31\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0029, weighted_loss: 0.0009, label: 0, bag_size: 74\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0026, weighted_loss: 0.0008, label: 1, bag_size: 77\n",
      "batch 79, loss: 0.0006, instance_loss: 0.0037, weighted_loss: 0.0015, label: 1, bag_size: 41\n",
      "batch 99, loss: 0.0089, instance_loss: 0.0265, weighted_loss: 0.0141, label: 1, bag_size: 17\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0005, label: 1, bag_size: 78\n",
      "batch 139, loss: 5.5298, instance_loss: 0.4422, weighted_loss: 4.0035, label: 1, bag_size: 65\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0009, weighted_loss: 0.0003, label: 0, bag_size: 51\n",
      "batch 179, loss: 1.0236, instance_loss: 1.7515, weighted_loss: 1.2420, label: 0, bag_size: 81\n",
      "batch 199, loss: 0.0002, instance_loss: 0.0084, weighted_loss: 0.0027, label: 0, bag_size: 24\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0044, weighted_loss: 0.0013, label: 1, bag_size: 75\n",
      "batch 239, loss: 0.0116, instance_loss: 0.0087, weighted_loss: 0.0107, label: 1, bag_size: 65\n",
      "batch 259, loss: 5.6665, instance_loss: 5.0278, weighted_loss: 5.4749, label: 1, bag_size: 67\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0147, weighted_loss: 0.0045, label: 0, bag_size: 87\n",
      "batch 299, loss: 0.0020, instance_loss: 0.0641, weighted_loss: 0.0206, label: 1, bag_size: 81\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0082, weighted_loss: 0.0025, label: 1, bag_size: 40\n",
      "batch 339, loss: 0.0113, instance_loss: 0.0465, weighted_loss: 0.0219, label: 1, bag_size: 71\n",
      "batch 359, loss: 0.0141, instance_loss: 0.1179, weighted_loss: 0.0452, label: 0, bag_size: 35\n",
      "batch 379, loss: 0.0001, instance_loss: 0.0076, weighted_loss: 0.0023, label: 1, bag_size: 58\n",
      "batch 399, loss: 0.0009, instance_loss: 0.0532, weighted_loss: 0.0166, label: 1, bag_size: 17\n",
      "batch 419, loss: 0.0009, instance_loss: 0.0004, weighted_loss: 0.0008, label: 0, bag_size: 66\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 101\n",
      "batch 459, loss: 0.0358, instance_loss: 0.2374, weighted_loss: 0.0963, label: 1, bag_size: 54\n",
      "batch 479, loss: 0.1796, instance_loss: 0.0072, weighted_loss: 0.1279, label: 1, bag_size: 128\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0080, weighted_loss: 0.0024, label: 0, bag_size: 78\n",
      "batch 519, loss: 0.0021, instance_loss: 0.7389, weighted_loss: 0.2231, label: 1, bag_size: 24\n",
      "batch 539, loss: 0.0028, instance_loss: 0.3589, weighted_loss: 0.1096, label: 1, bag_size: 34\n",
      "batch 559, loss: 0.0059, instance_loss: 0.3611, weighted_loss: 0.1125, label: 1, bag_size: 33\n",
      "batch 579, loss: 0.0000, instance_loss: 0.3829, weighted_loss: 0.1149, label: 1, bag_size: 33\n",
      "batch 599, loss: 0.0192, instance_loss: 0.4721, weighted_loss: 0.1550, label: 1, bag_size: 95\n",
      "batch 619, loss: 0.3425, instance_loss: 1.2489, weighted_loss: 0.6144, label: 0, bag_size: 103\n",
      "batch 639, loss: 0.0707, instance_loss: 0.3353, weighted_loss: 0.1501, label: 0, bag_size: 28\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9597115384615384: correct 9981/10400\n",
      "class 1 clustering acc 0.7888461538461539: correct 4102/5200\n",
      "Epoch: 32, train_loss: 0.3794, train_clustering_loss:  0.3725, train_error: 0.1108\n",
      "class 0: acc 0.890625, correct 285/320\n",
      "class 1: acc 0.8878787878787879, correct 293/330\n",
      "\n",
      "Val Set, val_loss: 0.2811, val_error: 0.0968, auc: 0.9713\n",
      "class 0 clustering acc 0.7681451612903226: correct 1143/1488\n",
      "class 1 clustering acc 0.6774193548387096: correct 504/744\n",
      "class 0: acc 1.0, correct 47/47\n",
      "class 1: acc 0.8043478260869565, correct 37/46\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0015, instance_loss: 0.2967, weighted_loss: 0.0900, label: 1, bag_size: 89\n",
      "batch 39, loss: 0.3811, instance_loss: 0.4584, weighted_loss: 0.4043, label: 0, bag_size: 14\n",
      "batch 59, loss: 0.4156, instance_loss: 0.3296, weighted_loss: 0.3898, label: 0, bag_size: 28\n",
      "batch 79, loss: 3.7409, instance_loss: 3.8153, weighted_loss: 3.7632, label: 1, bag_size: 59\n",
      "batch 99, loss: 0.4444, instance_loss: 0.3770, weighted_loss: 0.4242, label: 0, bag_size: 75\n",
      "batch 119, loss: 0.1253, instance_loss: 0.4285, weighted_loss: 0.2163, label: 1, bag_size: 52\n",
      "batch 139, loss: 0.0236, instance_loss: 0.1283, weighted_loss: 0.0550, label: 0, bag_size: 37\n",
      "batch 159, loss: 0.0109, instance_loss: 0.0693, weighted_loss: 0.0284, label: 0, bag_size: 87\n",
      "batch 179, loss: 0.0016, instance_loss: 0.0756, weighted_loss: 0.0238, label: 0, bag_size: 50\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0020, weighted_loss: 0.0006, label: 0, bag_size: 72\n",
      "batch 219, loss: 0.2302, instance_loss: 0.7396, weighted_loss: 0.3830, label: 1, bag_size: 30\n",
      "batch 239, loss: 0.0001, instance_loss: 0.3725, weighted_loss: 0.1118, label: 1, bag_size: 78\n",
      "batch 259, loss: 0.0003, instance_loss: 0.2934, weighted_loss: 0.0882, label: 1, bag_size: 75\n",
      "batch 279, loss: 0.7626, instance_loss: 0.3761, weighted_loss: 0.6466, label: 1, bag_size: 93\n",
      "batch 299, loss: 0.0128, instance_loss: 0.4724, weighted_loss: 0.1507, label: 0, bag_size: 94\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0163, weighted_loss: 0.0049, label: 0, bag_size: 35\n",
      "batch 339, loss: 0.0004, instance_loss: 0.0128, weighted_loss: 0.0041, label: 0, bag_size: 102\n",
      "batch 359, loss: 2.1021, instance_loss: 0.2894, weighted_loss: 1.5583, label: 0, bag_size: 58\n",
      "batch 379, loss: 0.0000, instance_loss: 0.2673, weighted_loss: 0.0802, label: 0, bag_size: 27\n",
      "batch 399, loss: 0.0000, instance_loss: 1.1850, weighted_loss: 0.3555, label: 0, bag_size: 28\n",
      "batch 419, loss: 0.0000, instance_loss: 0.5903, weighted_loss: 0.1771, label: 0, bag_size: 53\n",
      "batch 439, loss: 0.0015, instance_loss: 0.3544, weighted_loss: 0.1073, label: 0, bag_size: 32\n",
      "batch 459, loss: 0.0000, instance_loss: 0.4608, weighted_loss: 0.1382, label: 1, bag_size: 22\n",
      "batch 479, loss: 1.5923, instance_loss: 0.5088, weighted_loss: 1.2673, label: 1, bag_size: 80\n",
      "batch 499, loss: 0.0001, instance_loss: 0.4619, weighted_loss: 0.1387, label: 1, bag_size: 23\n",
      "batch 519, loss: 0.0000, instance_loss: 0.4557, weighted_loss: 0.1367, label: 1, bag_size: 37\n",
      "batch 539, loss: 0.0000, instance_loss: 0.1766, weighted_loss: 0.0530, label: 1, bag_size: 90\n",
      "batch 559, loss: 0.0000, instance_loss: 0.1159, weighted_loss: 0.0348, label: 0, bag_size: 67\n",
      "batch 579, loss: 0.0004, instance_loss: 0.1683, weighted_loss: 0.0508, label: 0, bag_size: 62\n",
      "batch 599, loss: 0.0011, instance_loss: 0.3323, weighted_loss: 0.1004, label: 1, bag_size: 75\n",
      "batch 619, loss: 0.0002, instance_loss: 0.3152, weighted_loss: 0.0947, label: 1, bag_size: 35\n",
      "batch 639, loss: 0.0054, instance_loss: 0.3893, weighted_loss: 0.1206, label: 1, bag_size: 64\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.94125: correct 9789/10400\n",
      "class 1 clustering acc 0.6834615384615385: correct 3554/5200\n",
      "Epoch: 33, train_loss: 0.7007, train_clustering_loss:  0.5204, train_error: 0.1385\n",
      "class 0: acc 0.8715596330275229, correct 285/327\n",
      "class 1: acc 0.8513931888544891, correct 275/323\n",
      "\n",
      "Val Set, val_loss: 0.3841, val_error: 0.0753, auc: 0.9741\n",
      "class 0 clustering acc 0.7842741935483871: correct 1167/1488\n",
      "class 1 clustering acc 0.6532258064516129: correct 486/744\n",
      "class 0: acc 1.0, correct 47/47\n",
      "class 1: acc 0.8478260869565217, correct 39/46\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.2254, weighted_loss: 0.0676, label: 1, bag_size: 56\n",
      "batch 39, loss: 1.5701, instance_loss: 0.9523, weighted_loss: 1.3847, label: 0, bag_size: 35\n",
      "batch 59, loss: 0.0104, instance_loss: 0.4452, weighted_loss: 0.1408, label: 1, bag_size: 48\n",
      "batch 79, loss: 0.7331, instance_loss: 0.4517, weighted_loss: 0.6487, label: 0, bag_size: 92\n",
      "batch 99, loss: 0.0000, instance_loss: 0.3358, weighted_loss: 0.1007, label: 1, bag_size: 31\n",
      "batch 119, loss: 3.6023, instance_loss: 1.5418, weighted_loss: 2.9841, label: 1, bag_size: 33\n",
      "batch 139, loss: 1.9005, instance_loss: 1.7114, weighted_loss: 1.8438, label: 1, bag_size: 79\n",
      "batch 159, loss: 0.0156, instance_loss: 0.3816, weighted_loss: 0.1254, label: 1, bag_size: 48\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0814, weighted_loss: 0.0245, label: 1, bag_size: 80\n",
      "batch 199, loss: 4.8728, instance_loss: 3.2963, weighted_loss: 4.3998, label: 0, bag_size: 78\n",
      "batch 219, loss: 0.0001, instance_loss: 0.1030, weighted_loss: 0.0309, label: 0, bag_size: 78\n",
      "batch 239, loss: 1.7737, instance_loss: 0.7819, weighted_loss: 1.4761, label: 0, bag_size: 86\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0396, weighted_loss: 0.0119, label: 0, bag_size: 49\n",
      "batch 279, loss: 0.1093, instance_loss: 0.6057, weighted_loss: 0.2582, label: 0, bag_size: 107\n",
      "batch 299, loss: 1.7780, instance_loss: 2.7369, weighted_loss: 2.0657, label: 1, bag_size: 32\n",
      "batch 319, loss: 0.1367, instance_loss: 0.3053, weighted_loss: 0.1873, label: 0, bag_size: 50\n",
      "batch 339, loss: 0.0000, instance_loss: 0.1002, weighted_loss: 0.0301, label: 0, bag_size: 94\n",
      "batch 359, loss: 0.0657, instance_loss: 0.2528, weighted_loss: 0.1218, label: 1, bag_size: 62\n",
      "batch 379, loss: 0.0000, instance_loss: 0.4903, weighted_loss: 0.1471, label: 1, bag_size: 48\n",
      "batch 399, loss: 9.0022, instance_loss: 5.1261, weighted_loss: 7.8393, label: 1, bag_size: 98\n",
      "batch 419, loss: 0.0002, instance_loss: 0.1459, weighted_loss: 0.0439, label: 0, bag_size: 63\n",
      "batch 439, loss: 2.0073, instance_loss: 0.3848, weighted_loss: 1.5205, label: 0, bag_size: 78\n",
      "batch 459, loss: 0.0000, instance_loss: 0.3290, weighted_loss: 0.0987, label: 1, bag_size: 31\n",
      "batch 479, loss: 0.1489, instance_loss: 0.6819, weighted_loss: 0.3088, label: 1, bag_size: 62\n",
      "batch 499, loss: 0.0000, instance_loss: 0.1937, weighted_loss: 0.0581, label: 1, bag_size: 19\n",
      "batch 519, loss: 1.2370, instance_loss: 1.4196, weighted_loss: 1.2917, label: 0, bag_size: 72\n",
      "batch 539, loss: 0.6354, instance_loss: 0.4504, weighted_loss: 0.5799, label: 0, bag_size: 72\n",
      "batch 559, loss: 0.0000, instance_loss: 0.3253, weighted_loss: 0.0976, label: 0, bag_size: 21\n",
      "batch 579, loss: 0.0241, instance_loss: 0.1961, weighted_loss: 0.0757, label: 0, bag_size: 41\n",
      "batch 599, loss: 2.5473, instance_loss: 0.7239, weighted_loss: 2.0003, label: 1, bag_size: 28\n",
      "batch 619, loss: 6.2644, instance_loss: 1.3974, weighted_loss: 4.8043, label: 1, bag_size: 49\n",
      "batch 639, loss: 0.0066, instance_loss: 0.6666, weighted_loss: 0.2046, label: 1, bag_size: 77\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9400961538461539: correct 9777/10400\n",
      "class 1 clustering acc 0.7609615384615385: correct 3957/5200\n",
      "Epoch: 34, train_loss: 0.3476, train_clustering_loss:  0.4285, train_error: 0.0923\n",
      "class 0: acc 0.9096385542168675, correct 302/332\n",
      "class 1: acc 0.9056603773584906, correct 288/318\n",
      "\n",
      "Val Set, val_loss: 0.4531, val_error: 0.1505, auc: 0.9727\n",
      "class 0 clustering acc 0.7096774193548387: correct 1056/1488\n",
      "class 1 clustering acc 0.6612903225806451: correct 492/744\n",
      "class 0: acc 0.7446808510638298, correct 35/47\n",
      "class 1: acc 0.9565217391304348, correct 44/46\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0772, weighted_loss: 0.0232, label: 1, bag_size: 34\n",
      "batch 39, loss: 0.0000, instance_loss: 0.2929, weighted_loss: 0.0879, label: 1, bag_size: 75\n",
      "batch 59, loss: 0.0012, instance_loss: 0.2879, weighted_loss: 0.0872, label: 0, bag_size: 58\n",
      "batch 79, loss: 3.3903, instance_loss: 2.4589, weighted_loss: 3.1109, label: 1, bag_size: 48\n",
      "batch 99, loss: 1.4174, instance_loss: 0.8368, weighted_loss: 1.2432, label: 0, bag_size: 33\n",
      "batch 119, loss: 0.0000, instance_loss: 0.1263, weighted_loss: 0.0379, label: 0, bag_size: 31\n",
      "batch 139, loss: 0.0001, instance_loss: 0.2629, weighted_loss: 0.0789, label: 0, bag_size: 90\n",
      "batch 159, loss: 0.0002, instance_loss: 0.1809, weighted_loss: 0.0544, label: 0, bag_size: 29\n",
      "batch 179, loss: 0.0158, instance_loss: 0.3627, weighted_loss: 0.1199, label: 0, bag_size: 83\n",
      "batch 199, loss: 0.0000, instance_loss: 0.4575, weighted_loss: 0.1373, label: 1, bag_size: 45\n",
      "batch 219, loss: 0.0000, instance_loss: 0.3143, weighted_loss: 0.0943, label: 1, bag_size: 16\n",
      "batch 239, loss: 0.0184, instance_loss: 0.5652, weighted_loss: 0.1824, label: 1, bag_size: 31\n",
      "batch 259, loss: 0.0028, instance_loss: 0.3132, weighted_loss: 0.0959, label: 1, bag_size: 20\n",
      "batch 279, loss: 0.0000, instance_loss: 0.1475, weighted_loss: 0.0443, label: 1, bag_size: 56\n",
      "batch 299, loss: 0.0019, instance_loss: 0.3455, weighted_loss: 0.1050, label: 1, bag_size: 45\n",
      "batch 319, loss: 2.6599, instance_loss: 1.3532, weighted_loss: 2.2679, label: 1, bag_size: 51\n",
      "batch 339, loss: 0.0036, instance_loss: 0.1903, weighted_loss: 0.0596, label: 0, bag_size: 44\n",
      "batch 359, loss: 0.0048, instance_loss: 0.2806, weighted_loss: 0.0875, label: 1, bag_size: 51\n",
      "batch 379, loss: 0.0000, instance_loss: 0.1152, weighted_loss: 0.0346, label: 1, bag_size: 72\n",
      "batch 399, loss: 0.0000, instance_loss: 0.1275, weighted_loss: 0.0382, label: 0, bag_size: 74\n",
      "batch 419, loss: 0.0000, instance_loss: 0.3673, weighted_loss: 0.1102, label: 0, bag_size: 27\n",
      "batch 439, loss: 0.0003, instance_loss: 0.2549, weighted_loss: 0.0767, label: 1, bag_size: 61\n",
      "batch 459, loss: 0.3884, instance_loss: 1.1888, weighted_loss: 0.6285, label: 1, bag_size: 78\n",
      "batch 479, loss: 0.1965, instance_loss: 0.2900, weighted_loss: 0.2245, label: 1, bag_size: 38\n",
      "batch 499, loss: 0.0952, instance_loss: 1.1813, weighted_loss: 0.4210, label: 0, bag_size: 48\n",
      "batch 519, loss: 0.0317, instance_loss: 0.3194, weighted_loss: 0.1180, label: 1, bag_size: 51\n",
      "batch 539, loss: 0.0057, instance_loss: 0.0428, weighted_loss: 0.0168, label: 0, bag_size: 65\n",
      "batch 559, loss: 5.3416, instance_loss: 2.0500, weighted_loss: 4.3541, label: 1, bag_size: 31\n",
      "batch 579, loss: 0.0044, instance_loss: 0.3518, weighted_loss: 0.1086, label: 1, bag_size: 89\n",
      "batch 599, loss: 0.0041, instance_loss: 0.0478, weighted_loss: 0.0172, label: 0, bag_size: 25\n",
      "batch 619, loss: 0.0011, instance_loss: 0.0698, weighted_loss: 0.0217, label: 0, bag_size: 58\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0086, weighted_loss: 0.0026, label: 0, bag_size: 55\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9290384615384616: correct 9662/10400\n",
      "class 1 clustering acc 0.6957692307692308: correct 3618/5200\n",
      "Epoch: 35, train_loss: 0.7522, train_clustering_loss:  0.5299, train_error: 0.1492\n",
      "class 0: acc 0.8425925925925926, correct 273/324\n",
      "class 1: acc 0.8588957055214724, correct 280/326\n",
      "\n",
      "Val Set, val_loss: 0.3354, val_error: 0.1183, auc: 0.9653\n",
      "class 0 clustering acc 0.8521505376344086: correct 1268/1488\n",
      "class 1 clustering acc 0.3481182795698925: correct 259/744\n",
      "class 0: acc 0.851063829787234, correct 40/47\n",
      "class 1: acc 0.9130434782608695, correct 42/46\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5471, instance_loss: 1.8885, weighted_loss: 0.9495, label: 0, bag_size: 58\n",
      "batch 39, loss: 0.0112, instance_loss: 0.0341, weighted_loss: 0.0180, label: 0, bag_size: 78\n",
      "batch 59, loss: 0.1907, instance_loss: 0.6222, weighted_loss: 0.3202, label: 1, bag_size: 39\n",
      "batch 79, loss: 0.7628, instance_loss: 0.3950, weighted_loss: 0.6525, label: 1, bag_size: 28\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0049, weighted_loss: 0.0015, label: 0, bag_size: 21\n",
      "batch 119, loss: 0.0005, instance_loss: 0.4914, weighted_loss: 0.1477, label: 1, bag_size: 76\n",
      "batch 139, loss: 0.0069, instance_loss: 0.2361, weighted_loss: 0.0757, label: 1, bag_size: 81\n",
      "batch 159, loss: 2.8928, instance_loss: 0.6822, weighted_loss: 2.2296, label: 0, bag_size: 27\n",
      "batch 179, loss: 0.0000, instance_loss: 0.4232, weighted_loss: 0.1270, label: 1, bag_size: 69\n",
      "batch 199, loss: 6.2239, instance_loss: 2.3693, weighted_loss: 5.0675, label: 0, bag_size: 31\n",
      "batch 219, loss: 0.0003, instance_loss: 0.2505, weighted_loss: 0.0754, label: 1, bag_size: 76\n",
      "batch 239, loss: 0.0001, instance_loss: 0.0571, weighted_loss: 0.0172, label: 0, bag_size: 59\n",
      "batch 259, loss: 0.0913, instance_loss: 0.3308, weighted_loss: 0.1632, label: 1, bag_size: 30\n",
      "batch 279, loss: 0.0122, instance_loss: 0.1407, weighted_loss: 0.0507, label: 0, bag_size: 73\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0984, weighted_loss: 0.0295, label: 0, bag_size: 61\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0759, weighted_loss: 0.0228, label: 1, bag_size: 74\n",
      "batch 339, loss: 0.0000, instance_loss: 0.2408, weighted_loss: 0.0722, label: 0, bag_size: 51\n",
      "batch 359, loss: 2.2010, instance_loss: 0.9593, weighted_loss: 1.8285, label: 0, bag_size: 43\n",
      "batch 379, loss: 0.0181, instance_loss: 0.0621, weighted_loss: 0.0313, label: 0, bag_size: 27\n",
      "batch 399, loss: 0.1901, instance_loss: 0.7572, weighted_loss: 0.3602, label: 0, bag_size: 77\n",
      "batch 419, loss: 0.0129, instance_loss: 0.1430, weighted_loss: 0.0519, label: 0, bag_size: 71\n",
      "batch 439, loss: 0.0007, instance_loss: 0.0260, weighted_loss: 0.0083, label: 0, bag_size: 83\n",
      "batch 459, loss: 0.0187, instance_loss: 0.3948, weighted_loss: 0.1315, label: 1, bag_size: 83\n",
      "batch 479, loss: 0.3610, instance_loss: 0.0919, weighted_loss: 0.2803, label: 0, bag_size: 57\n",
      "batch 499, loss: 0.7975, instance_loss: 1.6142, weighted_loss: 1.0425, label: 0, bag_size: 50\n",
      "batch 519, loss: 0.0001, instance_loss: 0.1978, weighted_loss: 0.0594, label: 1, bag_size: 76\n",
      "batch 539, loss: 0.0193, instance_loss: 0.3484, weighted_loss: 0.1180, label: 1, bag_size: 84\n",
      "batch 559, loss: 0.0081, instance_loss: 0.2853, weighted_loss: 0.0912, label: 1, bag_size: 30\n",
      "batch 579, loss: 0.3317, instance_loss: 0.9627, weighted_loss: 0.5210, label: 0, bag_size: 78\n",
      "batch 599, loss: 0.0001, instance_loss: 0.2154, weighted_loss: 0.0647, label: 1, bag_size: 107\n",
      "batch 619, loss: 0.0329, instance_loss: 0.2832, weighted_loss: 0.1080, label: 1, bag_size: 36\n",
      "batch 639, loss: 0.0716, instance_loss: 0.1507, weighted_loss: 0.0953, label: 1, bag_size: 57\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9470192307692308: correct 9849/10400\n",
      "class 1 clustering acc 0.7471153846153846: correct 3885/5200\n",
      "Epoch: 36, train_loss: 0.3537, train_clustering_loss:  0.4185, train_error: 0.1277\n",
      "class 0: acc 0.8730650154798761, correct 282/323\n",
      "class 1: acc 0.8715596330275229, correct 285/327\n",
      "\n",
      "Val Set, val_loss: 0.3495, val_error: 0.1183, auc: 0.9769\n",
      "class 0 clustering acc 0.8185483870967742: correct 1218/1488\n",
      "class 1 clustering acc 0.6236559139784946: correct 464/744\n",
      "class 0: acc 0.8085106382978723, correct 38/47\n",
      "class 1: acc 0.9565217391304348, correct 44/46\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1805, instance_loss: 0.8644, weighted_loss: 0.3857, label: 0, bag_size: 56\n",
      "batch 39, loss: 1.2545, instance_loss: 1.4427, weighted_loss: 1.3109, label: 1, bag_size: 62\n",
      "batch 59, loss: 0.5805, instance_loss: 0.8602, weighted_loss: 0.6644, label: 1, bag_size: 62\n",
      "batch 79, loss: 3.9006, instance_loss: 3.9243, weighted_loss: 3.9077, label: 0, bag_size: 51\n",
      "batch 99, loss: 0.0004, instance_loss: 0.1420, weighted_loss: 0.0429, label: 0, bag_size: 26\n",
      "batch 119, loss: 0.0279, instance_loss: 0.1531, weighted_loss: 0.0655, label: 0, bag_size: 79\n",
      "batch 139, loss: 0.0029, instance_loss: 0.1661, weighted_loss: 0.0519, label: 0, bag_size: 76\n",
      "batch 159, loss: 0.7943, instance_loss: 0.8766, weighted_loss: 0.8190, label: 1, bag_size: 82\n",
      "batch 179, loss: 4.9083, instance_loss: 4.4316, weighted_loss: 4.7653, label: 1, bag_size: 51\n",
      "batch 199, loss: 0.0364, instance_loss: 0.1829, weighted_loss: 0.0803, label: 0, bag_size: 34\n",
      "batch 219, loss: 1.0858, instance_loss: 2.5005, weighted_loss: 1.5102, label: 1, bag_size: 73\n",
      "batch 239, loss: 0.0000, instance_loss: 0.1500, weighted_loss: 0.0450, label: 0, bag_size: 24\n",
      "batch 259, loss: 0.0001, instance_loss: 0.1685, weighted_loss: 0.0506, label: 0, bag_size: 110\n",
      "batch 279, loss: 0.0000, instance_loss: 0.1384, weighted_loss: 0.0415, label: 1, bag_size: 72\n",
      "batch 299, loss: 0.0411, instance_loss: 0.4633, weighted_loss: 0.1677, label: 1, bag_size: 51\n",
      "batch 319, loss: 0.0002, instance_loss: 0.1088, weighted_loss: 0.0328, label: 1, bag_size: 41\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0241, weighted_loss: 0.0073, label: 1, bag_size: 89\n",
      "batch 359, loss: 0.0000, instance_loss: 0.2163, weighted_loss: 0.0649, label: 0, bag_size: 122\n",
      "batch 379, loss: 0.0000, instance_loss: 0.1360, weighted_loss: 0.0408, label: 1, bag_size: 79\n",
      "batch 399, loss: 0.0000, instance_loss: 0.1472, weighted_loss: 0.0442, label: 1, bag_size: 56\n",
      "batch 419, loss: 0.0000, instance_loss: 0.7722, weighted_loss: 0.2316, label: 1, bag_size: 80\n",
      "batch 439, loss: 0.0010, instance_loss: 0.1706, weighted_loss: 0.0519, label: 0, bag_size: 122\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0229, weighted_loss: 0.0069, label: 0, bag_size: 114\n",
      "batch 479, loss: 0.0002, instance_loss: 0.0911, weighted_loss: 0.0275, label: 0, bag_size: 65\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0107, weighted_loss: 0.0032, label: 0, bag_size: 37\n",
      "batch 519, loss: 0.0005, instance_loss: 0.1748, weighted_loss: 0.0527, label: 1, bag_size: 87\n",
      "batch 539, loss: 4.3804, instance_loss: 2.0355, weighted_loss: 3.6769, label: 1, bag_size: 51\n",
      "batch 559, loss: 0.2521, instance_loss: 0.0556, weighted_loss: 0.1931, label: 0, bag_size: 89\n",
      "batch 579, loss: 0.3615, instance_loss: 0.6234, weighted_loss: 0.4401, label: 0, bag_size: 132\n",
      "batch 599, loss: 0.0062, instance_loss: 0.0328, weighted_loss: 0.0142, label: 0, bag_size: 78\n",
      "batch 619, loss: 0.0000, instance_loss: 0.1195, weighted_loss: 0.0359, label: 1, bag_size: 48\n",
      "batch 639, loss: 0.0384, instance_loss: 0.0946, weighted_loss: 0.0553, label: 0, bag_size: 25\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9465384615384616: correct 9844/10400\n",
      "class 1 clustering acc 0.7536538461538461: correct 3919/5200\n",
      "Epoch: 37, train_loss: 0.5056, train_clustering_loss:  0.4271, train_error: 0.1400\n",
      "class 0: acc 0.8584615384615385, correct 279/325\n",
      "class 1: acc 0.8615384615384616, correct 280/325\n",
      "\n",
      "Val Set, val_loss: 0.1794, val_error: 0.0645, auc: 0.9857\n",
      "class 0 clustering acc 0.6297043010752689: correct 937/1488\n",
      "class 1 clustering acc 0.8373655913978495: correct 623/744\n",
      "class 0: acc 0.9787234042553191, correct 46/47\n",
      "class 1: acc 0.8913043478260869, correct 41/46\n",
      "Validation loss decreased (0.249291 --> 0.179373).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0003, instance_loss: 0.2598, weighted_loss: 0.0782, label: 1, bag_size: 89\n",
      "batch 39, loss: 0.0000, instance_loss: 0.3006, weighted_loss: 0.0902, label: 1, bag_size: 74\n",
      "batch 59, loss: 0.0000, instance_loss: 0.2210, weighted_loss: 0.0663, label: 0, bag_size: 116\n",
      "batch 79, loss: 0.0000, instance_loss: 0.2180, weighted_loss: 0.0654, label: 1, bag_size: 53\n",
      "batch 99, loss: 0.0000, instance_loss: 0.3017, weighted_loss: 0.0905, label: 1, bag_size: 122\n",
      "batch 119, loss: 0.0000, instance_loss: 0.1399, weighted_loss: 0.0420, label: 1, bag_size: 40\n",
      "batch 139, loss: 0.0000, instance_loss: 0.1346, weighted_loss: 0.0404, label: 0, bag_size: 43\n",
      "batch 159, loss: 0.0000, instance_loss: 0.2596, weighted_loss: 0.0779, label: 1, bag_size: 63\n",
      "batch 179, loss: 0.0011, instance_loss: 0.0731, weighted_loss: 0.0227, label: 0, bag_size: 33\n",
      "batch 199, loss: 0.0000, instance_loss: 0.1301, weighted_loss: 0.0390, label: 1, bag_size: 79\n",
      "batch 219, loss: 0.3708, instance_loss: 0.5434, weighted_loss: 0.4226, label: 0, bag_size: 56\n",
      "batch 239, loss: 0.7588, instance_loss: 1.6833, weighted_loss: 1.0362, label: 1, bag_size: 42\n",
      "batch 259, loss: 0.0053, instance_loss: 0.1719, weighted_loss: 0.0553, label: 0, bag_size: 34\n",
      "batch 279, loss: 0.0379, instance_loss: 0.0450, weighted_loss: 0.0400, label: 0, bag_size: 27\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0145, weighted_loss: 0.0044, label: 0, bag_size: 63\n",
      "batch 319, loss: 0.0023, instance_loss: 0.1850, weighted_loss: 0.0571, label: 1, bag_size: 21\n",
      "batch 339, loss: 0.0000, instance_loss: 0.1410, weighted_loss: 0.0423, label: 1, bag_size: 122\n",
      "batch 359, loss: 0.0000, instance_loss: 0.1137, weighted_loss: 0.0341, label: 1, bag_size: 38\n",
      "batch 379, loss: 2.9108, instance_loss: 3.2883, weighted_loss: 3.0241, label: 0, bag_size: 75\n",
      "batch 399, loss: 0.0000, instance_loss: 0.2150, weighted_loss: 0.0645, label: 1, bag_size: 25\n",
      "batch 419, loss: 0.3300, instance_loss: 2.2003, weighted_loss: 0.8911, label: 0, bag_size: 58\n",
      "batch 439, loss: 0.0000, instance_loss: 0.1861, weighted_loss: 0.0558, label: 1, bag_size: 56\n",
      "batch 459, loss: 0.0008, instance_loss: 0.0614, weighted_loss: 0.0190, label: 0, bag_size: 61\n",
      "batch 479, loss: 0.0141, instance_loss: 1.6906, weighted_loss: 0.5170, label: 0, bag_size: 32\n",
      "batch 499, loss: 0.0000, instance_loss: 0.1123, weighted_loss: 0.0337, label: 1, bag_size: 21\n",
      "batch 519, loss: 0.0025, instance_loss: 0.0979, weighted_loss: 0.0312, label: 1, bag_size: 29\n",
      "batch 539, loss: 0.0000, instance_loss: 0.1400, weighted_loss: 0.0420, label: 1, bag_size: 25\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0633, weighted_loss: 0.0190, label: 0, bag_size: 102\n",
      "batch 579, loss: 0.0012, instance_loss: 0.0812, weighted_loss: 0.0252, label: 0, bag_size: 67\n",
      "batch 599, loss: 0.0000, instance_loss: 0.1064, weighted_loss: 0.0319, label: 0, bag_size: 55\n",
      "batch 619, loss: 0.0000, instance_loss: 0.1396, weighted_loss: 0.0419, label: 1, bag_size: 68\n",
      "batch 639, loss: 0.7680, instance_loss: 1.4856, weighted_loss: 0.9833, label: 0, bag_size: 17\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9513461538461538: correct 9894/10400\n",
      "class 1 clustering acc 0.7842307692307692: correct 4078/5200\n",
      "Epoch: 38, train_loss: 0.3905, train_clustering_loss:  0.3801, train_error: 0.1092\n",
      "class 0: acc 0.881578947368421, correct 268/304\n",
      "class 1: acc 0.8988439306358381, correct 311/346\n",
      "\n",
      "Val Set, val_loss: 0.1528, val_error: 0.0538, auc: 0.9898\n",
      "class 0 clustering acc 0.8595430107526881: correct 1279/1488\n",
      "class 1 clustering acc 0.7271505376344086: correct 541/744\n",
      "class 0: acc 0.9574468085106383, correct 45/47\n",
      "class 1: acc 0.9347826086956522, correct 43/46\n",
      "Validation loss decreased (0.179373 --> 0.152806).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0949, weighted_loss: 0.0285, label: 1, bag_size: 86\n",
      "batch 39, loss: 0.0000, instance_loss: 0.1396, weighted_loss: 0.0419, label: 1, bag_size: 97\n",
      "batch 59, loss: 5.9111, instance_loss: 1.9311, weighted_loss: 4.7171, label: 0, bag_size: 41\n",
      "batch 79, loss: 0.5811, instance_loss: 0.3040, weighted_loss: 0.4980, label: 0, bag_size: 83\n",
      "batch 99, loss: 0.0002, instance_loss: 0.1671, weighted_loss: 0.0502, label: 1, bag_size: 21\n",
      "batch 119, loss: 0.0001, instance_loss: 0.1673, weighted_loss: 0.0503, label: 1, bag_size: 78\n",
      "batch 139, loss: 0.0601, instance_loss: 0.4983, weighted_loss: 0.1916, label: 0, bag_size: 95\n",
      "batch 159, loss: 0.8231, instance_loss: 0.4080, weighted_loss: 0.6986, label: 0, bag_size: 85\n",
      "batch 179, loss: 0.0000, instance_loss: 0.2859, weighted_loss: 0.0858, label: 0, bag_size: 91\n",
      "batch 199, loss: 0.0000, instance_loss: 0.2073, weighted_loss: 0.0622, label: 0, bag_size: 75\n",
      "batch 219, loss: 0.0000, instance_loss: 0.1021, weighted_loss: 0.0306, label: 1, bag_size: 52\n",
      "batch 239, loss: 0.0000, instance_loss: 0.1511, weighted_loss: 0.0453, label: 1, bag_size: 65\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0430, weighted_loss: 0.0129, label: 1, bag_size: 127\n",
      "batch 279, loss: 3.9802, instance_loss: 0.6891, weighted_loss: 2.9928, label: 1, bag_size: 71\n",
      "batch 299, loss: 0.0000, instance_loss: 0.1018, weighted_loss: 0.0305, label: 0, bag_size: 108\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0006, label: 0, bag_size: 132\n",
      "batch 339, loss: 0.0994, instance_loss: 0.2128, weighted_loss: 0.1334, label: 1, bag_size: 16\n",
      "batch 359, loss: 0.0010, instance_loss: 0.5066, weighted_loss: 0.1527, label: 0, bag_size: 29\n",
      "batch 379, loss: 0.0001, instance_loss: 0.1028, weighted_loss: 0.0309, label: 1, bag_size: 68\n",
      "batch 399, loss: 0.1541, instance_loss: 0.0580, weighted_loss: 0.1252, label: 1, bag_size: 48\n",
      "batch 419, loss: 0.0000, instance_loss: 0.2458, weighted_loss: 0.0737, label: 1, bag_size: 60\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0121, weighted_loss: 0.0036, label: 1, bag_size: 84\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0019, weighted_loss: 0.0006, label: 0, bag_size: 13\n",
      "batch 479, loss: 0.0002, instance_loss: 0.0027, weighted_loss: 0.0010, label: 0, bag_size: 31\n",
      "batch 499, loss: 0.0001, instance_loss: 0.2823, weighted_loss: 0.0847, label: 0, bag_size: 88\n",
      "batch 519, loss: 0.0000, instance_loss: 0.9024, weighted_loss: 0.2707, label: 0, bag_size: 37\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0291, weighted_loss: 0.0087, label: 0, bag_size: 57\n",
      "batch 559, loss: 0.0250, instance_loss: 0.7306, weighted_loss: 0.2367, label: 0, bag_size: 27\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0102, weighted_loss: 0.0031, label: 0, bag_size: 81\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0810, weighted_loss: 0.0243, label: 0, bag_size: 81\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0965, weighted_loss: 0.0289, label: 1, bag_size: 67\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0100, weighted_loss: 0.0030, label: 0, bag_size: 29\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9491346153846154: correct 9871/10400\n",
      "class 1 clustering acc 0.785: correct 4082/5200\n",
      "Epoch: 39, train_loss: 0.6992, train_clustering_loss:  0.4398, train_error: 0.1246\n",
      "class 0: acc 0.8720238095238095, correct 293/336\n",
      "class 1: acc 0.8789808917197452, correct 276/314\n",
      "\n",
      "Val Set, val_loss: 0.2588, val_error: 0.0860, auc: 0.9759\n",
      "class 0 clustering acc 0.9744623655913979: correct 1450/1488\n",
      "class 1 clustering acc 0.8467741935483871: correct 630/744\n",
      "class 0: acc 0.9574468085106383, correct 45/47\n",
      "class 1: acc 0.8695652173913043, correct 40/46\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0227, weighted_loss: 0.0069, label: 0, bag_size: 40\n",
      "batch 39, loss: 0.0000, instance_loss: 0.2081, weighted_loss: 0.0624, label: 1, bag_size: 32\n",
      "batch 59, loss: 0.0010, instance_loss: 0.0266, weighted_loss: 0.0087, label: 0, bag_size: 79\n",
      "batch 79, loss: 0.0094, instance_loss: 0.2672, weighted_loss: 0.0867, label: 1, bag_size: 71\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0717, weighted_loss: 0.0215, label: 1, bag_size: 31\n",
      "batch 119, loss: 3.2116, instance_loss: 1.1419, weighted_loss: 2.5907, label: 1, bag_size: 79\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0070, weighted_loss: 0.0021, label: 0, bag_size: 57\n",
      "batch 159, loss: 0.0001, instance_loss: 0.2409, weighted_loss: 0.0723, label: 1, bag_size: 84\n",
      "batch 179, loss: 0.0002, instance_loss: 0.0702, weighted_loss: 0.0212, label: 0, bag_size: 96\n",
      "batch 199, loss: 0.5497, instance_loss: 0.8659, weighted_loss: 0.6445, label: 1, bag_size: 60\n",
      "batch 219, loss: 0.1019, instance_loss: 0.7840, weighted_loss: 0.3065, label: 0, bag_size: 43\n",
      "batch 239, loss: 0.0000, instance_loss: 0.5833, weighted_loss: 0.1750, label: 1, bag_size: 14\n",
      "batch 259, loss: 0.0000, instance_loss: 0.1476, weighted_loss: 0.0443, label: 0, bag_size: 92\n",
      "batch 279, loss: 0.0010, instance_loss: 0.1489, weighted_loss: 0.0454, label: 1, bag_size: 88\n",
      "batch 299, loss: 1.8123, instance_loss: 1.2701, weighted_loss: 1.6496, label: 1, bag_size: 96\n",
      "batch 319, loss: 0.0000, instance_loss: 0.1200, weighted_loss: 0.0360, label: 1, bag_size: 67\n",
      "batch 339, loss: 0.0000, instance_loss: 0.2205, weighted_loss: 0.0661, label: 0, bag_size: 65\n",
      "batch 359, loss: 0.0000, instance_loss: 0.2751, weighted_loss: 0.0825, label: 0, bag_size: 72\n",
      "batch 379, loss: 0.0135, instance_loss: 0.5303, weighted_loss: 0.1685, label: 1, bag_size: 83\n",
      "batch 399, loss: 0.0000, instance_loss: 0.2430, weighted_loss: 0.0729, label: 0, bag_size: 34\n",
      "batch 419, loss: 0.0000, instance_loss: 0.2577, weighted_loss: 0.0773, label: 1, bag_size: 56\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0017, weighted_loss: 0.0005, label: 0, bag_size: 37\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0337, weighted_loss: 0.0101, label: 0, bag_size: 78\n",
      "batch 479, loss: 0.0000, instance_loss: 0.2626, weighted_loss: 0.0788, label: 1, bag_size: 47\n",
      "batch 499, loss: 0.0001, instance_loss: 0.3304, weighted_loss: 0.0992, label: 1, bag_size: 73\n",
      "batch 519, loss: 0.0000, instance_loss: 0.2430, weighted_loss: 0.0729, label: 1, bag_size: 56\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0407, weighted_loss: 0.0122, label: 0, bag_size: 21\n",
      "batch 559, loss: 0.0026, instance_loss: 0.0180, weighted_loss: 0.0073, label: 0, bag_size: 83\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0234, weighted_loss: 0.0070, label: 0, bag_size: 75\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0068, weighted_loss: 0.0021, label: 0, bag_size: 66\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0057, weighted_loss: 0.0017, label: 0, bag_size: 26\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0048, weighted_loss: 0.0014, label: 0, bag_size: 112\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9494230769230769: correct 9874/10400\n",
      "class 1 clustering acc 0.7955769230769231: correct 4137/5200\n",
      "Epoch: 40, train_loss: 0.2754, train_clustering_loss:  0.3831, train_error: 0.0908\n",
      "class 0: acc 0.9134328358208955, correct 306/335\n",
      "class 1: acc 0.9047619047619048, correct 285/315\n",
      "\n",
      "Val Set, val_loss: 0.4446, val_error: 0.0860, auc: 0.9759\n",
      "class 0 clustering acc 0.9764784946236559: correct 1453/1488\n",
      "class 1 clustering acc 0.6545698924731183: correct 487/744\n",
      "class 0: acc 0.9787234042553191, correct 46/47\n",
      "class 1: acc 0.8478260869565217, correct 39/46\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0072, instance_loss: 0.2415, weighted_loss: 0.0775, label: 1, bag_size: 29\n",
      "batch 39, loss: 1.2000, instance_loss: 0.7699, weighted_loss: 1.0709, label: 1, bag_size: 18\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0507, weighted_loss: 0.0152, label: 1, bag_size: 88\n",
      "batch 79, loss: 0.0013, instance_loss: 0.0010, weighted_loss: 0.0012, label: 0, bag_size: 103\n",
      "batch 99, loss: 0.0009, instance_loss: 0.0001, weighted_loss: 0.0006, label: 0, bag_size: 104\n",
      "batch 119, loss: 0.0001, instance_loss: 0.3388, weighted_loss: 0.1017, label: 1, bag_size: 33\n",
      "batch 139, loss: 0.0024, instance_loss: 0.2332, weighted_loss: 0.0716, label: 1, bag_size: 52\n",
      "batch 159, loss: 0.1288, instance_loss: 0.2650, weighted_loss: 0.1696, label: 0, bag_size: 28\n",
      "batch 179, loss: 0.0031, instance_loss: 0.0110, weighted_loss: 0.0055, label: 0, bag_size: 61\n",
      "batch 199, loss: 0.0010, instance_loss: 0.0265, weighted_loss: 0.0087, label: 0, bag_size: 126\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0902, weighted_loss: 0.0271, label: 1, bag_size: 82\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 40\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 67\n",
      "batch 279, loss: 0.0098, instance_loss: 0.2915, weighted_loss: 0.0943, label: 1, bag_size: 20\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0145, weighted_loss: 0.0044, label: 1, bag_size: 44\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 62\n",
      "batch 339, loss: 0.0025, instance_loss: 0.0747, weighted_loss: 0.0242, label: 0, bag_size: 44\n",
      "batch 359, loss: 7.9259, instance_loss: 4.1212, weighted_loss: 6.7845, label: 0, bag_size: 58\n",
      "batch 379, loss: 0.1460, instance_loss: 0.6978, weighted_loss: 0.3115, label: 0, bag_size: 27\n",
      "batch 399, loss: 0.0063, instance_loss: 0.1982, weighted_loss: 0.0639, label: 1, bag_size: 31\n",
      "batch 419, loss: 0.0006, instance_loss: 0.1015, weighted_loss: 0.0309, label: 1, bag_size: 61\n",
      "batch 439, loss: 0.0009, instance_loss: 0.0066, weighted_loss: 0.0026, label: 0, bag_size: 78\n",
      "batch 459, loss: 0.1751, instance_loss: 0.1387, weighted_loss: 0.1642, label: 1, bag_size: 34\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 126\n",
      "batch 499, loss: 0.0112, instance_loss: 0.0460, weighted_loss: 0.0217, label: 0, bag_size: 39\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0985, weighted_loss: 0.0295, label: 1, bag_size: 86\n",
      "batch 539, loss: 0.0070, instance_loss: 0.0134, weighted_loss: 0.0089, label: 0, bag_size: 18\n",
      "batch 559, loss: 0.0051, instance_loss: 0.1005, weighted_loss: 0.0337, label: 1, bag_size: 25\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 65\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0905, weighted_loss: 0.0272, label: 1, bag_size: 56\n",
      "batch 619, loss: 0.4158, instance_loss: 1.1108, weighted_loss: 0.6243, label: 0, bag_size: 17\n",
      "batch 639, loss: 0.5409, instance_loss: 0.5147, weighted_loss: 0.5331, label: 1, bag_size: 22\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9623076923076923: correct 10008/10400\n",
      "class 1 clustering acc 0.8334615384615385: correct 4334/5200\n",
      "Epoch: 41, train_loss: 0.3126, train_clustering_loss:  0.3206, train_error: 0.0923\n",
      "class 0: acc 0.9096209912536443, correct 312/343\n",
      "class 1: acc 0.9055374592833876, correct 278/307\n",
      "\n",
      "Val Set, val_loss: 0.3403, val_error: 0.1075, auc: 0.9718\n",
      "class 0 clustering acc 0.8931451612903226: correct 1329/1488\n",
      "class 1 clustering acc 0.8508064516129032: correct 633/744\n",
      "class 0: acc 0.8723404255319149, correct 41/47\n",
      "class 1: acc 0.9130434782608695, correct 42/46\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 89\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 88\n",
      "batch 59, loss: 0.0011, instance_loss: 0.0008, weighted_loss: 0.0010, label: 0, bag_size: 38\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0057, weighted_loss: 0.0017, label: 0, bag_size: 78\n",
      "batch 99, loss: 4.5135, instance_loss: 0.8354, weighted_loss: 3.4101, label: 0, bag_size: 75\n",
      "batch 119, loss: 0.0004, instance_loss: 0.1231, weighted_loss: 0.0372, label: 1, bag_size: 109\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0329, weighted_loss: 0.0099, label: 1, bag_size: 118\n",
      "batch 159, loss: 0.0649, instance_loss: 0.5577, weighted_loss: 0.2128, label: 0, bag_size: 29\n",
      "batch 179, loss: 0.0012, instance_loss: 0.1858, weighted_loss: 0.0566, label: 0, bag_size: 46\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0248, weighted_loss: 0.0074, label: 0, bag_size: 49\n",
      "batch 219, loss: 0.0007, instance_loss: 0.0185, weighted_loss: 0.0060, label: 1, bag_size: 20\n",
      "batch 239, loss: 0.0000, instance_loss: 0.1030, weighted_loss: 0.0309, label: 0, bag_size: 25\n",
      "batch 259, loss: 0.0005, instance_loss: 0.0296, weighted_loss: 0.0092, label: 0, bag_size: 37\n",
      "batch 279, loss: 0.0031, instance_loss: 0.0021, weighted_loss: 0.0028, label: 0, bag_size: 78\n",
      "batch 299, loss: 0.0003, instance_loss: 0.0454, weighted_loss: 0.0139, label: 1, bag_size: 77\n",
      "batch 319, loss: 0.4229, instance_loss: 0.2927, weighted_loss: 0.3838, label: 1, bag_size: 60\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0057, weighted_loss: 0.0017, label: 0, bag_size: 65\n",
      "batch 359, loss: 0.0016, instance_loss: 0.1168, weighted_loss: 0.0361, label: 0, bag_size: 73\n",
      "batch 379, loss: 0.0312, instance_loss: 0.0185, weighted_loss: 0.0274, label: 1, bag_size: 118\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 25\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 0, bag_size: 40\n",
      "batch 439, loss: 0.0003, instance_loss: 0.0158, weighted_loss: 0.0049, label: 0, bag_size: 29\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0001, label: 0, bag_size: 114\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 51\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0104, weighted_loss: 0.0031, label: 1, bag_size: 116\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0037, weighted_loss: 0.0011, label: 0, bag_size: 75\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0056, weighted_loss: 0.0017, label: 0, bag_size: 21\n",
      "batch 559, loss: 0.0926, instance_loss: 0.5138, weighted_loss: 0.2190, label: 1, bag_size: 70\n",
      "batch 579, loss: 0.0003, instance_loss: 0.1913, weighted_loss: 0.0576, label: 0, bag_size: 47\n",
      "batch 599, loss: 1.6405, instance_loss: 0.9660, weighted_loss: 1.4382, label: 1, bag_size: 60\n",
      "batch 619, loss: 6.3282, instance_loss: 1.1960, weighted_loss: 4.7886, label: 0, bag_size: 28\n",
      "batch 639, loss: 0.1502, instance_loss: 0.9436, weighted_loss: 0.3882, label: 0, bag_size: 17\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9608653846153846: correct 9993/10400\n",
      "class 1 clustering acc 0.8461538461538461: correct 4400/5200\n",
      "Epoch: 42, train_loss: 0.3473, train_clustering_loss:  0.2914, train_error: 0.1092\n",
      "class 0: acc 0.8941176470588236, correct 304/340\n",
      "class 1: acc 0.8870967741935484, correct 275/310\n",
      "\n",
      "Val Set, val_loss: 0.8352, val_error: 0.1505, auc: 0.9630\n",
      "class 0 clustering acc 0.6901881720430108: correct 1027/1488\n",
      "class 1 clustering acc 0.8024193548387096: correct 597/744\n",
      "class 0: acc 1.0, correct 47/47\n",
      "class 1: acc 0.6956521739130435, correct 32/46\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0682, weighted_loss: 0.0205, label: 1, bag_size: 41\n",
      "batch 39, loss: 0.0000, instance_loss: 0.1222, weighted_loss: 0.0367, label: 1, bag_size: 100\n",
      "batch 59, loss: 0.0000, instance_loss: 0.2730, weighted_loss: 0.0819, label: 0, bag_size: 102\n",
      "batch 79, loss: 0.0002, instance_loss: 0.0151, weighted_loss: 0.0047, label: 0, bag_size: 95\n",
      "batch 99, loss: 2.3542, instance_loss: 0.5624, weighted_loss: 1.8167, label: 1, bag_size: 43\n",
      "batch 119, loss: 0.0030, instance_loss: 0.0434, weighted_loss: 0.0151, label: 1, bag_size: 62\n",
      "batch 139, loss: 0.0020, instance_loss: 0.1151, weighted_loss: 0.0359, label: 1, bag_size: 42\n",
      "batch 159, loss: 0.0000, instance_loss: 1.9749, weighted_loss: 0.5925, label: 1, bag_size: 38\n",
      "batch 179, loss: 0.0001, instance_loss: 0.2618, weighted_loss: 0.0786, label: 1, bag_size: 77\n",
      "batch 199, loss: 0.0003, instance_loss: 0.0487, weighted_loss: 0.0148, label: 1, bag_size: 73\n",
      "batch 219, loss: 0.0000, instance_loss: 0.4032, weighted_loss: 0.1210, label: 0, bag_size: 43\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0362, weighted_loss: 0.0109, label: 1, bag_size: 101\n",
      "batch 259, loss: 0.0000, instance_loss: 0.2677, weighted_loss: 0.0803, label: 0, bag_size: 24\n",
      "batch 279, loss: 0.0000, instance_loss: 0.1190, weighted_loss: 0.0357, label: 0, bag_size: 63\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0087, weighted_loss: 0.0026, label: 0, bag_size: 79\n",
      "batch 319, loss: 1.3396, instance_loss: 0.1352, weighted_loss: 0.9783, label: 0, bag_size: 44\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0091, weighted_loss: 0.0027, label: 0, bag_size: 118\n",
      "batch 359, loss: 9.0555, instance_loss: 5.4124, weighted_loss: 7.9626, label: 0, bag_size: 31\n",
      "batch 379, loss: 0.1186, instance_loss: 0.1444, weighted_loss: 0.1264, label: 1, bag_size: 23\n",
      "batch 399, loss: 0.0004, instance_loss: 0.1536, weighted_loss: 0.0464, label: 0, bag_size: 21\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0806, weighted_loss: 0.0242, label: 0, bag_size: 86\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0126, weighted_loss: 0.0038, label: 1, bag_size: 136\n",
      "batch 459, loss: 0.0000, instance_loss: 0.1454, weighted_loss: 0.0436, label: 1, bag_size: 82\n",
      "batch 479, loss: 0.0038, instance_loss: 0.1344, weighted_loss: 0.0430, label: 1, bag_size: 30\n",
      "batch 499, loss: 0.2625, instance_loss: 0.2450, weighted_loss: 0.2573, label: 0, bag_size: 86\n",
      "batch 519, loss: 0.7793, instance_loss: 0.3234, weighted_loss: 0.6425, label: 1, bag_size: 89\n",
      "batch 539, loss: 0.0000, instance_loss: 0.1318, weighted_loss: 0.0395, label: 1, bag_size: 79\n",
      "batch 559, loss: 0.0000, instance_loss: 0.3849, weighted_loss: 0.1155, label: 1, bag_size: 26\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0163, weighted_loss: 0.0049, label: 0, bag_size: 104\n",
      "batch 599, loss: 2.0447, instance_loss: 1.1971, weighted_loss: 1.7904, label: 1, bag_size: 60\n",
      "batch 619, loss: 0.0754, instance_loss: 0.2191, weighted_loss: 0.1185, label: 1, bag_size: 88\n",
      "batch 639, loss: 0.0000, instance_loss: 0.5201, weighted_loss: 0.1560, label: 0, bag_size: 35\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9463461538461538: correct 9842/10400\n",
      "class 1 clustering acc 0.8153846153846154: correct 4240/5200\n",
      "Epoch: 43, train_loss: 0.4485, train_clustering_loss:  0.3893, train_error: 0.1046\n",
      "class 0: acc 0.8934169278996865, correct 285/319\n",
      "class 1: acc 0.8972809667673716, correct 297/331\n",
      "\n",
      "Val Set, val_loss: 1.1269, val_error: 0.2258, auc: 0.9371\n",
      "class 0 clustering acc 0.9146505376344086: correct 1361/1488\n",
      "class 1 clustering acc 0.07258064516129033: correct 54/744\n",
      "class 0: acc 0.5957446808510638, correct 28/47\n",
      "class 1: acc 0.9565217391304348, correct 44/46\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.7125, instance_loss: 0.4716, weighted_loss: 1.3402, label: 1, bag_size: 39\n",
      "batch 39, loss: 0.0001, instance_loss: 0.4254, weighted_loss: 0.1277, label: 0, bag_size: 74\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0154, weighted_loss: 0.0046, label: 1, bag_size: 87\n",
      "batch 79, loss: 0.0000, instance_loss: 0.1384, weighted_loss: 0.0415, label: 1, bag_size: 76\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0049, weighted_loss: 0.0015, label: 0, bag_size: 57\n",
      "batch 119, loss: 0.0000, instance_loss: 0.1773, weighted_loss: 0.0532, label: 1, bag_size: 31\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0660, weighted_loss: 0.0198, label: 1, bag_size: 98\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0754, weighted_loss: 0.0226, label: 0, bag_size: 113\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0142, weighted_loss: 0.0043, label: 1, bag_size: 81\n",
      "batch 199, loss: 1.3241, instance_loss: 0.2472, weighted_loss: 1.0010, label: 1, bag_size: 23\n",
      "batch 219, loss: 0.0000, instance_loss: 0.1115, weighted_loss: 0.0335, label: 1, bag_size: 16\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0758, weighted_loss: 0.0228, label: 1, bag_size: 85\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0453, weighted_loss: 0.0136, label: 1, bag_size: 22\n",
      "batch 279, loss: 0.1509, instance_loss: 3.7404, weighted_loss: 1.2278, label: 0, bag_size: 56\n",
      "batch 299, loss: 0.0017, instance_loss: 0.1617, weighted_loss: 0.0497, label: 0, bag_size: 73\n",
      "batch 319, loss: 0.0008, instance_loss: 0.0759, weighted_loss: 0.0234, label: 0, bag_size: 103\n",
      "batch 339, loss: 0.2918, instance_loss: 1.1083, weighted_loss: 0.5367, label: 0, bag_size: 18\n",
      "batch 359, loss: 0.0074, instance_loss: 0.0404, weighted_loss: 0.0173, label: 0, bag_size: 66\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0106, weighted_loss: 0.0032, label: 1, bag_size: 33\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0496, weighted_loss: 0.0149, label: 1, bag_size: 17\n",
      "batch 419, loss: 2.2410, instance_loss: 1.7684, weighted_loss: 2.0993, label: 1, bag_size: 67\n",
      "batch 439, loss: 0.0158, instance_loss: 1.0208, weighted_loss: 0.3173, label: 0, bag_size: 93\n",
      "batch 459, loss: 0.0000, instance_loss: 0.4345, weighted_loss: 0.1304, label: 1, bag_size: 51\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0400, weighted_loss: 0.0120, label: 0, bag_size: 42\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0039, weighted_loss: 0.0012, label: 0, bag_size: 72\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 65\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 0, bag_size: 49\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 0, bag_size: 92\n",
      "batch 579, loss: 0.0072, instance_loss: 0.0084, weighted_loss: 0.0076, label: 1, bag_size: 41\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 39\n",
      "batch 619, loss: 0.1109, instance_loss: 0.9641, weighted_loss: 0.3669, label: 0, bag_size: 32\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0022, weighted_loss: 0.0007, label: 0, bag_size: 36\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9576923076923077: correct 9960/10400\n",
      "class 1 clustering acc 0.8011538461538461: correct 4166/5200\n",
      "Epoch: 44, train_loss: 0.6412, train_clustering_loss:  0.3882, train_error: 0.1185\n",
      "class 0: acc 0.8858858858858859, correct 295/333\n",
      "class 1: acc 0.8769716088328076, correct 278/317\n",
      "\n",
      "Val Set, val_loss: 0.3883, val_error: 0.1075, auc: 0.9769\n",
      "class 0 clustering acc 0.9475806451612904: correct 1410/1488\n",
      "class 1 clustering acc 0.8346774193548387: correct 621/744\n",
      "class 0: acc 0.9787234042553191, correct 46/47\n",
      "class 1: acc 0.8043478260869565, correct 37/46\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0028, weighted_loss: 0.0009, label: 1, bag_size: 84\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0612, weighted_loss: 0.0184, label: 0, bag_size: 39\n",
      "batch 59, loss: 0.0003, instance_loss: 0.0122, weighted_loss: 0.0039, label: 1, bag_size: 82\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 1, bag_size: 71\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 77\n",
      "batch 119, loss: 0.7633, instance_loss: 0.0451, weighted_loss: 0.5479, label: 0, bag_size: 67\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0299, weighted_loss: 0.0090, label: 0, bag_size: 43\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0377, weighted_loss: 0.0113, label: 1, bag_size: 112\n",
      "batch 179, loss: 0.0003, instance_loss: 0.0079, weighted_loss: 0.0026, label: 1, bag_size: 87\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0293, weighted_loss: 0.0089, label: 0, bag_size: 63\n",
      "batch 219, loss: 0.4352, instance_loss: 0.6958, weighted_loss: 0.5134, label: 0, bag_size: 20\n",
      "batch 239, loss: 0.4874, instance_loss: 0.4545, weighted_loss: 0.4775, label: 0, bag_size: 102\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0029, weighted_loss: 0.0009, label: 1, bag_size: 111\n",
      "batch 279, loss: 0.2673, instance_loss: 0.1296, weighted_loss: 0.2260, label: 0, bag_size: 40\n",
      "batch 299, loss: 0.0233, instance_loss: 0.9393, weighted_loss: 0.2981, label: 1, bag_size: 60\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0671, weighted_loss: 0.0202, label: 1, bag_size: 53\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0096, weighted_loss: 0.0029, label: 1, bag_size: 34\n",
      "batch 359, loss: 0.0001, instance_loss: 0.1252, weighted_loss: 0.0376, label: 0, bag_size: 73\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0287, weighted_loss: 0.0086, label: 0, bag_size: 43\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0846, weighted_loss: 0.0254, label: 1, bag_size: 84\n",
      "batch 419, loss: 0.1745, instance_loss: 0.1799, weighted_loss: 0.1761, label: 0, bag_size: 97\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0019, weighted_loss: 0.0006, label: 1, bag_size: 31\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0016, weighted_loss: 0.0005, label: 1, bag_size: 83\n",
      "batch 479, loss: 3.8166, instance_loss: 2.3340, weighted_loss: 3.3718, label: 0, bag_size: 73\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0143, weighted_loss: 0.0044, label: 1, bag_size: 61\n",
      "batch 519, loss: 0.0571, instance_loss: 0.1912, weighted_loss: 0.0973, label: 1, bag_size: 42\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0204, weighted_loss: 0.0061, label: 0, bag_size: 27\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0069, weighted_loss: 0.0021, label: 1, bag_size: 97\n",
      "batch 579, loss: 1.0646, instance_loss: 0.1067, weighted_loss: 0.7772, label: 1, bag_size: 41\n",
      "batch 599, loss: 0.0006, instance_loss: 0.1259, weighted_loss: 0.0382, label: 1, bag_size: 45\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0176, weighted_loss: 0.0053, label: 0, bag_size: 38\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0145, weighted_loss: 0.0044, label: 0, bag_size: 63\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9716346153846154: correct 10105/10400\n",
      "class 1 clustering acc 0.8771153846153846: correct 4561/5200\n",
      "Epoch: 45, train_loss: 0.4903, train_clustering_loss:  0.2624, train_error: 0.1000\n",
      "class 0: acc 0.9048991354466859, correct 314/347\n",
      "class 1: acc 0.8943894389438944, correct 271/303\n",
      "\n",
      "Val Set, val_loss: 0.3656, val_error: 0.0968, auc: 0.9658\n",
      "class 0 clustering acc 0.8951612903225806: correct 1332/1488\n",
      "class 1 clustering acc 0.853494623655914: correct 635/744\n",
      "class 0: acc 0.9148936170212766, correct 43/47\n",
      "class 1: acc 0.8913043478260869, correct 41/46\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0192, weighted_loss: 0.0058, label: 0, bag_size: 104\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0020, weighted_loss: 0.0006, label: 1, bag_size: 82\n",
      "batch 59, loss: 0.0065, instance_loss: 0.0637, weighted_loss: 0.0237, label: 0, bag_size: 43\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0000, label: 1, bag_size: 76\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 51\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 88\n",
      "batch 139, loss: 1.9897, instance_loss: 1.0912, weighted_loss: 1.7202, label: 1, bag_size: 85\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0024, weighted_loss: 0.0008, label: 1, bag_size: 72\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0064, weighted_loss: 0.0019, label: 0, bag_size: 45\n",
      "batch 199, loss: 0.3402, instance_loss: 0.9568, weighted_loss: 0.5252, label: 0, bag_size: 62\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0179, weighted_loss: 0.0054, label: 1, bag_size: 34\n",
      "batch 239, loss: 0.0020, instance_loss: 0.0042, weighted_loss: 0.0026, label: 1, bag_size: 40\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0723, weighted_loss: 0.0217, label: 1, bag_size: 33\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 88\n",
      "batch 299, loss: 1.6577, instance_loss: 0.4858, weighted_loss: 1.3061, label: 0, bag_size: 73\n",
      "batch 319, loss: 0.0004, instance_loss: 0.0017, weighted_loss: 0.0008, label: 0, bag_size: 17\n",
      "batch 339, loss: 3.4342, instance_loss: 1.7871, weighted_loss: 2.9400, label: 0, bag_size: 90\n",
      "batch 359, loss: 0.0701, instance_loss: 0.0306, weighted_loss: 0.0582, label: 1, bag_size: 99\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0045, weighted_loss: 0.0014, label: 1, bag_size: 89\n",
      "batch 399, loss: 0.0161, instance_loss: 0.0747, weighted_loss: 0.0337, label: 0, bag_size: 54\n",
      "batch 419, loss: 0.0555, instance_loss: 0.0326, weighted_loss: 0.0486, label: 0, bag_size: 18\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 0, bag_size: 31\n",
      "batch 459, loss: 0.0002, instance_loss: 0.0003, weighted_loss: 0.0002, label: 1, bag_size: 67\n",
      "batch 479, loss: 0.0073, instance_loss: 0.5062, weighted_loss: 0.1569, label: 0, bag_size: 50\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 39\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 1, bag_size: 56\n",
      "batch 539, loss: 0.0008, instance_loss: 0.0172, weighted_loss: 0.0057, label: 1, bag_size: 24\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 78\n",
      "batch 579, loss: 0.0029, instance_loss: 0.2440, weighted_loss: 0.0752, label: 0, bag_size: 28\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0442, weighted_loss: 0.0133, label: 1, bag_size: 58\n",
      "batch 619, loss: 3.6966, instance_loss: 2.0899, weighted_loss: 3.2146, label: 0, bag_size: 81\n",
      "batch 639, loss: 0.0046, instance_loss: 0.1828, weighted_loss: 0.0581, label: 0, bag_size: 101\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9705769230769231: correct 10094/10400\n",
      "class 1 clustering acc 0.8776923076923077: correct 4564/5200\n",
      "Epoch: 46, train_loss: 0.3232, train_clustering_loss:  0.2541, train_error: 0.0923\n",
      "class 0: acc 0.9127725856697819, correct 293/321\n",
      "class 1: acc 0.9027355623100304, correct 297/329\n",
      "\n",
      "Val Set, val_loss: 0.7051, val_error: 0.1183, auc: 0.9639\n",
      "class 0 clustering acc 0.9576612903225806: correct 1425/1488\n",
      "class 1 clustering acc 0.8306451612903226: correct 618/744\n",
      "class 0: acc 0.9574468085106383, correct 45/47\n",
      "class 1: acc 0.8043478260869565, correct 37/46\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0129, weighted_loss: 0.0039, label: 0, bag_size: 94\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0004, label: 1, bag_size: 72\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 71\n",
      "batch 79, loss: 0.0001, instance_loss: 0.0907, weighted_loss: 0.0273, label: 1, bag_size: 32\n",
      "batch 99, loss: 0.0080, instance_loss: 0.0007, weighted_loss: 0.0058, label: 0, bag_size: 69\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 67\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0050, weighted_loss: 0.0015, label: 1, bag_size: 44\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 0, bag_size: 87\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0089, weighted_loss: 0.0027, label: 1, bag_size: 107\n",
      "batch 199, loss: 0.0008, instance_loss: 0.0076, weighted_loss: 0.0028, label: 1, bag_size: 86\n",
      "batch 219, loss: 0.3220, instance_loss: 1.3031, weighted_loss: 0.6163, label: 1, bag_size: 60\n",
      "batch 239, loss: 0.0033, instance_loss: 0.0622, weighted_loss: 0.0210, label: 1, bag_size: 60\n",
      "batch 259, loss: 27.9354, instance_loss: 0.4590, weighted_loss: 19.6925, label: 0, bag_size: 53\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0747, weighted_loss: 0.0224, label: 1, bag_size: 76\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 67\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0509, weighted_loss: 0.0154, label: 1, bag_size: 89\n",
      "batch 339, loss: 0.0253, instance_loss: 0.4831, weighted_loss: 0.1626, label: 0, bag_size: 21\n",
      "batch 359, loss: 0.0000, instance_loss: 0.3795, weighted_loss: 0.1139, label: 1, bag_size: 29\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0376, weighted_loss: 0.0113, label: 1, bag_size: 48\n",
      "batch 399, loss: 0.0002, instance_loss: 0.6455, weighted_loss: 0.1938, label: 0, bag_size: 28\n",
      "batch 419, loss: 0.0000, instance_loss: 0.4176, weighted_loss: 0.1253, label: 0, bag_size: 65\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0525, weighted_loss: 0.0157, label: 1, bag_size: 24\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0105, weighted_loss: 0.0031, label: 1, bag_size: 48\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0425, weighted_loss: 0.0127, label: 0, bag_size: 75\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0005, label: 1, bag_size: 41\n",
      "batch 519, loss: 0.0000, instance_loss: 0.1107, weighted_loss: 0.0332, label: 1, bag_size: 17\n",
      "batch 539, loss: 4.4903, instance_loss: 0.8948, weighted_loss: 3.4117, label: 1, bag_size: 84\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0025, weighted_loss: 0.0007, label: 0, bag_size: 108\n",
      "batch 579, loss: 0.0005, instance_loss: 0.3051, weighted_loss: 0.0919, label: 0, bag_size: 39\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 63\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0142, weighted_loss: 0.0043, label: 0, bag_size: 111\n",
      "batch 639, loss: 0.0005, instance_loss: 0.0199, weighted_loss: 0.0063, label: 0, bag_size: 46\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9595192307692307: correct 9979/10400\n",
      "class 1 clustering acc 0.8190384615384615: correct 4259/5200\n",
      "Epoch: 47, train_loss: 1.0463, train_clustering_loss:  0.3552, train_error: 0.1523\n",
      "class 0: acc 0.8381877022653722, correct 259/309\n",
      "class 1: acc 0.8563049853372434, correct 292/341\n",
      "\n",
      "Val Set, val_loss: 2.9589, val_error: 0.2581, auc: 0.9084\n",
      "class 0 clustering acc 0.9704301075268817: correct 1444/1488\n",
      "class 1 clustering acc 0.8266129032258065: correct 615/744\n",
      "class 0: acc 0.5106382978723404, correct 24/47\n",
      "class 1: acc 0.9782608695652174, correct 45/46\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0220, instance_loss: 0.7987, weighted_loss: 0.2550, label: 1, bag_size: 98\n",
      "batch 39, loss: 0.0287, instance_loss: 0.5804, weighted_loss: 0.1942, label: 1, bag_size: 77\n",
      "batch 59, loss: 0.0003, instance_loss: 0.3742, weighted_loss: 0.1124, label: 0, bag_size: 44\n",
      "batch 79, loss: 0.0000, instance_loss: 0.2601, weighted_loss: 0.0780, label: 0, bag_size: 41\n",
      "batch 99, loss: 0.0001, instance_loss: 0.2778, weighted_loss: 0.0834, label: 0, bag_size: 95\n",
      "batch 119, loss: 0.0000, instance_loss: 0.4131, weighted_loss: 0.1239, label: 1, bag_size: 52\n",
      "batch 139, loss: 0.0230, instance_loss: 0.8496, weighted_loss: 0.2710, label: 0, bag_size: 54\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0447, weighted_loss: 0.0134, label: 1, bag_size: 25\n",
      "batch 179, loss: 20.6746, instance_loss: 4.5186, weighted_loss: 15.8278, label: 0, bag_size: 18\n",
      "batch 199, loss: 0.0223, instance_loss: 0.5195, weighted_loss: 0.1715, label: 0, bag_size: 82\n",
      "batch 219, loss: 3.8187, instance_loss: 1.2004, weighted_loss: 3.0332, label: 1, bag_size: 41\n",
      "batch 239, loss: 0.0439, instance_loss: 0.4173, weighted_loss: 0.1559, label: 0, bag_size: 34\n",
      "batch 259, loss: 3.2882, instance_loss: 0.5230, weighted_loss: 2.4586, label: 0, bag_size: 86\n",
      "batch 279, loss: 0.0000, instance_loss: 0.1214, weighted_loss: 0.0364, label: 0, bag_size: 101\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0145, weighted_loss: 0.0044, label: 0, bag_size: 103\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0221, weighted_loss: 0.0066, label: 0, bag_size: 91\n",
      "batch 339, loss: 7.5519, instance_loss: 3.2650, weighted_loss: 6.2659, label: 1, bag_size: 42\n",
      "batch 359, loss: 0.0000, instance_loss: 0.2359, weighted_loss: 0.0708, label: 1, bag_size: 22\n",
      "batch 379, loss: 0.0614, instance_loss: 0.7846, weighted_loss: 0.2783, label: 1, bag_size: 65\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0156, weighted_loss: 0.0047, label: 0, bag_size: 110\n",
      "batch 419, loss: 3.0000, instance_loss: 1.4819, weighted_loss: 2.5445, label: 0, bag_size: 25\n",
      "batch 439, loss: 0.0149, instance_loss: 1.9657, weighted_loss: 0.6002, label: 0, bag_size: 27\n",
      "batch 459, loss: 0.0000, instance_loss: 0.1371, weighted_loss: 0.0411, label: 1, bag_size: 82\n",
      "batch 479, loss: 0.0026, instance_loss: 0.0890, weighted_loss: 0.0285, label: 0, bag_size: 26\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0026, weighted_loss: 0.0008, label: 0, bag_size: 85\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0030, weighted_loss: 0.0009, label: 0, bag_size: 25\n",
      "batch 539, loss: 0.0028, instance_loss: 0.0418, weighted_loss: 0.0145, label: 0, bag_size: 62\n",
      "batch 559, loss: 0.0000, instance_loss: 0.1518, weighted_loss: 0.0456, label: 1, bag_size: 50\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0001, label: 1, bag_size: 112\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0825, weighted_loss: 0.0248, label: 0, bag_size: 17\n",
      "batch 619, loss: 0.0008, instance_loss: 0.0430, weighted_loss: 0.0134, label: 1, bag_size: 21\n",
      "batch 639, loss: 0.0006, instance_loss: 0.0046, weighted_loss: 0.0018, label: 1, bag_size: 67\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9472115384615385: correct 9851/10400\n",
      "class 1 clustering acc 0.7917307692307692: correct 4117/5200\n",
      "Epoch: 48, train_loss: 0.8708, train_clustering_loss:  0.4021, train_error: 0.1200\n",
      "class 0: acc 0.8802395209580839, correct 294/334\n",
      "class 1: acc 0.879746835443038, correct 278/316\n",
      "\n",
      "Val Set, val_loss: 0.4417, val_error: 0.0753, auc: 0.9672\n",
      "class 0 clustering acc 0.967741935483871: correct 1440/1488\n",
      "class 1 clustering acc 0.8279569892473119: correct 616/744\n",
      "class 0: acc 0.9148936170212766, correct 43/47\n",
      "class 1: acc 0.9347826086956522, correct 43/46\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 6.3399, instance_loss: 4.7706, weighted_loss: 5.8691, label: 0, bag_size: 50\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0017, weighted_loss: 0.0005, label: 1, bag_size: 107\n",
      "batch 59, loss: 0.0002, instance_loss: 0.1604, weighted_loss: 0.0483, label: 0, bag_size: 48\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 100\n",
      "batch 99, loss: 0.0000, instance_loss: 0.2179, weighted_loss: 0.0654, label: 0, bag_size: 35\n",
      "batch 119, loss: 0.8252, instance_loss: 0.2485, weighted_loss: 0.6522, label: 0, bag_size: 28\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0212, weighted_loss: 0.0064, label: 1, bag_size: 67\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0020, weighted_loss: 0.0006, label: 0, bag_size: 33\n",
      "batch 179, loss: 0.0005, instance_loss: 0.0416, weighted_loss: 0.0129, label: 0, bag_size: 66\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 90\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 1, bag_size: 82\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 39\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0046, weighted_loss: 0.0014, label: 0, bag_size: 52\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0141, weighted_loss: 0.0043, label: 1, bag_size: 95\n",
      "batch 299, loss: 0.0000, instance_loss: 0.1196, weighted_loss: 0.0359, label: 0, bag_size: 18\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 0, bag_size: 74\n",
      "batch 339, loss: 0.2628, instance_loss: 0.1116, weighted_loss: 0.2175, label: 1, bag_size: 24\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0170, weighted_loss: 0.0051, label: 1, bag_size: 31\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 41\n",
      "batch 399, loss: 0.0881, instance_loss: 0.6269, weighted_loss: 0.2497, label: 1, bag_size: 51\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 1, bag_size: 70\n",
      "batch 439, loss: 0.4466, instance_loss: 0.0718, weighted_loss: 0.3341, label: 1, bag_size: 58\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 1, bag_size: 45\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0037, weighted_loss: 0.0011, label: 0, bag_size: 24\n",
      "batch 499, loss: 1.9843, instance_loss: 0.6381, weighted_loss: 1.5804, label: 0, bag_size: 72\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0812, weighted_loss: 0.0244, label: 1, bag_size: 25\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0244, weighted_loss: 0.0073, label: 1, bag_size: 97\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 37\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 116\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 67\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0025, weighted_loss: 0.0008, label: 0, bag_size: 83\n",
      "batch 639, loss: 0.0013, instance_loss: 0.0046, weighted_loss: 0.0023, label: 1, bag_size: 82\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9698076923076923: correct 10086/10400\n",
      "class 1 clustering acc 0.8723076923076923: correct 4536/5200\n",
      "Epoch: 49, train_loss: 0.3265, train_clustering_loss:  0.2616, train_error: 0.0892\n",
      "class 0: acc 0.9059561128526645, correct 289/319\n",
      "class 1: acc 0.9154078549848943, correct 303/331\n",
      "\n",
      "Val Set, val_loss: 0.5915, val_error: 0.0860, auc: 0.9579\n",
      "class 0 clustering acc 0.9616935483870968: correct 1431/1488\n",
      "class 1 clustering acc 0.8830645161290323: correct 657/744\n",
      "class 0: acc 0.9787234042553191, correct 46/47\n",
      "class 1: acc 0.8478260869565217, correct 39/46\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 86\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 1, bag_size: 19\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 34\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 0, bag_size: 62\n",
      "batch 99, loss: 0.0285, instance_loss: 0.5624, weighted_loss: 0.1887, label: 1, bag_size: 45\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0176, weighted_loss: 0.0053, label: 1, bag_size: 89\n",
      "batch 139, loss: 0.0012, instance_loss: 0.0844, weighted_loss: 0.0262, label: 1, bag_size: 76\n",
      "batch 159, loss: 0.0001, instance_loss: 0.1089, weighted_loss: 0.0328, label: 1, bag_size: 99\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0242, weighted_loss: 0.0073, label: 0, bag_size: 65\n",
      "batch 199, loss: 0.0002, instance_loss: 0.0051, weighted_loss: 0.0016, label: 0, bag_size: 122\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0242, weighted_loss: 0.0073, label: 1, bag_size: 74\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 0, bag_size: 83\n",
      "batch 259, loss: 0.0005, instance_loss: 0.1765, weighted_loss: 0.0533, label: 0, bag_size: 53\n",
      "batch 279, loss: 0.0000, instance_loss: 0.1023, weighted_loss: 0.0307, label: 0, bag_size: 26\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 34\n",
      "batch 319, loss: 0.3982, instance_loss: 0.2466, weighted_loss: 0.3527, label: 0, bag_size: 79\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 108\n",
      "batch 359, loss: 0.3170, instance_loss: 1.4066, weighted_loss: 0.6439, label: 1, bag_size: 51\n",
      "batch 379, loss: 0.0003, instance_loss: 0.0039, weighted_loss: 0.0014, label: 1, bag_size: 30\n",
      "batch 399, loss: 0.0002, instance_loss: 0.0080, weighted_loss: 0.0025, label: 0, bag_size: 75\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 43\n",
      "batch 439, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 123\n",
      "batch 459, loss: 0.0002, instance_loss: 0.0194, weighted_loss: 0.0060, label: 0, bag_size: 46\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0060, weighted_loss: 0.0018, label: 1, bag_size: 62\n",
      "batch 499, loss: 1.2164, instance_loss: 1.4959, weighted_loss: 1.3003, label: 0, bag_size: 18\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 65\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0048, weighted_loss: 0.0014, label: 0, bag_size: 27\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 31\n",
      "batch 579, loss: 5.9679, instance_loss: 3.4041, weighted_loss: 5.1987, label: 0, bag_size: 14\n",
      "batch 599, loss: 0.0012, instance_loss: 0.0898, weighted_loss: 0.0278, label: 0, bag_size: 27\n",
      "batch 619, loss: 1.7845, instance_loss: 0.5784, weighted_loss: 1.4227, label: 1, bag_size: 62\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0037, weighted_loss: 0.0012, label: 1, bag_size: 32\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9740384615384615: correct 10130/10400\n",
      "class 1 clustering acc 0.8888461538461538: correct 4622/5200\n",
      "Epoch: 50, train_loss: 0.2559, train_clustering_loss:  0.2274, train_error: 0.0708\n",
      "class 0: acc 0.921311475409836, correct 281/305\n",
      "class 1: acc 0.936231884057971, correct 323/345\n",
      "\n",
      "Val Set, val_loss: 0.4287, val_error: 0.1075, auc: 0.9611\n",
      "class 0 clustering acc 0.9576612903225806: correct 1425/1488\n",
      "class 1 clustering acc 0.8185483870967742: correct 609/744\n",
      "class 0: acc 0.9148936170212766, correct 43/47\n",
      "class 1: acc 0.8695652173913043, correct 40/46\n",
      "EarlyStopping counter: 12 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.0486, weighted_loss: 0.0147, label: 0, bag_size: 25\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 62\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 86\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 118\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 20\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 61\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 107\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 81\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 113\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 0, bag_size: 35\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0518, weighted_loss: 0.0156, label: 1, bag_size: 24\n",
      "batch 239, loss: 0.1109, instance_loss: 0.0134, weighted_loss: 0.0816, label: 0, bag_size: 88\n",
      "batch 259, loss: 0.0017, instance_loss: 0.0009, weighted_loss: 0.0014, label: 0, bag_size: 36\n",
      "batch 279, loss: 0.0002, instance_loss: 0.0269, weighted_loss: 0.0082, label: 0, bag_size: 25\n",
      "batch 299, loss: 0.0098, instance_loss: 0.7086, weighted_loss: 0.2195, label: 0, bag_size: 66\n",
      "batch 319, loss: 0.0065, instance_loss: 0.0562, weighted_loss: 0.0214, label: 0, bag_size: 28\n",
      "batch 339, loss: 0.9126, instance_loss: 2.0745, weighted_loss: 1.2612, label: 1, bag_size: 53\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0410, weighted_loss: 0.0123, label: 1, bag_size: 70\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0205, weighted_loss: 0.0062, label: 1, bag_size: 39\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0019, weighted_loss: 0.0006, label: 0, bag_size: 35\n",
      "batch 419, loss: 3.6081, instance_loss: 1.7318, weighted_loss: 3.0452, label: 0, bag_size: 90\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 43\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 55\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0005, label: 0, bag_size: 83\n",
      "batch 499, loss: 0.0004, instance_loss: 0.0018, weighted_loss: 0.0008, label: 1, bag_size: 45\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 48\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 85\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 25\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 85\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 84\n",
      "batch 619, loss: 0.0002, instance_loss: 0.0082, weighted_loss: 0.0026, label: 1, bag_size: 41\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9774038461538461: correct 10165/10400\n",
      "class 1 clustering acc 0.9017307692307692: correct 4689/5200\n",
      "Epoch: 51, train_loss: 0.2373, train_clustering_loss:  0.1915, train_error: 0.0631\n",
      "class 0: acc 0.9357798165137615, correct 306/327\n",
      "class 1: acc 0.9380804953560371, correct 303/323\n",
      "\n",
      "Val Set, val_loss: 0.6677, val_error: 0.1720, auc: 0.9681\n",
      "class 0 clustering acc 0.948252688172043: correct 1411/1488\n",
      "class 1 clustering acc 0.7983870967741935: correct 594/744\n",
      "class 0: acc 0.6808510638297872, correct 32/47\n",
      "class 1: acc 0.9782608695652174, correct 45/46\n",
      "EarlyStopping counter: 13 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 2.5814, instance_loss: 1.3328, weighted_loss: 2.2068, label: 1, bag_size: 111\n",
      "batch 39, loss: 0.0002, instance_loss: 0.0004, weighted_loss: 0.0003, label: 0, bag_size: 85\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 44\n",
      "batch 79, loss: 0.0019, instance_loss: 0.0201, weighted_loss: 0.0074, label: 0, bag_size: 25\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 23\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 73\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 127\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 68\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 1, bag_size: 92\n",
      "batch 199, loss: 0.4800, instance_loss: 0.0905, weighted_loss: 0.3632, label: 0, bag_size: 58\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 65\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0097, weighted_loss: 0.0029, label: 0, bag_size: 87\n",
      "batch 259, loss: 0.0018, instance_loss: 0.0034, weighted_loss: 0.0023, label: 1, bag_size: 28\n",
      "batch 279, loss: 0.1475, instance_loss: 0.3321, weighted_loss: 0.2029, label: 0, bag_size: 28\n",
      "batch 299, loss: 2.8511, instance_loss: 1.1661, weighted_loss: 2.3456, label: 0, bag_size: 73\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 86\n",
      "batch 339, loss: 0.1180, instance_loss: 0.0111, weighted_loss: 0.0859, label: 0, bag_size: 126\n",
      "batch 359, loss: 0.0005, instance_loss: 0.0080, weighted_loss: 0.0027, label: 0, bag_size: 42\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 1, bag_size: 96\n",
      "batch 399, loss: 0.1846, instance_loss: 0.1296, weighted_loss: 0.1681, label: 0, bag_size: 31\n",
      "batch 419, loss: 0.0847, instance_loss: 0.2138, weighted_loss: 0.1234, label: 0, bag_size: 94\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0097, weighted_loss: 0.0029, label: 1, bag_size: 33\n",
      "batch 459, loss: 0.3739, instance_loss: 1.2258, weighted_loss: 0.6295, label: 1, bag_size: 100\n",
      "batch 479, loss: 0.0527, instance_loss: 0.0431, weighted_loss: 0.0498, label: 1, bag_size: 21\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 110\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0085, weighted_loss: 0.0026, label: 1, bag_size: 37\n",
      "batch 539, loss: 0.0087, instance_loss: 0.0411, weighted_loss: 0.0184, label: 0, bag_size: 77\n",
      "batch 559, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 45\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 95\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 50\n",
      "batch 619, loss: 0.0007, instance_loss: 0.0008, weighted_loss: 0.0007, label: 1, bag_size: 56\n",
      "batch 639, loss: 0.0226, instance_loss: 0.1096, weighted_loss: 0.0487, label: 1, bag_size: 71\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9838461538461538: correct 10232/10400\n",
      "class 1 clustering acc 0.9115384615384615: correct 4740/5200\n",
      "Epoch: 52, train_loss: 0.2064, train_clustering_loss:  0.1574, train_error: 0.0631\n",
      "class 0: acc 0.9406528189910979, correct 317/337\n",
      "class 1: acc 0.9329073482428115, correct 292/313\n",
      "\n",
      "Val Set, val_loss: 0.8770, val_error: 0.1828, auc: 0.9672\n",
      "class 0 clustering acc 0.956989247311828: correct 1424/1488\n",
      "class 1 clustering acc 0.7970430107526881: correct 593/744\n",
      "class 0: acc 0.6595744680851063, correct 31/47\n",
      "class 1: acc 0.9782608695652174, correct 45/46\n",
      "EarlyStopping counter: 14 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 33\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 58\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0203, weighted_loss: 0.0061, label: 0, bag_size: 66\n",
      "batch 79, loss: 0.0008, instance_loss: 0.1711, weighted_loss: 0.0519, label: 1, bag_size: 18\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 113\n",
      "batch 119, loss: 4.6283, instance_loss: 1.2495, weighted_loss: 3.6147, label: 1, bag_size: 62\n",
      "batch 139, loss: 0.0000, instance_loss: 0.3178, weighted_loss: 0.0953, label: 1, bag_size: 89\n",
      "batch 159, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 87\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0000, label: 0, bag_size: 33\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 59\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 90\n",
      "batch 239, loss: 0.0003, instance_loss: 0.0005, weighted_loss: 0.0003, label: 1, bag_size: 81\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 86\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 31\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 81\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 61\n",
      "batch 339, loss: 0.5520, instance_loss: 0.0279, weighted_loss: 0.3948, label: 0, bag_size: 71\n",
      "batch 359, loss: 0.0000, instance_loss: 0.1241, weighted_loss: 0.0372, label: 0, bag_size: 104\n",
      "batch 379, loss: 0.0000, instance_loss: 0.1313, weighted_loss: 0.0394, label: 0, bag_size: 38\n",
      "batch 399, loss: 0.0194, instance_loss: 1.4924, weighted_loss: 0.4613, label: 0, bag_size: 20\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0023, weighted_loss: 0.0007, label: 0, bag_size: 102\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0026, weighted_loss: 0.0008, label: 1, bag_size: 32\n",
      "batch 459, loss: 0.0002, instance_loss: 0.0035, weighted_loss: 0.0012, label: 0, bag_size: 45\n",
      "batch 479, loss: 0.1606, instance_loss: 0.0485, weighted_loss: 0.1270, label: 1, bag_size: 65\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 29\n",
      "batch 519, loss: 0.1042, instance_loss: 0.0459, weighted_loss: 0.0867, label: 0, bag_size: 74\n",
      "batch 539, loss: 0.0017, instance_loss: 0.4737, weighted_loss: 0.1433, label: 0, bag_size: 33\n",
      "batch 559, loss: 0.0000, instance_loss: 2.0742, weighted_loss: 0.6222, label: 1, bag_size: 60\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0003, label: 1, bag_size: 80\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 1, bag_size: 52\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 0, bag_size: 71\n",
      "batch 639, loss: 2.9409, instance_loss: 0.9972, weighted_loss: 2.3578, label: 1, bag_size: 84\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9736538461538462: correct 10126/10400\n",
      "class 1 clustering acc 0.8828846153846154: correct 4591/5200\n",
      "Epoch: 53, train_loss: 0.5230, train_clustering_loss:  0.2419, train_error: 0.0938\n",
      "class 0: acc 0.9015873015873016, correct 284/315\n",
      "class 1: acc 0.9104477611940298, correct 305/335\n",
      "\n",
      "Val Set, val_loss: 1.9939, val_error: 0.2581, auc: 0.9685\n",
      "class 0 clustering acc 0.9247311827956989: correct 1376/1488\n",
      "class 1 clustering acc 0.7217741935483871: correct 537/744\n",
      "class 0: acc 0.48936170212765956, correct 23/47\n",
      "class 1: acc 1.0, correct 46/46\n",
      "EarlyStopping counter: 15 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0003, instance_loss: 0.0089, weighted_loss: 0.0029, label: 0, bag_size: 52\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 110\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 71\n",
      "batch 79, loss: 7.4419, instance_loss: 1.6055, weighted_loss: 5.6910, label: 0, bag_size: 46\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 1, bag_size: 99\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 41\n",
      "batch 139, loss: 0.0000, instance_loss: 0.1544, weighted_loss: 0.0463, label: 0, bag_size: 28\n",
      "batch 159, loss: 5.3543, instance_loss: 1.1564, weighted_loss: 4.0949, label: 1, bag_size: 83\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 87\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 53\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0746, weighted_loss: 0.0225, label: 0, bag_size: 54\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 96\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 1, bag_size: 21\n",
      "batch 279, loss: 0.0049, instance_loss: 0.4849, weighted_loss: 0.1489, label: 1, bag_size: 67\n",
      "batch 299, loss: 0.0023, instance_loss: 0.0277, weighted_loss: 0.0099, label: 1, bag_size: 72\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0028, weighted_loss: 0.0008, label: 0, bag_size: 85\n",
      "batch 339, loss: 0.0000, instance_loss: 0.4508, weighted_loss: 0.1352, label: 0, bag_size: 78\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0356, weighted_loss: 0.0107, label: 0, bag_size: 89\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0025, weighted_loss: 0.0008, label: 0, bag_size: 110\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0661, weighted_loss: 0.0198, label: 1, bag_size: 24\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0039, weighted_loss: 0.0012, label: 1, bag_size: 16\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0074, weighted_loss: 0.0022, label: 0, bag_size: 35\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0177, weighted_loss: 0.0053, label: 1, bag_size: 91\n",
      "batch 479, loss: 3.4091, instance_loss: 1.0339, weighted_loss: 2.6965, label: 1, bag_size: 60\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 0, bag_size: 87\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 76\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 23\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0031, weighted_loss: 0.0009, label: 0, bag_size: 31\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0028, weighted_loss: 0.0008, label: 1, bag_size: 67\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 1, bag_size: 94\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 82\n",
      "batch 639, loss: 5.1042, instance_loss: 0.6138, weighted_loss: 3.7571, label: 0, bag_size: 18\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9771153846153846: correct 10162/10400\n",
      "class 1 clustering acc 0.884423076923077: correct 4599/5200\n",
      "Epoch: 54, train_loss: 0.5637, train_clustering_loss:  0.2211, train_error: 0.0862\n",
      "class 0: acc 0.9122257053291536, correct 291/319\n",
      "class 1: acc 0.9154078549848943, correct 303/331\n",
      "\n",
      "Val Set, val_loss: 0.6774, val_error: 0.1075, auc: 0.9487\n",
      "class 0 clustering acc 0.9603494623655914: correct 1429/1488\n",
      "class 1 clustering acc 0.803763440860215: correct 598/744\n",
      "class 0: acc 0.9361702127659575, correct 44/47\n",
      "class 1: acc 0.8478260869565217, correct 39/46\n",
      "EarlyStopping counter: 16 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 85\n",
      "batch 39, loss: 0.0390, instance_loss: 0.5297, weighted_loss: 0.1862, label: 1, bag_size: 50\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0272, weighted_loss: 0.0082, label: 1, bag_size: 89\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 29\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 42\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 86\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 72\n",
      "batch 159, loss: 0.0006, instance_loss: 0.0738, weighted_loss: 0.0226, label: 1, bag_size: 44\n",
      "batch 179, loss: 0.0010, instance_loss: 0.0158, weighted_loss: 0.0055, label: 0, bag_size: 88\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 120\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 33\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 114\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0073, weighted_loss: 0.0022, label: 0, bag_size: 31\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0089, weighted_loss: 0.0027, label: 1, bag_size: 84\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 89\n",
      "batch 319, loss: 0.0082, instance_loss: 0.2420, weighted_loss: 0.0783, label: 1, bag_size: 59\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 71\n",
      "batch 359, loss: 1.0833, instance_loss: 0.7914, weighted_loss: 0.9958, label: 0, bag_size: 25\n",
      "batch 379, loss: 0.7471, instance_loss: 0.2913, weighted_loss: 0.6103, label: 1, bag_size: 60\n",
      "batch 399, loss: 0.0003, instance_loss: 0.0135, weighted_loss: 0.0042, label: 0, bag_size: 27\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 72\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 45\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 51\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0001, label: 1, bag_size: 26\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 53\n",
      "batch 519, loss: 1.7401, instance_loss: 1.2497, weighted_loss: 1.5930, label: 0, bag_size: 93\n",
      "batch 539, loss: 0.0000, instance_loss: 0.1749, weighted_loss: 0.0525, label: 0, bag_size: 72\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0210, weighted_loss: 0.0063, label: 0, bag_size: 118\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 23\n",
      "batch 599, loss: 0.0096, instance_loss: 0.1853, weighted_loss: 0.0623, label: 1, bag_size: 83\n",
      "batch 619, loss: 1.5521, instance_loss: 1.5051, weighted_loss: 1.5380, label: 0, bag_size: 44\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 96\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9857692307692307: correct 10252/10400\n",
      "class 1 clustering acc 0.9263461538461538: correct 4817/5200\n",
      "Epoch: 55, train_loss: 0.1944, train_clustering_loss:  0.1374, train_error: 0.0492\n",
      "class 0: acc 0.946843853820598, correct 285/301\n",
      "class 1: acc 0.9541547277936963, correct 333/349\n",
      "\n",
      "Val Set, val_loss: 0.4456, val_error: 0.1720, auc: 0.9551\n",
      "class 0 clustering acc 0.9704301075268817: correct 1444/1488\n",
      "class 1 clustering acc 0.8239247311827957: correct 613/744\n",
      "class 0: acc 0.851063829787234, correct 40/47\n",
      "class 1: acc 0.8043478260869565, correct 37/46\n",
      "EarlyStopping counter: 17 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 87\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 59\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 35\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 62\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 74\n",
      "batch 119, loss: 0.0170, instance_loss: 1.0435, weighted_loss: 0.3249, label: 1, bag_size: 69\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 72\n",
      "batch 159, loss: 0.0002, instance_loss: 0.0335, weighted_loss: 0.0102, label: 1, bag_size: 53\n",
      "batch 179, loss: 14.0198, instance_loss: 6.6956, weighted_loss: 11.8226, label: 0, bag_size: 20\n",
      "batch 199, loss: 0.0037, instance_loss: 0.0047, weighted_loss: 0.0040, label: 1, bag_size: 17\n",
      "batch 219, loss: 0.0000, instance_loss: 0.6055, weighted_loss: 0.1817, label: 0, bag_size: 50\n",
      "batch 239, loss: 0.0000, instance_loss: 0.5286, weighted_loss: 0.1586, label: 1, bag_size: 53\n",
      "batch 259, loss: 0.7599, instance_loss: 1.3785, weighted_loss: 0.9455, label: 1, bag_size: 82\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 31\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 0, bag_size: 46\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0025, weighted_loss: 0.0007, label: 1, bag_size: 39\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0017, weighted_loss: 0.0005, label: 1, bag_size: 69\n",
      "batch 359, loss: 0.0000, instance_loss: 0.6746, weighted_loss: 0.2024, label: 0, bag_size: 83\n",
      "batch 379, loss: 0.0000, instance_loss: 0.1904, weighted_loss: 0.0571, label: 0, bag_size: 72\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0464, weighted_loss: 0.0139, label: 0, bag_size: 88\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0364, weighted_loss: 0.0109, label: 1, bag_size: 71\n",
      "batch 439, loss: 0.0000, instance_loss: 0.2241, weighted_loss: 0.0672, label: 0, bag_size: 43\n",
      "batch 459, loss: 0.0000, instance_loss: 1.0204, weighted_loss: 0.3061, label: 0, bag_size: 27\n",
      "batch 479, loss: 0.0019, instance_loss: 0.0822, weighted_loss: 0.0260, label: 0, bag_size: 54\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0519, weighted_loss: 0.0156, label: 0, bag_size: 79\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 128\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 37\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0656, weighted_loss: 0.0197, label: 1, bag_size: 76\n",
      "batch 579, loss: 0.0282, instance_loss: 0.0022, weighted_loss: 0.0204, label: 1, bag_size: 127\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0066, weighted_loss: 0.0020, label: 0, bag_size: 66\n",
      "batch 619, loss: 0.2670, instance_loss: 0.0922, weighted_loss: 0.2145, label: 0, bag_size: 29\n",
      "batch 639, loss: 0.0037, instance_loss: 0.0000, weighted_loss: 0.0026, label: 0, bag_size: 31\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9569230769230769: correct 9952/10400\n",
      "class 1 clustering acc 0.8451923076923077: correct 4395/5200\n",
      "Epoch: 56, train_loss: 0.8477, train_clustering_loss:  0.3580, train_error: 0.1015\n",
      "class 0: acc 0.9028213166144201, correct 288/319\n",
      "class 1: acc 0.8942598187311178, correct 296/331\n",
      "\n",
      "Val Set, val_loss: 0.6413, val_error: 0.0968, auc: 0.9598\n",
      "class 0 clustering acc 0.9583333333333334: correct 1426/1488\n",
      "class 1 clustering acc 0.8279569892473119: correct 616/744\n",
      "class 0: acc 0.9574468085106383, correct 45/47\n",
      "class 1: acc 0.8478260869565217, correct 39/46\n",
      "EarlyStopping counter: 18 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 75\n",
      "batch 39, loss: 0.0000, instance_loss: 0.2269, weighted_loss: 0.0681, label: 1, bag_size: 21\n",
      "batch 59, loss: 8.4597, instance_loss: 0.4347, weighted_loss: 6.0522, label: 1, bag_size: 51\n",
      "batch 79, loss: 6.2652, instance_loss: 0.4947, weighted_loss: 4.5340, label: 1, bag_size: 44\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 0, bag_size: 69\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0027, weighted_loss: 0.0008, label: 0, bag_size: 115\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 35\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 50\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 77\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0269, weighted_loss: 0.0081, label: 0, bag_size: 49\n",
      "batch 219, loss: 0.0005, instance_loss: 0.0105, weighted_loss: 0.0035, label: 1, bag_size: 85\n",
      "batch 239, loss: 0.7894, instance_loss: 0.3425, weighted_loss: 0.6554, label: 0, bag_size: 35\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 107\n",
      "batch 279, loss: 0.0003, instance_loss: 0.0132, weighted_loss: 0.0042, label: 1, bag_size: 46\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 67\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 1, bag_size: 17\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0239, weighted_loss: 0.0072, label: 1, bag_size: 80\n",
      "batch 359, loss: 3.1319, instance_loss: 0.4763, weighted_loss: 2.3352, label: 1, bag_size: 31\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 89\n",
      "batch 399, loss: 0.0010, instance_loss: 0.0720, weighted_loss: 0.0223, label: 1, bag_size: 92\n",
      "batch 419, loss: 0.0005, instance_loss: 0.0260, weighted_loss: 0.0081, label: 0, bag_size: 43\n",
      "batch 439, loss: 0.0014, instance_loss: 0.1324, weighted_loss: 0.0407, label: 0, bag_size: 19\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 38\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 92\n",
      "batch 499, loss: 0.0271, instance_loss: 0.8227, weighted_loss: 0.2658, label: 0, bag_size: 18\n",
      "batch 519, loss: 0.0085, instance_loss: 0.0025, weighted_loss: 0.0067, label: 1, bag_size: 82\n",
      "batch 539, loss: 0.0486, instance_loss: 0.0173, weighted_loss: 0.0392, label: 1, bag_size: 79\n",
      "batch 559, loss: 0.2550, instance_loss: 0.1239, weighted_loss: 0.2157, label: 0, bag_size: 24\n",
      "batch 579, loss: 0.2473, instance_loss: 1.0229, weighted_loss: 0.4800, label: 1, bag_size: 53\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 27\n",
      "batch 619, loss: 0.0021, instance_loss: 0.0064, weighted_loss: 0.0034, label: 0, bag_size: 49\n",
      "batch 639, loss: 0.0067, instance_loss: 0.0100, weighted_loss: 0.0077, label: 1, bag_size: 51\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9741346153846154: correct 10131/10400\n",
      "class 1 clustering acc 0.8834615384615384: correct 4594/5200\n",
      "Epoch: 57, train_loss: 0.5751, train_clustering_loss:  0.2554, train_error: 0.0985\n",
      "class 0: acc 0.9015384615384615, correct 293/325\n",
      "class 1: acc 0.9015384615384615, correct 293/325\n",
      "\n",
      "Val Set, val_loss: 0.4241, val_error: 0.1183, auc: 0.9519\n",
      "class 0 clustering acc 0.956989247311828: correct 1424/1488\n",
      "class 1 clustering acc 0.875: correct 651/744\n",
      "class 0: acc 0.8936170212765957, correct 42/47\n",
      "class 1: acc 0.8695652173913043, correct 40/46\n",
      "EarlyStopping counter: 19 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 36\n",
      "batch 39, loss: 0.0066, instance_loss: 0.0000, weighted_loss: 0.0046, label: 1, bag_size: 101\n",
      "batch 59, loss: 0.1073, instance_loss: 0.1743, weighted_loss: 0.1274, label: 0, bag_size: 81\n",
      "batch 79, loss: 0.0019, instance_loss: 0.0028, weighted_loss: 0.0021, label: 1, bag_size: 85\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 60\n",
      "batch 119, loss: 0.0024, instance_loss: 0.0088, weighted_loss: 0.0044, label: 0, bag_size: 24\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 97\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 87\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 104\n",
      "batch 199, loss: 0.0747, instance_loss: 0.1277, weighted_loss: 0.0906, label: 1, bag_size: 29\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 114\n",
      "batch 239, loss: 0.9427, instance_loss: 0.3491, weighted_loss: 0.7646, label: 0, bag_size: 20\n",
      "batch 259, loss: 0.3073, instance_loss: 0.9732, weighted_loss: 0.5071, label: 1, bag_size: 67\n",
      "batch 279, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 74\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 71\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 25\n",
      "batch 339, loss: 0.0088, instance_loss: 0.0138, weighted_loss: 0.0103, label: 0, bag_size: 88\n",
      "batch 359, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 85\n",
      "batch 379, loss: 0.0030, instance_loss: 0.5790, weighted_loss: 0.1758, label: 1, bag_size: 94\n",
      "batch 399, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 31\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0031, weighted_loss: 0.0009, label: 1, bag_size: 112\n",
      "batch 439, loss: 1.0310, instance_loss: 1.5033, weighted_loss: 1.1726, label: 1, bag_size: 18\n",
      "batch 459, loss: 0.0000, instance_loss: 0.1092, weighted_loss: 0.0328, label: 1, bag_size: 56\n",
      "batch 479, loss: 0.0000, instance_loss: 0.6672, weighted_loss: 0.2002, label: 0, bag_size: 85\n",
      "batch 499, loss: 1.7427, instance_loss: 0.1840, weighted_loss: 1.2751, label: 0, bag_size: 75\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 41\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0405, weighted_loss: 0.0121, label: 0, bag_size: 108\n",
      "batch 559, loss: 0.0000, instance_loss: 0.2522, weighted_loss: 0.0757, label: 1, bag_size: 100\n",
      "batch 579, loss: 0.0151, instance_loss: 0.0595, weighted_loss: 0.0284, label: 1, bag_size: 77\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 0, bag_size: 26\n",
      "batch 619, loss: 0.0010, instance_loss: 0.0326, weighted_loss: 0.0105, label: 1, bag_size: 92\n",
      "batch 639, loss: 0.0001, instance_loss: 0.2150, weighted_loss: 0.0645, label: 1, bag_size: 39\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9778846153846154: correct 10170/10400\n",
      "class 1 clustering acc 0.905: correct 4706/5200\n",
      "Epoch: 58, train_loss: 0.2243, train_clustering_loss:  0.1801, train_error: 0.0723\n",
      "class 0: acc 0.9185667752442996, correct 282/307\n",
      "class 1: acc 0.9358600583090378, correct 321/343\n",
      "\n",
      "Val Set, val_loss: 0.4673, val_error: 0.0968, auc: 0.9695\n",
      "class 0 clustering acc 0.959005376344086: correct 1427/1488\n",
      "class 1 clustering acc 0.8655913978494624: correct 644/744\n",
      "class 0: acc 0.9574468085106383, correct 45/47\n",
      "class 1: acc 0.8478260869565217, correct 39/46\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Val error: 0.0538, ROC AUC: 0.9898\n",
      "Test error: 0.1176, ROC AUC: 0.9612\n",
      "class 0: acc 0.8181818181818182, correct 36/44\n",
      "class 1: acc 0.9512195121951219, correct 39/41\n",
      "finished!\n",
      "end script\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python main_myself.py --drop_out --early_stopping --lr 2e-4 --max_lr 2e-3 --k 5 --label_frac 1 --opt adam\\\n",
    "--exp_code tcga_nsclc_sub_100_level13_MCBAT_d1f2l3 --weighted_sample --bag_loss ce --inst_loss svm \\\n",
    "--task task_2_tumor_subtyping --model_type mcbat_sb --log_data --data_low_dir /home/sci/Disk_data/Datasets/TCGA-NSCLC-sub/FEATURES_level3 \\\n",
    "--data_high_dir /home/sci/Disk_data/Datasets/TCGA-NSCLC-sub/FEATURES_level1 \\\n",
    "--split_dir /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/tcga_nsclc_100 --subtyping \\\n",
    "--csv_path dataset_csv/tcga_lung_subtyping_sub.csv "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA_VISIBLE_DEVICES=0 python eval.py \\\n",
    "--drop_out \\\n",
    "--k 10 \\\n",
    "--models_exp_code task_1_tumor_vs_normal_CLAM_50_s1 \\\n",
    "--save_exp_code task_1_tumor_vs_normal_CLAM_50_s1_cv \\\n",
    "--task task_1_tumor_vs_normal \\\n",
    "--model_type clam_sb \\\n",
    "--results_dir results \\\n",
    "--data_root_dir /media/yuansh/14THHD/CLAM/FEATURES_DIRECTORY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': 'task_2_tumor_subtyping', 'split': 'test', 'save_dir': './eval_results/EVAL_tcga_nsclc_100_mil_fornlst100', 'models_dir': 'results/tcga_nsclc_sub_100_level1_mil_s1', 'model_type': 'mil', 'drop_out': True, 'model_size': 'small'}\n",
      "label column: label\n",
      "label dictionary: {'LUAD': 0, 'LUSC': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 0    552\n",
      "1    324\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 194\n",
      "Slide-LVL; Number of samples registered in class 0: 552\n",
      "Patient-LVL; Number of samples registered in class 1: 117\n",
      "Slide-LVL; Number of samples registered in class 1: 324\n",
      "Init Model\n",
      "MIL_fc(\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 525826\n",
      "Total number of trainable parameters: 525826\n",
      "Init Loaders\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sci/PycharmProjects/chaofan/projects/CLAM/eval.py\", line 135, in <module>\n",
      "    model, patient_results, test_error, auc, df  = eval(split_dataset, args, ckpt_paths[ckpt_idx])\n",
      "  File \"/home/sci/PycharmProjects/chaofan/projects/CLAM/utils/eval_utils.py\", line 56, in eval\n",
      "    patient_results, test_error, auc, df, _ = summary(model, loader, args)\n",
      "  File \"/home/sci/PycharmProjects/chaofan/projects/CLAM/utils/eval_utils.py\", line 73, in summary\n",
      "    for batch_idx, (data, label) in enumerate(loader):\n",
      "  File \"/home/sci/anaconda3/envs/lcf/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 633, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/sci/anaconda3/envs/lcf/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1345, in _next_data\n",
      "    return self._process_data(data)\n",
      "  File \"/home/sci/anaconda3/envs/lcf/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1371, in _process_data\n",
      "    data.reraise()\n",
      "  File \"/home/sci/anaconda3/envs/lcf/lib/python3.9/site-packages/torch/_utils.py\", line 644, in reraise\n",
      "    raise exception\n",
      "TypeError: Caught TypeError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/sci/anaconda3/envs/lcf/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"/home/sci/anaconda3/envs/lcf/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/home/sci/PycharmProjects/chaofan/projects/CLAM/utils/utils.py\", line 39, in collate_MIL\n",
      "    label = torch.LongTensor([np.array(item[1]) for item in batch])\n",
      "TypeError: len() of unsized object\n",
      "\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python eval.py --drop_out --k 5 --models_exp_code tcga_nsclc_sub_100_level1_mil_s1 \\\n",
    "    --save_exp_code tcga_nsclc_100_mil_fornlst100 \\\n",
    "    --task task_2_tumor_subtyping --model_type mil --results_dir results \\\n",
    "    --data_root_dir /home/sci/Disk_data/Datasets/NLST/FEATURES_level1\\\n",
    "    --csv_path dataset_csv/NLST_offical.csv\\\n",
    "    --splits_dir /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/NLST_100/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': 'task_2_tumor_subtyping', 'split': 'test', 'save_dir': './eval_results/EVAL_tcga_nsclc_100_mcbat_sb', 'models_dir': 'results/tcga_nsclc_sub_100_level13_mcbat_sb_depth1_s1', 'model_type': 'mcbat_sb', 'drop_out': True, 'model_size': 'small'}\n",
      "label column: label\n",
      "label dictionary: {'LUAD': 0, 'LUSC': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 0    394\n",
      "1    434\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 333\n",
      "Slide-LVL; Number of samples registered in class 0: 394\n",
      "Patient-LVL; Number of samples registered in class 1: 400\n",
      "Slide-LVL; Number of samples registered in class 1: 434\n",
      "Init Model\n",
      "MCBAT_SB(\n",
      "  (instance_loss_fn): CrossEntropyLoss()\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (transformer_low): TransformerEncoder_FLASH(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FLASH(\n",
      "            (attn_fn): LaplacianAttnFn()\n",
      "            (rel_pos_bias): T5RelativePositionBias(\n",
      "              (relative_attention_bias): Embedding(32, 1)\n",
      "            )\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (to_hidden): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (to_qk): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (qk_offset_scale): OffsetScale()\n",
      "            (to_out): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer_high): TransformerEncoder_FLASH(\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FLASH(\n",
      "            (attn_fn): LaplacianAttnFn()\n",
      "            (rel_pos_bias): T5RelativePositionBias(\n",
      "              (relative_attention_bias): Embedding(32, 1)\n",
      "            )\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (to_hidden): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (to_qk): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (qk_offset_scale): OffsetScale()\n",
      "            (to_out): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attention): Attn_Net_Gated(\n",
      "    (attention_a): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_b): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (fusion_encoder): FusionEncoder()\n",
      "  (attention_V2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (attention_U2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (attention_weights2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Total number of parameters: 9596235\n",
      "Total number of trainable parameters: 9596235\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sci/PycharmProjects/chaofan/projects/CLAM/eval_myself.py\", line 139, in <module>\n",
      "    model, patient_results, test_error, auc, df  = eval(split_dataset, args, ckpt_paths[ckpt_idx])\n",
      "  File \"/home/sci/PycharmProjects/chaofan/projects/CLAM/utils/eval_utils_myself.py\", line 55, in eval\n",
      "    model = initiate_model(args, ckpt_path)\n",
      "  File \"/home/sci/PycharmProjects/chaofan/projects/CLAM/utils/eval_utils_myself.py\", line 48, in initiate_model\n",
      "    model.load_state_dict(ckpt_clean, strict=True)\n",
      "  File \"/home/sci/anaconda3/envs/lcf/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 2041, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for MCBAT_SB:\n",
      "\tMissing key(s) in state_dict: \"transformer_low.layers.0.0.fn.rel_pos_bias.relative_attention_bias.weight\", \"transformer_low.layers.0.0.fn.norm.weight\", \"transformer_low.layers.0.0.fn.norm.bias\", \"transformer_low.layers.0.0.fn.to_hidden.0.weight\", \"transformer_low.layers.0.0.fn.to_hidden.0.bias\", \"transformer_low.layers.0.0.fn.to_qk.0.weight\", \"transformer_low.layers.0.0.fn.to_qk.0.bias\", \"transformer_low.layers.0.0.fn.qk_offset_scale.gamma\", \"transformer_low.layers.0.0.fn.qk_offset_scale.beta\", \"transformer_high.layers.0.0.fn.rel_pos_bias.relative_attention_bias.weight\", \"transformer_high.layers.0.0.fn.norm.weight\", \"transformer_high.layers.0.0.fn.norm.bias\", \"transformer_high.layers.0.0.fn.to_hidden.0.weight\", \"transformer_high.layers.0.0.fn.to_hidden.0.bias\", \"transformer_high.layers.0.0.fn.to_qk.0.weight\", \"transformer_high.layers.0.0.fn.to_qk.0.bias\", \"transformer_high.layers.0.0.fn.qk_offset_scale.gamma\", \"transformer_high.layers.0.0.fn.qk_offset_scale.beta\". \n",
      "\tUnexpected key(s) in state_dict: \"transformer_low.layers.0.0.fn.fast_attention.projection_matrix\", \"transformer_low.layers.0.0.fn.to_q.weight\", \"transformer_low.layers.0.0.fn.to_k.weight\", \"transformer_low.layers.0.0.fn.to_v.weight\", \"transformer_high.layers.0.0.fn.fast_attention.projection_matrix\", \"transformer_high.layers.0.0.fn.to_q.weight\", \"transformer_high.layers.0.0.fn.to_k.weight\", \"transformer_high.layers.0.0.fn.to_v.weight\". \n",
      "\tsize mismatch for transformer_low.layers.0.0.fn.to_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([512, 1024]).\n",
      "\tsize mismatch for transformer_high.layers.0.0.fn.to_out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([512, 1024]).\n"
     ]
    }
   ],
   "source": [
    "!python eval_myself.py --drop_out --k 5 --models_exp_code tcga_nsclc_sub_100_level13_mcbat_sb_depth1_s1 \\\n",
    "    --save_exp_code tcga_nsclc_100_mcbat_sb \\\n",
    "    --task task_2_tumor_subtyping --model_type mcbat_sb --results_dir results \\\n",
    "    --data_low_dir /home/sci/Disk_data/Datasets/TCGA-NSCLC-sub/FEATURES_level3\\\n",
    "    --data_high_dir /home/sci/Disk_data/Datasets/TCGA-NSCLC-sub/FEATURES_level1\\\n",
    "    --csv_path dataset_csv/tcga_lung_subtyping_sub.csv\\\n",
    "    --splits_dir /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/tcga_nsclc_100/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_heatmaps.py --config config_template.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('lcf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dddf4cc44ccb2cf796823d3e6277e76da85a74a2d70ce7f32ed1ef0f62e61e3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
