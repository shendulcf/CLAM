{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## csv_file process\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df_csv = pd.read_csv('dataset_csv/cptac_lung_subtyping.csv')\n",
    "dataset_dir = '/home/sci/Disk2/CPTAC-LUNG/WSI'\n",
    "index = df_csv.index\n",
    "rows_to_delete = []\n",
    "for name in os.listdir(dataset_dir):\n",
    "    slide_id = os.path.splitext(name)[0]\n",
    "\n",
    "    row_index = df_csv[df_csv['slide_id'] == slide_id].index\n",
    "    rows_to_delete.append(row_index.item())\n",
    "    \n",
    "remaining_index = index.difference(rows_to_delete)\n",
    "df_delete = df_csv.drop(remaining_index).reset_index(drop=True)\n",
    "df_delete\n",
    "# df = df_csv.drop(~rows_to_delete)\n",
    "\n",
    "df_delete.to_csv('dataset_csv/cptac_lung_subtyping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已经删除指定列\n"
     ]
    }
   ],
   "source": [
    "# delete columns\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset_csv/cptac_lung_subtyping.csv')\n",
    "if 'specimen_type' in df.columns:\n",
    "    df.drop('specimen_type', axis=1, inplace=True)\n",
    "\n",
    "df.to_csv('dataset_csv/cptac_lung_subtyping.csv', index=True)\n",
    "\n",
    "print('已经删除指定列')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, 2.0001000528019754, 4.00060037688892, 8.002010593284831, 16.007188719102707, 32.04003318547134)\n",
      "OpenSlide('/home/sci/Disk2/CPTAC-LUNG/WSI/C3L-03642-21.svs')\n",
      "(1.0, 2.0000446548182547, 4.0002679528403, 8.001458840613687, 16.009471545761812, 32.045191308417884)\n",
      "OpenSlide('/home/sci/Disk2/CPTAC-LUNG/WSI/C3L-02513-21.svs')\n",
      "(1.0, 2.0000474383301707, 4.0000948766603415, 8.001227636039935, 16.009647386087043, 32.04810298959235)\n",
      "OpenSlide('/home/sci/Disk2/CPTAC-LUNG/WSI/C3L-02513-22.svs')\n",
      "(1.0, 2.00003452800221, 4.000373461003514, 8.000746922007028, 16.004155409255155, 32.01896771155954)\n",
      "OpenSlide('/home/sci/Disk2/CPTAC-LUNG/WSI/C3L-02515-22.svs')\n",
      "(1.0, 2.0, 4.000173190162799, 8.001280087141657, 16.005332417097122, 32.03672210196119)\n",
      "OpenSlide('/home/sci/Disk2/CPTAC-LUNG/WSI/C3L-02513-23.svs')\n",
      "(1.0, 2.0000335390394417, 4.000226950181202, 8.001093542150732, 16.006894025795802, 32.03263256916757)\n",
      "OpenSlide('/home/sci/Disk2/CPTAC-LUNG/WSI/C3L-02515-21.svs')\n",
      "(1.0, 2.0000776061058305, 4.000465675500808, 8.000931351001617, 16.006832188483017, 32.02634956668237, 64.1035607035607)\n",
      "OpenSlide('/home/sci/Disk2/CPTAC-LUNG/WSI/C3L-02515-23.svs')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WSI 属性查看\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import openslide\n",
    "import os\n",
    "import torch\n",
    "\n",
    "slide_path = r'/home/sci/Disk_data/Datasets/TCGA-NSCLC/WSI/TCGA-05-4244-01A-01-BS1.svs'\n",
    "path = r'/home/sci/Disk2/CPTAC-LUNG/WSI'\n",
    "\n",
    "# # ---->> for single slide\n",
    "# slide = openslide.OpenSlide(slide_path)\n",
    "# slide.dimensions\n",
    "# slide.associated_images\n",
    "# slide.level_dimensions\n",
    "# slide.level_count\n",
    "# slide.properties['aperio.AppMag']\n",
    "# slide.properties['aperio.MPP']\n",
    "\n",
    "# ---->> for slide folder\n",
    "slides = os.listdir(path)\n",
    "dimentions_list = []\n",
    "i = 0 \n",
    "for slide in slides:\n",
    "    slide = os.path.join(path,slide)\n",
    "    slide = openslide.OpenSlide(slide)\n",
    "    # print(slide.level_dimensions)\n",
    "    # print(slide.properties['aperio.AppMag'])\n",
    "    # print(slide.level_count)\n",
    "    # print(slide.properties)\n",
    "    if slide.level_count >4:\n",
    "        print(slide.level_downsamples)\n",
    "        print(slide)\n",
    "i\n",
    "\n",
    "    ## slide thumbnails\n",
    "    # slide = slide.get_thumbnail((slide.level_dimensions[-1]))\n",
    "    # slide.save(r'/home/sci/Disk2/tcga_crc/DATA_DIRECTORY/test')\n",
    "    # print('save_success')\n",
    "    # dimentions_list.append(slide.level_dimensions[-1])\n",
    "# dimentions_list=torch.tensor(dimentions_list)\n",
    "# index = torch.topk(dimentions_list, 1, dim=1)\n",
    "# maxMap = torch.index_select(dimentions_list, index=index, dim=0 )\n",
    "# maxMap\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"coords\": shape (2989, 2), type \"<i4\">\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2989, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "path = r'E:\\Workspace\\Project\\CLAM\\data\\RESULTS_DIRECTORY\\patches\\TCGA-A6-2671-01A-01-BS1.h5'\n",
    "f = h5py.File(path,'r')\n",
    "f.keys()\n",
    "data = f.get('coords')\n",
    "print(data)\n",
    "data.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step_1 get patch\n",
    "\n",
    "source svs 文件地址\\\n",
    "save_dir 结果文件\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python create_patches_fp.py \\\n",
    "--source 'svs_dir' \\\n",
    "--save_dir 'result_dir' \\\n",
    "--patch_size 256 \\\n",
    "--seg \\ # 分割=True\n",
    "--patch \\ # 切分patch=True\n",
    "--stitch \\ # \n",
    "process_list_autogen.csv # 这里记录了每张 image 的处理的参数\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:  /home/sci/Disk2/CPTAC-LUNG/WSI\n",
      "patch_save_dir:  /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "mask_save_dir:  /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/masks\n",
      "stitch_save_dir:  /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/stitches\n",
      "thumbnail_save_dir:  /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/thumbnails\n",
      "source : /home/sci/Disk2/CPTAC-LUNG/WSI\n",
      "save_dir : /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2\n",
      "patch_save_dir : /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "mask_save_dir : /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/masks\n",
      "stitch_save_dir : /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/stitches\n",
      "thumbnail_save_dir : /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/thumbnails\n",
      "{'seg_params': {'seg_level': -1, 'sthresh': 8, 'mthresh': 7, 'close': 4, 'use_otsu': False, 'keep_ids': 'none', 'exclude_ids': 'none'}, 'filter_params': {'a_t': 100, 'a_h': 16, 'max_n_holes': 8}, 'patch_params': {'use_padding': True, 'contour_fn': 'four_pt'}, 'vis_params': {'vis_level': -1, 'line_thickness': 250}}\n",
      "\n",
      "\n",
      "progress: 0.00, 0/1027\n",
      "processing C3L-00001-21.svs\n",
      "C3L-00001-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.00, 1/1027\n",
      "processing C3L-00009-21.svs\n",
      "C3L-00009-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.00, 2/1027\n",
      "processing C3L-00080-21.svs\n",
      "C3L-00080-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.00, 3/1027\n",
      "processing C3L-00081-21.svs\n",
      "C3L-00081-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.00, 4/1027\n",
      "processing C3L-00083-21.svs\n",
      "C3L-00083-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.00, 5/1027\n",
      "processing C3L-00093-21.svs\n",
      "C3L-00093-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.01, 6/1027\n",
      "processing C3L-00094-21.svs\n",
      "C3L-00094-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.01, 7/1027\n",
      "processing C3L-00095-21.svs\n",
      "C3L-00095-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.01, 8/1027\n",
      "processing C3L-00095-22.svs\n",
      "C3L-00095-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.01, 9/1027\n",
      "processing C3L-00095-23.svs\n",
      "C3L-00095-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.01, 10/1027\n",
      "processing C3L-00140-21.svs\n",
      "C3L-00140-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.01, 11/1027\n",
      "processing C3L-00140-22.svs\n",
      "C3L-00140-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.01, 12/1027\n",
      "processing C3L-00144-21.svs\n",
      "C3L-00144-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.01, 13/1027\n",
      "processing C3L-00263-21.svs\n",
      "C3L-00263-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.01, 14/1027\n",
      "processing C3L-00263-22.svs\n",
      "C3L-00263-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.01, 15/1027\n",
      "processing C3L-00263-23.svs\n",
      "C3L-00263-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.02, 16/1027\n",
      "processing C3L-00263-24.svs\n",
      "C3L-00263-24 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.02, 17/1027\n",
      "processing C3L-00263-25.svs\n",
      "C3L-00263-25 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.02, 18/1027\n",
      "processing C3L-00279-21.svs\n",
      "C3L-00279-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.02, 19/1027\n",
      "processing C3L-00279-22.svs\n",
      "C3L-00279-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.02, 20/1027\n",
      "processing C3L-00279-23.svs\n",
      "C3L-00279-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.02, 21/1027\n",
      "processing C3L-00368-21.svs\n",
      "C3L-00368-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.02, 22/1027\n",
      "processing C3L-00368-22.svs\n",
      "C3L-00368-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.02, 23/1027\n",
      "processing C3L-00412-21.svs\n",
      "C3L-00412-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.02, 24/1027\n",
      "processing C3L-00415-21.svs\n",
      "C3L-00415-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.02, 25/1027\n",
      "processing C3L-00415-23.svs\n",
      "C3L-00415-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.03, 26/1027\n",
      "processing C3L-00422-21.svs\n",
      "C3L-00422-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.03, 27/1027\n",
      "processing C3L-00422-22.svs\n",
      "C3L-00422-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.03, 28/1027\n",
      "processing C3L-00422-23.svs\n",
      "C3L-00422-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.03, 29/1027\n",
      "processing C3L-00444-21.svs\n",
      "C3L-00444-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.03, 30/1027\n",
      "processing C3L-00444-22.svs\n",
      "C3L-00444-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.03, 31/1027\n",
      "processing C3L-00444-23.svs\n",
      "C3L-00444-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.03, 32/1027\n",
      "processing C3L-00445-21.svs\n",
      "C3L-00445-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.03, 33/1027\n",
      "processing C3L-00445-25.svs\n",
      "C3L-00445-25 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.03, 34/1027\n",
      "processing C3L-00445-26.svs\n",
      "C3L-00445-26 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.03, 35/1027\n",
      "processing C3L-00446-21.svs\n",
      "C3L-00446-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.04, 36/1027\n",
      "processing C3L-00446-22.svs\n",
      "C3L-00446-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.04, 37/1027\n",
      "processing C3L-00446-23.svs\n",
      "C3L-00446-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.04, 38/1027\n",
      "processing C3L-00446-24.svs\n",
      "C3L-00446-24 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.04, 39/1027\n",
      "processing C3L-00503-21.svs\n",
      "C3L-00503-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.04, 40/1027\n",
      "processing C3L-00503-22.svs\n",
      "C3L-00503-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.04, 41/1027\n",
      "processing C3L-00503-23.svs\n",
      "C3L-00503-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.04, 42/1027\n",
      "processing C3L-00510-21.svs\n",
      "C3L-00510-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.04, 43/1027\n",
      "processing C3L-00510-22.svs\n",
      "C3L-00510-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.04, 44/1027\n",
      "processing C3L-00510-23.svs\n",
      "C3L-00510-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.04, 45/1027\n",
      "processing C3L-00568-21.svs\n",
      "C3L-00568-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.04, 46/1027\n",
      "processing C3L-00568-22.svs\n",
      "C3L-00568-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.05, 47/1027\n",
      "processing C3L-00568-23.svs\n",
      "C3L-00568-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.05, 48/1027\n",
      "processing C3L-00603-21.svs\n",
      "C3L-00603-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.05, 49/1027\n",
      "processing C3L-00603-22.svs\n",
      "C3L-00603-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.05, 50/1027\n",
      "processing C3L-00604-21.svs\n",
      "C3L-00604-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.05, 51/1027\n",
      "processing C3L-00604-22.svs\n",
      "C3L-00604-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.05, 52/1027\n",
      "processing C3L-00604-23.svs\n",
      "C3L-00604-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.05, 53/1027\n",
      "processing C3L-00893-21.svs\n",
      "C3L-00893-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.05, 54/1027\n",
      "processing C3L-00893-22.svs\n",
      "C3L-00893-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.05, 55/1027\n",
      "processing C3L-00893-23.svs\n",
      "C3L-00893-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.05, 56/1027\n",
      "processing C3L-00904-21.svs\n",
      "C3L-00904-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.06, 57/1027\n",
      "processing C3L-00904-22.svs\n",
      "C3L-00904-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.06, 58/1027\n",
      "processing C3L-00913-21.svs\n",
      "C3L-00913-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.06, 59/1027\n",
      "processing C3L-00913-22.svs\n",
      "C3L-00913-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.06, 60/1027\n",
      "processing C3L-00913-23.svs\n",
      "C3L-00913-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.06, 61/1027\n",
      "processing C3L-00923-23.svs\n",
      "C3L-00923-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.06, 62/1027\n",
      "processing C3L-00927-21.svs\n",
      "C3L-00927-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.06, 63/1027\n",
      "processing C3L-00927-22.svs\n",
      "C3L-00927-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.06, 64/1027\n",
      "processing C3L-00927-23.svs\n",
      "C3L-00927-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.06, 65/1027\n",
      "processing C3L-00965-21.svs\n",
      "C3L-00965-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.06, 66/1027\n",
      "processing C3L-00965-22.svs\n",
      "C3L-00965-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.07, 67/1027\n",
      "processing C3L-00965-23.svs\n",
      "C3L-00965-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.07, 68/1027\n",
      "processing C3L-00965-24.svs\n",
      "C3L-00965-24 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.07, 69/1027\n",
      "processing C3L-00973-21.svs\n",
      "C3L-00973-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.07, 70/1027\n",
      "processing C3L-00973-22.svs\n",
      "C3L-00973-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.07, 71/1027\n",
      "processing C3L-00973-23.svs\n",
      "C3L-00973-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.07, 72/1027\n",
      "processing C3L-00973-24.svs\n",
      "C3L-00973-24 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.07, 73/1027\n",
      "processing C3L-00973-25.svs\n",
      "C3L-00973-25 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.07, 74/1027\n",
      "processing C3L-00993-21.svs\n",
      "C3L-00993-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.07, 75/1027\n",
      "processing C3L-00993-22.svs\n",
      "C3L-00993-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.07, 76/1027\n",
      "processing C3L-00993-23.svs\n",
      "C3L-00993-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.07, 77/1027\n",
      "processing C3L-00993-24.svs\n",
      "C3L-00993-24 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.08, 78/1027\n",
      "processing C3L-01000-24.svs\n",
      "C3L-01000-24 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.08, 79/1027\n",
      "processing C3L-01000-25.svs\n",
      "C3L-01000-25 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.08, 80/1027\n",
      "processing C3L-01285-21.svs\n",
      "C3L-01285-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.08, 81/1027\n",
      "processing C3L-01285-22.svs\n",
      "C3L-01285-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.08, 82/1027\n",
      "processing C3L-01285-23.svs\n",
      "C3L-01285-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.08, 83/1027\n",
      "processing C3L-01330-21.svs\n",
      "C3L-01330-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.08, 84/1027\n",
      "processing C3L-01330-22.svs\n",
      "C3L-01330-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.08, 85/1027\n",
      "processing C3L-01330-23.svs\n",
      "C3L-01330-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.08, 86/1027\n",
      "processing C3L-01455-21.svs\n",
      "C3L-01455-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.08, 87/1027\n",
      "processing C3L-01455-22.svs\n",
      "C3L-01455-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.09, 88/1027\n",
      "processing C3L-01455-23.svs\n",
      "C3L-01455-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.09, 89/1027\n",
      "processing C3L-01606-21.svs\n",
      "C3L-01606-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.09, 90/1027\n",
      "processing C3L-01632-21.svs\n",
      "C3L-01632-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.09, 91/1027\n",
      "processing C3L-01632-22.svs\n",
      "C3L-01632-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.09, 92/1027\n",
      "processing C3L-01632-23.svs\n",
      "C3L-01632-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.09, 93/1027\n",
      "processing C3L-01663-22.svs\n",
      "C3L-01663-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.09, 94/1027\n",
      "processing C3L-01682-21.svs\n",
      "C3L-01682-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.09, 95/1027\n",
      "processing C3L-01682-23.svs\n",
      "C3L-01682-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.09, 96/1027\n",
      "processing C3L-01683-26.svs\n",
      "C3L-01683-26 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.09, 97/1027\n",
      "processing C3L-01683-27.svs\n",
      "C3L-01683-27 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.10, 98/1027\n",
      "processing C3L-01838-23.svs\n",
      "C3L-01838-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.10, 99/1027\n",
      "processing C3L-01862-21.svs\n",
      "C3L-01862-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.10, 100/1027\n",
      "processing C3L-01862-22.svs\n",
      "C3L-01862-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.10, 101/1027\n",
      "processing C3L-01862-23.svs\n",
      "C3L-01862-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.10, 102/1027\n",
      "processing C3L-01884-21.svs\n",
      "C3L-01884-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.10, 103/1027\n",
      "processing C3L-01884-22.svs\n",
      "C3L-01884-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.10, 104/1027\n",
      "processing C3L-01889-21.svs\n",
      "C3L-01889-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.10, 105/1027\n",
      "processing C3L-01890-22.svs\n",
      "C3L-01890-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.10, 106/1027\n",
      "processing C3L-01924-21.svs\n",
      "C3L-01924-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.10, 107/1027\n",
      "processing C3L-01924-22.svs\n",
      "C3L-01924-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.11, 108/1027\n",
      "processing C3L-01924-23.svs\n",
      "C3L-01924-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.11, 109/1027\n",
      "processing C3L-01924-24.svs\n",
      "C3L-01924-24 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.11, 110/1027\n",
      "processing C3L-01924-25.svs\n",
      "C3L-01924-25 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.11, 111/1027\n",
      "processing C3L-02127-21.svs\n",
      "C3L-02127-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.11, 112/1027\n",
      "processing C3L-02127-22.svs\n",
      "C3L-02127-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.11, 113/1027\n",
      "processing C3L-02127-23.svs\n",
      "C3L-02127-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.11, 114/1027\n",
      "processing C3L-02127-24.svs\n",
      "C3L-02127-24 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.11, 115/1027\n",
      "processing C3L-02130-21.svs\n",
      "C3L-02130-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.11, 116/1027\n",
      "processing C3L-02130-22.svs\n",
      "C3L-02130-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.11, 117/1027\n",
      "processing C3L-02130-23.svs\n",
      "C3L-02130-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.11, 118/1027\n",
      "processing C3L-02130-24.svs\n",
      "C3L-02130-24 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.12, 119/1027\n",
      "processing C3L-02164-21.svs\n",
      "C3L-02164-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.12, 120/1027\n",
      "processing C3L-02164-22.svs\n",
      "C3L-02164-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.12, 121/1027\n",
      "processing C3L-02164-23.svs\n",
      "C3L-02164-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.12, 122/1027\n",
      "processing C3L-02165-21.svs\n",
      "C3L-02165-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.12, 123/1027\n",
      "processing C3L-02165-22.svs\n",
      "C3L-02165-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.12, 124/1027\n",
      "processing C3L-02165-23.svs\n",
      "C3L-02165-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.12, 125/1027\n",
      "processing C3L-02168-21.svs\n",
      "C3L-02168-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.12, 126/1027\n",
      "processing C3L-02168-22.svs\n",
      "C3L-02168-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.12, 127/1027\n",
      "processing C3L-02168-23.svs\n",
      "C3L-02168-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.12, 128/1027\n",
      "processing C3L-02170-21.svs\n",
      "C3L-02170-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.13, 129/1027\n",
      "processing C3L-02170-22.svs\n",
      "C3L-02170-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.13, 130/1027\n",
      "processing C3L-02170-23.svs\n",
      "C3L-02170-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.13, 131/1027\n",
      "processing C3L-02219-21.svs\n",
      "C3L-02219-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.13, 132/1027\n",
      "processing C3L-02219-22.svs\n",
      "C3L-02219-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.13, 133/1027\n",
      "processing C3L-02345-21.svs\n",
      "C3L-02345-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.13, 134/1027\n",
      "processing C3L-02345-22.svs\n",
      "C3L-02345-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.13, 135/1027\n",
      "processing C3L-02345-23.svs\n",
      "C3L-02345-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.13, 136/1027\n",
      "processing C3L-02348-21.svs\n",
      "C3L-02348-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.13, 137/1027\n",
      "processing C3L-02348-22.svs\n",
      "C3L-02348-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.13, 138/1027\n",
      "processing C3L-02349-21.svs\n",
      "C3L-02349-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.14, 139/1027\n",
      "processing C3L-02358-21.svs\n",
      "C3L-02358-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.14, 140/1027\n",
      "processing C3L-02358-22.svs\n",
      "C3L-02358-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.14, 141/1027\n",
      "processing C3L-02358-23.svs\n",
      "C3L-02358-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.14, 142/1027\n",
      "processing C3L-02365-21.svs\n",
      "C3L-02365-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.14, 143/1027\n",
      "processing C3L-02365-24.svs\n",
      "C3L-02365-24 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.14, 144/1027\n",
      "processing C3L-02365-25.svs\n",
      "C3L-02365-25 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.14, 145/1027\n",
      "processing C3L-02508-22.svs\n",
      "C3L-02508-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.14, 146/1027\n",
      "processing C3L-02508-23.svs\n",
      "C3L-02508-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.14, 147/1027\n",
      "processing C3L-02513-21.svs\n",
      "C3L-02513-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.14, 148/1027\n",
      "processing C3L-02513-22.svs\n",
      "C3L-02513-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.15, 149/1027\n",
      "processing C3L-02513-23.svs\n",
      "C3L-02513-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.15, 150/1027\n",
      "processing C3L-02515-21.svs\n",
      "C3L-02515-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.15, 151/1027\n",
      "processing C3L-02515-22.svs\n",
      "C3L-02515-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.15, 152/1027\n",
      "processing C3L-02515-23.svs\n",
      "C3L-02515-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.15, 153/1027\n",
      "processing C3L-02546-21.svs\n",
      "C3L-02546-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.15, 154/1027\n",
      "processing C3L-02546-23.svs\n",
      "C3L-02546-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.15, 155/1027\n",
      "processing C3L-02549-21.svs\n",
      "C3L-02549-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.15, 156/1027\n",
      "processing C3L-02549-22.svs\n",
      "C3L-02549-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.15, 157/1027\n",
      "processing C3L-02552-22.svs\n",
      "C3L-02552-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.15, 158/1027\n",
      "processing C3L-02552-23.svs\n",
      "C3L-02552-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.15, 159/1027\n",
      "processing C3L-02560-21.svs\n",
      "C3L-02560-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.16, 160/1027\n",
      "processing C3L-02560-22.svs\n",
      "C3L-02560-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.16, 161/1027\n",
      "processing C3L-02601-21.svs\n",
      "C3L-02601-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.16, 162/1027\n",
      "processing C3L-02616-21.svs\n",
      "C3L-02616-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.16, 163/1027\n",
      "processing C3L-02616-22.svs\n",
      "C3L-02616-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.16, 164/1027\n",
      "processing C3L-02616-23.svs\n",
      "C3L-02616-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.16, 165/1027\n",
      "processing C3L-02616-24.svs\n",
      "C3L-02616-24 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.16, 166/1027\n",
      "processing C3L-02616-25.svs\n",
      "C3L-02616-25 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.16, 167/1027\n",
      "processing C3L-02624-21.svs\n",
      "C3L-02624-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.16, 168/1027\n",
      "processing C3L-02624-22.svs\n",
      "C3L-02624-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.16, 169/1027\n",
      "processing C3L-02624-23.svs\n",
      "C3L-02624-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.17, 170/1027\n",
      "processing C3L-02624-24.svs\n",
      "C3L-02624-24 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.17, 171/1027\n",
      "processing C3L-02624-25.svs\n",
      "C3L-02624-25 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.17, 172/1027\n",
      "processing C3L-02625-21.svs\n",
      "C3L-02625-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.17, 173/1027\n",
      "processing C3L-02625-22.svs\n",
      "C3L-02625-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.17, 174/1027\n",
      "processing C3L-02625-23.svs\n",
      "C3L-02625-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.17, 175/1027\n",
      "processing C3L-02625-24.svs\n",
      "C3L-02625-24 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.17, 176/1027\n",
      "processing C3L-02625-25.svs\n",
      "C3L-02625-25 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.17, 177/1027\n",
      "processing C3L-02627-21.svs\n",
      "C3L-02627-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.17, 178/1027\n",
      "processing C3L-02627-22.svs\n",
      "C3L-02627-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.17, 179/1027\n",
      "processing C3L-02627-23.svs\n",
      "C3L-02627-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.18, 180/1027\n",
      "processing C3L-02627-24.svs\n",
      "C3L-02627-24 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.18, 181/1027\n",
      "processing C3L-02627-25.svs\n",
      "C3L-02627-25 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.18, 182/1027\n",
      "processing C3L-02629-21.svs\n",
      "C3L-02629-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.18, 183/1027\n",
      "processing C3L-02629-22.svs\n",
      "C3L-02629-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.18, 184/1027\n",
      "processing C3L-02629-23.svs\n",
      "C3L-02629-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.18, 185/1027\n",
      "processing C3L-02629-24.svs\n",
      "C3L-02629-24 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.18, 186/1027\n",
      "processing C3L-02629-25.svs\n",
      "C3L-02629-25 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.18, 187/1027\n",
      "processing C3L-02648-21.svs\n",
      "C3L-02648-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.18, 188/1027\n",
      "processing C3L-02648-22.svs\n",
      "C3L-02648-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.18, 189/1027\n",
      "processing C3L-02648-23.svs\n",
      "C3L-02648-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.19, 190/1027\n",
      "processing C3L-02648-24.svs\n",
      "C3L-02648-24 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.19, 191/1027\n",
      "processing C3L-02648-25.svs\n",
      "C3L-02648-25 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.19, 192/1027\n",
      "processing C3L-02649-21.svs\n",
      "C3L-02649-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.19, 193/1027\n",
      "processing C3L-02649-22.svs\n",
      "C3L-02649-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.19, 194/1027\n",
      "processing C3L-02649-23.svs\n",
      "C3L-02649-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.19, 195/1027\n",
      "processing C3L-02649-24.svs\n",
      "C3L-02649-24 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.19, 196/1027\n",
      "processing C3L-02649-25.svs\n",
      "C3L-02649-25 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.19, 197/1027\n",
      "processing C3L-02661-21.svs\n",
      "C3L-02661-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.19, 198/1027\n",
      "processing C3L-02661-22.svs\n",
      "C3L-02661-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.19, 199/1027\n",
      "processing C3L-02661-23.svs\n",
      "C3L-02661-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.19, 200/1027\n",
      "processing C3L-02661-24.svs\n",
      "C3L-02661-24 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.20, 201/1027\n",
      "processing C3L-02661-25.svs\n",
      "C3L-02661-25 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.20, 202/1027\n",
      "processing C3L-02834-21.svs\n",
      "C3L-02834-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.20, 203/1027\n",
      "processing C3L-02834-22.svs\n",
      "C3L-02834-22 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.20, 204/1027\n",
      "processing C3L-02834-23.svs\n",
      "C3L-02834-23 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.20, 205/1027\n",
      "processing C3L-02891-21.svs\n",
      "C3L-02891-21 already exist in destination location, skipped\n",
      "\n",
      "\n",
      "progress: 0.20, 206/1027\n",
      "processing C3L-02891-23.svs\n",
      "Creating patches for:  C3L-02891-23 ...\n",
      "Total number of contours to process:  1\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sci/PycharmProjects/chaofan/projects/CLAM/create_patches_fp.py\", line 325, in <module>\n",
      "    seg_times, patch_times = seg_and_patch(**directories, **parameters,\n",
      "  File \"/home/sci/PycharmProjects/chaofan/projects/CLAM/create_patches_fp.py\", line 215, in seg_and_patch\n",
      "    file_path, patch_time_elapsed = patching(WSI_object = WSI_object,  **current_patch_params,)\n",
      "  File \"/home/sci/PycharmProjects/chaofan/projects/CLAM/create_patches_fp.py\", line 42, in patching\n",
      "    file_path = WSI_object.process_contours(**kwargs)\n",
      "  File \"/home/sci/PycharmProjects/chaofan/projects/CLAM/wsi_core/WholeSlideImage.py\", line 391, in process_contours\n",
      "    asset_dict, attr_dict = self.process_contour(cont, self.holes_tissue[idx], patch_level, save_path, patch_size, step_size, **kwargs)\n",
      "  File \"/home/sci/PycharmProjects/chaofan/projects/CLAM/wsi_core/WholeSlideImage.py\", line 406, in process_contour\n",
      "    patch_downsample = (int(self.level_downsamples[patch_level][0]), int(self.level_downsamples[patch_level][1]))\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "##改 savedir and patch_level\n",
    "!python create_patches_fp.py --source /home/sci/Disk2/CPTAC-LUNG/WSI --save_dir /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2 --patch_level 2 --patch_size 256 --seg --patch --stitch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step_2 get patch features\n",
    "data_h5_dir  输出文件地址\\\n",
    "data_slide_dir svs文件地址\\\n",
    "上一步生成的csv csv_path\\\n",
    "feat_dir 输出文件地址\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CUDA_VISIBLE_DEVICES=0 python extract_features_fp.py \\\n",
    "--data_h5_dir data/RESULTS_DIRECTORY/patches \\\n",
    "--data_slide_dir /media/yuansh/14THHD/CLAM/DataSet/toy_example \\\n",
    "--csv_path /media/yuansh/14THHD/CLAM/Step_2.csv \\\n",
    "--feat_dir /media/yuansh/14THHD/CLAM/FEATURES_DIRECTORY \\\n",
    "--batch_size 512 \\\n",
    "--slide_ext .svs\n",
    "'''\n",
    "!python extract_features_fp.py\\\n",
    "    --data_h5_dir E:\\Workspace\\Project\\CLAM\\data\\RESULTS_DIRECTORY \\\n",
    "    --data_slide_dir F:/Download/TCGA \\\n",
    "    --csv_path data\\RESULTS_DIRECTORY\\Step_2.csv \\\n",
    "    --feat_dir E:\\Workspace\\Project\\CLAM\\data\\FEATURES_DIRECTORY \\\n",
    "    --batch_size 256 \\\n",
    "    --slide_ext .svs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成第2步骤需要的csv文件\n",
    "from utils.csv_gen import *\n",
    "\n",
    "csv_dir = r'/home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/process_list_autogen.csv'\n",
    "# sort_csv = pd.read_csv(csv_dir).sort_values('slide_id')\n",
    "result_dir = r'/home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/step2_get_features.csv'\n",
    "patch_dir = r'/home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches'\n",
    "csv_gen_step1(csv_dir,result_dir,patch_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing dataset\n",
      "loading model checkpoint\n",
      "\n",
      "progress: 0/206\n",
      "C3L-00001-21\n",
      "downsample [8.00216316 8.00101523]\n",
      "downsampled_level_dim [3236 2955]\n",
      "level_dim [3236 2955]\n",
      "name C3L-00001-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00001-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00001-21.h5 took 2.953198194503784 s\n",
      "features size:  (71, 1024)\n",
      "coordinates size:  (71, 2)\n",
      "\n",
      "progress: 1/206\n",
      "C3L-00009-21\n",
      "downsample [8.00281237 8.00182949]\n",
      "downsampled_level_dim [2489 2733]\n",
      "level_dim [2489 2733]\n",
      "name C3L-00009-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00009-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00009-21.h5 took 0.49354004859924316 s\n",
      "features size:  (41, 1024)\n",
      "coordinates size:  (41, 2)\n",
      "\n",
      "progress: 2/206\n",
      "C3L-00080-21\n",
      "downsample [16.00432633 16.00611309]\n",
      "downsampled_level_dim [1618 1963]\n",
      "level_dim [1618 1963]\n",
      "name C3L-00080-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00080-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00080-21.h5 took 0.328214168548584 s\n",
      "features size:  (14, 1024)\n",
      "coordinates size:  (14, 2)\n",
      "\n",
      "progress: 3/206\n",
      "C3L-00081-21\n",
      "downsample [16.00432633 16.00324675]\n",
      "downsampled_level_dim [1618 1540]\n",
      "level_dim [1618 1540]\n",
      "name C3L-00081-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00081-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00081-21.h5 took 0.35224294662475586 s\n",
      "features size:  (19, 1024)\n",
      "coordinates size:  (19, 2)\n",
      "\n",
      "progress: 4/206\n",
      "C3L-00083-21\n",
      "downsample [16.0075339  16.00102775]\n",
      "downsampled_level_dim [1991 1946]\n",
      "level_dim [1991 1946]\n",
      "name C3L-00083-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00083-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00083-21.h5 took 0.3948934078216553 s\n",
      "features size:  (23, 1024)\n",
      "coordinates size:  (23, 2)\n",
      "\n",
      "progress: 5/206\n",
      "C3L-00093-21\n",
      "downsample [8.00216316 8.        ]\n",
      "downsampled_level_dim [3236 2574]\n",
      "level_dim [3236 2574]\n",
      "name C3L-00093-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00093-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00093-21.h5 took 0.5396156311035156 s\n",
      "features size:  (48, 1024)\n",
      "coordinates size:  (48, 2)\n",
      "\n",
      "progress: 6/206\n",
      "C3L-00094-21\n",
      "downsample [16.01004689 16.00184843]\n",
      "downsampled_level_dim [1493 1623]\n",
      "level_dim [1493 1623]\n",
      "name C3L-00094-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00094-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00094-21.h5 took 0.31485986709594727 s\n",
      "features size:  (13, 1024)\n",
      "coordinates size:  (13, 2)\n",
      "\n",
      "progress: 7/206\n",
      "C3L-00095-21\n",
      "downsample [8.00281237 8.0003413 ]\n",
      "downsampled_level_dim [2489 2930]\n",
      "level_dim [2489 2930]\n",
      "name C3L-00095-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00095-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00095-21.h5 took 0.4452040195465088 s\n",
      "features size:  (31, 1024)\n",
      "coordinates size:  (31, 2)\n",
      "\n",
      "progress: 8/206\n",
      "C3L-00095-22\n",
      "downsample [8.00200861 8.00181357]\n",
      "downsampled_level_dim [3485 2757]\n",
      "level_dim [3485 2757]\n",
      "name C3L-00095-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00095-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00095-22.h5 took 0.5131120681762695 s\n",
      "features size:  (42, 1024)\n",
      "coordinates size:  (42, 2)\n",
      "\n",
      "progress: 9/206\n",
      "C3L-00095-23\n",
      "downsample [8.00234349 8.00072993]\n",
      "downsampled_level_dim [2987 2740]\n",
      "level_dim [2987 2740]\n",
      "name C3L-00095-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00095-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00095-23.h5 took 0.43389177322387695 s\n",
      "features size:  (30, 1024)\n",
      "coordinates size:  (30, 2)\n",
      "\n",
      "progress: 10/206\n",
      "C3L-00140-21\n",
      "downsample [8.00255661 8.        ]\n",
      "downsampled_level_dim [2738 2670]\n",
      "level_dim [2738 2670]\n",
      "name C3L-00140-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00140-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00140-21.h5 took 0.49988484382629395 s\n",
      "features size:  (41, 1024)\n",
      "coordinates size:  (41, 2)\n",
      "\n",
      "progress: 11/206\n",
      "C3L-00140-22\n",
      "downsample [16.01205788 16.00478755]\n",
      "downsampled_level_dim [1244 1671]\n",
      "level_dim [1244 1671]\n",
      "name C3L-00140-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00140-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00140-22.h5 took 0.30614185333251953 s\n",
      "features size:  (14, 1024)\n",
      "coordinates size:  (14, 2)\n",
      "\n",
      "progress: 12/206\n",
      "C3L-00144-21\n",
      "downsample [8.00281237 8.00193143]\n",
      "downsampled_level_dim [2489 2071]\n",
      "level_dim [2489 2071]\n",
      "name C3L-00144-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00144-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00144-21.h5 took 0.41596102714538574 s\n",
      "features size:  (29, 1024)\n",
      "coordinates size:  (29, 2)\n",
      "\n",
      "progress: 13/206\n",
      "C3L-00263-21\n",
      "downsample [16.00669643 16.00051975]\n",
      "downsampled_level_dim [2240 1924]\n",
      "level_dim [2240 1924]\n",
      "name C3L-00263-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00263-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00263-21.h5 took 0.33431291580200195 s\n",
      "features size:  (16, 1024)\n",
      "coordinates size:  (16, 2)\n",
      "\n",
      "progress: 14/206\n",
      "C3L-00263-22\n",
      "downsample [16.0075339  16.00277971]\n",
      "downsampled_level_dim [1991 1439]\n",
      "level_dim [1991 1439]\n",
      "name C3L-00263-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00263-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00263-22.h5 took 0.2872350215911865 s\n",
      "features size:  (8, 1024)\n",
      "coordinates size:  (8, 2)\n",
      "\n",
      "progress: 15/206\n",
      "C3L-00263-23\n",
      "downsample [16.00374933 16.00583942]\n",
      "downsampled_level_dim [1867 2055]\n",
      "level_dim [1867 2055]\n",
      "name C3L-00263-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00263-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00263-23.h5 took 0.35723280906677246 s\n",
      "features size:  (20, 1024)\n",
      "coordinates size:  (20, 2)\n",
      "\n",
      "progress: 16/206\n",
      "C3L-00263-24\n",
      "downsample [16.00330813 16.00608828]\n",
      "downsampled_level_dim [2116 1314]\n",
      "level_dim [2116 1314]\n",
      "name C3L-00263-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00263-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00263-24.h5 took 0.31761765480041504 s\n",
      "features size:  (16, 1024)\n",
      "coordinates size:  (16, 2)\n",
      "\n",
      "progress: 17/206\n",
      "C3L-00263-25\n",
      "downsample [16.00861079 16.00105208]\n",
      "downsampled_level_dim [1742 1901]\n",
      "level_dim [1742 1901]\n",
      "name C3L-00263-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00263-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00263-25.h5 took 0.30028653144836426 s\n",
      "features size:  (14, 1024)\n",
      "coordinates size:  (14, 2)\n",
      "\n",
      "progress: 18/206\n",
      "C3L-00279-21\n",
      "downsample [8.00255661 8.00172652]\n",
      "downsampled_level_dim [2738 2896]\n",
      "level_dim [2738 2896]\n",
      "name C3L-00279-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00279-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00279-21.h5 took 0.5791141986846924 s\n",
      "features size:  (55, 1024)\n",
      "coordinates size:  (55, 2)\n",
      "\n",
      "progress: 19/206\n",
      "C3L-00279-22\n",
      "downsample [8.00255661 8.00213295]\n",
      "downsampled_level_dim [2738 2813]\n",
      "level_dim [2738 2813]\n",
      "name C3L-00279-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00279-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00279-22.h5 took 0.5303330421447754 s\n",
      "features size:  (46, 1024)\n",
      "coordinates size:  (46, 2)\n",
      "\n",
      "progress: 20/206\n",
      "C3L-00279-23\n",
      "downsample [8.00255661 8.        ]\n",
      "downsampled_level_dim [2738 2618]\n",
      "level_dim [2738 2618]\n",
      "name C3L-00279-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00279-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00279-23.h5 took 0.5018181800842285 s\n",
      "features size:  (43, 1024)\n",
      "coordinates size:  (43, 2)\n",
      "\n",
      "progress: 21/206\n",
      "C3L-00368-21\n",
      "downsample [16.00511322 16.0087925 ]\n",
      "downsampled_level_dim [1369 1706]\n",
      "level_dim [1369 1706]\n",
      "name C3L-00368-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00368-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00368-21.h5 took 0.31067466735839844 s\n",
      "features size:  (16, 1024)\n",
      "coordinates size:  (16, 2)\n",
      "\n",
      "progress: 22/206\n",
      "C3L-00368-22\n",
      "downsample [16.0075339  16.00949153]\n",
      "downsampled_level_dim [1991 1475]\n",
      "level_dim [1991 1475]\n",
      "name C3L-00368-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00368-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00368-22.h5 took 0.3393378257751465 s\n",
      "features size:  (17, 1024)\n",
      "coordinates size:  (17, 2)\n",
      "\n",
      "progress: 23/206\n",
      "C3L-00412-21\n",
      "downsample [16.00330813 16.00146056]\n",
      "downsampled_level_dim [2116 2054]\n",
      "level_dim [2116 2054]\n",
      "name C3L-00412-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00412-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00412-21.h5 took 0.4351472854614258 s\n",
      "features size:  (32, 1024)\n",
      "coordinates size:  (32, 2)\n",
      "\n",
      "progress: 24/206\n",
      "C3L-00415-21\n",
      "downsample [16.00669643 16.00500626]\n",
      "downsampled_level_dim [2240 1598]\n",
      "level_dim [2240 1598]\n",
      "name C3L-00415-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00415-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00415-21.h5 took 0.326460599899292 s\n",
      "features size:  (17, 1024)\n",
      "coordinates size:  (17, 2)\n",
      "\n",
      "progress: 25/206\n",
      "C3L-00415-23\n",
      "downsample [16.00602652 16.00544617]\n",
      "downsampled_level_dim [2489 2387]\n",
      "level_dim [2489 2387]\n",
      "name C3L-00415-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00415-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00415-23.h5 took 0.42401599884033203 s\n",
      "features size:  (32, 1024)\n",
      "coordinates size:  (32, 2)\n",
      "\n",
      "progress: 26/206\n",
      "C3L-00422-21\n",
      "downsample [8.00216316 8.00091283]\n",
      "downsampled_level_dim [3236 2191]\n",
      "level_dim [3236 2191]\n",
      "name C3L-00422-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00422-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00422-21.h5 took 0.4899313449859619 s\n",
      "features size:  (40, 1024)\n",
      "coordinates size:  (40, 2)\n",
      "\n",
      "progress: 27/206\n",
      "C3L-00422-22\n",
      "downsample [8.00255661 8.        ]\n",
      "downsampled_level_dim [2738 1914]\n",
      "level_dim [2738 1914]\n",
      "name C3L-00422-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00422-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00422-22.h5 took 0.411602258682251 s\n",
      "features size:  (30, 1024)\n",
      "coordinates size:  (30, 2)\n",
      "\n",
      "progress: 28/206\n",
      "C3L-00422-23\n",
      "downsample [16.00330813 16.        ]\n",
      "downsampled_level_dim [2116 1837]\n",
      "level_dim [2116 1837]\n",
      "name C3L-00422-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00422-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00422-23.h5 took 0.2766706943511963 s\n",
      "features size:  (8, 1024)\n",
      "coordinates size:  (8, 2)\n",
      "\n",
      "progress: 29/206\n",
      "C3L-00444-21\n",
      "downsample [16.00110254 16.00517063]\n",
      "downsampled_level_dim [6349 2901]\n",
      "level_dim [6349 2901]\n",
      "name C3L-00444-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00444-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00444-21.h5 took 1.1633684635162354 s\n",
      "features size:  (100, 1024)\n",
      "coordinates size:  (100, 2)\n",
      "\n",
      "progress: 30/206\n",
      "C3L-00444-22\n",
      "downsample [16.00317125 16.00407886]\n",
      "downsampled_level_dim [4730 2942]\n",
      "level_dim [4730 2942]\n",
      "name C3L-00444-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00444-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00444-22.h5 took 0.8456614017486572 s\n",
      "features size:  (82, 1024)\n",
      "coordinates size:  (82, 2)\n",
      "\n",
      "progress: 31/206\n",
      "C3L-00444-23\n",
      "downsample [16.00273873 16.0034965 ]\n",
      "downsampled_level_dim [5477 2860]\n",
      "level_dim [5477 2860]\n",
      "name C3L-00444-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00444-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00444-23.h5 took 0.7859649658203125 s\n",
      "features size:  (65, 1024)\n",
      "coordinates size:  (65, 2)\n",
      "\n",
      "progress: 32/206\n",
      "C3L-00445-21\n",
      "downsample [16.00215177 16.00524705]\n",
      "downsampled_level_dim [6971 2287]\n",
      "level_dim [6971 2287]\n",
      "name C3L-00445-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00445-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00445-21.h5 took 1.2385718822479248 s\n",
      "features size:  (134, 1024)\n",
      "coordinates size:  (134, 2)\n",
      "\n",
      "progress: 33/206\n",
      "C3L-00445-25\n",
      "downsample [16.00106093 16.0029889 ]\n",
      "downsampled_level_dim [6598 2342]\n",
      "level_dim [6598 2342]\n",
      "name C3L-00445-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00445-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00445-25.h5 took 1.0781352519989014 s\n",
      "features size:  (119, 1024)\n",
      "coordinates size:  (119, 2)\n",
      "\n",
      "progress: 34/206\n",
      "C3L-00445-26\n",
      "downsample [16.00102235 16.00462784]\n",
      "downsampled_level_dim [6847 2593]\n",
      "level_dim [6847 2593]\n",
      "name C3L-00445-26\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00445-26.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00445-26.h5 took 1.3930692672729492 s\n",
      "features size:  (152, 1024)\n",
      "coordinates size:  (152, 2)\n",
      "\n",
      "progress: 35/206\n",
      "C3L-00446-21\n",
      "downsample [16.00223148 16.00268817]\n",
      "downsampled_level_dim [6722 2232]\n",
      "level_dim [6722 2232]\n",
      "name C3L-00446-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00446-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00446-21.h5 took 1.1882355213165283 s\n",
      "features size:  (119, 1024)\n",
      "coordinates size:  (119, 2)\n",
      "\n",
      "progress: 36/206\n",
      "C3L-00446-22\n",
      "downsample [16.00273873 16.00175131]\n",
      "downsampled_level_dim [5477 1713]\n",
      "level_dim [5477 1713]\n",
      "name C3L-00446-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00446-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00446-22.h5 took 0.6671068668365479 s\n",
      "features size:  (58, 1024)\n",
      "coordinates size:  (58, 2)\n",
      "\n",
      "progress: 37/206\n",
      "C3L-00446-23\n",
      "downsample [16.00130768 16.00207147]\n",
      "downsampled_level_dim [5353 1931]\n",
      "level_dim [5353 1931]\n",
      "name C3L-00446-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00446-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00446-23.h5 took 0.697249174118042 s\n",
      "features size:  (68, 1024)\n",
      "coordinates size:  (68, 2)\n",
      "\n",
      "progress: 38/206\n",
      "C3L-00446-24\n",
      "downsample [16.00286917 16.        ]\n",
      "downsampled_level_dim [5228 2304]\n",
      "level_dim [5228 2304]\n",
      "name C3L-00446-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00446-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00446-24.h5 took 0.813330888748169 s\n",
      "features size:  (76, 1024)\n",
      "coordinates size:  (76, 2)\n",
      "\n",
      "progress: 39/206\n",
      "C3L-00503-21\n",
      "downsample [16.00432633 16.0096463 ]\n",
      "downsampled_level_dim [1618 1555]\n",
      "level_dim [1618 1555]\n",
      "name C3L-00503-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00503-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00503-21.h5 took 0.3134760856628418 s\n",
      "features size:  (9, 1024)\n",
      "coordinates size:  (9, 2)\n",
      "\n",
      "progress: 40/206\n",
      "C3L-00503-22\n",
      "downsample [8.00255661 8.00116959]\n",
      "downsampled_level_dim [2738 2565]\n",
      "level_dim [2738 2565]\n",
      "name C3L-00503-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00503-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00503-22.h5 took 0.4478611946105957 s\n",
      "features size:  (33, 1024)\n",
      "coordinates size:  (33, 2)\n",
      "\n",
      "progress: 41/206\n",
      "C3L-00503-23\n",
      "downsample [16.00374933 16.00228571]\n",
      "downsampled_level_dim [1867 1750]\n",
      "level_dim [1867 1750]\n",
      "name C3L-00503-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00503-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00503-23.h5 took 0.3674173355102539 s\n",
      "features size:  (21, 1024)\n",
      "coordinates size:  (21, 2)\n",
      "\n",
      "progress: 42/206\n",
      "C3L-00510-21\n",
      "downsample [16.0075339  16.00568182]\n",
      "downsampled_level_dim [1991 1584]\n",
      "level_dim [1991 1584]\n",
      "name C3L-00510-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00510-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00510-21.h5 took 0.33763718605041504 s\n",
      "features size:  (19, 1024)\n",
      "coordinates size:  (19, 2)\n",
      "\n",
      "progress: 43/206\n",
      "C3L-00510-22\n",
      "downsample [16.00374933 16.00374532]\n",
      "downsampled_level_dim [1867 1869]\n",
      "level_dim [1867 1869]\n",
      "name C3L-00510-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00510-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00510-22.h5 took 0.3024892807006836 s\n",
      "features size:  (14, 1024)\n",
      "coordinates size:  (14, 2)\n",
      "\n",
      "progress: 44/206\n",
      "C3L-00510-23\n",
      "downsample [8.00234349 8.00035817]\n",
      "downsampled_level_dim [2987 2792]\n",
      "level_dim [2987 2792]\n",
      "name C3L-00510-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00510-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00510-23.h5 took 0.554924726486206 s\n",
      "features size:  (51, 1024)\n",
      "coordinates size:  (51, 2)\n",
      "\n",
      "progress: 45/206\n",
      "C3L-00568-21\n",
      "downsample [16.01004689 16.00335796]\n",
      "downsampled_level_dim [1493 1489]\n",
      "level_dim [1493 1489]\n",
      "name C3L-00568-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00568-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00568-21.h5 took 0.32436680793762207 s\n",
      "features size:  (12, 1024)\n",
      "coordinates size:  (12, 2)\n",
      "\n",
      "progress: 46/206\n",
      "C3L-00568-22\n",
      "downsample [8.003125   8.00230858]\n",
      "downsampled_level_dim [2240 2599]\n",
      "level_dim [2240 2599]\n",
      "name C3L-00568-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00568-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00568-22.h5 took 0.41684460639953613 s\n",
      "features size:  (29, 1024)\n",
      "coordinates size:  (29, 2)\n",
      "\n",
      "progress: 47/206\n",
      "C3L-00568-23\n",
      "downsample [8.00281237 8.00261097]\n",
      "downsampled_level_dim [2489 2298]\n",
      "level_dim [2489 2298]\n",
      "name C3L-00568-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00568-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00568-23.h5 took 0.3132803440093994 s\n",
      "features size:  (14, 1024)\n",
      "coordinates size:  (14, 2)\n",
      "\n",
      "progress: 48/206\n",
      "C3L-00603-21\n",
      "downsample [8.00351582 8.0027688 ]\n",
      "downsampled_level_dim [1991 2167]\n",
      "level_dim [1991 2167]\n",
      "name C3L-00603-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00603-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00603-21.h5 took 0.3523070812225342 s\n",
      "features size:  (18, 1024)\n",
      "coordinates size:  (18, 2)\n",
      "\n",
      "progress: 49/206\n",
      "C3L-00603-22\n",
      "downsample [8.00281237 8.00180963]\n",
      "downsampled_level_dim [2489 2763]\n",
      "level_dim [2489 2763]\n",
      "name C3L-00603-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00603-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00603-22.h5 took 0.46324658393859863 s\n",
      "features size:  (35, 1024)\n",
      "coordinates size:  (35, 2)\n",
      "\n",
      "progress: 50/206\n",
      "C3L-00604-21\n",
      "downsample [8.00281237 8.00110619]\n",
      "downsampled_level_dim [2489 2712]\n",
      "level_dim [2489 2712]\n",
      "name C3L-00604-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00604-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00604-21.h5 took 0.45235705375671387 s\n",
      "features size:  (32, 1024)\n",
      "coordinates size:  (32, 2)\n",
      "\n",
      "progress: 51/206\n",
      "C3L-00604-22\n",
      "downsample [8.00281237 8.00082576]\n",
      "downsampled_level_dim [2489 2422]\n",
      "level_dim [2489 2422]\n",
      "name C3L-00604-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00604-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00604-22.h5 took 0.4244678020477295 s\n",
      "features size:  (31, 1024)\n",
      "coordinates size:  (31, 2)\n",
      "\n",
      "progress: 52/206\n",
      "C3L-00604-23\n",
      "downsample [8.00234349 8.00039952]\n",
      "downsampled_level_dim [2987 2503]\n",
      "level_dim [2987 2503]\n",
      "name C3L-00604-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00604-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00604-23.h5 took 0.485414981842041 s\n",
      "features size:  (38, 1024)\n",
      "coordinates size:  (38, 2)\n",
      "\n",
      "progress: 53/206\n",
      "C3L-00893-21\n",
      "downsample [8.00281237 8.00083612]\n",
      "downsampled_level_dim [2489 2392]\n",
      "level_dim [2489 2392]\n",
      "name C3L-00893-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00893-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00893-21.h5 took 0.3669869899749756 s\n",
      "features size:  (23, 1024)\n",
      "coordinates size:  (23, 2)\n",
      "\n",
      "progress: 54/206\n",
      "C3L-00893-22\n",
      "downsample [8.00200861 8.00245614]\n",
      "downsampled_level_dim [3485 2850]\n",
      "level_dim [3485 2850]\n",
      "name C3L-00893-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00893-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00893-22.h5 took 0.46326112747192383 s\n",
      "features size:  (34, 1024)\n",
      "coordinates size:  (34, 2)\n",
      "\n",
      "progress: 55/206\n",
      "C3L-00893-23\n",
      "downsample [16.00511322 16.        ]\n",
      "downsampled_level_dim [1369 1552]\n",
      "level_dim [1369 1552]\n",
      "name C3L-00893-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00893-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00893-23.h5 took 0.3079719543457031 s\n",
      "features size:  (10, 1024)\n",
      "coordinates size:  (10, 2)\n",
      "\n",
      "progress: 56/206\n",
      "C3L-00904-21\n",
      "downsample [16.00295983 16.        ]\n",
      "downsampled_level_dim [2365 3159]\n",
      "level_dim [2365 3159]\n",
      "name C3L-00904-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00904-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00904-21.h5 took 0.6009085178375244 s\n",
      "features size:  (54, 1024)\n",
      "coordinates size:  (54, 2)\n",
      "\n",
      "progress: 57/206\n",
      "C3L-00904-22\n",
      "downsample [16.00374933 16.004     ]\n",
      "downsampled_level_dim [1867 1750]\n",
      "level_dim [1867 1750]\n",
      "name C3L-00904-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00904-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00904-22.h5 took 0.39412569999694824 s\n",
      "features size:  (24, 1024)\n",
      "coordinates size:  (24, 2)\n",
      "\n",
      "progress: 58/206\n",
      "C3L-00913-21\n",
      "downsample [16.00432633 16.        ]\n",
      "downsampled_level_dim [1618 1845]\n",
      "level_dim [1618 1845]\n",
      "name C3L-00913-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00913-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00913-21.h5 took 0.30746912956237793 s\n",
      "features size:  (13, 1024)\n",
      "coordinates size:  (13, 2)\n",
      "\n",
      "progress: 59/206\n",
      "C3L-00913-22\n",
      "downsample [16.00669643 16.00260191]\n",
      "downsampled_level_dim [2240 2306]\n",
      "level_dim [2240 2306]\n",
      "name C3L-00913-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00913-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00913-22.h5 took 0.37636566162109375 s\n",
      "features size:  (24, 1024)\n",
      "coordinates size:  (24, 2)\n",
      "\n",
      "progress: 60/206\n",
      "C3L-00913-23\n",
      "downsample [16.01004689 16.00230947]\n",
      "downsampled_level_dim [1493 1732]\n",
      "level_dim [1493 1732]\n",
      "name C3L-00913-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00913-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00913-23.h5 took 0.27725934982299805 s\n",
      "features size:  (9, 1024)\n",
      "coordinates size:  (9, 2)\n",
      "\n",
      "progress: 61/206\n",
      "C3L-00923-23\n",
      "downsample [16.00861079 16.00229753]\n",
      "downsampled_level_dim [1742 1741]\n",
      "level_dim [1742 1741]\n",
      "name C3L-00923-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00923-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00923-23.h5 took 0.33798718452453613 s\n",
      "features size:  (18, 1024)\n",
      "coordinates size:  (18, 2)\n",
      "\n",
      "progress: 62/206\n",
      "C3L-00927-21\n",
      "downsample [8.00281237 8.00229991]\n",
      "downsampled_level_dim [2489 2174]\n",
      "level_dim [2489 2174]\n",
      "name C3L-00927-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00927-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00927-21.h5 took 0.4325263500213623 s\n",
      "features size:  (31, 1024)\n",
      "coordinates size:  (31, 2)\n",
      "\n",
      "progress: 63/206\n",
      "C3L-00927-22\n",
      "downsample [8.00281237 8.00310284]\n",
      "downsampled_level_dim [2489 2256]\n",
      "level_dim [2489 2256]\n",
      "name C3L-00927-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00927-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00927-22.h5 took 0.44827866554260254 s\n",
      "features size:  (34, 1024)\n",
      "coordinates size:  (34, 2)\n",
      "\n",
      "progress: 64/206\n",
      "C3L-00927-23\n",
      "downsample [8.00255661 8.0025344 ]\n",
      "downsampled_level_dim [2738 2762]\n",
      "level_dim [2738 2762]\n",
      "name C3L-00927-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00927-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00927-23.h5 took 0.47113823890686035 s\n",
      "features size:  (36, 1024)\n",
      "coordinates size:  (36, 2)\n",
      "\n",
      "progress: 65/206\n",
      "C3L-00965-21\n",
      "downsample [8.00281237 8.00197044]\n",
      "downsampled_level_dim [2489 2030]\n",
      "level_dim [2489 2030]\n",
      "name C3L-00965-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00965-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00965-21.h5 took 0.4380149841308594 s\n",
      "features size:  (33, 1024)\n",
      "coordinates size:  (33, 2)\n",
      "\n",
      "progress: 66/206\n",
      "C3L-00965-22\n",
      "downsample [8.003125   8.00048473]\n",
      "downsampled_level_dim [2240 2063]\n",
      "level_dim [2240 2063]\n",
      "name C3L-00965-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00965-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00965-22.h5 took 0.4134495258331299 s\n",
      "features size:  (29, 1024)\n",
      "coordinates size:  (29, 2)\n",
      "\n",
      "progress: 67/206\n",
      "C3L-00965-23\n",
      "downsample [8.00351582 8.001999  ]\n",
      "downsampled_level_dim [1991 2001]\n",
      "level_dim [1991 2001]\n",
      "name C3L-00965-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00965-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00965-23.h5 took 0.384845495223999 s\n",
      "features size:  (24, 1024)\n",
      "coordinates size:  (24, 2)\n",
      "\n",
      "progress: 68/206\n",
      "C3L-00965-24\n",
      "downsample [8.003125   8.00194704]\n",
      "downsampled_level_dim [2240 2568]\n",
      "level_dim [2240 2568]\n",
      "name C3L-00965-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00965-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00965-24.h5 took 0.44971418380737305 s\n",
      "features size:  (34, 1024)\n",
      "coordinates size:  (34, 2)\n",
      "\n",
      "progress: 69/206\n",
      "C3L-00973-21\n",
      "downsample [16.00267789 16.00427148]\n",
      "downsampled_level_dim [2614 2107]\n",
      "level_dim [2614 2107]\n",
      "name C3L-00973-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00973-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00973-21.h5 took 0.4508092403411865 s\n",
      "features size:  (34, 1024)\n",
      "coordinates size:  (34, 2)\n",
      "\n",
      "progress: 70/206\n",
      "C3L-00973-22\n",
      "downsample [16.00244499 16.00273473]\n",
      "downsampled_level_dim [2863 2194]\n",
      "level_dim [2863 2194]\n",
      "name C3L-00973-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00973-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00973-22.h5 took 0.5305333137512207 s\n",
      "features size:  (44, 1024)\n",
      "coordinates size:  (44, 2)\n",
      "\n",
      "progress: 71/206\n",
      "C3L-00973-23\n",
      "downsample [16.00244499 16.00437254]\n",
      "downsampled_level_dim [2863 2287]\n",
      "level_dim [2863 2287]\n",
      "name C3L-00973-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00973-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00973-23.h5 took 0.5450026988983154 s\n",
      "features size:  (47, 1024)\n",
      "coordinates size:  (47, 2)\n",
      "\n",
      "progress: 72/206\n",
      "C3L-00973-24\n",
      "downsample [16.00267789 16.        ]\n",
      "downsampled_level_dim [2614 2077]\n",
      "level_dim [2614 2077]\n",
      "name C3L-00973-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00973-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00973-24.h5 took 0.4531559944152832 s\n",
      "features size:  (34, 1024)\n",
      "coordinates size:  (34, 2)\n",
      "\n",
      "progress: 73/206\n",
      "C3L-00973-25\n",
      "downsample [16.0075339  16.00054113]\n",
      "downsampled_level_dim [1991 1848]\n",
      "level_dim [1991 1848]\n",
      "name C3L-00973-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00973-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00973-25.h5 took 0.37053775787353516 s\n",
      "features size:  (23, 1024)\n",
      "coordinates size:  (23, 2)\n",
      "\n",
      "progress: 74/206\n",
      "C3L-00993-21\n",
      "downsample [16.00374933 16.00633312]\n",
      "downsampled_level_dim [1867 1579]\n",
      "level_dim [1867 1579]\n",
      "name C3L-00993-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00993-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00993-21.h5 took 0.3506441116333008 s\n",
      "features size:  (19, 1024)\n",
      "coordinates size:  (19, 2)\n",
      "\n",
      "progress: 75/206\n",
      "C3L-00993-22\n",
      "downsample [16.00669643 16.00216826]\n",
      "downsampled_level_dim [2240 2306]\n",
      "level_dim [2240 2306]\n",
      "name C3L-00993-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00993-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00993-22.h5 took 0.42247748374938965 s\n",
      "features size:  (28, 1024)\n",
      "coordinates size:  (28, 2)\n",
      "\n",
      "progress: 76/206\n",
      "C3L-00993-23\n",
      "downsample [16.0075339  16.00231696]\n",
      "downsampled_level_dim [1991 2158]\n",
      "level_dim [1991 2158]\n",
      "name C3L-00993-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00993-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00993-23.h5 took 0.3773837089538574 s\n",
      "features size:  (24, 1024)\n",
      "coordinates size:  (24, 2)\n",
      "\n",
      "progress: 77/206\n",
      "C3L-00993-24\n",
      "downsample [16.00330813 16.00044131]\n",
      "downsampled_level_dim [2116 2266]\n",
      "level_dim [2116 2266]\n",
      "name C3L-00993-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00993-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00993-24.h5 took 0.41525840759277344 s\n",
      "features size:  (27, 1024)\n",
      "coordinates size:  (27, 2)\n",
      "\n",
      "progress: 78/206\n",
      "C3L-01000-24\n",
      "downsample [16.00244499 16.00675676]\n",
      "downsampled_level_dim [2863 1036]\n",
      "level_dim [2863 1036]\n",
      "name C3L-01000-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01000-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01000-24.h5 took 0.35448217391967773 s\n",
      "features size:  (21, 1024)\n",
      "coordinates size:  (21, 2)\n",
      "\n",
      "progress: 79/206\n",
      "C3L-01000-25\n",
      "downsample [16.00602652 16.00121729]\n",
      "downsampled_level_dim [2489 1643]\n",
      "level_dim [2489 1643]\n",
      "name C3L-01000-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01000-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01000-25.h5 took 0.4040699005126953 s\n",
      "features size:  (26, 1024)\n",
      "coordinates size:  (26, 2)\n",
      "\n",
      "progress: 80/206\n",
      "C3L-01285-21\n",
      "downsample [16.00511322 16.00713822]\n",
      "downsampled_level_dim [1369 1541]\n",
      "level_dim [1369 1541]\n",
      "name C3L-01285-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01285-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01285-21.h5 took 0.31719517707824707 s\n",
      "features size:  (11, 1024)\n",
      "coordinates size:  (11, 2)\n",
      "\n",
      "progress: 81/206\n",
      "C3L-01285-22\n",
      "downsample [8.00255661 8.        ]\n",
      "downsampled_level_dim [2738 2391]\n",
      "level_dim [2738 2391]\n",
      "name C3L-01285-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01285-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01285-22.h5 took 0.43665242195129395 s\n",
      "features size:  (33, 1024)\n",
      "coordinates size:  (33, 2)\n",
      "\n",
      "progress: 82/206\n",
      "C3L-01285-23\n",
      "downsample [16.00625    16.00650118]\n",
      "downsampled_level_dim [1120 1692]\n",
      "level_dim [1120 1692]\n",
      "name C3L-01285-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01285-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01285-23.h5 took 0.3050210475921631 s\n",
      "features size:  (12, 1024)\n",
      "coordinates size:  (12, 2)\n",
      "\n",
      "progress: 83/206\n",
      "C3L-01330-21\n",
      "downsample [8.00401837 8.00309119]\n",
      "downsampled_level_dim [1742 1941]\n",
      "level_dim [1742 1941]\n",
      "name C3L-01330-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01330-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01330-21.h5 took 0.37706899642944336 s\n",
      "features size:  (23, 1024)\n",
      "coordinates size:  (23, 2)\n",
      "\n",
      "progress: 84/206\n",
      "C3L-01330-22\n",
      "downsample [8.00255661 8.00045167]\n",
      "downsampled_level_dim [2738 2214]\n",
      "level_dim [2738 2214]\n",
      "name C3L-01330-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01330-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01330-22.h5 took 0.3584463596343994 s\n",
      "features size:  (21, 1024)\n",
      "coordinates size:  (21, 2)\n",
      "\n",
      "progress: 85/206\n",
      "C3L-01330-23\n",
      "downsample [8.00281237 8.00198413]\n",
      "downsampled_level_dim [2489 1512]\n",
      "level_dim [2489 1512]\n",
      "name C3L-01330-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01330-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01330-23.h5 took 0.39415597915649414 s\n",
      "features size:  (25, 1024)\n",
      "coordinates size:  (25, 2)\n",
      "\n",
      "progress: 86/206\n",
      "C3L-01455-21\n",
      "downsample [16.01507538 16.00854139]\n",
      "downsampled_level_dim [ 995 1522]\n",
      "level_dim [ 995 1522]\n",
      "name C3L-01455-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01455-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01455-21.h5 took 0.28600144386291504 s\n",
      "features size:  (7, 1024)\n",
      "coordinates size:  (7, 2)\n",
      "\n",
      "progress: 87/206\n",
      "C3L-01455-22\n",
      "downsample [8.00401837 8.00156801]\n",
      "downsampled_level_dim [1742 2551]\n",
      "level_dim [1742 2551]\n",
      "name C3L-01455-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01455-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01455-22.h5 took 0.35356593132019043 s\n",
      "features size:  (20, 1024)\n",
      "coordinates size:  (20, 2)\n",
      "\n",
      "progress: 88/206\n",
      "C3L-01455-23\n",
      "downsample [16.00511322 16.00406504]\n",
      "downsampled_level_dim [1369 1722]\n",
      "level_dim [1369 1722]\n",
      "name C3L-01455-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01455-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01455-23.h5 took 0.263735294342041 s\n",
      "features size:  (7, 1024)\n",
      "coordinates size:  (7, 2)\n",
      "\n",
      "progress: 89/206\n",
      "C3L-01606-21\n",
      "downsample [16.00511322 16.0077381 ]\n",
      "downsampled_level_dim [1369 1680]\n",
      "level_dim [1369 1680]\n",
      "name C3L-01606-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01606-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01606-21.h5 took 0.30367398262023926 s\n",
      "features size:  (13, 1024)\n",
      "coordinates size:  (13, 2)\n",
      "\n",
      "progress: 90/206\n",
      "C3L-01632-21\n",
      "downsample [16.00511322 16.00722022]\n",
      "downsampled_level_dim [1369 1939]\n",
      "level_dim [1369 1939]\n",
      "name C3L-01632-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01632-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01632-21.h5 took 0.3331880569458008 s\n",
      "features size:  (17, 1024)\n",
      "coordinates size:  (17, 2)\n",
      "\n",
      "progress: 91/206\n",
      "C3L-01632-22\n",
      "downsample [16.00511322 16.00516432]\n",
      "downsampled_level_dim [1369 2130]\n",
      "level_dim [1369 2130]\n",
      "name C3L-01632-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01632-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01632-22.h5 took 0.36768269538879395 s\n",
      "features size:  (23, 1024)\n",
      "coordinates size:  (23, 2)\n",
      "\n",
      "progress: 92/206\n",
      "C3L-01632-23\n",
      "downsample [8.00351582 8.00214516]\n",
      "downsampled_level_dim [1991 2797]\n",
      "level_dim [1991 2797]\n",
      "name C3L-01632-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01632-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01632-23.h5 took 0.4650604724884033 s\n",
      "features size:  (36, 1024)\n",
      "coordinates size:  (36, 2)\n",
      "\n",
      "progress: 93/206\n",
      "C3L-01663-22\n",
      "downsample [8.00281237 8.00141543]\n",
      "downsampled_level_dim [2489 2826]\n",
      "level_dim [2489 2826]\n",
      "name C3L-01663-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01663-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01663-22.h5 took 0.4376039505004883 s\n",
      "features size:  (30, 1024)\n",
      "coordinates size:  (30, 2)\n",
      "\n",
      "progress: 94/206\n",
      "C3L-01682-21\n",
      "downsample [8.00281237 8.        ]\n",
      "downsampled_level_dim [2489 2257]\n",
      "level_dim [2489 2257]\n",
      "name C3L-01682-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01682-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01682-21.h5 took 0.43923306465148926 s\n",
      "features size:  (32, 1024)\n",
      "coordinates size:  (32, 2)\n",
      "\n",
      "progress: 95/206\n",
      "C3L-01682-23\n",
      "downsample [8.00281237 8.00185701]\n",
      "downsampled_level_dim [2489 2154]\n",
      "level_dim [2489 2154]\n",
      "name C3L-01682-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01682-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01682-23.h5 took 0.334550142288208 s\n",
      "features size:  (17, 1024)\n",
      "coordinates size:  (17, 2)\n",
      "\n",
      "progress: 96/206\n",
      "C3L-01683-26\n",
      "downsample [8.00216316 8.00136752]\n",
      "downsampled_level_dim [3236 2925]\n",
      "level_dim [3236 2925]\n",
      "name C3L-01683-26\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01683-26.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01683-26.h5 took 0.5617473125457764 s\n",
      "features size:  (49, 1024)\n",
      "coordinates size:  (49, 2)\n",
      "\n",
      "progress: 97/206\n",
      "C3L-01683-27\n",
      "downsample [8.00255661 8.00039761]\n",
      "downsampled_level_dim [2738 2515]\n",
      "level_dim [2738 2515]\n",
      "name C3L-01683-27\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01683-27.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01683-27.h5 took 0.5131685733795166 s\n",
      "features size:  (39, 1024)\n",
      "coordinates size:  (39, 2)\n",
      "\n",
      "progress: 98/206\n",
      "C3L-01838-23\n",
      "downsample [8.00255661 8.00218103]\n",
      "downsampled_level_dim [2738 2751]\n",
      "level_dim [2738 2751]\n",
      "name C3L-01838-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01838-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01838-23.h5 took 0.4671206474304199 s\n",
      "features size:  (36, 1024)\n",
      "coordinates size:  (36, 2)\n",
      "\n",
      "progress: 99/206\n",
      "C3L-01862-21\n",
      "downsample [8.00200861 8.00168634]\n",
      "downsampled_level_dim [3485 2965]\n",
      "level_dim [3485 2965]\n",
      "name C3L-01862-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01862-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01862-21.h5 took 0.498776912689209 s\n",
      "features size:  (41, 1024)\n",
      "coordinates size:  (41, 2)\n",
      "\n",
      "progress: 100/206\n",
      "C3L-01862-22\n",
      "downsample [8.00281237 8.00116414]\n",
      "downsampled_level_dim [2489 2577]\n",
      "level_dim [2489 2577]\n",
      "name C3L-01862-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01862-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01862-22.h5 took 0.37625885009765625 s\n",
      "features size:  (22, 1024)\n",
      "coordinates size:  (22, 2)\n",
      "\n",
      "progress: 101/206\n",
      "C3L-01862-23\n",
      "downsample [8.00281237 8.00238569]\n",
      "downsampled_level_dim [2489 2515]\n",
      "level_dim [2489 2515]\n",
      "name C3L-01862-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01862-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01862-23.h5 took 0.4274260997772217 s\n",
      "features size:  (31, 1024)\n",
      "coordinates size:  (31, 2)\n",
      "\n",
      "progress: 102/206\n",
      "C3L-01884-21\n",
      "downsample [16.00511322 16.00508906]\n",
      "downsampled_level_dim [1369 1572]\n",
      "level_dim [1369 1572]\n",
      "name C3L-01884-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01884-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01884-21.h5 took 0.2912435531616211 s\n",
      "features size:  (11, 1024)\n",
      "coordinates size:  (11, 2)\n",
      "\n",
      "progress: 103/206\n",
      "C3L-01884-22\n",
      "downsample [8.00255661 8.00037608]\n",
      "downsampled_level_dim [2738 2659]\n",
      "level_dim [2738 2659]\n",
      "name C3L-01884-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01884-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01884-22.h5 took 0.378248929977417 s\n",
      "features size:  (22, 1024)\n",
      "coordinates size:  (22, 2)\n",
      "\n",
      "progress: 104/206\n",
      "C3L-01889-21\n",
      "downsample [16.01004689 16.        ]\n",
      "downsampled_level_dim [1493 1546]\n",
      "level_dim [1493 1546]\n",
      "name C3L-01889-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01889-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01889-21.h5 took 0.27780771255493164 s\n",
      "features size:  (9, 1024)\n",
      "coordinates size:  (9, 2)\n",
      "\n",
      "progress: 105/206\n",
      "C3L-01890-22\n",
      "downsample [8.00255661 8.00084424]\n",
      "downsampled_level_dim [2738 2369]\n",
      "level_dim [2738 2369]\n",
      "name C3L-01890-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01890-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01890-22.h5 took 0.4135415554046631 s\n",
      "features size:  (29, 1024)\n",
      "coordinates size:  (29, 2)\n",
      "\n",
      "progress: 106/206\n",
      "C3L-01924-21\n",
      "downsample [8.         8.00141643]\n",
      "downsampled_level_dim [2378 2824]\n",
      "level_dim [2378 2824]\n",
      "name C3L-01924-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01924-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01924-21.h5 took 0.6751134395599365 s\n",
      "features size:  (58, 1024)\n",
      "coordinates size:  (58, 2)\n",
      "\n",
      "progress: 107/206\n",
      "C3L-01924-22\n",
      "downsample [16.00337648 16.        ]\n",
      "downsampled_level_dim [1777 1682]\n",
      "level_dim [1777 1682]\n",
      "name C3L-01924-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01924-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01924-22.h5 took 0.433671236038208 s\n",
      "features size:  (28, 1024)\n",
      "coordinates size:  (28, 2)\n",
      "\n",
      "progress: 108/206\n",
      "C3L-01924-23\n",
      "downsample [8.00317662 8.00199283]\n",
      "downsampled_level_dim [1574 2509]\n",
      "level_dim [1574 2509]\n",
      "name C3L-01924-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01924-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01924-23.h5 took 0.5621843338012695 s\n",
      "features size:  (43, 1024)\n",
      "coordinates size:  (43, 2)\n",
      "\n",
      "progress: 109/206\n",
      "C3L-01924-24\n",
      "downsample [16.00555942 16.00451031]\n",
      "downsampled_level_dim [1439 1552]\n",
      "level_dim [1439 1552]\n",
      "name C3L-01924-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01924-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01924-24.h5 took 0.40117979049682617 s\n",
      "features size:  (24, 1024)\n",
      "coordinates size:  (24, 2)\n",
      "\n",
      "progress: 110/206\n",
      "C3L-01924-25\n",
      "downsample [8.000342   8.00234192]\n",
      "downsampled_level_dim [2924 2562]\n",
      "level_dim [2924 2562]\n",
      "name C3L-01924-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01924-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01924-25.h5 took 0.7740817070007324 s\n",
      "features size:  (68, 1024)\n",
      "coordinates size:  (68, 2)\n",
      "\n",
      "progress: 111/206\n",
      "C3L-02127-21\n",
      "downsample [16.00317125 16.0043573 ]\n",
      "downsampled_level_dim [4730 1836]\n",
      "level_dim [4730 1836]\n",
      "name C3L-02127-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02127-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02127-21.h5 took 0.5822112560272217 s\n",
      "features size:  (39, 1024)\n",
      "coordinates size:  (39, 2)\n",
      "\n",
      "progress: 112/206\n",
      "C3L-02127-22\n",
      "downsample [16.00137147 16.00526039]\n",
      "downsampled_level_dim [5104 1901]\n",
      "level_dim [5104 1901]\n",
      "name C3L-02127-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02127-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02127-22.h5 took 0.6340057849884033 s\n",
      "features size:  (53, 1024)\n",
      "coordinates size:  (53, 2)\n",
      "\n",
      "progress: 113/206\n",
      "C3L-02127-23\n",
      "downsample [16.00181394 16.01074444]\n",
      "downsampled_level_dim [3859 1303]\n",
      "level_dim [3859 1303]\n",
      "name C3L-02127-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02127-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02127-23.h5 took 0.42826390266418457 s\n",
      "features size:  (30, 1024)\n",
      "coordinates size:  (30, 2)\n",
      "\n",
      "progress: 114/206\n",
      "C3L-02127-24\n",
      "downsample [16.00301265 16.00874636]\n",
      "downsampled_level_dim [4979 1715]\n",
      "level_dim [4979 1715]\n",
      "name C3L-02127-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02127-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02127-24.h5 took 0.5889055728912354 s\n",
      "features size:  (47, 1024)\n",
      "coordinates size:  (47, 2)\n",
      "\n",
      "progress: 115/206\n",
      "C3L-02130-21\n",
      "downsample [16.00261963 16.00583885]\n",
      "downsampled_level_dim [5726 2569]\n",
      "level_dim [5726 2569]\n",
      "name C3L-02130-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02130-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02130-21.h5 took 0.9192636013031006 s\n",
      "features size:  (89, 1024)\n",
      "coordinates size:  (89, 2)\n",
      "\n",
      "progress: 116/206\n",
      "C3L-02130-22\n",
      "downsample [16.00273873 16.00613766]\n",
      "downsampled_level_dim [5477 2281]\n",
      "level_dim [5477 2281]\n",
      "name C3L-02130-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02130-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02130-22.h5 took 0.7468752861022949 s\n",
      "features size:  (66, 1024)\n",
      "coordinates size:  (66, 2)\n",
      "\n",
      "progress: 117/206\n",
      "C3L-02130-23\n",
      "downsample [16.00119638 16.00409836]\n",
      "downsampled_level_dim [5851 2196]\n",
      "level_dim [5851 2196]\n",
      "name C3L-02130-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02130-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02130-23.h5 took 0.8334052562713623 s\n",
      "features size:  (83, 1024)\n",
      "coordinates size:  (83, 2)\n",
      "\n",
      "progress: 118/206\n",
      "C3L-02130-24\n",
      "downsample [16.00130768 16.        ]\n",
      "downsampled_level_dim [5353 2334]\n",
      "level_dim [5353 2334]\n",
      "name C3L-02130-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02130-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02130-24.h5 took 0.8474154472351074 s\n",
      "features size:  (86, 1024)\n",
      "coordinates size:  (86, 2)\n",
      "\n",
      "progress: 119/206\n",
      "C3L-02164-21\n",
      "downsample [16.00330813 16.00424854]\n",
      "downsampled_level_dim [2116 1883]\n",
      "level_dim [2116 1883]\n",
      "name C3L-02164-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02164-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02164-21.h5 took 0.42244935035705566 s\n",
      "features size:  (29, 1024)\n",
      "coordinates size:  (29, 2)\n",
      "\n",
      "progress: 120/206\n",
      "C3L-02164-22\n",
      "downsample [16.0075339  16.01081471]\n",
      "downsampled_level_dim [1991 1387]\n",
      "level_dim [1991 1387]\n",
      "name C3L-02164-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02164-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02164-22.h5 took 0.40438199043273926 s\n",
      "features size:  (26, 1024)\n",
      "coordinates size:  (26, 2)\n",
      "\n",
      "progress: 121/206\n",
      "C3L-02164-23\n",
      "downsample [16.00295983 16.00851393]\n",
      "downsampled_level_dim [2365 1292]\n",
      "level_dim [2365 1292]\n",
      "name C3L-02164-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02164-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02164-23.h5 took 0.43347883224487305 s\n",
      "features size:  (32, 1024)\n",
      "coordinates size:  (32, 2)\n",
      "\n",
      "progress: 122/206\n",
      "C3L-02165-21\n",
      "downsample [16.00215177 16.00697211]\n",
      "downsampled_level_dim [6971 2008]\n",
      "level_dim [6971 2008]\n",
      "name C3L-02165-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02165-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02165-21.h5 took 1.1048107147216797 s\n",
      "features size:  (116, 1024)\n",
      "coordinates size:  (116, 2)\n",
      "\n",
      "progress: 123/206\n",
      "C3L-02165-22\n",
      "downsample [16.00231732 16.00125549]\n",
      "downsampled_level_dim [6473 1593]\n",
      "level_dim [6473 1593]\n",
      "name C3L-02165-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02165-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02165-22.h5 took 1.133815050125122 s\n",
      "features size:  (86, 1024)\n",
      "coordinates size:  (86, 2)\n",
      "\n",
      "progress: 124/206\n",
      "C3L-02165-23\n",
      "downsample [16.00224936 16.0020284 ]\n",
      "downsampled_level_dim [3112 1479]\n",
      "level_dim [3112 1479]\n",
      "name C3L-02165-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02165-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02165-23.h5 took 0.5211613178253174 s\n",
      "features size:  (43, 1024)\n",
      "coordinates size:  (43, 2)\n",
      "\n",
      "progress: 125/206\n",
      "C3L-02168-21\n",
      "downsample [16.00144181 16.00334672]\n",
      "downsampled_level_dim [4855 1494]\n",
      "level_dim [4855 1494]\n",
      "name C3L-02168-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02168-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02168-21.h5 took 0.6254942417144775 s\n",
      "features size:  (52, 1024)\n",
      "coordinates size:  (52, 2)\n",
      "\n",
      "progress: 126/206\n",
      "C3L-02168-22\n",
      "downsample [16.00286917 16.00545384]\n",
      "downsampled_level_dim [5228 2567]\n",
      "level_dim [5228 2567]\n",
      "name C3L-02168-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02168-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02168-22.h5 took 1.0032835006713867 s\n",
      "features size:  (96, 1024)\n",
      "coordinates size:  (96, 2)\n",
      "\n",
      "progress: 127/206\n",
      "C3L-02168-23\n",
      "downsample [16.00273873 16.00376952]\n",
      "downsampled_level_dim [5477 1857]\n",
      "level_dim [5477 1857]\n",
      "name C3L-02168-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02168-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02168-23.h5 took 0.978339672088623 s\n",
      "features size:  (103, 1024)\n",
      "coordinates size:  (103, 2)\n",
      "\n",
      "progress: 128/206\n",
      "C3L-02170-21\n",
      "downsample [16.00144181 16.00825959]\n",
      "downsampled_level_dim [4855 1695]\n",
      "level_dim [4855 1695]\n",
      "name C3L-02170-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02170-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02170-21.h5 took 0.679572343826294 s\n",
      "features size:  (61, 1024)\n",
      "coordinates size:  (61, 2)\n",
      "\n",
      "progress: 129/206\n",
      "C3L-02170-22\n",
      "downsample [16.00317125 16.00482057]\n",
      "downsampled_level_dim [4730 1867]\n",
      "level_dim [4730 1867]\n",
      "name C3L-02170-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02170-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02170-22.h5 took 0.774327278137207 s\n",
      "features size:  (76, 1024)\n",
      "coordinates size:  (76, 2)\n",
      "\n",
      "progress: 130/206\n",
      "C3L-02170-23\n",
      "downsample [16.00130768 16.        ]\n",
      "downsampled_level_dim [5353 2308]\n",
      "level_dim [5353 2308]\n",
      "name C3L-02170-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02170-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02170-23.h5 took 0.9350957870483398 s\n",
      "features size:  (98, 1024)\n",
      "coordinates size:  (98, 2)\n",
      "\n",
      "progress: 131/206\n",
      "C3L-02219-21\n",
      "downsample [8.00200861 8.00173491]\n",
      "downsampled_level_dim [3485 2882]\n",
      "level_dim [3485 2882]\n",
      "name C3L-02219-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02219-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02219-21.h5 took 0.5229077339172363 s\n",
      "features size:  (44, 1024)\n",
      "coordinates size:  (44, 2)\n",
      "\n",
      "progress: 132/206\n",
      "C3L-02219-22\n",
      "downsample [8.003125 8.      ]\n",
      "downsampled_level_dim [2240 1949]\n",
      "level_dim [2240 1949]\n",
      "name C3L-02219-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02219-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02219-22.h5 took 0.35743165016174316 s\n",
      "features size:  (20, 1024)\n",
      "coordinates size:  (20, 2)\n",
      "\n",
      "progress: 133/206\n",
      "C3L-02345-21\n",
      "downsample [16.01004689 16.00795229]\n",
      "downsampled_level_dim [1493 1509]\n",
      "level_dim [1493 1509]\n",
      "name C3L-02345-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02345-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02345-21.h5 took 0.32340383529663086 s\n",
      "features size:  (14, 1024)\n",
      "coordinates size:  (14, 2)\n",
      "\n",
      "progress: 134/206\n",
      "C3L-02345-22\n",
      "downsample [8.00234349 8.00079618]\n",
      "downsampled_level_dim [2987 2512]\n",
      "level_dim [2987 2512]\n",
      "name C3L-02345-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02345-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02345-22.h5 took 0.5931880474090576 s\n",
      "features size:  (55, 1024)\n",
      "coordinates size:  (55, 2)\n",
      "\n",
      "progress: 135/206\n",
      "C3L-02345-23\n",
      "downsample [16.00511322 16.        ]\n",
      "downsampled_level_dim [1369 1697]\n",
      "level_dim [1369 1697]\n",
      "name C3L-02345-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02345-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02345-23.h5 took 0.3210892677307129 s\n",
      "features size:  (14, 1024)\n",
      "coordinates size:  (14, 2)\n",
      "\n",
      "progress: 136/206\n",
      "C3L-02348-21\n",
      "downsample [16.0075339  16.00690131]\n",
      "downsampled_level_dim [1991 1449]\n",
      "level_dim [1991 1449]\n",
      "name C3L-02348-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02348-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02348-21.h5 took 0.34625887870788574 s\n",
      "features size:  (18, 1024)\n",
      "coordinates size:  (18, 2)\n",
      "\n",
      "progress: 137/206\n",
      "C3L-02348-22\n",
      "downsample [16.00330813 16.        ]\n",
      "downsampled_level_dim [2116 1728]\n",
      "level_dim [2116 1728]\n",
      "name C3L-02348-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02348-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02348-22.h5 took 0.35877346992492676 s\n",
      "features size:  (20, 1024)\n",
      "coordinates size:  (20, 2)\n",
      "\n",
      "progress: 138/206\n",
      "C3L-02349-21\n",
      "downsample [8.00216316 8.00107488]\n",
      "downsampled_level_dim [3236 2791]\n",
      "level_dim [3236 2791]\n",
      "name C3L-02349-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02349-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02349-21.h5 took 0.5740976333618164 s\n",
      "features size:  (54, 1024)\n",
      "coordinates size:  (54, 2)\n",
      "\n",
      "progress: 139/206\n",
      "C3L-02358-21\n",
      "downsample [16.00374933 16.00356506]\n",
      "downsampled_level_dim [1867 1683]\n",
      "level_dim [1867 1683]\n",
      "name C3L-02358-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02358-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02358-21.h5 took 0.3322775363922119 s\n",
      "features size:  (17, 1024)\n",
      "coordinates size:  (17, 2)\n",
      "\n",
      "progress: 140/206\n",
      "C3L-02358-22\n",
      "downsample [8.00281237 8.0011592 ]\n",
      "downsampled_level_dim [2489 2588]\n",
      "level_dim [2489 2588]\n",
      "name C3L-02358-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02358-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02358-22.h5 took 0.494426965713501 s\n",
      "features size:  (37, 1024)\n",
      "coordinates size:  (37, 2)\n",
      "\n",
      "progress: 141/206\n",
      "C3L-02358-23\n",
      "downsample [16.01004689 16.        ]\n",
      "downsampled_level_dim [1493 1556]\n",
      "level_dim [1493 1556]\n",
      "name C3L-02358-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02358-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02358-23.h5 took 0.32719969749450684 s\n",
      "features size:  (16, 1024)\n",
      "coordinates size:  (16, 2)\n",
      "\n",
      "progress: 142/206\n",
      "C3L-02365-21\n",
      "downsample [8.00216316 8.00036792]\n",
      "downsampled_level_dim [3236 2718]\n",
      "level_dim [3236 2718]\n",
      "name C3L-02365-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02365-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02365-21.h5 took 0.6135783195495605 s\n",
      "features size:  (60, 1024)\n",
      "coordinates size:  (60, 2)\n",
      "\n",
      "progress: 143/206\n",
      "C3L-02365-24\n",
      "downsample [8.00216316 8.00147348]\n",
      "downsampled_level_dim [3236 2036]\n",
      "level_dim [3236 2036]\n",
      "name C3L-02365-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02365-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02365-24.h5 took 0.5019280910491943 s\n",
      "features size:  (41, 1024)\n",
      "coordinates size:  (41, 2)\n",
      "\n",
      "progress: 144/206\n",
      "C3L-02365-25\n",
      "downsample [8.00234349 8.00216216]\n",
      "downsampled_level_dim [2987 1850]\n",
      "level_dim [2987 1850]\n",
      "name C3L-02365-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02365-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02365-25.h5 took 0.4930753707885742 s\n",
      "features size:  (38, 1024)\n",
      "coordinates size:  (38, 2)\n",
      "\n",
      "progress: 145/206\n",
      "C3L-02508-22\n",
      "downsample [8.00234349 8.00078616]\n",
      "downsampled_level_dim [2987 2544]\n",
      "level_dim [2987 2544]\n",
      "name C3L-02508-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02508-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02508-22.h5 took 0.48308873176574707 s\n",
      "features size:  (39, 1024)\n",
      "coordinates size:  (39, 2)\n",
      "\n",
      "progress: 146/206\n",
      "C3L-02508-23\n",
      "downsample [8.00255661 8.00169563]\n",
      "downsampled_level_dim [2738 2359]\n",
      "level_dim [2738 2359]\n",
      "name C3L-02508-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02508-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02508-23.h5 took 0.3756546974182129 s\n",
      "features size:  (23, 1024)\n",
      "coordinates size:  (23, 2)\n",
      "\n",
      "progress: 147/206\n",
      "C3L-02513-21\n",
      "downsample [4.00053591 4.        ]\n",
      "downsampled_level_dim [5598 4335]\n",
      "level_dim [5598 4335]\n",
      "name C3L-02513-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02513-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02513-21.h5 took 1.9194142818450928 s\n",
      "features size:  (242, 1024)\n",
      "coordinates size:  (242, 2)\n",
      "\n",
      "progress: 148/206\n",
      "C3L-02513-22\n",
      "downsample [4.00018975 4.        ]\n",
      "downsampled_level_dim [5270 3855]\n",
      "level_dim [5270 3855]\n",
      "name C3L-02513-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02513-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02513-22.h5 took 1.7218201160430908 s\n",
      "features size:  (212, 1024)\n",
      "coordinates size:  (212, 2)\n",
      "\n",
      "progress: 149/206\n",
      "C3L-02513-23\n",
      "downsample [4.         4.00034638]\n",
      "downsampled_level_dim [4285 5774]\n",
      "level_dim [4285 5774]\n",
      "name C3L-02513-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02513-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02513-23.h5 took 2.1401469707489014 s\n",
      "features size:  (259, 1024)\n",
      "coordinates size:  (259, 2)\n",
      "\n",
      "progress: 150/206\n",
      "C3L-02515-21\n",
      "downsample [4.00031974 4.00013416]\n",
      "downsampled_level_dim [6255 7454]\n",
      "level_dim [6255 7454]\n",
      "name C3L-02515-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02515-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02515-21.h5 took 2.0690362453460693 s\n",
      "features size:  (238, 1024)\n",
      "coordinates size:  (238, 2)\n",
      "\n",
      "progress: 151/206\n",
      "C3L-02515-22\n",
      "downsample [4.00041436 4.00033256]\n",
      "downsampled_level_dim [7240 6014]\n",
      "level_dim [7240 6014]\n",
      "name C3L-02515-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02515-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02515-22.h5 took 2.740746021270752 s\n",
      "features size:  (346, 1024)\n",
      "coordinates size:  (346, 2)\n",
      "\n",
      "progress: 152/206\n",
      "C3L-02515-23\n",
      "downsample [4.00033776 4.00059359]\n",
      "downsampled_level_dim [8882 5054]\n",
      "level_dim [8882 5054]\n",
      "name C3L-02515-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02515-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02515-23.h5 took 3.1587753295898438 s\n",
      "features size:  (393, 1024)\n",
      "coordinates size:  (393, 2)\n",
      "\n",
      "progress: 153/206\n",
      "C3L-02546-21\n",
      "downsample [8.00255661 8.00044944]\n",
      "downsampled_level_dim [2738 2225]\n",
      "level_dim [2738 2225]\n",
      "name C3L-02546-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02546-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02546-21.h5 took 0.42901134490966797 s\n",
      "features size:  (30, 1024)\n",
      "coordinates size:  (30, 2)\n",
      "\n",
      "progress: 154/206\n",
      "C3L-02546-23\n",
      "downsample [8.00281237 8.00202102]\n",
      "downsampled_level_dim [2489 2474]\n",
      "level_dim [2489 2474]\n",
      "name C3L-02546-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02546-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02546-23.h5 took 0.40723681449890137 s\n",
      "features size:  (27, 1024)\n",
      "coordinates size:  (27, 2)\n",
      "\n",
      "progress: 155/206\n",
      "C3L-02549-21\n",
      "downsample [8.00200861 8.00078064]\n",
      "downsampled_level_dim [3485 2562]\n",
      "level_dim [3485 2562]\n",
      "name C3L-02549-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02549-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02549-21.h5 took 0.5234365463256836 s\n",
      "features size:  (45, 1024)\n",
      "coordinates size:  (45, 2)\n",
      "\n",
      "progress: 156/206\n",
      "C3L-02549-22\n",
      "downsample [8.00187467 8.00070947]\n",
      "downsampled_level_dim [3734 2819]\n",
      "level_dim [3734 2819]\n",
      "name C3L-02549-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02549-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02549-22.h5 took 0.5064182281494141 s\n",
      "features size:  (42, 1024)\n",
      "coordinates size:  (42, 2)\n",
      "\n",
      "progress: 157/206\n",
      "C3L-02552-22\n",
      "downsample [8.003125   8.00268302]\n",
      "downsampled_level_dim [2240 2609]\n",
      "level_dim [2240 2609]\n",
      "name C3L-02552-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02552-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02552-22.h5 took 0.4615137577056885 s\n",
      "features size:  (35, 1024)\n",
      "coordinates size:  (35, 2)\n",
      "\n",
      "progress: 158/206\n",
      "C3L-02552-23\n",
      "downsample [8.00281237 8.00301984]\n",
      "downsampled_level_dim [2489 2318]\n",
      "level_dim [2489 2318]\n",
      "name C3L-02552-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02552-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02552-23.h5 took 0.46951818466186523 s\n",
      "features size:  (35, 1024)\n",
      "coordinates size:  (35, 2)\n",
      "\n",
      "progress: 159/206\n",
      "C3L-02560-21\n",
      "downsample [8.00255661 8.00214777]\n",
      "downsampled_level_dim [2738 2328]\n",
      "level_dim [2738 2328]\n",
      "name C3L-02560-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02560-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02560-21.h5 took 0.4426143169403076 s\n",
      "features size:  (32, 1024)\n",
      "coordinates size:  (32, 2)\n",
      "\n",
      "progress: 160/206\n",
      "C3L-02560-22\n",
      "downsample [8.00255661 8.00082237]\n",
      "downsampled_level_dim [2738 2432]\n",
      "level_dim [2738 2432]\n",
      "name C3L-02560-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02560-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02560-22.h5 took 0.45064806938171387 s\n",
      "features size:  (32, 1024)\n",
      "coordinates size:  (32, 2)\n",
      "\n",
      "progress: 161/206\n",
      "C3L-02601-21\n",
      "downsample [8.00281237 8.00283057]\n",
      "downsampled_level_dim [2489 2473]\n",
      "level_dim [2489 2473]\n",
      "name C3L-02601-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02601-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02601-21.h5 took 0.45728063583374023 s\n",
      "features size:  (34, 1024)\n",
      "coordinates size:  (34, 2)\n",
      "\n",
      "progress: 162/206\n",
      "C3L-02616-21\n",
      "downsample [16.00463535 16.00693481]\n",
      "downsampled_level_dim [3236 1442]\n",
      "level_dim [3236 1442]\n",
      "name C3L-02616-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02616-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02616-21.h5 took 0.36746692657470703 s\n",
      "features size:  (21, 1024)\n",
      "coordinates size:  (21, 2)\n",
      "\n",
      "progress: 163/206\n",
      "C3L-02616-22\n",
      "downsample [16.00430416 16.00583942]\n",
      "downsampled_level_dim [3485 2055]\n",
      "level_dim [3485 2055]\n",
      "name C3L-02616-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02616-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02616-22.h5 took 0.4920015335083008 s\n",
      "features size:  (39, 1024)\n",
      "coordinates size:  (39, 2)\n",
      "\n",
      "progress: 164/206\n",
      "C3L-02616-23\n",
      "downsample [16.00401714 16.00429415]\n",
      "downsampled_level_dim [3734 1863]\n",
      "level_dim [3734 1863]\n",
      "name C3L-02616-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02616-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02616-23.h5 took 0.45470523834228516 s\n",
      "features size:  (34, 1024)\n",
      "coordinates size:  (34, 2)\n",
      "\n",
      "progress: 165/206\n",
      "C3L-02616-24\n",
      "downsample [16.00193906 16.00211082]\n",
      "downsampled_level_dim [3610 1895]\n",
      "level_dim [3610 1895]\n",
      "name C3L-02616-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02616-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02616-24.h5 took 0.3997330665588379 s\n",
      "features size:  (26, 1024)\n",
      "coordinates size:  (26, 2)\n",
      "\n",
      "progress: 166/206\n",
      "C3L-02616-25\n",
      "downsample [16.00181394 16.00618876]\n",
      "downsampled_level_dim [3859 1939]\n",
      "level_dim [3859 1939]\n",
      "name C3L-02616-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02616-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02616-25.h5 took 0.5445218086242676 s\n",
      "features size:  (46, 1024)\n",
      "coordinates size:  (46, 2)\n",
      "\n",
      "progress: 167/206\n",
      "C3L-02624-21\n",
      "downsample [16.00401714 16.00199867]\n",
      "downsampled_level_dim [3734 1501]\n",
      "level_dim [3734 1501]\n",
      "name C3L-02624-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02624-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02624-21.h5 took 0.46682286262512207 s\n",
      "features size:  (34, 1024)\n",
      "coordinates size:  (34, 2)\n",
      "\n",
      "progress: 168/206\n",
      "C3L-02624-22\n",
      "downsample [16.00354442 16.00139276]\n",
      "downsampled_level_dim [4232 1436]\n",
      "level_dim [4232 1436]\n",
      "name C3L-02624-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02624-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02624-22.h5 took 0.5717496871948242 s\n",
      "features size:  (42, 1024)\n",
      "coordinates size:  (42, 2)\n",
      "\n",
      "progress: 169/206\n",
      "C3L-02624-23\n",
      "downsample [16.00401714 16.00287081]\n",
      "downsampled_level_dim [3734 2090]\n",
      "level_dim [3734 2090]\n",
      "name C3L-02624-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02624-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02624-23.h5 took 0.5449457168579102 s\n",
      "features size:  (45, 1024)\n",
      "coordinates size:  (45, 2)\n",
      "\n",
      "progress: 170/206\n",
      "C3L-02624-24\n",
      "downsample [16.00354442 16.00395034]\n",
      "downsampled_level_dim [4232 1772]\n",
      "level_dim [4232 1772]\n",
      "name C3L-02624-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02624-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02624-24.h5 took 0.7047221660614014 s\n",
      "features size:  (58, 1024)\n",
      "coordinates size:  (58, 2)\n",
      "\n",
      "progress: 171/206\n",
      "C3L-02624-25\n",
      "downsample [16.00401714 16.00165654]\n",
      "downsampled_level_dim [3734 1811]\n",
      "level_dim [3734 1811]\n",
      "name C3L-02624-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02624-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02624-25.h5 took 0.5598065853118896 s\n",
      "features size:  (50, 1024)\n",
      "coordinates size:  (50, 2)\n",
      "\n",
      "progress: 172/206\n",
      "C3L-02625-21\n",
      "downsample [16.00430416 16.003861  ]\n",
      "downsampled_level_dim [3485 1554]\n",
      "level_dim [3485 1554]\n",
      "name C3L-02625-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02625-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02625-21.h5 took 0.45003390312194824 s\n",
      "features size:  (32, 1024)\n",
      "coordinates size:  (32, 2)\n",
      "\n",
      "progress: 173/206\n",
      "C3L-02625-22\n",
      "downsample [16.00463535 16.00669507]\n",
      "downsampled_level_dim [3236 1643]\n",
      "level_dim [3236 1643]\n",
      "name C3L-02625-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02625-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02625-22.h5 took 0.39313340187072754 s\n",
      "features size:  (25, 1024)\n",
      "coordinates size:  (25, 2)\n",
      "\n",
      "progress: 174/206\n",
      "C3L-02625-23\n",
      "downsample [8.00234349 8.00300946]\n",
      "downsampled_level_dim [2987 2326]\n",
      "level_dim [2987 2326]\n",
      "name C3L-02625-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02625-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02625-23.h5 took 0.5834648609161377 s\n",
      "features size:  (52, 1024)\n",
      "coordinates size:  (52, 2)\n",
      "\n",
      "progress: 175/206\n",
      "C3L-02625-24\n",
      "downsample [16.00432633 16.00604839]\n",
      "downsampled_level_dim [1618 1488]\n",
      "level_dim [1618 1488]\n",
      "name C3L-02625-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02625-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02625-24.h5 took 0.3587825298309326 s\n",
      "features size:  (19, 1024)\n",
      "coordinates size:  (19, 2)\n",
      "\n",
      "progress: 176/206\n",
      "C3L-02625-25\n",
      "downsample [16.00401714 16.        ]\n",
      "downsampled_level_dim [3734 1713]\n",
      "level_dim [3734 1713]\n",
      "name C3L-02625-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02625-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02625-25.h5 took 0.4764413833618164 s\n",
      "features size:  (36, 1024)\n",
      "coordinates size:  (36, 2)\n",
      "\n",
      "progress: 177/206\n",
      "C3L-02627-21\n",
      "downsample [16.00193906 16.0089153 ]\n",
      "downsampled_level_dim [3610 1346]\n",
      "level_dim [3610 1346]\n",
      "name C3L-02627-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02627-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02627-21.h5 took 0.3441803455352783 s\n",
      "features size:  (17, 1024)\n",
      "coordinates size:  (17, 2)\n",
      "\n",
      "progress: 178/206\n",
      "C3L-02627-22\n",
      "downsample [8.00234349 8.00038066]\n",
      "downsampled_level_dim [2987 2627]\n",
      "level_dim [2987 2627]\n",
      "name C3L-02627-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02627-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02627-22.h5 took 0.5030868053436279 s\n",
      "features size:  (39, 1024)\n",
      "coordinates size:  (39, 2)\n",
      "\n",
      "progress: 179/206\n",
      "C3L-02627-23\n",
      "downsample [16.00430416 16.00928218]\n",
      "downsampled_level_dim [3485 1616]\n",
      "level_dim [3485 1616]\n",
      "name C3L-02627-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02627-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02627-23.h5 took 0.36171483993530273 s\n",
      "features size:  (20, 1024)\n",
      "coordinates size:  (20, 2)\n",
      "\n",
      "progress: 180/206\n",
      "C3L-02627-24\n",
      "downsample [8.00234349 8.        ]\n",
      "downsampled_level_dim [2987 2896]\n",
      "level_dim [2987 2896]\n",
      "name C3L-02627-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02627-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02627-24.h5 took 0.52976393699646 s\n",
      "features size:  (45, 1024)\n",
      "coordinates size:  (45, 2)\n",
      "\n",
      "progress: 181/206\n",
      "C3L-02627-25\n",
      "downsample [16.00354442 16.        ]\n",
      "downsampled_level_dim [4232 1700]\n",
      "level_dim [4232 1700]\n",
      "name C3L-02627-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02627-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02627-25.h5 took 0.47921133041381836 s\n",
      "features size:  (31, 1024)\n",
      "coordinates size:  (31, 2)\n",
      "\n",
      "progress: 182/206\n",
      "C3L-02629-21\n",
      "downsample [16.00151976 16.00969619]\n",
      "downsampled_level_dim [4606 1547]\n",
      "level_dim [4606 1547]\n",
      "name C3L-02629-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02629-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02629-21.h5 took 0.51908278465271 s\n",
      "features size:  (36, 1024)\n",
      "coordinates size:  (36, 2)\n",
      "\n",
      "progress: 183/206\n",
      "C3L-02629-22\n",
      "downsample [16.00334747 16.00409836]\n",
      "downsampled_level_dim [4481 1708]\n",
      "level_dim [4481 1708]\n",
      "name C3L-02629-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02629-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02629-22.h5 took 0.53914475440979 s\n",
      "features size:  (34, 1024)\n",
      "coordinates size:  (34, 2)\n",
      "\n",
      "progress: 184/206\n",
      "C3L-02629-23\n",
      "downsample [16.00317125 16.00651702]\n",
      "downsampled_level_dim [4730 1381]\n",
      "level_dim [4730 1381]\n",
      "name C3L-02629-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02629-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02629-23.h5 took 0.5308058261871338 s\n",
      "features size:  (38, 1024)\n",
      "coordinates size:  (38, 2)\n",
      "\n",
      "progress: 185/206\n",
      "C3L-02629-24\n",
      "downsample [16.00151976 16.00461627]\n",
      "downsampled_level_dim [4606 1733]\n",
      "level_dim [4606 1733]\n",
      "name C3L-02629-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02629-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02629-24.h5 took 0.5651462078094482 s\n",
      "features size:  (38, 1024)\n",
      "coordinates size:  (38, 2)\n",
      "\n",
      "progress: 186/206\n",
      "C3L-02629-25\n",
      "downsample [16.00137147 16.0078172 ]\n",
      "downsampled_level_dim [5104 1663]\n",
      "level_dim [5104 1663]\n",
      "name C3L-02629-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02629-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02629-25.h5 took 0.5480403900146484 s\n",
      "features size:  (39, 1024)\n",
      "coordinates size:  (39, 2)\n",
      "\n",
      "progress: 187/206\n",
      "C3L-02648-21\n",
      "downsample [16.00144181 16.00773994]\n",
      "downsampled_level_dim [4855 1938]\n",
      "level_dim [4855 1938]\n",
      "name C3L-02648-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02648-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02648-21.h5 took 0.6071641445159912 s\n",
      "features size:  (50, 1024)\n",
      "coordinates size:  (50, 2)\n",
      "\n",
      "progress: 188/206\n",
      "C3L-02648-22\n",
      "downsample [16.00181394 16.00059666]\n",
      "downsampled_level_dim [3859 1676]\n",
      "level_dim [3859 1676]\n",
      "name C3L-02648-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02648-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02648-22.h5 took 0.5794799327850342 s\n",
      "features size:  (51, 1024)\n",
      "coordinates size:  (51, 2)\n",
      "\n",
      "progress: 189/206\n",
      "C3L-02648-23\n",
      "downsample [16.00334747 16.00554995]\n",
      "downsampled_level_dim [4481 1982]\n",
      "level_dim [4481 1982]\n",
      "name C3L-02648-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02648-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02648-23.h5 took 0.5727694034576416 s\n",
      "features size:  (44, 1024)\n",
      "coordinates size:  (44, 2)\n",
      "\n",
      "progress: 190/206\n",
      "C3L-02648-24\n",
      "downsample [16.00144181 16.        ]\n",
      "downsampled_level_dim [4855 2415]\n",
      "level_dim [4855 2415]\n",
      "name C3L-02648-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02648-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02648-24.h5 took 0.715785026550293 s\n",
      "features size:  (66, 1024)\n",
      "coordinates size:  (66, 2)\n",
      "\n",
      "progress: 191/206\n",
      "C3L-02648-25\n",
      "downsample [16.00430416 16.00503145]\n",
      "downsampled_level_dim [3485 1590]\n",
      "level_dim [3485 1590]\n",
      "name C3L-02648-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02648-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02648-25.h5 took 0.41635990142822266 s\n",
      "features size:  (28, 1024)\n",
      "coordinates size:  (28, 2)\n",
      "\n",
      "progress: 192/206\n",
      "C3L-02649-21\n",
      "downsample [16.00151976 16.0005767 ]\n",
      "downsampled_level_dim [4606 1734]\n",
      "level_dim [4606 1734]\n",
      "name C3L-02649-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02649-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02649-21.h5 took 0.412203311920166 s\n",
      "features size:  (23, 1024)\n",
      "coordinates size:  (23, 2)\n",
      "\n",
      "progress: 193/206\n",
      "C3L-02649-22\n",
      "downsample [16.00151976 16.00135593]\n",
      "downsampled_level_dim [4606 1475]\n",
      "level_dim [4606 1475]\n",
      "name C3L-02649-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02649-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02649-22.h5 took 0.5419468879699707 s\n",
      "features size:  (38, 1024)\n",
      "coordinates size:  (38, 2)\n",
      "\n",
      "progress: 194/206\n",
      "C3L-02649-23\n",
      "downsample [16.00160661 16.00303582]\n",
      "downsampled_level_dim [4357 1647]\n",
      "level_dim [4357 1647]\n",
      "name C3L-02649-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02649-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02649-23.h5 took 0.5671663284301758 s\n",
      "features size:  (36, 1024)\n",
      "coordinates size:  (36, 2)\n",
      "\n",
      "progress: 195/206\n",
      "C3L-02649-24\n",
      "downsample [16.00334747 16.00664251]\n",
      "downsampled_level_dim [4481 1656]\n",
      "level_dim [4481 1656]\n",
      "name C3L-02649-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02649-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02649-24.h5 took 0.5500888824462891 s\n",
      "features size:  (43, 1024)\n",
      "coordinates size:  (43, 2)\n",
      "\n",
      "progress: 196/206\n",
      "C3L-02649-25\n",
      "downsample [16.00334747 16.00228571]\n",
      "downsampled_level_dim [4481 1750]\n",
      "level_dim [4481 1750]\n",
      "name C3L-02649-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02649-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02649-25.h5 took 0.5330057144165039 s\n",
      "features size:  (39, 1024)\n",
      "coordinates size:  (39, 2)\n",
      "\n",
      "progress: 197/206\n",
      "C3L-02661-21\n",
      "downsample [16.00401714 16.00515464]\n",
      "downsampled_level_dim [3734 1940]\n",
      "level_dim [3734 1940]\n",
      "name C3L-02661-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02661-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02661-21.h5 took 0.6033086776733398 s\n",
      "features size:  (55, 1024)\n",
      "coordinates size:  (55, 2)\n",
      "\n",
      "progress: 198/206\n",
      "C3L-02661-22\n",
      "downsample [16.00181394 16.00242248]\n",
      "downsampled_level_dim [3859 2064]\n",
      "level_dim [3859 2064]\n",
      "name C3L-02661-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02661-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02661-22.h5 took 0.5069336891174316 s\n",
      "features size:  (40, 1024)\n",
      "coordinates size:  (40, 2)\n",
      "\n",
      "progress: 199/206\n",
      "C3L-02661-23\n",
      "downsample [16.00224936 16.0042343 ]\n",
      "downsampled_level_dim [3112 1417]\n",
      "level_dim [3112 1417]\n",
      "name C3L-02661-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02661-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02661-23.h5 took 0.4003634452819824 s\n",
      "features size:  (25, 1024)\n",
      "coordinates size:  (25, 2)\n",
      "\n",
      "progress: 200/206\n",
      "C3L-02661-24\n",
      "downsample [16.00170399 16.00846945]\n",
      "downsampled_level_dim [4108 1653]\n",
      "level_dim [4108 1653]\n",
      "name C3L-02661-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02661-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02661-24.h5 took 0.6142513751983643 s\n",
      "features size:  (48, 1024)\n",
      "coordinates size:  (48, 2)\n",
      "\n",
      "progress: 201/206\n",
      "C3L-02661-25\n",
      "downsample [16.00170399 16.00341964]\n",
      "downsampled_level_dim [4108 2047]\n",
      "level_dim [4108 2047]\n",
      "name C3L-02661-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02661-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02661-25.h5 took 0.6101255416870117 s\n",
      "features size:  (48, 1024)\n",
      "coordinates size:  (48, 2)\n",
      "\n",
      "progress: 202/206\n",
      "C3L-02834-21\n",
      "downsample [16.00861079 16.00839329]\n",
      "downsampled_level_dim [1742 1668]\n",
      "level_dim [1742 1668]\n",
      "name C3L-02834-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02834-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02834-21.h5 took 0.3557136058807373 s\n",
      "features size:  (15, 1024)\n",
      "coordinates size:  (15, 2)\n",
      "\n",
      "progress: 203/206\n",
      "C3L-02834-22\n",
      "downsample [16.00432633 16.00540541]\n",
      "downsampled_level_dim [1618 1850]\n",
      "level_dim [1618 1850]\n",
      "name C3L-02834-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02834-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02834-22.h5 took 0.33124494552612305 s\n",
      "features size:  (15, 1024)\n",
      "coordinates size:  (15, 2)\n",
      "\n",
      "progress: 204/206\n",
      "C3L-02834-23\n",
      "downsample [8.00255661 8.        ]\n",
      "downsampled_level_dim [2738 2649]\n",
      "level_dim [2738 2649]\n",
      "name C3L-02834-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02834-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02834-23.h5 took 0.45558977127075195 s\n",
      "features size:  (33, 1024)\n",
      "coordinates size:  (33, 2)\n",
      "\n",
      "progress: 205/206\n",
      "C3L-02891-21\n",
      "downsample [8.003125  8.0029254]\n",
      "downsampled_level_dim [2240 2051]\n",
      "level_dim [2240 2051]\n",
      "name C3L-02891-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02891-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02891-21.h5 took 0.44001245498657227 s\n",
      "features size:  (31, 1024)\n",
      "coordinates size:  (31, 2)\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python extract_features_fp.py --data_h5_dir /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2 \\\n",
    "--data_slide_dir /home/sci/Disk2/CPTAC-LUNG/WSI \\\n",
    "--csv_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/step2_get_features.csv \\\n",
    "--feat_dir /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2 --batch_size 512 --slide_ext .svs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step3 Create split\n",
    "注意修改Create_split_seq.py文件中的csv路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.csv_gen import * \n",
    "path = r'/home/sci/Disk_data/TCGA-NSCLC/WSI'\n",
    "# sort_csv = pd.read_csv(csv_dir).sort_values('slide_id')\n",
    "result_dir = r'/home/sci/Disk_data/TCGA-NSCLC/RESULTS_DIRECTORY/step3_get_splits.csv' ## 5 + 20\n",
    "patch_dir = r'/home/sci/Disk_data/TCGA-NSCLC/RESULTS_DIRECTORY/patches'\n",
    "csv_gen_test(path,result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "label column: label\n",
      "label dictionary: {'LUAD': 0, 'LUSC': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 0    507\n",
      "1    520\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 164\n",
      "Slide-LVL; Number of samples registered in class 0: 507\n",
      "Patient-LVL; Number of samples registered in class 1: 157\n",
      "Slide-LVL; Number of samples registered in class 1: 520\n",
      "\n",
      "number of training samples: 820\n",
      "number of samples in cls 0: 406\n",
      "number of samples in cls 1: 414\n",
      "\n",
      "number of val samples: 110\n",
      "number of samples in cls 0: 52\n",
      "number of samples in cls 1: 58\n",
      "\n",
      "number of test samples: 97\n",
      "number of samples in cls 0: 49\n",
      "number of samples in cls 1: 48\n",
      "\n",
      "\n",
      "\n",
      "number of training samples: 822\n",
      "number of samples in cls 0: 410\n",
      "number of samples in cls 1: 412\n",
      "\n",
      "number of val samples: 109\n",
      "number of samples in cls 0: 46\n",
      "number of samples in cls 1: 63\n",
      "\n",
      "number of test samples: 96\n",
      "number of samples in cls 0: 51\n",
      "number of samples in cls 1: 45\n",
      "\n",
      "\n",
      "\n",
      "number of training samples: 822\n",
      "number of samples in cls 0: 418\n",
      "number of samples in cls 1: 404\n",
      "\n",
      "number of val samples: 92\n",
      "number of samples in cls 0: 38\n",
      "number of samples in cls 1: 54\n",
      "\n",
      "number of test samples: 113\n",
      "number of samples in cls 0: 51\n",
      "number of samples in cls 1: 62\n",
      "\n",
      "\n",
      "\n",
      "number of training samples: 830\n",
      "number of samples in cls 0: 412\n",
      "number of samples in cls 1: 418\n",
      "\n",
      "number of val samples: 97\n",
      "number of samples in cls 0: 47\n",
      "number of samples in cls 1: 50\n",
      "\n",
      "number of test samples: 100\n",
      "number of samples in cls 0: 48\n",
      "number of samples in cls 1: 52\n",
      "\n",
      "\n",
      "\n",
      "number of training samples: 838\n",
      "number of samples in cls 0: 407\n",
      "number of samples in cls 1: 431\n",
      "\n",
      "number of val samples: 90\n",
      "number of samples in cls 0: 44\n",
      "number of samples in cls 1: 46\n",
      "\n",
      "number of test samples: 99\n",
      "number of samples in cls 0: 56\n",
      "number of samples in cls 1: 43\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python create_splits_seq.py --task task_2_tumor_subtyping --seed 1 --label_frac 1 --k 5 --csv_path dataset_csv/cptac_lung_subtyping.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 Train\n",
    "注意修改main.py中的csv路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA_VISIBLE_DEVICES=0 python main.py \\\n",
    "--drop_out \\\n",
    "--early_stopping \\\n",
    "--lr 2e-4 \\\n",
    "--k 10 \\\n",
    "--label_frac 0.75 \\\n",
    "--exp_code task_1_tumor_vs_normal_CLAM_50 --weighted_sample --bag_loss ce --inst_loss svm --task task_1_tumor_vs_normal --model_type clam_sb --log_data \\\n",
    "--data_root_dir /media/yuansh/14THHD/CLAM/FEATURES_DIRECTORY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load Dataset\n",
      "label column: label\n",
      "label dictionary: {'LUAD': 0, 'LUSC': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 0    507\n",
      "1    520\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 164\n",
      "Slide-LVL; Number of samples registered in class 0: 507\n",
      "Patient-LVL; Number of samples registered in class 1: 157\n",
      "Slide-LVL; Number of samples registered in class 1: 520\n",
      "split_dir:  /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/cptac_lung_100\n",
      "################# Settings ###################\n",
      "num_splits:  5\n",
      "k_start:  -1\n",
      "k_end:  -1\n",
      "task:  task_2_tumor_subtyping\n",
      "max_epochs:  200\n",
      "results_dir:  ./results\n",
      "lr:  0.0002\n",
      "experiment:  cptac_lung_100_level0_mil_adam\n",
      "reg:  1e-05\n",
      "label_frac:  1.0\n",
      "bag_loss:  ce\n",
      "seed:  1\n",
      "model_type:  mil\n",
      "model_size:  small\n",
      "use_drop_out:  True\n",
      "weighted_sample:  True\n",
      "opt:  adam\n",
      "split_dir:  /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/cptac_lung_100\n",
      "\n",
      "Training Fold 0!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 820 samples\n",
      "Validating on 110 samples\n",
      "Testing on 97 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Done!\n",
      "MIL_fc(\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 525826\n",
      "Total number of trainable parameters: 525826\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6818, label: 1, bag_size: 4128\n",
      "batch 39, loss: 0.6528, label: 0, bag_size: 1560\n",
      "batch 59, loss: 1.0508, label: 1, bag_size: 9533\n",
      "batch 79, loss: 0.5315, label: 0, bag_size: 16782\n",
      "batch 99, loss: 0.7114, label: 0, bag_size: 11390\n",
      "batch 119, loss: 0.6722, label: 1, bag_size: 15185\n",
      "batch 139, loss: 0.5311, label: 0, bag_size: 11922\n",
      "batch 159, loss: 0.6855, label: 1, bag_size: 11964\n",
      "batch 179, loss: 0.9588, label: 1, bag_size: 7989\n",
      "batch 199, loss: 0.9789, label: 1, bag_size: 11642\n",
      "batch 219, loss: 0.4111, label: 0, bag_size: 1052\n",
      "batch 239, loss: 0.5133, label: 0, bag_size: 20555\n",
      "batch 259, loss: 0.4009, label: 0, bag_size: 2244\n",
      "batch 279, loss: 0.5735, label: 0, bag_size: 890\n",
      "batch 299, loss: 0.6566, label: 0, bag_size: 5409\n",
      "batch 319, loss: 0.5537, label: 1, bag_size: 14433\n",
      "batch 339, loss: 0.5028, label: 1, bag_size: 1038\n",
      "batch 359, loss: 0.5791, label: 0, bag_size: 705\n",
      "batch 379, loss: 0.5226, label: 0, bag_size: 19466\n",
      "batch 399, loss: 0.4973, label: 1, bag_size: 11684\n",
      "batch 419, loss: 0.5023, label: 0, bag_size: 23714\n",
      "batch 439, loss: 0.3495, label: 0, bag_size: 1684\n",
      "batch 459, loss: 0.5029, label: 1, bag_size: 7217\n",
      "batch 479, loss: 0.5905, label: 0, bag_size: 10415\n",
      "batch 499, loss: 0.7250, label: 0, bag_size: 10751\n",
      "batch 519, loss: 0.4037, label: 0, bag_size: 9930\n",
      "batch 539, loss: 0.2425, label: 1, bag_size: 21701\n",
      "batch 559, loss: 0.6770, label: 1, bag_size: 5723\n",
      "batch 579, loss: 0.2968, label: 1, bag_size: 12611\n",
      "batch 599, loss: 1.1325, label: 1, bag_size: 1230\n",
      "batch 619, loss: 0.2039, label: 0, bag_size: 2063\n",
      "batch 639, loss: 0.5092, label: 0, bag_size: 27158\n",
      "batch 659, loss: 0.7305, label: 0, bag_size: 3198\n",
      "batch 679, loss: 0.6867, label: 0, bag_size: 10490\n",
      "batch 699, loss: 1.0698, label: 0, bag_size: 18738\n",
      "batch 719, loss: 0.5962, label: 1, bag_size: 8216\n",
      "batch 739, loss: 0.2825, label: 1, bag_size: 9877\n",
      "batch 759, loss: 0.2634, label: 1, bag_size: 9408\n",
      "batch 779, loss: 0.1863, label: 0, bag_size: 8372\n",
      "batch 799, loss: 0.4088, label: 0, bag_size: 11512\n",
      "batch 819, loss: 0.4462, label: 1, bag_size: 11195\n",
      "Epoch: 0, train_loss: 0.6005, train_error: 0.3195\n",
      "class 0: acc 0.7692307692307693, correct 340/442\n",
      "class 1: acc 0.5767195767195767, correct 218/378\n",
      "\n",
      "Val Set, val_loss: 0.5072, val_error: 0.2273, auc: 0.9586\n",
      "class 0: acc 1.0, correct 52/52\n",
      "class 1: acc 0.5689655172413793, correct 33/58\n",
      "Validation loss decreased (inf --> 0.507219).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6283, label: 1, bag_size: 5629\n",
      "batch 39, loss: 0.5199, label: 0, bag_size: 2270\n",
      "batch 59, loss: 0.1492, label: 1, bag_size: 6090\n",
      "batch 79, loss: 0.6937, label: 0, bag_size: 7835\n",
      "batch 99, loss: 0.0807, label: 1, bag_size: 9955\n",
      "batch 119, loss: 0.7549, label: 1, bag_size: 10622\n",
      "batch 139, loss: 1.3564, label: 1, bag_size: 5256\n",
      "batch 159, loss: 0.7216, label: 0, bag_size: 1458\n",
      "batch 179, loss: 0.6912, label: 0, bag_size: 8866\n",
      "batch 199, loss: 0.3392, label: 0, bag_size: 1437\n",
      "batch 219, loss: 0.0992, label: 1, bag_size: 18794\n",
      "batch 239, loss: 0.4567, label: 0, bag_size: 2213\n",
      "batch 259, loss: 0.5601, label: 0, bag_size: 6356\n",
      "batch 279, loss: 0.2470, label: 0, bag_size: 8948\n",
      "batch 299, loss: 0.2684, label: 1, bag_size: 6927\n",
      "batch 319, loss: 1.1042, label: 0, bag_size: 2815\n",
      "batch 339, loss: 0.1418, label: 0, bag_size: 2244\n",
      "batch 359, loss: 0.1103, label: 0, bag_size: 11865\n",
      "batch 379, loss: 1.2736, label: 1, bag_size: 2682\n",
      "batch 399, loss: 0.2799, label: 1, bag_size: 6606\n",
      "batch 419, loss: 0.1970, label: 1, bag_size: 5231\n",
      "batch 439, loss: 0.0612, label: 0, bag_size: 1760\n",
      "batch 459, loss: 0.1785, label: 0, bag_size: 11122\n",
      "batch 479, loss: 0.4349, label: 1, bag_size: 11421\n",
      "batch 499, loss: 1.1167, label: 0, bag_size: 6850\n",
      "batch 519, loss: 0.3352, label: 1, bag_size: 13947\n",
      "batch 539, loss: 0.2710, label: 0, bag_size: 1213\n",
      "batch 559, loss: 1.5451, label: 1, bag_size: 12340\n",
      "batch 579, loss: 0.2125, label: 0, bag_size: 21093\n",
      "batch 599, loss: 0.1299, label: 0, bag_size: 12149\n",
      "batch 619, loss: 0.1063, label: 0, bag_size: 8812\n",
      "batch 639, loss: 0.3414, label: 0, bag_size: 2367\n",
      "batch 659, loss: 0.6507, label: 0, bag_size: 9069\n",
      "batch 679, loss: 0.1457, label: 0, bag_size: 2322\n",
      "batch 699, loss: 0.7826, label: 0, bag_size: 15747\n",
      "batch 719, loss: 0.1490, label: 0, bag_size: 8981\n",
      "batch 739, loss: 0.1943, label: 1, bag_size: 13051\n",
      "batch 759, loss: 1.5889, label: 1, bag_size: 1831\n",
      "batch 779, loss: 0.6574, label: 0, bag_size: 11128\n",
      "batch 799, loss: 0.1136, label: 0, bag_size: 15001\n",
      "batch 819, loss: 0.6418, label: 1, bag_size: 1764\n",
      "Epoch: 1, train_loss: 0.4389, train_error: 0.1720\n",
      "class 0: acc 0.8747203579418344, correct 391/447\n",
      "class 1: acc 0.7721179624664879, correct 288/373\n",
      "\n",
      "Val Set, val_loss: 0.2945, val_error: 0.0727, auc: 0.9751\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "Validation loss decreased (0.507219 --> 0.294473).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0670, label: 1, bag_size: 13194\n",
      "batch 39, loss: 0.4127, label: 1, bag_size: 6343\n",
      "batch 59, loss: 0.5586, label: 0, bag_size: 3228\n",
      "batch 79, loss: 0.4802, label: 1, bag_size: 2522\n",
      "batch 99, loss: 0.0888, label: 0, bag_size: 1052\n",
      "batch 119, loss: 0.0387, label: 1, bag_size: 13194\n",
      "batch 139, loss: 0.0961, label: 1, bag_size: 5864\n",
      "batch 159, loss: 0.1417, label: 0, bag_size: 11146\n",
      "batch 179, loss: 0.4993, label: 0, bag_size: 10146\n",
      "batch 199, loss: 0.1171, label: 1, bag_size: 16034\n",
      "batch 219, loss: 0.2040, label: 0, bag_size: 2760\n",
      "batch 239, loss: 0.3588, label: 1, bag_size: 9404\n",
      "batch 259, loss: 0.8157, label: 1, bag_size: 2935\n",
      "batch 279, loss: 0.4176, label: 0, bag_size: 5009\n",
      "batch 299, loss: 0.1681, label: 0, bag_size: 10381\n",
      "batch 319, loss: 0.4971, label: 0, bag_size: 2006\n",
      "batch 339, loss: 0.0682, label: 1, bag_size: 6792\n",
      "batch 359, loss: 0.2706, label: 0, bag_size: 763\n",
      "batch 379, loss: 0.1234, label: 0, bag_size: 19067\n",
      "batch 399, loss: 0.1574, label: 0, bag_size: 15672\n",
      "batch 419, loss: 1.2907, label: 1, bag_size: 2937\n",
      "batch 439, loss: 0.2314, label: 0, bag_size: 15672\n",
      "batch 459, loss: 0.4202, label: 0, bag_size: 8330\n",
      "batch 479, loss: 0.2234, label: 1, bag_size: 11394\n",
      "batch 499, loss: 0.2036, label: 0, bag_size: 13591\n",
      "batch 519, loss: 0.0292, label: 0, bag_size: 21082\n",
      "batch 539, loss: 0.7965, label: 1, bag_size: 1255\n",
      "batch 559, loss: 0.0148, label: 1, bag_size: 18794\n",
      "batch 579, loss: 2.1085, label: 1, bag_size: 3121\n",
      "batch 599, loss: 0.1457, label: 1, bag_size: 6731\n",
      "batch 619, loss: 0.5613, label: 1, bag_size: 1064\n",
      "batch 639, loss: 0.3345, label: 1, bag_size: 1230\n",
      "batch 659, loss: 0.0384, label: 1, bag_size: 6736\n",
      "batch 679, loss: 0.0814, label: 1, bag_size: 7119\n",
      "batch 699, loss: 0.1385, label: 1, bag_size: 865\n",
      "batch 719, loss: 0.0728, label: 1, bag_size: 9878\n",
      "batch 739, loss: 0.0339, label: 0, bag_size: 23791\n",
      "batch 759, loss: 1.3738, label: 0, bag_size: 6356\n",
      "batch 779, loss: 0.2280, label: 1, bag_size: 10912\n",
      "batch 799, loss: 0.3569, label: 1, bag_size: 9983\n",
      "batch 819, loss: 0.0095, label: 1, bag_size: 19039\n",
      "Epoch: 2, train_loss: 0.3422, train_error: 0.1232\n",
      "class 0: acc 0.900990099009901, correct 364/404\n",
      "class 1: acc 0.8533653846153846, correct 355/416\n",
      "\n",
      "Val Set, val_loss: 0.2654, val_error: 0.0818, auc: 0.9768\n",
      "class 0: acc 0.8653846153846154, correct 45/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "Validation loss decreased (0.294473 --> 0.265411).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2747, label: 0, bag_size: 2322\n",
      "batch 39, loss: 0.4416, label: 1, bag_size: 21252\n",
      "batch 59, loss: 0.3663, label: 0, bag_size: 3893\n",
      "batch 79, loss: 0.2956, label: 0, bag_size: 15898\n",
      "batch 99, loss: 0.4903, label: 0, bag_size: 1920\n",
      "batch 119, loss: 0.1269, label: 0, bag_size: 11546\n",
      "batch 139, loss: 0.0609, label: 1, bag_size: 20161\n",
      "batch 159, loss: 0.1582, label: 1, bag_size: 7798\n",
      "batch 179, loss: 0.2398, label: 0, bag_size: 12796\n",
      "batch 199, loss: 0.2643, label: 0, bag_size: 2044\n",
      "batch 219, loss: 0.0838, label: 0, bag_size: 1831\n",
      "batch 239, loss: 0.1414, label: 0, bag_size: 1690\n",
      "batch 259, loss: 0.0871, label: 0, bag_size: 1962\n",
      "batch 279, loss: 0.0147, label: 1, bag_size: 4862\n",
      "batch 299, loss: 1.0186, label: 0, bag_size: 7239\n",
      "batch 319, loss: 0.0610, label: 1, bag_size: 689\n",
      "batch 339, loss: 0.6935, label: 1, bag_size: 5256\n",
      "batch 359, loss: 0.9398, label: 1, bag_size: 9942\n",
      "batch 379, loss: 0.1456, label: 0, bag_size: 8788\n",
      "batch 399, loss: 0.5678, label: 1, bag_size: 928\n",
      "batch 419, loss: 0.1222, label: 0, bag_size: 15841\n",
      "batch 439, loss: 0.1108, label: 0, bag_size: 1127\n",
      "batch 459, loss: 0.0609, label: 1, bag_size: 4394\n",
      "batch 479, loss: 0.0359, label: 1, bag_size: 19932\n",
      "batch 499, loss: 0.2752, label: 1, bag_size: 9519\n",
      "batch 519, loss: 0.2966, label: 1, bag_size: 1339\n",
      "batch 539, loss: 0.1179, label: 0, bag_size: 3557\n",
      "batch 559, loss: 0.2129, label: 1, bag_size: 5454\n",
      "batch 579, loss: 0.3848, label: 0, bag_size: 7637\n",
      "batch 599, loss: 0.0178, label: 0, bag_size: 9433\n",
      "batch 619, loss: 0.0747, label: 0, bag_size: 10068\n",
      "batch 639, loss: 0.0731, label: 1, bag_size: 12758\n",
      "batch 659, loss: 0.6387, label: 0, bag_size: 14249\n",
      "batch 679, loss: 0.0798, label: 1, bag_size: 10460\n",
      "batch 699, loss: 0.0249, label: 1, bag_size: 8448\n",
      "batch 719, loss: 0.3240, label: 0, bag_size: 2282\n",
      "batch 739, loss: 0.7381, label: 0, bag_size: 3552\n",
      "batch 759, loss: 1.2488, label: 0, bag_size: 47866\n",
      "batch 779, loss: 0.0338, label: 0, bag_size: 9949\n",
      "batch 799, loss: 0.0242, label: 1, bag_size: 1022\n",
      "batch 819, loss: 0.0323, label: 1, bag_size: 10498\n",
      "Epoch: 3, train_loss: 0.3103, train_error: 0.1171\n",
      "class 0: acc 0.8974358974358975, correct 350/390\n",
      "class 1: acc 0.8697674418604651, correct 374/430\n",
      "\n",
      "Val Set, val_loss: 0.2743, val_error: 0.0909, auc: 0.9728\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0434, label: 1, bag_size: 6752\n",
      "batch 39, loss: 0.3443, label: 1, bag_size: 7389\n",
      "batch 59, loss: 0.0185, label: 1, bag_size: 1101\n",
      "batch 79, loss: 0.1003, label: 1, bag_size: 12178\n",
      "batch 99, loss: 0.0210, label: 0, bag_size: 9885\n",
      "batch 119, loss: 0.0095, label: 0, bag_size: 18154\n",
      "batch 139, loss: 0.0429, label: 1, bag_size: 7669\n",
      "batch 159, loss: 0.0532, label: 0, bag_size: 12148\n",
      "batch 179, loss: 0.0059, label: 1, bag_size: 12349\n",
      "batch 199, loss: 1.0607, label: 1, bag_size: 1294\n",
      "batch 219, loss: 0.1158, label: 0, bag_size: 11151\n",
      "batch 239, loss: 0.0452, label: 0, bag_size: 9060\n",
      "batch 259, loss: 0.2513, label: 1, bag_size: 1493\n",
      "batch 279, loss: 0.7366, label: 1, bag_size: 621\n",
      "batch 299, loss: 0.0491, label: 1, bag_size: 14618\n",
      "batch 319, loss: 0.0811, label: 0, bag_size: 12212\n",
      "batch 339, loss: 0.0521, label: 0, bag_size: 19472\n",
      "batch 359, loss: 0.3779, label: 0, bag_size: 2609\n",
      "batch 379, loss: 0.2474, label: 0, bag_size: 11194\n",
      "batch 399, loss: 0.0366, label: 0, bag_size: 23398\n",
      "batch 419, loss: 1.7901, label: 0, bag_size: 4997\n",
      "batch 439, loss: 0.0231, label: 1, bag_size: 7935\n",
      "batch 459, loss: 0.3442, label: 0, bag_size: 24382\n",
      "batch 479, loss: 0.1990, label: 0, bag_size: 13339\n",
      "batch 499, loss: 0.1500, label: 0, bag_size: 7557\n",
      "batch 519, loss: 0.0874, label: 0, bag_size: 22426\n",
      "batch 539, loss: 0.0150, label: 1, bag_size: 12408\n",
      "batch 559, loss: 0.0511, label: 1, bag_size: 5345\n",
      "batch 579, loss: 0.1270, label: 0, bag_size: 3474\n",
      "batch 599, loss: 0.0552, label: 0, bag_size: 12217\n",
      "batch 619, loss: 0.0245, label: 1, bag_size: 4039\n",
      "batch 639, loss: 0.0130, label: 0, bag_size: 10481\n",
      "batch 659, loss: 0.4278, label: 0, bag_size: 8420\n",
      "batch 679, loss: 0.3226, label: 1, bag_size: 7381\n",
      "batch 699, loss: 0.0976, label: 1, bag_size: 2522\n",
      "batch 719, loss: 0.0719, label: 1, bag_size: 5292\n",
      "batch 739, loss: 0.0819, label: 0, bag_size: 21093\n",
      "batch 759, loss: 0.0432, label: 1, bag_size: 617\n",
      "batch 779, loss: 0.6404, label: 1, bag_size: 1919\n",
      "batch 799, loss: 0.0784, label: 0, bag_size: 12524\n",
      "batch 819, loss: 0.0269, label: 1, bag_size: 1622\n",
      "Epoch: 4, train_loss: 0.3065, train_error: 0.0976\n",
      "class 0: acc 0.9253012048192771, correct 384/415\n",
      "class 1: acc 0.8790123456790123, correct 356/405\n",
      "\n",
      "Val Set, val_loss: 0.2563, val_error: 0.1000, auc: 0.9728\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "Validation loss decreased (0.265411 --> 0.256271).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3500, label: 1, bag_size: 8012\n",
      "batch 39, loss: 0.0665, label: 1, bag_size: 3368\n",
      "batch 59, loss: 0.1611, label: 1, bag_size: 1888\n",
      "batch 79, loss: 0.0448, label: 0, bag_size: 9485\n",
      "batch 99, loss: 1.9704, label: 0, bag_size: 2219\n",
      "batch 119, loss: 0.0869, label: 0, bag_size: 11512\n",
      "batch 139, loss: 0.1272, label: 0, bag_size: 5225\n",
      "batch 159, loss: 0.0377, label: 1, bag_size: 2412\n",
      "batch 179, loss: 0.0430, label: 0, bag_size: 2091\n",
      "batch 199, loss: 0.0221, label: 1, bag_size: 12719\n",
      "batch 219, loss: 0.1808, label: 0, bag_size: 2654\n",
      "batch 239, loss: 0.1898, label: 0, bag_size: 12510\n",
      "batch 259, loss: 0.1090, label: 0, bag_size: 1712\n",
      "batch 279, loss: 0.0030, label: 0, bag_size: 9433\n",
      "batch 299, loss: 0.0673, label: 1, bag_size: 3968\n",
      "batch 319, loss: 0.0885, label: 1, bag_size: 2356\n",
      "batch 339, loss: 0.0784, label: 1, bag_size: 8475\n",
      "batch 359, loss: 0.1090, label: 0, bag_size: 1684\n",
      "batch 379, loss: 0.0170, label: 0, bag_size: 13892\n",
      "batch 399, loss: 0.9427, label: 0, bag_size: 9132\n",
      "batch 419, loss: 0.7592, label: 0, bag_size: 2098\n",
      "batch 439, loss: 0.4059, label: 1, bag_size: 621\n",
      "batch 459, loss: 0.3913, label: 0, bag_size: 3908\n",
      "batch 479, loss: 0.0307, label: 0, bag_size: 31780\n",
      "batch 499, loss: 1.4792, label: 1, bag_size: 1755\n",
      "batch 519, loss: 0.5300, label: 0, bag_size: 21361\n",
      "batch 539, loss: 0.0944, label: 0, bag_size: 1789\n",
      "batch 559, loss: 0.1336, label: 1, bag_size: 5723\n",
      "batch 579, loss: 0.1559, label: 0, bag_size: 3541\n",
      "batch 599, loss: 0.1069, label: 0, bag_size: 2367\n",
      "batch 619, loss: 0.5233, label: 1, bag_size: 21450\n",
      "batch 639, loss: 0.0824, label: 1, bag_size: 10498\n",
      "batch 659, loss: 0.6395, label: 1, bag_size: 20537\n",
      "batch 679, loss: 2.7441, label: 0, bag_size: 17279\n",
      "batch 699, loss: 0.0730, label: 1, bag_size: 621\n",
      "batch 719, loss: 0.0060, label: 0, bag_size: 1984\n",
      "batch 739, loss: 0.2948, label: 1, bag_size: 20537\n",
      "batch 759, loss: 0.1001, label: 0, bag_size: 18225\n",
      "batch 779, loss: 0.0130, label: 1, bag_size: 15213\n",
      "batch 799, loss: 0.0453, label: 0, bag_size: 8145\n",
      "batch 819, loss: 0.0275, label: 0, bag_size: 10415\n",
      "Epoch: 5, train_loss: 0.2815, train_error: 0.1037\n",
      "class 0: acc 0.9114832535885168, correct 381/418\n",
      "class 1: acc 0.8805970149253731, correct 354/402\n",
      "\n",
      "Val Set, val_loss: 0.2145, val_error: 0.0636, auc: 0.9751\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9137931034482759, correct 53/58\n",
      "Validation loss decreased (0.256271 --> 0.214470).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0879, label: 0, bag_size: 1349\n",
      "batch 39, loss: 0.5949, label: 0, bag_size: 3654\n",
      "batch 59, loss: 0.6548, label: 0, bag_size: 3228\n",
      "batch 79, loss: 0.0895, label: 0, bag_size: 21682\n",
      "batch 99, loss: 0.1955, label: 1, bag_size: 9446\n",
      "batch 119, loss: 3.4217, label: 0, bag_size: 5105\n",
      "batch 139, loss: 0.0103, label: 1, bag_size: 6164\n",
      "batch 159, loss: 0.0086, label: 1, bag_size: 10725\n",
      "batch 179, loss: 0.0110, label: 1, bag_size: 2412\n",
      "batch 199, loss: 0.0487, label: 1, bag_size: 617\n",
      "batch 219, loss: 2.1190, label: 0, bag_size: 2815\n",
      "batch 239, loss: 0.1339, label: 0, bag_size: 12510\n",
      "batch 259, loss: 0.0310, label: 1, bag_size: 11421\n",
      "batch 279, loss: 0.0158, label: 1, bag_size: 7110\n",
      "batch 299, loss: 0.1472, label: 1, bag_size: 5292\n",
      "batch 319, loss: 0.1927, label: 1, bag_size: 11223\n",
      "batch 339, loss: 0.0791, label: 1, bag_size: 11220\n",
      "batch 359, loss: 1.5781, label: 0, bag_size: 47866\n",
      "batch 379, loss: 0.0293, label: 0, bag_size: 12910\n",
      "batch 399, loss: 1.9111, label: 1, bag_size: 6360\n",
      "batch 419, loss: 0.6999, label: 0, bag_size: 1506\n",
      "batch 439, loss: 0.0036, label: 1, bag_size: 14618\n",
      "batch 459, loss: 0.0801, label: 1, bag_size: 7389\n",
      "batch 479, loss: 0.1758, label: 0, bag_size: 13332\n",
      "batch 499, loss: 0.2135, label: 0, bag_size: 9252\n",
      "batch 519, loss: 0.0405, label: 0, bag_size: 19466\n",
      "batch 539, loss: 0.0620, label: 1, bag_size: 12178\n",
      "batch 559, loss: 0.1606, label: 0, bag_size: 8549\n",
      "batch 579, loss: 0.4520, label: 1, bag_size: 5366\n",
      "batch 599, loss: 0.0267, label: 1, bag_size: 20333\n",
      "batch 619, loss: 0.3333, label: 0, bag_size: 7428\n",
      "batch 639, loss: 0.0317, label: 1, bag_size: 6343\n",
      "batch 659, loss: 0.9149, label: 1, bag_size: 1095\n",
      "batch 679, loss: 0.0186, label: 1, bag_size: 7669\n",
      "batch 699, loss: 0.0196, label: 1, bag_size: 15609\n",
      "batch 719, loss: 0.2453, label: 0, bag_size: 6367\n",
      "batch 739, loss: 0.0048, label: 1, bag_size: 12931\n",
      "batch 759, loss: 0.2296, label: 0, bag_size: 1458\n",
      "batch 779, loss: 0.6086, label: 0, bag_size: 9132\n",
      "batch 799, loss: 0.2097, label: 1, bag_size: 1759\n",
      "batch 819, loss: 0.3951, label: 1, bag_size: 7246\n",
      "Epoch: 6, train_loss: 0.2510, train_error: 0.0854\n",
      "class 0: acc 0.9177377892030848, correct 357/389\n",
      "class 1: acc 0.9118329466357309, correct 393/431\n",
      "\n",
      "Val Set, val_loss: 0.2014, val_error: 0.0545, auc: 0.9794\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "Validation loss decreased (0.214470 --> 0.201374).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6836, label: 0, bag_size: 23714\n",
      "batch 39, loss: 0.0352, label: 0, bag_size: 1127\n",
      "batch 59, loss: 0.0643, label: 1, bag_size: 1759\n",
      "batch 79, loss: 0.0590, label: 1, bag_size: 14230\n",
      "batch 99, loss: 0.3421, label: 0, bag_size: 5409\n",
      "batch 119, loss: 0.2349, label: 1, bag_size: 9004\n",
      "batch 139, loss: 0.0647, label: 1, bag_size: 5907\n",
      "batch 159, loss: 0.2576, label: 0, bag_size: 7823\n",
      "batch 179, loss: 0.0004, label: 1, bag_size: 11195\n",
      "batch 199, loss: 0.1354, label: 1, bag_size: 3856\n",
      "batch 219, loss: 0.1287, label: 1, bag_size: 1920\n",
      "batch 239, loss: 0.0731, label: 0, bag_size: 16607\n",
      "batch 259, loss: 0.0121, label: 0, bag_size: 16782\n",
      "batch 279, loss: 0.3247, label: 1, bag_size: 5516\n",
      "batch 299, loss: 0.0217, label: 1, bag_size: 14202\n",
      "batch 319, loss: 0.1925, label: 1, bag_size: 11032\n",
      "batch 339, loss: 0.0128, label: 1, bag_size: 5991\n",
      "batch 359, loss: 0.0262, label: 0, bag_size: 11187\n",
      "batch 379, loss: 0.1633, label: 1, bag_size: 10622\n",
      "batch 399, loss: 0.0195, label: 1, bag_size: 3968\n",
      "batch 419, loss: 0.0121, label: 0, bag_size: 16341\n",
      "batch 439, loss: 0.0574, label: 0, bag_size: 21864\n",
      "batch 459, loss: 0.0299, label: 1, bag_size: 13051\n",
      "batch 479, loss: 0.0511, label: 1, bag_size: 3674\n",
      "batch 499, loss: 0.3023, label: 0, bag_size: 2303\n",
      "batch 519, loss: 3.3473, label: 1, bag_size: 3879\n",
      "batch 539, loss: 0.3831, label: 0, bag_size: 11727\n",
      "batch 559, loss: 0.0097, label: 1, bag_size: 8475\n",
      "batch 579, loss: 0.0278, label: 0, bag_size: 10942\n",
      "batch 599, loss: 0.0516, label: 1, bag_size: 18161\n",
      "batch 619, loss: 0.0020, label: 1, bag_size: 1638\n",
      "batch 639, loss: 0.1023, label: 1, bag_size: 10492\n",
      "batch 659, loss: 0.8693, label: 1, bag_size: 9215\n",
      "batch 679, loss: 0.0035, label: 1, bag_size: 19039\n",
      "batch 699, loss: 0.1014, label: 1, bag_size: 11220\n",
      "batch 719, loss: 0.2936, label: 0, bag_size: 4523\n",
      "batch 739, loss: 0.0061, label: 1, bag_size: 15716\n",
      "batch 759, loss: 0.0662, label: 1, bag_size: 1888\n",
      "batch 779, loss: 0.0308, label: 0, bag_size: 10898\n",
      "batch 799, loss: 0.2742, label: 1, bag_size: 11256\n",
      "batch 819, loss: 0.0007, label: 1, bag_size: 5317\n",
      "Epoch: 7, train_loss: 0.2659, train_error: 0.1037\n",
      "class 0: acc 0.898989898989899, correct 356/396\n",
      "class 1: acc 0.8938679245283019, correct 379/424\n",
      "\n",
      "Val Set, val_loss: 0.2305, val_error: 0.1000, auc: 0.9801\n",
      "class 0: acc 0.8076923076923077, correct 42/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0136, label: 1, bag_size: 4394\n",
      "batch 39, loss: 0.5817, label: 1, bag_size: 12626\n",
      "batch 59, loss: 0.3865, label: 1, bag_size: 6360\n",
      "batch 79, loss: 0.0061, label: 0, bag_size: 12217\n",
      "batch 99, loss: 0.0050, label: 1, bag_size: 13255\n",
      "batch 119, loss: 0.0017, label: 1, bag_size: 6090\n",
      "batch 139, loss: 0.1916, label: 0, bag_size: 9069\n",
      "batch 159, loss: 0.0211, label: 1, bag_size: 5723\n",
      "batch 179, loss: 0.0051, label: 1, bag_size: 13947\n",
      "batch 199, loss: 0.0114, label: 1, bag_size: 15332\n",
      "batch 219, loss: 0.1444, label: 1, bag_size: 7768\n",
      "batch 239, loss: 0.0081, label: 0, bag_size: 8372\n",
      "batch 259, loss: 0.3435, label: 1, bag_size: 2522\n",
      "batch 279, loss: 0.0127, label: 1, bag_size: 3968\n",
      "batch 299, loss: 0.0684, label: 0, bag_size: 3670\n",
      "batch 319, loss: 0.0080, label: 0, bag_size: 6624\n",
      "batch 339, loss: 0.0753, label: 1, bag_size: 1493\n",
      "batch 359, loss: 0.0061, label: 0, bag_size: 1415\n",
      "batch 379, loss: 0.9417, label: 0, bag_size: 1714\n",
      "batch 399, loss: 0.0071, label: 1, bag_size: 8019\n",
      "batch 419, loss: 0.0955, label: 0, bag_size: 2160\n",
      "batch 439, loss: 0.1122, label: 1, bag_size: 10460\n",
      "batch 459, loss: 0.0507, label: 1, bag_size: 16267\n",
      "batch 479, loss: 0.0108, label: 0, bag_size: 11512\n",
      "batch 499, loss: 0.0280, label: 0, bag_size: 10068\n",
      "batch 519, loss: 0.0388, label: 0, bag_size: 14625\n",
      "batch 539, loss: 0.1251, label: 0, bag_size: 3265\n",
      "batch 559, loss: 0.0386, label: 0, bag_size: 22870\n",
      "batch 579, loss: 0.7729, label: 1, bag_size: 2344\n",
      "batch 599, loss: 0.0494, label: 0, bag_size: 2322\n",
      "batch 619, loss: 0.9299, label: 1, bag_size: 2314\n",
      "batch 639, loss: 0.0216, label: 0, bag_size: 5551\n",
      "batch 659, loss: 0.0233, label: 1, bag_size: 6736\n",
      "batch 679, loss: 0.3677, label: 0, bag_size: 2920\n",
      "batch 699, loss: 0.0122, label: 0, bag_size: 19518\n",
      "batch 719, loss: 0.0120, label: 0, bag_size: 9851\n",
      "batch 739, loss: 0.1747, label: 0, bag_size: 9485\n",
      "batch 759, loss: 0.0019, label: 1, bag_size: 9078\n",
      "batch 779, loss: 0.0006, label: 1, bag_size: 629\n",
      "batch 799, loss: 0.0001, label: 1, bag_size: 9644\n",
      "batch 819, loss: 0.0321, label: 0, bag_size: 9930\n",
      "Epoch: 8, train_loss: 0.2296, train_error: 0.0866\n",
      "class 0: acc 0.9212410501193318, correct 386/419\n",
      "class 1: acc 0.9052369077306733, correct 363/401\n",
      "\n",
      "Val Set, val_loss: 0.1922, val_error: 0.0636, auc: 0.9788\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "Validation loss decreased (0.201374 --> 0.192198).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1383, label: 0, bag_size: 1498\n",
      "batch 39, loss: 0.0076, label: 0, bag_size: 23791\n",
      "batch 59, loss: 0.1839, label: 1, bag_size: 1822\n",
      "batch 79, loss: 0.0108, label: 1, bag_size: 10969\n",
      "batch 99, loss: 0.0556, label: 1, bag_size: 13015\n",
      "batch 119, loss: 0.0212, label: 0, bag_size: 2179\n",
      "batch 139, loss: 0.0044, label: 0, bag_size: 1052\n",
      "batch 159, loss: 0.0281, label: 1, bag_size: 10072\n",
      "batch 179, loss: 0.1114, label: 0, bag_size: 10415\n",
      "batch 199, loss: 0.0084, label: 1, bag_size: 11600\n",
      "batch 219, loss: 0.0274, label: 1, bag_size: 8754\n",
      "batch 239, loss: 0.1873, label: 0, bag_size: 2760\n",
      "batch 259, loss: 0.0078, label: 0, bag_size: 2282\n",
      "batch 279, loss: 1.3652, label: 0, bag_size: 25814\n",
      "batch 299, loss: 0.7689, label: 0, bag_size: 17279\n",
      "batch 319, loss: 0.0031, label: 0, bag_size: 12732\n",
      "batch 339, loss: 0.0222, label: 1, bag_size: 11394\n",
      "batch 359, loss: 0.0016, label: 1, bag_size: 12865\n",
      "batch 379, loss: 0.2695, label: 1, bag_size: 19972\n",
      "batch 399, loss: 0.0145, label: 0, bag_size: 22828\n",
      "batch 419, loss: 0.0012, label: 1, bag_size: 3409\n",
      "batch 439, loss: 0.0819, label: 1, bag_size: 5292\n",
      "batch 459, loss: 0.0928, label: 0, bag_size: 1920\n",
      "batch 479, loss: 0.0312, label: 0, bag_size: 12524\n",
      "batch 499, loss: 0.0028, label: 1, bag_size: 4039\n",
      "batch 519, loss: 0.0535, label: 0, bag_size: 31106\n",
      "batch 539, loss: 0.8447, label: 1, bag_size: 16514\n",
      "batch 559, loss: 0.0160, label: 1, bag_size: 7873\n",
      "batch 579, loss: 0.0441, label: 0, bag_size: 1127\n",
      "batch 599, loss: 0.0260, label: 1, bag_size: 10460\n",
      "batch 619, loss: 0.0296, label: 1, bag_size: 14681\n",
      "batch 639, loss: 0.0033, label: 1, bag_size: 12795\n",
      "batch 659, loss: 0.1048, label: 1, bag_size: 16890\n",
      "batch 679, loss: 0.1299, label: 1, bag_size: 10501\n",
      "batch 699, loss: 0.0025, label: 1, bag_size: 5494\n",
      "batch 719, loss: 0.4230, label: 0, bag_size: 15898\n",
      "batch 739, loss: 0.0094, label: 0, bag_size: 14333\n",
      "batch 759, loss: 0.0437, label: 0, bag_size: 15841\n",
      "batch 779, loss: 0.0029, label: 1, bag_size: 12931\n",
      "batch 799, loss: 0.5541, label: 0, bag_size: 9387\n",
      "batch 819, loss: 0.0201, label: 1, bag_size: 6731\n",
      "Epoch: 9, train_loss: 0.2333, train_error: 0.0866\n",
      "class 0: acc 0.9209183673469388, correct 361/392\n",
      "class 1: acc 0.9065420560747663, correct 388/428\n",
      "\n",
      "Val Set, val_loss: 0.1827, val_error: 0.0545, auc: 0.9801\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "Validation loss decreased (0.192198 --> 0.182672).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0117, label: 1, bag_size: 14681\n",
      "batch 39, loss: 0.0073, label: 1, bag_size: 12795\n",
      "batch 59, loss: 1.5340, label: 1, bag_size: 2937\n",
      "batch 79, loss: 0.1457, label: 0, bag_size: 2534\n",
      "batch 99, loss: 0.0124, label: 0, bag_size: 13777\n",
      "batch 119, loss: 0.0141, label: 0, bag_size: 16720\n",
      "batch 139, loss: 0.0001, label: 1, bag_size: 629\n",
      "batch 159, loss: 0.0469, label: 0, bag_size: 2844\n",
      "batch 179, loss: 0.1678, label: 0, bag_size: 2998\n",
      "batch 199, loss: 0.0767, label: 0, bag_size: 2360\n",
      "batch 219, loss: 0.0396, label: 0, bag_size: 11778\n",
      "batch 239, loss: 3.5862, label: 0, bag_size: 3897\n",
      "batch 259, loss: 0.0144, label: 0, bag_size: 9888\n",
      "batch 279, loss: 0.9874, label: 0, bag_size: 7835\n",
      "batch 299, loss: 0.0087, label: 0, bag_size: 21076\n",
      "batch 319, loss: 0.0061, label: 1, bag_size: 2412\n",
      "batch 339, loss: 0.0113, label: 0, bag_size: 1962\n",
      "batch 359, loss: 0.4663, label: 0, bag_size: 3783\n",
      "batch 379, loss: 0.0537, label: 1, bag_size: 2455\n",
      "batch 399, loss: 0.0038, label: 1, bag_size: 11387\n",
      "batch 419, loss: 0.0004, label: 0, bag_size: 10481\n",
      "batch 439, loss: 0.0039, label: 1, bag_size: 5731\n",
      "batch 459, loss: 0.1146, label: 0, bag_size: 14249\n",
      "batch 479, loss: 0.4910, label: 1, bag_size: 15125\n",
      "batch 499, loss: 0.1501, label: 0, bag_size: 16211\n",
      "batch 519, loss: 0.0040, label: 0, bag_size: 1483\n",
      "batch 539, loss: 0.0046, label: 0, bag_size: 2628\n",
      "batch 559, loss: 0.0183, label: 1, bag_size: 3368\n",
      "batch 579, loss: 0.0563, label: 1, bag_size: 2412\n",
      "batch 599, loss: 0.0448, label: 1, bag_size: 25970\n",
      "batch 619, loss: 0.4528, label: 0, bag_size: 19808\n",
      "batch 639, loss: 0.0019, label: 1, bag_size: 9971\n",
      "batch 659, loss: 0.0602, label: 1, bag_size: 2522\n",
      "batch 679, loss: 0.0500, label: 1, bag_size: 13015\n",
      "batch 699, loss: 0.0012, label: 1, bag_size: 7078\n",
      "batch 719, loss: 0.5059, label: 0, bag_size: 5211\n",
      "batch 739, loss: 0.0041, label: 1, bag_size: 9571\n",
      "batch 759, loss: 0.0695, label: 1, bag_size: 5690\n",
      "batch 779, loss: 0.0139, label: 1, bag_size: 6606\n",
      "batch 799, loss: 0.3257, label: 0, bag_size: 14249\n",
      "batch 819, loss: 0.0460, label: 1, bag_size: 8438\n",
      "Epoch: 10, train_loss: 0.2676, train_error: 0.0951\n",
      "class 0: acc 0.9088607594936708, correct 359/395\n",
      "class 1: acc 0.9011764705882352, correct 383/425\n",
      "\n",
      "Val Set, val_loss: 0.2416, val_error: 0.1091, auc: 0.9811\n",
      "class 0: acc 0.7884615384615384, correct 41/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0089, label: 1, bag_size: 5612\n",
      "batch 39, loss: 0.5268, label: 0, bag_size: 9616\n",
      "batch 59, loss: 0.4461, label: 1, bag_size: 6726\n",
      "batch 79, loss: 0.0369, label: 0, bag_size: 23791\n",
      "batch 99, loss: 0.0063, label: 0, bag_size: 8948\n",
      "batch 119, loss: 0.0111, label: 0, bag_size: 12212\n",
      "batch 139, loss: 0.2553, label: 0, bag_size: 18516\n",
      "batch 159, loss: 0.0165, label: 1, bag_size: 16267\n",
      "batch 179, loss: 0.0256, label: 1, bag_size: 6736\n",
      "batch 199, loss: 0.3400, label: 1, bag_size: 1437\n",
      "batch 219, loss: 0.0372, label: 0, bag_size: 18954\n",
      "batch 239, loss: 0.0046, label: 1, bag_size: 9877\n",
      "batch 259, loss: 0.0068, label: 0, bag_size: 13964\n",
      "batch 279, loss: 0.0090, label: 0, bag_size: 6851\n",
      "batch 299, loss: 0.0255, label: 0, bag_size: 12524\n",
      "batch 319, loss: 1.5575, label: 0, bag_size: 7612\n",
      "batch 339, loss: 0.0488, label: 0, bag_size: 3774\n",
      "batch 359, loss: 0.0644, label: 1, bag_size: 9470\n",
      "batch 379, loss: 0.2775, label: 1, bag_size: 8026\n",
      "batch 399, loss: 0.0020, label: 1, bag_size: 6090\n",
      "batch 419, loss: 0.0367, label: 0, bag_size: 16782\n",
      "batch 439, loss: 0.0279, label: 0, bag_size: 8025\n",
      "batch 459, loss: 0.3142, label: 0, bag_size: 1508\n",
      "batch 479, loss: 0.0254, label: 0, bag_size: 1909\n",
      "batch 499, loss: 0.1693, label: 0, bag_size: 22498\n",
      "batch 519, loss: 0.1475, label: 0, bag_size: 15914\n",
      "batch 539, loss: 0.0214, label: 0, bag_size: 10898\n",
      "batch 559, loss: 0.0604, label: 1, bag_size: 9955\n",
      "batch 579, loss: 0.0042, label: 1, bag_size: 15233\n",
      "batch 599, loss: 0.0258, label: 1, bag_size: 9519\n",
      "batch 619, loss: 0.0423, label: 1, bag_size: 7119\n",
      "batch 639, loss: 0.1937, label: 1, bag_size: 12895\n",
      "batch 659, loss: 0.0306, label: 1, bag_size: 10396\n",
      "batch 679, loss: 0.4496, label: 0, bag_size: 7637\n",
      "batch 699, loss: 0.3646, label: 0, bag_size: 7835\n",
      "batch 719, loss: 0.0006, label: 0, bag_size: 10481\n",
      "batch 739, loss: 0.0011, label: 1, bag_size: 12611\n",
      "batch 759, loss: 0.0259, label: 1, bag_size: 1255\n",
      "batch 779, loss: 0.0028, label: 1, bag_size: 14433\n",
      "batch 799, loss: 0.0072, label: 0, bag_size: 12524\n",
      "batch 819, loss: 0.0227, label: 0, bag_size: 4902\n",
      "Epoch: 11, train_loss: 0.2591, train_error: 0.0927\n",
      "class 0: acc 0.9148418491484185, correct 376/411\n",
      "class 1: acc 0.8997555012224939, correct 368/409\n",
      "\n",
      "Val Set, val_loss: 0.2001, val_error: 0.0545, auc: 0.9838\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.1701, label: 0, bag_size: 14664\n",
      "batch 39, loss: 0.0176, label: 0, bag_size: 23368\n",
      "batch 59, loss: 0.0034, label: 1, bag_size: 17486\n",
      "batch 79, loss: 0.0138, label: 1, bag_size: 19039\n",
      "batch 99, loss: 0.0169, label: 1, bag_size: 11600\n",
      "batch 119, loss: 0.0078, label: 0, bag_size: 18225\n",
      "batch 139, loss: 0.3034, label: 1, bag_size: 4789\n",
      "batch 159, loss: 0.0076, label: 1, bag_size: 3453\n",
      "batch 179, loss: 0.0002, label: 1, bag_size: 5221\n",
      "batch 199, loss: 0.0300, label: 1, bag_size: 16890\n",
      "batch 219, loss: 0.0512, label: 0, bag_size: 1824\n",
      "batch 239, loss: 0.6132, label: 0, bag_size: 10113\n",
      "batch 259, loss: 0.0667, label: 1, bag_size: 7424\n",
      "batch 279, loss: 0.2005, label: 1, bag_size: 6478\n",
      "batch 299, loss: 0.0141, label: 0, bag_size: 16992\n",
      "batch 319, loss: 0.0053, label: 0, bag_size: 14956\n",
      "batch 339, loss: 0.0484, label: 1, bag_size: 2308\n",
      "batch 359, loss: 0.1932, label: 0, bag_size: 7557\n",
      "batch 379, loss: 0.0174, label: 1, bag_size: 8019\n",
      "batch 399, loss: 1.4442, label: 1, bag_size: 1242\n",
      "batch 419, loss: 0.0261, label: 1, bag_size: 11220\n",
      "batch 439, loss: 0.6506, label: 0, bag_size: 2920\n",
      "batch 459, loss: 0.0040, label: 1, bag_size: 13026\n",
      "batch 479, loss: 0.0728, label: 0, bag_size: 3089\n",
      "batch 499, loss: 0.2692, label: 0, bag_size: 10029\n",
      "batch 519, loss: 0.0033, label: 1, bag_size: 12931\n",
      "batch 539, loss: 0.0011, label: 1, bag_size: 13947\n",
      "batch 559, loss: 0.0313, label: 0, bag_size: 11527\n",
      "batch 579, loss: 0.0286, label: 1, bag_size: 10671\n",
      "batch 599, loss: 0.0466, label: 1, bag_size: 10501\n",
      "batch 619, loss: 0.0169, label: 0, bag_size: 1149\n",
      "batch 639, loss: 0.0603, label: 0, bag_size: 15747\n",
      "batch 659, loss: 0.0220, label: 0, bag_size: 9234\n",
      "batch 679, loss: 0.0190, label: 0, bag_size: 1651\n",
      "batch 699, loss: 0.0025, label: 0, bag_size: 10995\n",
      "batch 719, loss: 0.0164, label: 0, bag_size: 31106\n",
      "batch 739, loss: 0.0788, label: 1, bag_size: 16565\n",
      "batch 759, loss: 0.0006, label: 1, bag_size: 9065\n",
      "batch 779, loss: 0.0011, label: 1, bag_size: 7110\n",
      "batch 799, loss: 0.0156, label: 0, bag_size: 18415\n",
      "batch 819, loss: 0.0016, label: 1, bag_size: 15213\n",
      "Epoch: 12, train_loss: 0.2015, train_error: 0.0829\n",
      "class 0: acc 0.9314420803782506, correct 394/423\n",
      "class 1: acc 0.9017632241813602, correct 358/397\n",
      "\n",
      "Val Set, val_loss: 0.1729, val_error: 0.0455, auc: 0.9818\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "Validation loss decreased (0.182672 --> 0.172875).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3364, label: 0, bag_size: 26208\n",
      "batch 39, loss: 0.0567, label: 1, bag_size: 2455\n",
      "batch 59, loss: 0.1082, label: 0, bag_size: 24911\n",
      "batch 79, loss: 0.0005, label: 1, bag_size: 18794\n",
      "batch 99, loss: 0.1445, label: 0, bag_size: 5120\n",
      "batch 119, loss: 0.0272, label: 0, bag_size: 15914\n",
      "batch 139, loss: 0.0805, label: 0, bag_size: 3774\n",
      "batch 159, loss: 0.0103, label: 0, bag_size: 11113\n",
      "batch 179, loss: 0.0413, label: 0, bag_size: 3774\n",
      "batch 199, loss: 0.7845, label: 0, bag_size: 24382\n",
      "batch 219, loss: 0.4090, label: 1, bag_size: 11256\n",
      "batch 239, loss: 0.2783, label: 0, bag_size: 18215\n",
      "batch 259, loss: 0.0001, label: 1, bag_size: 9610\n",
      "batch 279, loss: 1.1699, label: 0, bag_size: 1701\n",
      "batch 299, loss: 0.1687, label: 0, bag_size: 2918\n",
      "batch 319, loss: 0.0619, label: 0, bag_size: 11187\n",
      "batch 339, loss: 0.1355, label: 1, bag_size: 1759\n",
      "batch 359, loss: 0.0124, label: 1, bag_size: 9878\n",
      "batch 379, loss: 0.0162, label: 0, bag_size: 32227\n",
      "batch 399, loss: 0.2976, label: 0, bag_size: 2043\n",
      "batch 419, loss: 0.0079, label: 1, bag_size: 9478\n",
      "batch 439, loss: 0.2270, label: 0, bag_size: 3783\n",
      "batch 459, loss: 0.4659, label: 1, bag_size: 10622\n",
      "batch 479, loss: 0.0290, label: 1, bag_size: 3450\n",
      "batch 499, loss: 0.3277, label: 0, bag_size: 11607\n",
      "batch 519, loss: 0.8468, label: 1, bag_size: 6360\n",
      "batch 539, loss: 0.0076, label: 0, bag_size: 14828\n",
      "batch 559, loss: 0.0053, label: 1, bag_size: 2904\n",
      "batch 579, loss: 0.0957, label: 1, bag_size: 13015\n",
      "batch 599, loss: 0.0119, label: 0, bag_size: 11187\n",
      "batch 619, loss: 0.0006, label: 1, bag_size: 10592\n",
      "batch 639, loss: 0.0016, label: 1, bag_size: 6752\n",
      "batch 659, loss: 0.9379, label: 1, bag_size: 2344\n",
      "batch 679, loss: 0.0227, label: 0, bag_size: 10791\n",
      "batch 699, loss: 0.0662, label: 1, bag_size: 7798\n",
      "batch 719, loss: 0.1139, label: 1, bag_size: 29832\n",
      "batch 739, loss: 0.0332, label: 0, bag_size: 11778\n",
      "batch 759, loss: 0.0032, label: 0, bag_size: 9786\n",
      "batch 779, loss: 0.3392, label: 0, bag_size: 25814\n",
      "batch 799, loss: 0.2203, label: 1, bag_size: 12178\n",
      "batch 819, loss: 0.5388, label: 1, bag_size: 1609\n",
      "Epoch: 13, train_loss: 0.2087, train_error: 0.0817\n",
      "class 0: acc 0.9234567901234568, correct 374/405\n",
      "class 1: acc 0.9132530120481928, correct 379/415\n",
      "\n",
      "Val Set, val_loss: 0.1658, val_error: 0.0636, auc: 0.9818\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9137931034482759, correct 53/58\n",
      "Validation loss decreased (0.172875 --> 0.165818).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.9577, label: 0, bag_size: 1732\n",
      "batch 39, loss: 0.0008, label: 1, bag_size: 6317\n",
      "batch 59, loss: 0.2496, label: 1, bag_size: 7768\n",
      "batch 79, loss: 0.0498, label: 1, bag_size: 8680\n",
      "batch 99, loss: 0.0004, label: 1, bag_size: 11389\n",
      "batch 119, loss: 0.0252, label: 0, bag_size: 12201\n",
      "batch 139, loss: 0.0027, label: 1, bag_size: 11884\n",
      "batch 159, loss: 0.0118, label: 0, bag_size: 1884\n",
      "batch 179, loss: 0.0209, label: 0, bag_size: 9888\n",
      "batch 199, loss: 0.0667, label: 1, bag_size: 1867\n",
      "batch 219, loss: 0.0155, label: 0, bag_size: 17791\n",
      "batch 239, loss: 0.3590, label: 0, bag_size: 23996\n",
      "batch 259, loss: 0.0008, label: 1, bag_size: 5833\n",
      "batch 279, loss: 0.0086, label: 0, bag_size: 2548\n",
      "batch 299, loss: 0.4416, label: 1, bag_size: 1822\n",
      "batch 319, loss: 0.0308, label: 0, bag_size: 13880\n",
      "batch 339, loss: 0.0015, label: 0, bag_size: 1962\n",
      "batch 359, loss: 0.0118, label: 1, bag_size: 1014\n",
      "batch 379, loss: 0.0310, label: 0, bag_size: 16720\n",
      "batch 399, loss: 0.0645, label: 1, bag_size: 12626\n",
      "batch 419, loss: 0.0157, label: 0, bag_size: 21138\n",
      "batch 439, loss: 0.0235, label: 0, bag_size: 5120\n",
      "batch 459, loss: 0.0520, label: 1, bag_size: 1015\n",
      "batch 479, loss: 0.2761, label: 0, bag_size: 11212\n",
      "batch 499, loss: 0.0025, label: 1, bag_size: 12931\n",
      "batch 519, loss: 0.2021, label: 0, bag_size: 15898\n",
      "batch 539, loss: 0.4675, label: 1, bag_size: 10622\n",
      "batch 559, loss: 0.2752, label: 0, bag_size: 11607\n",
      "batch 579, loss: 0.8641, label: 0, bag_size: 2070\n",
      "batch 599, loss: 0.0013, label: 0, bag_size: 3459\n",
      "batch 619, loss: 0.0057, label: 0, bag_size: 1909\n",
      "batch 639, loss: 0.0070, label: 0, bag_size: 12687\n",
      "batch 659, loss: 0.0001, label: 1, bag_size: 5221\n",
      "batch 679, loss: 0.0003, label: 1, bag_size: 13368\n",
      "batch 699, loss: 0.6913, label: 1, bag_size: 11964\n",
      "batch 719, loss: 0.0648, label: 0, bag_size: 4418\n",
      "batch 739, loss: 0.0012, label: 0, bag_size: 18154\n",
      "batch 759, loss: 0.1226, label: 0, bag_size: 3321\n",
      "batch 779, loss: 0.0238, label: 0, bag_size: 1831\n",
      "batch 799, loss: 0.1854, label: 1, bag_size: 1244\n",
      "batch 819, loss: 0.0735, label: 0, bag_size: 9866\n",
      "Epoch: 14, train_loss: 0.2398, train_error: 0.0890\n",
      "class 0: acc 0.9196217494089834, correct 389/423\n",
      "class 1: acc 0.9017632241813602, correct 358/397\n",
      "\n",
      "Val Set, val_loss: 0.1881, val_error: 0.0727, auc: 0.9847\n",
      "class 0: acc 0.9807692307692307, correct 51/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0114, label: 1, bag_size: 5991\n",
      "batch 39, loss: 0.0309, label: 0, bag_size: 6624\n",
      "batch 59, loss: 0.0236, label: 1, bag_size: 6731\n",
      "batch 79, loss: 0.0227, label: 1, bag_size: 10498\n",
      "batch 99, loss: 0.0053, label: 1, bag_size: 7110\n",
      "batch 119, loss: 0.8042, label: 1, bag_size: 12714\n",
      "batch 139, loss: 0.0079, label: 0, bag_size: 8025\n",
      "batch 159, loss: 0.0051, label: 1, bag_size: 12349\n",
      "batch 179, loss: 0.3277, label: 1, bag_size: 5907\n",
      "batch 199, loss: 0.1989, label: 1, bag_size: 2682\n",
      "batch 219, loss: 0.0061, label: 1, bag_size: 5991\n",
      "batch 239, loss: 0.0062, label: 0, bag_size: 13225\n",
      "batch 259, loss: 0.0183, label: 0, bag_size: 5225\n",
      "batch 279, loss: 0.0057, label: 0, bag_size: 21082\n",
      "batch 299, loss: 0.0800, label: 0, bag_size: 2652\n",
      "batch 319, loss: 0.0076, label: 1, bag_size: 16267\n",
      "batch 339, loss: 0.2606, label: 1, bag_size: 1064\n",
      "batch 359, loss: 0.0121, label: 0, bag_size: 23368\n",
      "batch 379, loss: 0.0110, label: 1, bag_size: 12460\n",
      "batch 399, loss: 0.0757, label: 0, bag_size: 2998\n",
      "batch 419, loss: 0.1302, label: 1, bag_size: 5903\n",
      "batch 439, loss: 0.0800, label: 0, bag_size: 10898\n",
      "batch 459, loss: 0.0744, label: 1, bag_size: 4786\n",
      "batch 479, loss: 0.1949, label: 1, bag_size: 8680\n",
      "batch 499, loss: 0.0070, label: 0, bag_size: 1452\n",
      "batch 519, loss: 0.0175, label: 1, bag_size: 11642\n",
      "batch 539, loss: 0.0064, label: 0, bag_size: 12212\n",
      "batch 559, loss: 0.0633, label: 0, bag_size: 16211\n",
      "batch 579, loss: 0.0088, label: 0, bag_size: 1438\n",
      "batch 599, loss: 0.0831, label: 1, bag_size: 7119\n",
      "batch 619, loss: 0.9279, label: 0, bag_size: 21361\n",
      "batch 639, loss: 0.0415, label: 1, bag_size: 20333\n",
      "batch 659, loss: 0.3178, label: 0, bag_size: 3552\n",
      "batch 679, loss: 3.0257, label: 1, bag_size: 2314\n",
      "batch 699, loss: 0.0962, label: 1, bag_size: 15609\n",
      "batch 719, loss: 0.0027, label: 1, bag_size: 4039\n",
      "batch 739, loss: 0.0364, label: 0, bag_size: 4523\n",
      "batch 759, loss: 0.3099, label: 0, bag_size: 2959\n",
      "batch 779, loss: 0.9663, label: 1, bag_size: 1919\n",
      "batch 799, loss: 0.2812, label: 0, bag_size: 3228\n",
      "batch 819, loss: 0.0094, label: 0, bag_size: 1483\n",
      "Epoch: 15, train_loss: 0.2619, train_error: 0.1012\n",
      "class 0: acc 0.9114219114219114, correct 391/429\n",
      "class 1: acc 0.8849104859335039, correct 346/391\n",
      "\n",
      "Val Set, val_loss: 0.2033, val_error: 0.0545, auc: 0.9841\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7023, label: 1, bag_size: 1284\n",
      "batch 39, loss: 0.3448, label: 0, bag_size: 11922\n",
      "batch 59, loss: 0.5542, label: 1, bag_size: 2356\n",
      "batch 79, loss: 2.7626, label: 0, bag_size: 2694\n",
      "batch 99, loss: 0.9232, label: 0, bag_size: 2815\n",
      "batch 119, loss: 0.4122, label: 1, bag_size: 1244\n",
      "batch 139, loss: 1.3195, label: 0, bag_size: 12840\n",
      "batch 159, loss: 0.1505, label: 0, bag_size: 15747\n",
      "batch 179, loss: 0.0071, label: 1, bag_size: 12712\n",
      "batch 199, loss: 0.0195, label: 1, bag_size: 14030\n",
      "batch 219, loss: 0.0134, label: 0, bag_size: 9888\n",
      "batch 239, loss: 0.1128, label: 0, bag_size: 2044\n",
      "batch 259, loss: 0.0637, label: 0, bag_size: 18415\n",
      "batch 279, loss: 0.0136, label: 1, bag_size: 5991\n",
      "batch 299, loss: 0.0530, label: 1, bag_size: 13786\n",
      "batch 319, loss: 0.0069, label: 1, bag_size: 4929\n",
      "batch 339, loss: 0.0539, label: 0, bag_size: 2266\n",
      "batch 359, loss: 0.0174, label: 0, bag_size: 14377\n",
      "batch 379, loss: 0.4956, label: 0, bag_size: 4598\n",
      "batch 399, loss: 0.0048, label: 0, bag_size: 17630\n",
      "batch 419, loss: 0.0058, label: 0, bag_size: 3970\n",
      "batch 439, loss: 0.0066, label: 1, bag_size: 9478\n",
      "batch 459, loss: 0.7492, label: 1, bag_size: 12494\n",
      "batch 479, loss: 0.0180, label: 1, bag_size: 13051\n",
      "batch 499, loss: 0.0044, label: 0, bag_size: 10942\n",
      "batch 519, loss: 0.0098, label: 0, bag_size: 18240\n",
      "batch 539, loss: 0.0188, label: 0, bag_size: 5965\n",
      "batch 559, loss: 0.0026, label: 1, bag_size: 7650\n",
      "batch 579, loss: 0.0741, label: 0, bag_size: 1508\n",
      "batch 599, loss: 0.0124, label: 0, bag_size: 12524\n",
      "batch 619, loss: 0.5430, label: 0, bag_size: 1732\n",
      "batch 639, loss: 1.4793, label: 1, bag_size: 1703\n",
      "batch 659, loss: 0.1003, label: 1, bag_size: 12425\n",
      "batch 679, loss: 0.0455, label: 0, bag_size: 14681\n",
      "batch 699, loss: 0.0010, label: 1, bag_size: 11389\n",
      "batch 719, loss: 0.0067, label: 0, bag_size: 2748\n",
      "batch 739, loss: 0.0154, label: 0, bag_size: 5551\n",
      "batch 759, loss: 0.0182, label: 0, bag_size: 9060\n",
      "batch 779, loss: 0.1392, label: 1, bag_size: 13732\n",
      "batch 799, loss: 0.0440, label: 0, bag_size: 9234\n",
      "batch 819, loss: 0.1565, label: 0, bag_size: 15747\n",
      "Epoch: 16, train_loss: 0.2005, train_error: 0.0768\n",
      "class 0: acc 0.9236641221374046, correct 363/393\n",
      "class 1: acc 0.9227166276346604, correct 394/427\n",
      "\n",
      "Val Set, val_loss: 0.1823, val_error: 0.0545, auc: 0.9857\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0062, label: 1, bag_size: 3437\n",
      "batch 39, loss: 0.0358, label: 0, bag_size: 10263\n",
      "batch 59, loss: 0.0065, label: 1, bag_size: 5494\n",
      "batch 79, loss: 0.3956, label: 1, bag_size: 1123\n",
      "batch 99, loss: 0.2773, label: 0, bag_size: 15003\n",
      "batch 119, loss: 0.0001, label: 1, bag_size: 6792\n",
      "batch 139, loss: 0.0030, label: 0, bag_size: 10995\n",
      "batch 159, loss: 0.0009, label: 1, bag_size: 9673\n",
      "batch 179, loss: 0.1490, label: 1, bag_size: 549\n",
      "batch 199, loss: 0.0773, label: 0, bag_size: 1831\n",
      "batch 219, loss: 0.0231, label: 0, bag_size: 22426\n",
      "batch 239, loss: 0.0176, label: 0, bag_size: 10128\n",
      "batch 259, loss: 0.0117, label: 1, bag_size: 6731\n",
      "batch 279, loss: 0.0891, label: 1, bag_size: 9322\n",
      "batch 299, loss: 0.4592, label: 0, bag_size: 24382\n",
      "batch 319, loss: 0.0005, label: 1, bag_size: 9065\n",
      "batch 339, loss: 0.1341, label: 1, bag_size: 7351\n",
      "batch 359, loss: 0.0432, label: 0, bag_size: 9470\n",
      "batch 379, loss: 0.0071, label: 1, bag_size: 1638\n",
      "batch 399, loss: 2.5099, label: 1, bag_size: 1051\n",
      "batch 419, loss: 0.0095, label: 1, bag_size: 5256\n",
      "batch 439, loss: 0.0368, label: 1, bag_size: 15125\n",
      "batch 459, loss: 0.0240, label: 0, bag_size: 23796\n",
      "batch 479, loss: 0.0130, label: 0, bag_size: 13880\n",
      "batch 499, loss: 0.0018, label: 1, bag_size: 14618\n",
      "batch 519, loss: 1.6293, label: 1, bag_size: 3879\n",
      "batch 539, loss: 0.1496, label: 0, bag_size: 21319\n",
      "batch 559, loss: 0.0110, label: 1, bag_size: 8019\n",
      "batch 579, loss: 0.0001, label: 1, bag_size: 9644\n",
      "batch 599, loss: 0.0221, label: 0, bag_size: 4271\n",
      "batch 619, loss: 0.0143, label: 0, bag_size: 4959\n",
      "batch 639, loss: 0.1499, label: 0, bag_size: 4598\n",
      "batch 659, loss: 0.0004, label: 1, bag_size: 9610\n",
      "batch 679, loss: 0.1707, label: 0, bag_size: 2004\n",
      "batch 699, loss: 0.0390, label: 1, bag_size: 18603\n",
      "batch 719, loss: 0.0569, label: 0, bag_size: 10381\n",
      "batch 739, loss: 1.9242, label: 1, bag_size: 3879\n",
      "batch 759, loss: 0.0251, label: 0, bag_size: 6367\n",
      "batch 779, loss: 0.2468, label: 1, bag_size: 13732\n",
      "batch 799, loss: 0.0228, label: 1, bag_size: 6927\n",
      "batch 819, loss: 0.6691, label: 0, bag_size: 1701\n",
      "Epoch: 17, train_loss: 0.2187, train_error: 0.0768\n",
      "class 0: acc 0.9183168316831684, correct 371/404\n",
      "class 1: acc 0.9278846153846154, correct 386/416\n",
      "\n",
      "Val Set, val_loss: 0.1783, val_error: 0.0455, auc: 0.9841\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3399, label: 0, bag_size: 10113\n",
      "batch 39, loss: 0.0085, label: 1, bag_size: 5454\n",
      "batch 59, loss: 0.0021, label: 0, bag_size: 2044\n",
      "batch 79, loss: 0.1354, label: 0, bag_size: 15898\n",
      "batch 99, loss: 0.0709, label: 0, bag_size: 1909\n",
      "batch 119, loss: 0.1057, label: 0, bag_size: 8788\n",
      "batch 139, loss: 0.0791, label: 0, bag_size: 5297\n",
      "batch 159, loss: 0.0048, label: 0, bag_size: 11199\n",
      "batch 179, loss: 0.0204, label: 1, bag_size: 5629\n",
      "batch 199, loss: 0.0103, label: 0, bag_size: 11122\n",
      "batch 219, loss: 0.3028, label: 0, bag_size: 2043\n",
      "batch 239, loss: 0.1215, label: 1, bag_size: 13732\n",
      "batch 259, loss: 0.1296, label: 0, bag_size: 15914\n",
      "batch 279, loss: 1.1827, label: 1, bag_size: 2344\n",
      "batch 299, loss: 0.0005, label: 1, bag_size: 6317\n",
      "batch 319, loss: 2.4237, label: 1, bag_size: 2731\n",
      "batch 339, loss: 0.0133, label: 0, bag_size: 10304\n",
      "batch 359, loss: 0.0292, label: 1, bag_size: 5907\n",
      "batch 379, loss: 0.9250, label: 0, bag_size: 1437\n",
      "batch 399, loss: 0.3724, label: 1, bag_size: 1924\n",
      "batch 419, loss: 0.0173, label: 0, bag_size: 10751\n",
      "batch 439, loss: 0.0386, label: 1, bag_size: 9689\n",
      "batch 459, loss: 0.8004, label: 1, bag_size: 1703\n",
      "batch 479, loss: 0.0084, label: 1, bag_size: 4239\n",
      "batch 499, loss: 4.6946, label: 1, bag_size: 2565\n",
      "batch 519, loss: 0.0000, label: 1, bag_size: 12611\n",
      "batch 539, loss: 0.0488, label: 0, bag_size: 15841\n",
      "batch 559, loss: 0.1513, label: 1, bag_size: 1064\n",
      "batch 579, loss: 0.0031, label: 1, bag_size: 7650\n",
      "batch 599, loss: 0.0015, label: 0, bag_size: 3787\n",
      "batch 619, loss: 0.1294, label: 1, bag_size: 8012\n",
      "batch 639, loss: 0.0152, label: 1, bag_size: 16890\n",
      "batch 659, loss: 0.0986, label: 1, bag_size: 2140\n",
      "batch 679, loss: 0.0011, label: 1, bag_size: 9321\n",
      "batch 699, loss: 0.0716, label: 1, bag_size: 9062\n",
      "batch 719, loss: 0.0015, label: 0, bag_size: 3787\n",
      "batch 739, loss: 0.0132, label: 1, bag_size: 3980\n",
      "batch 759, loss: 0.0037, label: 1, bag_size: 12865\n",
      "batch 779, loss: 0.3015, label: 1, bag_size: 10622\n",
      "batch 799, loss: 0.0199, label: 1, bag_size: 1609\n",
      "batch 819, loss: 0.3182, label: 1, bag_size: 12340\n",
      "Epoch: 18, train_loss: 0.2377, train_error: 0.0878\n",
      "class 0: acc 0.9183673469387755, correct 360/392\n",
      "class 1: acc 0.9065420560747663, correct 388/428\n",
      "\n",
      "Val Set, val_loss: 0.2777, val_error: 0.1545, auc: 0.9831\n",
      "class 0: acc 0.6923076923076923, correct 36/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0474, label: 0, bag_size: 2367\n",
      "batch 39, loss: 0.0178, label: 1, bag_size: 12575\n",
      "batch 59, loss: 0.0019, label: 1, bag_size: 4394\n",
      "batch 79, loss: 0.5211, label: 1, bag_size: 6360\n",
      "batch 99, loss: 0.2736, label: 0, bag_size: 7637\n",
      "batch 119, loss: 0.0053, label: 0, bag_size: 23796\n",
      "batch 139, loss: 0.0002, label: 1, bag_size: 10112\n",
      "batch 159, loss: 0.0017, label: 0, bag_size: 19518\n",
      "batch 179, loss: 1.2121, label: 0, bag_size: 14249\n",
      "batch 199, loss: 0.0041, label: 1, bag_size: 4821\n",
      "batch 219, loss: 0.0107, label: 0, bag_size: 1415\n",
      "batch 239, loss: 2.7377, label: 1, bag_size: 1051\n",
      "batch 259, loss: 0.0150, label: 0, bag_size: 12731\n",
      "batch 279, loss: 1.0665, label: 0, bag_size: 4692\n",
      "batch 299, loss: 0.0182, label: 0, bag_size: 16782\n",
      "batch 319, loss: 0.0001, label: 1, bag_size: 18468\n",
      "batch 339, loss: 0.0005, label: 1, bag_size: 12611\n",
      "batch 359, loss: 1.1820, label: 1, bag_size: 4939\n",
      "batch 379, loss: 0.0043, label: 1, bag_size: 8466\n",
      "batch 399, loss: 0.0066, label: 0, bag_size: 2548\n",
      "batch 419, loss: 0.0056, label: 0, bag_size: 21138\n",
      "batch 439, loss: 0.0011, label: 0, bag_size: 7191\n",
      "batch 459, loss: 0.0013, label: 0, bag_size: 1984\n",
      "batch 479, loss: 4.4370, label: 0, bag_size: 3468\n",
      "batch 499, loss: 0.0003, label: 1, bag_size: 15464\n",
      "batch 519, loss: 0.0642, label: 0, bag_size: 1234\n",
      "batch 539, loss: 0.0047, label: 1, bag_size: 9878\n",
      "batch 559, loss: 0.0285, label: 1, bag_size: 4956\n",
      "batch 579, loss: 0.0371, label: 1, bag_size: 13026\n",
      "batch 599, loss: 0.0352, label: 0, bag_size: 13332\n",
      "batch 619, loss: 0.0388, label: 1, bag_size: 1493\n",
      "batch 639, loss: 0.0022, label: 1, bag_size: 11266\n",
      "batch 659, loss: 0.1996, label: 1, bag_size: 6665\n",
      "batch 679, loss: 0.3817, label: 0, bag_size: 24382\n",
      "batch 699, loss: 0.0156, label: 1, bag_size: 15689\n",
      "batch 719, loss: 1.9393, label: 1, bag_size: 1831\n",
      "batch 739, loss: 0.0077, label: 1, bag_size: 4423\n",
      "batch 759, loss: 0.0012, label: 1, bag_size: 15008\n",
      "batch 779, loss: 0.0041, label: 0, bag_size: 21218\n",
      "batch 799, loss: 0.2408, label: 0, bag_size: 18738\n",
      "batch 819, loss: 0.0215, label: 0, bag_size: 8959\n",
      "Epoch: 19, train_loss: 0.2324, train_error: 0.0878\n",
      "class 0: acc 0.9160671462829736, correct 382/417\n",
      "class 1: acc 0.9081885856079405, correct 366/403\n",
      "\n",
      "Val Set, val_loss: 0.2106, val_error: 0.0818, auc: 0.9828\n",
      "class 0: acc 0.9807692307692307, correct 51/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0069, label: 0, bag_size: 9060\n",
      "batch 39, loss: 0.0503, label: 1, bag_size: 8475\n",
      "batch 59, loss: 0.7659, label: 0, bag_size: 7428\n",
      "batch 79, loss: 0.0055, label: 1, bag_size: 14604\n",
      "batch 99, loss: 0.0498, label: 0, bag_size: 2511\n",
      "batch 119, loss: 0.0011, label: 1, bag_size: 12931\n",
      "batch 139, loss: 1.2553, label: 1, bag_size: 12494\n",
      "batch 159, loss: 0.0096, label: 0, bag_size: 12148\n",
      "batch 179, loss: 0.0096, label: 0, bag_size: 14828\n",
      "batch 199, loss: 0.0014, label: 1, bag_size: 5340\n",
      "batch 219, loss: 0.3101, label: 0, bag_size: 9616\n",
      "batch 239, loss: 0.0005, label: 1, bag_size: 30675\n",
      "batch 259, loss: 1.5877, label: 1, bag_size: 2937\n",
      "batch 279, loss: 0.0119, label: 1, bag_size: 9561\n",
      "batch 299, loss: 0.0352, label: 1, bag_size: 9519\n",
      "batch 319, loss: 0.0016, label: 1, bag_size: 5340\n",
      "batch 339, loss: 0.0059, label: 1, bag_size: 6950\n",
      "batch 359, loss: 0.0275, label: 0, bag_size: 22498\n",
      "batch 379, loss: 0.1008, label: 1, bag_size: 1683\n",
      "batch 399, loss: 0.0676, label: 0, bag_size: 6624\n",
      "batch 419, loss: 0.0886, label: 0, bag_size: 30751\n",
      "batch 439, loss: 0.0928, label: 0, bag_size: 2457\n",
      "batch 459, loss: 0.0340, label: 0, bag_size: 1614\n",
      "batch 479, loss: 0.0011, label: 0, bag_size: 20150\n",
      "batch 499, loss: 0.0034, label: 0, bag_size: 22828\n",
      "batch 519, loss: 0.2412, label: 1, bag_size: 1437\n",
      "batch 539, loss: 0.0008, label: 1, bag_size: 7381\n",
      "batch 559, loss: 0.0196, label: 1, bag_size: 13477\n",
      "batch 579, loss: 0.0006, label: 1, bag_size: 9478\n",
      "batch 599, loss: 0.0929, label: 0, bag_size: 19390\n",
      "batch 619, loss: 0.6758, label: 0, bag_size: 2070\n",
      "batch 639, loss: 0.1148, label: 0, bag_size: 18516\n",
      "batch 659, loss: 0.0449, label: 1, bag_size: 5025\n",
      "batch 679, loss: 0.8834, label: 0, bag_size: 1953\n",
      "batch 699, loss: 0.0112, label: 0, bag_size: 11199\n",
      "batch 719, loss: 0.0108, label: 0, bag_size: 1452\n",
      "batch 739, loss: 0.1168, label: 0, bag_size: 24911\n",
      "batch 759, loss: 0.0040, label: 1, bag_size: 13026\n",
      "batch 779, loss: 0.0212, label: 0, bag_size: 10898\n",
      "batch 799, loss: 0.0045, label: 0, bag_size: 12201\n",
      "batch 819, loss: 0.0095, label: 0, bag_size: 12910\n",
      "Epoch: 20, train_loss: 0.1930, train_error: 0.0695\n",
      "class 0: acc 0.9464285714285714, correct 371/392\n",
      "class 1: acc 0.9158878504672897, correct 392/428\n",
      "\n",
      "Val Set, val_loss: 0.1705, val_error: 0.0455, auc: 0.9838\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3730, label: 1, bag_size: 2179\n",
      "batch 39, loss: 0.0170, label: 1, bag_size: 13051\n",
      "batch 59, loss: 0.2318, label: 1, bag_size: 1683\n",
      "batch 79, loss: 0.2139, label: 1, bag_size: 1823\n",
      "batch 99, loss: 0.0183, label: 0, bag_size: 12796\n",
      "batch 119, loss: 0.0158, label: 0, bag_size: 11900\n",
      "batch 139, loss: 0.0011, label: 1, bag_size: 15213\n",
      "batch 159, loss: 0.0885, label: 1, bag_size: 29832\n",
      "batch 179, loss: 1.0583, label: 0, bag_size: 2219\n",
      "batch 199, loss: 0.8678, label: 0, bag_size: 2815\n",
      "batch 219, loss: 0.0106, label: 0, bag_size: 11727\n",
      "batch 239, loss: 0.1283, label: 0, bag_size: 1789\n",
      "batch 259, loss: 0.0048, label: 1, bag_size: 13026\n",
      "batch 279, loss: 0.0061, label: 1, bag_size: 14230\n",
      "batch 299, loss: 0.0532, label: 1, bag_size: 7768\n",
      "batch 319, loss: 0.0008, label: 0, bag_size: 18154\n",
      "batch 339, loss: 0.0083, label: 0, bag_size: 10898\n",
      "batch 359, loss: 0.0266, label: 0, bag_size: 1814\n",
      "batch 379, loss: 0.0038, label: 0, bag_size: 18415\n",
      "batch 399, loss: 0.0374, label: 0, bag_size: 3160\n",
      "batch 419, loss: 0.0461, label: 1, bag_size: 1888\n",
      "batch 439, loss: 0.0317, label: 0, bag_size: 14625\n",
      "batch 459, loss: 0.0603, label: 0, bag_size: 5965\n",
      "batch 479, loss: 0.0073, label: 0, bag_size: 8661\n",
      "batch 499, loss: 0.0110, label: 1, bag_size: 4956\n",
      "batch 519, loss: 0.4118, label: 0, bag_size: 23996\n",
      "batch 539, loss: 0.0763, label: 0, bag_size: 10029\n",
      "batch 559, loss: 0.0060, label: 0, bag_size: 518\n",
      "batch 579, loss: 0.0283, label: 0, bag_size: 2609\n",
      "batch 599, loss: 0.0424, label: 0, bag_size: 8959\n",
      "batch 619, loss: 0.9868, label: 0, bag_size: 1701\n",
      "batch 639, loss: 0.0030, label: 0, bag_size: 16341\n",
      "batch 659, loss: 0.0020, label: 0, bag_size: 17791\n",
      "batch 679, loss: 0.3398, label: 1, bag_size: 2146\n",
      "batch 699, loss: 0.6905, label: 0, bag_size: 14264\n",
      "batch 719, loss: 0.0007, label: 0, bag_size: 13964\n",
      "batch 739, loss: 0.0782, label: 0, bag_size: 14956\n",
      "batch 759, loss: 0.0277, label: 1, bag_size: 9955\n",
      "batch 779, loss: 0.0028, label: 1, bag_size: 9971\n",
      "batch 799, loss: 0.4562, label: 1, bag_size: 1919\n",
      "batch 819, loss: 0.0109, label: 0, bag_size: 11727\n",
      "Epoch: 21, train_loss: 0.2212, train_error: 0.0805\n",
      "class 0: acc 0.9205955334987593, correct 371/403\n",
      "class 1: acc 0.9184652278177458, correct 383/417\n",
      "\n",
      "Val Set, val_loss: 0.1769, val_error: 0.0364, auc: 0.9844\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0019, label: 0, bag_size: 8948\n",
      "batch 39, loss: 0.0718, label: 0, bag_size: 17155\n",
      "batch 59, loss: 0.2973, label: 0, bag_size: 2266\n",
      "batch 79, loss: 0.2143, label: 0, bag_size: 2336\n",
      "batch 99, loss: 0.0953, label: 0, bag_size: 1213\n",
      "batch 119, loss: 0.0002, label: 1, bag_size: 5221\n",
      "batch 139, loss: 0.0100, label: 0, bag_size: 14681\n",
      "batch 159, loss: 0.0307, label: 0, bag_size: 10365\n",
      "batch 179, loss: 0.0038, label: 0, bag_size: 10263\n",
      "batch 199, loss: 0.0377, label: 0, bag_size: 2244\n",
      "batch 219, loss: 0.0110, label: 1, bag_size: 15093\n",
      "batch 239, loss: 0.5546, label: 0, bag_size: 7612\n",
      "batch 259, loss: 0.0323, label: 1, bag_size: 4239\n",
      "batch 279, loss: 1.1402, label: 0, bag_size: 47866\n",
      "batch 299, loss: 1.7480, label: 0, bag_size: 2815\n",
      "batch 319, loss: 0.0101, label: 1, bag_size: 12712\n",
      "batch 339, loss: 0.0273, label: 1, bag_size: 5155\n",
      "batch 359, loss: 0.6282, label: 0, bag_size: 2213\n",
      "batch 379, loss: 0.0182, label: 0, bag_size: 11527\n",
      "batch 399, loss: 0.0518, label: 0, bag_size: 22870\n",
      "batch 419, loss: 0.0674, label: 0, bag_size: 4845\n",
      "batch 439, loss: 0.2432, label: 1, bag_size: 7989\n",
      "batch 459, loss: 0.0021, label: 1, bag_size: 5561\n",
      "batch 479, loss: 0.0024, label: 0, bag_size: 6851\n",
      "batch 499, loss: 0.0626, label: 0, bag_size: 1824\n",
      "batch 519, loss: 0.1324, label: 0, bag_size: 931\n",
      "batch 539, loss: 0.0012, label: 1, bag_size: 10105\n",
      "batch 559, loss: 0.0011, label: 0, bag_size: 3459\n",
      "batch 579, loss: 0.1607, label: 1, bag_size: 2480\n",
      "batch 599, loss: 0.0039, label: 1, bag_size: 8019\n",
      "batch 619, loss: 0.0258, label: 0, bag_size: 8025\n",
      "batch 639, loss: 0.0973, label: 1, bag_size: 8026\n",
      "batch 659, loss: 0.0018, label: 0, bag_size: 2282\n",
      "batch 679, loss: 0.0020, label: 0, bag_size: 15967\n",
      "batch 699, loss: 0.1763, label: 1, bag_size: 2278\n",
      "batch 719, loss: 0.0080, label: 1, bag_size: 10592\n",
      "batch 739, loss: 2.6294, label: 0, bag_size: 2815\n",
      "batch 759, loss: 0.0021, label: 0, bag_size: 4497\n",
      "batch 779, loss: 0.0156, label: 0, bag_size: 763\n",
      "batch 799, loss: 0.0001, label: 1, bag_size: 10867\n",
      "batch 819, loss: 0.0034, label: 0, bag_size: 2091\n",
      "Epoch: 22, train_loss: 0.1953, train_error: 0.0756\n",
      "class 0: acc 0.9276807980049875, correct 372/401\n",
      "class 1: acc 0.9212410501193318, correct 386/419\n",
      "\n",
      "Val Set, val_loss: 0.1697, val_error: 0.0364, auc: 0.9841\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0005, label: 1, bag_size: 16417\n",
      "batch 39, loss: 0.0071, label: 0, bag_size: 17268\n",
      "batch 59, loss: 0.4028, label: 0, bag_size: 26208\n",
      "batch 79, loss: 0.0258, label: 1, bag_size: 3683\n",
      "batch 99, loss: 0.0499, label: 0, bag_size: 1814\n",
      "batch 119, loss: 0.8764, label: 1, bag_size: 1284\n",
      "batch 139, loss: 0.0102, label: 1, bag_size: 21701\n",
      "batch 159, loss: 0.0121, label: 1, bag_size: 3437\n",
      "batch 179, loss: 0.1851, label: 1, bag_size: 13367\n",
      "batch 199, loss: 0.0605, label: 0, bag_size: 4845\n",
      "batch 219, loss: 0.1260, label: 1, bag_size: 8216\n",
      "batch 239, loss: 0.0166, label: 0, bag_size: 2873\n",
      "batch 259, loss: 0.0012, label: 1, bag_size: 2936\n",
      "batch 279, loss: 0.0053, label: 1, bag_size: 2495\n",
      "batch 299, loss: 2.0981, label: 0, bag_size: 2959\n",
      "batch 319, loss: 0.4441, label: 1, bag_size: 4308\n",
      "batch 339, loss: 0.0134, label: 0, bag_size: 11865\n",
      "batch 359, loss: 0.0094, label: 1, bag_size: 13692\n",
      "batch 379, loss: 0.1351, label: 0, bag_size: 2457\n",
      "batch 399, loss: 0.0027, label: 0, bag_size: 21093\n",
      "batch 419, loss: 0.0187, label: 1, bag_size: 13692\n",
      "batch 439, loss: 0.0871, label: 1, bag_size: 1845\n",
      "batch 459, loss: 0.2649, label: 0, bag_size: 18215\n",
      "batch 479, loss: 0.0032, label: 1, bag_size: 4259\n",
      "batch 499, loss: 0.0068, label: 0, bag_size: 11187\n",
      "batch 519, loss: 0.0918, label: 1, bag_size: 4929\n",
      "batch 539, loss: 0.0060, label: 0, bag_size: 22870\n",
      "batch 559, loss: 0.0633, label: 1, bag_size: 3683\n",
      "batch 579, loss: 0.0127, label: 0, bag_size: 2036\n",
      "batch 599, loss: 0.0782, label: 1, bag_size: 8026\n",
      "batch 619, loss: 0.0149, label: 1, bag_size: 20537\n",
      "batch 639, loss: 1.2443, label: 0, bag_size: 2270\n",
      "batch 659, loss: 0.0010, label: 1, bag_size: 12795\n",
      "batch 679, loss: 0.2312, label: 0, bag_size: 15898\n",
      "batch 699, loss: 0.1133, label: 1, bag_size: 7768\n",
      "batch 719, loss: 0.0021, label: 1, bag_size: 7381\n",
      "batch 739, loss: 0.0007, label: 1, bag_size: 10592\n",
      "batch 759, loss: 0.0614, label: 1, bag_size: 16565\n",
      "batch 779, loss: 0.0107, label: 0, bag_size: 3228\n",
      "batch 799, loss: 0.0014, label: 1, bag_size: 19039\n",
      "batch 819, loss: 0.0131, label: 1, bag_size: 12575\n",
      "Epoch: 23, train_loss: 0.1806, train_error: 0.0695\n",
      "class 0: acc 0.9405940594059405, correct 380/404\n",
      "class 1: acc 0.9206730769230769, correct 383/416\n",
      "\n",
      "Val Set, val_loss: 0.1761, val_error: 0.0727, auc: 0.9861\n",
      "class 0: acc 0.8653846153846154, correct 45/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0050, label: 0, bag_size: 9888\n",
      "batch 39, loss: 0.0080, label: 1, bag_size: 11684\n",
      "batch 59, loss: 0.0537, label: 1, bag_size: 1339\n",
      "batch 79, loss: 0.0003, label: 1, bag_size: 689\n",
      "batch 99, loss: 0.0000, label: 1, bag_size: 9644\n",
      "batch 119, loss: 1.3805, label: 1, bag_size: 1819\n",
      "batch 139, loss: 0.0008, label: 1, bag_size: 12349\n",
      "batch 159, loss: 0.0112, label: 0, bag_size: 13880\n",
      "batch 179, loss: 0.0791, label: 0, bag_size: 9597\n",
      "batch 199, loss: 0.1025, label: 1, bag_size: 8680\n",
      "batch 219, loss: 0.0152, label: 1, bag_size: 11220\n",
      "batch 239, loss: 0.0466, label: 1, bag_size: 9230\n",
      "batch 259, loss: 0.0466, label: 0, bag_size: 14377\n",
      "batch 279, loss: 0.0015, label: 1, bag_size: 10498\n",
      "batch 299, loss: 0.5962, label: 0, bag_size: 4418\n",
      "batch 319, loss: 0.0250, label: 0, bag_size: 4845\n",
      "batch 339, loss: 0.0006, label: 1, bag_size: 20767\n",
      "batch 359, loss: 0.0077, label: 1, bag_size: 9004\n",
      "batch 379, loss: 0.6494, label: 1, bag_size: 6726\n",
      "batch 399, loss: 5.7019, label: 1, bag_size: 3121\n",
      "batch 419, loss: 0.0487, label: 0, bag_size: 2457\n",
      "batch 439, loss: 0.0037, label: 0, bag_size: 16052\n",
      "batch 459, loss: 0.2103, label: 0, bag_size: 2070\n",
      "batch 479, loss: 0.0273, label: 1, bag_size: 12946\n",
      "batch 499, loss: 0.0537, label: 1, bag_size: 11386\n",
      "batch 519, loss: 0.0378, label: 1, bag_size: 865\n",
      "batch 539, loss: 0.0029, label: 1, bag_size: 14223\n",
      "batch 559, loss: 0.0529, label: 0, bag_size: 9596\n",
      "batch 579, loss: 1.2377, label: 0, bag_size: 1732\n",
      "batch 599, loss: 0.0001, label: 1, bag_size: 5317\n",
      "batch 619, loss: 0.3195, label: 0, bag_size: 3444\n",
      "batch 639, loss: 0.0113, label: 0, bag_size: 2748\n",
      "batch 659, loss: 0.0892, label: 1, bag_size: 29832\n",
      "batch 679, loss: 0.0565, label: 0, bag_size: 6850\n",
      "batch 699, loss: 0.2918, label: 0, bag_size: 2609\n",
      "batch 719, loss: 0.0045, label: 1, bag_size: 9971\n",
      "batch 739, loss: 0.0169, label: 0, bag_size: 19466\n",
      "batch 759, loss: 0.6961, label: 0, bag_size: 18777\n",
      "batch 779, loss: 0.0831, label: 0, bag_size: 9060\n",
      "batch 799, loss: 0.0159, label: 1, bag_size: 6736\n",
      "batch 819, loss: 0.0306, label: 1, bag_size: 8438\n",
      "Epoch: 24, train_loss: 0.2245, train_error: 0.0732\n",
      "class 0: acc 0.9312039312039312, correct 379/407\n",
      "class 1: acc 0.9225181598062954, correct 381/413\n",
      "\n",
      "Val Set, val_loss: 0.2121, val_error: 0.0727, auc: 0.9841\n",
      "class 0: acc 0.8653846153846154, correct 45/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0528, label: 1, bag_size: 8475\n",
      "batch 39, loss: 0.0072, label: 1, bag_size: 5256\n",
      "batch 59, loss: 0.0389, label: 0, bag_size: 3783\n",
      "batch 79, loss: 0.6608, label: 1, bag_size: 1230\n",
      "batch 99, loss: 0.0001, label: 1, bag_size: 9644\n",
      "batch 119, loss: 0.0019, label: 1, bag_size: 5991\n",
      "batch 139, loss: 0.1378, label: 1, bag_size: 12425\n",
      "batch 159, loss: 0.0054, label: 0, bag_size: 518\n",
      "batch 179, loss: 1.2281, label: 1, bag_size: 2842\n",
      "batch 199, loss: 0.0295, label: 0, bag_size: 10068\n",
      "batch 219, loss: 0.0213, label: 1, bag_size: 13026\n",
      "batch 239, loss: 0.0078, label: 0, bag_size: 3657\n",
      "batch 259, loss: 0.0040, label: 1, bag_size: 8040\n",
      "batch 279, loss: 0.0019, label: 1, bag_size: 4039\n",
      "batch 299, loss: 0.0120, label: 1, bag_size: 13692\n",
      "batch 319, loss: 0.0258, label: 0, bag_size: 7011\n",
      "batch 339, loss: 0.0100, label: 0, bag_size: 15636\n",
      "batch 359, loss: 0.1615, label: 0, bag_size: 8744\n",
      "batch 379, loss: 0.2703, label: 1, bag_size: 1924\n",
      "batch 399, loss: 0.5438, label: 1, bag_size: 2842\n",
      "batch 419, loss: 0.0133, label: 0, bag_size: 4902\n",
      "batch 439, loss: 0.0702, label: 1, bag_size: 3082\n",
      "batch 459, loss: 0.1499, label: 0, bag_size: 2043\n",
      "batch 479, loss: 0.0032, label: 0, bag_size: 20150\n",
      "batch 499, loss: 0.0170, label: 1, bag_size: 1920\n",
      "batch 519, loss: 0.1075, label: 1, bag_size: 3211\n",
      "batch 539, loss: 0.1188, label: 1, bag_size: 4054\n",
      "batch 559, loss: 0.2030, label: 1, bag_size: 7989\n",
      "batch 579, loss: 0.1594, label: 0, bag_size: 16087\n",
      "batch 599, loss: 0.0006, label: 1, bag_size: 14223\n",
      "batch 619, loss: 0.0016, label: 1, bag_size: 6453\n",
      "batch 639, loss: 0.0594, label: 1, bag_size: 1823\n",
      "batch 659, loss: 1.2670, label: 1, bag_size: 771\n",
      "batch 679, loss: 0.0001, label: 1, bag_size: 3295\n",
      "batch 699, loss: 0.0125, label: 0, bag_size: 15071\n",
      "batch 719, loss: 0.0099, label: 1, bag_size: 19606\n",
      "batch 739, loss: 0.2201, label: 0, bag_size: 2652\n",
      "batch 759, loss: 0.0044, label: 1, bag_size: 22264\n",
      "batch 779, loss: 0.0052, label: 1, bag_size: 11316\n",
      "batch 799, loss: 0.0202, label: 0, bag_size: 2534\n",
      "batch 819, loss: 0.0024, label: 0, bag_size: 9455\n",
      "Epoch: 25, train_loss: 0.1925, train_error: 0.0720\n",
      "class 0: acc 0.9324009324009324, correct 400/429\n",
      "class 1: acc 0.9232736572890026, correct 361/391\n",
      "\n",
      "Val Set, val_loss: 0.1941, val_error: 0.0909, auc: 0.9834\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "EarlyStopping counter: 12 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4402, label: 1, bag_size: 5903\n",
      "batch 39, loss: 0.0060, label: 0, bag_size: 13777\n",
      "batch 59, loss: 0.0058, label: 0, bag_size: 11187\n",
      "batch 79, loss: 0.0024, label: 1, bag_size: 7381\n",
      "batch 99, loss: 0.0037, label: 0, bag_size: 9851\n",
      "batch 119, loss: 0.0023, label: 0, bag_size: 16052\n",
      "batch 139, loss: 0.0079, label: 1, bag_size: 3437\n",
      "batch 159, loss: 0.0297, label: 0, bag_size: 1814\n",
      "batch 179, loss: 0.1062, label: 0, bag_size: 931\n",
      "batch 199, loss: 0.8179, label: 0, bag_size: 2815\n",
      "batch 219, loss: 0.0019, label: 0, bag_size: 1415\n",
      "batch 239, loss: 0.0048, label: 1, bag_size: 1459\n",
      "batch 259, loss: 0.3882, label: 1, bag_size: 12340\n",
      "batch 279, loss: 0.0205, label: 1, bag_size: 9519\n",
      "batch 299, loss: 0.0099, label: 0, bag_size: 6851\n",
      "batch 319, loss: 0.1937, label: 0, bag_size: 1684\n",
      "batch 339, loss: 0.0014, label: 1, bag_size: 16417\n",
      "batch 359, loss: 0.0034, label: 0, bag_size: 21404\n",
      "batch 379, loss: 0.0137, label: 1, bag_size: 14202\n",
      "batch 399, loss: 0.0490, label: 0, bag_size: 1881\n",
      "batch 419, loss: 0.0217, label: 1, bag_size: 9470\n",
      "batch 439, loss: 0.0018, label: 0, bag_size: 2006\n",
      "batch 459, loss: 0.0098, label: 0, bag_size: 21082\n",
      "batch 479, loss: 0.0030, label: 0, bag_size: 19518\n",
      "batch 499, loss: 0.0017, label: 0, bag_size: 1052\n",
      "batch 519, loss: 0.0147, label: 0, bag_size: 11146\n",
      "batch 539, loss: 0.0686, label: 0, bag_size: 1438\n",
      "batch 559, loss: 0.0038, label: 1, bag_size: 11642\n",
      "batch 579, loss: 0.0008, label: 1, bag_size: 15716\n",
      "batch 599, loss: 0.0184, label: 0, bag_size: 2748\n",
      "batch 619, loss: 0.0313, label: 1, bag_size: 5025\n",
      "batch 639, loss: 0.0105, label: 1, bag_size: 1493\n",
      "batch 659, loss: 0.0006, label: 0, bag_size: 2628\n",
      "batch 679, loss: 0.0189, label: 1, bag_size: 1244\n",
      "batch 699, loss: 0.6105, label: 1, bag_size: 8868\n",
      "batch 719, loss: 0.0631, label: 1, bag_size: 7424\n",
      "batch 739, loss: 0.0706, label: 0, bag_size: 10365\n",
      "batch 759, loss: 0.0004, label: 1, bag_size: 7873\n",
      "batch 779, loss: 0.0051, label: 1, bag_size: 1622\n",
      "batch 799, loss: 0.0005, label: 0, bag_size: 9433\n",
      "batch 819, loss: 0.0870, label: 1, bag_size: 8660\n",
      "Epoch: 26, train_loss: 0.1763, train_error: 0.0695\n",
      "class 0: acc 0.9465116279069767, correct 407/430\n",
      "class 1: acc 0.9128205128205128, correct 356/390\n",
      "\n",
      "Val Set, val_loss: 0.2244, val_error: 0.0909, auc: 0.9801\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 13 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0901, label: 0, bag_size: 8959\n",
      "batch 39, loss: 0.1604, label: 1, bag_size: 22286\n",
      "batch 59, loss: 0.0524, label: 1, bag_size: 3980\n",
      "batch 79, loss: 0.4552, label: 0, bag_size: 10410\n",
      "batch 99, loss: 1.4797, label: 1, bag_size: 6726\n",
      "batch 119, loss: 0.0142, label: 0, bag_size: 4465\n",
      "batch 139, loss: 0.7495, label: 0, bag_size: 3654\n",
      "batch 159, loss: 0.0233, label: 0, bag_size: 13880\n",
      "batch 179, loss: 0.0087, label: 0, bag_size: 12510\n",
      "batch 199, loss: 0.0125, label: 0, bag_size: 10128\n",
      "batch 219, loss: 0.0029, label: 1, bag_size: 2278\n",
      "batch 239, loss: 0.0286, label: 0, bag_size: 5225\n",
      "batch 259, loss: 0.1927, label: 0, bag_size: 13339\n",
      "batch 279, loss: 0.1446, label: 0, bag_size: 9132\n",
      "batch 299, loss: 0.0031, label: 0, bag_size: 20796\n",
      "batch 319, loss: 0.0162, label: 1, bag_size: 21827\n",
      "batch 339, loss: 0.0010, label: 1, bag_size: 9065\n",
      "batch 359, loss: 0.0546, label: 0, bag_size: 15071\n",
      "batch 379, loss: 0.2153, label: 0, bag_size: 1831\n",
      "batch 399, loss: 0.4475, label: 0, bag_size: 10029\n",
      "batch 419, loss: 0.3226, label: 0, bag_size: 2219\n",
      "batch 439, loss: 0.0006, label: 1, bag_size: 2904\n",
      "batch 459, loss: 0.0161, label: 0, bag_size: 32227\n",
      "batch 479, loss: 0.0402, label: 0, bag_size: 3552\n",
      "batch 499, loss: 0.0060, label: 0, bag_size: 11727\n",
      "batch 519, loss: 0.0029, label: 0, bag_size: 8981\n",
      "batch 539, loss: 0.1687, label: 1, bag_size: 5516\n",
      "batch 559, loss: 0.0433, label: 0, bag_size: 1349\n",
      "batch 579, loss: 0.1078, label: 0, bag_size: 11306\n",
      "batch 599, loss: 0.0085, label: 1, bag_size: 18095\n",
      "batch 619, loss: 0.0002, label: 1, bag_size: 6875\n",
      "batch 639, loss: 0.2915, label: 0, bag_size: 9485\n",
      "batch 659, loss: 0.0016, label: 1, bag_size: 15213\n",
      "batch 679, loss: 0.4128, label: 0, bag_size: 3557\n",
      "batch 699, loss: 0.0030, label: 0, bag_size: 13777\n",
      "batch 719, loss: 0.4574, label: 0, bag_size: 18738\n",
      "batch 739, loss: 0.1203, label: 1, bag_size: 7981\n",
      "batch 759, loss: 3.3871, label: 0, bag_size: 3897\n",
      "batch 779, loss: 0.0001, label: 1, bag_size: 9644\n",
      "batch 799, loss: 0.0051, label: 1, bag_size: 1459\n",
      "batch 819, loss: 0.0462, label: 0, bag_size: 2244\n",
      "Epoch: 27, train_loss: 0.1949, train_error: 0.0744\n",
      "class 0: acc 0.9315403422982885, correct 381/409\n",
      "class 1: acc 0.9197080291970803, correct 378/411\n",
      "\n",
      "Val Set, val_loss: 0.2401, val_error: 0.0818, auc: 0.9788\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 14 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0271, label: 1, bag_size: 10072\n",
      "batch 39, loss: 0.0352, label: 0, bag_size: 1498\n",
      "batch 59, loss: 0.0014, label: 0, bag_size: 8252\n",
      "batch 79, loss: 0.0115, label: 1, bag_size: 20333\n",
      "batch 99, loss: 0.0143, label: 0, bag_size: 8145\n",
      "batch 119, loss: 0.0094, label: 0, bag_size: 2004\n",
      "batch 139, loss: 0.2036, label: 0, bag_size: 13619\n",
      "batch 159, loss: 1.9017, label: 1, bag_size: 2344\n",
      "batch 179, loss: 0.0472, label: 1, bag_size: 9519\n",
      "batch 199, loss: 0.0506, label: 0, bag_size: 1831\n",
      "batch 219, loss: 0.0104, label: 0, bag_size: 10751\n",
      "batch 239, loss: 0.0007, label: 1, bag_size: 2136\n",
      "batch 259, loss: 0.0210, label: 0, bag_size: 22800\n",
      "batch 279, loss: 0.0267, label: 0, bag_size: 17633\n",
      "batch 299, loss: 0.1361, label: 1, bag_size: 2785\n",
      "batch 319, loss: 0.0124, label: 0, bag_size: 16607\n",
      "batch 339, loss: 0.3547, label: 1, bag_size: 4308\n",
      "batch 359, loss: 0.0417, label: 0, bag_size: 14377\n",
      "batch 379, loss: 0.5273, label: 0, bag_size: 6850\n",
      "batch 399, loss: 0.0007, label: 1, bag_size: 15213\n",
      "batch 419, loss: 0.0186, label: 0, bag_size: 2511\n",
      "batch 439, loss: 0.0461, label: 1, bag_size: 9942\n",
      "batch 459, loss: 0.0006, label: 1, bag_size: 10394\n",
      "batch 479, loss: 0.0036, label: 0, bag_size: 2091\n",
      "batch 499, loss: 0.0000, label: 1, bag_size: 18468\n",
      "batch 519, loss: 0.0294, label: 1, bag_size: 10033\n",
      "batch 539, loss: 0.0795, label: 1, bag_size: 2662\n",
      "batch 559, loss: 0.0069, label: 1, bag_size: 13477\n",
      "batch 579, loss: 0.0026, label: 1, bag_size: 11316\n",
      "batch 599, loss: 0.0003, label: 1, bag_size: 7767\n",
      "batch 619, loss: 0.0012, label: 0, bag_size: 18154\n",
      "batch 639, loss: 0.2505, label: 0, bag_size: 9252\n",
      "batch 659, loss: 0.0011, label: 1, bag_size: 11266\n",
      "batch 679, loss: 0.0014, label: 1, bag_size: 11875\n",
      "batch 699, loss: 0.0039, label: 1, bag_size: 10281\n",
      "batch 719, loss: 0.0038, label: 0, bag_size: 2006\n",
      "batch 739, loss: 0.1001, label: 1, bag_size: 8592\n",
      "batch 759, loss: 0.0062, label: 1, bag_size: 10498\n",
      "batch 779, loss: 0.7612, label: 1, bag_size: 1609\n",
      "batch 799, loss: 0.0353, label: 0, bag_size: 4523\n",
      "batch 819, loss: 0.0029, label: 1, bag_size: 2136\n",
      "Epoch: 28, train_loss: 0.1866, train_error: 0.0561\n",
      "class 0: acc 0.9424657534246575, correct 344/365\n",
      "class 1: acc 0.945054945054945, correct 430/455\n",
      "\n",
      "Val Set, val_loss: 0.2066, val_error: 0.0727, auc: 0.9814\n",
      "class 0: acc 0.8653846153846154, correct 45/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 15 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0177, label: 1, bag_size: 1525\n",
      "batch 39, loss: 0.0905, label: 0, bag_size: 6884\n",
      "batch 59, loss: 0.0001, label: 1, bag_size: 18468\n",
      "batch 79, loss: 0.0068, label: 0, bag_size: 9060\n",
      "batch 99, loss: 1.8060, label: 1, bag_size: 15563\n",
      "batch 119, loss: 0.7577, label: 0, bag_size: 8788\n",
      "batch 139, loss: 0.0010, label: 1, bag_size: 16512\n",
      "batch 159, loss: 0.0072, label: 0, bag_size: 10791\n",
      "batch 179, loss: 0.0921, label: 1, bag_size: 6927\n",
      "batch 199, loss: 0.0004, label: 1, bag_size: 15665\n",
      "batch 219, loss: 0.0571, label: 1, bag_size: 3082\n",
      "batch 239, loss: 0.0913, label: 0, bag_size: 5639\n",
      "batch 259, loss: 0.1048, label: 1, bag_size: 8026\n",
      "batch 279, loss: 0.0124, label: 1, bag_size: 8448\n",
      "batch 299, loss: 0.0000, label: 1, bag_size: 12931\n",
      "batch 319, loss: 0.0002, label: 1, bag_size: 20767\n",
      "batch 339, loss: 0.0030, label: 1, bag_size: 9732\n",
      "batch 359, loss: 0.0413, label: 1, bag_size: 13015\n",
      "batch 379, loss: 0.0459, label: 0, bag_size: 23996\n",
      "batch 399, loss: 0.0038, label: 1, bag_size: 1244\n",
      "batch 419, loss: 0.0160, label: 1, bag_size: 6745\n",
      "batch 439, loss: 0.0799, label: 1, bag_size: 9519\n",
      "batch 459, loss: 0.2492, label: 1, bag_size: 2179\n",
      "batch 479, loss: 0.0053, label: 0, bag_size: 763\n",
      "batch 499, loss: 0.0014, label: 1, bag_size: 20161\n",
      "batch 519, loss: 0.0159, label: 1, bag_size: 6731\n",
      "batch 539, loss: 0.0050, label: 0, bag_size: 12687\n",
      "batch 559, loss: 0.0001, label: 1, bag_size: 10920\n",
      "batch 579, loss: 0.0089, label: 0, bag_size: 14333\n",
      "batch 599, loss: 0.0001, label: 1, bag_size: 8410\n",
      "batch 619, loss: 0.1788, label: 0, bag_size: 11128\n",
      "batch 639, loss: 0.0040, label: 1, bag_size: 2412\n",
      "batch 659, loss: 0.0669, label: 1, bag_size: 7798\n",
      "batch 679, loss: 0.4239, label: 1, bag_size: 1512\n",
      "batch 699, loss: 0.0600, label: 0, bag_size: 9597\n",
      "batch 719, loss: 0.0115, label: 0, bag_size: 1891\n",
      "batch 739, loss: 0.0072, label: 1, bag_size: 5454\n",
      "batch 759, loss: 0.0043, label: 0, bag_size: 9455\n",
      "batch 779, loss: 0.6695, label: 0, bag_size: 47866\n",
      "batch 799, loss: 0.0095, label: 0, bag_size: 21138\n",
      "batch 819, loss: 0.0026, label: 1, bag_size: 11518\n",
      "Epoch: 29, train_loss: 0.1501, train_error: 0.0512\n",
      "class 0: acc 0.9533169533169533, correct 388/407\n",
      "class 1: acc 0.9443099273607748, correct 390/413\n",
      "\n",
      "Val Set, val_loss: 0.2326, val_error: 0.1091, auc: 0.9834\n",
      "class 0: acc 0.7884615384615384, correct 41/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 16 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0144, label: 0, bag_size: 10415\n",
      "batch 39, loss: 0.0238, label: 1, bag_size: 12719\n",
      "batch 59, loss: 0.0141, label: 0, bag_size: 7557\n",
      "batch 79, loss: 0.1399, label: 0, bag_size: 18215\n",
      "batch 99, loss: 0.0001, label: 1, bag_size: 5731\n",
      "batch 119, loss: 0.1788, label: 0, bag_size: 3541\n",
      "batch 139, loss: 0.8633, label: 0, bag_size: 4692\n",
      "batch 159, loss: 0.0002, label: 1, bag_size: 4862\n",
      "batch 179, loss: 0.1281, label: 1, bag_size: 11256\n",
      "batch 199, loss: 0.0240, label: 1, bag_size: 9955\n",
      "batch 219, loss: 0.0417, label: 1, bag_size: 3453\n",
      "batch 239, loss: 0.0268, label: 0, bag_size: 11259\n",
      "batch 259, loss: 1.3077, label: 0, bag_size: 3654\n",
      "batch 279, loss: 0.3635, label: 1, bag_size: 5160\n",
      "batch 299, loss: 0.1101, label: 0, bag_size: 5120\n",
      "batch 319, loss: 0.0886, label: 1, bag_size: 2681\n",
      "batch 339, loss: 0.0057, label: 0, bag_size: 10942\n",
      "batch 359, loss: 0.0999, label: 0, bag_size: 7428\n",
      "batch 379, loss: 0.0005, label: 0, bag_size: 9433\n",
      "batch 399, loss: 0.0715, label: 0, bag_size: 2654\n",
      "batch 419, loss: 0.0876, label: 1, bag_size: 5025\n",
      "batch 439, loss: 0.0029, label: 1, bag_size: 11518\n",
      "batch 459, loss: 0.0339, label: 0, bag_size: 21864\n",
      "batch 479, loss: 0.0718, label: 1, bag_size: 2140\n",
      "batch 499, loss: 0.2841, label: 0, bag_size: 2624\n",
      "batch 519, loss: 0.0163, label: 0, bag_size: 17791\n",
      "batch 539, loss: 0.0499, label: 0, bag_size: 4465\n",
      "batch 559, loss: 0.0001, label: 1, bag_size: 5221\n",
      "batch 579, loss: 0.0289, label: 0, bag_size: 7011\n",
      "batch 599, loss: 0.0357, label: 0, bag_size: 12796\n",
      "batch 619, loss: 0.0113, label: 1, bag_size: 1493\n",
      "batch 639, loss: 0.0037, label: 1, bag_size: 2695\n",
      "batch 659, loss: 0.0082, label: 0, bag_size: 19043\n",
      "batch 679, loss: 0.0049, label: 0, bag_size: 3160\n",
      "batch 699, loss: 0.1000, label: 1, bag_size: 2146\n",
      "batch 719, loss: 0.0239, label: 0, bag_size: 2006\n",
      "batch 739, loss: 0.0720, label: 1, bag_size: 1244\n",
      "batch 759, loss: 0.0151, label: 0, bag_size: 1508\n",
      "batch 779, loss: 0.0006, label: 1, bag_size: 4394\n",
      "batch 799, loss: 0.0637, label: 0, bag_size: 15672\n",
      "batch 819, loss: 0.3653, label: 1, bag_size: 2395\n",
      "Epoch: 30, train_loss: 0.1855, train_error: 0.0634\n",
      "class 0: acc 0.9356435643564357, correct 378/404\n",
      "class 1: acc 0.9375, correct 390/416\n",
      "\n",
      "Val Set, val_loss: 0.1922, val_error: 0.0727, auc: 0.9804\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 17 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0589, label: 1, bag_size: 7371\n",
      "batch 39, loss: 0.0017, label: 1, bag_size: 15332\n",
      "batch 59, loss: 0.0105, label: 0, bag_size: 3670\n",
      "batch 79, loss: 0.5066, label: 1, bag_size: 15185\n",
      "batch 99, loss: 0.0002, label: 1, bag_size: 3409\n",
      "batch 119, loss: 0.0000, label: 1, bag_size: 11389\n",
      "batch 139, loss: 0.0003, label: 1, bag_size: 6343\n",
      "batch 159, loss: 0.0175, label: 1, bag_size: 3652\n",
      "batch 179, loss: 0.0616, label: 1, bag_size: 11032\n",
      "batch 199, loss: 0.3389, label: 0, bag_size: 9949\n",
      "batch 219, loss: 0.9129, label: 0, bag_size: 2098\n",
      "batch 239, loss: 0.0817, label: 0, bag_size: 3774\n",
      "batch 259, loss: 1.6624, label: 1, bag_size: 2759\n",
      "batch 279, loss: 0.0001, label: 1, bag_size: 9321\n",
      "batch 299, loss: 1.3456, label: 1, bag_size: 771\n",
      "batch 319, loss: 0.0962, label: 0, bag_size: 12910\n",
      "batch 339, loss: 0.0002, label: 1, bag_size: 6343\n",
      "batch 359, loss: 0.5266, label: 1, bag_size: 5231\n",
      "batch 379, loss: 0.0564, label: 1, bag_size: 5723\n",
      "batch 399, loss: 0.2090, label: 0, bag_size: 18738\n",
      "batch 419, loss: 0.0062, label: 0, bag_size: 11917\n",
      "batch 439, loss: 0.0070, label: 0, bag_size: 1891\n",
      "batch 459, loss: 0.0140, label: 1, bag_size: 12946\n",
      "batch 479, loss: 0.0068, label: 0, bag_size: 10751\n",
      "batch 499, loss: 0.0005, label: 1, bag_size: 5561\n",
      "batch 519, loss: 0.0279, label: 0, bag_size: 3552\n",
      "batch 539, loss: 0.0001, label: 0, bag_size: 2820\n",
      "batch 559, loss: 0.0003, label: 0, bag_size: 10535\n",
      "batch 579, loss: 0.0133, label: 0, bag_size: 1052\n",
      "batch 599, loss: 0.0038, label: 1, bag_size: 11684\n",
      "batch 619, loss: 0.0025, label: 1, bag_size: 1255\n",
      "batch 639, loss: 0.0465, label: 0, bag_size: 15914\n",
      "batch 659, loss: 0.0063, label: 1, bag_size: 9478\n",
      "batch 679, loss: 0.1541, label: 0, bag_size: 13332\n",
      "batch 699, loss: 0.0111, label: 0, bag_size: 2296\n",
      "batch 719, loss: 0.0023, label: 0, bag_size: 9851\n",
      "batch 739, loss: 0.0013, label: 0, bag_size: 1560\n",
      "batch 759, loss: 0.0088, label: 1, bag_size: 3651\n",
      "batch 779, loss: 0.0008, label: 1, bag_size: 19606\n",
      "batch 799, loss: 0.0039, label: 1, bag_size: 20333\n",
      "batch 819, loss: 0.0043, label: 1, bag_size: 7513\n",
      "Epoch: 31, train_loss: 0.2059, train_error: 0.0780\n",
      "class 0: acc 0.9225, correct 369/400\n",
      "class 1: acc 0.9214285714285714, correct 387/420\n",
      "\n",
      "Val Set, val_loss: 0.1864, val_error: 0.0636, auc: 0.9814\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "EarlyStopping counter: 18 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0146, label: 1, bag_size: 1022\n",
      "batch 39, loss: 0.0022, label: 1, bag_size: 4877\n",
      "batch 59, loss: 0.0167, label: 0, bag_size: 23368\n",
      "batch 79, loss: 3.4316, label: 1, bag_size: 9162\n",
      "batch 99, loss: 0.0912, label: 0, bag_size: 3089\n",
      "batch 119, loss: 0.0976, label: 0, bag_size: 11865\n",
      "batch 139, loss: 0.1950, label: 0, bag_size: 6281\n",
      "batch 159, loss: 0.0119, label: 0, bag_size: 22800\n",
      "batch 179, loss: 0.0208, label: 0, bag_size: 13880\n",
      "batch 199, loss: 0.0103, label: 0, bag_size: 1370\n",
      "batch 219, loss: 0.3245, label: 0, bag_size: 10415\n",
      "batch 239, loss: 0.0019, label: 1, bag_size: 9147\n",
      "batch 259, loss: 0.0151, label: 0, bag_size: 6884\n",
      "batch 279, loss: 0.7670, label: 1, bag_size: 8191\n",
      "batch 299, loss: 0.2897, label: 0, bag_size: 25814\n",
      "batch 319, loss: 0.1071, label: 1, bag_size: 5231\n",
      "batch 339, loss: 0.1013, label: 0, bag_size: 4271\n",
      "batch 359, loss: 0.4437, label: 0, bag_size: 11128\n",
      "batch 379, loss: 0.1690, label: 1, bag_size: 11394\n",
      "batch 399, loss: 0.0003, label: 1, bag_size: 13365\n",
      "batch 419, loss: 0.0309, label: 0, bag_size: 22870\n",
      "batch 439, loss: 0.0015, label: 0, bag_size: 1416\n",
      "batch 459, loss: 0.0004, label: 0, bag_size: 2820\n",
      "batch 479, loss: 0.6954, label: 0, bag_size: 3802\n",
      "batch 499, loss: 0.0046, label: 0, bag_size: 12201\n",
      "batch 519, loss: 0.2341, label: 1, bag_size: 4789\n",
      "batch 539, loss: 0.0356, label: 0, bag_size: 6898\n",
      "batch 559, loss: 1.8662, label: 0, bag_size: 65728\n",
      "batch 579, loss: 0.0040, label: 1, bag_size: 13440\n",
      "batch 599, loss: 0.0001, label: 1, bag_size: 6875\n",
      "batch 619, loss: 0.0001, label: 1, bag_size: 6792\n",
      "batch 639, loss: 0.3772, label: 0, bag_size: 6850\n",
      "batch 659, loss: 0.1081, label: 1, bag_size: 8592\n",
      "batch 679, loss: 0.0030, label: 1, bag_size: 12575\n",
      "batch 699, loss: 0.0145, label: 1, bag_size: 5605\n",
      "batch 719, loss: 0.3024, label: 1, bag_size: 13786\n",
      "batch 739, loss: 0.0007, label: 1, bag_size: 10105\n",
      "batch 759, loss: 0.0089, label: 1, bag_size: 13015\n",
      "batch 779, loss: 0.0033, label: 0, bag_size: 13892\n",
      "batch 799, loss: 0.1257, label: 0, bag_size: 1701\n",
      "batch 819, loss: 0.3462, label: 1, bag_size: 3211\n",
      "Epoch: 32, train_loss: 0.1556, train_error: 0.0537\n",
      "class 0: acc 0.9460093896713615, correct 403/426\n",
      "class 1: acc 0.9467005076142132, correct 373/394\n",
      "\n",
      "Val Set, val_loss: 0.2044, val_error: 0.0909, auc: 0.9791\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "EarlyStopping counter: 19 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0063, label: 1, bag_size: 21827\n",
      "batch 39, loss: 0.0001, label: 1, bag_size: 8410\n",
      "batch 59, loss: 0.7680, label: 1, bag_size: 15185\n",
      "batch 79, loss: 0.0108, label: 0, bag_size: 22800\n",
      "batch 99, loss: 0.0073, label: 0, bag_size: 1814\n",
      "batch 119, loss: 0.2307, label: 0, bag_size: 2219\n",
      "batch 139, loss: 0.0008, label: 1, bag_size: 13947\n",
      "batch 159, loss: 0.0064, label: 1, bag_size: 7669\n",
      "batch 179, loss: 0.0216, label: 1, bag_size: 10492\n",
      "batch 199, loss: 0.0144, label: 0, bag_size: 1438\n",
      "batch 219, loss: 0.2132, label: 0, bag_size: 20555\n",
      "batch 239, loss: 0.0137, label: 0, bag_size: 11865\n",
      "batch 259, loss: 0.2196, label: 1, bag_size: 1845\n",
      "batch 279, loss: 0.0001, label: 1, bag_size: 9065\n",
      "batch 299, loss: 0.0488, label: 0, bag_size: 2104\n",
      "batch 319, loss: 0.6645, label: 1, bag_size: 13732\n",
      "batch 339, loss: 0.0000, label: 1, bag_size: 4862\n",
      "batch 359, loss: 0.3194, label: 0, bag_size: 12840\n",
      "batch 379, loss: 0.0034, label: 1, bag_size: 10498\n",
      "batch 399, loss: 1.0702, label: 1, bag_size: 1284\n",
      "batch 419, loss: 0.0246, label: 1, bag_size: 2356\n",
      "batch 439, loss: 0.0002, label: 1, bag_size: 9571\n",
      "batch 459, loss: 0.1973, label: 1, bag_size: 7981\n",
      "batch 479, loss: 0.0013, label: 0, bag_size: 1760\n",
      "batch 499, loss: 0.0600, label: 0, bag_size: 5120\n",
      "batch 519, loss: 0.0849, label: 1, bag_size: 8216\n",
      "batch 539, loss: 0.2392, label: 1, bag_size: 7246\n",
      "batch 559, loss: 0.0000, label: 1, bag_size: 14515\n",
      "batch 579, loss: 0.1450, label: 1, bag_size: 16154\n",
      "batch 599, loss: 0.0075, label: 1, bag_size: 11421\n",
      "batch 619, loss: 1.7490, label: 1, bag_size: 2759\n",
      "batch 639, loss: 0.0757, label: 0, bag_size: 3657\n",
      "batch 659, loss: 0.1067, label: 1, bag_size: 4250\n",
      "batch 679, loss: 0.0386, label: 1, bag_size: 15689\n",
      "batch 699, loss: 0.0239, label: 1, bag_size: 14779\n",
      "batch 719, loss: 2.4614, label: 1, bag_size: 1497\n",
      "batch 739, loss: 0.0031, label: 0, bag_size: 13225\n",
      "batch 759, loss: 0.0317, label: 0, bag_size: 2036\n",
      "batch 779, loss: 0.0040, label: 1, bag_size: 14681\n",
      "batch 799, loss: 0.0313, label: 0, bag_size: 2732\n",
      "batch 819, loss: 0.0336, label: 1, bag_size: 5864\n",
      "Epoch: 33, train_loss: 0.1847, train_error: 0.0720\n",
      "class 0: acc 0.9310344827586207, correct 378/406\n",
      "class 1: acc 0.9251207729468599, correct 383/414\n",
      "\n",
      "Val Set, val_loss: 0.1809, val_error: 0.0636, auc: 0.9824\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "EarlyStopping counter: 20 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1253, label: 1, bag_size: 1920\n",
      "batch 39, loss: 0.1074, label: 0, bag_size: 3541\n",
      "batch 59, loss: 0.0456, label: 1, bag_size: 12178\n",
      "batch 79, loss: 0.0035, label: 0, bag_size: 13795\n",
      "batch 99, loss: 0.4544, label: 0, bag_size: 25420\n",
      "batch 119, loss: 0.0429, label: 0, bag_size: 1800\n",
      "batch 139, loss: 0.2888, label: 1, bag_size: 1512\n",
      "batch 159, loss: 0.2554, label: 0, bag_size: 2266\n",
      "batch 179, loss: 0.0598, label: 1, bag_size: 1437\n",
      "batch 199, loss: 0.0055, label: 0, bag_size: 21319\n",
      "batch 219, loss: 0.0332, label: 1, bag_size: 13477\n",
      "batch 239, loss: 0.0012, label: 1, bag_size: 689\n",
      "batch 259, loss: 0.0194, label: 1, bag_size: 9446\n",
      "batch 279, loss: 0.0306, label: 1, bag_size: 3368\n",
      "batch 299, loss: 0.0311, label: 1, bag_size: 4239\n",
      "batch 319, loss: 0.0040, label: 0, bag_size: 19466\n",
      "batch 339, loss: 0.0177, label: 0, bag_size: 4465\n",
      "batch 359, loss: 0.0023, label: 1, bag_size: 9478\n",
      "batch 379, loss: 0.0255, label: 1, bag_size: 2814\n",
      "batch 399, loss: 0.1292, label: 0, bag_size: 23996\n",
      "batch 419, loss: 0.0202, label: 0, bag_size: 2063\n",
      "batch 439, loss: 0.2246, label: 1, bag_size: 11223\n",
      "batch 459, loss: 0.0864, label: 0, bag_size: 20555\n",
      "batch 479, loss: 0.0000, label: 1, bag_size: 10867\n",
      "batch 499, loss: 0.0580, label: 0, bag_size: 1690\n",
      "batch 519, loss: 0.0279, label: 1, bag_size: 10460\n",
      "batch 539, loss: 0.0091, label: 0, bag_size: 2036\n",
      "batch 559, loss: 0.3441, label: 1, bag_size: 12340\n",
      "batch 579, loss: 0.0768, label: 1, bag_size: 1525\n",
      "batch 599, loss: 0.0442, label: 1, bag_size: 8216\n",
      "batch 619, loss: 0.0537, label: 1, bag_size: 4789\n",
      "batch 639, loss: 0.1074, label: 0, bag_size: 11212\n",
      "batch 659, loss: 0.0417, label: 1, bag_size: 9561\n",
      "batch 679, loss: 0.1204, label: 1, bag_size: 7468\n",
      "batch 699, loss: 0.0188, label: 1, bag_size: 9446\n",
      "batch 719, loss: 1.3536, label: 0, bag_size: 4345\n",
      "batch 739, loss: 0.3595, label: 1, bag_size: 10622\n",
      "batch 759, loss: 0.0006, label: 0, bag_size: 2844\n",
      "batch 779, loss: 0.0001, label: 1, bag_size: 1781\n",
      "batch 799, loss: 0.0001, label: 1, bag_size: 2136\n",
      "batch 819, loss: 0.0076, label: 1, bag_size: 14202\n",
      "Epoch: 34, train_loss: 0.1701, train_error: 0.0622\n",
      "class 0: acc 0.9407407407407408, correct 381/405\n",
      "class 1: acc 0.9349397590361446, correct 388/415\n",
      "\n",
      "Val Set, val_loss: 0.1840, val_error: 0.0636, auc: 0.9808\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "EarlyStopping counter: 21 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1934, label: 1, bag_size: 12180\n",
      "batch 39, loss: 0.0090, label: 0, bag_size: 18240\n",
      "batch 59, loss: 0.0061, label: 0, bag_size: 12910\n",
      "batch 79, loss: 0.5531, label: 0, bag_size: 3444\n",
      "batch 99, loss: 0.0387, label: 1, bag_size: 11223\n",
      "batch 119, loss: 0.0013, label: 0, bag_size: 20150\n",
      "batch 139, loss: 0.0010, label: 0, bag_size: 1962\n",
      "batch 159, loss: 0.0066, label: 1, bag_size: 621\n",
      "batch 179, loss: 0.0060, label: 0, bag_size: 32227\n",
      "batch 199, loss: 0.0059, label: 1, bag_size: 8466\n",
      "batch 219, loss: 0.0006, label: 1, bag_size: 11642\n",
      "batch 239, loss: 0.0013, label: 1, bag_size: 9955\n",
      "batch 259, loss: 0.0004, label: 1, bag_size: 7650\n",
      "batch 279, loss: 0.1255, label: 0, bag_size: 2219\n",
      "batch 299, loss: 0.1212, label: 0, bag_size: 4523\n",
      "batch 319, loss: 0.0001, label: 1, bag_size: 3640\n",
      "batch 339, loss: 0.1361, label: 1, bag_size: 1525\n",
      "batch 359, loss: 0.0010, label: 1, bag_size: 4442\n",
      "batch 379, loss: 0.0620, label: 0, bag_size: 3552\n",
      "batch 399, loss: 0.0028, label: 1, bag_size: 5340\n",
      "batch 419, loss: 0.0031, label: 0, bag_size: 16992\n",
      "batch 439, loss: 0.1848, label: 0, bag_size: 23714\n",
      "batch 459, loss: 0.0000, label: 1, bag_size: 6752\n",
      "batch 479, loss: 0.2387, label: 1, bag_size: 8191\n",
      "batch 499, loss: 0.0037, label: 0, bag_size: 16782\n",
      "batch 519, loss: 0.0047, label: 0, bag_size: 21093\n",
      "batch 539, loss: 1.2880, label: 1, bag_size: 1284\n",
      "batch 559, loss: 0.0158, label: 0, bag_size: 6624\n",
      "batch 579, loss: 0.0072, label: 1, bag_size: 14230\n",
      "batch 599, loss: 0.0012, label: 0, bag_size: 12212\n",
      "batch 619, loss: 0.0771, label: 1, bag_size: 5231\n",
      "batch 639, loss: 1.0251, label: 1, bag_size: 6360\n",
      "batch 659, loss: 0.2046, label: 0, bag_size: 2920\n",
      "batch 679, loss: 0.0184, label: 0, bag_size: 1213\n",
      "batch 699, loss: 0.0000, label: 1, bag_size: 5221\n",
      "batch 719, loss: 0.0166, label: 0, bag_size: 9471\n",
      "batch 739, loss: 0.0329, label: 1, bag_size: 4956\n",
      "batch 759, loss: 0.0244, label: 1, bag_size: 14779\n",
      "batch 779, loss: 0.0581, label: 0, bag_size: 2266\n",
      "batch 799, loss: 0.0027, label: 1, bag_size: 15689\n",
      "batch 819, loss: 0.0881, label: 1, bag_size: 7798\n",
      "Epoch: 35, train_loss: 0.1531, train_error: 0.0561\n",
      "class 0: acc 0.9542168674698795, correct 396/415\n",
      "class 1: acc 0.9333333333333333, correct 378/405\n",
      "\n",
      "Val Set, val_loss: 0.1811, val_error: 0.0727, auc: 0.9818\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 22 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0048, label: 0, bag_size: 17630\n",
      "batch 39, loss: 0.0011, label: 1, bag_size: 3968\n",
      "batch 59, loss: 0.0408, label: 0, bag_size: 22498\n",
      "batch 79, loss: 0.0037, label: 0, bag_size: 11735\n",
      "batch 99, loss: 0.0243, label: 1, bag_size: 5612\n",
      "batch 119, loss: 0.0052, label: 0, bag_size: 22828\n",
      "batch 139, loss: 0.0013, label: 1, bag_size: 4442\n",
      "batch 159, loss: 0.3281, label: 0, bag_size: 23714\n",
      "batch 179, loss: 0.0033, label: 0, bag_size: 12201\n",
      "batch 199, loss: 0.0152, label: 0, bag_size: 2303\n",
      "batch 219, loss: 0.0108, label: 1, bag_size: 6606\n",
      "batch 239, loss: 0.7404, label: 0, bag_size: 5105\n",
      "batch 259, loss: 0.0044, label: 0, bag_size: 2296\n",
      "batch 279, loss: 0.0045, label: 0, bag_size: 21682\n",
      "batch 299, loss: 0.0009, label: 1, bag_size: 19606\n",
      "batch 319, loss: 0.0034, label: 1, bag_size: 1622\n",
      "batch 339, loss: 0.7527, label: 0, bag_size: 14664\n",
      "batch 359, loss: 0.1737, label: 1, bag_size: 12895\n",
      "batch 379, loss: 0.0034, label: 1, bag_size: 5723\n",
      "batch 399, loss: 0.0001, label: 1, bag_size: 8003\n",
      "batch 419, loss: 0.3242, label: 0, bag_size: 5409\n",
      "batch 439, loss: 0.0067, label: 1, bag_size: 7650\n",
      "batch 459, loss: 0.0048, label: 0, bag_size: 2844\n",
      "batch 479, loss: 0.4079, label: 1, bag_size: 7246\n",
      "batch 499, loss: 0.0088, label: 0, bag_size: 3190\n",
      "batch 519, loss: 0.1250, label: 0, bag_size: 3725\n",
      "batch 539, loss: 0.0611, label: 0, bag_size: 11212\n",
      "batch 559, loss: 0.1040, label: 1, bag_size: 2140\n",
      "batch 579, loss: 0.0005, label: 0, bag_size: 705\n",
      "batch 599, loss: 0.0942, label: 1, bag_size: 8012\n",
      "batch 619, loss: 0.0132, label: 1, bag_size: 3856\n",
      "batch 639, loss: 0.0583, label: 0, bag_size: 1508\n",
      "batch 659, loss: 0.8769, label: 0, bag_size: 2290\n",
      "batch 679, loss: 0.1354, label: 0, bag_size: 2043\n",
      "batch 699, loss: 0.0244, label: 1, bag_size: 2140\n",
      "batch 719, loss: 0.0043, label: 0, bag_size: 9470\n",
      "batch 739, loss: 0.0068, label: 1, bag_size: 13051\n",
      "batch 759, loss: 0.0001, label: 1, bag_size: 4394\n",
      "batch 779, loss: 0.7003, label: 1, bag_size: 21450\n",
      "batch 799, loss: 3.3493, label: 1, bag_size: 1038\n",
      "batch 819, loss: 0.0048, label: 0, bag_size: 10263\n",
      "Epoch: 36, train_loss: 0.1743, train_error: 0.0622\n",
      "class 0: acc 0.9432098765432099, correct 382/405\n",
      "class 1: acc 0.9325301204819277, correct 387/415\n",
      "\n",
      "Val Set, val_loss: 0.2220, val_error: 0.0909, auc: 0.9804\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 23 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0127, label: 0, bag_size: 11259\n",
      "batch 39, loss: 0.0013, label: 0, bag_size: 8252\n",
      "batch 59, loss: 0.0025, label: 0, bag_size: 14828\n",
      "batch 79, loss: 0.0501, label: 0, bag_size: 21864\n",
      "batch 99, loss: 1.8670, label: 1, bag_size: 1533\n",
      "batch 119, loss: 0.0010, label: 0, bag_size: 11917\n",
      "batch 139, loss: 0.0145, label: 0, bag_size: 5965\n",
      "batch 159, loss: 0.0047, label: 0, bag_size: 10751\n",
      "batch 179, loss: 0.0479, label: 0, bag_size: 7823\n",
      "batch 199, loss: 0.3685, label: 0, bag_size: 10381\n",
      "batch 219, loss: 0.0296, label: 1, bag_size: 1437\n",
      "batch 239, loss: 0.0002, label: 1, bag_size: 15213\n",
      "batch 259, loss: 0.0010, label: 0, bag_size: 14206\n",
      "batch 279, loss: 0.0000, label: 1, bag_size: 12611\n",
      "batch 299, loss: 0.0066, label: 1, bag_size: 11160\n",
      "batch 319, loss: 0.0547, label: 0, bag_size: 17155\n",
      "batch 339, loss: 0.0075, label: 0, bag_size: 705\n",
      "batch 359, loss: 1.3686, label: 0, bag_size: 2959\n",
      "batch 379, loss: 0.0555, label: 0, bag_size: 3810\n",
      "batch 399, loss: 0.4516, label: 1, bag_size: 12714\n",
      "batch 419, loss: 0.0066, label: 0, bag_size: 1452\n",
      "batch 439, loss: 0.0131, label: 1, bag_size: 4877\n",
      "batch 459, loss: 0.0003, label: 1, bag_size: 5833\n",
      "batch 479, loss: 0.0000, label: 1, bag_size: 6875\n",
      "batch 499, loss: 0.0001, label: 1, bag_size: 19039\n",
      "batch 519, loss: 0.5011, label: 1, bag_size: 4308\n",
      "batch 539, loss: 0.2039, label: 1, bag_size: 11223\n",
      "batch 559, loss: 0.0050, label: 0, bag_size: 11759\n",
      "batch 579, loss: 0.0141, label: 1, bag_size: 2662\n",
      "batch 599, loss: 0.0098, label: 0, bag_size: 8981\n",
      "batch 619, loss: 0.0035, label: 0, bag_size: 10791\n",
      "batch 639, loss: 0.0019, label: 0, bag_size: 14305\n",
      "batch 659, loss: 0.0325, label: 0, bag_size: 11259\n",
      "batch 679, loss: 0.4418, label: 0, bag_size: 2609\n",
      "batch 699, loss: 0.0325, label: 1, bag_size: 8660\n",
      "batch 719, loss: 0.0100, label: 0, bag_size: 11477\n",
      "batch 739, loss: 0.0045, label: 0, bag_size: 1909\n",
      "batch 759, loss: 0.2678, label: 1, bag_size: 21252\n",
      "batch 779, loss: 0.0000, label: 1, bag_size: 9673\n",
      "batch 799, loss: 0.0347, label: 1, bag_size: 7981\n",
      "batch 819, loss: 0.0002, label: 0, bag_size: 10481\n",
      "Epoch: 37, train_loss: 0.1626, train_error: 0.0659\n",
      "class 0: acc 0.9370277078085643, correct 372/397\n",
      "class 1: acc 0.9314420803782506, correct 394/423\n",
      "\n",
      "Val Set, val_loss: 0.1833, val_error: 0.0727, auc: 0.9811\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 24 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0022, label: 0, bag_size: 3657\n",
      "batch 39, loss: 0.0142, label: 1, bag_size: 4880\n",
      "batch 59, loss: 0.0024, label: 0, bag_size: 23037\n",
      "batch 79, loss: 0.0021, label: 0, bag_size: 11654\n",
      "batch 99, loss: 0.0013, label: 1, bag_size: 617\n",
      "batch 119, loss: 0.0004, label: 1, bag_size: 3640\n",
      "batch 139, loss: 0.0000, label: 1, bag_size: 7078\n",
      "batch 159, loss: 0.0179, label: 0, bag_size: 10381\n",
      "batch 179, loss: 0.0019, label: 1, bag_size: 21827\n",
      "batch 199, loss: 0.0002, label: 1, bag_size: 12712\n",
      "batch 219, loss: 0.1280, label: 0, bag_size: 2242\n",
      "batch 239, loss: 0.0114, label: 1, bag_size: 2678\n",
      "batch 259, loss: 0.3979, label: 0, bag_size: 9596\n",
      "batch 279, loss: 0.0055, label: 0, bag_size: 1891\n",
      "batch 299, loss: 0.0007, label: 1, bag_size: 4259\n",
      "batch 319, loss: 0.0016, label: 0, bag_size: 12731\n",
      "batch 339, loss: 0.0601, label: 0, bag_size: 3541\n",
      "batch 359, loss: 0.0011, label: 1, bag_size: 15213\n",
      "batch 379, loss: 0.0137, label: 0, bag_size: 2760\n",
      "batch 399, loss: 0.0002, label: 0, bag_size: 2424\n",
      "batch 419, loss: 0.0022, label: 0, bag_size: 5551\n",
      "batch 439, loss: 0.9282, label: 0, bag_size: 47866\n",
      "batch 459, loss: 0.0000, label: 1, bag_size: 14515\n",
      "batch 479, loss: 0.2276, label: 0, bag_size: 11922\n",
      "batch 499, loss: 0.0013, label: 0, bag_size: 1760\n",
      "batch 519, loss: 0.0256, label: 0, bag_size: 22681\n",
      "batch 539, loss: 0.0001, label: 1, bag_size: 15716\n",
      "batch 559, loss: 0.0222, label: 1, bag_size: 3450\n",
      "batch 579, loss: 0.0024, label: 0, bag_size: 3787\n",
      "batch 599, loss: 0.0358, label: 1, bag_size: 7798\n",
      "batch 619, loss: 0.3178, label: 0, bag_size: 10410\n",
      "batch 639, loss: 0.0130, label: 0, bag_size: 1814\n",
      "batch 659, loss: 0.0082, label: 0, bag_size: 2814\n",
      "batch 679, loss: 0.0000, label: 1, bag_size: 6752\n",
      "batch 699, loss: 0.0129, label: 1, bag_size: 20537\n",
      "batch 719, loss: 0.0007, label: 1, bag_size: 621\n",
      "batch 739, loss: 0.0274, label: 0, bag_size: 3228\n",
      "batch 759, loss: 0.0004, label: 1, bag_size: 8522\n",
      "batch 779, loss: 0.0001, label: 1, bag_size: 13947\n",
      "batch 799, loss: 0.2218, label: 0, bag_size: 7637\n",
      "batch 819, loss: 0.1481, label: 0, bag_size: 11128\n",
      "Epoch: 38, train_loss: 0.1917, train_error: 0.0707\n",
      "class 0: acc 0.9452380952380952, correct 397/420\n",
      "class 1: acc 0.9125, correct 365/400\n",
      "\n",
      "Val Set, val_loss: 0.1885, val_error: 0.0818, auc: 0.9791\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "EarlyStopping counter: 25 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0095, label: 0, bag_size: 9234\n",
      "batch 39, loss: 0.4116, label: 0, bag_size: 2270\n",
      "batch 59, loss: 0.0084, label: 0, bag_size: 23368\n",
      "batch 79, loss: 0.0695, label: 1, bag_size: 2179\n",
      "batch 99, loss: 0.0181, label: 1, bag_size: 1493\n",
      "batch 119, loss: 0.1598, label: 0, bag_size: 7239\n",
      "batch 139, loss: 0.0446, label: 1, bag_size: 2842\n",
      "batch 159, loss: 0.0955, label: 0, bag_size: 13619\n",
      "batch 179, loss: 0.5736, label: 0, bag_size: 1637\n",
      "batch 199, loss: 0.0023, label: 0, bag_size: 2282\n",
      "batch 219, loss: 0.0546, label: 1, bag_size: 16514\n",
      "batch 239, loss: 1.2264, label: 0, bag_size: 1701\n",
      "batch 259, loss: 0.0367, label: 1, bag_size: 4308\n",
      "batch 279, loss: 0.0003, label: 1, bag_size: 6453\n",
      "batch 299, loss: 0.0606, label: 1, bag_size: 2356\n",
      "batch 319, loss: 1.8536, label: 1, bag_size: 1051\n",
      "batch 339, loss: 0.5724, label: 0, bag_size: 18738\n",
      "batch 359, loss: 0.0101, label: 0, bag_size: 11654\n",
      "batch 379, loss: 0.0144, label: 0, bag_size: 9542\n",
      "batch 399, loss: 0.0016, label: 1, bag_size: 5723\n",
      "batch 419, loss: 0.0029, label: 0, bag_size: 8981\n",
      "batch 439, loss: 0.0015, label: 0, bag_size: 2820\n",
      "batch 459, loss: 0.0137, label: 1, bag_size: 5612\n",
      "batch 479, loss: 0.3194, label: 0, bag_size: 6367\n",
      "batch 499, loss: 0.0437, label: 0, bag_size: 2732\n",
      "batch 519, loss: 0.3075, label: 1, bag_size: 3450\n",
      "batch 539, loss: 0.1186, label: 1, bag_size: 12895\n",
      "batch 559, loss: 0.0000, label: 1, bag_size: 12349\n",
      "batch 579, loss: 0.0013, label: 1, bag_size: 4821\n",
      "batch 599, loss: 0.0053, label: 0, bag_size: 1984\n",
      "batch 619, loss: 0.0002, label: 1, bag_size: 14433\n",
      "batch 639, loss: 0.0074, label: 1, bag_size: 2638\n",
      "batch 659, loss: 0.0003, label: 1, bag_size: 14604\n",
      "batch 679, loss: 0.0060, label: 0, bag_size: 6851\n",
      "batch 699, loss: 0.0004, label: 0, bag_size: 2820\n",
      "batch 719, loss: 0.0003, label: 0, bag_size: 3459\n",
      "batch 739, loss: 0.0000, label: 1, bag_size: 10867\n",
      "batch 759, loss: 0.0002, label: 1, bag_size: 6453\n",
      "batch 779, loss: 0.0093, label: 1, bag_size: 4308\n",
      "batch 799, loss: 0.0116, label: 0, bag_size: 1438\n",
      "batch 819, loss: 0.0002, label: 1, bag_size: 6343\n",
      "Epoch: 39, train_loss: 0.1689, train_error: 0.0683\n",
      "class 0: acc 0.9439024390243902, correct 387/410\n",
      "class 1: acc 0.9195121951219513, correct 377/410\n",
      "\n",
      "Val Set, val_loss: 0.1805, val_error: 0.0636, auc: 0.9821\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9137931034482759, correct 53/58\n",
      "EarlyStopping counter: 26 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0501, label: 1, bag_size: 2814\n",
      "batch 39, loss: 1.1041, label: 0, bag_size: 2070\n",
      "batch 59, loss: 0.0023, label: 0, bag_size: 1483\n",
      "batch 79, loss: 1.2456, label: 1, bag_size: 1242\n",
      "batch 99, loss: 0.7672, label: 1, bag_size: 2344\n",
      "batch 119, loss: 0.4746, label: 1, bag_size: 983\n",
      "batch 139, loss: 0.0143, label: 0, bag_size: 2244\n",
      "batch 159, loss: 0.0000, label: 1, bag_size: 5731\n",
      "batch 179, loss: 0.0125, label: 0, bag_size: 14625\n",
      "batch 199, loss: 0.0029, label: 0, bag_size: 1349\n",
      "batch 219, loss: 0.0010, label: 0, bag_size: 4902\n",
      "batch 239, loss: 0.0001, label: 1, bag_size: 3295\n",
      "batch 259, loss: 0.3760, label: 0, bag_size: 18215\n",
      "batch 279, loss: 0.1888, label: 1, bag_size: 2140\n",
      "batch 299, loss: 0.0436, label: 0, bag_size: 8330\n",
      "batch 319, loss: 0.3661, label: 1, bag_size: 15563\n",
      "batch 339, loss: 0.0006, label: 1, bag_size: 7381\n",
      "batch 359, loss: 0.0209, label: 0, bag_size: 11187\n",
      "batch 379, loss: 0.2161, label: 0, bag_size: 5211\n",
      "batch 399, loss: 0.0183, label: 1, bag_size: 8592\n",
      "batch 419, loss: 0.0000, label: 1, bag_size: 7078\n",
      "batch 439, loss: 0.0030, label: 0, bag_size: 15636\n",
      "batch 459, loss: 0.0317, label: 1, bag_size: 2480\n",
      "batch 479, loss: 0.0111, label: 0, bag_size: 12593\n",
      "batch 499, loss: 0.2995, label: 0, bag_size: 8788\n",
      "batch 519, loss: 0.0493, label: 0, bag_size: 21319\n",
      "batch 539, loss: 0.0151, label: 0, bag_size: 9866\n",
      "batch 559, loss: 0.0008, label: 1, bag_size: 5629\n",
      "batch 579, loss: 0.0208, label: 1, bag_size: 10072\n",
      "batch 599, loss: 0.0058, label: 1, bag_size: 5723\n",
      "batch 619, loss: 0.4153, label: 0, bag_size: 2098\n",
      "batch 639, loss: 0.0592, label: 0, bag_size: 3810\n",
      "batch 659, loss: 0.0016, label: 0, bag_size: 12201\n",
      "batch 679, loss: 0.0014, label: 1, bag_size: 2136\n",
      "batch 699, loss: 0.4949, label: 0, bag_size: 3783\n",
      "batch 719, loss: 0.1226, label: 0, bag_size: 8661\n",
      "batch 739, loss: 0.0002, label: 1, bag_size: 10920\n",
      "batch 759, loss: 0.1273, label: 1, bag_size: 5894\n",
      "batch 779, loss: 0.0046, label: 0, bag_size: 15841\n",
      "batch 799, loss: 0.0277, label: 0, bag_size: 3552\n",
      "batch 819, loss: 1.5431, label: 1, bag_size: 1822\n",
      "Epoch: 40, train_loss: 0.1832, train_error: 0.0695\n",
      "class 0: acc 0.9382716049382716, correct 380/405\n",
      "class 1: acc 0.9228915662650602, correct 383/415\n",
      "\n",
      "Val Set, val_loss: 0.1710, val_error: 0.0545, auc: 0.9828\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "EarlyStopping counter: 27 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0030, label: 1, bag_size: 7217\n",
      "batch 39, loss: 0.9307, label: 1, bag_size: 2937\n",
      "batch 59, loss: 0.0032, label: 1, bag_size: 6606\n",
      "batch 79, loss: 0.0015, label: 0, bag_size: 5551\n",
      "batch 99, loss: 0.0199, label: 1, bag_size: 20333\n",
      "batch 119, loss: 0.7783, label: 0, bag_size: 2959\n",
      "batch 139, loss: 0.0010, label: 0, bag_size: 23791\n",
      "batch 159, loss: 2.4843, label: 0, bag_size: 2815\n",
      "batch 179, loss: 0.3581, label: 1, bag_size: 3453\n",
      "batch 199, loss: 0.0010, label: 0, bag_size: 13964\n",
      "batch 219, loss: 0.0109, label: 1, bag_size: 3368\n",
      "batch 239, loss: 1.1361, label: 1, bag_size: 15563\n",
      "batch 259, loss: 0.0010, label: 1, bag_size: 19932\n",
      "batch 279, loss: 0.3480, label: 0, bag_size: 6356\n",
      "batch 299, loss: 0.0087, label: 1, bag_size: 11600\n",
      "batch 319, loss: 0.0024, label: 1, bag_size: 2638\n",
      "batch 339, loss: 0.0988, label: 0, bag_size: 22498\n",
      "batch 359, loss: 0.0527, label: 0, bag_size: 5211\n",
      "batch 379, loss: 0.0042, label: 1, bag_size: 9877\n",
      "batch 399, loss: 0.0035, label: 1, bag_size: 13015\n",
      "batch 419, loss: 0.1100, label: 0, bag_size: 1592\n",
      "batch 439, loss: 0.0033, label: 0, bag_size: 1438\n",
      "batch 459, loss: 0.0105, label: 0, bag_size: 14956\n",
      "batch 479, loss: 0.0209, label: 0, bag_size: 2814\n",
      "batch 499, loss: 0.0004, label: 1, bag_size: 7873\n",
      "batch 519, loss: 0.3819, label: 0, bag_size: 14249\n",
      "batch 539, loss: 0.0078, label: 1, bag_size: 11684\n",
      "batch 559, loss: 0.0579, label: 0, bag_size: 1213\n",
      "batch 579, loss: 2.4034, label: 1, bag_size: 2344\n",
      "batch 599, loss: 0.0054, label: 1, bag_size: 16565\n",
      "batch 619, loss: 0.0151, label: 1, bag_size: 8438\n",
      "batch 639, loss: 0.0021, label: 1, bag_size: 6731\n",
      "batch 659, loss: 0.0057, label: 0, bag_size: 23037\n",
      "batch 679, loss: 0.0200, label: 0, bag_size: 2920\n",
      "batch 699, loss: 0.0180, label: 0, bag_size: 2244\n",
      "batch 719, loss: 0.0919, label: 0, bag_size: 15747\n",
      "batch 739, loss: 0.3292, label: 0, bag_size: 9252\n",
      "batch 759, loss: 0.0026, label: 0, bag_size: 19043\n",
      "batch 779, loss: 0.0229, label: 1, bag_size: 13477\n",
      "batch 799, loss: 0.0122, label: 0, bag_size: 14625\n",
      "batch 819, loss: 0.0763, label: 0, bag_size: 11194\n",
      "Epoch: 41, train_loss: 0.1798, train_error: 0.0720\n",
      "class 0: acc 0.9383561643835616, correct 411/438\n",
      "class 1: acc 0.9162303664921466, correct 350/382\n",
      "\n",
      "Val Set, val_loss: 0.2108, val_error: 0.0909, auc: 0.9798\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 28 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0090, label: 0, bag_size: 8755\n",
      "batch 39, loss: 0.0012, label: 1, bag_size: 1014\n",
      "batch 59, loss: 0.0013, label: 0, bag_size: 11199\n",
      "batch 79, loss: 0.0042, label: 1, bag_size: 11600\n",
      "batch 99, loss: 0.0031, label: 0, bag_size: 12217\n",
      "batch 119, loss: 0.1177, label: 1, bag_size: 1123\n",
      "batch 139, loss: 0.0020, label: 0, bag_size: 14625\n",
      "batch 159, loss: 0.0135, label: 1, bag_size: 11518\n",
      "batch 179, loss: 0.0353, label: 1, bag_size: 8103\n",
      "batch 199, loss: 0.2930, label: 0, bag_size: 5297\n",
      "batch 219, loss: 0.0053, label: 0, bag_size: 890\n",
      "batch 239, loss: 0.6183, label: 1, bag_size: 2935\n",
      "batch 259, loss: 0.4263, label: 0, bag_size: 12510\n",
      "batch 279, loss: 0.0137, label: 0, bag_size: 1349\n",
      "batch 299, loss: 0.0106, label: 0, bag_size: 1884\n",
      "batch 319, loss: 0.1066, label: 0, bag_size: 6356\n",
      "batch 339, loss: 0.0066, label: 0, bag_size: 10791\n",
      "batch 359, loss: 0.0004, label: 1, bag_size: 621\n",
      "batch 379, loss: 0.0869, label: 0, bag_size: 16087\n",
      "batch 399, loss: 0.0171, label: 1, bag_size: 16514\n",
      "batch 419, loss: 0.0096, label: 1, bag_size: 10492\n",
      "batch 439, loss: 0.0067, label: 1, bag_size: 20333\n",
      "batch 459, loss: 0.0075, label: 1, bag_size: 928\n",
      "batch 479, loss: 0.0459, label: 1, bag_size: 7389\n",
      "batch 499, loss: 0.0004, label: 1, bag_size: 4442\n",
      "batch 519, loss: 0.0537, label: 1, bag_size: 1064\n",
      "batch 539, loss: 0.0078, label: 0, bag_size: 9455\n",
      "batch 559, loss: 0.0820, label: 0, bag_size: 21864\n",
      "batch 579, loss: 0.0009, label: 0, bag_size: 3101\n",
      "batch 599, loss: 0.1658, label: 1, bag_size: 4054\n",
      "batch 619, loss: 0.0396, label: 1, bag_size: 16514\n",
      "batch 639, loss: 0.0008, label: 1, bag_size: 11220\n",
      "batch 659, loss: 0.0951, label: 0, bag_size: 3541\n",
      "batch 679, loss: 0.0388, label: 0, bag_size: 2624\n",
      "batch 699, loss: 0.0212, label: 0, bag_size: 2814\n",
      "batch 719, loss: 0.0105, label: 1, bag_size: 1493\n",
      "batch 739, loss: 0.0138, label: 1, bag_size: 5454\n",
      "batch 759, loss: 0.0295, label: 0, bag_size: 7381\n",
      "batch 779, loss: 0.0003, label: 0, bag_size: 10481\n",
      "batch 799, loss: 0.4670, label: 0, bag_size: 4418\n",
      "batch 819, loss: 0.4158, label: 0, bag_size: 12840\n",
      "Epoch: 42, train_loss: 0.1701, train_error: 0.0585\n",
      "class 0: acc 0.9382716049382716, correct 380/405\n",
      "class 1: acc 0.944578313253012, correct 392/415\n",
      "\n",
      "Val Set, val_loss: 0.1862, val_error: 0.0727, auc: 0.9821\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.896551724137931, correct 52/58\n",
      "EarlyStopping counter: 29 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0304, label: 0, bag_size: 4465\n",
      "batch 39, loss: 0.0008, label: 0, bag_size: 3787\n",
      "batch 59, loss: 0.0673, label: 1, bag_size: 1920\n",
      "batch 79, loss: 0.0089, label: 0, bag_size: 14305\n",
      "batch 99, loss: 0.0589, label: 0, bag_size: 3089\n",
      "batch 119, loss: 0.8802, label: 0, bag_size: 3802\n",
      "batch 139, loss: 0.0291, label: 1, bag_size: 1339\n",
      "batch 159, loss: 0.1146, label: 0, bag_size: 16087\n",
      "batch 179, loss: 0.0207, label: 1, bag_size: 22286\n",
      "batch 199, loss: 0.0192, label: 1, bag_size: 12603\n",
      "batch 219, loss: 0.0107, label: 0, bag_size: 14681\n",
      "batch 239, loss: 0.0197, label: 0, bag_size: 10415\n",
      "batch 259, loss: 0.1286, label: 1, bag_size: 9322\n",
      "batch 279, loss: 0.0115, label: 0, bag_size: 1772\n",
      "batch 299, loss: 0.0128, label: 1, bag_size: 25970\n",
      "batch 319, loss: 0.2933, label: 0, bag_size: 18777\n",
      "batch 339, loss: 4.0719, label: 1, bag_size: 1963\n",
      "batch 359, loss: 0.0478, label: 1, bag_size: 13732\n",
      "batch 379, loss: 0.1332, label: 0, bag_size: 2920\n",
      "batch 399, loss: 0.1642, label: 1, bag_size: 8680\n",
      "batch 419, loss: 0.0000, label: 1, bag_size: 12611\n",
      "batch 439, loss: 0.0033, label: 0, bag_size: 9171\n",
      "batch 459, loss: 0.0034, label: 0, bag_size: 18154\n",
      "batch 479, loss: 0.0090, label: 0, bag_size: 31780\n",
      "batch 499, loss: 0.0000, label: 1, bag_size: 11195\n",
      "batch 519, loss: 3.4896, label: 1, bag_size: 3879\n",
      "batch 539, loss: 0.1432, label: 0, bag_size: 4997\n",
      "batch 559, loss: 0.2064, label: 1, bag_size: 10912\n",
      "batch 579, loss: 0.0472, label: 0, bag_size: 24911\n",
      "batch 599, loss: 0.0040, label: 0, bag_size: 11199\n",
      "batch 619, loss: 0.0062, label: 0, bag_size: 11122\n",
      "batch 639, loss: 0.0052, label: 0, bag_size: 16341\n",
      "batch 659, loss: 0.0002, label: 1, bag_size: 9971\n",
      "batch 679, loss: 0.0203, label: 1, bag_size: 29832\n",
      "batch 699, loss: 0.0013, label: 1, bag_size: 15125\n",
      "batch 719, loss: 0.0187, label: 0, bag_size: 2534\n",
      "batch 739, loss: 0.0006, label: 0, bag_size: 16782\n",
      "batch 759, loss: 0.0060, label: 0, bag_size: 3657\n",
      "batch 779, loss: 0.3916, label: 0, bag_size: 6356\n",
      "batch 799, loss: 0.0002, label: 1, bag_size: 9078\n",
      "batch 819, loss: 0.0047, label: 0, bag_size: 11546\n",
      "Epoch: 43, train_loss: 0.1534, train_error: 0.0512\n",
      "class 0: acc 0.948905109489051, correct 390/411\n",
      "class 1: acc 0.9486552567237164, correct 388/409\n",
      "\n",
      "Val Set, val_loss: 0.2091, val_error: 0.0818, auc: 0.9811\n",
      "class 0: acc 0.9807692307692307, correct 51/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "EarlyStopping counter: 30 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0633, label: 0, bag_size: 8959\n",
      "batch 39, loss: 0.0002, label: 1, bag_size: 9065\n",
      "batch 59, loss: 1.1317, label: 0, bag_size: 4345\n",
      "batch 79, loss: 0.4709, label: 0, bag_size: 18738\n",
      "batch 99, loss: 0.5030, label: 0, bag_size: 7835\n",
      "batch 119, loss: 0.0022, label: 0, bag_size: 1984\n",
      "batch 139, loss: 1.0298, label: 1, bag_size: 2344\n",
      "batch 159, loss: 0.0073, label: 1, bag_size: 25970\n",
      "batch 179, loss: 0.0013, label: 1, bag_size: 1622\n",
      "batch 199, loss: 0.0008, label: 1, bag_size: 12795\n",
      "batch 219, loss: 4.9640, label: 0, bag_size: 2815\n",
      "batch 239, loss: 0.3083, label: 1, bag_size: 1819\n",
      "batch 259, loss: 0.0019, label: 1, bag_size: 19606\n",
      "batch 279, loss: 0.0014, label: 1, bag_size: 5256\n",
      "batch 299, loss: 0.0099, label: 0, bag_size: 3265\n",
      "batch 319, loss: 1.9867, label: 0, bag_size: 21361\n",
      "batch 339, loss: 0.0005, label: 0, bag_size: 9433\n",
      "batch 359, loss: 0.0020, label: 0, bag_size: 11199\n",
      "batch 379, loss: 0.0038, label: 0, bag_size: 18240\n",
      "batch 399, loss: 0.4601, label: 0, bag_size: 7835\n",
      "batch 419, loss: 0.9842, label: 0, bag_size: 4345\n",
      "batch 439, loss: 0.0066, label: 1, bag_size: 11316\n",
      "batch 459, loss: 0.0459, label: 0, bag_size: 11187\n",
      "batch 479, loss: 0.0053, label: 1, bag_size: 2678\n",
      "batch 499, loss: 0.0005, label: 1, bag_size: 11266\n",
      "batch 519, loss: 0.0073, label: 1, bag_size: 11642\n",
      "batch 539, loss: 0.0861, label: 0, bag_size: 13591\n",
      "batch 559, loss: 0.0005, label: 0, bag_size: 7191\n",
      "batch 579, loss: 0.0058, label: 0, bag_size: 23796\n",
      "batch 599, loss: 0.0006, label: 1, bag_size: 19039\n",
      "batch 619, loss: 0.0048, label: 1, bag_size: 12719\n",
      "batch 639, loss: 0.0143, label: 0, bag_size: 17268\n",
      "batch 659, loss: 0.0014, label: 0, bag_size: 9433\n",
      "batch 679, loss: 0.0014, label: 1, bag_size: 1014\n",
      "batch 699, loss: 0.1267, label: 1, bag_size: 2140\n",
      "batch 719, loss: 0.0036, label: 1, bag_size: 10392\n",
      "batch 739, loss: 0.0381, label: 0, bag_size: 2360\n",
      "batch 759, loss: 0.0189, label: 1, bag_size: 10028\n",
      "batch 779, loss: 0.0975, label: 1, bag_size: 2140\n",
      "batch 799, loss: 0.0291, label: 1, bag_size: 13089\n",
      "batch 819, loss: 0.0998, label: 1, bag_size: 1823\n",
      "Epoch: 44, train_loss: 0.1693, train_error: 0.0585\n",
      "class 0: acc 0.9543269230769231, correct 397/416\n",
      "class 1: acc 0.9282178217821783, correct 375/404\n",
      "\n",
      "Val Set, val_loss: 0.1777, val_error: 0.0636, auc: 0.9824\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 31 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3560, label: 0, bag_size: 9252\n",
      "batch 39, loss: 0.0016, label: 1, bag_size: 12575\n",
      "batch 59, loss: 0.0000, label: 1, bag_size: 7078\n",
      "batch 79, loss: 0.0588, label: 0, bag_size: 8959\n",
      "batch 99, loss: 0.0024, label: 0, bag_size: 23037\n",
      "batch 119, loss: 0.0007, label: 1, bag_size: 7935\n",
      "batch 139, loss: 0.2400, label: 1, bag_size: 21450\n",
      "batch 159, loss: 0.0241, label: 0, bag_size: 21218\n",
      "batch 179, loss: 0.0089, label: 1, bag_size: 4956\n",
      "batch 199, loss: 0.0038, label: 1, bag_size: 2278\n",
      "batch 219, loss: 0.0042, label: 1, bag_size: 1683\n",
      "batch 239, loss: 0.0021, label: 1, bag_size: 10392\n",
      "batch 259, loss: 0.0012, label: 1, bag_size: 4821\n",
      "batch 279, loss: 0.0092, label: 0, bag_size: 2814\n",
      "batch 299, loss: 0.0538, label: 0, bag_size: 2351\n",
      "batch 319, loss: 0.0009, label: 0, bag_size: 1416\n",
      "batch 339, loss: 0.1028, label: 1, bag_size: 6682\n",
      "batch 359, loss: 0.0646, label: 0, bag_size: 1592\n",
      "batch 379, loss: 0.0002, label: 1, bag_size: 5561\n",
      "batch 399, loss: 0.0021, label: 0, bag_size: 8252\n",
      "batch 419, loss: 0.0102, label: 0, bag_size: 23796\n",
      "batch 439, loss: 0.0020, label: 0, bag_size: 5225\n",
      "batch 459, loss: 3.0593, label: 1, bag_size: 684\n",
      "batch 479, loss: 0.0462, label: 0, bag_size: 1639\n",
      "batch 499, loss: 0.0014, label: 0, bag_size: 2296\n",
      "batch 519, loss: 0.0013, label: 1, bag_size: 3968\n",
      "batch 539, loss: 0.0040, label: 0, bag_size: 22828\n",
      "batch 559, loss: 0.0002, label: 0, bag_size: 2628\n",
      "batch 579, loss: 0.0000, label: 1, bag_size: 15233\n",
      "batch 599, loss: 0.0027, label: 0, bag_size: 6851\n",
      "batch 619, loss: 0.0643, label: 1, bag_size: 8982\n",
      "batch 639, loss: 0.0222, label: 1, bag_size: 16565\n",
      "batch 659, loss: 0.1344, label: 1, bag_size: 10432\n",
      "batch 679, loss: 0.1068, label: 0, bag_size: 9069\n",
      "batch 699, loss: 0.0000, label: 1, bag_size: 13947\n",
      "batch 719, loss: 0.0073, label: 1, bag_size: 7768\n",
      "batch 739, loss: 0.0046, label: 0, bag_size: 2367\n",
      "batch 759, loss: 0.0865, label: 0, bag_size: 11151\n",
      "batch 779, loss: 0.0001, label: 1, bag_size: 10392\n",
      "batch 799, loss: 0.0115, label: 1, bag_size: 8592\n",
      "batch 819, loss: 0.3957, label: 1, bag_size: 5160\n",
      "Epoch: 45, train_loss: 0.1448, train_error: 0.0610\n",
      "class 0: acc 0.9418886198547215, correct 389/413\n",
      "class 1: acc 0.9361179361179361, correct 381/407\n",
      "\n",
      "Val Set, val_loss: 0.1739, val_error: 0.0545, auc: 0.9831\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 32 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0042, label: 1, bag_size: 1022\n",
      "batch 39, loss: 0.6101, label: 0, bag_size: 25420\n",
      "batch 59, loss: 0.0085, label: 0, bag_size: 16992\n",
      "batch 79, loss: 0.1279, label: 0, bag_size: 20555\n",
      "batch 99, loss: 0.0082, label: 1, bag_size: 9877\n",
      "batch 119, loss: 0.0033, label: 0, bag_size: 12793\n",
      "batch 139, loss: 0.0831, label: 0, bag_size: 2104\n",
      "batch 159, loss: 0.0045, label: 0, bag_size: 18240\n",
      "batch 179, loss: 0.0000, label: 1, bag_size: 7767\n",
      "batch 199, loss: 0.0384, label: 0, bag_size: 4598\n",
      "batch 219, loss: 0.0044, label: 0, bag_size: 10490\n",
      "batch 239, loss: 0.0036, label: 1, bag_size: 12460\n",
      "batch 259, loss: 0.0075, label: 0, bag_size: 23791\n",
      "batch 279, loss: 0.0322, label: 1, bag_size: 11032\n",
      "batch 299, loss: 1.3303, label: 1, bag_size: 1284\n",
      "batch 319, loss: 0.0926, label: 1, bag_size: 7798\n",
      "batch 339, loss: 0.0138, label: 1, bag_size: 11256\n",
      "batch 359, loss: 0.0000, label: 1, bag_size: 14515\n",
      "batch 379, loss: 0.0014, label: 0, bag_size: 19518\n",
      "batch 399, loss: 0.0032, label: 0, bag_size: 17791\n",
      "batch 419, loss: 0.0138, label: 1, bag_size: 4789\n",
      "batch 439, loss: 0.0217, label: 1, bag_size: 9322\n",
      "batch 459, loss: 0.0176, label: 1, bag_size: 8448\n",
      "batch 479, loss: 0.0000, label: 1, bag_size: 11875\n",
      "batch 499, loss: 0.0974, label: 0, bag_size: 22681\n",
      "batch 519, loss: 0.0546, label: 1, bag_size: 10912\n",
      "batch 539, loss: 0.0000, label: 1, bag_size: 1781\n",
      "batch 559, loss: 0.0018, label: 0, bag_size: 23368\n",
      "batch 579, loss: 0.0022, label: 0, bag_size: 1438\n",
      "batch 599, loss: 0.0075, label: 1, bag_size: 8026\n",
      "batch 619, loss: 0.0067, label: 1, bag_size: 8592\n",
      "batch 639, loss: 0.0052, label: 1, bag_size: 3683\n",
      "batch 659, loss: 0.5669, label: 0, bag_size: 26208\n",
      "batch 679, loss: 0.0685, label: 0, bag_size: 11922\n",
      "batch 699, loss: 0.0375, label: 0, bag_size: 1920\n",
      "batch 719, loss: 0.3464, label: 1, bag_size: 8982\n",
      "batch 739, loss: 0.0024, label: 0, bag_size: 17630\n",
      "batch 759, loss: 0.0021, label: 1, bag_size: 5723\n",
      "batch 779, loss: 2.0323, label: 1, bag_size: 9162\n",
      "batch 799, loss: 0.0668, label: 0, bag_size: 9949\n",
      "batch 819, loss: 0.0820, label: 0, bag_size: 3810\n",
      "Epoch: 46, train_loss: 0.1650, train_error: 0.0573\n",
      "class 0: acc 0.9514066496163683, correct 372/391\n",
      "class 1: acc 0.9347319347319347, correct 401/429\n",
      "\n",
      "Val Set, val_loss: 0.2037, val_error: 0.0818, auc: 0.9824\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 33 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.0450, label: 1, bag_size: 2731\n",
      "batch 39, loss: 0.0006, label: 1, bag_size: 1412\n",
      "batch 59, loss: 0.0113, label: 0, bag_size: 763\n",
      "batch 79, loss: 0.0285, label: 0, bag_size: 24439\n",
      "batch 99, loss: 0.0000, label: 1, bag_size: 6875\n",
      "batch 119, loss: 0.2347, label: 1, bag_size: 8103\n",
      "batch 139, loss: 0.0027, label: 0, bag_size: 3101\n",
      "batch 159, loss: 0.0219, label: 1, bag_size: 16514\n",
      "batch 179, loss: 0.0024, label: 0, bag_size: 803\n",
      "batch 199, loss: 0.0002, label: 1, bag_size: 12712\n",
      "batch 219, loss: 0.3642, label: 1, bag_size: 1533\n",
      "batch 239, loss: 0.0000, label: 1, bag_size: 12349\n",
      "batch 259, loss: 0.0830, label: 1, bag_size: 549\n",
      "batch 279, loss: 0.3078, label: 0, bag_size: 5009\n",
      "batch 299, loss: 0.1761, label: 1, bag_size: 9561\n",
      "batch 319, loss: 0.0000, label: 1, bag_size: 14515\n",
      "batch 339, loss: 0.0839, label: 0, bag_size: 5409\n",
      "batch 359, loss: 0.0015, label: 1, bag_size: 5690\n",
      "batch 379, loss: 0.0124, label: 1, bag_size: 621\n",
      "batch 399, loss: 0.8347, label: 0, bag_size: 3468\n",
      "batch 419, loss: 0.2547, label: 1, bag_size: 645\n",
      "batch 439, loss: 0.3413, label: 1, bag_size: 1609\n",
      "batch 459, loss: 0.0003, label: 1, bag_size: 18794\n",
      "batch 479, loss: 0.0020, label: 1, bag_size: 12719\n",
      "batch 499, loss: 0.4366, label: 1, bag_size: 1242\n",
      "batch 519, loss: 0.1843, label: 1, bag_size: 7981\n",
      "batch 539, loss: 0.0016, label: 0, bag_size: 11654\n",
      "batch 559, loss: 0.0113, label: 1, bag_size: 10028\n",
      "batch 579, loss: 0.0002, label: 1, bag_size: 3409\n",
      "batch 599, loss: 0.0058, label: 0, bag_size: 8812\n",
      "batch 619, loss: 0.2472, label: 0, bag_size: 12840\n",
      "batch 639, loss: 0.0025, label: 1, bag_size: 19606\n",
      "batch 659, loss: 0.0016, label: 1, bag_size: 14202\n",
      "batch 679, loss: 0.0015, label: 1, bag_size: 16267\n",
      "batch 699, loss: 0.0031, label: 1, bag_size: 14230\n",
      "batch 719, loss: 0.0038, label: 0, bag_size: 15077\n",
      "batch 739, loss: 0.0032, label: 0, bag_size: 16607\n",
      "batch 759, loss: 0.0001, label: 1, bag_size: 7381\n",
      "batch 779, loss: 0.0318, label: 0, bag_size: 7823\n",
      "batch 799, loss: 1.9754, label: 0, bag_size: 3468\n",
      "batch 819, loss: 0.0016, label: 0, bag_size: 3265\n",
      "Epoch: 47, train_loss: 0.1685, train_error: 0.0646\n",
      "class 0: acc 0.9359605911330049, correct 380/406\n",
      "class 1: acc 0.9347826086956522, correct 387/414\n",
      "\n",
      "Val Set, val_loss: 0.2861, val_error: 0.1273, auc: 0.9847\n",
      "class 0: acc 0.9807692307692307, correct 51/52\n",
      "class 1: acc 0.7758620689655172, correct 45/58\n",
      "EarlyStopping counter: 34 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1290, label: 1, bag_size: 10912\n",
      "batch 39, loss: 0.0008, label: 1, bag_size: 11642\n",
      "batch 59, loss: 0.0525, label: 1, bag_size: 8754\n",
      "batch 79, loss: 0.1046, label: 1, bag_size: 8216\n",
      "batch 99, loss: 0.0461, label: 0, bag_size: 4418\n",
      "batch 119, loss: 0.0003, label: 1, bag_size: 6966\n",
      "batch 139, loss: 0.0319, label: 0, bag_size: 21138\n",
      "batch 159, loss: 0.1345, label: 1, bag_size: 4786\n",
      "batch 179, loss: 0.0209, label: 1, bag_size: 7613\n",
      "batch 199, loss: 0.0005, label: 1, bag_size: 5340\n",
      "batch 219, loss: 0.0285, label: 0, bag_size: 3228\n",
      "batch 239, loss: 0.0539, label: 0, bag_size: 9471\n",
      "batch 259, loss: 0.0422, label: 0, bag_size: 11113\n",
      "batch 279, loss: 0.5020, label: 1, bag_size: 10912\n",
      "batch 299, loss: 0.0000, label: 1, bag_size: 1781\n",
      "batch 319, loss: 0.0257, label: 0, bag_size: 21093\n",
      "batch 339, loss: 0.1102, label: 1, bag_size: 13732\n",
      "batch 359, loss: 0.0489, label: 0, bag_size: 11151\n",
      "batch 379, loss: 0.0647, label: 0, bag_size: 11259\n",
      "batch 399, loss: 0.0031, label: 1, bag_size: 11220\n",
      "batch 419, loss: 2.0215, label: 1, bag_size: 2565\n",
      "batch 439, loss: 0.1763, label: 0, bag_size: 22498\n",
      "batch 459, loss: 0.0242, label: 0, bag_size: 7989\n",
      "batch 479, loss: 1.5290, label: 1, bag_size: 1533\n",
      "batch 499, loss: 0.0219, label: 1, bag_size: 4239\n",
      "batch 519, loss: 0.0002, label: 1, bag_size: 8003\n",
      "batch 539, loss: 0.0207, label: 0, bag_size: 14319\n",
      "batch 559, loss: 0.0120, label: 0, bag_size: 30751\n",
      "batch 579, loss: 0.0214, label: 0, bag_size: 1498\n",
      "batch 599, loss: 0.0011, label: 1, bag_size: 12460\n",
      "batch 619, loss: 0.0030, label: 0, bag_size: 9455\n",
      "batch 639, loss: 1.6954, label: 0, bag_size: 2959\n",
      "batch 659, loss: 0.0062, label: 0, bag_size: 12524\n",
      "batch 679, loss: 0.8880, label: 1, bag_size: 12714\n",
      "batch 699, loss: 0.0051, label: 0, bag_size: 12593\n",
      "batch 719, loss: 0.0044, label: 0, bag_size: 15967\n",
      "batch 739, loss: 0.0008, label: 1, bag_size: 1638\n",
      "batch 759, loss: 0.0002, label: 1, bag_size: 12611\n",
      "batch 779, loss: 0.0700, label: 0, bag_size: 13880\n",
      "batch 799, loss: 0.0474, label: 0, bag_size: 2814\n",
      "batch 819, loss: 0.0003, label: 1, bag_size: 4394\n",
      "Epoch: 48, train_loss: 0.1829, train_error: 0.0720\n",
      "class 0: acc 0.95, correct 399/420\n",
      "class 1: acc 0.905, correct 362/400\n",
      "\n",
      "Val Set, val_loss: 0.1886, val_error: 0.0727, auc: 0.9841\n",
      "class 0: acc 0.9807692307692307, correct 51/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "EarlyStopping counter: 35 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0261, label: 0, bag_size: 9415\n",
      "batch 39, loss: 0.6810, label: 0, bag_size: 2815\n",
      "batch 59, loss: 0.0020, label: 0, bag_size: 14333\n",
      "batch 79, loss: 0.0013, label: 0, bag_size: 17268\n",
      "batch 99, loss: 0.0539, label: 0, bag_size: 10068\n",
      "batch 119, loss: 0.0292, label: 0, bag_size: 11727\n",
      "batch 139, loss: 0.0881, label: 0, bag_size: 22498\n",
      "batch 159, loss: 0.0110, label: 0, bag_size: 10128\n",
      "batch 179, loss: 0.1549, label: 0, bag_size: 1684\n",
      "batch 199, loss: 0.0883, label: 0, bag_size: 1614\n",
      "batch 219, loss: 0.0143, label: 0, bag_size: 24911\n",
      "batch 239, loss: 0.0000, label: 1, bag_size: 9610\n",
      "batch 259, loss: 0.0221, label: 0, bag_size: 27158\n",
      "batch 279, loss: 0.0002, label: 1, bag_size: 8019\n",
      "batch 299, loss: 0.0854, label: 0, bag_size: 4523\n",
      "batch 319, loss: 0.0343, label: 1, bag_size: 3224\n",
      "batch 339, loss: 0.0038, label: 0, bag_size: 23037\n",
      "batch 359, loss: 0.0020, label: 0, bag_size: 13964\n",
      "batch 379, loss: 0.1072, label: 1, bag_size: 2480\n",
      "batch 399, loss: 0.0078, label: 0, bag_size: 19808\n",
      "batch 419, loss: 0.5235, label: 0, bag_size: 2242\n",
      "batch 439, loss: 0.0011, label: 0, bag_size: 5551\n",
      "batch 459, loss: 0.0036, label: 0, bag_size: 17268\n",
      "batch 479, loss: 0.0489, label: 1, bag_size: 6927\n",
      "batch 499, loss: 0.7120, label: 0, bag_size: 11306\n",
      "batch 519, loss: 0.0059, label: 1, bag_size: 13051\n",
      "batch 539, loss: 0.0188, label: 1, bag_size: 12603\n",
      "batch 559, loss: 0.0046, label: 1, bag_size: 9942\n",
      "batch 579, loss: 0.0083, label: 1, bag_size: 4250\n",
      "batch 599, loss: 0.0000, label: 1, bag_size: 5317\n",
      "batch 619, loss: 0.1031, label: 1, bag_size: 1242\n",
      "batch 639, loss: 0.0894, label: 1, bag_size: 11701\n",
      "batch 659, loss: 0.0127, label: 0, bag_size: 2063\n",
      "batch 679, loss: 0.1299, label: 1, bag_size: 11701\n",
      "batch 699, loss: 0.0062, label: 0, bag_size: 17630\n",
      "batch 719, loss: 0.0054, label: 0, bag_size: 11527\n",
      "batch 739, loss: 0.1714, label: 1, bag_size: 9561\n",
      "batch 759, loss: 0.0017, label: 0, bag_size: 9885\n",
      "batch 779, loss: 0.0062, label: 1, bag_size: 1888\n",
      "batch 799, loss: 0.0165, label: 0, bag_size: 10444\n",
      "batch 819, loss: 1.0814, label: 0, bag_size: 2290\n",
      "Epoch: 49, train_loss: 0.1565, train_error: 0.0561\n",
      "class 0: acc 0.9543147208121827, correct 376/394\n",
      "class 1: acc 0.9342723004694836, correct 398/426\n",
      "\n",
      "Val Set, val_loss: 0.1805, val_error: 0.0636, auc: 0.9804\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "EarlyStopping counter: 36 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0021, label: 0, bag_size: 3670\n",
      "batch 39, loss: 0.0717, label: 0, bag_size: 13339\n",
      "batch 59, loss: 1.0481, label: 0, bag_size: 14264\n",
      "batch 79, loss: 0.0016, label: 1, bag_size: 18095\n",
      "batch 99, loss: 0.2135, label: 0, bag_size: 9252\n",
      "batch 119, loss: 0.0889, label: 0, bag_size: 9597\n",
      "batch 139, loss: 0.0001, label: 1, bag_size: 7381\n",
      "batch 159, loss: 0.0043, label: 1, bag_size: 13174\n",
      "batch 179, loss: 0.0000, label: 1, bag_size: 15233\n",
      "batch 199, loss: 0.1814, label: 1, bag_size: 11701\n",
      "batch 219, loss: 0.0062, label: 0, bag_size: 3893\n",
      "batch 239, loss: 0.3485, label: 1, bag_size: 1437\n",
      "batch 259, loss: 0.4589, label: 1, bag_size: 21450\n",
      "batch 279, loss: 0.0256, label: 0, bag_size: 22426\n",
      "batch 299, loss: 0.0000, label: 1, bag_size: 9673\n",
      "batch 319, loss: 0.0000, label: 1, bag_size: 5221\n",
      "batch 339, loss: 0.0250, label: 1, bag_size: 5907\n",
      "batch 359, loss: 0.0007, label: 1, bag_size: 2904\n",
      "batch 379, loss: 0.1296, label: 1, bag_size: 3211\n",
      "batch 399, loss: 0.0333, label: 1, bag_size: 3224\n",
      "batch 419, loss: 0.0804, label: 0, bag_size: 3321\n",
      "batch 439, loss: 0.0078, label: 0, bag_size: 3670\n",
      "batch 459, loss: 0.0134, label: 0, bag_size: 8330\n",
      "batch 479, loss: 0.0033, label: 1, bag_size: 14887\n",
      "batch 499, loss: 0.0049, label: 0, bag_size: 10263\n",
      "batch 519, loss: 0.0001, label: 1, bag_size: 6453\n",
      "batch 539, loss: 0.0564, label: 0, bag_size: 11922\n",
      "batch 559, loss: 0.0108, label: 0, bag_size: 23398\n",
      "batch 579, loss: 0.1594, label: 1, bag_size: 8982\n",
      "batch 599, loss: 0.0000, label: 1, bag_size: 12611\n",
      "batch 619, loss: 0.0092, label: 0, bag_size: 1614\n",
      "batch 639, loss: 0.0011, label: 1, bag_size: 5494\n",
      "batch 659, loss: 0.0722, label: 1, bag_size: 13477\n",
      "batch 679, loss: 0.0622, label: 1, bag_size: 3652\n",
      "batch 699, loss: 0.0003, label: 1, bag_size: 4442\n",
      "batch 719, loss: 0.3158, label: 0, bag_size: 7239\n",
      "batch 739, loss: 0.0020, label: 0, bag_size: 14681\n",
      "batch 759, loss: 0.0023, label: 0, bag_size: 518\n",
      "batch 779, loss: 0.0003, label: 0, bag_size: 2382\n",
      "batch 799, loss: 0.0013, label: 1, bag_size: 2638\n",
      "batch 819, loss: 0.0000, label: 1, bag_size: 10920\n",
      "Epoch: 50, train_loss: 0.1263, train_error: 0.0573\n",
      "class 0: acc 0.9396984924623115, correct 374/398\n",
      "class 1: acc 0.9454976303317536, correct 399/422\n",
      "\n",
      "Val Set, val_loss: 0.3415, val_error: 0.1545, auc: 0.9788\n",
      "class 0: acc 0.6923076923076923, correct 36/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 37 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0041, label: 1, bag_size: 11394\n",
      "batch 39, loss: 0.0001, label: 0, bag_size: 10481\n",
      "batch 59, loss: 0.3334, label: 0, bag_size: 2098\n",
      "batch 79, loss: 0.0001, label: 1, bag_size: 16512\n",
      "batch 99, loss: 0.0000, label: 1, bag_size: 5221\n",
      "batch 119, loss: 0.0000, label: 1, bag_size: 13368\n",
      "batch 139, loss: 0.0001, label: 0, bag_size: 803\n",
      "batch 159, loss: 0.0655, label: 0, bag_size: 13880\n",
      "batch 179, loss: 0.4086, label: 0, bag_size: 26208\n",
      "batch 199, loss: 0.0889, label: 1, bag_size: 19972\n",
      "batch 219, loss: 0.2672, label: 0, bag_size: 2219\n",
      "batch 239, loss: 0.0007, label: 1, bag_size: 7873\n",
      "batch 259, loss: 0.0006, label: 1, bag_size: 6453\n",
      "batch 279, loss: 0.0270, label: 1, bag_size: 549\n",
      "batch 299, loss: 0.0004, label: 1, bag_size: 5629\n",
      "batch 319, loss: 0.0560, label: 0, bag_size: 6281\n",
      "batch 339, loss: 0.2309, label: 0, bag_size: 4418\n",
      "batch 359, loss: 0.3248, label: 0, bag_size: 14264\n",
      "batch 379, loss: 0.0103, label: 0, bag_size: 2044\n",
      "batch 399, loss: 0.0325, label: 1, bag_size: 12895\n",
      "batch 419, loss: 0.0125, label: 0, bag_size: 1824\n",
      "batch 439, loss: 0.0093, label: 0, bag_size: 21138\n",
      "batch 459, loss: 1.9730, label: 1, bag_size: 2937\n",
      "batch 479, loss: 0.0073, label: 0, bag_size: 9455\n",
      "batch 499, loss: 0.0350, label: 0, bag_size: 16690\n",
      "batch 519, loss: 0.0712, label: 1, bag_size: 5516\n",
      "batch 539, loss: 0.0052, label: 1, bag_size: 13089\n",
      "batch 559, loss: 0.8626, label: 0, bag_size: 11306\n",
      "batch 579, loss: 0.0839, label: 1, bag_size: 7989\n",
      "batch 599, loss: 0.0060, label: 0, bag_size: 6624\n",
      "batch 619, loss: 0.3209, label: 1, bag_size: 8216\n",
      "batch 639, loss: 0.0567, label: 1, bag_size: 8982\n",
      "batch 659, loss: 0.0456, label: 0, bag_size: 5639\n",
      "batch 679, loss: 0.0104, label: 0, bag_size: 8948\n",
      "batch 699, loss: 0.0358, label: 0, bag_size: 25814\n",
      "batch 719, loss: 0.0016, label: 1, bag_size: 16162\n",
      "batch 739, loss: 0.0604, label: 0, bag_size: 12083\n",
      "batch 759, loss: 0.4953, label: 0, bag_size: 17279\n",
      "batch 779, loss: 0.0014, label: 0, bag_size: 3970\n",
      "batch 799, loss: 0.0058, label: 1, bag_size: 1888\n",
      "batch 819, loss: 5.2001, label: 1, bag_size: 3121\n",
      "Epoch: 51, train_loss: 0.1544, train_error: 0.0561\n",
      "class 0: acc 0.9588100686498856, correct 419/437\n",
      "class 1: acc 0.9268929503916449, correct 355/383\n",
      "\n",
      "Val Set, val_loss: 0.1869, val_error: 0.0818, auc: 0.9824\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "EarlyStopping counter: 38 out of 20\n",
      "Early stopping\n",
      "Val error: 0.0636, ROC AUC: 0.9818\n",
      "Test error: 0.0722, ROC AUC: 0.9592\n",
      "class 0: acc 0.9795918367346939, correct 48/49\n",
      "class 1: acc 0.875, correct 42/48\n",
      "\n",
      "Training Fold 1!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 822 samples\n",
      "Validating on 109 samples\n",
      "Testing on 96 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Done!\n",
      "MIL_fc(\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 525826\n",
      "Total number of trainable parameters: 525826\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7287, label: 1, bag_size: 30675\n",
      "batch 39, loss: 0.6778, label: 0, bag_size: 2036\n",
      "batch 59, loss: 1.0573, label: 1, bag_size: 8522\n",
      "batch 79, loss: 0.5717, label: 0, bag_size: 18240\n",
      "batch 99, loss: 0.6767, label: 0, bag_size: 20666\n",
      "batch 119, loss: 0.7198, label: 1, bag_size: 12494\n",
      "batch 139, loss: 0.6025, label: 0, bag_size: 2624\n",
      "batch 159, loss: 0.6583, label: 1, bag_size: 9065\n",
      "batch 179, loss: 1.1534, label: 1, bag_size: 11386\n",
      "batch 199, loss: 0.9626, label: 1, bag_size: 10920\n",
      "batch 219, loss: 0.3633, label: 0, bag_size: 1052\n",
      "batch 239, loss: 0.4451, label: 0, bag_size: 23368\n",
      "batch 259, loss: 0.5708, label: 0, bag_size: 4418\n",
      "batch 279, loss: 0.9712, label: 0, bag_size: 1549\n",
      "batch 299, loss: 0.4637, label: 0, bag_size: 11477\n",
      "batch 319, loss: 0.3456, label: 1, bag_size: 10969\n",
      "batch 339, loss: 0.3489, label: 1, bag_size: 6745\n",
      "batch 359, loss: 0.9420, label: 0, bag_size: 5485\n",
      "batch 379, loss: 0.4830, label: 0, bag_size: 2266\n",
      "batch 399, loss: 0.8535, label: 1, bag_size: 2455\n",
      "batch 419, loss: 0.3914, label: 0, bag_size: 11146\n",
      "batch 439, loss: 0.4665, label: 0, bag_size: 10410\n",
      "batch 459, loss: 0.9502, label: 1, bag_size: 1437\n",
      "batch 479, loss: 0.5562, label: 0, bag_size: 12510\n",
      "batch 499, loss: 0.6589, label: 0, bag_size: 3198\n",
      "batch 519, loss: 0.4391, label: 0, bag_size: 23996\n",
      "batch 539, loss: 0.4595, label: 1, bag_size: 3224\n",
      "batch 559, loss: 0.4709, label: 1, bag_size: 5256\n",
      "batch 579, loss: 0.1929, label: 1, bag_size: 10112\n",
      "batch 599, loss: 0.8501, label: 1, bag_size: 1051\n",
      "batch 619, loss: 0.4460, label: 0, bag_size: 3708\n",
      "batch 639, loss: 0.4045, label: 0, bag_size: 9171\n",
      "batch 659, loss: 0.7308, label: 0, bag_size: 14266\n",
      "batch 679, loss: 0.7799, label: 0, bag_size: 1814\n",
      "batch 699, loss: 0.5711, label: 0, bag_size: 2098\n",
      "batch 719, loss: 0.4823, label: 1, bag_size: 2638\n",
      "batch 739, loss: 0.2901, label: 1, bag_size: 21009\n",
      "batch 759, loss: 0.5014, label: 1, bag_size: 15665\n",
      "batch 779, loss: 0.4072, label: 0, bag_size: 9387\n",
      "batch 799, loss: 0.7833, label: 0, bag_size: 8755\n",
      "batch 819, loss: 0.9041, label: 1, bag_size: 645\n",
      "Epoch: 0, train_loss: 0.5768, train_error: 0.2725\n",
      "class 0: acc 0.8284424379232506, correct 367/443\n",
      "class 1: acc 0.6094986807387863, correct 231/379\n",
      "\n",
      "Val Set, val_loss: 0.6550, val_error: 0.3945, auc: 0.8892\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.3333333333333333, correct 21/63\n",
      "Validation loss decreased (inf --> 0.654953).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6298, label: 1, bag_size: 16514\n",
      "batch 39, loss: 0.2953, label: 0, bag_size: 2091\n",
      "batch 59, loss: 0.8826, label: 1, bag_size: 2314\n",
      "batch 79, loss: 1.2875, label: 1, bag_size: 21252\n",
      "batch 99, loss: 0.6316, label: 0, bag_size: 9596\n",
      "batch 119, loss: 0.1761, label: 0, bag_size: 15736\n",
      "batch 139, loss: 1.0356, label: 1, bag_size: 12946\n",
      "batch 159, loss: 1.0842, label: 0, bag_size: 7923\n",
      "batch 179, loss: 0.8806, label: 0, bag_size: 2959\n",
      "batch 199, loss: 1.4324, label: 1, bag_size: 1794\n",
      "batch 219, loss: 0.1724, label: 1, bag_size: 5731\n",
      "batch 239, loss: 0.8153, label: 0, bag_size: 25814\n",
      "batch 259, loss: 0.2928, label: 1, bag_size: 14618\n",
      "batch 279, loss: 0.1589, label: 1, bag_size: 15665\n",
      "batch 299, loss: 0.1702, label: 0, bag_size: 11113\n",
      "batch 319, loss: 0.1042, label: 0, bag_size: 10481\n",
      "batch 339, loss: 0.3362, label: 0, bag_size: 1684\n",
      "batch 359, loss: 0.8807, label: 1, bag_size: 5629\n",
      "batch 379, loss: 0.3895, label: 0, bag_size: 1483\n",
      "batch 399, loss: 0.4018, label: 0, bag_size: 1549\n",
      "batch 419, loss: 0.2464, label: 1, bag_size: 1920\n",
      "batch 439, loss: 0.2378, label: 1, bag_size: 14779\n",
      "batch 459, loss: 0.5565, label: 0, bag_size: 2079\n",
      "batch 479, loss: 0.0683, label: 0, bag_size: 8372\n",
      "batch 499, loss: 0.2496, label: 0, bag_size: 12083\n",
      "batch 519, loss: 0.5074, label: 0, bag_size: 19808\n",
      "batch 539, loss: 1.6151, label: 1, bag_size: 1533\n",
      "batch 559, loss: 0.0932, label: 0, bag_size: 10535\n",
      "batch 579, loss: 1.7565, label: 1, bag_size: 2935\n",
      "batch 599, loss: 0.3824, label: 0, bag_size: 9596\n",
      "batch 619, loss: 0.2290, label: 0, bag_size: 7235\n",
      "batch 639, loss: 0.6791, label: 1, bag_size: 1339\n",
      "batch 659, loss: 0.2171, label: 0, bag_size: 3375\n",
      "batch 679, loss: 0.4784, label: 1, bag_size: 13015\n",
      "batch 699, loss: 0.1531, label: 1, bag_size: 8660\n",
      "batch 719, loss: 0.1148, label: 0, bag_size: 10942\n",
      "batch 739, loss: 0.3857, label: 1, bag_size: 5723\n",
      "batch 759, loss: 0.2763, label: 0, bag_size: 12561\n",
      "batch 779, loss: 1.3968, label: 1, bag_size: 2935\n",
      "batch 799, loss: 0.2006, label: 1, bag_size: 6745\n",
      "batch 819, loss: 0.1608, label: 1, bag_size: 5612\n",
      "Epoch: 1, train_loss: 0.4410, train_error: 0.1727\n",
      "class 0: acc 0.8769574944071589, correct 392/447\n",
      "class 1: acc 0.768, correct 288/375\n",
      "\n",
      "Val Set, val_loss: 0.3805, val_error: 0.1560, auc: 0.9355\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.7619047619047619, correct 48/63\n",
      "Validation loss decreased (0.654953 --> 0.380491).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1331, label: 1, bag_size: 12758\n",
      "batch 39, loss: 0.3070, label: 0, bag_size: 4465\n",
      "batch 59, loss: 0.4579, label: 0, bag_size: 16607\n",
      "batch 79, loss: 0.7161, label: 1, bag_size: 2092\n",
      "batch 99, loss: 0.1841, label: 0, bag_size: 3101\n",
      "batch 119, loss: 0.0579, label: 1, bag_size: 14618\n",
      "batch 139, loss: 0.2103, label: 0, bag_size: 11865\n",
      "batch 159, loss: 0.2188, label: 1, bag_size: 9062\n",
      "batch 179, loss: 0.0570, label: 1, bag_size: 4862\n",
      "batch 199, loss: 0.2034, label: 0, bag_size: 16211\n",
      "batch 219, loss: 1.1493, label: 1, bag_size: 2935\n",
      "batch 239, loss: 0.0675, label: 1, bag_size: 14604\n",
      "batch 259, loss: 0.3415, label: 1, bag_size: 9330\n",
      "batch 279, loss: 0.6609, label: 1, bag_size: 2678\n",
      "batch 299, loss: 0.6144, label: 1, bag_size: 16703\n",
      "batch 319, loss: 0.1854, label: 0, bag_size: 1202\n",
      "batch 339, loss: 0.1087, label: 1, bag_size: 20333\n",
      "batch 359, loss: 0.1042, label: 1, bag_size: 7935\n",
      "batch 379, loss: 0.3274, label: 1, bag_size: 1255\n",
      "batch 399, loss: 0.1413, label: 1, bag_size: 7935\n",
      "batch 419, loss: 0.1923, label: 0, bag_size: 1953\n",
      "batch 439, loss: 0.1180, label: 0, bag_size: 13964\n",
      "batch 459, loss: 0.3546, label: 1, bag_size: 8012\n",
      "batch 479, loss: 0.1423, label: 1, bag_size: 5441\n",
      "batch 499, loss: 0.1284, label: 1, bag_size: 6171\n",
      "batch 519, loss: 0.0389, label: 0, bag_size: 2760\n",
      "batch 539, loss: 0.0639, label: 0, bag_size: 2006\n",
      "batch 559, loss: 0.0805, label: 1, bag_size: 3003\n",
      "batch 579, loss: 0.2074, label: 0, bag_size: 9583\n",
      "batch 599, loss: 0.1035, label: 1, bag_size: 10969\n",
      "batch 619, loss: 0.3190, label: 1, bag_size: 1920\n",
      "batch 639, loss: 0.1408, label: 1, bag_size: 8395\n",
      "batch 659, loss: 0.6089, label: 0, bag_size: 13332\n",
      "batch 679, loss: 0.0495, label: 1, bag_size: 12758\n",
      "batch 699, loss: 0.0550, label: 1, bag_size: 20161\n",
      "batch 719, loss: 0.1609, label: 1, bag_size: 1572\n",
      "batch 739, loss: 0.7379, label: 1, bag_size: 4821\n",
      "batch 759, loss: 0.7917, label: 0, bag_size: 18215\n",
      "batch 779, loss: 0.4942, label: 1, bag_size: 1838\n",
      "batch 799, loss: 0.5379, label: 0, bag_size: 22800\n",
      "batch 819, loss: 0.2584, label: 1, bag_size: 1014\n",
      "Epoch: 2, train_loss: 0.3654, train_error: 0.1314\n",
      "class 0: acc 0.8910891089108911, correct 360/404\n",
      "class 1: acc 0.84688995215311, correct 354/418\n",
      "\n",
      "Val Set, val_loss: 0.3155, val_error: 0.1376, auc: 0.9455\n",
      "class 0: acc 0.9130434782608695, correct 42/46\n",
      "class 1: acc 0.8253968253968254, correct 52/63\n",
      "Validation loss decreased (0.380491 --> 0.315499).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3114, label: 1, bag_size: 34356\n",
      "batch 39, loss: 0.8281, label: 1, bag_size: 1819\n",
      "batch 59, loss: 0.1738, label: 1, bag_size: 4330\n",
      "batch 79, loss: 1.0772, label: 1, bag_size: 1051\n",
      "batch 99, loss: 0.0943, label: 1, bag_size: 16379\n",
      "batch 119, loss: 0.1385, label: 1, bag_size: 15125\n",
      "batch 139, loss: 0.0253, label: 1, bag_size: 8040\n",
      "batch 159, loss: 0.2544, label: 0, bag_size: 30828\n",
      "batch 179, loss: 0.3103, label: 0, bag_size: 16087\n",
      "batch 199, loss: 0.0450, label: 1, bag_size: 15213\n",
      "batch 219, loss: 0.3995, label: 0, bag_size: 18777\n",
      "batch 239, loss: 0.1708, label: 1, bag_size: 9689\n",
      "batch 259, loss: 0.5407, label: 0, bag_size: 8427\n",
      "batch 279, loss: 0.0880, label: 1, bag_size: 7148\n",
      "batch 299, loss: 0.2230, label: 0, bag_size: 2079\n",
      "batch 319, loss: 1.1110, label: 1, bag_size: 1095\n",
      "batch 339, loss: 0.2019, label: 0, bag_size: 2322\n",
      "batch 359, loss: 0.9832, label: 0, bag_size: 6281\n",
      "batch 379, loss: 0.0789, label: 1, bag_size: 4715\n",
      "batch 399, loss: 0.0598, label: 1, bag_size: 8040\n",
      "batch 419, loss: 1.3979, label: 1, bag_size: 7989\n",
      "batch 439, loss: 0.1021, label: 1, bag_size: 3674\n",
      "batch 459, loss: 0.0565, label: 1, bag_size: 6927\n",
      "batch 479, loss: 0.2466, label: 0, bag_size: 6850\n",
      "batch 499, loss: 0.0421, label: 0, bag_size: 17482\n",
      "batch 519, loss: 0.0489, label: 1, bag_size: 5894\n",
      "batch 539, loss: 0.1053, label: 0, bag_size: 29270\n",
      "batch 559, loss: 0.0349, label: 1, bag_size: 15213\n",
      "batch 579, loss: 0.2462, label: 1, bag_size: 4821\n",
      "batch 599, loss: 0.2562, label: 1, bag_size: 9649\n",
      "batch 619, loss: 0.1059, label: 0, bag_size: 6727\n",
      "batch 639, loss: 0.6847, label: 1, bag_size: 2682\n",
      "batch 659, loss: 0.1804, label: 0, bag_size: 29270\n",
      "batch 679, loss: 1.7560, label: 1, bag_size: 2937\n",
      "batch 699, loss: 0.3285, label: 0, bag_size: 4271\n",
      "batch 719, loss: 0.7884, label: 1, bag_size: 15563\n",
      "batch 739, loss: 0.0179, label: 1, bag_size: 2485\n",
      "batch 759, loss: 0.0101, label: 0, bag_size: 8372\n",
      "batch 779, loss: 0.0442, label: 0, bag_size: 25027\n",
      "batch 799, loss: 0.7672, label: 1, bag_size: 12714\n",
      "batch 819, loss: 3.8467, label: 0, bag_size: 3802\n",
      "Epoch: 3, train_loss: 0.2981, train_error: 0.1046\n",
      "class 0: acc 0.9111675126903553, correct 359/394\n",
      "class 1: acc 0.8808411214953271, correct 377/428\n",
      "\n",
      "Val Set, val_loss: 0.3026, val_error: 0.1284, auc: 0.9496\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.8095238095238095, correct 51/63\n",
      "Validation loss decreased (0.315499 --> 0.302573).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1103, label: 1, bag_size: 9230\n",
      "batch 39, loss: 0.1659, label: 1, bag_size: 9533\n",
      "batch 59, loss: 0.6879, label: 1, bag_size: 2146\n",
      "batch 79, loss: 0.1820, label: 0, bag_size: 25558\n",
      "batch 99, loss: 0.3208, label: 1, bag_size: 3856\n",
      "batch 119, loss: 0.1976, label: 1, bag_size: 8012\n",
      "batch 139, loss: 0.0109, label: 1, bag_size: 3453\n",
      "batch 159, loss: 0.0433, label: 0, bag_size: 3232\n",
      "batch 179, loss: 0.2287, label: 1, bag_size: 6752\n",
      "batch 199, loss: 0.0119, label: 0, bag_size: 13964\n",
      "batch 219, loss: 0.0532, label: 1, bag_size: 19932\n",
      "batch 239, loss: 0.0386, label: 1, bag_size: 4877\n",
      "batch 259, loss: 0.2466, label: 0, bag_size: 8420\n",
      "batch 279, loss: 2.9789, label: 0, bag_size: 3468\n",
      "batch 299, loss: 1.9816, label: 1, bag_size: 1284\n",
      "batch 319, loss: 1.9991, label: 0, bag_size: 47866\n",
      "batch 339, loss: 0.6042, label: 0, bag_size: 11922\n",
      "batch 359, loss: 0.0232, label: 0, bag_size: 3459\n",
      "batch 379, loss: 0.0473, label: 0, bag_size: 1881\n",
      "batch 399, loss: 0.1473, label: 1, bag_size: 7798\n",
      "batch 419, loss: 0.0569, label: 0, bag_size: 19435\n",
      "batch 439, loss: 0.4596, label: 1, bag_size: 16514\n",
      "batch 459, loss: 0.0205, label: 1, bag_size: 4367\n",
      "batch 479, loss: 0.1531, label: 1, bag_size: 2356\n",
      "batch 499, loss: 0.0239, label: 1, bag_size: 8040\n",
      "batch 519, loss: 0.0543, label: 0, bag_size: 4497\n",
      "batch 539, loss: 0.0217, label: 1, bag_size: 9971\n",
      "batch 559, loss: 0.0042, label: 0, bag_size: 1052\n",
      "batch 579, loss: 0.0224, label: 1, bag_size: 12095\n",
      "batch 599, loss: 0.0448, label: 0, bag_size: 11146\n",
      "batch 619, loss: 0.0200, label: 1, bag_size: 699\n",
      "batch 639, loss: 0.1530, label: 0, bag_size: 1142\n",
      "batch 659, loss: 0.4856, label: 1, bag_size: 7515\n",
      "batch 679, loss: 0.9925, label: 1, bag_size: 21252\n",
      "batch 699, loss: 0.1596, label: 0, bag_size: 9060\n",
      "batch 719, loss: 0.0846, label: 0, bag_size: 7235\n",
      "batch 739, loss: 1.3993, label: 1, bag_size: 1191\n",
      "batch 759, loss: 0.1168, label: 1, bag_size: 8754\n",
      "batch 779, loss: 0.1684, label: 1, bag_size: 8680\n",
      "batch 799, loss: 0.0332, label: 0, bag_size: 1826\n",
      "batch 819, loss: 0.6429, label: 0, bag_size: 4997\n",
      "Epoch: 4, train_loss: 0.3053, train_error: 0.1034\n",
      "class 0: acc 0.9016786570743405, correct 376/417\n",
      "class 1: acc 0.891358024691358, correct 361/405\n",
      "\n",
      "Val Set, val_loss: 0.3449, val_error: 0.1376, auc: 0.9538\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.7777777777777778, correct 49/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0065, label: 0, bag_size: 8372\n",
      "batch 39, loss: 0.1580, label: 0, bag_size: 7989\n",
      "batch 59, loss: 0.5098, label: 0, bag_size: 2270\n",
      "batch 79, loss: 0.0123, label: 0, bag_size: 1438\n",
      "batch 99, loss: 0.0170, label: 1, bag_size: 13786\n",
      "batch 119, loss: 0.0585, label: 1, bag_size: 10072\n",
      "batch 139, loss: 0.0389, label: 0, bag_size: 2063\n",
      "batch 159, loss: 0.0186, label: 0, bag_size: 16782\n",
      "batch 179, loss: 0.0983, label: 1, bag_size: 4423\n",
      "batch 199, loss: 0.1781, label: 1, bag_size: 7583\n",
      "batch 219, loss: 0.0723, label: 1, bag_size: 4054\n",
      "batch 239, loss: 0.0470, label: 1, bag_size: 9446\n",
      "batch 259, loss: 1.0611, label: 0, bag_size: 20555\n",
      "batch 279, loss: 0.0304, label: 1, bag_size: 12349\n",
      "batch 299, loss: 0.0091, label: 1, bag_size: 1412\n",
      "batch 319, loss: 0.0285, label: 1, bag_size: 9971\n",
      "batch 339, loss: 0.0249, label: 1, bag_size: 10392\n",
      "batch 359, loss: 0.0163, label: 1, bag_size: 2412\n",
      "batch 379, loss: 0.0773, label: 1, bag_size: 21701\n",
      "batch 399, loss: 0.1045, label: 0, bag_size: 3670\n",
      "batch 419, loss: 0.5425, label: 0, bag_size: 1732\n",
      "batch 439, loss: 0.0492, label: 0, bag_size: 18215\n",
      "batch 459, loss: 0.0125, label: 1, bag_size: 4715\n",
      "batch 479, loss: 0.0024, label: 0, bag_size: 9433\n",
      "batch 499, loss: 0.7942, label: 0, bag_size: 20230\n",
      "batch 519, loss: 0.2587, label: 0, bag_size: 2043\n",
      "batch 539, loss: 0.1966, label: 0, bag_size: 2624\n",
      "batch 559, loss: 0.0057, label: 1, bag_size: 2904\n",
      "batch 579, loss: 0.0205, label: 0, bag_size: 18954\n",
      "batch 599, loss: 0.2303, label: 1, bag_size: 15192\n",
      "batch 619, loss: 0.2067, label: 0, bag_size: 6850\n",
      "batch 639, loss: 0.0143, label: 0, bag_size: 31106\n",
      "batch 659, loss: 0.1345, label: 1, bag_size: 8438\n",
      "batch 679, loss: 0.2137, label: 0, bag_size: 31085\n",
      "batch 699, loss: 0.0141, label: 1, bag_size: 12349\n",
      "batch 719, loss: 0.1186, label: 0, bag_size: 15898\n",
      "batch 739, loss: 0.0103, label: 1, bag_size: 7873\n",
      "batch 759, loss: 0.0216, label: 0, bag_size: 14956\n",
      "batch 779, loss: 0.2843, label: 0, bag_size: 11390\n",
      "batch 799, loss: 0.2470, label: 0, bag_size: 6281\n",
      "batch 819, loss: 0.2449, label: 1, bag_size: 9548\n",
      "Epoch: 5, train_loss: 0.2475, train_error: 0.0803\n",
      "class 0: acc 0.9333333333333333, correct 392/420\n",
      "class 1: acc 0.9054726368159204, correct 364/402\n",
      "\n",
      "Val Set, val_loss: 0.6757, val_error: 0.2477, auc: 0.9465\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.5873015873015873, correct 37/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0949, label: 1, bag_size: 6736\n",
      "batch 39, loss: 0.0019, label: 1, bag_size: 5731\n",
      "batch 59, loss: 0.9960, label: 1, bag_size: 2937\n",
      "batch 79, loss: 0.0033, label: 1, bag_size: 11122\n",
      "batch 99, loss: 0.1228, label: 0, bag_size: 2760\n",
      "batch 119, loss: 0.1077, label: 0, bag_size: 25420\n",
      "batch 139, loss: 0.0142, label: 1, bag_size: 3453\n",
      "batch 159, loss: 0.0023, label: 1, bag_size: 13255\n",
      "batch 179, loss: 0.1667, label: 1, bag_size: 1867\n",
      "batch 199, loss: 0.0343, label: 0, bag_size: 9583\n",
      "batch 219, loss: 0.1487, label: 1, bag_size: 1123\n",
      "batch 239, loss: 0.1304, label: 0, bag_size: 18516\n",
      "batch 259, loss: 0.5393, label: 1, bag_size: 13089\n",
      "batch 279, loss: 0.0143, label: 1, bag_size: 4039\n",
      "batch 299, loss: 0.0169, label: 0, bag_size: 22828\n",
      "batch 319, loss: 0.2320, label: 1, bag_size: 15192\n",
      "batch 339, loss: 0.0206, label: 0, bag_size: 4959\n",
      "batch 359, loss: 0.0102, label: 0, bag_size: 2063\n",
      "batch 379, loss: 0.0580, label: 0, bag_size: 1920\n",
      "batch 399, loss: 0.0945, label: 0, bag_size: 15464\n",
      "batch 419, loss: 0.0045, label: 1, bag_size: 5991\n",
      "batch 439, loss: 0.0679, label: 0, bag_size: 10721\n",
      "batch 459, loss: 0.0121, label: 1, bag_size: 3450\n",
      "batch 479, loss: 1.0457, label: 0, bag_size: 15003\n",
      "batch 499, loss: 0.0134, label: 1, bag_size: 21009\n",
      "batch 519, loss: 0.0323, label: 0, bag_size: 19472\n",
      "batch 539, loss: 0.1210, label: 0, bag_size: 1592\n",
      "batch 559, loss: 0.0163, label: 1, bag_size: 14887\n",
      "batch 579, loss: 0.0926, label: 1, bag_size: 13015\n",
      "batch 599, loss: 0.0362, label: 1, bag_size: 10033\n",
      "batch 619, loss: 0.0129, label: 0, bag_size: 18225\n",
      "batch 639, loss: 0.1747, label: 0, bag_size: 2004\n",
      "batch 659, loss: 0.0035, label: 1, bag_size: 1412\n",
      "batch 679, loss: 0.0020, label: 1, bag_size: 8019\n",
      "batch 699, loss: 0.1660, label: 0, bag_size: 3474\n",
      "batch 719, loss: 0.0036, label: 1, bag_size: 13368\n",
      "batch 739, loss: 0.0060, label: 1, bag_size: 9971\n",
      "batch 759, loss: 0.0903, label: 1, bag_size: 1014\n",
      "batch 779, loss: 0.0999, label: 1, bag_size: 11266\n",
      "batch 799, loss: 0.0326, label: 0, bag_size: 13225\n",
      "batch 819, loss: 2.0689, label: 1, bag_size: 1038\n",
      "Epoch: 6, train_loss: 0.2923, train_error: 0.1168\n",
      "class 0: acc 0.8891752577319587, correct 345/388\n",
      "class 1: acc 0.8778801843317973, correct 381/434\n",
      "\n",
      "Val Set, val_loss: 0.3661, val_error: 0.1376, auc: 0.9472\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.7777777777777778, correct 49/63\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1467, label: 0, bag_size: 24911\n",
      "batch 39, loss: 0.2302, label: 1, bag_size: 4308\n",
      "batch 59, loss: 0.0655, label: 1, bag_size: 7613\n",
      "batch 79, loss: 0.2763, label: 0, bag_size: 15898\n",
      "batch 99, loss: 0.7598, label: 1, bag_size: 7389\n",
      "batch 119, loss: 0.0515, label: 0, bag_size: 10898\n",
      "batch 139, loss: 0.0050, label: 1, bag_size: 14618\n",
      "batch 159, loss: 0.1031, label: 1, bag_size: 2848\n",
      "batch 179, loss: 0.0033, label: 1, bag_size: 2904\n",
      "batch 199, loss: 0.0136, label: 0, bag_size: 23037\n",
      "batch 219, loss: 0.2494, label: 0, bag_size: 10721\n",
      "batch 239, loss: 0.4036, label: 0, bag_size: 4523\n",
      "batch 259, loss: 0.0034, label: 1, bag_size: 629\n",
      "batch 279, loss: 1.5544, label: 1, bag_size: 1038\n",
      "batch 299, loss: 0.0922, label: 0, bag_size: 19043\n",
      "batch 319, loss: 0.0571, label: 0, bag_size: 15057\n",
      "batch 339, loss: 0.1361, label: 1, bag_size: 13732\n",
      "batch 359, loss: 0.0259, label: 1, bag_size: 7381\n",
      "batch 379, loss: 0.0151, label: 1, bag_size: 8935\n",
      "batch 399, loss: 0.5126, label: 1, bag_size: 21252\n",
      "batch 419, loss: 0.1128, label: 1, bag_size: 3980\n",
      "batch 439, loss: 0.1075, label: 0, bag_size: 7011\n",
      "batch 459, loss: 0.0152, label: 0, bag_size: 18954\n",
      "batch 479, loss: 0.1450, label: 0, bag_size: 11390\n",
      "batch 499, loss: 0.6229, label: 1, bag_size: 11256\n",
      "batch 519, loss: 0.0338, label: 1, bag_size: 7371\n",
      "batch 539, loss: 0.0188, label: 1, bag_size: 8592\n",
      "batch 559, loss: 0.8632, label: 0, bag_size: 1701\n",
      "batch 579, loss: 1.8374, label: 1, bag_size: 11386\n",
      "batch 599, loss: 0.0268, label: 1, bag_size: 3656\n",
      "batch 619, loss: 0.0232, label: 1, bag_size: 5763\n",
      "batch 639, loss: 0.0870, label: 1, bag_size: 13732\n",
      "batch 659, loss: 0.0025, label: 1, bag_size: 3003\n",
      "batch 679, loss: 0.0232, label: 1, bag_size: 10033\n",
      "batch 699, loss: 0.0401, label: 0, bag_size: 15077\n",
      "batch 719, loss: 0.0009, label: 1, bag_size: 5317\n",
      "batch 739, loss: 0.2767, label: 0, bag_size: 3654\n",
      "batch 759, loss: 0.0402, label: 1, bag_size: 8660\n",
      "batch 779, loss: 0.0416, label: 1, bag_size: 13015\n",
      "batch 799, loss: 0.0037, label: 1, bag_size: 9955\n",
      "batch 819, loss: 0.1173, label: 0, bag_size: 12561\n",
      "Epoch: 7, train_loss: 0.2438, train_error: 0.0803\n",
      "class 0: acc 0.9314720812182741, correct 367/394\n",
      "class 1: acc 0.9088785046728972, correct 389/428\n",
      "\n",
      "Val Set, val_loss: 0.2812, val_error: 0.1101, auc: 0.9503\n",
      "class 0: acc 0.8913043478260869, correct 41/46\n",
      "class 1: acc 0.8888888888888888, correct 56/63\n",
      "Validation loss decreased (0.302573 --> 0.281245).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0305, label: 0, bag_size: 4959\n",
      "batch 39, loss: 0.0939, label: 0, bag_size: 6093\n",
      "batch 59, loss: 0.6462, label: 0, bag_size: 15003\n",
      "batch 79, loss: 2.5271, label: 1, bag_size: 2937\n",
      "batch 99, loss: 0.1012, label: 0, bag_size: 2918\n",
      "batch 119, loss: 0.2185, label: 1, bag_size: 9649\n",
      "batch 139, loss: 0.1195, label: 0, bag_size: 17268\n",
      "batch 159, loss: 0.0006, label: 1, bag_size: 7078\n",
      "batch 179, loss: 0.0373, label: 1, bag_size: 9747\n",
      "batch 199, loss: 0.1139, label: 1, bag_size: 9446\n",
      "batch 219, loss: 0.0026, label: 1, bag_size: 15665\n",
      "batch 239, loss: 0.0057, label: 0, bag_size: 10535\n",
      "batch 259, loss: 0.2367, label: 1, bag_size: 5110\n",
      "batch 279, loss: 0.0035, label: 0, bag_size: 1052\n",
      "batch 299, loss: 0.4432, label: 0, bag_size: 2458\n",
      "batch 319, loss: 0.0490, label: 0, bag_size: 3474\n",
      "batch 339, loss: 0.1748, label: 0, bag_size: 29270\n",
      "batch 359, loss: 0.0253, label: 1, bag_size: 5155\n",
      "batch 379, loss: 0.2803, label: 1, bag_size: 4929\n",
      "batch 399, loss: 0.5966, label: 1, bag_size: 1497\n",
      "batch 419, loss: 1.1912, label: 1, bag_size: 2814\n",
      "batch 439, loss: 0.0034, label: 0, bag_size: 9885\n",
      "batch 459, loss: 0.0062, label: 0, bag_size: 9786\n",
      "batch 479, loss: 0.0494, label: 0, bag_size: 931\n",
      "batch 499, loss: 0.0771, label: 1, bag_size: 7445\n",
      "batch 519, loss: 0.0077, label: 1, bag_size: 19500\n",
      "batch 539, loss: 0.0071, label: 1, bag_size: 2638\n",
      "batch 559, loss: 0.0476, label: 1, bag_size: 5690\n",
      "batch 579, loss: 0.2535, label: 1, bag_size: 10501\n",
      "batch 599, loss: 0.1632, label: 1, bag_size: 7613\n",
      "batch 619, loss: 0.0813, label: 0, bag_size: 16087\n",
      "batch 639, loss: 0.4197, label: 0, bag_size: 30828\n",
      "batch 659, loss: 0.1542, label: 0, bag_size: 4523\n",
      "batch 679, loss: 0.0283, label: 1, bag_size: 5441\n",
      "batch 699, loss: 0.0582, label: 0, bag_size: 1415\n",
      "batch 719, loss: 0.0059, label: 1, bag_size: 13786\n",
      "batch 739, loss: 0.0034, label: 0, bag_size: 21218\n",
      "batch 759, loss: 0.0608, label: 0, bag_size: 5161\n",
      "batch 779, loss: 0.0207, label: 0, bag_size: 17368\n",
      "batch 799, loss: 0.1063, label: 1, bag_size: 34356\n",
      "batch 819, loss: 1.2328, label: 1, bag_size: 1230\n",
      "Epoch: 8, train_loss: 0.2651, train_error: 0.0973\n",
      "class 0: acc 0.9212410501193318, correct 386/419\n",
      "class 1: acc 0.8833746898263027, correct 356/403\n",
      "\n",
      "Val Set, val_loss: 0.2949, val_error: 0.1193, auc: 0.9545\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.8095238095238095, correct 51/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0279, label: 1, bag_size: 5991\n",
      "batch 39, loss: 0.0021, label: 1, bag_size: 7110\n",
      "batch 59, loss: 1.4099, label: 0, bag_size: 7428\n",
      "batch 79, loss: 0.0060, label: 0, bag_size: 17155\n",
      "batch 99, loss: 0.0130, label: 1, bag_size: 699\n",
      "batch 119, loss: 0.0051, label: 1, bag_size: 21009\n",
      "batch 139, loss: 0.2501, label: 1, bag_size: 7989\n",
      "batch 159, loss: 0.0069, label: 1, bag_size: 14223\n",
      "batch 179, loss: 0.0119, label: 0, bag_size: 1483\n",
      "batch 199, loss: 0.0256, label: 0, bag_size: 2760\n",
      "batch 219, loss: 0.2887, label: 1, bag_size: 11701\n",
      "batch 239, loss: 0.0453, label: 1, bag_size: 7515\n",
      "batch 259, loss: 0.0087, label: 1, bag_size: 7371\n",
      "batch 279, loss: 0.0667, label: 1, bag_size: 6205\n",
      "batch 299, loss: 0.1252, label: 1, bag_size: 13015\n",
      "batch 319, loss: 0.2426, label: 0, bag_size: 15003\n",
      "batch 339, loss: 0.0057, label: 1, bag_size: 1022\n",
      "batch 359, loss: 0.0491, label: 0, bag_size: 8549\n",
      "batch 379, loss: 0.0016, label: 1, bag_size: 2485\n",
      "batch 399, loss: 0.1728, label: 0, bag_size: 23714\n",
      "batch 419, loss: 0.0641, label: 0, bag_size: 21138\n",
      "batch 439, loss: 0.1648, label: 0, bag_size: 14739\n",
      "batch 459, loss: 0.0060, label: 1, bag_size: 7110\n",
      "batch 479, loss: 0.0901, label: 1, bag_size: 15125\n",
      "batch 499, loss: 0.0869, label: 0, bag_size: 1772\n",
      "batch 519, loss: 0.0293, label: 0, bag_size: 3541\n",
      "batch 539, loss: 0.7785, label: 1, bag_size: 1242\n",
      "batch 559, loss: 0.1522, label: 0, bag_size: 1202\n",
      "batch 579, loss: 0.1563, label: 1, bag_size: 4956\n",
      "batch 599, loss: 1.2418, label: 1, bag_size: 21450\n",
      "batch 619, loss: 0.0080, label: 0, bag_size: 14206\n",
      "batch 639, loss: 0.0482, label: 1, bag_size: 5612\n",
      "batch 659, loss: 0.2836, label: 0, bag_size: 18738\n",
      "batch 679, loss: 0.1545, label: 0, bag_size: 3876\n",
      "batch 699, loss: 0.0041, label: 1, bag_size: 4480\n",
      "batch 719, loss: 0.0390, label: 0, bag_size: 2732\n",
      "batch 739, loss: 0.0025, label: 0, bag_size: 11122\n",
      "batch 759, loss: 0.0373, label: 1, bag_size: 3004\n",
      "batch 779, loss: 0.2643, label: 1, bag_size: 6928\n",
      "batch 799, loss: 0.0068, label: 1, bag_size: 2904\n",
      "batch 819, loss: 0.0577, label: 0, bag_size: 6367\n",
      "Epoch: 9, train_loss: 0.2340, train_error: 0.0876\n",
      "class 0: acc 0.9045226130653267, correct 360/398\n",
      "class 1: acc 0.9198113207547169, correct 390/424\n",
      "\n",
      "Val Set, val_loss: 0.3954, val_error: 0.1560, auc: 0.9562\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.746031746031746, correct 47/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0386, label: 1, bag_size: 20333\n",
      "batch 39, loss: 0.8511, label: 1, bag_size: 2314\n",
      "batch 59, loss: 0.0582, label: 0, bag_size: 8755\n",
      "batch 79, loss: 0.1004, label: 0, bag_size: 3557\n",
      "batch 99, loss: 0.0115, label: 0, bag_size: 10898\n",
      "batch 119, loss: 0.0050, label: 1, bag_size: 2405\n",
      "batch 139, loss: 0.3251, label: 0, bag_size: 15071\n",
      "batch 159, loss: 0.7397, label: 0, bag_size: 3710\n",
      "batch 179, loss: 0.1685, label: 0, bag_size: 6093\n",
      "batch 199, loss: 0.2955, label: 0, bag_size: 15003\n",
      "batch 219, loss: 1.1000, label: 0, bag_size: 7835\n",
      "batch 239, loss: 0.0045, label: 0, bag_size: 3459\n",
      "batch 259, loss: 0.0044, label: 0, bag_size: 12217\n",
      "batch 279, loss: 0.0124, label: 0, bag_size: 15967\n",
      "batch 299, loss: 0.0420, label: 1, bag_size: 11684\n",
      "batch 319, loss: 0.2154, label: 0, bag_size: 12083\n",
      "batch 339, loss: 0.5347, label: 0, bag_size: 3783\n",
      "batch 359, loss: 0.0870, label: 1, bag_size: 5921\n",
      "batch 379, loss: 0.0015, label: 1, bag_size: 5340\n",
      "batch 399, loss: 0.1170, label: 0, bag_size: 2458\n",
      "batch 419, loss: 0.0032, label: 1, bag_size: 15665\n",
      "batch 439, loss: 1.2501, label: 0, bag_size: 4692\n",
      "batch 459, loss: 0.1505, label: 1, bag_size: 9561\n",
      "batch 479, loss: 0.2074, label: 0, bag_size: 23618\n",
      "batch 499, loss: 0.0014, label: 0, bag_size: 1483\n",
      "batch 519, loss: 0.0929, label: 0, bag_size: 18516\n",
      "batch 539, loss: 0.0355, label: 1, bag_size: 10498\n",
      "batch 559, loss: 0.3841, label: 1, bag_size: 11684\n",
      "batch 579, loss: 0.0131, label: 1, bag_size: 16034\n",
      "batch 599, loss: 0.4896, label: 0, bag_size: 3228\n",
      "batch 619, loss: 0.0043, label: 1, bag_size: 14223\n",
      "batch 639, loss: 0.0558, label: 1, bag_size: 8680\n",
      "batch 659, loss: 0.0279, label: 1, bag_size: 8685\n",
      "batch 679, loss: 0.0044, label: 1, bag_size: 15716\n",
      "batch 699, loss: 0.0207, label: 0, bag_size: 30751\n",
      "batch 719, loss: 0.0257, label: 1, bag_size: 13026\n",
      "batch 739, loss: 0.0145, label: 1, bag_size: 5690\n",
      "batch 759, loss: 0.0000, label: 1, bag_size: 11195\n",
      "batch 779, loss: 1.6601, label: 0, bag_size: 4692\n",
      "batch 799, loss: 0.3402, label: 1, bag_size: 10622\n",
      "batch 819, loss: 0.2590, label: 0, bag_size: 3198\n",
      "Epoch: 10, train_loss: 0.2427, train_error: 0.0998\n",
      "class 0: acc 0.910941475826972, correct 358/393\n",
      "class 1: acc 0.8904428904428905, correct 382/429\n",
      "\n",
      "Val Set, val_loss: 0.2741, val_error: 0.1193, auc: 0.9531\n",
      "class 0: acc 0.8478260869565217, correct 39/46\n",
      "class 1: acc 0.9047619047619048, correct 57/63\n",
      "Validation loss decreased (0.281245 --> 0.274097).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0370, label: 0, bag_size: 11187\n",
      "batch 39, loss: 0.1837, label: 1, bag_size: 3651\n",
      "batch 59, loss: 0.0321, label: 1, bag_size: 2193\n",
      "batch 79, loss: 0.0223, label: 0, bag_size: 14681\n",
      "batch 99, loss: 0.2345, label: 0, bag_size: 11922\n",
      "batch 119, loss: 0.0944, label: 0, bag_size: 4241\n",
      "batch 139, loss: 0.6672, label: 1, bag_size: 5310\n",
      "batch 159, loss: 0.0322, label: 0, bag_size: 14681\n",
      "batch 179, loss: 0.0822, label: 0, bag_size: 1920\n",
      "batch 199, loss: 0.0038, label: 0, bag_size: 20150\n",
      "batch 219, loss: 0.0724, label: 0, bag_size: 4465\n",
      "batch 239, loss: 0.0350, label: 0, bag_size: 11527\n",
      "batch 259, loss: 0.1384, label: 0, bag_size: 14885\n",
      "batch 279, loss: 0.0021, label: 0, bag_size: 16341\n",
      "batch 299, loss: 0.1412, label: 1, bag_size: 2455\n",
      "batch 319, loss: 0.0571, label: 1, bag_size: 1746\n",
      "batch 339, loss: 0.2876, label: 0, bag_size: 9132\n",
      "batch 359, loss: 0.0044, label: 0, bag_size: 3190\n",
      "batch 379, loss: 0.0103, label: 0, bag_size: 14266\n",
      "batch 399, loss: 0.2906, label: 1, bag_size: 3224\n",
      "batch 419, loss: 0.0038, label: 1, bag_size: 9571\n",
      "batch 439, loss: 0.0228, label: 1, bag_size: 10848\n",
      "batch 459, loss: 0.0075, label: 1, bag_size: 2405\n",
      "batch 479, loss: 0.0483, label: 0, bag_size: 13218\n",
      "batch 499, loss: 0.0028, label: 1, bag_size: 3437\n",
      "batch 519, loss: 0.0459, label: 0, bag_size: 19880\n",
      "batch 539, loss: 0.1865, label: 1, bag_size: 2681\n",
      "batch 559, loss: 0.0050, label: 1, bag_size: 13194\n",
      "batch 579, loss: 0.0132, label: 1, bag_size: 1746\n",
      "batch 599, loss: 0.1886, label: 0, bag_size: 18738\n",
      "batch 619, loss: 0.1708, label: 1, bag_size: 7148\n",
      "batch 639, loss: 0.0253, label: 0, bag_size: 21138\n",
      "batch 659, loss: 0.0007, label: 1, bag_size: 10482\n",
      "batch 679, loss: 0.1625, label: 1, bag_size: 1437\n",
      "batch 699, loss: 0.0027, label: 1, bag_size: 8410\n",
      "batch 719, loss: 0.0005, label: 0, bag_size: 8372\n",
      "batch 739, loss: 0.0203, label: 1, bag_size: 2695\n",
      "batch 759, loss: 0.4060, label: 0, bag_size: 3444\n",
      "batch 779, loss: 0.0414, label: 0, bag_size: 2004\n",
      "batch 799, loss: 0.0143, label: 1, bag_size: 9913\n",
      "batch 819, loss: 0.0245, label: 0, bag_size: 11735\n",
      "Epoch: 11, train_loss: 0.2222, train_error: 0.0888\n",
      "class 0: acc 0.9227053140096618, correct 382/414\n",
      "class 1: acc 0.8995098039215687, correct 367/408\n",
      "\n",
      "Val Set, val_loss: 0.2748, val_error: 0.1193, auc: 0.9538\n",
      "class 0: acc 0.8478260869565217, correct 39/46\n",
      "class 1: acc 0.9047619047619048, correct 57/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0535, label: 0, bag_size: 7923\n",
      "batch 39, loss: 0.0378, label: 0, bag_size: 8981\n",
      "batch 59, loss: 0.0330, label: 1, bag_size: 16703\n",
      "batch 79, loss: 0.2094, label: 1, bag_size: 7583\n",
      "batch 99, loss: 0.0018, label: 0, bag_size: 2748\n",
      "batch 119, loss: 0.0203, label: 0, bag_size: 19390\n",
      "batch 139, loss: 1.1914, label: 1, bag_size: 1051\n",
      "batch 159, loss: 0.0020, label: 1, bag_size: 2936\n",
      "batch 179, loss: 0.0229, label: 1, bag_size: 21701\n",
      "batch 199, loss: 0.3754, label: 0, bag_size: 2043\n",
      "batch 219, loss: 0.1471, label: 0, bag_size: 23714\n",
      "batch 239, loss: 0.2354, label: 0, bag_size: 9616\n",
      "batch 259, loss: 0.0003, label: 1, bag_size: 5221\n",
      "batch 279, loss: 0.0058, label: 0, bag_size: 803\n",
      "batch 299, loss: 0.0521, label: 0, bag_size: 2160\n",
      "batch 319, loss: 0.1776, label: 0, bag_size: 1800\n",
      "batch 339, loss: 0.0065, label: 0, bag_size: 17368\n",
      "batch 359, loss: 0.0338, label: 0, bag_size: 11727\n",
      "batch 379, loss: 0.1036, label: 1, bag_size: 15093\n",
      "batch 399, loss: 0.0077, label: 0, bag_size: 31780\n",
      "batch 419, loss: 0.0075, label: 0, bag_size: 11690\n",
      "batch 439, loss: 0.0012, label: 1, bag_size: 19606\n",
      "batch 459, loss: 0.1913, label: 1, bag_size: 29832\n",
      "batch 479, loss: 0.1031, label: 1, bag_size: 16703\n",
      "batch 499, loss: 0.0031, label: 1, bag_size: 13194\n",
      "batch 519, loss: 0.0024, label: 1, bag_size: 11642\n",
      "batch 539, loss: 0.0170, label: 0, bag_size: 19067\n",
      "batch 559, loss: 0.0890, label: 0, bag_size: 8959\n",
      "batch 579, loss: 0.0994, label: 0, bag_size: 4465\n",
      "batch 599, loss: 0.0022, label: 1, bag_size: 5049\n",
      "batch 619, loss: 0.0132, label: 0, bag_size: 10721\n",
      "batch 639, loss: 0.0217, label: 0, bag_size: 11512\n",
      "batch 659, loss: 0.0188, label: 1, bag_size: 10033\n",
      "batch 679, loss: 0.0322, label: 1, bag_size: 22264\n",
      "batch 699, loss: 0.2710, label: 0, bag_size: 9387\n",
      "batch 719, loss: 0.1105, label: 1, bag_size: 9649\n",
      "batch 739, loss: 0.2923, label: 0, bag_size: 3557\n",
      "batch 759, loss: 0.0354, label: 0, bag_size: 2732\n",
      "batch 779, loss: 0.0082, label: 1, bag_size: 19500\n",
      "batch 799, loss: 0.4701, label: 0, bag_size: 20478\n",
      "batch 819, loss: 0.0253, label: 1, bag_size: 15689\n",
      "Epoch: 12, train_loss: 0.2188, train_error: 0.0815\n",
      "class 0: acc 0.9295774647887324, correct 396/426\n",
      "class 1: acc 0.9065656565656566, correct 359/396\n",
      "\n",
      "Val Set, val_loss: 0.3564, val_error: 0.1376, auc: 0.9517\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.7777777777777778, correct 49/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0277, label: 1, bag_size: 3450\n",
      "batch 39, loss: 0.0086, label: 0, bag_size: 16607\n",
      "batch 59, loss: 0.0129, label: 0, bag_size: 1213\n",
      "batch 79, loss: 0.0162, label: 1, bag_size: 3937\n",
      "batch 99, loss: 0.0036, label: 0, bag_size: 3190\n",
      "batch 119, loss: 0.0005, label: 0, bag_size: 18574\n",
      "batch 139, loss: 0.0027, label: 0, bag_size: 17368\n",
      "batch 159, loss: 0.0096, label: 1, bag_size: 4394\n",
      "batch 179, loss: 0.0020, label: 0, bag_size: 12137\n",
      "batch 199, loss: 0.0113, label: 0, bag_size: 14305\n",
      "batch 219, loss: 0.0407, label: 1, bag_size: 13015\n",
      "batch 239, loss: 0.0104, label: 0, bag_size: 11865\n",
      "batch 259, loss: 0.0204, label: 1, bag_size: 1786\n",
      "batch 279, loss: 0.0846, label: 0, bag_size: 15736\n",
      "batch 299, loss: 0.0299, label: 1, bag_size: 7119\n",
      "batch 319, loss: 0.0500, label: 0, bag_size: 16211\n",
      "batch 339, loss: 0.1344, label: 0, bag_size: 20555\n",
      "batch 359, loss: 0.0393, label: 1, bag_size: 8438\n",
      "batch 379, loss: 0.0367, label: 0, bag_size: 11727\n",
      "batch 399, loss: 0.0129, label: 0, bag_size: 10898\n",
      "batch 419, loss: 0.0570, label: 0, bag_size: 2351\n",
      "batch 439, loss: 0.0074, label: 1, bag_size: 16512\n",
      "batch 459, loss: 0.5742, label: 0, bag_size: 11607\n",
      "batch 479, loss: 0.0132, label: 0, bag_size: 19472\n",
      "batch 499, loss: 0.0273, label: 1, bag_size: 2405\n",
      "batch 519, loss: 0.4066, label: 1, bag_size: 2842\n",
      "batch 539, loss: 0.0280, label: 0, bag_size: 23714\n",
      "batch 559, loss: 0.0193, label: 1, bag_size: 5454\n",
      "batch 579, loss: 0.0234, label: 0, bag_size: 2079\n",
      "batch 599, loss: 0.0175, label: 0, bag_size: 4497\n",
      "batch 619, loss: 1.0431, label: 0, bag_size: 11922\n",
      "batch 639, loss: 0.1098, label: 1, bag_size: 6599\n",
      "batch 659, loss: 0.0203, label: 1, bag_size: 9971\n",
      "batch 679, loss: 0.0005, label: 0, bag_size: 2091\n",
      "batch 699, loss: 0.1492, label: 0, bag_size: 14893\n",
      "batch 719, loss: 0.0039, label: 0, bag_size: 20150\n",
      "batch 739, loss: 0.0007, label: 0, bag_size: 3459\n",
      "batch 759, loss: 0.6259, label: 0, bag_size: 2959\n",
      "batch 779, loss: 0.0038, label: 1, bag_size: 20161\n",
      "batch 799, loss: 0.0048, label: 0, bag_size: 11199\n",
      "batch 819, loss: 0.0317, label: 0, bag_size: 763\n",
      "Epoch: 13, train_loss: 0.2340, train_error: 0.0742\n",
      "class 0: acc 0.9285714285714286, correct 377/406\n",
      "class 1: acc 0.9230769230769231, correct 384/416\n",
      "\n",
      "Val Set, val_loss: 0.3572, val_error: 0.1468, auc: 0.9538\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.7619047619047619, correct 48/63\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0032, label: 0, bag_size: 27158\n",
      "batch 39, loss: 0.0175, label: 0, bag_size: 14828\n",
      "batch 59, loss: 0.0009, label: 0, bag_size: 13964\n",
      "batch 79, loss: 0.1022, label: 0, bag_size: 19470\n",
      "batch 99, loss: 0.0433, label: 1, bag_size: 2193\n",
      "batch 119, loss: 0.0494, label: 0, bag_size: 3670\n",
      "batch 139, loss: 0.2067, label: 1, bag_size: 13089\n",
      "batch 159, loss: 0.0390, label: 1, bag_size: 7515\n",
      "batch 179, loss: 0.0031, label: 0, bag_size: 13964\n",
      "batch 199, loss: 0.0157, label: 1, bag_size: 9548\n",
      "batch 219, loss: 0.0022, label: 0, bag_size: 20150\n",
      "batch 239, loss: 0.0014, label: 0, bag_size: 18574\n",
      "batch 259, loss: 0.0578, label: 0, bag_size: 18215\n",
      "batch 279, loss: 0.0122, label: 1, bag_size: 16034\n",
      "batch 299, loss: 0.0803, label: 1, bag_size: 6205\n",
      "batch 319, loss: 0.1070, label: 0, bag_size: 5161\n",
      "batch 339, loss: 0.0062, label: 1, bag_size: 5723\n",
      "batch 359, loss: 0.0442, label: 0, bag_size: 14377\n",
      "batch 379, loss: 0.0186, label: 0, bag_size: 24911\n",
      "batch 399, loss: 0.1554, label: 1, bag_size: 1242\n",
      "batch 419, loss: 0.0022, label: 0, bag_size: 12687\n",
      "batch 439, loss: 0.0684, label: 0, bag_size: 8959\n",
      "batch 459, loss: 0.0800, label: 0, bag_size: 763\n",
      "batch 479, loss: 0.1224, label: 1, bag_size: 1920\n",
      "batch 499, loss: 1.5976, label: 1, bag_size: 12340\n",
      "batch 519, loss: 0.0270, label: 0, bag_size: 11122\n",
      "batch 539, loss: 3.0597, label: 0, bag_size: 2815\n",
      "batch 559, loss: 0.3060, label: 1, bag_size: 4715\n",
      "batch 579, loss: 0.8397, label: 1, bag_size: 2681\n",
      "batch 599, loss: 0.0166, label: 0, bag_size: 19390\n",
      "batch 619, loss: 0.0314, label: 0, bag_size: 2160\n",
      "batch 639, loss: 0.0626, label: 0, bag_size: 2079\n",
      "batch 659, loss: 0.0516, label: 0, bag_size: 16087\n",
      "batch 679, loss: 0.0030, label: 0, bag_size: 21218\n",
      "batch 699, loss: 0.0083, label: 0, bag_size: 17633\n",
      "batch 719, loss: 0.0170, label: 0, bag_size: 10490\n",
      "batch 739, loss: 0.0981, label: 0, bag_size: 21361\n",
      "batch 759, loss: 0.0828, label: 1, bag_size: 9689\n",
      "batch 779, loss: 0.0068, label: 1, bag_size: 10394\n",
      "batch 799, loss: 0.0723, label: 1, bag_size: 14681\n",
      "batch 819, loss: 0.0042, label: 0, bag_size: 31106\n",
      "Epoch: 14, train_loss: 0.2349, train_error: 0.0888\n",
      "class 0: acc 0.9158653846153846, correct 381/416\n",
      "class 1: acc 0.9064039408866995, correct 368/406\n",
      "\n",
      "Val Set, val_loss: 0.2787, val_error: 0.1101, auc: 0.9555\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0059, label: 1, bag_size: 14618\n",
      "batch 39, loss: 0.0924, label: 1, bag_size: 21827\n",
      "batch 59, loss: 0.3617, label: 0, bag_size: 7612\n",
      "batch 79, loss: 0.0989, label: 0, bag_size: 2996\n",
      "batch 99, loss: 0.5142, label: 0, bag_size: 3444\n",
      "batch 119, loss: 0.0011, label: 0, bag_size: 17630\n",
      "batch 139, loss: 3.8222, label: 1, bag_size: 1038\n",
      "batch 159, loss: 0.0110, label: 1, bag_size: 3674\n",
      "batch 179, loss: 0.0227, label: 0, bag_size: 1712\n",
      "batch 199, loss: 0.0415, label: 0, bag_size: 30751\n",
      "batch 219, loss: 0.7187, label: 0, bag_size: 8427\n",
      "batch 239, loss: 0.0944, label: 0, bag_size: 1142\n",
      "batch 259, loss: 0.0216, label: 0, bag_size: 2548\n",
      "batch 279, loss: 0.2434, label: 0, bag_size: 5409\n",
      "batch 299, loss: 0.3401, label: 1, bag_size: 15192\n",
      "batch 319, loss: 1.6040, label: 0, bag_size: 47866\n",
      "batch 339, loss: 0.0026, label: 0, bag_size: 16341\n",
      "batch 359, loss: 0.2525, label: 0, bag_size: 3708\n",
      "batch 379, loss: 0.0095, label: 1, bag_size: 4250\n",
      "batch 399, loss: 0.0007, label: 1, bag_size: 6752\n",
      "batch 419, loss: 0.0053, label: 1, bag_size: 8602\n",
      "batch 439, loss: 2.2993, label: 0, bag_size: 2732\n",
      "batch 459, loss: 0.0537, label: 0, bag_size: 11390\n",
      "batch 479, loss: 0.0095, label: 0, bag_size: 21138\n",
      "batch 499, loss: 0.0184, label: 1, bag_size: 2278\n",
      "batch 519, loss: 0.0060, label: 1, bag_size: 2412\n",
      "batch 539, loss: 0.0028, label: 0, bag_size: 2820\n",
      "batch 559, loss: 0.3251, label: 0, bag_size: 10410\n",
      "batch 579, loss: 0.0018, label: 0, bag_size: 7191\n",
      "batch 599, loss: 0.3686, label: 0, bag_size: 2270\n",
      "batch 619, loss: 0.0013, label: 0, bag_size: 12148\n",
      "batch 639, loss: 0.0237, label: 1, bag_size: 9955\n",
      "batch 659, loss: 0.0344, label: 1, bag_size: 5345\n",
      "batch 679, loss: 0.0018, label: 1, bag_size: 4317\n",
      "batch 699, loss: 0.0436, label: 0, bag_size: 2236\n",
      "batch 719, loss: 0.2326, label: 0, bag_size: 5211\n",
      "batch 739, loss: 0.0098, label: 0, bag_size: 1712\n",
      "batch 759, loss: 0.0206, label: 1, bag_size: 13786\n",
      "batch 779, loss: 0.0299, label: 0, bag_size: 2490\n",
      "batch 799, loss: 0.0736, label: 1, bag_size: 9877\n",
      "batch 819, loss: 0.0007, label: 1, bag_size: 1360\n",
      "Epoch: 15, train_loss: 0.2645, train_error: 0.0998\n",
      "class 0: acc 0.9147465437788018, correct 397/434\n",
      "class 1: acc 0.884020618556701, correct 343/388\n",
      "\n",
      "Val Set, val_loss: 0.2751, val_error: 0.1284, auc: 0.9565\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.8253968253968254, correct 52/63\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0016, label: 1, bag_size: 16512\n",
      "batch 39, loss: 0.0180, label: 0, bag_size: 32227\n",
      "batch 59, loss: 0.0329, label: 1, bag_size: 1746\n",
      "batch 79, loss: 0.1851, label: 0, bag_size: 931\n",
      "batch 99, loss: 0.0056, label: 1, bag_size: 2638\n",
      "batch 119, loss: 0.1892, label: 0, bag_size: 1800\n",
      "batch 139, loss: 0.3176, label: 1, bag_size: 1294\n",
      "batch 159, loss: 0.0172, label: 0, bag_size: 22681\n",
      "batch 179, loss: 0.0016, label: 0, bag_size: 9433\n",
      "batch 199, loss: 0.1019, label: 1, bag_size: 1064\n",
      "batch 219, loss: 0.0010, label: 1, bag_size: 8522\n",
      "batch 239, loss: 0.1679, label: 1, bag_size: 3224\n",
      "batch 259, loss: 0.0534, label: 1, bag_size: 4821\n",
      "batch 279, loss: 0.0231, label: 1, bag_size: 14681\n",
      "batch 299, loss: 0.0122, label: 0, bag_size: 18954\n",
      "batch 319, loss: 0.0066, label: 1, bag_size: 19932\n",
      "batch 339, loss: 0.0048, label: 1, bag_size: 5256\n",
      "batch 359, loss: 0.1263, label: 0, bag_size: 25814\n",
      "batch 379, loss: 0.0266, label: 1, bag_size: 7119\n",
      "batch 399, loss: 0.3160, label: 0, bag_size: 20230\n",
      "batch 419, loss: 0.1642, label: 1, bag_size: 7148\n",
      "batch 439, loss: 0.0843, label: 1, bag_size: 2682\n",
      "batch 459, loss: 0.0049, label: 0, bag_size: 11690\n",
      "batch 479, loss: 0.0519, label: 1, bag_size: 2848\n",
      "batch 499, loss: 0.0028, label: 1, bag_size: 6734\n",
      "batch 519, loss: 0.0025, label: 0, bag_size: 3459\n",
      "batch 539, loss: 0.0571, label: 0, bag_size: 9930\n",
      "batch 559, loss: 0.1628, label: 0, bag_size: 9471\n",
      "batch 579, loss: 0.6095, label: 0, bag_size: 1714\n",
      "batch 599, loss: 0.0042, label: 1, bag_size: 2412\n",
      "batch 619, loss: 0.0401, label: 1, bag_size: 1064\n",
      "batch 639, loss: 0.0167, label: 0, bag_size: 4497\n",
      "batch 659, loss: 1.0116, label: 1, bag_size: 1497\n",
      "batch 679, loss: 0.0906, label: 0, bag_size: 19808\n",
      "batch 699, loss: 0.0685, label: 1, bag_size: 12697\n",
      "batch 719, loss: 0.2079, label: 1, bag_size: 7066\n",
      "batch 739, loss: 0.0056, label: 0, bag_size: 16607\n",
      "batch 759, loss: 1.3957, label: 0, bag_size: 7835\n",
      "batch 779, loss: 0.0342, label: 0, bag_size: 13332\n",
      "batch 799, loss: 0.0191, label: 1, bag_size: 5155\n",
      "batch 819, loss: 0.0017, label: 0, bag_size: 10995\n",
      "Epoch: 16, train_loss: 0.1947, train_error: 0.0657\n",
      "class 0: acc 0.9335038363171355, correct 365/391\n",
      "class 1: acc 0.9350348027842227, correct 403/431\n",
      "\n",
      "Val Set, val_loss: 0.3059, val_error: 0.1193, auc: 0.9569\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.8253968253968254, correct 52/63\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0033, label: 0, bag_size: 2236\n",
      "batch 39, loss: 0.0182, label: 0, bag_size: 16782\n",
      "batch 59, loss: 0.6277, label: 0, bag_size: 9597\n",
      "batch 79, loss: 0.1561, label: 0, bag_size: 3541\n",
      "batch 99, loss: 0.0126, label: 1, bag_size: 5605\n",
      "batch 119, loss: 0.0439, label: 0, bag_size: 11187\n",
      "batch 139, loss: 0.0382, label: 0, bag_size: 3198\n",
      "batch 159, loss: 0.8750, label: 0, bag_size: 3876\n",
      "batch 179, loss: 0.1540, label: 1, bag_size: 11684\n",
      "batch 199, loss: 0.0240, label: 1, bag_size: 1459\n",
      "batch 219, loss: 3.1235, label: 0, bag_size: 2815\n",
      "batch 239, loss: 0.1564, label: 1, bag_size: 7445\n",
      "batch 259, loss: 0.0761, label: 1, bag_size: 12895\n",
      "batch 279, loss: 0.0280, label: 0, bag_size: 1920\n",
      "batch 299, loss: 0.0162, label: 0, bag_size: 2179\n",
      "batch 319, loss: 0.0092, label: 1, bag_size: 6927\n",
      "batch 339, loss: 1.0019, label: 1, bag_size: 12494\n",
      "batch 359, loss: 0.0039, label: 0, bag_size: 21093\n",
      "batch 379, loss: 0.0083, label: 0, bag_size: 14305\n",
      "batch 399, loss: 0.1148, label: 0, bag_size: 7031\n",
      "batch 419, loss: 0.0043, label: 0, bag_size: 32227\n",
      "batch 439, loss: 0.0010, label: 1, bag_size: 10112\n",
      "batch 459, loss: 0.0006, label: 1, bag_size: 17486\n",
      "batch 479, loss: 0.0758, label: 1, bag_size: 14230\n",
      "batch 499, loss: 0.0370, label: 1, bag_size: 15609\n",
      "batch 519, loss: 0.0092, label: 1, bag_size: 9971\n",
      "batch 539, loss: 0.0100, label: 0, bag_size: 11735\n",
      "batch 559, loss: 0.0108, label: 0, bag_size: 17791\n",
      "batch 579, loss: 0.0387, label: 0, bag_size: 31085\n",
      "batch 599, loss: 0.0037, label: 1, bag_size: 6343\n",
      "batch 619, loss: 0.3863, label: 1, bag_size: 7445\n",
      "batch 639, loss: 0.0483, label: 1, bag_size: 12425\n",
      "batch 659, loss: 1.1464, label: 0, bag_size: 2270\n",
      "batch 679, loss: 0.0165, label: 0, bag_size: 15898\n",
      "batch 699, loss: 0.0255, label: 1, bag_size: 6781\n",
      "batch 719, loss: 0.1063, label: 1, bag_size: 3980\n",
      "batch 739, loss: 0.0811, label: 0, bag_size: 9387\n",
      "batch 759, loss: 0.1623, label: 0, bag_size: 2652\n",
      "batch 779, loss: 0.0467, label: 1, bag_size: 4054\n",
      "batch 799, loss: 0.0143, label: 0, bag_size: 3670\n",
      "batch 819, loss: 0.0171, label: 0, bag_size: 18240\n",
      "Epoch: 17, train_loss: 0.2162, train_error: 0.0693\n",
      "class 0: acc 0.9344660194174758, correct 385/412\n",
      "class 1: acc 0.926829268292683, correct 380/410\n",
      "\n",
      "Val Set, val_loss: 0.3692, val_error: 0.1376, auc: 0.9569\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.7777777777777778, correct 49/63\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0039, label: 1, bag_size: 4959\n",
      "batch 39, loss: 0.3095, label: 1, bag_size: 2682\n",
      "batch 59, loss: 0.0138, label: 1, bag_size: 4715\n",
      "batch 79, loss: 0.0038, label: 1, bag_size: 6734\n",
      "batch 99, loss: 0.0075, label: 0, bag_size: 22828\n",
      "batch 119, loss: 0.0291, label: 0, bag_size: 1825\n",
      "batch 139, loss: 0.0787, label: 0, bag_size: 9387\n",
      "batch 159, loss: 0.9140, label: 0, bag_size: 2458\n",
      "batch 179, loss: 0.2031, label: 1, bag_size: 7798\n",
      "batch 199, loss: 0.1203, label: 1, bag_size: 4054\n",
      "batch 219, loss: 0.0295, label: 0, bag_size: 16211\n",
      "batch 239, loss: 0.0334, label: 0, bag_size: 8812\n",
      "batch 259, loss: 0.2273, label: 0, bag_size: 21864\n",
      "batch 279, loss: 0.0360, label: 0, bag_size: 13332\n",
      "batch 299, loss: 0.0105, label: 0, bag_size: 10898\n",
      "batch 319, loss: 0.3521, label: 1, bag_size: 2681\n",
      "batch 339, loss: 0.0679, label: 1, bag_size: 15931\n",
      "batch 359, loss: 0.1581, label: 1, bag_size: 13786\n",
      "batch 379, loss: 0.0064, label: 1, bag_size: 6769\n",
      "batch 399, loss: 0.2095, label: 1, bag_size: 1064\n",
      "batch 419, loss: 0.0256, label: 1, bag_size: 3856\n",
      "batch 439, loss: 0.0010, label: 1, bag_size: 10394\n",
      "batch 459, loss: 0.0246, label: 1, bag_size: 16565\n",
      "batch 479, loss: 0.0405, label: 0, bag_size: 26271\n",
      "batch 499, loss: 0.0072, label: 1, bag_size: 10969\n",
      "batch 519, loss: 0.0464, label: 1, bag_size: 3674\n",
      "batch 539, loss: 0.3686, label: 0, bag_size: 5211\n",
      "batch 559, loss: 0.0469, label: 0, bag_size: 8866\n",
      "batch 579, loss: 0.0852, label: 1, bag_size: 12575\n",
      "batch 599, loss: 0.0175, label: 0, bag_size: 6727\n",
      "batch 619, loss: 0.0031, label: 0, bag_size: 14828\n",
      "batch 639, loss: 0.0198, label: 0, bag_size: 21076\n",
      "batch 659, loss: 0.0044, label: 0, bag_size: 20666\n",
      "batch 679, loss: 0.0077, label: 0, bag_size: 9433\n",
      "batch 699, loss: 0.0112, label: 0, bag_size: 13777\n",
      "batch 719, loss: 0.1919, label: 1, bag_size: 7468\n",
      "batch 739, loss: 0.0093, label: 1, bag_size: 2278\n",
      "batch 759, loss: 0.0574, label: 1, bag_size: 2092\n",
      "batch 779, loss: 0.4177, label: 1, bag_size: 2140\n",
      "batch 799, loss: 0.0230, label: 1, bag_size: 3937\n",
      "batch 819, loss: 0.7196, label: 0, bag_size: 2270\n",
      "Epoch: 18, train_loss: 0.2030, train_error: 0.0730\n",
      "class 0: acc 0.9265822784810127, correct 366/395\n",
      "class 1: acc 0.927400468384075, correct 396/427\n",
      "\n",
      "Val Set, val_loss: 0.3779, val_error: 0.1468, auc: 0.9555\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.7619047619047619, correct 48/63\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0012, label: 1, bag_size: 6343\n",
      "batch 39, loss: 0.0004, label: 1, bag_size: 4128\n",
      "batch 59, loss: 0.2108, label: 1, bag_size: 7066\n",
      "batch 79, loss: 0.0027, label: 0, bag_size: 20150\n",
      "batch 99, loss: 0.0006, label: 1, bag_size: 2579\n",
      "batch 119, loss: 0.0272, label: 1, bag_size: 18603\n",
      "batch 139, loss: 0.0936, label: 0, bag_size: 23618\n",
      "batch 159, loss: 0.0942, label: 1, bag_size: 9689\n",
      "batch 179, loss: 0.0178, label: 0, bag_size: 1920\n",
      "batch 199, loss: 0.2825, label: 1, bag_size: 6928\n",
      "batch 219, loss: 0.2549, label: 0, bag_size: 2457\n",
      "batch 239, loss: 0.0317, label: 0, bag_size: 7557\n",
      "batch 259, loss: 0.8378, label: 1, bag_size: 1444\n",
      "batch 279, loss: 0.0021, label: 1, bag_size: 6734\n",
      "batch 299, loss: 0.1584, label: 0, bag_size: 3725\n",
      "batch 319, loss: 0.0202, label: 0, bag_size: 7637\n",
      "batch 339, loss: 0.0238, label: 1, bag_size: 2480\n",
      "batch 359, loss: 0.0292, label: 0, bag_size: 9470\n",
      "batch 379, loss: 0.0303, label: 0, bag_size: 15841\n",
      "batch 399, loss: 0.0123, label: 0, bag_size: 16992\n",
      "batch 419, loss: 0.1071, label: 1, bag_size: 8448\n",
      "batch 439, loss: 0.0015, label: 0, bag_size: 2628\n",
      "batch 459, loss: 0.0246, label: 0, bag_size: 17268\n",
      "batch 479, loss: 0.0467, label: 0, bag_size: 1789\n",
      "batch 499, loss: 0.0105, label: 0, bag_size: 2282\n",
      "batch 519, loss: 0.0690, label: 0, bag_size: 2920\n",
      "batch 539, loss: 2.1217, label: 1, bag_size: 1703\n",
      "batch 559, loss: 0.3028, label: 1, bag_size: 771\n",
      "batch 579, loss: 0.0028, label: 1, bag_size: 2904\n",
      "batch 599, loss: 0.0405, label: 0, bag_size: 10263\n",
      "batch 619, loss: 0.4545, label: 0, bag_size: 10146\n",
      "batch 639, loss: 0.0197, label: 1, bag_size: 16379\n",
      "batch 659, loss: 0.0007, label: 1, bag_size: 2966\n",
      "batch 679, loss: 1.7686, label: 1, bag_size: 1038\n",
      "batch 699, loss: 0.0285, label: 1, bag_size: 34356\n",
      "batch 719, loss: 0.0139, label: 1, bag_size: 14779\n",
      "batch 739, loss: 0.0006, label: 1, bag_size: 10482\n",
      "batch 759, loss: 0.0329, label: 0, bag_size: 2367\n",
      "batch 779, loss: 0.0014, label: 0, bag_size: 14305\n",
      "batch 799, loss: 0.0415, label: 0, bag_size: 6281\n",
      "batch 819, loss: 0.0428, label: 0, bag_size: 15057\n",
      "Epoch: 19, train_loss: 0.2194, train_error: 0.0827\n",
      "class 0: acc 0.9158653846153846, correct 381/416\n",
      "class 1: acc 0.9187192118226601, correct 373/406\n",
      "\n",
      "Val Set, val_loss: 0.3814, val_error: 0.1284, auc: 0.9551\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.7936507936507936, correct 50/63\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3217, label: 0, bag_size: 1701\n",
      "batch 39, loss: 0.0011, label: 1, bag_size: 11642\n",
      "batch 59, loss: 0.3700, label: 0, bag_size: 7428\n",
      "batch 79, loss: 0.0796, label: 1, bag_size: 13015\n",
      "batch 99, loss: 0.0597, label: 1, bag_size: 15192\n",
      "batch 119, loss: 0.0052, label: 0, bag_size: 4959\n",
      "batch 139, loss: 0.0067, label: 0, bag_size: 31085\n",
      "batch 159, loss: 0.0013, label: 1, bag_size: 6090\n",
      "batch 179, loss: 0.0278, label: 0, bag_size: 11735\n",
      "batch 199, loss: 0.0080, label: 1, bag_size: 8003\n",
      "batch 219, loss: 0.2008, label: 1, bag_size: 13089\n",
      "batch 239, loss: 0.2189, label: 1, bag_size: 2559\n",
      "batch 259, loss: 0.0142, label: 1, bag_size: 12697\n",
      "batch 279, loss: 0.0018, label: 1, bag_size: 6090\n",
      "batch 299, loss: 0.0032, label: 1, bag_size: 4102\n",
      "batch 319, loss: 0.0050, label: 0, bag_size: 12131\n",
      "batch 339, loss: 0.0104, label: 1, bag_size: 1249\n",
      "batch 359, loss: 0.0312, label: 0, bag_size: 17268\n",
      "batch 379, loss: 0.1462, label: 0, bag_size: 13023\n",
      "batch 399, loss: 0.1665, label: 0, bag_size: 6093\n",
      "batch 419, loss: 0.0019, label: 0, bag_size: 803\n",
      "batch 439, loss: 0.0084, label: 0, bag_size: 4465\n",
      "batch 459, loss: 0.0002, label: 0, bag_size: 9433\n",
      "batch 479, loss: 0.0151, label: 1, bag_size: 22264\n",
      "batch 499, loss: 0.0011, label: 1, bag_size: 7381\n",
      "batch 519, loss: 0.0209, label: 1, bag_size: 2480\n",
      "batch 539, loss: 0.0276, label: 1, bag_size: 2785\n",
      "batch 559, loss: 0.0089, label: 0, bag_size: 19466\n",
      "batch 579, loss: 3.8295, label: 0, bag_size: 3897\n",
      "batch 599, loss: 0.0005, label: 0, bag_size: 19659\n",
      "batch 619, loss: 0.0016, label: 1, bag_size: 19932\n",
      "batch 639, loss: 0.6395, label: 0, bag_size: 21361\n",
      "batch 659, loss: 0.0016, label: 0, bag_size: 7191\n",
      "batch 679, loss: 1.6521, label: 0, bag_size: 2959\n",
      "batch 699, loss: 0.0160, label: 0, bag_size: 18132\n",
      "batch 719, loss: 0.1754, label: 1, bag_size: 4308\n",
      "batch 739, loss: 0.0071, label: 0, bag_size: 8582\n",
      "batch 759, loss: 0.0256, label: 0, bag_size: 8866\n",
      "batch 779, loss: 0.0239, label: 0, bag_size: 1592\n",
      "batch 799, loss: 2.7649, label: 1, bag_size: 1533\n",
      "batch 819, loss: 0.3958, label: 1, bag_size: 3879\n",
      "Epoch: 20, train_loss: 0.1935, train_error: 0.0633\n",
      "class 0: acc 0.9358974358974359, correct 365/390\n",
      "class 1: acc 0.9375, correct 405/432\n",
      "\n",
      "Val Set, val_loss: 0.2557, val_error: 0.0917, auc: 0.9596\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.8888888888888888, correct 56/63\n",
      "Validation loss decreased (0.274097 --> 0.255700).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0033, label: 1, bag_size: 11220\n",
      "batch 39, loss: 0.3689, label: 0, bag_size: 4241\n",
      "batch 59, loss: 0.0031, label: 0, bag_size: 14305\n",
      "batch 79, loss: 0.0017, label: 1, bag_size: 7873\n",
      "batch 99, loss: 0.0639, label: 0, bag_size: 26208\n",
      "batch 119, loss: 0.1079, label: 1, bag_size: 4789\n",
      "batch 139, loss: 0.2460, label: 0, bag_size: 1690\n",
      "batch 159, loss: 0.0005, label: 1, bag_size: 10112\n",
      "batch 179, loss: 0.0233, label: 1, bag_size: 3368\n",
      "batch 199, loss: 0.0152, label: 1, bag_size: 1888\n",
      "batch 219, loss: 0.0003, label: 1, bag_size: 5340\n",
      "batch 239, loss: 0.0237, label: 0, bag_size: 11383\n",
      "batch 259, loss: 0.0005, label: 1, bag_size: 9065\n",
      "batch 279, loss: 0.0280, label: 1, bag_size: 7613\n",
      "batch 299, loss: 0.0086, label: 1, bag_size: 12460\n",
      "batch 319, loss: 0.0018, label: 1, bag_size: 11642\n",
      "batch 339, loss: 0.0987, label: 1, bag_size: 3656\n",
      "batch 359, loss: 0.0045, label: 1, bag_size: 11884\n",
      "batch 379, loss: 0.0373, label: 1, bag_size: 5763\n",
      "batch 399, loss: 0.0608, label: 1, bag_size: 7066\n",
      "batch 419, loss: 0.0001, label: 1, bag_size: 7110\n",
      "batch 439, loss: 0.0010, label: 1, bag_size: 9571\n",
      "batch 459, loss: 0.0015, label: 1, bag_size: 1412\n",
      "batch 479, loss: 0.0006, label: 0, bag_size: 1052\n",
      "batch 499, loss: 0.2200, label: 1, bag_size: 1483\n",
      "batch 519, loss: 0.0013, label: 1, bag_size: 3968\n",
      "batch 539, loss: 0.0010, label: 1, bag_size: 4317\n",
      "batch 559, loss: 0.0255, label: 0, bag_size: 6884\n",
      "batch 579, loss: 0.0001, label: 1, bag_size: 9571\n",
      "batch 599, loss: 0.0272, label: 1, bag_size: 5441\n",
      "batch 619, loss: 0.0277, label: 0, bag_size: 11512\n",
      "batch 639, loss: 0.1241, label: 0, bag_size: 23714\n",
      "batch 659, loss: 0.0838, label: 1, bag_size: 2455\n",
      "batch 679, loss: 0.0812, label: 0, bag_size: 1213\n",
      "batch 699, loss: 0.0027, label: 1, bag_size: 20537\n",
      "batch 719, loss: 0.0000, label: 1, bag_size: 10867\n",
      "batch 739, loss: 0.0291, label: 0, bag_size: 2351\n",
      "batch 759, loss: 0.7067, label: 1, bag_size: 9330\n",
      "batch 779, loss: 0.2368, label: 1, bag_size: 6478\n",
      "batch 799, loss: 0.1035, label: 0, bag_size: 9387\n",
      "batch 819, loss: 0.0797, label: 0, bag_size: 7557\n",
      "Epoch: 21, train_loss: 0.1745, train_error: 0.0681\n",
      "class 0: acc 0.9384236453201971, correct 381/406\n",
      "class 1: acc 0.9254807692307693, correct 385/416\n",
      "\n",
      "Val Set, val_loss: 0.3575, val_error: 0.1376, auc: 0.9579\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.7777777777777778, correct 49/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1714, label: 0, bag_size: 3876\n",
      "batch 39, loss: 0.0037, label: 1, bag_size: 15213\n",
      "batch 59, loss: 0.0277, label: 1, bag_size: 13440\n",
      "batch 79, loss: 1.2496, label: 1, bag_size: 1703\n",
      "batch 99, loss: 0.0014, label: 0, bag_size: 13225\n",
      "batch 119, loss: 0.0116, label: 1, bag_size: 2686\n",
      "batch 139, loss: 0.0012, label: 0, bag_size: 12212\n",
      "batch 159, loss: 0.0430, label: 0, bag_size: 25558\n",
      "batch 179, loss: 0.2056, label: 1, bag_size: 2814\n",
      "batch 199, loss: 0.0225, label: 1, bag_size: 6731\n",
      "batch 219, loss: 0.0101, label: 0, bag_size: 17268\n",
      "batch 239, loss: 0.0004, label: 0, bag_size: 7191\n",
      "batch 259, loss: 0.2219, label: 0, bag_size: 2609\n",
      "batch 279, loss: 0.3432, label: 1, bag_size: 15931\n",
      "batch 299, loss: 0.0007, label: 1, bag_size: 28527\n",
      "batch 319, loss: 0.0003, label: 1, bag_size: 3409\n",
      "batch 339, loss: 0.1283, label: 1, bag_size: 1242\n",
      "batch 359, loss: 0.0146, label: 0, bag_size: 21385\n",
      "batch 379, loss: 0.0052, label: 0, bag_size: 27158\n",
      "batch 399, loss: 0.2420, label: 1, bag_size: 7989\n",
      "batch 419, loss: 0.0310, label: 1, bag_size: 9533\n",
      "batch 439, loss: 0.0021, label: 1, bag_size: 4480\n",
      "batch 459, loss: 0.0265, label: 1, bag_size: 12946\n",
      "batch 479, loss: 0.0577, label: 1, bag_size: 8660\n",
      "batch 499, loss: 0.0387, label: 0, bag_size: 9485\n",
      "batch 519, loss: 0.0341, label: 0, bag_size: 1498\n",
      "batch 539, loss: 0.0012, label: 0, bag_size: 14956\n",
      "batch 559, loss: 0.0024, label: 0, bag_size: 16782\n",
      "batch 579, loss: 0.1747, label: 1, bag_size: 6842\n",
      "batch 599, loss: 0.0016, label: 0, bag_size: 11527\n",
      "batch 619, loss: 0.0263, label: 1, bag_size: 8660\n",
      "batch 639, loss: 0.0070, label: 1, bag_size: 6927\n",
      "batch 659, loss: 0.0082, label: 1, bag_size: 10396\n",
      "batch 679, loss: 0.0824, label: 1, bag_size: 621\n",
      "batch 699, loss: 0.0119, label: 0, bag_size: 3725\n",
      "batch 719, loss: 0.0003, label: 1, bag_size: 3576\n",
      "batch 739, loss: 0.0094, label: 1, bag_size: 7217\n",
      "batch 759, loss: 2.7149, label: 0, bag_size: 5105\n",
      "batch 779, loss: 0.0499, label: 1, bag_size: 16890\n",
      "batch 799, loss: 0.0001, label: 0, bag_size: 10481\n",
      "batch 819, loss: 0.0105, label: 0, bag_size: 10898\n",
      "Epoch: 22, train_loss: 0.1760, train_error: 0.0584\n",
      "class 0: acc 0.9433497536945813, correct 383/406\n",
      "class 1: acc 0.9399038461538461, correct 391/416\n",
      "\n",
      "Val Set, val_loss: 0.2820, val_error: 0.1009, auc: 0.9617\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0014, label: 1, bag_size: 6734\n",
      "batch 39, loss: 0.1479, label: 0, bag_size: 12083\n",
      "batch 59, loss: 0.0744, label: 0, bag_size: 8330\n",
      "batch 79, loss: 0.0300, label: 0, bag_size: 2079\n",
      "batch 99, loss: 0.0008, label: 0, bag_size: 17368\n",
      "batch 119, loss: 0.0019, label: 0, bag_size: 9786\n",
      "batch 139, loss: 0.0013, label: 0, bag_size: 1416\n",
      "batch 159, loss: 0.0010, label: 0, bag_size: 11146\n",
      "batch 179, loss: 0.0141, label: 0, bag_size: 2179\n",
      "batch 199, loss: 0.0005, label: 1, bag_size: 3576\n",
      "batch 219, loss: 0.0003, label: 1, bag_size: 9065\n",
      "batch 239, loss: 0.0279, label: 1, bag_size: 8395\n",
      "batch 259, loss: 0.0013, label: 0, bag_size: 12687\n",
      "batch 279, loss: 0.4218, label: 1, bag_size: 4786\n",
      "batch 299, loss: 0.0083, label: 0, bag_size: 14319\n",
      "batch 319, loss: 0.0012, label: 1, bag_size: 3409\n",
      "batch 339, loss: 0.0005, label: 0, bag_size: 13964\n",
      "batch 359, loss: 0.0006, label: 1, bag_size: 5833\n",
      "batch 379, loss: 0.0452, label: 1, bag_size: 3651\n",
      "batch 399, loss: 0.0023, label: 0, bag_size: 15850\n",
      "batch 419, loss: 0.0078, label: 0, bag_size: 19472\n",
      "batch 439, loss: 0.0634, label: 0, bag_size: 30751\n",
      "batch 459, loss: 0.1853, label: 1, bag_size: 13089\n",
      "batch 479, loss: 0.0478, label: 0, bag_size: 18738\n",
      "batch 499, loss: 0.0015, label: 1, bag_size: 19606\n",
      "batch 519, loss: 0.4250, label: 0, bag_size: 11128\n",
      "batch 539, loss: 0.0019, label: 1, bag_size: 6734\n",
      "batch 559, loss: 0.0074, label: 1, bag_size: 9955\n",
      "batch 579, loss: 0.0305, label: 1, bag_size: 5894\n",
      "batch 599, loss: 0.0022, label: 1, bag_size: 13194\n",
      "batch 619, loss: 0.0509, label: 0, bag_size: 1639\n",
      "batch 639, loss: 0.0135, label: 1, bag_size: 16703\n",
      "batch 659, loss: 0.2502, label: 0, bag_size: 9421\n",
      "batch 679, loss: 0.1464, label: 1, bag_size: 1236\n",
      "batch 699, loss: 0.0047, label: 0, bag_size: 11146\n",
      "batch 719, loss: 0.0089, label: 1, bag_size: 5256\n",
      "batch 739, loss: 0.0280, label: 0, bag_size: 14885\n",
      "batch 759, loss: 0.0121, label: 0, bag_size: 1826\n",
      "batch 779, loss: 0.0009, label: 1, bag_size: 3207\n",
      "batch 799, loss: 0.2395, label: 0, bag_size: 1592\n",
      "batch 819, loss: 1.3405, label: 0, bag_size: 2219\n",
      "Epoch: 23, train_loss: 0.1925, train_error: 0.0693\n",
      "class 0: acc 0.9271356783919598, correct 369/398\n",
      "class 1: acc 0.9339622641509434, correct 396/424\n",
      "\n",
      "Val Set, val_loss: 0.2670, val_error: 0.0917, auc: 0.9610\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 3.6948, label: 0, bag_size: 2732\n",
      "batch 39, loss: 0.0130, label: 0, bag_size: 16782\n",
      "batch 59, loss: 0.0156, label: 0, bag_size: 14828\n",
      "batch 79, loss: 0.0075, label: 0, bag_size: 11546\n",
      "batch 99, loss: 0.0009, label: 1, bag_size: 4394\n",
      "batch 119, loss: 0.0132, label: 0, bag_size: 1458\n",
      "batch 139, loss: 0.0176, label: 0, bag_size: 22800\n",
      "batch 159, loss: 0.1454, label: 0, bag_size: 2652\n",
      "batch 179, loss: 0.0013, label: 0, bag_size: 17630\n",
      "batch 199, loss: 0.0023, label: 1, bag_size: 10394\n",
      "batch 219, loss: 0.0025, label: 1, bag_size: 7382\n",
      "batch 239, loss: 0.0003, label: 1, bag_size: 4039\n",
      "batch 259, loss: 0.0599, label: 0, bag_size: 14885\n",
      "batch 279, loss: 0.0305, label: 1, bag_size: 10969\n",
      "batch 299, loss: 0.1926, label: 0, bag_size: 30828\n",
      "batch 319, loss: 0.0205, label: 0, bag_size: 16087\n",
      "batch 339, loss: 0.0329, label: 0, bag_size: 7989\n",
      "batch 359, loss: 0.0087, label: 1, bag_size: 12603\n",
      "batch 379, loss: 0.0004, label: 0, bag_size: 9433\n",
      "batch 399, loss: 0.0153, label: 1, bag_size: 2278\n",
      "batch 419, loss: 0.0028, label: 1, bag_size: 11875\n",
      "batch 439, loss: 0.0064, label: 0, bag_size: 13225\n",
      "batch 459, loss: 0.0003, label: 1, bag_size: 7110\n",
      "batch 479, loss: 0.0033, label: 0, bag_size: 19659\n",
      "batch 499, loss: 0.0018, label: 1, bag_size: 11642\n",
      "batch 519, loss: 0.0064, label: 0, bag_size: 3502\n",
      "batch 539, loss: 0.0076, label: 0, bag_size: 5551\n",
      "batch 559, loss: 0.0029, label: 0, bag_size: 2820\n",
      "batch 579, loss: 0.4650, label: 1, bag_size: 1822\n",
      "batch 599, loss: 0.1932, label: 0, bag_size: 16521\n",
      "batch 619, loss: 0.0065, label: 1, bag_size: 8019\n",
      "batch 639, loss: 0.0480, label: 0, bag_size: 10444\n",
      "batch 659, loss: 0.0017, label: 0, bag_size: 14206\n",
      "batch 679, loss: 0.0027, label: 0, bag_size: 27158\n",
      "batch 699, loss: 0.0122, label: 0, bag_size: 22762\n",
      "batch 719, loss: 0.0108, label: 0, bag_size: 10490\n",
      "batch 739, loss: 0.0314, label: 1, bag_size: 13692\n",
      "batch 759, loss: 0.1934, label: 0, bag_size: 7557\n",
      "batch 779, loss: 0.1491, label: 0, bag_size: 3708\n",
      "batch 799, loss: 0.0009, label: 1, bag_size: 11122\n",
      "batch 819, loss: 0.0006, label: 1, bag_size: 14223\n",
      "Epoch: 24, train_loss: 0.1853, train_error: 0.0645\n",
      "class 0: acc 0.9513381995133819, correct 391/411\n",
      "class 1: acc 0.9197080291970803, correct 378/411\n",
      "\n",
      "Val Set, val_loss: 0.3183, val_error: 0.0917, auc: 0.9579\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0064, label: 0, bag_size: 5225\n",
      "batch 39, loss: 0.0384, label: 1, bag_size: 2790\n",
      "batch 59, loss: 0.0803, label: 0, bag_size: 5161\n",
      "batch 79, loss: 0.0096, label: 1, bag_size: 9747\n",
      "batch 99, loss: 0.0355, label: 0, bag_size: 11122\n",
      "batch 119, loss: 0.0082, label: 0, bag_size: 2652\n",
      "batch 139, loss: 0.0165, label: 1, bag_size: 7613\n",
      "batch 159, loss: 0.0005, label: 0, bag_size: 8252\n",
      "batch 179, loss: 0.0157, label: 0, bag_size: 15636\n",
      "batch 199, loss: 0.0108, label: 1, bag_size: 11600\n",
      "batch 219, loss: 0.1216, label: 1, bag_size: 2842\n",
      "batch 239, loss: 0.1433, label: 0, bag_size: 2920\n",
      "batch 259, loss: 0.0086, label: 0, bag_size: 19043\n",
      "batch 279, loss: 0.0108, label: 1, bag_size: 20333\n",
      "batch 299, loss: 0.0276, label: 1, bag_size: 12575\n",
      "batch 319, loss: 0.0001, label: 1, bag_size: 17486\n",
      "batch 339, loss: 0.0357, label: 0, bag_size: 13332\n",
      "batch 359, loss: 0.4547, label: 0, bag_size: 2070\n",
      "batch 379, loss: 0.0006, label: 1, bag_size: 9759\n",
      "batch 399, loss: 0.0021, label: 0, bag_size: 20150\n",
      "batch 419, loss: 0.0031, label: 1, bag_size: 2579\n",
      "batch 439, loss: 0.0002, label: 1, bag_size: 21009\n",
      "batch 459, loss: 0.0519, label: 0, bag_size: 1142\n",
      "batch 479, loss: 0.0007, label: 1, bag_size: 6731\n",
      "batch 499, loss: 0.0401, label: 0, bag_size: 14377\n",
      "batch 519, loss: 0.0808, label: 0, bag_size: 1814\n",
      "batch 539, loss: 1.2790, label: 0, bag_size: 3710\n",
      "batch 559, loss: 0.0047, label: 1, bag_size: 11220\n",
      "batch 579, loss: 1.4629, label: 0, bag_size: 2070\n",
      "batch 599, loss: 0.0045, label: 0, bag_size: 16720\n",
      "batch 619, loss: 1.2642, label: 1, bag_size: 1444\n",
      "batch 639, loss: 0.3051, label: 0, bag_size: 2458\n",
      "batch 659, loss: 0.0247, label: 0, bag_size: 1549\n",
      "batch 679, loss: 0.3768, label: 1, bag_size: 1444\n",
      "batch 699, loss: 0.0447, label: 0, bag_size: 7235\n",
      "batch 719, loss: 0.3690, label: 0, bag_size: 3238\n",
      "batch 739, loss: 0.0806, label: 0, bag_size: 1690\n",
      "batch 759, loss: 1.8943, label: 1, bag_size: 1703\n",
      "batch 779, loss: 1.1869, label: 0, bag_size: 10113\n",
      "batch 799, loss: 0.0919, label: 0, bag_size: 9387\n",
      "batch 819, loss: 0.0121, label: 0, bag_size: 14319\n",
      "Epoch: 25, train_loss: 0.1661, train_error: 0.0474\n",
      "class 0: acc 0.9563218390804598, correct 416/435\n",
      "class 1: acc 0.9483204134366925, correct 367/387\n",
      "\n",
      "Val Set, val_loss: 0.2861, val_error: 0.0917, auc: 0.9607\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5260, label: 1, bag_size: 21252\n",
      "batch 39, loss: 1.8120, label: 0, bag_size: 11306\n",
      "batch 59, loss: 0.0087, label: 0, bag_size: 1202\n",
      "batch 79, loss: 0.0015, label: 0, bag_size: 8661\n",
      "batch 99, loss: 0.0031, label: 0, bag_size: 9060\n",
      "batch 119, loss: 0.0016, label: 1, bag_size: 3634\n",
      "batch 139, loss: 0.0103, label: 0, bag_size: 25558\n",
      "batch 159, loss: 0.0156, label: 0, bag_size: 21218\n",
      "batch 179, loss: 0.3835, label: 0, bag_size: 25420\n",
      "batch 199, loss: 0.1085, label: 0, bag_size: 2098\n",
      "batch 219, loss: 0.0211, label: 0, bag_size: 1639\n",
      "batch 239, loss: 0.0125, label: 0, bag_size: 19470\n",
      "batch 259, loss: 0.0057, label: 0, bag_size: 11383\n",
      "batch 279, loss: 0.0195, label: 1, bag_size: 1022\n",
      "batch 299, loss: 1.4405, label: 1, bag_size: 15185\n",
      "batch 319, loss: 0.0487, label: 1, bag_size: 12946\n",
      "batch 339, loss: 0.0004, label: 1, bag_size: 14515\n",
      "batch 359, loss: 0.7644, label: 1, bag_size: 1822\n",
      "batch 379, loss: 0.0315, label: 0, bag_size: 9866\n",
      "batch 399, loss: 0.0086, label: 1, bag_size: 13692\n",
      "batch 419, loss: 0.0130, label: 1, bag_size: 16379\n",
      "batch 439, loss: 0.2146, label: 0, bag_size: 7835\n",
      "batch 459, loss: 0.0767, label: 1, bag_size: 7424\n",
      "batch 479, loss: 0.0100, label: 0, bag_size: 4465\n",
      "batch 499, loss: 0.0833, label: 0, bag_size: 24439\n",
      "batch 519, loss: 0.0724, label: 1, bag_size: 4239\n",
      "batch 539, loss: 0.0030, label: 1, bag_size: 18699\n",
      "batch 559, loss: 0.0334, label: 0, bag_size: 7557\n",
      "batch 579, loss: 0.4691, label: 0, bag_size: 3541\n",
      "batch 599, loss: 0.0095, label: 0, bag_size: 1202\n",
      "batch 619, loss: 0.0004, label: 0, bag_size: 2628\n",
      "batch 639, loss: 0.0313, label: 1, bag_size: 12575\n",
      "batch 659, loss: 0.0024, label: 1, bag_size: 12408\n",
      "batch 679, loss: 1.2715, label: 0, bag_size: 26208\n",
      "batch 699, loss: 0.0024, label: 1, bag_size: 6731\n",
      "batch 719, loss: 0.3249, label: 0, bag_size: 16690\n",
      "batch 739, loss: 0.0155, label: 1, bag_size: 3619\n",
      "batch 759, loss: 0.0008, label: 1, bag_size: 10394\n",
      "batch 779, loss: 0.4024, label: 1, bag_size: 1236\n",
      "batch 799, loss: 0.0073, label: 1, bag_size: 19606\n",
      "batch 819, loss: 3.9652, label: 0, bag_size: 2815\n",
      "Epoch: 26, train_loss: 0.1894, train_error: 0.0742\n",
      "class 0: acc 0.933649289099526, correct 394/422\n",
      "class 1: acc 0.9175, correct 367/400\n",
      "\n",
      "Val Set, val_loss: 0.3245, val_error: 0.1009, auc: 0.9545\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.8412698412698413, correct 53/63\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0274, label: 0, bag_size: 1825\n",
      "batch 39, loss: 0.0291, label: 0, bag_size: 2548\n",
      "batch 59, loss: 0.0063, label: 0, bag_size: 12137\n",
      "batch 79, loss: 0.1305, label: 1, bag_size: 8592\n",
      "batch 99, loss: 0.0088, label: 0, bag_size: 10535\n",
      "batch 119, loss: 0.0741, label: 1, bag_size: 3619\n",
      "batch 139, loss: 0.0570, label: 0, bag_size: 15841\n",
      "batch 159, loss: 0.6742, label: 1, bag_size: 1191\n",
      "batch 179, loss: 0.0049, label: 1, bag_size: 7935\n",
      "batch 199, loss: 0.0011, label: 0, bag_size: 12148\n",
      "batch 219, loss: 0.9700, label: 1, bag_size: 7748\n",
      "batch 239, loss: 0.0754, label: 1, bag_size: 2522\n",
      "batch 259, loss: 0.0424, label: 1, bag_size: 25695\n",
      "batch 279, loss: 0.1121, label: 1, bag_size: 1920\n",
      "batch 299, loss: 0.0029, label: 0, bag_size: 2628\n",
      "batch 319, loss: 0.0022, label: 0, bag_size: 14266\n",
      "batch 339, loss: 0.1243, label: 1, bag_size: 3656\n",
      "batch 359, loss: 0.0064, label: 0, bag_size: 12217\n",
      "batch 379, loss: 0.0046, label: 1, bag_size: 4128\n",
      "batch 399, loss: 0.2314, label: 1, bag_size: 2785\n",
      "batch 419, loss: 0.0034, label: 1, bag_size: 15093\n",
      "batch 439, loss: 0.0036, label: 1, bag_size: 1255\n",
      "batch 459, loss: 0.0056, label: 1, bag_size: 1022\n",
      "batch 479, loss: 0.0093, label: 0, bag_size: 2104\n",
      "batch 499, loss: 0.0037, label: 1, bag_size: 15609\n",
      "batch 519, loss: 0.0606, label: 0, bag_size: 2732\n",
      "batch 539, loss: 0.0029, label: 1, bag_size: 6090\n",
      "batch 559, loss: 0.0065, label: 1, bag_size: 6769\n",
      "batch 579, loss: 0.0103, label: 0, bag_size: 9866\n",
      "batch 599, loss: 0.5969, label: 1, bag_size: 7989\n",
      "batch 619, loss: 0.0007, label: 1, bag_size: 18649\n",
      "batch 639, loss: 0.1507, label: 1, bag_size: 6825\n",
      "batch 659, loss: 0.0625, label: 0, bag_size: 15841\n",
      "batch 679, loss: 1.5115, label: 1, bag_size: 21252\n",
      "batch 699, loss: 0.0004, label: 1, bag_size: 4394\n",
      "batch 719, loss: 0.0002, label: 1, bag_size: 11387\n",
      "batch 739, loss: 0.0370, label: 1, bag_size: 1875\n",
      "batch 759, loss: 0.0063, label: 0, bag_size: 10942\n",
      "batch 779, loss: 0.1845, label: 1, bag_size: 16154\n",
      "batch 799, loss: 0.1530, label: 0, bag_size: 1684\n",
      "batch 819, loss: 0.2063, label: 0, bag_size: 3238\n",
      "Epoch: 27, train_loss: 0.1746, train_error: 0.0681\n",
      "class 0: acc 0.9394673123486683, correct 388/413\n",
      "class 1: acc 0.9242053789731052, correct 378/409\n",
      "\n",
      "Val Set, val_loss: 0.3019, val_error: 0.1101, auc: 0.9586\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.8412698412698413, correct 53/63\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0005, label: 1, bag_size: 3409\n",
      "batch 39, loss: 0.0035, label: 1, bag_size: 21827\n",
      "batch 59, loss: 0.3436, label: 0, bag_size: 2968\n",
      "batch 79, loss: 0.0113, label: 1, bag_size: 2405\n",
      "batch 99, loss: 0.0257, label: 0, bag_size: 19067\n",
      "batch 119, loss: 0.0268, label: 0, bag_size: 2760\n",
      "batch 139, loss: 0.0147, label: 1, bag_size: 4956\n",
      "batch 159, loss: 0.1184, label: 0, bag_size: 1592\n",
      "batch 179, loss: 0.0209, label: 0, bag_size: 14681\n",
      "batch 199, loss: 0.0134, label: 0, bag_size: 12217\n",
      "batch 219, loss: 0.3706, label: 0, bag_size: 4997\n",
      "batch 239, loss: 0.0009, label: 1, bag_size: 4317\n",
      "batch 259, loss: 0.3548, label: 0, bag_size: 9616\n",
      "batch 279, loss: 0.0929, label: 1, bag_size: 549\n",
      "batch 299, loss: 0.0598, label: 1, bag_size: 4789\n",
      "batch 319, loss: 0.0000, label: 1, bag_size: 4862\n",
      "batch 339, loss: 0.0267, label: 1, bag_size: 9561\n",
      "batch 359, loss: 0.0149, label: 0, bag_size: 2044\n",
      "batch 379, loss: 0.0010, label: 1, bag_size: 4259\n",
      "batch 399, loss: 0.1438, label: 1, bag_size: 12178\n",
      "batch 419, loss: 0.7460, label: 0, bag_size: 30828\n",
      "batch 439, loss: 0.8635, label: 0, bag_size: 5105\n",
      "batch 459, loss: 0.0441, label: 0, bag_size: 22800\n",
      "batch 479, loss: 0.3874, label: 0, bag_size: 11212\n",
      "batch 499, loss: 0.0324, label: 1, bag_size: 15125\n",
      "batch 519, loss: 0.0411, label: 0, bag_size: 2266\n",
      "batch 539, loss: 1.1275, label: 0, bag_size: 10410\n",
      "batch 559, loss: 0.1656, label: 1, bag_size: 1609\n",
      "batch 579, loss: 0.0028, label: 1, bag_size: 2579\n",
      "batch 599, loss: 0.1525, label: 0, bag_size: 7381\n",
      "batch 619, loss: 0.0693, label: 1, bag_size: 12178\n",
      "batch 639, loss: 0.0062, label: 0, bag_size: 22828\n",
      "batch 659, loss: 0.0001, label: 1, bag_size: 15716\n",
      "batch 679, loss: 0.0039, label: 1, bag_size: 5763\n",
      "batch 699, loss: 0.0190, label: 1, bag_size: 9955\n",
      "batch 719, loss: 0.0024, label: 0, bag_size: 10898\n",
      "batch 739, loss: 0.0024, label: 0, bag_size: 20666\n",
      "batch 759, loss: 0.0006, label: 1, bag_size: 4394\n",
      "batch 779, loss: 0.0009, label: 1, bag_size: 5690\n",
      "batch 799, loss: 0.0994, label: 0, bag_size: 2360\n",
      "batch 819, loss: 0.0013, label: 1, bag_size: 8522\n",
      "Epoch: 28, train_loss: 0.2088, train_error: 0.0766\n",
      "class 0: acc 0.9164420485175202, correct 340/371\n",
      "class 1: acc 0.9290465631929047, correct 419/451\n",
      "\n",
      "Val Set, val_loss: 0.3569, val_error: 0.1284, auc: 0.9579\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.7936507936507936, correct 50/63\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0487, label: 1, bag_size: 2678\n",
      "batch 39, loss: 0.0013, label: 1, bag_size: 11875\n",
      "batch 59, loss: 0.5525, label: 1, bag_size: 1497\n",
      "batch 79, loss: 0.0617, label: 0, bag_size: 3399\n",
      "batch 99, loss: 0.0332, label: 1, bag_size: 10969\n",
      "batch 119, loss: 0.0005, label: 0, bag_size: 11690\n",
      "batch 139, loss: 0.0224, label: 0, bag_size: 15898\n",
      "batch 159, loss: 0.0099, label: 1, bag_size: 2495\n",
      "batch 179, loss: 0.0291, label: 0, bag_size: 2732\n",
      "batch 199, loss: 0.1234, label: 1, bag_size: 1437\n",
      "batch 219, loss: 0.0144, label: 1, bag_size: 5441\n",
      "batch 239, loss: 0.0080, label: 1, bag_size: 4715\n",
      "batch 259, loss: 0.0021, label: 1, bag_size: 3437\n",
      "batch 279, loss: 0.0147, label: 1, bag_size: 2790\n",
      "batch 299, loss: 0.0009, label: 1, bag_size: 6343\n",
      "batch 319, loss: 0.0361, label: 0, bag_size: 7637\n",
      "batch 339, loss: 0.0004, label: 0, bag_size: 23037\n",
      "batch 359, loss: 0.7069, label: 0, bag_size: 13023\n",
      "batch 379, loss: 0.0008, label: 0, bag_size: 8372\n",
      "batch 399, loss: 0.0204, label: 1, bag_size: 16565\n",
      "batch 419, loss: 0.0024, label: 0, bag_size: 23398\n",
      "batch 439, loss: 0.2410, label: 1, bag_size: 1867\n",
      "batch 459, loss: 0.1193, label: 0, bag_size: 1800\n",
      "batch 479, loss: 0.1186, label: 1, bag_size: 7066\n",
      "batch 499, loss: 0.0281, label: 1, bag_size: 12425\n",
      "batch 519, loss: 0.0051, label: 0, bag_size: 26271\n",
      "batch 539, loss: 0.3095, label: 1, bag_size: 6682\n",
      "batch 559, loss: 0.1085, label: 0, bag_size: 2732\n",
      "batch 579, loss: 0.0039, label: 1, bag_size: 12095\n",
      "batch 599, loss: 0.0058, label: 0, bag_size: 11512\n",
      "batch 619, loss: 1.0879, label: 1, bag_size: 5903\n",
      "batch 639, loss: 0.3030, label: 1, bag_size: 7389\n",
      "batch 659, loss: 0.1897, label: 0, bag_size: 7557\n",
      "batch 679, loss: 0.0205, label: 1, bag_size: 3453\n",
      "batch 699, loss: 1.0138, label: 0, bag_size: 2815\n",
      "batch 719, loss: 0.2861, label: 1, bag_size: 1683\n",
      "batch 739, loss: 0.0011, label: 0, bag_size: 12687\n",
      "batch 759, loss: 0.1699, label: 1, bag_size: 13732\n",
      "batch 779, loss: 0.0047, label: 0, bag_size: 14828\n",
      "batch 799, loss: 0.0021, label: 0, bag_size: 1881\n",
      "batch 819, loss: 0.0003, label: 1, bag_size: 4394\n",
      "Epoch: 29, train_loss: 0.1760, train_error: 0.0693\n",
      "class 0: acc 0.9336609336609336, correct 380/407\n",
      "class 1: acc 0.927710843373494, correct 385/415\n",
      "\n",
      "Val Set, val_loss: 0.2688, val_error: 0.1101, auc: 0.9607\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3740, label: 0, bag_size: 9597\n",
      "batch 39, loss: 0.0012, label: 1, bag_size: 10394\n",
      "batch 59, loss: 1.2355, label: 0, bag_size: 3876\n",
      "batch 79, loss: 0.0541, label: 0, bag_size: 3725\n",
      "batch 99, loss: 0.0370, label: 1, bag_size: 9519\n",
      "batch 119, loss: 0.3774, label: 1, bag_size: 1444\n",
      "batch 139, loss: 0.0012, label: 1, bag_size: 11122\n",
      "batch 159, loss: 0.0045, label: 1, bag_size: 7798\n",
      "batch 179, loss: 0.0042, label: 0, bag_size: 12593\n",
      "batch 199, loss: 0.0669, label: 0, bag_size: 24911\n",
      "batch 219, loss: 0.8645, label: 1, bag_size: 2935\n",
      "batch 239, loss: 0.0495, label: 0, bag_size: 15841\n",
      "batch 259, loss: 0.0023, label: 1, bag_size: 19832\n",
      "batch 279, loss: 0.0063, label: 0, bag_size: 13205\n",
      "batch 299, loss: 0.1730, label: 0, bag_size: 1701\n",
      "batch 319, loss: 0.0146, label: 0, bag_size: 2004\n",
      "batch 339, loss: 0.0053, label: 0, bag_size: 1127\n",
      "batch 359, loss: 0.0005, label: 1, bag_size: 19932\n",
      "batch 379, loss: 0.0087, label: 1, bag_size: 12575\n",
      "batch 399, loss: 0.0295, label: 0, bag_size: 14893\n",
      "batch 419, loss: 0.0593, label: 1, bag_size: 1888\n",
      "batch 439, loss: 0.0023, label: 0, bag_size: 3265\n",
      "batch 459, loss: 0.1597, label: 0, bag_size: 10063\n",
      "batch 479, loss: 0.0188, label: 0, bag_size: 31085\n",
      "batch 499, loss: 0.0009, label: 1, bag_size: 20161\n",
      "batch 519, loss: 0.0586, label: 0, bag_size: 11194\n",
      "batch 539, loss: 0.0027, label: 0, bag_size: 21682\n",
      "batch 559, loss: 0.1416, label: 1, bag_size: 4786\n",
      "batch 579, loss: 5.9929, label: 1, bag_size: 3121\n",
      "batch 599, loss: 0.0263, label: 0, bag_size: 21093\n",
      "batch 619, loss: 0.5629, label: 0, bag_size: 18738\n",
      "batch 639, loss: 0.0083, label: 1, bag_size: 3004\n",
      "batch 659, loss: 0.0393, label: 0, bag_size: 2873\n",
      "batch 679, loss: 0.0066, label: 1, bag_size: 2381\n",
      "batch 699, loss: 0.0107, label: 0, bag_size: 2748\n",
      "batch 719, loss: 0.0671, label: 1, bag_size: 2682\n",
      "batch 739, loss: 0.0787, label: 0, bag_size: 12840\n",
      "batch 759, loss: 1.3329, label: 1, bag_size: 2395\n",
      "batch 779, loss: 0.0001, label: 1, bag_size: 9065\n",
      "batch 799, loss: 1.5671, label: 1, bag_size: 1230\n",
      "batch 819, loss: 0.0146, label: 1, bag_size: 6927\n",
      "Epoch: 30, train_loss: 0.1933, train_error: 0.0693\n",
      "class 0: acc 0.9392405063291139, correct 371/395\n",
      "class 1: acc 0.9227166276346604, correct 394/427\n",
      "\n",
      "Val Set, val_loss: 0.2711, val_error: 0.1101, auc: 0.9579\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0025, label: 1, bag_size: 11220\n",
      "batch 39, loss: 0.0138, label: 1, bag_size: 7932\n",
      "batch 59, loss: 0.0597, label: 0, bag_size: 1651\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python main.py --drop_out --early_stopping --lr 2e-4 --k 5 --label_frac 1\\\n",
    "--exp_code cptac_lung_100_level0_mil_adam --weighted_sample --bag_loss ce --inst_loss svm \\\n",
    "--task task_2_tumor_subtyping --model_type mil --log_data --data_root_dir /home/sci/Disk2/CPTAC-LUNG/FEATURES_level0 \\\n",
    "--split_dir /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/cptac_lung_100 --subtyping \\\n",
    "--csv_path dataset_csv/cptac_lung_subtyping.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransformerMIL实验结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load Dataset\n",
      "label column: label\n",
      "label dictionary: {'LUAD': 0, 'LUSC': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 0    507\n",
      "1    520\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 164\n",
      "Slide-LVL; Number of samples registered in class 0: 507\n",
      "Patient-LVL; Number of samples registered in class 1: 157\n",
      "Slide-LVL; Number of samples registered in class 1: 520\n",
      "split_dir:  /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/cptac_lung_100\n",
      "################# Settings ###################\n",
      "num_splits:  5\n",
      "k_start:  -1\n",
      "k_end:  -1\n",
      "task:  task_2_tumor_subtyping\n",
      "max_epochs:  200\n",
      "results_dir:  ./results\n",
      "lr:  0.0002\n",
      "experiment:  cptac_lung_100_level0_transformer_adam\n",
      "reg:  1e-05\n",
      "label_frac:  1.0\n",
      "bag_loss:  ce\n",
      "seed:  1\n",
      "model_type:  transmil\n",
      "model_size:  small\n",
      "use_drop_out:  True\n",
      "weighted_sample:  True\n",
      "opt:  adam\n",
      "bag_weight:  0.7\n",
      "inst_loss:  svm\n",
      "B:  8\n",
      "split_dir:  /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/cptac_lung_100\n",
      "\n",
      "Training Fold 0!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 820 samples\n",
      "Validating on 110 samples\n",
      "Testing on 97 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Setting tau to 1.0\n",
      "Done!\n",
      "TransformerMIL_SB(\n",
      "  (instance_loss_fn): SmoothTop1SVM()\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (transformer): TransformerEncoder_PerformerAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): SelfAttention(\n",
      "            (fast_attention): FastAttention(\n",
      "              (kernel_fn): ReLU()\n",
      "            )\n",
      "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attention): Attn_Net_Gated(\n",
      "    (attention_a): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_b): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (attention_V2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (attention_U2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (attention_weights2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Total number of parameters: 8406537\n",
      "Total number of trainable parameters: 8406537\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 12.8556, instance_loss: 1.6393, weighted_loss: 9.4907, label: 0, bag_size: 21138\n",
      "batch 39, loss: 1.6271, instance_loss: 3.1113, weighted_loss: 2.0724, label: 1, bag_size: 4039\n",
      "batch 59, loss: 0.1468, instance_loss: 1.7191, weighted_loss: 0.6185, label: 0, bag_size: 11281\n",
      "batch 79, loss: 1.2280, instance_loss: 1.1473, weighted_loss: 1.2038, label: 1, bag_size: 6745\n",
      "batch 99, loss: 2.3620, instance_loss: 0.9209, weighted_loss: 1.9297, label: 1, bag_size: 5025\n",
      "batch 119, loss: 2.2441, instance_loss: 0.7176, weighted_loss: 1.7861, label: 1, bag_size: 11389\n",
      "batch 139, loss: 0.1107, instance_loss: 0.6791, weighted_loss: 0.2812, label: 0, bag_size: 2609\n",
      "batch 159, loss: 2.4096, instance_loss: 0.7044, weighted_loss: 1.8980, label: 0, bag_size: 13619\n",
      "batch 179, loss: 3.6146, instance_loss: 1.1896, weighted_loss: 2.8871, label: 1, bag_size: 15563\n",
      "batch 199, loss: 0.4196, instance_loss: 1.0541, weighted_loss: 0.6099, label: 0, bag_size: 16087\n",
      "batch 219, loss: 5.5958, instance_loss: 1.6074, weighted_loss: 4.3993, label: 0, bag_size: 763\n",
      "batch 239, loss: 0.8047, instance_loss: 0.9872, weighted_loss: 0.8594, label: 0, bag_size: 12840\n",
      "batch 259, loss: 0.0028, instance_loss: 0.6637, weighted_loss: 0.2011, label: 0, bag_size: 3657\n",
      "batch 279, loss: 0.1030, instance_loss: 1.0209, weighted_loss: 0.3784, label: 0, bag_size: 8948\n",
      "batch 299, loss: 2.6045, instance_loss: 2.0158, weighted_loss: 2.4279, label: 0, bag_size: 22498\n",
      "batch 319, loss: 0.1709, instance_loss: 1.2870, weighted_loss: 0.5057, label: 1, bag_size: 3856\n",
      "batch 339, loss: 0.0774, instance_loss: 1.3309, weighted_loss: 0.4534, label: 1, bag_size: 8466\n",
      "batch 359, loss: 0.0664, instance_loss: 1.6557, weighted_loss: 0.5432, label: 1, bag_size: 4054\n",
      "batch 379, loss: 0.0983, instance_loss: 0.8201, weighted_loss: 0.3149, label: 1, bag_size: 6453\n",
      "batch 399, loss: 0.0995, instance_loss: 0.7635, weighted_loss: 0.2987, label: 0, bag_size: 26208\n",
      "batch 419, loss: 0.9718, instance_loss: 0.8194, weighted_loss: 0.9261, label: 1, bag_size: 9533\n",
      "batch 439, loss: 0.1982, instance_loss: 0.7024, weighted_loss: 0.3495, label: 1, bag_size: 12611\n",
      "batch 459, loss: 0.0285, instance_loss: 0.7456, weighted_loss: 0.2436, label: 1, bag_size: 5256\n",
      "batch 479, loss: 0.1518, instance_loss: 1.1304, weighted_loss: 0.4454, label: 0, bag_size: 23618\n",
      "batch 499, loss: 0.0716, instance_loss: 0.8999, weighted_loss: 0.3201, label: 1, bag_size: 2935\n",
      "batch 519, loss: 1.2603, instance_loss: 0.7254, weighted_loss: 1.0999, label: 0, bag_size: 21093\n",
      "batch 539, loss: 0.0719, instance_loss: 0.6957, weighted_loss: 0.2590, label: 0, bag_size: 9786\n",
      "batch 559, loss: 0.0109, instance_loss: 0.6924, weighted_loss: 0.2153, label: 0, bag_size: 12148\n",
      "batch 579, loss: 2.0382, instance_loss: 1.2904, weighted_loss: 1.8138, label: 1, bag_size: 2937\n",
      "batch 599, loss: 0.0055, instance_loss: 0.6591, weighted_loss: 0.2016, label: 0, bag_size: 22870\n",
      "batch 619, loss: 0.0051, instance_loss: 0.7145, weighted_loss: 0.2179, label: 1, bag_size: 16890\n",
      "batch 639, loss: 0.0002, instance_loss: 0.6640, weighted_loss: 0.1993, label: 1, bag_size: 7246\n",
      "batch 659, loss: 0.0057, instance_loss: 0.7146, weighted_loss: 0.2184, label: 0, bag_size: 9455\n",
      "batch 679, loss: 3.5353, instance_loss: 1.0165, weighted_loss: 2.7796, label: 0, bag_size: 12731\n",
      "batch 699, loss: 0.6830, instance_loss: 0.6787, weighted_loss: 0.6817, label: 0, bag_size: 3502\n",
      "batch 719, loss: 0.3889, instance_loss: 0.7181, weighted_loss: 0.4876, label: 0, bag_size: 11259\n",
      "batch 739, loss: 5.9555, instance_loss: 1.3302, weighted_loss: 4.5679, label: 1, bag_size: 3856\n",
      "batch 759, loss: 0.1525, instance_loss: 1.1564, weighted_loss: 0.4537, label: 1, bag_size: 1764\n",
      "batch 779, loss: 5.8810, instance_loss: 1.9159, weighted_loss: 4.6914, label: 0, bag_size: 6281\n",
      "batch 799, loss: 5.3985, instance_loss: 1.3697, weighted_loss: 4.1899, label: 1, bag_size: 12178\n",
      "batch 819, loss: 0.2648, instance_loss: 0.8345, weighted_loss: 0.4357, label: 1, bag_size: 11729\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.879420731707317: correct 11538/13120\n",
      "class 1 clustering acc 0.2013719512195122: correct 1321/6560\n",
      "Epoch: 0, train_loss: 0.9830, train_clustering_loss:  1.0481, train_error: 0.3512\n",
      "class 0: acc 0.6339066339066339, correct 258/407\n",
      "class 1: acc 0.6634382566585957, correct 274/413\n",
      "\n",
      "Val Set, val_loss: 0.3988, val_error: 0.2000, auc: 0.9486\n",
      "class 0 clustering acc 0.9477272727272728: correct 1668/1760\n",
      "class 1 clustering acc 0.014772727272727272: correct 13/880\n",
      "class 0: acc 0.5961538461538461, correct 31/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "Validation loss decreased (inf --> 0.398787).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0125, instance_loss: 0.8617, weighted_loss: 0.2672, label: 0, bag_size: 18045\n",
      "batch 39, loss: 0.5818, instance_loss: 1.4505, weighted_loss: 0.8424, label: 1, bag_size: 12180\n",
      "batch 59, loss: 0.0025, instance_loss: 0.7226, weighted_loss: 0.2185, label: 1, bag_size: 689\n",
      "batch 79, loss: 0.0003, instance_loss: 0.7072, weighted_loss: 0.2124, label: 1, bag_size: 25970\n",
      "batch 99, loss: 1.0563, instance_loss: 1.7015, weighted_loss: 1.2499, label: 1, bag_size: 4956\n",
      "batch 119, loss: 0.0009, instance_loss: 1.0014, weighted_loss: 0.3011, label: 0, bag_size: 9234\n",
      "batch 139, loss: 0.0967, instance_loss: 0.6367, weighted_loss: 0.2587, label: 1, bag_size: 25970\n",
      "batch 159, loss: 0.0588, instance_loss: 0.7394, weighted_loss: 0.2630, label: 1, bag_size: 2179\n",
      "batch 179, loss: 0.0000, instance_loss: 0.6811, weighted_loss: 0.2044, label: 1, bag_size: 6875\n",
      "batch 199, loss: 0.1487, instance_loss: 2.4665, weighted_loss: 0.8440, label: 1, bag_size: 15185\n",
      "batch 219, loss: 0.2636, instance_loss: 1.3684, weighted_loss: 0.5950, label: 0, bag_size: 4465\n",
      "batch 239, loss: 0.3959, instance_loss: 1.5892, weighted_loss: 0.7539, label: 1, bag_size: 12178\n",
      "batch 259, loss: 0.0179, instance_loss: 0.8975, weighted_loss: 0.2818, label: 1, bag_size: 5345\n",
      "batch 279, loss: 0.0187, instance_loss: 0.8041, weighted_loss: 0.2543, label: 0, bag_size: 2322\n",
      "batch 299, loss: 0.0362, instance_loss: 0.6051, weighted_loss: 0.2069, label: 0, bag_size: 9252\n",
      "batch 319, loss: 0.0413, instance_loss: 0.6263, weighted_loss: 0.2168, label: 0, bag_size: 12687\n",
      "batch 339, loss: 0.0107, instance_loss: 0.6664, weighted_loss: 0.2074, label: 1, bag_size: 10394\n",
      "batch 359, loss: 0.9876, instance_loss: 0.9397, weighted_loss: 0.9732, label: 1, bag_size: 22264\n",
      "batch 379, loss: 2.0850, instance_loss: 1.2826, weighted_loss: 1.8443, label: 0, bag_size: 65728\n",
      "batch 399, loss: 0.0145, instance_loss: 0.6158, weighted_loss: 0.1949, label: 1, bag_size: 25970\n",
      "batch 419, loss: 0.0177, instance_loss: 0.6785, weighted_loss: 0.2159, label: 1, bag_size: 865\n",
      "batch 439, loss: 0.1953, instance_loss: 0.8356, weighted_loss: 0.3874, label: 1, bag_size: 8602\n",
      "batch 459, loss: 0.0007, instance_loss: 0.6082, weighted_loss: 0.1830, label: 0, bag_size: 3190\n",
      "batch 479, loss: 1.3375, instance_loss: 0.6937, weighted_loss: 1.1444, label: 1, bag_size: 11220\n",
      "batch 499, loss: 0.0016, instance_loss: 0.6446, weighted_loss: 0.1945, label: 0, bag_size: 15077\n",
      "batch 519, loss: 0.0000, instance_loss: 1.1521, weighted_loss: 0.3456, label: 0, bag_size: 8866\n",
      "batch 539, loss: 0.3281, instance_loss: 0.7469, weighted_loss: 0.4537, label: 0, bag_size: 3810\n",
      "batch 559, loss: 0.0644, instance_loss: 1.1540, weighted_loss: 0.3913, label: 1, bag_size: 1683\n",
      "batch 579, loss: 0.0272, instance_loss: 0.6874, weighted_loss: 0.2253, label: 0, bag_size: 1789\n",
      "batch 599, loss: 0.0750, instance_loss: 1.1601, weighted_loss: 0.4005, label: 0, bag_size: 1560\n",
      "batch 619, loss: 0.1534, instance_loss: 0.6818, weighted_loss: 0.3119, label: 0, bag_size: 14305\n",
      "batch 639, loss: 0.0069, instance_loss: 0.6970, weighted_loss: 0.2140, label: 0, bag_size: 11917\n",
      "batch 659, loss: 0.5064, instance_loss: 0.9215, weighted_loss: 0.6309, label: 1, bag_size: 5690\n",
      "batch 679, loss: 0.9684, instance_loss: 1.4072, weighted_loss: 1.1000, label: 0, bag_size: 1772\n",
      "batch 699, loss: 0.1103, instance_loss: 0.7844, weighted_loss: 0.3126, label: 1, bag_size: 4259\n",
      "batch 719, loss: 0.0776, instance_loss: 0.7460, weighted_loss: 0.2781, label: 0, bag_size: 4902\n",
      "batch 739, loss: 1.1168, instance_loss: 1.3481, weighted_loss: 1.1862, label: 1, bag_size: 2395\n",
      "batch 759, loss: 0.0984, instance_loss: 1.1704, weighted_loss: 0.4200, label: 0, bag_size: 2534\n",
      "batch 779, loss: 0.1178, instance_loss: 0.8051, weighted_loss: 0.3240, label: 0, bag_size: 2179\n",
      "batch 799, loss: 0.4688, instance_loss: 1.1406, weighted_loss: 0.6704, label: 1, bag_size: 10460\n",
      "batch 819, loss: 0.0188, instance_loss: 0.6991, weighted_loss: 0.2229, label: 1, bag_size: 16417\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.8610518292682927: correct 11297/13120\n",
      "class 1 clustering acc 0.25228658536585363: correct 1655/6560\n",
      "Epoch: 1, train_loss: 0.8171, train_clustering_loss:  1.0350, train_error: 0.2707\n",
      "class 0: acc 0.6928934010152284, correct 273/394\n",
      "class 1: acc 0.7629107981220657, correct 325/426\n",
      "\n",
      "Val Set, val_loss: 0.2699, val_error: 0.1182, auc: 0.9572\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 0.8448275862068966, correct 49/58\n",
      "Validation loss decreased (0.398787 --> 0.269856).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0023, instance_loss: 0.6534, weighted_loss: 0.1976, label: 0, bag_size: 2336\n",
      "batch 39, loss: 0.4411, instance_loss: 1.2320, weighted_loss: 0.6784, label: 1, bag_size: 16514\n",
      "batch 59, loss: 0.1253, instance_loss: 0.8500, weighted_loss: 0.3427, label: 1, bag_size: 15464\n",
      "batch 79, loss: 0.0020, instance_loss: 0.8066, weighted_loss: 0.2434, label: 0, bag_size: 23398\n",
      "batch 99, loss: 0.0221, instance_loss: 0.7066, weighted_loss: 0.2274, label: 1, bag_size: 3652\n",
      "batch 119, loss: 0.0182, instance_loss: 0.6732, weighted_loss: 0.2147, label: 0, bag_size: 21319\n",
      "batch 139, loss: 0.0019, instance_loss: 0.6317, weighted_loss: 0.1909, label: 1, bag_size: 13051\n",
      "batch 159, loss: 0.1733, instance_loss: 0.6617, weighted_loss: 0.3198, label: 0, bag_size: 5225\n",
      "batch 179, loss: 0.4584, instance_loss: 0.7556, weighted_loss: 0.5476, label: 0, bag_size: 12149\n",
      "batch 199, loss: 7.7491, instance_loss: 2.5610, weighted_loss: 6.1927, label: 0, bag_size: 9132\n",
      "batch 219, loss: 0.0533, instance_loss: 0.7987, weighted_loss: 0.2769, label: 1, bag_size: 10592\n",
      "batch 239, loss: 4.7322, instance_loss: 3.3352, weighted_loss: 4.3131, label: 0, bag_size: 3468\n",
      "batch 259, loss: 0.2038, instance_loss: 0.6813, weighted_loss: 0.3471, label: 1, bag_size: 699\n",
      "batch 279, loss: 0.0697, instance_loss: 0.8619, weighted_loss: 0.3073, label: 0, bag_size: 8582\n",
      "batch 299, loss: 2.4543, instance_loss: 0.8928, weighted_loss: 1.9858, label: 0, bag_size: 3783\n",
      "batch 319, loss: 0.0015, instance_loss: 0.7001, weighted_loss: 0.2111, label: 1, bag_size: 5894\n",
      "batch 339, loss: 0.0479, instance_loss: 0.6171, weighted_loss: 0.2187, label: 0, bag_size: 12148\n",
      "batch 359, loss: 0.0335, instance_loss: 0.8364, weighted_loss: 0.2744, label: 1, bag_size: 17769\n",
      "batch 379, loss: 0.1754, instance_loss: 0.6958, weighted_loss: 0.3315, label: 1, bag_size: 2759\n",
      "batch 399, loss: 0.9604, instance_loss: 0.7488, weighted_loss: 0.8969, label: 1, bag_size: 10920\n",
      "batch 419, loss: 0.1398, instance_loss: 0.6557, weighted_loss: 0.2946, label: 1, bag_size: 10920\n",
      "batch 439, loss: 0.0324, instance_loss: 0.7293, weighted_loss: 0.2415, label: 1, bag_size: 2140\n",
      "batch 459, loss: 1.9879, instance_loss: 1.0546, weighted_loss: 1.7079, label: 0, bag_size: 5409\n",
      "batch 479, loss: 0.0056, instance_loss: 0.3426, weighted_loss: 0.1067, label: 0, bag_size: 8145\n",
      "batch 499, loss: 0.0065, instance_loss: 0.6249, weighted_loss: 0.1920, label: 1, bag_size: 7798\n",
      "batch 519, loss: 0.2481, instance_loss: 0.8140, weighted_loss: 0.4179, label: 0, bag_size: 4523\n",
      "batch 539, loss: 1.9163, instance_loss: 0.7005, weighted_loss: 1.5516, label: 1, bag_size: 9147\n",
      "batch 559, loss: 1.8246, instance_loss: 1.3278, weighted_loss: 1.6755, label: 0, bag_size: 1437\n",
      "batch 579, loss: 1.6684, instance_loss: 1.1798, weighted_loss: 1.5218, label: 0, bag_size: 1714\n",
      "batch 599, loss: 5.2492, instance_loss: 3.2408, weighted_loss: 4.6467, label: 1, bag_size: 16514\n",
      "batch 619, loss: 0.2579, instance_loss: 0.8605, weighted_loss: 0.4387, label: 1, bag_size: 4039\n",
      "batch 639, loss: 0.0942, instance_loss: 0.1258, weighted_loss: 0.1037, label: 0, bag_size: 3787\n",
      "batch 659, loss: 0.1285, instance_loss: 0.6693, weighted_loss: 0.2907, label: 0, bag_size: 3502\n",
      "batch 679, loss: 0.0408, instance_loss: 0.0873, weighted_loss: 0.0548, label: 0, bag_size: 5999\n",
      "batch 699, loss: 0.0356, instance_loss: 0.6292, weighted_loss: 0.2137, label: 1, bag_size: 13174\n",
      "batch 719, loss: 0.0477, instance_loss: 0.1843, weighted_loss: 0.0887, label: 0, bag_size: 11194\n",
      "batch 739, loss: 0.4646, instance_loss: 0.6609, weighted_loss: 0.5235, label: 1, bag_size: 8191\n",
      "batch 759, loss: 1.6555, instance_loss: 0.7915, weighted_loss: 1.3963, label: 1, bag_size: 689\n",
      "batch 779, loss: 1.6580, instance_loss: 0.9239, weighted_loss: 1.4377, label: 1, bag_size: 4039\n",
      "batch 799, loss: 0.0035, instance_loss: 0.0269, weighted_loss: 0.0106, label: 0, bag_size: 11900\n",
      "batch 819, loss: 0.0240, instance_loss: 0.6507, weighted_loss: 0.2120, label: 1, bag_size: 14515\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.8984756097560975: correct 11788/13120\n",
      "class 1 clustering acc 0.3358231707317073: correct 2203/6560\n",
      "Epoch: 2, train_loss: 0.5101, train_clustering_loss:  0.8025, train_error: 0.2000\n",
      "class 0: acc 0.7964824120603015, correct 317/398\n",
      "class 1: acc 0.8033175355450237, correct 339/422\n",
      "\n",
      "Val Set, val_loss: 0.3025, val_error: 0.1182, auc: 0.9489\n",
      "class 0 clustering acc 0.9875: correct 1738/1760\n",
      "class 1 clustering acc 0.14772727272727273: correct 130/880\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0076, instance_loss: 0.0184, weighted_loss: 0.0109, label: 0, bag_size: 18415\n",
      "batch 39, loss: 0.0226, instance_loss: 0.3808, weighted_loss: 0.1301, label: 0, bag_size: 4598\n",
      "batch 59, loss: 0.4051, instance_loss: 0.7509, weighted_loss: 0.5089, label: 1, bag_size: 1095\n",
      "batch 79, loss: 0.0212, instance_loss: 0.5689, weighted_loss: 0.1855, label: 1, bag_size: 2356\n",
      "batch 99, loss: 0.0291, instance_loss: 0.6264, weighted_loss: 0.2083, label: 1, bag_size: 5731\n",
      "batch 119, loss: 0.1789, instance_loss: 0.1173, weighted_loss: 0.1605, label: 0, bag_size: 8549\n",
      "batch 139, loss: 0.0011, instance_loss: 0.6722, weighted_loss: 0.2024, label: 1, bag_size: 17769\n",
      "batch 159, loss: 0.6044, instance_loss: 0.4166, weighted_loss: 0.5480, label: 0, bag_size: 15464\n",
      "batch 179, loss: 0.9157, instance_loss: 0.9301, weighted_loss: 0.9200, label: 0, bag_size: 9069\n",
      "batch 199, loss: 0.2122, instance_loss: 0.1785, weighted_loss: 0.2021, label: 0, bag_size: 11512\n",
      "batch 219, loss: 0.0078, instance_loss: 0.0996, weighted_loss: 0.0353, label: 0, bag_size: 16936\n",
      "batch 239, loss: 3.9191, instance_loss: 2.1367, weighted_loss: 3.3844, label: 1, bag_size: 13089\n",
      "batch 259, loss: 0.0011, instance_loss: 0.0447, weighted_loss: 0.0142, label: 0, bag_size: 18415\n",
      "batch 279, loss: 0.0852, instance_loss: 0.5115, weighted_loss: 0.2131, label: 0, bag_size: 1690\n",
      "batch 299, loss: 0.0054, instance_loss: 0.0271, weighted_loss: 0.0119, label: 0, bag_size: 17791\n",
      "batch 319, loss: 0.0042, instance_loss: 0.7525, weighted_loss: 0.2287, label: 1, bag_size: 6731\n",
      "batch 339, loss: 0.0009, instance_loss: 0.0189, weighted_loss: 0.0063, label: 0, bag_size: 10898\n",
      "batch 359, loss: 0.1318, instance_loss: 0.8178, weighted_loss: 0.3376, label: 1, bag_size: 7468\n",
      "batch 379, loss: 0.0663, instance_loss: 0.8730, weighted_loss: 0.3083, label: 1, bag_size: 18649\n",
      "batch 399, loss: 0.0035, instance_loss: 0.6235, weighted_loss: 0.1895, label: 1, bag_size: 1525\n",
      "batch 419, loss: 0.0005, instance_loss: 0.0477, weighted_loss: 0.0147, label: 0, bag_size: 19466\n",
      "batch 439, loss: 0.0177, instance_loss: 0.0199, weighted_loss: 0.0183, label: 0, bag_size: 6624\n",
      "batch 459, loss: 0.0203, instance_loss: 0.5501, weighted_loss: 0.1792, label: 1, bag_size: 10622\n",
      "batch 479, loss: 4.5646, instance_loss: 3.5684, weighted_loss: 4.2658, label: 1, bag_size: 898\n",
      "batch 499, loss: 0.0175, instance_loss: 0.0794, weighted_loss: 0.0361, label: 0, bag_size: 10068\n",
      "batch 519, loss: 0.2172, instance_loss: 0.5264, weighted_loss: 0.3099, label: 0, bag_size: 3502\n",
      "batch 539, loss: 0.0320, instance_loss: 0.4013, weighted_loss: 0.1428, label: 0, bag_size: 1506\n",
      "batch 559, loss: 0.8709, instance_loss: 1.1261, weighted_loss: 0.9475, label: 0, bag_size: 9387\n",
      "batch 579, loss: 0.3964, instance_loss: 1.3908, weighted_loss: 0.6947, label: 1, bag_size: 4929\n",
      "batch 599, loss: 0.0158, instance_loss: 0.7055, weighted_loss: 0.2227, label: 1, bag_size: 6090\n",
      "batch 619, loss: 0.0126, instance_loss: 0.6022, weighted_loss: 0.1895, label: 1, bag_size: 10028\n",
      "batch 639, loss: 0.1730, instance_loss: 1.1049, weighted_loss: 0.4526, label: 1, bag_size: 6734\n",
      "batch 659, loss: 0.3535, instance_loss: 1.1359, weighted_loss: 0.5882, label: 1, bag_size: 865\n",
      "batch 679, loss: 0.0180, instance_loss: 0.2636, weighted_loss: 0.0917, label: 0, bag_size: 19390\n",
      "batch 699, loss: 0.0024, instance_loss: 0.0359, weighted_loss: 0.0124, label: 0, bag_size: 2179\n",
      "batch 719, loss: 2.3077, instance_loss: 2.0434, weighted_loss: 2.2284, label: 0, bag_size: 1701\n",
      "batch 739, loss: 0.0724, instance_loss: 0.1400, weighted_loss: 0.0927, label: 0, bag_size: 16211\n",
      "batch 759, loss: 0.0260, instance_loss: 0.6572, weighted_loss: 0.2154, label: 1, bag_size: 12575\n",
      "batch 779, loss: 0.0080, instance_loss: 0.6931, weighted_loss: 0.2135, label: 1, bag_size: 617\n",
      "batch 799, loss: 1.1348, instance_loss: 0.9465, weighted_loss: 1.0783, label: 1, bag_size: 2785\n",
      "batch 819, loss: 1.6735, instance_loss: 0.9106, weighted_loss: 1.4446, label: 0, bag_size: 1714\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9310213414634146: correct 12215/13120\n",
      "class 1 clustering acc 0.469359756097561: correct 3079/6560\n",
      "Epoch: 3, train_loss: 0.4752, train_clustering_loss:  0.6956, train_error: 0.1866\n",
      "class 0: acc 0.8091787439613527, correct 335/414\n",
      "class 1: acc 0.8177339901477833, correct 332/406\n",
      "\n",
      "Val Set, val_loss: 0.6049, val_error: 0.3000, auc: 0.9542\n",
      "class 0 clustering acc 0.8306818181818182: correct 1462/1760\n",
      "class 1 clustering acc 0.38636363636363635: correct 340/880\n",
      "class 0: acc 0.36538461538461536, correct 19/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0239, instance_loss: 0.2329, weighted_loss: 0.0866, label: 0, bag_size: 23996\n",
      "batch 39, loss: 1.3129, instance_loss: 1.8650, weighted_loss: 1.4786, label: 0, bag_size: 1800\n",
      "batch 59, loss: 0.1516, instance_loss: 1.1921, weighted_loss: 0.4638, label: 1, bag_size: 5366\n",
      "batch 79, loss: 4.7066, instance_loss: 3.8323, weighted_loss: 4.4443, label: 1, bag_size: 16514\n",
      "batch 99, loss: 0.0620, instance_loss: 0.4577, weighted_loss: 0.1807, label: 0, bag_size: 2918\n",
      "batch 119, loss: 0.4215, instance_loss: 0.5059, weighted_loss: 0.4468, label: 0, bag_size: 2367\n",
      "batch 139, loss: 0.0929, instance_loss: 0.5053, weighted_loss: 0.2166, label: 1, bag_size: 4862\n",
      "batch 159, loss: 0.0090, instance_loss: 0.0260, weighted_loss: 0.0141, label: 0, bag_size: 5225\n",
      "batch 179, loss: 0.2202, instance_loss: 0.3618, weighted_loss: 0.2627, label: 0, bag_size: 11194\n",
      "batch 199, loss: 0.0002, instance_loss: 0.5426, weighted_loss: 0.1630, label: 1, bag_size: 4250\n",
      "batch 219, loss: 0.4354, instance_loss: 1.3046, weighted_loss: 0.6961, label: 0, bag_size: 3160\n",
      "batch 239, loss: 1.2135, instance_loss: 0.3108, weighted_loss: 0.9427, label: 0, bag_size: 2609\n",
      "batch 259, loss: 0.0116, instance_loss: 0.5880, weighted_loss: 0.1845, label: 1, bag_size: 4039\n",
      "batch 279, loss: 0.2074, instance_loss: 0.4782, weighted_loss: 0.2886, label: 0, bag_size: 1213\n",
      "batch 299, loss: 0.0145, instance_loss: 0.5971, weighted_loss: 0.1893, label: 1, bag_size: 3409\n",
      "batch 319, loss: 0.8873, instance_loss: 0.6417, weighted_loss: 0.8136, label: 0, bag_size: 1370\n",
      "batch 339, loss: 0.3882, instance_loss: 1.5246, weighted_loss: 0.7291, label: 0, bag_size: 14249\n",
      "batch 359, loss: 0.0165, instance_loss: 0.5030, weighted_loss: 0.1625, label: 0, bag_size: 9786\n",
      "batch 379, loss: 0.0795, instance_loss: 0.4519, weighted_loss: 0.1912, label: 0, bag_size: 11735\n",
      "batch 399, loss: 0.2209, instance_loss: 0.5716, weighted_loss: 0.3261, label: 1, bag_size: 11964\n",
      "batch 419, loss: 0.0112, instance_loss: 0.5697, weighted_loss: 0.1787, label: 1, bag_size: 16565\n",
      "batch 439, loss: 0.0306, instance_loss: 0.4707, weighted_loss: 0.1626, label: 1, bag_size: 12719\n",
      "batch 459, loss: 0.1757, instance_loss: 0.5943, weighted_loss: 0.3013, label: 1, bag_size: 2179\n",
      "batch 479, loss: 0.0320, instance_loss: 0.3640, weighted_loss: 0.1316, label: 0, bag_size: 2070\n",
      "batch 499, loss: 0.0241, instance_loss: 0.0663, weighted_loss: 0.0367, label: 0, bag_size: 23996\n",
      "batch 519, loss: 0.0020, instance_loss: 0.0062, weighted_loss: 0.0032, label: 0, bag_size: 15077\n",
      "batch 539, loss: 0.0660, instance_loss: 0.0375, weighted_loss: 0.0575, label: 0, bag_size: 15003\n",
      "batch 559, loss: 0.0262, instance_loss: 0.7508, weighted_loss: 0.2436, label: 1, bag_size: 4423\n",
      "batch 579, loss: 1.3572, instance_loss: 1.7994, weighted_loss: 1.4899, label: 1, bag_size: 1123\n",
      "batch 599, loss: 0.0043, instance_loss: 0.0769, weighted_loss: 0.0261, label: 0, bag_size: 10481\n",
      "batch 619, loss: 0.0751, instance_loss: 0.1176, weighted_loss: 0.0878, label: 0, bag_size: 2036\n",
      "batch 639, loss: 0.0446, instance_loss: 0.4198, weighted_loss: 0.1572, label: 1, bag_size: 7613\n",
      "batch 659, loss: 0.0177, instance_loss: 0.4734, weighted_loss: 0.1544, label: 0, bag_size: 1881\n",
      "batch 679, loss: 0.0513, instance_loss: 0.6624, weighted_loss: 0.2346, label: 0, bag_size: 1213\n",
      "batch 699, loss: 0.0859, instance_loss: 0.5344, weighted_loss: 0.2204, label: 0, bag_size: 6624\n",
      "batch 719, loss: 0.7281, instance_loss: 1.0667, weighted_loss: 0.8296, label: 1, bag_size: 2937\n",
      "batch 739, loss: 0.0481, instance_loss: 0.0222, weighted_loss: 0.0403, label: 0, bag_size: 12796\n",
      "batch 759, loss: 0.0160, instance_loss: 0.1054, weighted_loss: 0.0428, label: 0, bag_size: 9234\n",
      "batch 779, loss: 0.9621, instance_loss: 0.6643, weighted_loss: 0.8728, label: 0, bag_size: 11212\n",
      "batch 799, loss: 0.2531, instance_loss: 0.7835, weighted_loss: 0.4122, label: 1, bag_size: 7989\n",
      "batch 819, loss: 0.2365, instance_loss: 0.6599, weighted_loss: 0.3635, label: 1, bag_size: 8868\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9293445121951219: correct 12193/13120\n",
      "class 1 clustering acc 0.5525914634146342: correct 3625/6560\n",
      "Epoch: 4, train_loss: 0.4191, train_clustering_loss:  0.6415, train_error: 0.1585\n",
      "class 0: acc 0.8483412322274881, correct 358/422\n",
      "class 1: acc 0.8341708542713567, correct 332/398\n",
      "\n",
      "Val Set, val_loss: 0.3916, val_error: 0.1818, auc: 0.9476\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9807692307692307, correct 51/52\n",
      "class 1: acc 0.6724137931034483, correct 39/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6840, instance_loss: 0.6188, weighted_loss: 0.6644, label: 1, bag_size: 1284\n",
      "batch 39, loss: 0.0125, instance_loss: 0.5183, weighted_loss: 0.1643, label: 1, bag_size: 14618\n",
      "batch 59, loss: 0.3416, instance_loss: 1.1976, weighted_loss: 0.5984, label: 1, bag_size: 1015\n",
      "batch 79, loss: 0.0255, instance_loss: 0.4052, weighted_loss: 0.1394, label: 1, bag_size: 8592\n",
      "batch 99, loss: 0.0123, instance_loss: 0.1121, weighted_loss: 0.0423, label: 0, bag_size: 2732\n",
      "batch 119, loss: 0.0697, instance_loss: 0.2076, weighted_loss: 0.1110, label: 0, bag_size: 2382\n",
      "batch 139, loss: 0.0015, instance_loss: 0.5703, weighted_loss: 0.1721, label: 1, bag_size: 14202\n",
      "batch 159, loss: 0.0006, instance_loss: 0.5347, weighted_loss: 0.1608, label: 1, bag_size: 9408\n",
      "batch 179, loss: 0.0087, instance_loss: 0.0048, weighted_loss: 0.0075, label: 0, bag_size: 11125\n",
      "batch 199, loss: 0.3278, instance_loss: 0.7080, weighted_loss: 0.4419, label: 1, bag_size: 11386\n",
      "batch 219, loss: 0.0677, instance_loss: 0.4825, weighted_loss: 0.1921, label: 1, bag_size: 11964\n",
      "batch 239, loss: 0.2109, instance_loss: 0.5922, weighted_loss: 0.3253, label: 1, bag_size: 1015\n",
      "batch 259, loss: 0.0449, instance_loss: 0.5577, weighted_loss: 0.1988, label: 1, bag_size: 3651\n",
      "batch 279, loss: 0.0916, instance_loss: 0.6483, weighted_loss: 0.2587, label: 1, bag_size: 2278\n",
      "batch 299, loss: 0.0722, instance_loss: 0.1171, weighted_loss: 0.0857, label: 0, bag_size: 1884\n",
      "batch 319, loss: 0.7225, instance_loss: 0.3404, weighted_loss: 0.6079, label: 1, bag_size: 699\n",
      "batch 339, loss: 0.1174, instance_loss: 0.5545, weighted_loss: 0.2486, label: 1, bag_size: 12603\n",
      "batch 359, loss: 0.9351, instance_loss: 1.4250, weighted_loss: 1.0821, label: 1, bag_size: 4786\n",
      "batch 379, loss: 0.0318, instance_loss: 0.5506, weighted_loss: 0.1875, label: 0, bag_size: 9415\n",
      "batch 399, loss: 2.6765, instance_loss: 1.8519, weighted_loss: 2.4291, label: 0, bag_size: 1498\n",
      "batch 419, loss: 0.0466, instance_loss: 0.4172, weighted_loss: 0.1578, label: 0, bag_size: 8145\n",
      "batch 439, loss: 0.0301, instance_loss: 0.2976, weighted_loss: 0.1104, label: 0, bag_size: 1072\n",
      "batch 459, loss: 0.3401, instance_loss: 0.5038, weighted_loss: 0.3892, label: 0, bag_size: 8788\n",
      "batch 479, loss: 1.0704, instance_loss: 1.4358, weighted_loss: 1.1800, label: 1, bag_size: 1867\n",
      "batch 499, loss: 0.0163, instance_loss: 0.2135, weighted_loss: 0.0755, label: 0, bag_size: 3725\n",
      "batch 519, loss: 0.0068, instance_loss: 0.0120, weighted_loss: 0.0084, label: 0, bag_size: 11900\n",
      "batch 539, loss: 0.0122, instance_loss: 0.4968, weighted_loss: 0.1576, label: 1, bag_size: 5864\n",
      "batch 559, loss: 0.0066, instance_loss: 0.0642, weighted_loss: 0.0239, label: 0, bag_size: 2652\n",
      "batch 579, loss: 0.0015, instance_loss: 0.0043, weighted_loss: 0.0024, label: 0, bag_size: 9252\n",
      "batch 599, loss: 0.1928, instance_loss: 0.2650, weighted_loss: 0.2144, label: 1, bag_size: 15332\n",
      "batch 619, loss: 1.0861, instance_loss: 0.8442, weighted_loss: 1.0135, label: 0, bag_size: 1416\n",
      "batch 639, loss: 0.3465, instance_loss: 1.0630, weighted_loss: 0.5614, label: 1, bag_size: 865\n",
      "batch 659, loss: 0.0002, instance_loss: 0.3926, weighted_loss: 0.1179, label: 1, bag_size: 20767\n",
      "batch 679, loss: 0.1294, instance_loss: 0.4062, weighted_loss: 0.2125, label: 0, bag_size: 1202\n",
      "batch 699, loss: 0.0311, instance_loss: 0.2584, weighted_loss: 0.0993, label: 1, bag_size: 5231\n",
      "batch 719, loss: 0.4679, instance_loss: 1.7070, weighted_loss: 0.8396, label: 1, bag_size: 10671\n",
      "batch 739, loss: 0.0181, instance_loss: 0.0762, weighted_loss: 0.0355, label: 0, bag_size: 14266\n",
      "batch 759, loss: 1.1774, instance_loss: 0.7062, weighted_loss: 1.0361, label: 0, bag_size: 1370\n",
      "batch 779, loss: 0.0261, instance_loss: 1.2362, weighted_loss: 0.3891, label: 1, bag_size: 12626\n",
      "batch 799, loss: 0.0310, instance_loss: 0.2733, weighted_loss: 0.1037, label: 0, bag_size: 8788\n",
      "batch 819, loss: 0.0111, instance_loss: 0.4114, weighted_loss: 0.1312, label: 1, bag_size: 6453\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9377286585365854: correct 12303/13120\n",
      "class 1 clustering acc 0.572560975609756: correct 3756/6560\n",
      "Epoch: 5, train_loss: 0.4004, train_clustering_loss:  0.6327, train_error: 0.1646\n",
      "class 0: acc 0.8337349397590361, correct 346/415\n",
      "class 1: acc 0.837037037037037, correct 339/405\n",
      "\n",
      "Val Set, val_loss: 0.2952, val_error: 0.1182, auc: 0.9586\n",
      "class 0 clustering acc 0.9659090909090909: correct 1700/1760\n",
      "class 1 clustering acc 0.3704545454545455: correct 326/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.8275862068965517, correct 48/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1404, instance_loss: 0.5694, weighted_loss: 0.2691, label: 1, bag_size: 12931\n",
      "batch 39, loss: 0.0563, instance_loss: 0.2726, weighted_loss: 0.1212, label: 0, bag_size: 14305\n",
      "batch 59, loss: 0.0004, instance_loss: 0.4551, weighted_loss: 0.1368, label: 1, bag_size: 2638\n",
      "batch 79, loss: 0.0223, instance_loss: 1.1668, weighted_loss: 0.3657, label: 1, bag_size: 9644\n",
      "batch 99, loss: 0.0376, instance_loss: 0.0207, weighted_loss: 0.0325, label: 0, bag_size: 9786\n",
      "batch 119, loss: 0.0036, instance_loss: 0.6052, weighted_loss: 0.1841, label: 1, bag_size: 1014\n",
      "batch 139, loss: 0.0251, instance_loss: 0.5994, weighted_loss: 0.1974, label: 1, bag_size: 2140\n",
      "batch 159, loss: 0.0070, instance_loss: 0.5044, weighted_loss: 0.1563, label: 1, bag_size: 15716\n",
      "batch 179, loss: 0.7284, instance_loss: 0.6992, weighted_loss: 0.7196, label: 1, bag_size: 3211\n",
      "batch 199, loss: 0.0016, instance_loss: 0.1506, weighted_loss: 0.0463, label: 0, bag_size: 763\n",
      "batch 219, loss: 0.0358, instance_loss: 0.4297, weighted_loss: 0.1540, label: 1, bag_size: 3453\n",
      "batch 239, loss: 0.4703, instance_loss: 0.7699, weighted_loss: 0.5602, label: 0, bag_size: 18777\n",
      "batch 259, loss: 1.2384, instance_loss: 1.4024, weighted_loss: 1.2876, label: 1, bag_size: 5903\n",
      "batch 279, loss: 0.0379, instance_loss: 0.5690, weighted_loss: 0.1972, label: 1, bag_size: 5345\n",
      "batch 299, loss: 2.4450, instance_loss: 1.2655, weighted_loss: 2.0911, label: 0, bag_size: 2351\n",
      "batch 319, loss: 0.2172, instance_loss: 0.6659, weighted_loss: 0.3518, label: 1, bag_size: 1838\n",
      "batch 339, loss: 0.0115, instance_loss: 0.4514, weighted_loss: 0.1435, label: 1, bag_size: 20767\n",
      "batch 359, loss: 0.0006, instance_loss: 0.0057, weighted_loss: 0.0021, label: 0, bag_size: 6652\n",
      "batch 379, loss: 0.1450, instance_loss: 0.1496, weighted_loss: 0.1464, label: 0, bag_size: 12149\n",
      "batch 399, loss: 0.2833, instance_loss: 0.6299, weighted_loss: 0.3873, label: 1, bag_size: 16548\n",
      "batch 419, loss: 0.1601, instance_loss: 0.4829, weighted_loss: 0.2569, label: 1, bag_size: 8103\n",
      "batch 439, loss: 0.0198, instance_loss: 0.0919, weighted_loss: 0.0414, label: 0, bag_size: 11477\n",
      "batch 459, loss: 0.0361, instance_loss: 0.1701, weighted_loss: 0.0763, label: 0, bag_size: 1712\n",
      "batch 479, loss: 0.0625, instance_loss: 0.2523, weighted_loss: 0.1195, label: 0, bag_size: 5551\n",
      "batch 499, loss: 0.0028, instance_loss: 0.4659, weighted_loss: 0.1418, label: 1, bag_size: 2278\n",
      "batch 519, loss: 0.4086, instance_loss: 0.8610, weighted_loss: 0.5443, label: 1, bag_size: 3968\n",
      "batch 539, loss: 0.1292, instance_loss: 0.4564, weighted_loss: 0.2274, label: 1, bag_size: 6734\n",
      "batch 559, loss: 0.0016, instance_loss: 0.0039, weighted_loss: 0.0023, label: 0, bag_size: 31106\n",
      "batch 579, loss: 0.0494, instance_loss: 0.3768, weighted_loss: 0.1476, label: 1, bag_size: 9689\n",
      "batch 599, loss: 0.0363, instance_loss: 0.4556, weighted_loss: 0.1621, label: 1, bag_size: 16051\n",
      "batch 619, loss: 0.0052, instance_loss: 0.4441, weighted_loss: 0.1368, label: 1, bag_size: 6090\n",
      "batch 639, loss: 1.0288, instance_loss: 1.2786, weighted_loss: 1.1037, label: 1, bag_size: 10460\n",
      "batch 659, loss: 0.2992, instance_loss: 0.9242, weighted_loss: 0.4867, label: 0, bag_size: 23618\n",
      "batch 679, loss: 0.0399, instance_loss: 0.0214, weighted_loss: 0.0343, label: 0, bag_size: 18415\n",
      "batch 699, loss: 0.0052, instance_loss: 0.4427, weighted_loss: 0.1365, label: 0, bag_size: 15313\n",
      "batch 719, loss: 0.0057, instance_loss: 0.0670, weighted_loss: 0.0241, label: 0, bag_size: 19470\n",
      "batch 739, loss: 0.3598, instance_loss: 0.3096, weighted_loss: 0.3447, label: 0, bag_size: 2732\n",
      "batch 759, loss: 0.3005, instance_loss: 0.4691, weighted_loss: 0.3511, label: 1, bag_size: 5340\n",
      "batch 779, loss: 0.0084, instance_loss: 0.0159, weighted_loss: 0.0107, label: 0, bag_size: 19470\n",
      "batch 799, loss: 0.2573, instance_loss: 0.3064, weighted_loss: 0.2720, label: 0, bag_size: 11212\n",
      "batch 819, loss: 0.0144, instance_loss: 0.2112, weighted_loss: 0.0734, label: 1, bag_size: 10969\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9483993902439024: correct 12443/13120\n",
      "class 1 clustering acc 0.626219512195122: correct 4108/6560\n",
      "Epoch: 6, train_loss: 0.3898, train_clustering_loss:  0.5665, train_error: 0.1488\n",
      "class 0: acc 0.851581508515815, correct 350/411\n",
      "class 1: acc 0.8508557457212714, correct 348/409\n",
      "\n",
      "Val Set, val_loss: 0.2629, val_error: 0.1091, auc: 0.9605\n",
      "class 0 clustering acc 0.9926136363636363: correct 1747/1760\n",
      "class 1 clustering acc 0.35454545454545455: correct 312/880\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "Validation loss decreased (0.269856 --> 0.262937).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1937, instance_loss: 0.4821, weighted_loss: 0.2802, label: 0, bag_size: 3783\n",
      "batch 39, loss: 0.0044, instance_loss: 0.0087, weighted_loss: 0.0057, label: 0, bag_size: 18045\n",
      "batch 59, loss: 0.0179, instance_loss: 0.0637, weighted_loss: 0.0317, label: 0, bag_size: 10535\n",
      "batch 79, loss: 0.0425, instance_loss: 0.1368, weighted_loss: 0.0708, label: 0, bag_size: 1684\n",
      "batch 99, loss: 0.0744, instance_loss: 0.2440, weighted_loss: 0.1253, label: 1, bag_size: 16162\n",
      "batch 119, loss: 0.4356, instance_loss: 0.8055, weighted_loss: 0.5466, label: 1, bag_size: 6478\n",
      "batch 139, loss: 0.2623, instance_loss: 0.3653, weighted_loss: 0.2932, label: 1, bag_size: 6781\n",
      "batch 159, loss: 0.0342, instance_loss: 0.3208, weighted_loss: 0.1202, label: 0, bag_size: 2160\n",
      "batch 179, loss: 1.9229, instance_loss: 1.5649, weighted_loss: 1.8155, label: 0, bag_size: 2179\n",
      "batch 199, loss: 0.0115, instance_loss: 0.4275, weighted_loss: 0.1363, label: 1, bag_size: 8466\n",
      "batch 219, loss: 0.0935, instance_loss: 0.1592, weighted_loss: 0.1132, label: 0, bag_size: 1712\n",
      "batch 239, loss: 2.7086, instance_loss: 1.3253, weighted_loss: 2.2936, label: 1, bag_size: 9162\n",
      "batch 259, loss: 0.5215, instance_loss: 1.3724, weighted_loss: 0.7768, label: 0, bag_size: 11306\n",
      "batch 279, loss: 0.1165, instance_loss: 0.4272, weighted_loss: 0.2097, label: 0, bag_size: 11212\n",
      "batch 299, loss: 0.0407, instance_loss: 0.2252, weighted_loss: 0.0960, label: 0, bag_size: 6850\n",
      "batch 319, loss: 4.2476, instance_loss: 3.7514, weighted_loss: 4.0988, label: 0, bag_size: 1637\n",
      "batch 339, loss: 0.0556, instance_loss: 0.1775, weighted_loss: 0.0922, label: 0, bag_size: 31780\n",
      "batch 359, loss: 0.2264, instance_loss: 0.4407, weighted_loss: 0.2907, label: 0, bag_size: 3657\n",
      "batch 379, loss: 0.0057, instance_loss: 0.2630, weighted_loss: 0.0829, label: 1, bag_size: 8685\n",
      "batch 399, loss: 0.2585, instance_loss: 0.7779, weighted_loss: 0.4143, label: 0, bag_size: 2043\n",
      "batch 419, loss: 0.0094, instance_loss: 0.3055, weighted_loss: 0.0982, label: 1, bag_size: 4128\n",
      "batch 439, loss: 0.0042, instance_loss: 0.0119, weighted_loss: 0.0065, label: 0, bag_size: 8252\n",
      "batch 459, loss: 0.0009, instance_loss: 0.3074, weighted_loss: 0.0928, label: 1, bag_size: 10920\n",
      "batch 479, loss: 0.0337, instance_loss: 0.2764, weighted_loss: 0.1065, label: 0, bag_size: 13795\n",
      "batch 499, loss: 0.0673, instance_loss: 0.2023, weighted_loss: 0.1078, label: 0, bag_size: 7011\n",
      "batch 519, loss: 0.0651, instance_loss: 0.1741, weighted_loss: 0.0978, label: 1, bag_size: 10033\n",
      "batch 539, loss: 0.0795, instance_loss: 0.4256, weighted_loss: 0.1833, label: 1, bag_size: 13692\n",
      "batch 559, loss: 0.0082, instance_loss: 0.1708, weighted_loss: 0.0569, label: 1, bag_size: 15233\n",
      "batch 579, loss: 0.0028, instance_loss: 0.0145, weighted_loss: 0.0063, label: 0, bag_size: 2652\n",
      "batch 599, loss: 0.0017, instance_loss: 0.0109, weighted_loss: 0.0045, label: 0, bag_size: 22681\n",
      "batch 619, loss: 1.1012, instance_loss: 1.4360, weighted_loss: 1.2017, label: 0, bag_size: 9597\n",
      "batch 639, loss: 1.5655, instance_loss: 2.2442, weighted_loss: 1.7691, label: 1, bag_size: 2681\n",
      "batch 659, loss: 0.0286, instance_loss: 0.3615, weighted_loss: 0.1285, label: 0, bag_size: 2036\n",
      "batch 679, loss: 0.0010, instance_loss: 0.2500, weighted_loss: 0.0757, label: 1, bag_size: 645\n",
      "batch 699, loss: 0.0516, instance_loss: 0.1120, weighted_loss: 0.0697, label: 0, bag_size: 2367\n",
      "batch 719, loss: 0.0379, instance_loss: 0.1481, weighted_loss: 0.0710, label: 0, bag_size: 17155\n",
      "batch 739, loss: 0.0011, instance_loss: 0.1082, weighted_loss: 0.0332, label: 1, bag_size: 7798\n",
      "batch 759, loss: 0.0118, instance_loss: 0.0748, weighted_loss: 0.0307, label: 1, bag_size: 8602\n",
      "batch 779, loss: 0.0449, instance_loss: 0.0990, weighted_loss: 0.0611, label: 0, bag_size: 8755\n",
      "batch 799, loss: 0.0889, instance_loss: 0.1594, weighted_loss: 0.1100, label: 0, bag_size: 15967\n",
      "batch 819, loss: 0.0085, instance_loss: 0.2304, weighted_loss: 0.0751, label: 0, bag_size: 10444\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.947560975609756: correct 12432/13120\n",
      "class 1 clustering acc 0.6490853658536585: correct 4258/6560\n",
      "Epoch: 7, train_loss: 0.3430, train_clustering_loss:  0.5351, train_error: 0.1451\n",
      "class 0: acc 0.8535353535353535, correct 338/396\n",
      "class 1: acc 0.8561320754716981, correct 363/424\n",
      "\n",
      "Val Set, val_loss: 0.2916, val_error: 0.1364, auc: 0.9642\n",
      "class 0 clustering acc 0.9863636363636363: correct 1736/1760\n",
      "class 1 clustering acc 0.33636363636363636: correct 296/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.7758620689655172, correct 45/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1680, instance_loss: 0.2755, weighted_loss: 0.2002, label: 1, bag_size: 18468\n",
      "batch 39, loss: 0.2666, instance_loss: 0.3696, weighted_loss: 0.2975, label: 0, bag_size: 11390\n",
      "batch 59, loss: 0.7854, instance_loss: 0.5077, weighted_loss: 0.7021, label: 1, bag_size: 22264\n",
      "batch 79, loss: 0.0285, instance_loss: 0.4202, weighted_loss: 0.1460, label: 1, bag_size: 18161\n",
      "batch 99, loss: 0.0035, instance_loss: 0.1755, weighted_loss: 0.0551, label: 0, bag_size: 16782\n",
      "batch 119, loss: 0.0148, instance_loss: 0.1111, weighted_loss: 0.0437, label: 1, bag_size: 10392\n",
      "batch 139, loss: 0.0112, instance_loss: 0.0750, weighted_loss: 0.0303, label: 1, bag_size: 21701\n",
      "batch 159, loss: 0.1545, instance_loss: 0.3454, weighted_loss: 0.2118, label: 0, bag_size: 25814\n",
      "batch 179, loss: 0.0027, instance_loss: 0.1065, weighted_loss: 0.0338, label: 0, bag_size: 14956\n",
      "batch 199, loss: 0.0302, instance_loss: 0.2246, weighted_loss: 0.0885, label: 1, bag_size: 7613\n",
      "batch 219, loss: 0.0076, instance_loss: 0.0229, weighted_loss: 0.0122, label: 0, bag_size: 21404\n",
      "batch 239, loss: 0.5198, instance_loss: 0.5954, weighted_loss: 0.5425, label: 0, bag_size: 2382\n",
      "batch 259, loss: 0.6655, instance_loss: 1.9767, weighted_loss: 1.0588, label: 0, bag_size: 17279\n",
      "batch 279, loss: 0.0051, instance_loss: 0.3566, weighted_loss: 0.1106, label: 1, bag_size: 7798\n",
      "batch 299, loss: 0.0323, instance_loss: 0.0499, weighted_loss: 0.0376, label: 1, bag_size: 15093\n",
      "batch 319, loss: 0.0481, instance_loss: 0.0548, weighted_loss: 0.0501, label: 1, bag_size: 9878\n",
      "batch 339, loss: 0.0331, instance_loss: 0.2036, weighted_loss: 0.0843, label: 0, bag_size: 11122\n",
      "batch 359, loss: 0.0733, instance_loss: 0.4218, weighted_loss: 0.1779, label: 0, bag_size: 12524\n",
      "batch 379, loss: 0.0419, instance_loss: 0.3484, weighted_loss: 0.1338, label: 1, bag_size: 6745\n",
      "batch 399, loss: 0.5501, instance_loss: 1.9297, weighted_loss: 0.9640, label: 0, bag_size: 1701\n",
      "batch 419, loss: 0.6271, instance_loss: 1.7877, weighted_loss: 0.9753, label: 1, bag_size: 11729\n",
      "batch 439, loss: 2.1179, instance_loss: 2.8192, weighted_loss: 2.3283, label: 1, bag_size: 1493\n",
      "batch 459, loss: 1.2811, instance_loss: 1.4952, weighted_loss: 1.3454, label: 1, bag_size: 2937\n",
      "batch 479, loss: 0.0377, instance_loss: 0.6495, weighted_loss: 0.2212, label: 0, bag_size: 1438\n",
      "batch 499, loss: 0.0655, instance_loss: 0.5184, weighted_loss: 0.2014, label: 0, bag_size: 22800\n",
      "batch 519, loss: 0.3436, instance_loss: 1.1377, weighted_loss: 0.5818, label: 1, bag_size: 1845\n",
      "batch 539, loss: 0.1369, instance_loss: 0.5621, weighted_loss: 0.2644, label: 1, bag_size: 4128\n",
      "batch 559, loss: 0.0024, instance_loss: 0.1415, weighted_loss: 0.0441, label: 1, bag_size: 11032\n",
      "batch 579, loss: 0.4999, instance_loss: 0.9195, weighted_loss: 0.6258, label: 1, bag_size: 4394\n",
      "batch 599, loss: 0.0733, instance_loss: 0.5500, weighted_loss: 0.2163, label: 1, bag_size: 8475\n",
      "batch 619, loss: 0.1199, instance_loss: 0.4949, weighted_loss: 0.2324, label: 1, bag_size: 15093\n",
      "batch 639, loss: 0.7748, instance_loss: 2.2185, weighted_loss: 1.2079, label: 0, bag_size: 7428\n",
      "batch 659, loss: 0.8245, instance_loss: 0.9429, weighted_loss: 0.8600, label: 1, bag_size: 4956\n",
      "batch 679, loss: 0.2021, instance_loss: 0.5995, weighted_loss: 0.3213, label: 0, bag_size: 2296\n",
      "batch 699, loss: 0.0002, instance_loss: 0.3686, weighted_loss: 0.1107, label: 0, bag_size: 10535\n",
      "batch 719, loss: 0.1727, instance_loss: 0.3614, weighted_loss: 0.2293, label: 1, bag_size: 4929\n",
      "batch 739, loss: 0.0019, instance_loss: 0.1267, weighted_loss: 0.0393, label: 1, bag_size: 9673\n",
      "batch 759, loss: 0.0681, instance_loss: 0.2878, weighted_loss: 0.1340, label: 1, bag_size: 1823\n",
      "batch 779, loss: 0.0005, instance_loss: 0.2301, weighted_loss: 0.0694, label: 1, bag_size: 13026\n",
      "batch 799, loss: 0.1442, instance_loss: 0.3077, weighted_loss: 0.1933, label: 1, bag_size: 7371\n",
      "batch 819, loss: 0.3108, instance_loss: 0.8153, weighted_loss: 0.4621, label: 1, bag_size: 4929\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9304115853658537: correct 12207/13120\n",
      "class 1 clustering acc 0.583079268292683: correct 3825/6560\n",
      "Epoch: 8, train_loss: 0.3691, train_clustering_loss:  0.6397, train_error: 0.1402\n",
      "class 0: acc 0.8542713567839196, correct 340/398\n",
      "class 1: acc 0.8649289099526066, correct 365/422\n",
      "\n",
      "Val Set, val_loss: 0.2658, val_error: 0.1273, auc: 0.9625\n",
      "class 0 clustering acc 0.9954545454545455: correct 1752/1760\n",
      "class 1 clustering acc 0.3886363636363636: correct 342/880\n",
      "class 0: acc 0.8076923076923077, correct 42/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 2.2064, instance_loss: 2.6292, weighted_loss: 2.3332, label: 1, bag_size: 16514\n",
      "batch 39, loss: 0.0010, instance_loss: 0.1816, weighted_loss: 0.0551, label: 0, bag_size: 10481\n",
      "batch 59, loss: 0.6167, instance_loss: 0.5983, weighted_loss: 0.6112, label: 0, bag_size: 1560\n",
      "batch 79, loss: 1.4023, instance_loss: 1.2627, weighted_loss: 1.3604, label: 1, bag_size: 8040\n",
      "batch 99, loss: 0.0033, instance_loss: 0.1552, weighted_loss: 0.0489, label: 0, bag_size: 13225\n",
      "batch 119, loss: 0.0604, instance_loss: 0.1443, weighted_loss: 0.0856, label: 0, bag_size: 22426\n",
      "batch 139, loss: 0.2538, instance_loss: 0.3052, weighted_loss: 0.2692, label: 1, bag_size: 12719\n",
      "batch 159, loss: 3.1402, instance_loss: 1.9987, weighted_loss: 2.7977, label: 0, bag_size: 1498\n",
      "batch 179, loss: 0.0567, instance_loss: 0.3953, weighted_loss: 0.1583, label: 1, bag_size: 11642\n",
      "batch 199, loss: 0.4431, instance_loss: 0.7109, weighted_loss: 0.5234, label: 0, bag_size: 931\n",
      "batch 219, loss: 0.1787, instance_loss: 0.3915, weighted_loss: 0.2425, label: 1, bag_size: 12178\n",
      "batch 239, loss: 0.0120, instance_loss: 0.1483, weighted_loss: 0.0529, label: 1, bag_size: 8012\n",
      "batch 259, loss: 0.0008, instance_loss: 0.1232, weighted_loss: 0.0375, label: 1, bag_size: 5612\n",
      "batch 279, loss: 0.0220, instance_loss: 0.3374, weighted_loss: 0.1166, label: 0, bag_size: 2424\n",
      "batch 299, loss: 0.0152, instance_loss: 0.1031, weighted_loss: 0.0416, label: 0, bag_size: 8866\n",
      "batch 319, loss: 0.1064, instance_loss: 0.1545, weighted_loss: 0.1208, label: 0, bag_size: 24911\n",
      "batch 339, loss: 0.4473, instance_loss: 0.9705, weighted_loss: 0.6043, label: 0, bag_size: 18215\n",
      "batch 359, loss: 0.0529, instance_loss: 0.2726, weighted_loss: 0.1188, label: 0, bag_size: 25814\n",
      "batch 379, loss: 0.7261, instance_loss: 1.3285, weighted_loss: 0.9068, label: 1, bag_size: 13089\n",
      "batch 399, loss: 8.7831, instance_loss: 3.9470, weighted_loss: 7.3322, label: 0, bag_size: 3468\n",
      "batch 419, loss: 0.0640, instance_loss: 0.4536, weighted_loss: 0.1809, label: 0, bag_size: 9455\n",
      "batch 439, loss: 0.0234, instance_loss: 0.0912, weighted_loss: 0.0438, label: 1, bag_size: 1622\n",
      "batch 459, loss: 0.0107, instance_loss: 0.0876, weighted_loss: 0.0338, label: 1, bag_size: 4880\n",
      "batch 479, loss: 0.4460, instance_loss: 0.3843, weighted_loss: 0.4275, label: 1, bag_size: 16890\n",
      "batch 499, loss: 0.0419, instance_loss: 0.2727, weighted_loss: 0.1111, label: 1, bag_size: 11421\n",
      "batch 519, loss: 0.2201, instance_loss: 0.4928, weighted_loss: 0.3019, label: 0, bag_size: 1690\n",
      "batch 539, loss: 0.0290, instance_loss: 0.1752, weighted_loss: 0.0729, label: 1, bag_size: 22286\n",
      "batch 559, loss: 0.2449, instance_loss: 0.4422, weighted_loss: 0.3041, label: 1, bag_size: 5256\n",
      "batch 579, loss: 0.0083, instance_loss: 0.1231, weighted_loss: 0.0427, label: 1, bag_size: 17769\n",
      "batch 599, loss: 0.0083, instance_loss: 0.2733, weighted_loss: 0.0878, label: 1, bag_size: 699\n",
      "batch 619, loss: 0.0052, instance_loss: 0.0901, weighted_loss: 0.0307, label: 0, bag_size: 1234\n",
      "batch 639, loss: 0.0493, instance_loss: 0.1942, weighted_loss: 0.0928, label: 1, bag_size: 6726\n",
      "batch 659, loss: 0.0125, instance_loss: 0.0797, weighted_loss: 0.0327, label: 0, bag_size: 11527\n",
      "batch 679, loss: 0.0002, instance_loss: 0.0317, weighted_loss: 0.0097, label: 0, bag_size: 1984\n",
      "batch 699, loss: 0.0008, instance_loss: 0.0063, weighted_loss: 0.0025, label: 0, bag_size: 20150\n",
      "batch 719, loss: 0.6820, instance_loss: 0.6888, weighted_loss: 0.6840, label: 0, bag_size: 10381\n",
      "batch 739, loss: 0.0926, instance_loss: 0.1416, weighted_loss: 0.1073, label: 1, bag_size: 10591\n",
      "batch 759, loss: 0.1771, instance_loss: 0.3219, weighted_loss: 0.2205, label: 1, bag_size: 7351\n",
      "batch 779, loss: 0.0346, instance_loss: 0.1714, weighted_loss: 0.0757, label: 1, bag_size: 7119\n",
      "batch 799, loss: 0.0035, instance_loss: 0.0397, weighted_loss: 0.0144, label: 0, bag_size: 24911\n",
      "batch 819, loss: 0.2353, instance_loss: 0.3340, weighted_loss: 0.2649, label: 1, bag_size: 3450\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9578506097560976: correct 12567/13120\n",
      "class 1 clustering acc 0.7077743902439024: correct 4643/6560\n",
      "Epoch: 9, train_loss: 0.3166, train_clustering_loss:  0.4635, train_error: 0.1329\n",
      "class 0: acc 0.8664921465968587, correct 331/382\n",
      "class 1: acc 0.867579908675799, correct 380/438\n",
      "\n",
      "Val Set, val_loss: 0.2426, val_error: 0.1091, auc: 0.9682\n",
      "class 0 clustering acc 0.9846590909090909: correct 1733/1760\n",
      "class 1 clustering acc 0.7079545454545455: correct 623/880\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "Validation loss decreased (0.262937 --> 0.242626).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0453, instance_loss: 0.1039, weighted_loss: 0.0629, label: 1, bag_size: 14604\n",
      "batch 39, loss: 0.0039, instance_loss: 0.0126, weighted_loss: 0.0065, label: 0, bag_size: 14249\n",
      "batch 59, loss: 0.1328, instance_loss: 0.1256, weighted_loss: 0.1306, label: 0, bag_size: 11281\n",
      "batch 79, loss: 0.0033, instance_loss: 0.0338, weighted_loss: 0.0125, label: 1, bag_size: 22264\n",
      "batch 99, loss: 0.2523, instance_loss: 0.1973, weighted_loss: 0.2358, label: 0, bag_size: 7835\n",
      "batch 119, loss: 0.1998, instance_loss: 0.3791, weighted_loss: 0.2536, label: 1, bag_size: 2278\n",
      "batch 139, loss: 0.0108, instance_loss: 0.0394, weighted_loss: 0.0194, label: 1, bag_size: 7217\n",
      "batch 159, loss: 0.3135, instance_loss: 0.4022, weighted_loss: 0.3401, label: 0, bag_size: 11128\n",
      "batch 179, loss: 0.0527, instance_loss: 0.0893, weighted_loss: 0.0637, label: 1, bag_size: 18161\n",
      "batch 199, loss: 0.0362, instance_loss: 0.1407, weighted_loss: 0.0675, label: 0, bag_size: 12732\n",
      "batch 219, loss: 0.0252, instance_loss: 0.0418, weighted_loss: 0.0302, label: 1, bag_size: 19606\n",
      "batch 239, loss: 0.0017, instance_loss: 0.1361, weighted_loss: 0.0421, label: 1, bag_size: 1781\n",
      "batch 259, loss: 0.0022, instance_loss: 0.0274, weighted_loss: 0.0097, label: 1, bag_size: 6317\n",
      "batch 279, loss: 0.8780, instance_loss: 1.1089, weighted_loss: 0.9473, label: 1, bag_size: 12946\n",
      "batch 299, loss: 0.0376, instance_loss: 0.0592, weighted_loss: 0.0441, label: 1, bag_size: 5292\n",
      "batch 319, loss: 0.0198, instance_loss: 0.0194, weighted_loss: 0.0197, label: 0, bag_size: 9888\n",
      "batch 339, loss: 2.7884, instance_loss: 3.2785, weighted_loss: 2.9354, label: 1, bag_size: 11316\n",
      "batch 359, loss: 0.0122, instance_loss: 0.0071, weighted_loss: 0.0107, label: 1, bag_size: 9147\n",
      "batch 379, loss: 0.0176, instance_loss: 0.4884, weighted_loss: 0.1588, label: 1, bag_size: 8019\n",
      "batch 399, loss: 0.0263, instance_loss: 0.1023, weighted_loss: 0.0491, label: 1, bag_size: 11220\n",
      "batch 419, loss: 0.0025, instance_loss: 0.0420, weighted_loss: 0.0143, label: 0, bag_size: 16052\n",
      "batch 439, loss: 0.3112, instance_loss: 0.1692, weighted_loss: 0.2686, label: 0, bag_size: 18777\n",
      "batch 459, loss: 0.0061, instance_loss: 0.0257, weighted_loss: 0.0120, label: 0, bag_size: 11194\n",
      "batch 479, loss: 0.0851, instance_loss: 0.0891, weighted_loss: 0.0863, label: 0, bag_size: 10444\n",
      "batch 499, loss: 0.2012, instance_loss: 0.8984, weighted_loss: 0.4104, label: 0, bag_size: 1789\n",
      "batch 519, loss: 0.0110, instance_loss: 0.3039, weighted_loss: 0.0988, label: 0, bag_size: 2303\n",
      "batch 539, loss: 0.0540, instance_loss: 0.1996, weighted_loss: 0.0977, label: 0, bag_size: 2624\n",
      "batch 559, loss: 0.0027, instance_loss: 0.0114, weighted_loss: 0.0053, label: 0, bag_size: 18154\n",
      "batch 579, loss: 0.1236, instance_loss: 0.0689, weighted_loss: 0.1072, label: 1, bag_size: 11701\n",
      "batch 599, loss: 0.0042, instance_loss: 0.0623, weighted_loss: 0.0216, label: 0, bag_size: 4902\n",
      "batch 619, loss: 0.2470, instance_loss: 0.5596, weighted_loss: 0.3408, label: 0, bag_size: 20555\n",
      "batch 639, loss: 0.0077, instance_loss: 0.0773, weighted_loss: 0.0286, label: 1, bag_size: 5494\n",
      "batch 659, loss: 0.0072, instance_loss: 0.0181, weighted_loss: 0.0105, label: 0, bag_size: 9415\n",
      "batch 679, loss: 2.7415, instance_loss: 2.0787, weighted_loss: 2.5426, label: 0, bag_size: 2219\n",
      "batch 699, loss: 0.0260, instance_loss: 0.0311, weighted_loss: 0.0275, label: 0, bag_size: 11113\n",
      "batch 719, loss: 0.0001, instance_loss: 0.1583, weighted_loss: 0.0476, label: 0, bag_size: 3787\n",
      "batch 739, loss: 1.1189, instance_loss: 1.2064, weighted_loss: 1.1452, label: 0, bag_size: 5009\n",
      "batch 759, loss: 0.0054, instance_loss: 0.0150, weighted_loss: 0.0083, label: 0, bag_size: 1962\n",
      "batch 779, loss: 1.0494, instance_loss: 1.9290, weighted_loss: 1.3133, label: 1, bag_size: 13367\n",
      "batch 799, loss: 2.4505, instance_loss: 2.4308, weighted_loss: 2.4446, label: 1, bag_size: 13089\n",
      "batch 819, loss: 0.1967, instance_loss: 0.5054, weighted_loss: 0.2893, label: 1, bag_size: 7989\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9596798780487805: correct 12591/13120\n",
      "class 1 clustering acc 0.7698170731707317: correct 5050/6560\n",
      "Epoch: 10, train_loss: 0.3252, train_clustering_loss:  0.4317, train_error: 0.1232\n",
      "class 0: acc 0.8716049382716049, correct 353/405\n",
      "class 1: acc 0.8819277108433735, correct 366/415\n",
      "\n",
      "Val Set, val_loss: 0.2920, val_error: 0.1273, auc: 0.9645\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.3375: correct 297/880\n",
      "class 0: acc 0.7692307692307693, correct 40/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0103, instance_loss: 0.0638, weighted_loss: 0.0263, label: 0, bag_size: 18240\n",
      "batch 39, loss: 0.0338, instance_loss: 0.0187, weighted_loss: 0.0292, label: 1, bag_size: 16512\n",
      "batch 59, loss: 0.1261, instance_loss: 0.4830, weighted_loss: 0.2331, label: 1, bag_size: 1014\n",
      "batch 79, loss: 0.1468, instance_loss: 0.2737, weighted_loss: 0.1849, label: 0, bag_size: 3101\n",
      "batch 99, loss: 0.0002, instance_loss: 0.2465, weighted_loss: 0.0741, label: 0, bag_size: 5225\n",
      "batch 119, loss: 0.0023, instance_loss: 0.0077, weighted_loss: 0.0039, label: 1, bag_size: 1781\n",
      "batch 139, loss: 0.4294, instance_loss: 0.5744, weighted_loss: 0.4729, label: 0, bag_size: 18516\n",
      "batch 159, loss: 0.0183, instance_loss: 0.0186, weighted_loss: 0.0184, label: 0, bag_size: 1588\n",
      "batch 179, loss: 0.0034, instance_loss: 0.0758, weighted_loss: 0.0252, label: 1, bag_size: 4054\n",
      "batch 199, loss: 0.1333, instance_loss: 0.2009, weighted_loss: 0.1536, label: 1, bag_size: 14515\n",
      "batch 219, loss: 4.9247, instance_loss: 5.0839, weighted_loss: 4.9725, label: 1, bag_size: 1703\n",
      "batch 239, loss: 0.0362, instance_loss: 0.1685, weighted_loss: 0.0759, label: 1, bag_size: 6665\n",
      "batch 259, loss: 0.0022, instance_loss: 0.2539, weighted_loss: 0.0777, label: 1, bag_size: 617\n",
      "batch 279, loss: 3.2657, instance_loss: 3.7100, weighted_loss: 3.3990, label: 1, bag_size: 13089\n",
      "batch 299, loss: 0.0821, instance_loss: 0.0122, weighted_loss: 0.0612, label: 1, bag_size: 4128\n",
      "batch 319, loss: 0.0004, instance_loss: 0.1043, weighted_loss: 0.0315, label: 0, bag_size: 1234\n",
      "batch 339, loss: 0.0441, instance_loss: 0.0949, weighted_loss: 0.0593, label: 0, bag_size: 12217\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0088, weighted_loss: 0.0027, label: 0, bag_size: 8981\n",
      "batch 379, loss: 0.0397, instance_loss: 0.0537, weighted_loss: 0.0439, label: 1, bag_size: 8680\n",
      "batch 399, loss: 0.1888, instance_loss: 0.0433, weighted_loss: 0.1452, label: 1, bag_size: 8754\n",
      "batch 419, loss: 0.0778, instance_loss: 0.0928, weighted_loss: 0.0823, label: 0, bag_size: 2104\n",
      "batch 439, loss: 0.0086, instance_loss: 0.0101, weighted_loss: 0.0090, label: 1, bag_size: 9004\n",
      "batch 459, loss: 4.7758, instance_loss: 4.5058, weighted_loss: 4.6948, label: 0, bag_size: 2694\n",
      "batch 479, loss: 0.1700, instance_loss: 0.1031, weighted_loss: 0.1499, label: 0, bag_size: 65728\n",
      "batch 499, loss: 0.1167, instance_loss: 0.1513, weighted_loss: 0.1271, label: 0, bag_size: 3893\n",
      "batch 519, loss: 0.0584, instance_loss: 0.0291, weighted_loss: 0.0496, label: 0, bag_size: 19043\n",
      "batch 539, loss: 0.2822, instance_loss: 0.1245, weighted_loss: 0.2349, label: 1, bag_size: 6205\n",
      "batch 559, loss: 0.0087, instance_loss: 0.0921, weighted_loss: 0.0337, label: 1, bag_size: 9408\n",
      "batch 579, loss: 0.2600, instance_loss: 0.4675, weighted_loss: 0.3223, label: 1, bag_size: 1483\n",
      "batch 599, loss: 3.0371, instance_loss: 2.7439, weighted_loss: 2.9492, label: 0, bag_size: 15898\n",
      "batch 619, loss: 0.0212, instance_loss: 0.0834, weighted_loss: 0.0398, label: 1, bag_size: 7217\n",
      "batch 639, loss: 0.1368, instance_loss: 0.3452, weighted_loss: 0.1993, label: 0, bag_size: 3810\n",
      "batch 659, loss: 0.0135, instance_loss: 0.1041, weighted_loss: 0.0407, label: 0, bag_size: 2732\n",
      "batch 679, loss: 0.6839, instance_loss: 0.4874, weighted_loss: 0.6249, label: 0, bag_size: 7381\n",
      "batch 699, loss: 0.0190, instance_loss: 0.0152, weighted_loss: 0.0179, label: 1, bag_size: 8868\n",
      "batch 719, loss: 0.1996, instance_loss: 0.5848, weighted_loss: 0.3151, label: 0, bag_size: 11122\n",
      "batch 739, loss: 0.0217, instance_loss: 0.0315, weighted_loss: 0.0246, label: 1, bag_size: 9065\n",
      "batch 759, loss: 0.0302, instance_loss: 0.0504, weighted_loss: 0.0363, label: 0, bag_size: 9234\n",
      "batch 779, loss: 0.4017, instance_loss: 0.6067, weighted_loss: 0.4632, label: 1, bag_size: 8475\n",
      "batch 799, loss: 0.1748, instance_loss: 0.2254, weighted_loss: 0.1900, label: 1, bag_size: 9470\n",
      "batch 819, loss: 0.0078, instance_loss: 0.0056, weighted_loss: 0.0071, label: 0, bag_size: 931\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9636432926829268: correct 12643/13120\n",
      "class 1 clustering acc 0.806859756097561: correct 5293/6560\n",
      "Epoch: 11, train_loss: 0.2936, train_clustering_loss:  0.3637, train_error: 0.1085\n",
      "class 0: acc 0.8848920863309353, correct 369/417\n",
      "class 1: acc 0.8982630272952854, correct 362/403\n",
      "\n",
      "Val Set, val_loss: 0.2786, val_error: 0.1182, auc: 0.9649\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.36363636363636365: correct 320/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8103448275862069, correct 47/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0007, instance_loss: 0.0014, weighted_loss: 0.0009, label: 1, bag_size: 8466\n",
      "batch 39, loss: 0.0019, instance_loss: 0.0015, weighted_loss: 0.0018, label: 0, bag_size: 13225\n",
      "batch 59, loss: 0.0339, instance_loss: 0.0329, weighted_loss: 0.0336, label: 0, bag_size: 2091\n",
      "batch 79, loss: 0.0009, instance_loss: 0.0034, weighted_loss: 0.0016, label: 0, bag_size: 8948\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0271, weighted_loss: 0.0082, label: 0, bag_size: 10898\n",
      "batch 119, loss: 0.1660, instance_loss: 0.2157, weighted_loss: 0.1809, label: 1, bag_size: 1015\n",
      "batch 139, loss: 0.3869, instance_loss: 0.4223, weighted_loss: 0.3976, label: 1, bag_size: 2136\n",
      "batch 159, loss: 0.0870, instance_loss: 0.1628, weighted_loss: 0.1098, label: 0, bag_size: 2322\n",
      "batch 179, loss: 1.6849, instance_loss: 2.3795, weighted_loss: 1.8933, label: 1, bag_size: 2395\n",
      "batch 199, loss: 0.5437, instance_loss: 0.4752, weighted_loss: 0.5231, label: 0, bag_size: 1789\n",
      "batch 219, loss: 0.0331, instance_loss: 0.0555, weighted_loss: 0.0398, label: 1, bag_size: 4821\n",
      "batch 239, loss: 1.2916, instance_loss: 1.0846, weighted_loss: 1.2295, label: 1, bag_size: 12180\n",
      "batch 259, loss: 5.3670, instance_loss: 4.1906, weighted_loss: 5.0141, label: 0, bag_size: 2694\n",
      "batch 279, loss: 0.7779, instance_loss: 0.7688, weighted_loss: 0.7752, label: 0, bag_size: 1506\n",
      "batch 299, loss: 1.1052, instance_loss: 1.0588, weighted_loss: 1.0913, label: 0, bag_size: 1592\n",
      "batch 319, loss: 0.0027, instance_loss: 0.0066, weighted_loss: 0.0038, label: 0, bag_size: 15747\n",
      "batch 339, loss: 4.0061, instance_loss: 4.4326, weighted_loss: 4.1341, label: 1, bag_size: 3879\n",
      "batch 359, loss: 0.0276, instance_loss: 0.0241, weighted_loss: 0.0265, label: 1, bag_size: 6453\n",
      "batch 379, loss: 0.0180, instance_loss: 0.0218, weighted_loss: 0.0192, label: 1, bag_size: 13194\n",
      "batch 399, loss: 0.0026, instance_loss: 0.0040, weighted_loss: 0.0030, label: 0, bag_size: 15001\n",
      "batch 419, loss: 0.0714, instance_loss: 0.1275, weighted_loss: 0.0882, label: 0, bag_size: 5409\n",
      "batch 439, loss: 0.0037, instance_loss: 0.0033, weighted_loss: 0.0036, label: 0, bag_size: 10304\n",
      "batch 459, loss: 0.0244, instance_loss: 0.0075, weighted_loss: 0.0193, label: 1, bag_size: 12719\n",
      "batch 479, loss: 2.1818, instance_loss: 2.7123, weighted_loss: 2.3409, label: 1, bag_size: 1867\n",
      "batch 499, loss: 0.0956, instance_loss: 0.0083, weighted_loss: 0.0694, label: 1, bag_size: 12611\n",
      "batch 519, loss: 0.0078, instance_loss: 0.0074, weighted_loss: 0.0077, label: 1, bag_size: 3640\n",
      "batch 539, loss: 0.0098, instance_loss: 0.0255, weighted_loss: 0.0145, label: 0, bag_size: 16052\n",
      "batch 559, loss: 0.0049, instance_loss: 0.0015, weighted_loss: 0.0039, label: 0, bag_size: 23996\n",
      "batch 579, loss: 0.1297, instance_loss: 0.3576, weighted_loss: 0.1981, label: 1, bag_size: 6682\n",
      "batch 599, loss: 0.0029, instance_loss: 0.0157, weighted_loss: 0.0068, label: 0, bag_size: 1234\n",
      "batch 619, loss: 0.0117, instance_loss: 0.0091, weighted_loss: 0.0109, label: 1, bag_size: 5025\n",
      "batch 639, loss: 1.3307, instance_loss: 1.4742, weighted_loss: 1.3738, label: 1, bag_size: 7989\n",
      "batch 659, loss: 0.0070, instance_loss: 0.1840, weighted_loss: 0.0601, label: 1, bag_size: 14618\n",
      "batch 679, loss: 0.1709, instance_loss: 0.4527, weighted_loss: 0.2554, label: 1, bag_size: 8191\n",
      "batch 699, loss: 0.3540, instance_loss: 0.1846, weighted_loss: 0.3032, label: 0, bag_size: 4959\n",
      "batch 719, loss: 0.1833, instance_loss: 0.1335, weighted_loss: 0.1683, label: 1, bag_size: 5690\n",
      "batch 739, loss: 0.0448, instance_loss: 0.0512, weighted_loss: 0.0467, label: 0, bag_size: 2004\n",
      "batch 759, loss: 0.0707, instance_loss: 0.0630, weighted_loss: 0.0684, label: 0, bag_size: 3190\n",
      "batch 779, loss: 0.0573, instance_loss: 0.0453, weighted_loss: 0.0537, label: 1, bag_size: 20161\n",
      "batch 799, loss: 0.0245, instance_loss: 0.0092, weighted_loss: 0.0199, label: 0, bag_size: 8252\n",
      "batch 819, loss: 0.0072, instance_loss: 0.0094, weighted_loss: 0.0079, label: 0, bag_size: 2760\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9636432926829268: correct 12643/13120\n",
      "class 1 clustering acc 0.8228658536585366: correct 5398/6560\n",
      "Epoch: 12, train_loss: 0.2893, train_clustering_loss:  0.3481, train_error: 0.1171\n",
      "class 0: acc 0.8963133640552995, correct 389/434\n",
      "class 1: acc 0.8678756476683938, correct 335/386\n",
      "\n",
      "Val Set, val_loss: 0.3334, val_error: 0.1273, auc: 0.9708\n",
      "class 0 clustering acc 0.9926136363636363: correct 1747/1760\n",
      "class 1 clustering acc 0.08181818181818182: correct 72/880\n",
      "class 0: acc 1.0, correct 52/52\n",
      "class 1: acc 0.7586206896551724, correct 44/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0070, instance_loss: 0.0112, weighted_loss: 0.0083, label: 1, bag_size: 3409\n",
      "batch 39, loss: 0.0299, instance_loss: 0.0243, weighted_loss: 0.0282, label: 0, bag_size: 1881\n",
      "batch 59, loss: 0.0020, instance_loss: 0.0076, weighted_loss: 0.0036, label: 1, bag_size: 5317\n",
      "batch 79, loss: 1.9541, instance_loss: 3.3827, weighted_loss: 2.3827, label: 1, bag_size: 15185\n",
      "batch 99, loss: 0.2267, instance_loss: 0.9091, weighted_loss: 0.4314, label: 0, bag_size: 1953\n",
      "batch 119, loss: 1.0488, instance_loss: 1.4516, weighted_loss: 1.1697, label: 1, bag_size: 2731\n",
      "batch 139, loss: 0.0011, instance_loss: 0.0055, weighted_loss: 0.0024, label: 1, bag_size: 6745\n",
      "batch 159, loss: 0.0108, instance_loss: 0.0143, weighted_loss: 0.0118, label: 1, bag_size: 3082\n",
      "batch 179, loss: 1.6988, instance_loss: 2.2396, weighted_loss: 1.8611, label: 1, bag_size: 21252\n",
      "batch 199, loss: 1.5083, instance_loss: 1.7062, weighted_loss: 1.5677, label: 1, bag_size: 1493\n",
      "batch 219, loss: 0.0034, instance_loss: 0.0075, weighted_loss: 0.0046, label: 1, bag_size: 11875\n",
      "batch 239, loss: 3.2265, instance_loss: 2.9882, weighted_loss: 3.1550, label: 1, bag_size: 2455\n",
      "batch 259, loss: 0.0261, instance_loss: 0.0087, weighted_loss: 0.0209, label: 1, bag_size: 15689\n",
      "batch 279, loss: 0.0139, instance_loss: 0.0124, weighted_loss: 0.0134, label: 1, bag_size: 11256\n",
      "batch 299, loss: 0.0251, instance_loss: 0.0431, weighted_loss: 0.0305, label: 1, bag_size: 9408\n",
      "batch 319, loss: 0.5718, instance_loss: 0.7401, weighted_loss: 0.6223, label: 0, bag_size: 1684\n",
      "batch 339, loss: 0.0106, instance_loss: 0.0254, weighted_loss: 0.0150, label: 0, bag_size: 18738\n",
      "batch 359, loss: 0.0763, instance_loss: 0.0506, weighted_loss: 0.0686, label: 0, bag_size: 8788\n",
      "batch 379, loss: 0.0145, instance_loss: 0.0043, weighted_loss: 0.0114, label: 0, bag_size: 4465\n",
      "batch 399, loss: 0.3019, instance_loss: 0.5807, weighted_loss: 0.3856, label: 0, bag_size: 23796\n",
      "batch 419, loss: 4.9633, instance_loss: 4.7565, weighted_loss: 4.9013, label: 1, bag_size: 2731\n",
      "batch 439, loss: 2.3967, instance_loss: 2.4751, weighted_loss: 2.4202, label: 0, bag_size: 3897\n",
      "batch 459, loss: 0.0035, instance_loss: 0.0846, weighted_loss: 0.0279, label: 0, bag_size: 1824\n",
      "batch 479, loss: 0.0100, instance_loss: 0.0048, weighted_loss: 0.0084, label: 1, bag_size: 8868\n",
      "batch 499, loss: 0.0716, instance_loss: 0.0294, weighted_loss: 0.0589, label: 0, bag_size: 4271\n",
      "batch 519, loss: 0.0546, instance_loss: 0.0311, weighted_loss: 0.0476, label: 1, bag_size: 9610\n",
      "batch 539, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0015, label: 0, bag_size: 18225\n",
      "batch 559, loss: 0.0097, instance_loss: 0.0370, weighted_loss: 0.0179, label: 1, bag_size: 11884\n",
      "batch 579, loss: 0.3134, instance_loss: 0.5241, weighted_loss: 0.3766, label: 1, bag_size: 2785\n",
      "batch 599, loss: 0.0300, instance_loss: 0.0133, weighted_loss: 0.0250, label: 0, bag_size: 10128\n",
      "batch 619, loss: 0.0150, instance_loss: 0.0197, weighted_loss: 0.0164, label: 0, bag_size: 21138\n",
      "batch 639, loss: 0.0037, instance_loss: 0.0045, weighted_loss: 0.0039, label: 0, bag_size: 3725\n",
      "batch 659, loss: 0.0384, instance_loss: 0.0515, weighted_loss: 0.0423, label: 0, bag_size: 3774\n",
      "batch 679, loss: 0.0004, instance_loss: 0.0009, weighted_loss: 0.0006, label: 0, bag_size: 9470\n",
      "batch 699, loss: 0.2071, instance_loss: 0.6248, weighted_loss: 0.3324, label: 1, bag_size: 2480\n",
      "batch 719, loss: 0.0128, instance_loss: 0.4304, weighted_loss: 0.1381, label: 1, bag_size: 6090\n",
      "batch 739, loss: 0.0443, instance_loss: 0.0444, weighted_loss: 0.0443, label: 0, bag_size: 1202\n",
      "batch 759, loss: 0.3736, instance_loss: 0.2651, weighted_loss: 0.3411, label: 0, bag_size: 6884\n",
      "batch 779, loss: 0.1558, instance_loss: 0.0879, weighted_loss: 0.1354, label: 0, bag_size: 1498\n",
      "batch 799, loss: 0.0407, instance_loss: 0.0297, weighted_loss: 0.0374, label: 1, bag_size: 15665\n",
      "batch 819, loss: 0.0069, instance_loss: 0.0452, weighted_loss: 0.0184, label: 1, bag_size: 6792\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9599085365853659: correct 12594/13120\n",
      "class 1 clustering acc 0.7900914634146341: correct 5183/6560\n",
      "Epoch: 13, train_loss: 0.3131, train_clustering_loss:  0.3863, train_error: 0.1293\n",
      "class 0: acc 0.8891454965357968, correct 385/433\n",
      "class 1: acc 0.8501291989664083, correct 329/387\n",
      "\n",
      "Val Set, val_loss: 0.2364, val_error: 0.1182, auc: 0.9731\n",
      "class 0 clustering acc 0.9954545454545455: correct 1752/1760\n",
      "class 1 clustering acc 0.07272727272727272: correct 64/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8103448275862069, correct 47/58\n",
      "Validation loss decreased (0.242626 --> 0.236387).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0224, instance_loss: 0.0291, weighted_loss: 0.0244, label: 0, bag_size: 9866\n",
      "batch 39, loss: 0.0249, instance_loss: 0.0221, weighted_loss: 0.0241, label: 0, bag_size: 2424\n",
      "batch 59, loss: 0.6263, instance_loss: 1.3101, weighted_loss: 0.8314, label: 0, bag_size: 1592\n",
      "batch 79, loss: 0.6858, instance_loss: 1.2581, weighted_loss: 0.8575, label: 1, bag_size: 12180\n",
      "batch 99, loss: 0.3127, instance_loss: 0.3087, weighted_loss: 0.3115, label: 1, bag_size: 19972\n",
      "batch 119, loss: 0.0589, instance_loss: 1.2306, weighted_loss: 0.4104, label: 0, bag_size: 2844\n",
      "batch 139, loss: 0.2844, instance_loss: 0.4327, weighted_loss: 0.3288, label: 0, bag_size: 16690\n",
      "batch 159, loss: 0.0130, instance_loss: 0.0175, weighted_loss: 0.0143, label: 1, bag_size: 7613\n",
      "batch 179, loss: 0.0129, instance_loss: 0.0682, weighted_loss: 0.0295, label: 0, bag_size: 16052\n",
      "batch 199, loss: 0.0016, instance_loss: 0.3890, weighted_loss: 0.1179, label: 1, bag_size: 5612\n",
      "batch 219, loss: 0.0040, instance_loss: 0.4195, weighted_loss: 0.1286, label: 1, bag_size: 11387\n",
      "batch 239, loss: 0.0043, instance_loss: 0.1797, weighted_loss: 0.0569, label: 0, bag_size: 2424\n",
      "batch 259, loss: 0.1917, instance_loss: 0.1391, weighted_loss: 0.1760, label: 1, bag_size: 8680\n",
      "batch 279, loss: 0.0061, instance_loss: 0.0193, weighted_loss: 0.0101, label: 0, bag_size: 10995\n",
      "batch 299, loss: 0.0683, instance_loss: 0.1776, weighted_loss: 0.1011, label: 1, bag_size: 10072\n",
      "batch 319, loss: 0.0624, instance_loss: 0.1433, weighted_loss: 0.0867, label: 0, bag_size: 32227\n",
      "batch 339, loss: 0.0122, instance_loss: 0.0007, weighted_loss: 0.0088, label: 1, bag_size: 1015\n",
      "batch 359, loss: 0.4228, instance_loss: 0.5287, weighted_loss: 0.4546, label: 0, bag_size: 23618\n",
      "batch 379, loss: 0.0003, instance_loss: 0.1747, weighted_loss: 0.0526, label: 0, bag_size: 16992\n",
      "batch 399, loss: 0.0415, instance_loss: 0.0593, weighted_loss: 0.0468, label: 1, bag_size: 1525\n",
      "batch 419, loss: 0.4772, instance_loss: 0.2552, weighted_loss: 0.4106, label: 1, bag_size: 12946\n",
      "batch 439, loss: 0.0275, instance_loss: 0.0313, weighted_loss: 0.0286, label: 1, bag_size: 9078\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0325, weighted_loss: 0.0098, label: 0, bag_size: 32227\n",
      "batch 479, loss: 0.0037, instance_loss: 0.0098, weighted_loss: 0.0055, label: 1, bag_size: 20161\n",
      "batch 499, loss: 0.9302, instance_loss: 0.9176, weighted_loss: 0.9264, label: 1, bag_size: 16034\n",
      "batch 519, loss: 0.0994, instance_loss: 0.5066, weighted_loss: 0.2215, label: 0, bag_size: 30751\n",
      "batch 539, loss: 0.0121, instance_loss: 0.0647, weighted_loss: 0.0279, label: 0, bag_size: 9471\n",
      "batch 559, loss: 0.0156, instance_loss: 0.0086, weighted_loss: 0.0135, label: 1, bag_size: 5991\n",
      "batch 579, loss: 0.0241, instance_loss: 0.0224, weighted_loss: 0.0236, label: 0, bag_size: 30751\n",
      "batch 599, loss: 0.0037, instance_loss: 0.0047, weighted_loss: 0.0040, label: 0, bag_size: 22800\n",
      "batch 619, loss: 0.0084, instance_loss: 0.0122, weighted_loss: 0.0095, label: 1, bag_size: 29832\n",
      "batch 639, loss: 0.0013, instance_loss: 0.0024, weighted_loss: 0.0016, label: 1, bag_size: 3453\n",
      "batch 659, loss: 1.1059, instance_loss: 1.9361, weighted_loss: 1.3549, label: 1, bag_size: 1703\n",
      "batch 679, loss: 0.0189, instance_loss: 0.0473, weighted_loss: 0.0274, label: 0, bag_size: 2511\n",
      "batch 699, loss: 0.4757, instance_loss: 0.4155, weighted_loss: 0.4576, label: 1, bag_size: 2681\n",
      "batch 719, loss: 0.0026, instance_loss: 0.0040, weighted_loss: 0.0030, label: 1, bag_size: 6317\n",
      "batch 739, loss: 0.3100, instance_loss: 0.4107, weighted_loss: 0.3402, label: 1, bag_size: 2179\n",
      "batch 759, loss: 0.7421, instance_loss: 0.3298, weighted_loss: 0.6184, label: 1, bag_size: 7989\n",
      "batch 779, loss: 0.0001, instance_loss: 0.0014, weighted_loss: 0.0005, label: 0, bag_size: 11654\n",
      "batch 799, loss: 0.2108, instance_loss: 0.4740, weighted_loss: 0.2897, label: 0, bag_size: 11151\n",
      "batch 819, loss: 0.1586, instance_loss: 0.1046, weighted_loss: 0.1424, label: 1, bag_size: 9062\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9491615853658537: correct 12453/13120\n",
      "class 1 clustering acc 0.7890243902439025: correct 5176/6560\n",
      "Epoch: 14, train_loss: 0.3215, train_clustering_loss:  0.4485, train_error: 0.1220\n",
      "class 0: acc 0.8730964467005076, correct 344/394\n",
      "class 1: acc 0.8826291079812206, correct 376/426\n",
      "\n",
      "Val Set, val_loss: 0.4197, val_error: 0.2000, auc: 0.9579\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.05: correct 44/880\n",
      "class 0: acc 0.5961538461538461, correct 31/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0052, instance_loss: 0.0130, weighted_loss: 0.0076, label: 0, bag_size: 18045\n",
      "batch 39, loss: 0.7740, instance_loss: 0.7812, weighted_loss: 0.7761, label: 0, bag_size: 9387\n",
      "batch 59, loss: 0.0221, instance_loss: 0.0190, weighted_loss: 0.0211, label: 1, bag_size: 1022\n",
      "batch 79, loss: 0.0081, instance_loss: 0.0096, weighted_loss: 0.0086, label: 1, bag_size: 10112\n",
      "batch 99, loss: 0.0111, instance_loss: 0.0165, weighted_loss: 0.0127, label: 1, bag_size: 8685\n",
      "batch 119, loss: 0.0629, instance_loss: 0.0404, weighted_loss: 0.0562, label: 1, bag_size: 10432\n",
      "batch 139, loss: 0.2109, instance_loss: 0.2023, weighted_loss: 0.2083, label: 0, bag_size: 1370\n",
      "batch 159, loss: 0.0293, instance_loss: 0.0206, weighted_loss: 0.0267, label: 0, bag_size: 705\n",
      "batch 179, loss: 0.0239, instance_loss: 0.0684, weighted_loss: 0.0372, label: 0, bag_size: 10263\n",
      "batch 199, loss: 0.0069, instance_loss: 0.0075, weighted_loss: 0.0071, label: 0, bag_size: 12593\n",
      "batch 219, loss: 0.2106, instance_loss: 0.2368, weighted_loss: 0.2184, label: 1, bag_size: 9561\n",
      "batch 239, loss: 0.2167, instance_loss: 0.0764, weighted_loss: 0.1746, label: 0, bag_size: 4997\n",
      "batch 259, loss: 0.0271, instance_loss: 0.0263, weighted_loss: 0.0269, label: 0, bag_size: 16211\n",
      "batch 279, loss: 0.0024, instance_loss: 0.0027, weighted_loss: 0.0025, label: 1, bag_size: 7217\n",
      "batch 299, loss: 0.0019, instance_loss: 0.0052, weighted_loss: 0.0029, label: 0, bag_size: 21682\n",
      "batch 319, loss: 0.0298, instance_loss: 0.0241, weighted_loss: 0.0281, label: 1, bag_size: 4789\n",
      "batch 339, loss: 0.2689, instance_loss: 0.3203, weighted_loss: 0.2843, label: 1, bag_size: 4939\n",
      "batch 359, loss: 0.0388, instance_loss: 0.0179, weighted_loss: 0.0325, label: 1, bag_size: 9065\n",
      "batch 379, loss: 0.0514, instance_loss: 0.0265, weighted_loss: 0.0439, label: 1, bag_size: 16034\n",
      "batch 399, loss: 0.0462, instance_loss: 0.1055, weighted_loss: 0.0640, label: 0, bag_size: 2036\n",
      "batch 419, loss: 0.0199, instance_loss: 0.0242, weighted_loss: 0.0212, label: 0, bag_size: 13205\n",
      "batch 439, loss: 0.0530, instance_loss: 0.0308, weighted_loss: 0.0463, label: 1, bag_size: 12626\n",
      "batch 459, loss: 1.2201, instance_loss: 1.5516, weighted_loss: 1.3195, label: 1, bag_size: 1683\n",
      "batch 479, loss: 0.0100, instance_loss: 0.0118, weighted_loss: 0.0105, label: 1, bag_size: 11387\n",
      "batch 499, loss: 0.0194, instance_loss: 0.0151, weighted_loss: 0.0181, label: 1, bag_size: 2695\n",
      "batch 519, loss: 4.2889, instance_loss: 4.6849, weighted_loss: 4.4077, label: 0, bag_size: 4692\n",
      "batch 539, loss: 0.0460, instance_loss: 0.0301, weighted_loss: 0.0412, label: 0, bag_size: 3502\n",
      "batch 559, loss: 0.0397, instance_loss: 0.0202, weighted_loss: 0.0338, label: 0, bag_size: 10791\n",
      "batch 579, loss: 0.0048, instance_loss: 0.0068, weighted_loss: 0.0054, label: 0, bag_size: 3970\n",
      "batch 599, loss: 0.0035, instance_loss: 0.0024, weighted_loss: 0.0032, label: 1, bag_size: 11875\n",
      "batch 619, loss: 0.0029, instance_loss: 0.0033, weighted_loss: 0.0030, label: 0, bag_size: 11900\n",
      "batch 639, loss: 0.2026, instance_loss: 0.2405, weighted_loss: 0.2140, label: 0, bag_size: 2998\n",
      "batch 659, loss: 0.0020, instance_loss: 0.0075, weighted_loss: 0.0036, label: 0, bag_size: 9885\n",
      "batch 679, loss: 0.0606, instance_loss: 0.0252, weighted_loss: 0.0500, label: 1, bag_size: 8602\n",
      "batch 699, loss: 0.0531, instance_loss: 0.0539, weighted_loss: 0.0533, label: 1, bag_size: 11220\n",
      "batch 719, loss: 0.0128, instance_loss: 0.0040, weighted_loss: 0.0102, label: 1, bag_size: 10912\n",
      "batch 739, loss: 0.3861, instance_loss: 0.4899, weighted_loss: 0.4173, label: 1, bag_size: 3368\n",
      "batch 759, loss: 0.0212, instance_loss: 0.0176, weighted_loss: 0.0201, label: 1, bag_size: 5345\n",
      "batch 779, loss: 3.0744, instance_loss: 3.7937, weighted_loss: 3.2902, label: 1, bag_size: 15563\n",
      "batch 799, loss: 0.0780, instance_loss: 0.0475, weighted_loss: 0.0688, label: 0, bag_size: 6281\n",
      "batch 819, loss: 0.0004, instance_loss: 0.0033, weighted_loss: 0.0013, label: 0, bag_size: 3190\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9720274390243903: correct 12753/13120\n",
      "class 1 clustering acc 0.8652439024390244: correct 5676/6560\n",
      "Epoch: 15, train_loss: 0.2361, train_clustering_loss:  0.2793, train_error: 0.0817\n",
      "class 0: acc 0.930952380952381, correct 391/420\n",
      "class 1: acc 0.905, correct 362/400\n",
      "\n",
      "Val Set, val_loss: 0.5629, val_error: 0.1818, auc: 0.9698\n",
      "class 0 clustering acc 0.990909090909091: correct 1744/1760\n",
      "class 1 clustering acc 0.08181818181818182: correct 72/880\n",
      "class 0: acc 1.0, correct 52/52\n",
      "class 1: acc 0.6551724137931034, correct 38/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0487, instance_loss: 0.0349, weighted_loss: 0.0446, label: 0, bag_size: 21082\n",
      "batch 39, loss: 0.0092, instance_loss: 0.0129, weighted_loss: 0.0103, label: 0, bag_size: 19067\n",
      "batch 59, loss: 0.0592, instance_loss: 0.0389, weighted_loss: 0.0531, label: 0, bag_size: 11187\n",
      "batch 79, loss: 0.1149, instance_loss: 0.0972, weighted_loss: 0.1096, label: 0, bag_size: 7557\n",
      "batch 99, loss: 0.0694, instance_loss: 0.0518, weighted_loss: 0.0641, label: 1, bag_size: 18649\n",
      "batch 119, loss: 0.0014, instance_loss: 0.0137, weighted_loss: 0.0051, label: 0, bag_size: 4465\n",
      "batch 139, loss: 0.0944, instance_loss: 0.0886, weighted_loss: 0.0927, label: 1, bag_size: 7989\n",
      "batch 159, loss: 0.0020, instance_loss: 0.0022, weighted_loss: 0.0020, label: 1, bag_size: 8868\n",
      "batch 179, loss: 0.0002, instance_loss: 0.0002, weighted_loss: 0.0002, label: 0, bag_size: 20150\n",
      "batch 199, loss: 0.0379, instance_loss: 0.0579, weighted_loss: 0.0439, label: 1, bag_size: 1015\n",
      "batch 219, loss: 0.2335, instance_loss: 0.2045, weighted_loss: 0.2248, label: 1, bag_size: 16154\n",
      "batch 239, loss: 0.1921, instance_loss: 0.2966, weighted_loss: 0.2235, label: 0, bag_size: 8549\n",
      "batch 259, loss: 0.0131, instance_loss: 0.0239, weighted_loss: 0.0164, label: 0, bag_size: 19470\n",
      "batch 279, loss: 0.0011, instance_loss: 0.0005, weighted_loss: 0.0009, label: 0, bag_size: 17791\n",
      "batch 299, loss: 0.0459, instance_loss: 0.0529, weighted_loss: 0.0480, label: 0, bag_size: 11917\n",
      "batch 319, loss: 0.3583, instance_loss: 0.5187, weighted_loss: 0.4064, label: 1, bag_size: 1123\n",
      "batch 339, loss: 0.0212, instance_loss: 0.0443, weighted_loss: 0.0281, label: 0, bag_size: 2998\n",
      "batch 359, loss: 0.0239, instance_loss: 0.0130, weighted_loss: 0.0206, label: 0, bag_size: 9930\n",
      "batch 379, loss: 0.0014, instance_loss: 0.0011, weighted_loss: 0.0013, label: 1, bag_size: 7515\n",
      "batch 399, loss: 0.1597, instance_loss: 0.2185, weighted_loss: 0.1773, label: 1, bag_size: 1609\n",
      "batch 419, loss: 0.0111, instance_loss: 0.0076, weighted_loss: 0.0100, label: 1, bag_size: 18468\n",
      "batch 439, loss: 0.2196, instance_loss: 0.1812, weighted_loss: 0.2081, label: 1, bag_size: 2314\n",
      "batch 459, loss: 0.0111, instance_loss: 0.0099, weighted_loss: 0.0107, label: 0, bag_size: 15464\n",
      "batch 479, loss: 0.6840, instance_loss: 0.8771, weighted_loss: 0.7419, label: 0, bag_size: 2918\n",
      "batch 499, loss: 0.0120, instance_loss: 0.0067, weighted_loss: 0.0104, label: 0, bag_size: 10146\n",
      "batch 519, loss: 0.0225, instance_loss: 0.0157, weighted_loss: 0.0204, label: 1, bag_size: 13015\n",
      "batch 539, loss: 0.0020, instance_loss: 0.0086, weighted_loss: 0.0040, label: 1, bag_size: 16512\n",
      "batch 559, loss: 0.0099, instance_loss: 0.0377, weighted_loss: 0.0182, label: 1, bag_size: 10969\n",
      "batch 579, loss: 0.0126, instance_loss: 0.0138, weighted_loss: 0.0130, label: 1, bag_size: 13692\n",
      "batch 599, loss: 0.3670, instance_loss: 0.5278, weighted_loss: 0.4153, label: 0, bag_size: 931\n",
      "batch 619, loss: 0.0991, instance_loss: 0.1480, weighted_loss: 0.1138, label: 0, bag_size: 9930\n",
      "batch 639, loss: 0.0804, instance_loss: 0.0380, weighted_loss: 0.0677, label: 1, bag_size: 1919\n",
      "batch 659, loss: 0.3295, instance_loss: 0.2663, weighted_loss: 0.3105, label: 0, bag_size: 8744\n",
      "batch 679, loss: 0.6643, instance_loss: 1.2052, weighted_loss: 0.8266, label: 1, bag_size: 14515\n",
      "batch 699, loss: 0.7938, instance_loss: 1.8897, weighted_loss: 1.1226, label: 0, bag_size: 2091\n",
      "batch 719, loss: 0.0159, instance_loss: 0.0446, weighted_loss: 0.0245, label: 0, bag_size: 705\n",
      "batch 739, loss: 0.7211, instance_loss: 0.4764, weighted_loss: 0.6477, label: 0, bag_size: 1370\n",
      "batch 759, loss: 0.2119, instance_loss: 0.2425, weighted_loss: 0.2211, label: 1, bag_size: 8026\n",
      "batch 779, loss: 0.0056, instance_loss: 0.0077, weighted_loss: 0.0063, label: 1, bag_size: 15008\n",
      "batch 799, loss: 0.0018, instance_loss: 0.0110, weighted_loss: 0.0046, label: 1, bag_size: 3968\n",
      "batch 819, loss: 0.0027, instance_loss: 0.0032, weighted_loss: 0.0029, label: 1, bag_size: 5894\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9648628048780488: correct 12659/13120\n",
      "class 1 clustering acc 0.8202743902439025: correct 5381/6560\n",
      "Epoch: 16, train_loss: 0.2756, train_clustering_loss:  0.3522, train_error: 0.1085\n",
      "class 0: acc 0.8816120906801007, correct 350/397\n",
      "class 1: acc 0.900709219858156, correct 381/423\n",
      "\n",
      "Val Set, val_loss: 0.2666, val_error: 0.1000, auc: 0.9639\n",
      "class 0 clustering acc 0.9897727272727272: correct 1742/1760\n",
      "class 1 clustering acc 0.3534090909090909: correct 311/880\n",
      "class 0: acc 0.8653846153846154, correct 45/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0003, instance_loss: 0.0060, weighted_loss: 0.0020, label: 0, bag_size: 11759\n",
      "batch 39, loss: 0.1177, instance_loss: 0.0345, weighted_loss: 0.0927, label: 0, bag_size: 6898\n",
      "batch 59, loss: 0.0970, instance_loss: 0.1574, weighted_loss: 0.1151, label: 0, bag_size: 3089\n",
      "batch 79, loss: 0.0674, instance_loss: 0.0559, weighted_loss: 0.0639, label: 0, bag_size: 8744\n",
      "batch 99, loss: 0.0043, instance_loss: 0.0078, weighted_loss: 0.0053, label: 0, bag_size: 17630\n",
      "batch 119, loss: 1.0411, instance_loss: 1.1941, weighted_loss: 1.0870, label: 0, bag_size: 2242\n",
      "batch 139, loss: 0.0035, instance_loss: 0.0022, weighted_loss: 0.0031, label: 1, bag_size: 1294\n",
      "batch 159, loss: 0.0012, instance_loss: 0.0015, weighted_loss: 0.0013, label: 1, bag_size: 10920\n",
      "batch 179, loss: 0.0817, instance_loss: 0.0696, weighted_loss: 0.0781, label: 0, bag_size: 13339\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0029, weighted_loss: 0.0009, label: 0, bag_size: 2748\n",
      "batch 219, loss: 0.0025, instance_loss: 0.0026, weighted_loss: 0.0025, label: 0, bag_size: 9866\n",
      "batch 239, loss: 0.1060, instance_loss: 0.0931, weighted_loss: 0.1022, label: 1, bag_size: 5921\n",
      "batch 259, loss: 0.0265, instance_loss: 0.0365, weighted_loss: 0.0295, label: 1, bag_size: 30675\n",
      "batch 279, loss: 0.0062, instance_loss: 0.0193, weighted_loss: 0.0102, label: 1, bag_size: 10033\n",
      "batch 299, loss: 0.7037, instance_loss: 0.9637, weighted_loss: 0.7817, label: 0, bag_size: 9387\n",
      "batch 319, loss: 0.0005, instance_loss: 0.0001, weighted_loss: 0.0004, label: 0, bag_size: 4465\n",
      "batch 339, loss: 1.5994, instance_loss: 3.0914, weighted_loss: 2.0470, label: 0, bag_size: 7428\n",
      "batch 359, loss: 0.0345, instance_loss: 0.0661, weighted_loss: 0.0440, label: 1, bag_size: 9321\n",
      "batch 379, loss: 0.0790, instance_loss: 0.0854, weighted_loss: 0.0809, label: 1, bag_size: 11223\n",
      "batch 399, loss: 0.0124, instance_loss: 0.0339, weighted_loss: 0.0188, label: 0, bag_size: 1458\n",
      "batch 419, loss: 0.0419, instance_loss: 0.0814, weighted_loss: 0.0538, label: 1, bag_size: 9321\n",
      "batch 439, loss: 0.1690, instance_loss: 0.0870, weighted_loss: 0.1444, label: 1, bag_size: 10432\n",
      "batch 459, loss: 0.0747, instance_loss: 0.0430, weighted_loss: 0.0652, label: 1, bag_size: 5921\n",
      "batch 479, loss: 0.0745, instance_loss: 0.1245, weighted_loss: 0.0895, label: 1, bag_size: 4239\n",
      "batch 499, loss: 0.0042, instance_loss: 0.0250, weighted_loss: 0.0104, label: 1, bag_size: 5864\n",
      "batch 519, loss: 2.1832, instance_loss: 2.6448, weighted_loss: 2.3217, label: 0, bag_size: 2815\n",
      "batch 539, loss: 0.0019, instance_loss: 0.0075, weighted_loss: 0.0036, label: 1, bag_size: 3437\n",
      "batch 559, loss: 0.2188, instance_loss: 0.2815, weighted_loss: 0.2376, label: 1, bag_size: 30675\n",
      "batch 579, loss: 3.5042, instance_loss: 4.5956, weighted_loss: 3.8316, label: 0, bag_size: 4692\n",
      "batch 599, loss: 0.1076, instance_loss: 0.0732, weighted_loss: 0.0973, label: 0, bag_size: 9069\n",
      "batch 619, loss: 1.7552, instance_loss: 2.1676, weighted_loss: 1.8789, label: 1, bag_size: 13089\n",
      "batch 639, loss: 0.0136, instance_loss: 0.0291, weighted_loss: 0.0182, label: 1, bag_size: 2140\n",
      "batch 659, loss: 2.4526, instance_loss: 2.5887, weighted_loss: 2.4934, label: 0, bag_size: 7239\n",
      "batch 679, loss: 0.1166, instance_loss: 0.1038, weighted_loss: 0.1128, label: 1, bag_size: 3409\n",
      "batch 699, loss: 0.0094, instance_loss: 0.0082, weighted_loss: 0.0090, label: 1, bag_size: 12349\n",
      "batch 719, loss: 1.5706, instance_loss: 1.7676, weighted_loss: 1.6297, label: 1, bag_size: 1095\n",
      "batch 739, loss: 0.0227, instance_loss: 0.0259, weighted_loss: 0.0237, label: 0, bag_size: 30751\n",
      "batch 759, loss: 0.0012, instance_loss: 0.0010, weighted_loss: 0.0012, label: 0, bag_size: 18240\n",
      "batch 779, loss: 0.0309, instance_loss: 0.0220, weighted_loss: 0.0282, label: 0, bag_size: 15464\n",
      "batch 799, loss: 0.0531, instance_loss: 0.0452, weighted_loss: 0.0507, label: 0, bag_size: 8330\n",
      "batch 819, loss: 0.1533, instance_loss: 0.1126, weighted_loss: 0.1411, label: 1, bag_size: 2140\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9678353658536586: correct 12698/13120\n",
      "class 1 clustering acc 0.844359756097561: correct 5539/6560\n",
      "Epoch: 17, train_loss: 0.2793, train_clustering_loss:  0.3182, train_error: 0.1073\n",
      "class 0: acc 0.9, correct 378/420\n",
      "class 1: acc 0.885, correct 354/400\n",
      "\n",
      "Val Set, val_loss: 0.3470, val_error: 0.1455, auc: 0.9675\n",
      "class 0 clustering acc 0.9954545454545455: correct 1752/1760\n",
      "class 1 clustering acc 0.2034090909090909: correct 179/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.7586206896551724, correct 44/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0951, instance_loss: 0.0266, weighted_loss: 0.0745, label: 1, bag_size: 13255\n",
      "batch 39, loss: 0.0159, instance_loss: 0.0124, weighted_loss: 0.0149, label: 0, bag_size: 15313\n",
      "batch 59, loss: 1.0945, instance_loss: 1.6789, weighted_loss: 1.2699, label: 1, bag_size: 9404\n",
      "batch 79, loss: 0.1654, instance_loss: 0.1643, weighted_loss: 0.1651, label: 0, bag_size: 14828\n",
      "batch 99, loss: 0.0466, instance_loss: 0.0413, weighted_loss: 0.0450, label: 1, bag_size: 9004\n",
      "batch 119, loss: 0.0004, instance_loss: 0.0013, weighted_loss: 0.0007, label: 0, bag_size: 2748\n",
      "batch 139, loss: 0.3462, instance_loss: 0.5132, weighted_loss: 0.3963, label: 0, bag_size: 10410\n",
      "batch 159, loss: 0.0671, instance_loss: 0.0630, weighted_loss: 0.0658, label: 1, bag_size: 29832\n",
      "batch 179, loss: 0.0017, instance_loss: 0.0007, weighted_loss: 0.0014, label: 1, bag_size: 9408\n",
      "batch 199, loss: 0.1305, instance_loss: 0.1333, weighted_loss: 0.1313, label: 0, bag_size: 11281\n",
      "batch 219, loss: 0.0051, instance_loss: 0.0036, weighted_loss: 0.0047, label: 0, bag_size: 19466\n",
      "batch 239, loss: 0.0988, instance_loss: 0.0927, weighted_loss: 0.0969, label: 0, bag_size: 2351\n",
      "batch 259, loss: 0.0158, instance_loss: 0.0113, weighted_loss: 0.0145, label: 1, bag_size: 10432\n",
      "batch 279, loss: 0.2167, instance_loss: 0.3583, weighted_loss: 0.2592, label: 0, bag_size: 2104\n",
      "batch 299, loss: 0.6362, instance_loss: 0.6993, weighted_loss: 0.6551, label: 1, bag_size: 2179\n",
      "batch 319, loss: 0.1128, instance_loss: 0.1829, weighted_loss: 0.1338, label: 0, bag_size: 5999\n",
      "batch 339, loss: 0.1477, instance_loss: 0.1189, weighted_loss: 0.1390, label: 1, bag_size: 2785\n",
      "batch 359, loss: 0.3471, instance_loss: 0.5504, weighted_loss: 0.4081, label: 1, bag_size: 1339\n",
      "batch 379, loss: 0.0184, instance_loss: 0.0437, weighted_loss: 0.0260, label: 1, bag_size: 9446\n",
      "batch 399, loss: 0.0187, instance_loss: 0.0088, weighted_loss: 0.0157, label: 1, bag_size: 2140\n",
      "batch 419, loss: 0.0083, instance_loss: 0.0169, weighted_loss: 0.0109, label: 0, bag_size: 10146\n",
      "batch 439, loss: 0.0311, instance_loss: 0.0324, weighted_loss: 0.0315, label: 0, bag_size: 25814\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0452, weighted_loss: 0.0136, label: 0, bag_size: 18225\n",
      "batch 479, loss: 0.0455, instance_loss: 0.0362, weighted_loss: 0.0427, label: 0, bag_size: 11922\n",
      "batch 499, loss: 0.0003, instance_loss: 0.0023, weighted_loss: 0.0009, label: 1, bag_size: 3437\n",
      "batch 519, loss: 0.0148, instance_loss: 0.0060, weighted_loss: 0.0122, label: 1, bag_size: 6606\n",
      "batch 539, loss: 0.4561, instance_loss: 0.6813, weighted_loss: 0.5237, label: 0, bag_size: 2998\n",
      "batch 559, loss: 0.0479, instance_loss: 0.0488, weighted_loss: 0.0482, label: 0, bag_size: 6898\n",
      "batch 579, loss: 0.0025, instance_loss: 0.0009, weighted_loss: 0.0020, label: 0, bag_size: 14266\n",
      "batch 599, loss: 0.0015, instance_loss: 0.0006, weighted_loss: 0.0012, label: 1, bag_size: 13255\n",
      "batch 619, loss: 0.0428, instance_loss: 0.0371, weighted_loss: 0.0411, label: 0, bag_size: 9542\n",
      "batch 639, loss: 0.0667, instance_loss: 0.0602, weighted_loss: 0.0648, label: 0, bag_size: 9616\n",
      "batch 659, loss: 0.1222, instance_loss: 1.3445, weighted_loss: 0.4889, label: 0, bag_size: 2179\n",
      "batch 679, loss: 0.0559, instance_loss: 0.3992, weighted_loss: 0.1589, label: 0, bag_size: 14956\n",
      "batch 699, loss: 0.3095, instance_loss: 0.5353, weighted_loss: 0.3773, label: 1, bag_size: 7424\n",
      "batch 719, loss: 0.0642, instance_loss: 0.1475, weighted_loss: 0.0892, label: 0, bag_size: 7011\n",
      "batch 739, loss: 0.0089, instance_loss: 0.0119, weighted_loss: 0.0098, label: 1, bag_size: 2344\n",
      "batch 759, loss: 0.0510, instance_loss: 0.0500, weighted_loss: 0.0507, label: 0, bag_size: 8582\n",
      "batch 779, loss: 0.0409, instance_loss: 0.0419, weighted_loss: 0.0412, label: 0, bag_size: 10995\n",
      "batch 799, loss: 0.0057, instance_loss: 0.0083, weighted_loss: 0.0065, label: 0, bag_size: 2044\n",
      "batch 819, loss: 0.0029, instance_loss: 0.0018, weighted_loss: 0.0026, label: 1, bag_size: 12178\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9682926829268292: correct 12704/13120\n",
      "class 1 clustering acc 0.8452743902439024: correct 5545/6560\n",
      "Epoch: 18, train_loss: 0.2413, train_clustering_loss:  0.3112, train_error: 0.0866\n",
      "class 0: acc 0.9296116504854369, correct 383/412\n",
      "class 1: acc 0.8970588235294118, correct 366/408\n",
      "\n",
      "Val Set, val_loss: 0.4333, val_error: 0.1909, auc: 0.9705\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.07045454545454545: correct 62/880\n",
      "class 0: acc 0.6153846153846154, correct 32/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0972, instance_loss: 0.0668, weighted_loss: 0.0881, label: 0, bag_size: 10029\n",
      "batch 39, loss: 0.0027, instance_loss: 0.0028, weighted_loss: 0.0027, label: 1, bag_size: 7371\n",
      "batch 59, loss: 0.0085, instance_loss: 0.0056, weighted_loss: 0.0076, label: 1, bag_size: 13026\n",
      "batch 79, loss: 0.0048, instance_loss: 0.0013, weighted_loss: 0.0038, label: 1, bag_size: 6343\n",
      "batch 99, loss: 0.2085, instance_loss: 0.1496, weighted_loss: 0.1908, label: 1, bag_size: 5723\n",
      "batch 119, loss: 0.3434, instance_loss: 0.3379, weighted_loss: 0.3417, label: 1, bag_size: 1822\n",
      "batch 139, loss: 0.0322, instance_loss: 0.0316, weighted_loss: 0.0320, label: 1, bag_size: 3652\n",
      "batch 159, loss: 0.0003, instance_loss: 0.0006, weighted_loss: 0.0004, label: 1, bag_size: 4862\n",
      "batch 179, loss: 0.0006, instance_loss: 0.0027, weighted_loss: 0.0012, label: 0, bag_size: 8866\n",
      "batch 199, loss: 0.0730, instance_loss: 0.0591, weighted_loss: 0.0688, label: 1, bag_size: 1759\n",
      "batch 219, loss: 0.0006, instance_loss: 0.0035, weighted_loss: 0.0015, label: 0, bag_size: 13892\n",
      "batch 239, loss: 0.0083, instance_loss: 0.0014, weighted_loss: 0.0062, label: 1, bag_size: 7935\n",
      "batch 259, loss: 0.0618, instance_loss: 0.0260, weighted_loss: 0.0511, label: 0, bag_size: 1234\n",
      "batch 279, loss: 0.0175, instance_loss: 0.0294, weighted_loss: 0.0211, label: 1, bag_size: 1638\n",
      "batch 299, loss: 0.0034, instance_loss: 0.0009, weighted_loss: 0.0026, label: 0, bag_size: 21076\n",
      "batch 319, loss: 0.0384, instance_loss: 0.0181, weighted_loss: 0.0323, label: 0, bag_size: 2351\n",
      "batch 339, loss: 1.0252, instance_loss: 1.3725, weighted_loss: 1.1294, label: 1, bag_size: 7468\n",
      "batch 359, loss: 0.1576, instance_loss: 0.2478, weighted_loss: 0.1847, label: 0, bag_size: 2242\n",
      "batch 379, loss: 2.7910, instance_loss: 3.2198, weighted_loss: 2.9197, label: 0, bag_size: 5211\n",
      "batch 399, loss: 0.0051, instance_loss: 0.0054, weighted_loss: 0.0052, label: 0, bag_size: 17268\n",
      "batch 419, loss: 0.0038, instance_loss: 0.0025, weighted_loss: 0.0034, label: 0, bag_size: 11654\n",
      "batch 439, loss: 0.0015, instance_loss: 0.0003, weighted_loss: 0.0011, label: 0, bag_size: 12524\n",
      "batch 459, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 19518\n",
      "batch 479, loss: 0.0089, instance_loss: 0.0051, weighted_loss: 0.0077, label: 0, bag_size: 9542\n",
      "batch 499, loss: 0.0841, instance_loss: 0.0687, weighted_loss: 0.0795, label: 1, bag_size: 5345\n",
      "batch 519, loss: 4.3171, instance_loss: 4.4353, weighted_loss: 4.3526, label: 0, bag_size: 3468\n",
      "batch 539, loss: 0.0140, instance_loss: 0.0232, weighted_loss: 0.0168, label: 1, bag_size: 8660\n",
      "batch 559, loss: 0.0014, instance_loss: 0.0021, weighted_loss: 0.0016, label: 0, bag_size: 11727\n",
      "batch 579, loss: 0.0043, instance_loss: 0.0074, weighted_loss: 0.0052, label: 0, bag_size: 13880\n",
      "batch 599, loss: 1.9652, instance_loss: 2.2090, weighted_loss: 2.0384, label: 1, bag_size: 1638\n",
      "batch 619, loss: 0.0839, instance_loss: 0.0570, weighted_loss: 0.0759, label: 0, bag_size: 2266\n",
      "batch 639, loss: 1.0239, instance_loss: 1.1486, weighted_loss: 1.0613, label: 1, bag_size: 9215\n",
      "batch 659, loss: 0.0139, instance_loss: 0.0109, weighted_loss: 0.0130, label: 1, bag_size: 3640\n",
      "batch 679, loss: 0.5744, instance_loss: 0.6012, weighted_loss: 0.5824, label: 1, bag_size: 8982\n",
      "batch 699, loss: 0.1120, instance_loss: 0.2099, weighted_loss: 0.1414, label: 1, bag_size: 2678\n",
      "batch 719, loss: 0.0162, instance_loss: 0.0044, weighted_loss: 0.0126, label: 1, bag_size: 30675\n",
      "batch 739, loss: 0.0015, instance_loss: 0.0044, weighted_loss: 0.0024, label: 0, bag_size: 8898\n",
      "batch 759, loss: 0.0556, instance_loss: 0.0155, weighted_loss: 0.0436, label: 1, bag_size: 12575\n",
      "batch 779, loss: 0.0087, instance_loss: 0.0144, weighted_loss: 0.0104, label: 1, bag_size: 16051\n",
      "batch 799, loss: 0.0026, instance_loss: 0.0025, weighted_loss: 0.0026, label: 1, bag_size: 8448\n",
      "batch 819, loss: 0.0104, instance_loss: 0.0105, weighted_loss: 0.0105, label: 0, bag_size: 13992\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9754573170731707: correct 12798/13120\n",
      "class 1 clustering acc 0.8815548780487805: correct 5783/6560\n",
      "Epoch: 19, train_loss: 0.2204, train_clustering_loss:  0.2592, train_error: 0.0732\n",
      "class 0: acc 0.9225181598062954, correct 381/413\n",
      "class 1: acc 0.9312039312039312, correct 379/407\n",
      "\n",
      "Val Set, val_loss: 0.2278, val_error: 0.1000, auc: 0.9708\n",
      "class 0 clustering acc 0.9926136363636363: correct 1747/1760\n",
      "class 1 clustering acc 0.1: correct 88/880\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 0.896551724137931, correct 52/58\n",
      "Validation loss decreased (0.236387 --> 0.227805).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0226, instance_loss: 0.0079, weighted_loss: 0.0182, label: 1, bag_size: 10622\n",
      "batch 39, loss: 0.0027, instance_loss: 0.0075, weighted_loss: 0.0042, label: 0, bag_size: 1370\n",
      "batch 59, loss: 0.0116, instance_loss: 0.0122, weighted_loss: 0.0118, label: 0, bag_size: 9930\n",
      "batch 79, loss: 0.0008, instance_loss: 0.0011, weighted_loss: 0.0009, label: 0, bag_size: 15313\n",
      "batch 99, loss: 0.0093, instance_loss: 0.0064, weighted_loss: 0.0084, label: 1, bag_size: 9732\n",
      "batch 119, loss: 0.0010, instance_loss: 0.0009, weighted_loss: 0.0010, label: 1, bag_size: 7513\n",
      "batch 139, loss: 0.0075, instance_loss: 0.0009, weighted_loss: 0.0055, label: 1, bag_size: 9561\n",
      "batch 159, loss: 0.0005, instance_loss: 0.0031, weighted_loss: 0.0013, label: 1, bag_size: 6453\n",
      "batch 179, loss: 0.0217, instance_loss: 0.0109, weighted_loss: 0.0185, label: 1, bag_size: 10460\n",
      "batch 199, loss: 0.0215, instance_loss: 0.0199, weighted_loss: 0.0210, label: 0, bag_size: 12083\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0001, weighted_loss: 0.0002, label: 0, bag_size: 4497\n",
      "batch 239, loss: 0.0017, instance_loss: 0.0009, weighted_loss: 0.0014, label: 0, bag_size: 2270\n",
      "batch 259, loss: 0.0100, instance_loss: 0.0073, weighted_loss: 0.0092, label: 0, bag_size: 3502\n",
      "batch 279, loss: 0.0529, instance_loss: 0.0403, weighted_loss: 0.0491, label: 1, bag_size: 2480\n",
      "batch 299, loss: 0.3579, instance_loss: 0.7543, weighted_loss: 0.4768, label: 0, bag_size: 1800\n",
      "batch 319, loss: 0.1066, instance_loss: 0.1034, weighted_loss: 0.1057, label: 0, bag_size: 11212\n",
      "batch 339, loss: 0.0137, instance_loss: 0.0036, weighted_loss: 0.0107, label: 1, bag_size: 10867\n",
      "batch 359, loss: 0.0051, instance_loss: 0.0142, weighted_loss: 0.0078, label: 1, bag_size: 11981\n",
      "batch 379, loss: 0.0014, instance_loss: 0.0025, weighted_loss: 0.0017, label: 0, bag_size: 1797\n",
      "batch 399, loss: 0.0009, instance_loss: 0.0027, weighted_loss: 0.0015, label: 0, bag_size: 3908\n",
      "batch 419, loss: 0.0258, instance_loss: 0.0218, weighted_loss: 0.0246, label: 1, bag_size: 18603\n",
      "batch 439, loss: 0.0016, instance_loss: 0.0140, weighted_loss: 0.0053, label: 1, bag_size: 1022\n",
      "batch 459, loss: 3.2627, instance_loss: 3.3617, weighted_loss: 3.2924, label: 0, bag_size: 7835\n",
      "batch 479, loss: 0.0040, instance_loss: 0.0018, weighted_loss: 0.0033, label: 0, bag_size: 11727\n",
      "batch 499, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0011, label: 1, bag_size: 6875\n",
      "batch 519, loss: 1.7788, instance_loss: 2.0605, weighted_loss: 1.8633, label: 1, bag_size: 3121\n",
      "batch 539, loss: 0.0011, instance_loss: 0.0011, weighted_loss: 0.0011, label: 0, bag_size: 18954\n",
      "batch 559, loss: 0.3809, instance_loss: 0.3591, weighted_loss: 0.3744, label: 0, bag_size: 11607\n",
      "batch 579, loss: 2.5149, instance_loss: 2.6150, weighted_loss: 2.5450, label: 1, bag_size: 2681\n",
      "batch 599, loss: 0.0081, instance_loss: 0.0079, weighted_loss: 0.0081, label: 1, bag_size: 10105\n",
      "batch 619, loss: 0.0037, instance_loss: 0.0052, weighted_loss: 0.0042, label: 1, bag_size: 11600\n",
      "batch 639, loss: 0.0302, instance_loss: 0.0270, weighted_loss: 0.0292, label: 1, bag_size: 1294\n",
      "batch 659, loss: 0.0009, instance_loss: 0.0012, weighted_loss: 0.0010, label: 0, bag_size: 1213\n",
      "batch 679, loss: 0.2297, instance_loss: 0.3340, weighted_loss: 0.2610, label: 0, bag_size: 2303\n",
      "batch 699, loss: 0.0519, instance_loss: 0.0350, weighted_loss: 0.0468, label: 0, bag_size: 17630\n",
      "batch 719, loss: 0.1561, instance_loss: 0.2616, weighted_loss: 0.1877, label: 0, bag_size: 1370\n",
      "batch 739, loss: 0.0158, instance_loss: 0.0290, weighted_loss: 0.0198, label: 1, bag_size: 9689\n",
      "batch 759, loss: 0.0024, instance_loss: 0.0104, weighted_loss: 0.0048, label: 1, bag_size: 9971\n",
      "batch 779, loss: 0.0029, instance_loss: 0.0068, weighted_loss: 0.0041, label: 0, bag_size: 1452\n",
      "batch 799, loss: 1.5299, instance_loss: 2.0648, weighted_loss: 1.6903, label: 0, bag_size: 7428\n",
      "batch 819, loss: 0.0967, instance_loss: 0.1020, weighted_loss: 0.0983, label: 1, bag_size: 7389\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9723323170731707: correct 12757/13120\n",
      "class 1 clustering acc 0.8754573170731708: correct 5743/6560\n",
      "Epoch: 20, train_loss: 0.2119, train_clustering_loss:  0.2561, train_error: 0.0805\n",
      "class 0: acc 0.9254807692307693, correct 385/416\n",
      "class 1: acc 0.9133663366336634, correct 369/404\n",
      "\n",
      "Val Set, val_loss: 0.2213, val_error: 0.1000, auc: 0.9712\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8448275862068966, correct 49/58\n",
      "Validation loss decreased (0.227805 --> 0.221276).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0022, instance_loss: 0.0029, weighted_loss: 0.0024, label: 1, bag_size: 11389\n",
      "batch 39, loss: 0.0006, instance_loss: 0.0019, weighted_loss: 0.0010, label: 0, bag_size: 15077\n",
      "batch 59, loss: 0.0459, instance_loss: 0.0146, weighted_loss: 0.0365, label: 1, bag_size: 18603\n",
      "batch 79, loss: 4.6311, instance_loss: 4.8708, weighted_loss: 4.7030, label: 0, bag_size: 3802\n",
      "batch 99, loss: 0.3854, instance_loss: 0.5270, weighted_loss: 0.4279, label: 1, bag_size: 13440\n",
      "batch 119, loss: 0.0601, instance_loss: 0.0508, weighted_loss: 0.0573, label: 1, bag_size: 8040\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 11390\n",
      "batch 159, loss: 0.0790, instance_loss: 0.0321, weighted_loss: 0.0649, label: 1, bag_size: 9408\n",
      "batch 179, loss: 0.0079, instance_loss: 0.0095, weighted_loss: 0.0084, label: 0, bag_size: 14319\n",
      "batch 199, loss: 0.0540, instance_loss: 0.0530, weighted_loss: 0.0537, label: 0, bag_size: 763\n",
      "batch 219, loss: 0.0289, instance_loss: 0.1861, weighted_loss: 0.0761, label: 1, bag_size: 10920\n",
      "batch 239, loss: 0.0032, instance_loss: 0.0049, weighted_loss: 0.0037, label: 0, bag_size: 2844\n",
      "batch 259, loss: 0.0009, instance_loss: 0.0007, weighted_loss: 0.0008, label: 0, bag_size: 15003\n",
      "batch 279, loss: 0.8882, instance_loss: 1.4887, weighted_loss: 1.0683, label: 1, bag_size: 9942\n",
      "batch 299, loss: 0.0033, instance_loss: 0.0228, weighted_loss: 0.0092, label: 0, bag_size: 2534\n",
      "batch 319, loss: 0.1189, instance_loss: 0.1680, weighted_loss: 0.1336, label: 0, bag_size: 1072\n",
      "batch 339, loss: 0.0038, instance_loss: 0.0000, weighted_loss: 0.0027, label: 0, bag_size: 9786\n",
      "batch 359, loss: 0.6208, instance_loss: 1.0051, weighted_loss: 0.7361, label: 0, bag_size: 8420\n",
      "batch 379, loss: 0.0033, instance_loss: 0.0000, weighted_loss: 0.0023, label: 0, bag_size: 8755\n",
      "batch 399, loss: 0.1462, instance_loss: 1.2209, weighted_loss: 0.4687, label: 0, bag_size: 4598\n",
      "batch 419, loss: 0.0049, instance_loss: 0.0028, weighted_loss: 0.0043, label: 0, bag_size: 803\n",
      "batch 439, loss: 0.0030, instance_loss: 0.0058, weighted_loss: 0.0039, label: 0, bag_size: 931\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 15093\n",
      "batch 479, loss: 0.2020, instance_loss: 0.0965, weighted_loss: 0.1703, label: 1, bag_size: 1888\n",
      "batch 499, loss: 0.0039, instance_loss: 0.0704, weighted_loss: 0.0238, label: 1, bag_size: 16051\n",
      "batch 519, loss: 0.5352, instance_loss: 0.5687, weighted_loss: 0.5452, label: 1, bag_size: 2681\n",
      "batch 539, loss: 0.0274, instance_loss: 0.0155, weighted_loss: 0.0238, label: 1, bag_size: 6842\n",
      "batch 559, loss: 0.0373, instance_loss: 0.0376, weighted_loss: 0.0374, label: 0, bag_size: 9930\n",
      "batch 579, loss: 0.9022, instance_loss: 1.3861, weighted_loss: 1.0473, label: 1, bag_size: 13440\n",
      "batch 599, loss: 0.0387, instance_loss: 0.0674, weighted_loss: 0.0473, label: 0, bag_size: 1684\n",
      "batch 619, loss: 0.0217, instance_loss: 0.0152, weighted_loss: 0.0197, label: 1, bag_size: 9533\n",
      "batch 639, loss: 0.0073, instance_loss: 0.0073, weighted_loss: 0.0073, label: 0, bag_size: 3810\n",
      "batch 659, loss: 0.0591, instance_loss: 0.0531, weighted_loss: 0.0573, label: 1, bag_size: 12758\n",
      "batch 679, loss: 0.0702, instance_loss: 0.0719, weighted_loss: 0.0707, label: 0, bag_size: 23791\n",
      "batch 699, loss: 0.0073, instance_loss: 0.0044, weighted_loss: 0.0065, label: 0, bag_size: 15003\n",
      "batch 719, loss: 0.0021, instance_loss: 0.0003, weighted_loss: 0.0016, label: 1, bag_size: 9732\n",
      "batch 739, loss: 0.7490, instance_loss: 0.8007, weighted_loss: 0.7645, label: 1, bag_size: 16514\n",
      "batch 759, loss: 0.0262, instance_loss: 0.0253, weighted_loss: 0.0259, label: 0, bag_size: 1416\n",
      "batch 779, loss: 0.0063, instance_loss: 0.0074, weighted_loss: 0.0066, label: 0, bag_size: 11546\n",
      "batch 799, loss: 0.0196, instance_loss: 0.0068, weighted_loss: 0.0157, label: 1, bag_size: 18095\n",
      "batch 819, loss: 0.4700, instance_loss: 0.8105, weighted_loss: 0.5721, label: 0, bag_size: 2653\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9696646341463414: correct 12722/13120\n",
      "class 1 clustering acc 0.864329268292683: correct 5670/6560\n",
      "Epoch: 21, train_loss: 0.2413, train_clustering_loss:  0.2956, train_error: 0.0805\n",
      "class 0: acc 0.9204545454545454, correct 405/440\n",
      "class 1: acc 0.9184210526315789, correct 349/380\n",
      "\n",
      "Val Set, val_loss: 0.2756, val_error: 0.1091, auc: 0.9735\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8275862068965517, correct 48/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0045, instance_loss: 0.0280, weighted_loss: 0.0115, label: 1, bag_size: 10592\n",
      "batch 39, loss: 0.0037, instance_loss: 0.0017, weighted_loss: 0.0031, label: 1, bag_size: 14887\n",
      "batch 59, loss: 2.3592, instance_loss: 2.9634, weighted_loss: 2.5405, label: 0, bag_size: 15898\n",
      "batch 79, loss: 0.8020, instance_loss: 0.9058, weighted_loss: 0.8331, label: 1, bag_size: 1819\n",
      "batch 99, loss: 0.0184, instance_loss: 0.0087, weighted_loss: 0.0155, label: 0, bag_size: 10490\n",
      "batch 119, loss: 0.0064, instance_loss: 0.0081, weighted_loss: 0.0069, label: 1, bag_size: 1051\n",
      "batch 139, loss: 0.0002, instance_loss: 0.0035, weighted_loss: 0.0012, label: 0, bag_size: 1984\n",
      "batch 159, loss: 0.6223, instance_loss: 0.9951, weighted_loss: 0.7341, label: 1, bag_size: 1497\n",
      "batch 179, loss: 0.0070, instance_loss: 0.0242, weighted_loss: 0.0121, label: 0, bag_size: 1560\n",
      "batch 199, loss: 0.0041, instance_loss: 0.0097, weighted_loss: 0.0057, label: 0, bag_size: 18954\n",
      "batch 219, loss: 0.0019, instance_loss: 0.0008, weighted_loss: 0.0015, label: 1, bag_size: 20161\n",
      "batch 239, loss: 0.0011, instance_loss: 0.0048, weighted_loss: 0.0022, label: 0, bag_size: 5409\n",
      "batch 259, loss: 0.0024, instance_loss: 0.0022, weighted_loss: 0.0023, label: 1, bag_size: 9408\n",
      "batch 279, loss: 0.0200, instance_loss: 0.0117, weighted_loss: 0.0175, label: 1, bag_size: 6731\n",
      "batch 299, loss: 0.0066, instance_loss: 0.0058, weighted_loss: 0.0064, label: 1, bag_size: 9878\n",
      "batch 319, loss: 0.0381, instance_loss: 0.0345, weighted_loss: 0.0370, label: 0, bag_size: 9171\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0029, weighted_loss: 0.0010, label: 0, bag_size: 5965\n",
      "batch 359, loss: 0.0007, instance_loss: 0.0007, weighted_loss: 0.0007, label: 1, bag_size: 11964\n",
      "batch 379, loss: 0.0002, instance_loss: 0.0006, weighted_loss: 0.0003, label: 1, bag_size: 4877\n",
      "batch 399, loss: 0.2234, instance_loss: 0.3973, weighted_loss: 0.2755, label: 1, bag_size: 1755\n",
      "batch 419, loss: 0.2977, instance_loss: 0.2609, weighted_loss: 0.2867, label: 0, bag_size: 21864\n",
      "batch 439, loss: 0.0887, instance_loss: 0.0861, weighted_loss: 0.0879, label: 0, bag_size: 2360\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 15077\n",
      "batch 479, loss: 3.0085, instance_loss: 3.7180, weighted_loss: 3.2213, label: 0, bag_size: 3468\n",
      "batch 499, loss: 0.0014, instance_loss: 0.0010, weighted_loss: 0.0013, label: 0, bag_size: 5551\n",
      "batch 519, loss: 0.0028, instance_loss: 0.0010, weighted_loss: 0.0022, label: 0, bag_size: 18240\n",
      "batch 539, loss: 0.0168, instance_loss: 0.0210, weighted_loss: 0.0180, label: 0, bag_size: 2044\n",
      "batch 559, loss: 0.2775, instance_loss: 0.7672, weighted_loss: 0.4244, label: 1, bag_size: 1437\n",
      "batch 579, loss: 0.0243, instance_loss: 0.0106, weighted_loss: 0.0202, label: 1, bag_size: 5494\n",
      "batch 599, loss: 0.0002, instance_loss: 0.0007, weighted_loss: 0.0004, label: 0, bag_size: 11735\n",
      "batch 619, loss: 0.0094, instance_loss: 0.0070, weighted_loss: 0.0087, label: 0, bag_size: 8145\n",
      "batch 639, loss: 0.0412, instance_loss: 0.0170, weighted_loss: 0.0340, label: 0, bag_size: 1349\n",
      "batch 659, loss: 0.0008, instance_loss: 0.0006, weighted_loss: 0.0007, label: 0, bag_size: 15077\n",
      "batch 679, loss: 0.0298, instance_loss: 0.0264, weighted_loss: 0.0288, label: 1, bag_size: 1051\n",
      "batch 699, loss: 0.2910, instance_loss: 0.3457, weighted_loss: 0.3074, label: 1, bag_size: 5723\n",
      "batch 719, loss: 0.0401, instance_loss: 0.0262, weighted_loss: 0.0360, label: 0, bag_size: 3444\n",
      "batch 739, loss: 0.0015, instance_loss: 0.0022, weighted_loss: 0.0017, label: 0, bag_size: 13795\n",
      "batch 759, loss: 0.2544, instance_loss: 0.2583, weighted_loss: 0.2556, label: 0, bag_size: 1149\n",
      "batch 779, loss: 0.0861, instance_loss: 0.0095, weighted_loss: 0.0631, label: 0, bag_size: 9069\n",
      "batch 799, loss: 0.5145, instance_loss: 0.2024, weighted_loss: 0.4208, label: 1, bag_size: 20161\n",
      "batch 819, loss: 0.0022, instance_loss: 0.0537, weighted_loss: 0.0177, label: 1, bag_size: 17486\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9720274390243903: correct 12753/13120\n",
      "class 1 clustering acc 0.8618902439024391: correct 5654/6560\n",
      "Epoch: 22, train_loss: 0.2333, train_clustering_loss:  0.2757, train_error: 0.0915\n",
      "class 0: acc 0.916256157635468, correct 372/406\n",
      "class 1: acc 0.9009661835748792, correct 373/414\n",
      "\n",
      "Val Set, val_loss: 0.2203, val_error: 0.0818, auc: 0.9738\n",
      "class 0 clustering acc 0.9363636363636364: correct 1648/1760\n",
      "class 1 clustering acc 0.35568181818181815: correct 313/880\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "Validation loss decreased (0.221276 --> 0.220261).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0019, instance_loss: 0.0007, weighted_loss: 0.0016, label: 0, bag_size: 13964\n",
      "batch 39, loss: 0.0171, instance_loss: 0.0118, weighted_loss: 0.0155, label: 1, bag_size: 5454\n",
      "batch 59, loss: 0.0003, instance_loss: 0.0312, weighted_loss: 0.0095, label: 1, bag_size: 4442\n",
      "batch 79, loss: 2.5706, instance_loss: 2.6830, weighted_loss: 2.6043, label: 0, bag_size: 2959\n",
      "batch 99, loss: 0.0206, instance_loss: 0.0368, weighted_loss: 0.0255, label: 1, bag_size: 12425\n",
      "batch 119, loss: 1.9367, instance_loss: 2.4521, weighted_loss: 2.0913, label: 0, bag_size: 23618\n",
      "batch 139, loss: 0.0188, instance_loss: 0.0249, weighted_loss: 0.0206, label: 0, bag_size: 2873\n",
      "batch 159, loss: 0.0057, instance_loss: 0.0044, weighted_loss: 0.0053, label: 0, bag_size: 22870\n",
      "batch 179, loss: 0.0323, instance_loss: 0.0186, weighted_loss: 0.0282, label: 1, bag_size: 21701\n",
      "batch 199, loss: 0.0369, instance_loss: 0.0422, weighted_loss: 0.0385, label: 0, bag_size: 9455\n",
      "batch 219, loss: 1.5077, instance_loss: 2.0012, weighted_loss: 1.6557, label: 0, bag_size: 1732\n",
      "batch 239, loss: 0.0010, instance_loss: 0.0017, weighted_loss: 0.0012, label: 0, bag_size: 10995\n",
      "batch 259, loss: 0.2426, instance_loss: 0.3914, weighted_loss: 0.2872, label: 0, bag_size: 3375\n",
      "batch 279, loss: 0.0003, instance_loss: 0.0003, weighted_loss: 0.0003, label: 0, bag_size: 2654\n",
      "batch 299, loss: 0.0144, instance_loss: 0.0211, weighted_loss: 0.0165, label: 0, bag_size: 3970\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0042, weighted_loss: 0.0013, label: 1, bag_size: 6792\n",
      "batch 339, loss: 0.0569, instance_loss: 0.0790, weighted_loss: 0.0635, label: 0, bag_size: 8582\n",
      "batch 359, loss: 0.0580, instance_loss: 0.0266, weighted_loss: 0.0485, label: 1, bag_size: 9078\n",
      "batch 379, loss: 0.0045, instance_loss: 0.0072, weighted_loss: 0.0053, label: 0, bag_size: 16341\n",
      "batch 399, loss: 0.0129, instance_loss: 0.0148, weighted_loss: 0.0135, label: 1, bag_size: 3409\n",
      "batch 419, loss: 0.0685, instance_loss: 0.1061, weighted_loss: 0.0798, label: 0, bag_size: 2873\n",
      "batch 439, loss: 0.0017, instance_loss: 0.0017, weighted_loss: 0.0017, label: 1, bag_size: 9971\n",
      "batch 459, loss: 0.0081, instance_loss: 0.0050, weighted_loss: 0.0072, label: 0, bag_size: 5999\n",
      "batch 479, loss: 0.0016, instance_loss: 0.0002, weighted_loss: 0.0012, label: 0, bag_size: 18154\n",
      "batch 499, loss: 0.0085, instance_loss: 0.0070, weighted_loss: 0.0081, label: 1, bag_size: 2356\n",
      "batch 519, loss: 0.0152, instance_loss: 0.0094, weighted_loss: 0.0135, label: 1, bag_size: 4394\n",
      "batch 539, loss: 1.1117, instance_loss: 1.3365, weighted_loss: 1.1791, label: 1, bag_size: 21252\n",
      "batch 559, loss: 0.0256, instance_loss: 0.0217, weighted_loss: 0.0244, label: 0, bag_size: 1881\n",
      "batch 579, loss: 0.4843, instance_loss: 0.6875, weighted_loss: 0.5452, label: 0, bag_size: 2266\n",
      "batch 599, loss: 0.0171, instance_loss: 0.0145, weighted_loss: 0.0163, label: 0, bag_size: 5009\n",
      "batch 619, loss: 0.0275, instance_loss: 0.0209, weighted_loss: 0.0255, label: 0, bag_size: 2367\n",
      "batch 639, loss: 0.0667, instance_loss: 0.1313, weighted_loss: 0.0861, label: 0, bag_size: 10535\n",
      "batch 659, loss: 0.7377, instance_loss: 1.3172, weighted_loss: 0.9115, label: 0, bag_size: 15898\n",
      "batch 679, loss: 0.7026, instance_loss: 0.6576, weighted_loss: 0.6891, label: 0, bag_size: 19043\n",
      "batch 699, loss: 0.3695, instance_loss: 1.2075, weighted_loss: 0.6209, label: 1, bag_size: 13174\n",
      "batch 719, loss: 0.0979, instance_loss: 0.2549, weighted_loss: 0.1450, label: 1, bag_size: 13947\n",
      "batch 739, loss: 0.0203, instance_loss: 0.0558, weighted_loss: 0.0309, label: 0, bag_size: 31780\n",
      "batch 759, loss: 0.3324, instance_loss: 0.4273, weighted_loss: 0.3609, label: 0, bag_size: 3541\n",
      "batch 779, loss: 0.1622, instance_loss: 0.0695, weighted_loss: 0.1344, label: 0, bag_size: 2104\n",
      "batch 799, loss: 0.0067, instance_loss: 0.1329, weighted_loss: 0.0446, label: 1, bag_size: 13015\n",
      "batch 819, loss: 0.0004, instance_loss: 0.0003, weighted_loss: 0.0003, label: 0, bag_size: 16211\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9657774390243903: correct 12671/13120\n",
      "class 1 clustering acc 0.8342987804878049: correct 5473/6560\n",
      "Epoch: 23, train_loss: 0.2305, train_clustering_loss:  0.3073, train_error: 0.0927\n",
      "class 0: acc 0.9133489461358314, correct 390/427\n",
      "class 1: acc 0.9007633587786259, correct 354/393\n",
      "\n",
      "Val Set, val_loss: 0.6595, val_error: 0.1818, auc: 0.9675\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 1.0, correct 52/52\n",
      "class 1: acc 0.6551724137931034, correct 38/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0184, instance_loss: 0.0365, weighted_loss: 0.0239, label: 1, bag_size: 5155\n",
      "batch 39, loss: 0.4016, instance_loss: 0.3548, weighted_loss: 0.3875, label: 0, bag_size: 10381\n",
      "batch 59, loss: 0.0019, instance_loss: 0.0156, weighted_loss: 0.0060, label: 1, bag_size: 4862\n",
      "batch 79, loss: 0.0018, instance_loss: 0.0076, weighted_loss: 0.0035, label: 1, bag_size: 11964\n",
      "batch 99, loss: 0.4728, instance_loss: 0.4747, weighted_loss: 0.4734, label: 1, bag_size: 12340\n",
      "batch 119, loss: 0.0217, instance_loss: 0.0148, weighted_loss: 0.0196, label: 1, bag_size: 1823\n",
      "batch 139, loss: 0.0002, instance_loss: 0.0007, weighted_loss: 0.0003, label: 0, bag_size: 11865\n",
      "batch 159, loss: 0.0542, instance_loss: 0.1496, weighted_loss: 0.0828, label: 1, bag_size: 9478\n",
      "batch 179, loss: 0.0079, instance_loss: 0.0067, weighted_loss: 0.0076, label: 0, bag_size: 16087\n",
      "batch 199, loss: 0.2502, instance_loss: 0.3291, weighted_loss: 0.2738, label: 0, bag_size: 2815\n",
      "batch 219, loss: 0.2672, instance_loss: 0.2793, weighted_loss: 0.2708, label: 0, bag_size: 2006\n",
      "batch 239, loss: 0.0100, instance_loss: 0.0095, weighted_loss: 0.0099, label: 1, bag_size: 13026\n",
      "batch 259, loss: 2.2386, instance_loss: 3.0569, weighted_loss: 2.4841, label: 0, bag_size: 17279\n",
      "batch 279, loss: 0.0155, instance_loss: 0.0324, weighted_loss: 0.0206, label: 1, bag_size: 7246\n",
      "batch 299, loss: 0.0910, instance_loss: 0.2529, weighted_loss: 0.1396, label: 0, bag_size: 3101\n",
      "batch 319, loss: 0.5425, instance_loss: 0.6632, weighted_loss: 0.5787, label: 1, bag_size: 2937\n",
      "batch 339, loss: 0.0067, instance_loss: 0.0016, weighted_loss: 0.0052, label: 1, bag_size: 12349\n",
      "batch 359, loss: 0.0055, instance_loss: 0.0053, weighted_loss: 0.0055, label: 0, bag_size: 3190\n",
      "batch 379, loss: 0.2191, instance_loss: 0.2050, weighted_loss: 0.2149, label: 1, bag_size: 16890\n",
      "batch 399, loss: 0.6133, instance_loss: 0.8556, weighted_loss: 0.6860, label: 1, bag_size: 16548\n",
      "batch 419, loss: 0.0235, instance_loss: 0.0174, weighted_loss: 0.0217, label: 1, bag_size: 9147\n",
      "batch 439, loss: 0.3107, instance_loss: 0.1765, weighted_loss: 0.2705, label: 1, bag_size: 1512\n",
      "batch 459, loss: 0.0350, instance_loss: 0.0522, weighted_loss: 0.0402, label: 1, bag_size: 7981\n",
      "batch 479, loss: 0.0035, instance_loss: 0.0040, weighted_loss: 0.0037, label: 0, bag_size: 2063\n",
      "batch 499, loss: 0.2047, instance_loss: 0.1409, weighted_loss: 0.1855, label: 1, bag_size: 2455\n",
      "batch 519, loss: 4.6978, instance_loss: 4.3348, weighted_loss: 4.5889, label: 0, bag_size: 4692\n",
      "batch 539, loss: 0.0062, instance_loss: 0.0031, weighted_loss: 0.0052, label: 0, bag_size: 15313\n",
      "batch 559, loss: 0.0051, instance_loss: 0.0066, weighted_loss: 0.0056, label: 1, bag_size: 8522\n",
      "batch 579, loss: 0.0147, instance_loss: 0.0150, weighted_loss: 0.0148, label: 0, bag_size: 12593\n",
      "batch 599, loss: 0.0442, instance_loss: 0.0337, weighted_loss: 0.0411, label: 0, bag_size: 21093\n",
      "batch 619, loss: 0.1279, instance_loss: 0.1369, weighted_loss: 0.1306, label: 1, bag_size: 20767\n",
      "batch 639, loss: 0.0340, instance_loss: 0.0321, weighted_loss: 0.0335, label: 1, bag_size: 15689\n",
      "batch 659, loss: 0.1065, instance_loss: 0.0939, weighted_loss: 0.1027, label: 1, bag_size: 7351\n",
      "batch 679, loss: 0.3360, instance_loss: 0.3415, weighted_loss: 0.3376, label: 1, bag_size: 1525\n",
      "batch 699, loss: 6.1405, instance_loss: 4.8991, weighted_loss: 5.7681, label: 1, bag_size: 2565\n",
      "batch 719, loss: 0.0202, instance_loss: 0.0187, weighted_loss: 0.0198, label: 1, bag_size: 6927\n",
      "batch 739, loss: 0.0753, instance_loss: 0.0730, weighted_loss: 0.0746, label: 1, bag_size: 5921\n",
      "batch 759, loss: 1.4485, instance_loss: 1.8565, weighted_loss: 1.5709, label: 1, bag_size: 6360\n",
      "batch 779, loss: 0.0314, instance_loss: 0.0184, weighted_loss: 0.0275, label: 1, bag_size: 9230\n",
      "batch 799, loss: 0.0076, instance_loss: 0.0027, weighted_loss: 0.0061, label: 1, bag_size: 12795\n",
      "batch 819, loss: 0.0644, instance_loss: 0.0443, weighted_loss: 0.0583, label: 1, bag_size: 2682\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9712652439024391: correct 12743/13120\n",
      "class 1 clustering acc 0.8753048780487804: correct 5742/6560\n",
      "Epoch: 24, train_loss: 0.2243, train_clustering_loss:  0.2601, train_error: 0.0829\n",
      "class 0: acc 0.9108910891089109, correct 368/404\n",
      "class 1: acc 0.9230769230769231, correct 384/416\n",
      "\n",
      "Val Set, val_loss: 0.2477, val_error: 0.1182, auc: 0.9748\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.7884615384615384, correct 41/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0013, instance_loss: 0.0037, weighted_loss: 0.0020, label: 1, bag_size: 13051\n",
      "batch 39, loss: 0.0464, instance_loss: 0.0580, weighted_loss: 0.0499, label: 1, bag_size: 6734\n",
      "batch 59, loss: 0.0107, instance_loss: 0.0069, weighted_loss: 0.0095, label: 1, bag_size: 10501\n",
      "batch 79, loss: 0.0018, instance_loss: 0.0029, weighted_loss: 0.0021, label: 0, bag_size: 3787\n",
      "batch 99, loss: 0.0218, instance_loss: 0.0125, weighted_loss: 0.0190, label: 1, bag_size: 2136\n",
      "batch 119, loss: 0.0038, instance_loss: 0.0047, weighted_loss: 0.0041, label: 0, bag_size: 5551\n",
      "batch 139, loss: 1.6095, instance_loss: 1.9279, weighted_loss: 1.7050, label: 1, bag_size: 2678\n",
      "batch 159, loss: 0.0243, instance_loss: 0.0079, weighted_loss: 0.0193, label: 0, bag_size: 15313\n",
      "batch 179, loss: 0.0426, instance_loss: 0.0355, weighted_loss: 0.0405, label: 0, bag_size: 5409\n",
      "batch 199, loss: 0.0044, instance_loss: 0.0043, weighted_loss: 0.0044, label: 0, bag_size: 1984\n",
      "batch 219, loss: 0.0119, instance_loss: 0.0040, weighted_loss: 0.0095, label: 0, bag_size: 10995\n",
      "batch 239, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 25420\n",
      "batch 259, loss: 0.0077, instance_loss: 0.0234, weighted_loss: 0.0124, label: 1, bag_size: 5516\n",
      "batch 279, loss: 0.0148, instance_loss: 0.0100, weighted_loss: 0.0133, label: 0, bag_size: 12910\n",
      "batch 299, loss: 0.0027, instance_loss: 0.0027, weighted_loss: 0.0027, label: 0, bag_size: 15747\n",
      "batch 319, loss: 0.0617, instance_loss: 0.0568, weighted_loss: 0.0602, label: 0, bag_size: 21361\n",
      "batch 339, loss: 0.0140, instance_loss: 0.0171, weighted_loss: 0.0149, label: 1, bag_size: 9533\n",
      "batch 359, loss: 0.0323, instance_loss: 0.0205, weighted_loss: 0.0288, label: 0, bag_size: 6652\n",
      "batch 379, loss: 0.0023, instance_loss: 0.0015, weighted_loss: 0.0020, label: 0, bag_size: 18415\n",
      "batch 399, loss: 1.6165, instance_loss: 2.0697, weighted_loss: 1.7525, label: 0, bag_size: 2653\n",
      "batch 419, loss: 0.4068, instance_loss: 0.3773, weighted_loss: 0.3979, label: 0, bag_size: 9596\n",
      "batch 439, loss: 0.1486, instance_loss: 0.1375, weighted_loss: 0.1452, label: 1, bag_size: 2842\n",
      "batch 459, loss: 0.3409, instance_loss: 0.3856, weighted_loss: 0.3543, label: 1, bag_size: 3368\n",
      "batch 479, loss: 0.1079, instance_loss: 0.0771, weighted_loss: 0.0987, label: 0, bag_size: 2004\n",
      "batch 499, loss: 0.1541, instance_loss: 0.2151, weighted_loss: 0.1724, label: 1, bag_size: 2678\n",
      "batch 519, loss: 0.4185, instance_loss: 0.4404, weighted_loss: 0.4250, label: 0, bag_size: 2219\n",
      "batch 539, loss: 0.0105, instance_loss: 0.0074, weighted_loss: 0.0096, label: 1, bag_size: 6606\n",
      "batch 559, loss: 0.0109, instance_loss: 0.0056, weighted_loss: 0.0093, label: 1, bag_size: 12931\n",
      "batch 579, loss: 0.0709, instance_loss: 0.0705, weighted_loss: 0.0708, label: 1, bag_size: 14779\n",
      "batch 599, loss: 0.1289, instance_loss: 0.1530, weighted_loss: 0.1361, label: 1, bag_size: 6726\n",
      "batch 619, loss: 0.0838, instance_loss: 0.1076, weighted_loss: 0.0909, label: 0, bag_size: 10304\n",
      "batch 639, loss: 0.0043, instance_loss: 0.0086, weighted_loss: 0.0056, label: 0, bag_size: 19518\n",
      "batch 659, loss: 1.2641, instance_loss: 1.3355, weighted_loss: 1.2855, label: 0, bag_size: 21361\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0036, weighted_loss: 0.0011, label: 0, bag_size: 8948\n",
      "batch 699, loss: 0.1488, instance_loss: 0.1808, weighted_loss: 0.1584, label: 0, bag_size: 16052\n",
      "batch 719, loss: 0.0022, instance_loss: 0.0021, weighted_loss: 0.0022, label: 0, bag_size: 2548\n",
      "batch 739, loss: 0.0054, instance_loss: 0.0029, weighted_loss: 0.0047, label: 1, bag_size: 6606\n",
      "batch 759, loss: 0.2486, instance_loss: 0.3132, weighted_loss: 0.2680, label: 1, bag_size: 7768\n",
      "batch 779, loss: 0.0024, instance_loss: 0.0002, weighted_loss: 0.0017, label: 1, bag_size: 5025\n",
      "batch 799, loss: 0.0038, instance_loss: 0.0024, weighted_loss: 0.0034, label: 1, bag_size: 15008\n",
      "batch 819, loss: 1.3732, instance_loss: 1.6438, weighted_loss: 1.4544, label: 0, bag_size: 1498\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.968140243902439: correct 12702/13120\n",
      "class 1 clustering acc 0.8605182926829268: correct 5645/6560\n",
      "Epoch: 25, train_loss: 0.2358, train_clustering_loss:  0.2769, train_error: 0.0939\n",
      "class 0: acc 0.9113300492610837, correct 370/406\n",
      "class 1: acc 0.9009661835748792, correct 373/414\n",
      "\n",
      "Val Set, val_loss: 0.4143, val_error: 0.2000, auc: 0.9715\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.5961538461538461, correct 31/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6360, instance_loss: 0.7239, weighted_loss: 0.6624, label: 0, bag_size: 21361\n",
      "batch 39, loss: 0.0785, instance_loss: 0.1420, weighted_loss: 0.0975, label: 0, bag_size: 1891\n",
      "batch 59, loss: 0.0019, instance_loss: 0.0139, weighted_loss: 0.0055, label: 1, bag_size: 5221\n",
      "batch 79, loss: 0.1495, instance_loss: 0.2437, weighted_loss: 0.1778, label: 1, bag_size: 5366\n",
      "batch 99, loss: 0.0099, instance_loss: 0.0079, weighted_loss: 0.0093, label: 0, bag_size: 9851\n",
      "batch 119, loss: 0.1166, instance_loss: 0.1429, weighted_loss: 0.1245, label: 1, bag_size: 2137\n",
      "batch 139, loss: 0.0644, instance_loss: 0.0712, weighted_loss: 0.0664, label: 0, bag_size: 1614\n",
      "batch 159, loss: 0.4820, instance_loss: 0.5946, weighted_loss: 0.5157, label: 1, bag_size: 1051\n",
      "batch 179, loss: 0.2486, instance_loss: 0.2840, weighted_loss: 0.2592, label: 1, bag_size: 1609\n",
      "batch 199, loss: 0.0252, instance_loss: 0.0215, weighted_loss: 0.0241, label: 0, bag_size: 17268\n",
      "batch 219, loss: 0.0284, instance_loss: 0.0241, weighted_loss: 0.0271, label: 0, bag_size: 10898\n",
      "batch 239, loss: 0.0151, instance_loss: 0.0146, weighted_loss: 0.0150, label: 0, bag_size: 10791\n",
      "batch 259, loss: 0.0786, instance_loss: 0.1001, weighted_loss: 0.0850, label: 1, bag_size: 4786\n",
      "batch 279, loss: 0.0074, instance_loss: 0.0032, weighted_loss: 0.0061, label: 0, bag_size: 890\n",
      "batch 299, loss: 0.0005, instance_loss: 0.0006, weighted_loss: 0.0005, label: 0, bag_size: 9069\n",
      "batch 319, loss: 0.0146, instance_loss: 0.0109, weighted_loss: 0.0135, label: 0, bag_size: 20796\n",
      "batch 339, loss: 0.0042, instance_loss: 0.0060, weighted_loss: 0.0048, label: 1, bag_size: 10969\n",
      "batch 359, loss: 0.0242, instance_loss: 0.0114, weighted_loss: 0.0204, label: 1, bag_size: 13732\n",
      "batch 379, loss: 0.0004, instance_loss: 0.0006, weighted_loss: 0.0005, label: 1, bag_size: 6966\n",
      "batch 399, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 15003\n",
      "batch 419, loss: 0.0006, instance_loss: 0.0025, weighted_loss: 0.0011, label: 1, bag_size: 9673\n",
      "batch 439, loss: 0.0075, instance_loss: 0.0060, weighted_loss: 0.0070, label: 0, bag_size: 10263\n",
      "batch 459, loss: 0.0831, instance_loss: 0.0963, weighted_loss: 0.0870, label: 1, bag_size: 5137\n",
      "batch 479, loss: 0.0127, instance_loss: 0.0097, weighted_loss: 0.0118, label: 0, bag_size: 11146\n",
      "batch 499, loss: 0.0221, instance_loss: 0.0158, weighted_loss: 0.0202, label: 1, bag_size: 1014\n",
      "batch 519, loss: 1.2575, instance_loss: 1.7269, weighted_loss: 1.3983, label: 1, bag_size: 8103\n",
      "batch 539, loss: 0.0259, instance_loss: 0.0161, weighted_loss: 0.0230, label: 1, bag_size: 7389\n",
      "batch 559, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 19472\n",
      "batch 579, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 13964\n",
      "batch 599, loss: 0.0018, instance_loss: 0.0001, weighted_loss: 0.0013, label: 0, bag_size: 19067\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0049, weighted_loss: 0.0015, label: 1, bag_size: 5612\n",
      "batch 639, loss: 0.0018, instance_loss: 0.0026, weighted_loss: 0.0021, label: 1, bag_size: 3640\n",
      "batch 659, loss: 0.0062, instance_loss: 0.0046, weighted_loss: 0.0057, label: 1, bag_size: 14887\n",
      "batch 679, loss: 0.6879, instance_loss: 0.7375, weighted_loss: 0.7028, label: 0, bag_size: 11390\n",
      "batch 699, loss: 0.0037, instance_loss: 0.0030, weighted_loss: 0.0035, label: 1, bag_size: 1512\n",
      "batch 719, loss: 0.0002, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 3552\n",
      "batch 739, loss: 0.0111, instance_loss: 0.0043, weighted_loss: 0.0090, label: 0, bag_size: 10751\n",
      "batch 759, loss: 0.0052, instance_loss: 0.0040, weighted_loss: 0.0049, label: 1, bag_size: 11600\n",
      "batch 779, loss: 0.1584, instance_loss: 0.1848, weighted_loss: 0.1663, label: 0, bag_size: 2006\n",
      "batch 799, loss: 0.0212, instance_loss: 0.0213, weighted_loss: 0.0213, label: 0, bag_size: 1588\n",
      "batch 819, loss: 0.1791, instance_loss: 0.2536, weighted_loss: 0.2015, label: 0, bag_size: 2548\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9738567073170732: correct 12777/13120\n",
      "class 1 clustering acc 0.875: correct 5740/6560\n",
      "Epoch: 26, train_loss: 0.2093, train_clustering_loss:  0.2427, train_error: 0.0780\n",
      "class 0: acc 0.9325301204819277, correct 387/415\n",
      "class 1: acc 0.9111111111111111, correct 369/405\n",
      "\n",
      "Val Set, val_loss: 0.4125, val_error: 0.1636, auc: 0.9741\n",
      "class 0 clustering acc 0.9880681818181818: correct 1739/1760\n",
      "class 1 clustering acc 0.11136363636363636: correct 98/880\n",
      "class 0: acc 0.6923076923076923, correct 36/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3173, instance_loss: 0.5043, weighted_loss: 0.3734, label: 0, bag_size: 9387\n",
      "batch 39, loss: 0.0189, instance_loss: 0.0100, weighted_loss: 0.0162, label: 0, bag_size: 705\n",
      "batch 59, loss: 0.3770, instance_loss: 0.3778, weighted_loss: 0.3772, label: 1, bag_size: 5903\n",
      "batch 79, loss: 0.0913, instance_loss: 0.0698, weighted_loss: 0.0849, label: 0, bag_size: 6652\n",
      "batch 99, loss: 0.0019, instance_loss: 0.0002, weighted_loss: 0.0014, label: 0, bag_size: 9888\n",
      "batch 119, loss: 0.0115, instance_loss: 0.0092, weighted_loss: 0.0108, label: 1, bag_size: 19972\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0057, weighted_loss: 0.0018, label: 1, bag_size: 3224\n",
      "batch 159, loss: 0.0066, instance_loss: 0.0039, weighted_loss: 0.0058, label: 0, bag_size: 3774\n",
      "batch 179, loss: 0.0607, instance_loss: 0.0384, weighted_loss: 0.0540, label: 1, bag_size: 2140\n",
      "batch 199, loss: 0.0015, instance_loss: 0.0006, weighted_loss: 0.0012, label: 0, bag_size: 7612\n",
      "batch 219, loss: 0.0006, instance_loss: 0.0005, weighted_loss: 0.0006, label: 1, bag_size: 10396\n",
      "batch 239, loss: 1.9732, instance_loss: 2.4516, weighted_loss: 2.1167, label: 0, bag_size: 2070\n",
      "batch 259, loss: 0.0008, instance_loss: 0.0015, weighted_loss: 0.0010, label: 0, bag_size: 15313\n",
      "batch 279, loss: 0.0700, instance_loss: 0.0590, weighted_loss: 0.0667, label: 0, bag_size: 1458\n",
      "batch 299, loss: 0.0055, instance_loss: 0.0028, weighted_loss: 0.0047, label: 0, bag_size: 3810\n",
      "batch 319, loss: 0.0077, instance_loss: 0.0068, weighted_loss: 0.0075, label: 1, bag_size: 2140\n",
      "batch 339, loss: 0.1910, instance_loss: 0.1961, weighted_loss: 0.1925, label: 0, bag_size: 2006\n",
      "batch 359, loss: 0.0273, instance_loss: 0.0298, weighted_loss: 0.0281, label: 0, bag_size: 1745\n",
      "batch 379, loss: 0.0016, instance_loss: 0.0008, weighted_loss: 0.0013, label: 0, bag_size: 11759\n",
      "batch 399, loss: 0.3956, instance_loss: 0.4586, weighted_loss: 0.4145, label: 1, bag_size: 5155\n",
      "batch 419, loss: 0.0364, instance_loss: 0.0298, weighted_loss: 0.0344, label: 1, bag_size: 9533\n",
      "batch 439, loss: 0.0026, instance_loss: 0.0016, weighted_loss: 0.0023, label: 0, bag_size: 10146\n",
      "batch 459, loss: 0.0549, instance_loss: 0.0391, weighted_loss: 0.0502, label: 1, bag_size: 13365\n",
      "batch 479, loss: 0.1300, instance_loss: 0.1173, weighted_loss: 0.1262, label: 1, bag_size: 1014\n",
      "batch 499, loss: 0.0908, instance_loss: 0.0612, weighted_loss: 0.0819, label: 0, bag_size: 1438\n",
      "batch 519, loss: 1.9377, instance_loss: 2.8929, weighted_loss: 2.2242, label: 0, bag_size: 5211\n",
      "batch 539, loss: 0.0012, instance_loss: 0.0002, weighted_loss: 0.0009, label: 0, bag_size: 16992\n",
      "batch 559, loss: 0.3882, instance_loss: 0.6255, weighted_loss: 0.4594, label: 1, bag_size: 13440\n",
      "batch 579, loss: 0.0051, instance_loss: 0.0033, weighted_loss: 0.0045, label: 1, bag_size: 10492\n",
      "batch 599, loss: 0.0023, instance_loss: 0.0025, weighted_loss: 0.0024, label: 1, bag_size: 9470\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 0, bag_size: 6652\n",
      "batch 639, loss: 1.6391, instance_loss: 2.3319, weighted_loss: 1.8469, label: 1, bag_size: 13089\n",
      "batch 659, loss: 0.3392, instance_loss: 0.3895, weighted_loss: 0.3543, label: 1, bag_size: 8103\n",
      "batch 679, loss: 0.0156, instance_loss: 0.0080, weighted_loss: 0.0134, label: 1, bag_size: 6343\n",
      "batch 699, loss: 0.0582, instance_loss: 0.0289, weighted_loss: 0.0494, label: 1, bag_size: 3674\n",
      "batch 719, loss: 0.1334, instance_loss: 0.1288, weighted_loss: 0.1320, label: 0, bag_size: 13992\n",
      "batch 739, loss: 1.0474, instance_loss: 1.0762, weighted_loss: 1.0560, label: 1, bag_size: 13440\n",
      "batch 759, loss: 0.0009, instance_loss: 0.0003, weighted_loss: 0.0008, label: 0, bag_size: 3552\n",
      "batch 779, loss: 0.1329, instance_loss: 0.1090, weighted_loss: 0.1257, label: 1, bag_size: 10072\n",
      "batch 799, loss: 0.1364, instance_loss: 0.1065, weighted_loss: 0.1275, label: 0, bag_size: 2091\n",
      "batch 819, loss: 0.0150, instance_loss: 0.0172, weighted_loss: 0.0157, label: 1, bag_size: 13194\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9708079268292683: correct 12737/13120\n",
      "class 1 clustering acc 0.8620426829268293: correct 5655/6560\n",
      "Epoch: 27, train_loss: 0.2342, train_clustering_loss:  0.2756, train_error: 0.0890\n",
      "class 0: acc 0.9093137254901961, correct 371/408\n",
      "class 1: acc 0.912621359223301, correct 376/412\n",
      "\n",
      "Val Set, val_loss: 0.1996, val_error: 0.0909, auc: 0.9778\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.01818181818181818: correct 16/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "Validation loss decreased (0.220261 --> 0.199585).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0141, instance_loss: 0.0290, weighted_loss: 0.0186, label: 0, bag_size: 12593\n",
      "batch 39, loss: 0.0295, instance_loss: 0.0317, weighted_loss: 0.0302, label: 0, bag_size: 3198\n",
      "batch 59, loss: 0.0202, instance_loss: 0.0091, weighted_loss: 0.0169, label: 1, bag_size: 16267\n",
      "batch 79, loss: 0.0009, instance_loss: 0.0021, weighted_loss: 0.0013, label: 1, bag_size: 15093\n",
      "batch 99, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 1, bag_size: 4259\n",
      "batch 119, loss: 0.0041, instance_loss: 0.0027, weighted_loss: 0.0037, label: 0, bag_size: 2063\n",
      "batch 139, loss: 0.0242, instance_loss: 0.4637, weighted_loss: 0.1560, label: 1, bag_size: 2662\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0766, weighted_loss: 0.0231, label: 0, bag_size: 518\n",
      "batch 179, loss: 0.0070, instance_loss: 0.0018, weighted_loss: 0.0054, label: 0, bag_size: 11199\n",
      "batch 199, loss: 0.0043, instance_loss: 0.0037, weighted_loss: 0.0041, label: 1, bag_size: 1638\n",
      "batch 219, loss: 0.1283, instance_loss: 0.1374, weighted_loss: 0.1310, label: 1, bag_size: 4786\n",
      "batch 239, loss: 0.0503, instance_loss: 0.0276, weighted_loss: 0.0435, label: 0, bag_size: 1506\n",
      "batch 259, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 5317\n",
      "batch 279, loss: 0.0026, instance_loss: 0.0017, weighted_loss: 0.0023, label: 1, bag_size: 12795\n",
      "batch 299, loss: 0.0024, instance_loss: 0.0015, weighted_loss: 0.0022, label: 1, bag_size: 11394\n",
      "batch 319, loss: 0.0021, instance_loss: 0.0007, weighted_loss: 0.0017, label: 0, bag_size: 30751\n",
      "batch 339, loss: 0.0985, instance_loss: 0.0656, weighted_loss: 0.0886, label: 1, bag_size: 22264\n",
      "batch 359, loss: 0.1053, instance_loss: 0.0609, weighted_loss: 0.0920, label: 0, bag_size: 9930\n",
      "batch 379, loss: 0.0110, instance_loss: 0.0077, weighted_loss: 0.0100, label: 1, bag_size: 2495\n",
      "batch 399, loss: 0.0004, instance_loss: 0.0054, weighted_loss: 0.0019, label: 1, bag_size: 4880\n",
      "batch 419, loss: 0.0045, instance_loss: 0.0079, weighted_loss: 0.0055, label: 1, bag_size: 1051\n",
      "batch 439, loss: 0.0166, instance_loss: 0.0103, weighted_loss: 0.0147, label: 0, bag_size: 1483\n",
      "batch 459, loss: 0.0016, instance_loss: 0.0022, weighted_loss: 0.0017, label: 1, bag_size: 1255\n",
      "batch 479, loss: 0.3056, instance_loss: 0.4002, weighted_loss: 0.3340, label: 0, bag_size: 15898\n",
      "batch 499, loss: 0.0015, instance_loss: 0.0028, weighted_loss: 0.0019, label: 1, bag_size: 15233\n",
      "batch 519, loss: 0.0286, instance_loss: 0.0211, weighted_loss: 0.0264, label: 0, bag_size: 8582\n",
      "batch 539, loss: 0.0045, instance_loss: 0.0034, weighted_loss: 0.0042, label: 0, bag_size: 8959\n",
      "batch 559, loss: 0.0205, instance_loss: 0.0226, weighted_loss: 0.0211, label: 1, bag_size: 689\n",
      "batch 579, loss: 0.0171, instance_loss: 0.0132, weighted_loss: 0.0159, label: 0, bag_size: 3190\n",
      "batch 599, loss: 0.1225, instance_loss: 0.1301, weighted_loss: 0.1248, label: 0, bag_size: 21864\n",
      "batch 619, loss: 2.0731, instance_loss: 2.5164, weighted_loss: 2.2061, label: 0, bag_size: 9616\n",
      "batch 639, loss: 0.0309, instance_loss: 0.0181, weighted_loss: 0.0271, label: 0, bag_size: 11259\n",
      "batch 659, loss: 0.0123, instance_loss: 0.0100, weighted_loss: 0.0116, label: 0, bag_size: 16211\n",
      "batch 679, loss: 0.0201, instance_loss: 0.0222, weighted_loss: 0.0207, label: 1, bag_size: 1255\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0074, weighted_loss: 0.0023, label: 1, bag_size: 4877\n",
      "batch 719, loss: 0.0124, instance_loss: 0.0182, weighted_loss: 0.0142, label: 1, bag_size: 2904\n",
      "batch 739, loss: 0.0172, instance_loss: 0.0092, weighted_loss: 0.0148, label: 0, bag_size: 13880\n",
      "batch 759, loss: 0.9891, instance_loss: 1.4107, weighted_loss: 1.1156, label: 0, bag_size: 7428\n",
      "batch 779, loss: 0.0389, instance_loss: 0.0388, weighted_loss: 0.0389, label: 1, bag_size: 11316\n",
      "batch 799, loss: 0.6646, instance_loss: 0.9581, weighted_loss: 0.7526, label: 0, bag_size: 2959\n",
      "batch 819, loss: 0.0097, instance_loss: 0.0054, weighted_loss: 0.0084, label: 1, bag_size: 8754\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9772865853658537: correct 12822/13120\n",
      "class 1 clustering acc 0.879420731707317: correct 5769/6560\n",
      "Epoch: 28, train_loss: 0.1992, train_clustering_loss:  0.2343, train_error: 0.0732\n",
      "class 0: acc 0.9292929292929293, correct 368/396\n",
      "class 1: acc 0.9245283018867925, correct 392/424\n",
      "\n",
      "Val Set, val_loss: 0.2319, val_error: 0.1000, auc: 0.9695\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0035, instance_loss: 0.0156, weighted_loss: 0.0071, label: 1, bag_size: 5864\n",
      "batch 39, loss: 0.0023, instance_loss: 0.0008, weighted_loss: 0.0018, label: 0, bag_size: 8981\n",
      "batch 59, loss: 0.0002, instance_loss: 0.0007, weighted_loss: 0.0003, label: 1, bag_size: 14202\n",
      "batch 79, loss: 0.0300, instance_loss: 0.0214, weighted_loss: 0.0274, label: 0, bag_size: 1814\n",
      "batch 99, loss: 0.0957, instance_loss: 0.1061, weighted_loss: 0.0988, label: 0, bag_size: 2351\n",
      "batch 119, loss: 0.0196, instance_loss: 0.0200, weighted_loss: 0.0198, label: 0, bag_size: 12793\n",
      "batch 139, loss: 0.0028, instance_loss: 0.0037, weighted_loss: 0.0031, label: 0, bag_size: 14266\n",
      "batch 159, loss: 0.0068, instance_loss: 0.0081, weighted_loss: 0.0072, label: 0, bag_size: 8025\n",
      "batch 179, loss: 0.0967, instance_loss: 0.0893, weighted_loss: 0.0944, label: 0, bag_size: 1508\n",
      "batch 199, loss: 0.0072, instance_loss: 0.0066, weighted_loss: 0.0070, label: 1, bag_size: 2495\n",
      "batch 219, loss: 0.0040, instance_loss: 0.0014, weighted_loss: 0.0032, label: 0, bag_size: 47866\n",
      "batch 239, loss: 0.0451, instance_loss: 0.0414, weighted_loss: 0.0440, label: 0, bag_size: 1772\n",
      "batch 259, loss: 0.8438, instance_loss: 1.1852, weighted_loss: 0.9462, label: 0, bag_size: 6281\n",
      "batch 279, loss: 0.0005, instance_loss: 0.0007, weighted_loss: 0.0006, label: 0, bag_size: 10751\n",
      "batch 299, loss: 0.0035, instance_loss: 0.0035, weighted_loss: 0.0035, label: 1, bag_size: 5340\n",
      "batch 319, loss: 0.0000, instance_loss: 0.1383, weighted_loss: 0.0415, label: 1, bag_size: 2385\n",
      "batch 339, loss: 0.0058, instance_loss: 0.0596, weighted_loss: 0.0219, label: 1, bag_size: 621\n",
      "batch 359, loss: 0.0144, instance_loss: 0.0063, weighted_loss: 0.0120, label: 0, bag_size: 3265\n",
      "batch 379, loss: 0.0022, instance_loss: 0.0005, weighted_loss: 0.0017, label: 1, bag_size: 7389\n",
      "batch 399, loss: 0.1834, instance_loss: 0.3292, weighted_loss: 0.2272, label: 0, bag_size: 11390\n",
      "batch 419, loss: 0.0066, instance_loss: 0.0021, weighted_loss: 0.0052, label: 0, bag_size: 13777\n",
      "batch 439, loss: 0.1080, instance_loss: 0.0883, weighted_loss: 0.1021, label: 1, bag_size: 9162\n",
      "batch 459, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 1881\n",
      "batch 479, loss: 0.0011, instance_loss: 0.0028, weighted_loss: 0.0016, label: 1, bag_size: 2136\n",
      "batch 499, loss: 0.5862, instance_loss: 0.6322, weighted_loss: 0.6000, label: 0, bag_size: 9387\n",
      "batch 519, loss: 3.2733, instance_loss: 4.5014, weighted_loss: 3.6417, label: 0, bag_size: 2815\n",
      "batch 539, loss: 0.0016, instance_loss: 0.0123, weighted_loss: 0.0048, label: 0, bag_size: 65728\n",
      "batch 559, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 11512\n",
      "batch 579, loss: 0.0294, instance_loss: 0.0241, weighted_loss: 0.0278, label: 1, bag_size: 11729\n",
      "batch 599, loss: 0.0131, instance_loss: 0.0076, weighted_loss: 0.0115, label: 1, bag_size: 5690\n",
      "batch 619, loss: 0.0780, instance_loss: 0.0915, weighted_loss: 0.0820, label: 0, bag_size: 2004\n",
      "batch 639, loss: 0.0041, instance_loss: 3.1449, weighted_loss: 0.9463, label: 0, bag_size: 3725\n",
      "batch 659, loss: 0.0027, instance_loss: 0.0004, weighted_loss: 0.0020, label: 1, bag_size: 13947\n",
      "batch 679, loss: 0.0035, instance_loss: 0.0051, weighted_loss: 0.0040, label: 1, bag_size: 17769\n",
      "batch 699, loss: 0.0257, instance_loss: 0.0624, weighted_loss: 0.0367, label: 0, bag_size: 24439\n",
      "batch 719, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 1, bag_size: 11642\n",
      "batch 739, loss: 0.0080, instance_loss: 0.0202, weighted_loss: 0.0116, label: 0, bag_size: 10490\n",
      "batch 759, loss: 0.0008, instance_loss: 0.0022, weighted_loss: 0.0012, label: 0, bag_size: 21864\n",
      "batch 779, loss: 0.0041, instance_loss: 0.0082, weighted_loss: 0.0053, label: 1, bag_size: 1064\n",
      "batch 799, loss: 0.0068, instance_loss: 0.0029, weighted_loss: 0.0056, label: 1, bag_size: 15689\n",
      "batch 819, loss: 0.0789, instance_loss: 0.0716, weighted_loss: 0.0767, label: 1, bag_size: 16565\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.977515243902439: correct 12825/13120\n",
      "class 1 clustering acc 0.875609756097561: correct 5744/6560\n",
      "Epoch: 29, train_loss: 0.1707, train_clustering_loss:  0.2470, train_error: 0.0720\n",
      "class 0: acc 0.9267676767676768, correct 367/396\n",
      "class 1: acc 0.9292452830188679, correct 394/424\n",
      "\n",
      "Val Set, val_loss: 0.2310, val_error: 0.1000, auc: 0.9771\n",
      "class 0 clustering acc 0.9863636363636363: correct 1736/1760\n",
      "class 1 clustering acc 0.08977272727272727: correct 79/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8448275862068966, correct 49/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0245, instance_loss: 0.0162, weighted_loss: 0.0220, label: 0, bag_size: 1651\n",
      "batch 39, loss: 0.0296, instance_loss: 0.0316, weighted_loss: 0.0302, label: 0, bag_size: 1920\n",
      "batch 59, loss: 0.0029, instance_loss: 0.0044, weighted_loss: 0.0034, label: 0, bag_size: 12731\n",
      "batch 79, loss: 0.0092, instance_loss: 0.0038, weighted_loss: 0.0076, label: 1, bag_size: 11981\n",
      "batch 99, loss: 0.0004, instance_loss: 0.0039, weighted_loss: 0.0014, label: 0, bag_size: 10898\n",
      "batch 119, loss: 0.0015, instance_loss: 0.0019, weighted_loss: 0.0017, label: 0, bag_size: 11527\n",
      "batch 139, loss: 4.4517, instance_loss: 4.4459, weighted_loss: 4.4499, label: 0, bag_size: 5120\n",
      "batch 159, loss: 0.0294, instance_loss: 0.0193, weighted_loss: 0.0264, label: 1, bag_size: 7110\n",
      "batch 179, loss: 0.0340, instance_loss: 0.0241, weighted_loss: 0.0310, label: 1, bag_size: 4939\n",
      "batch 199, loss: 0.0089, instance_loss: 0.0080, weighted_loss: 0.0086, label: 1, bag_size: 9561\n",
      "batch 219, loss: 0.0501, instance_loss: 0.0574, weighted_loss: 0.0523, label: 0, bag_size: 1416\n",
      "batch 239, loss: 3.0581, instance_loss: 3.3605, weighted_loss: 3.1488, label: 1, bag_size: 2565\n",
      "batch 259, loss: 0.0043, instance_loss: 0.0020, weighted_loss: 0.0036, label: 1, bag_size: 15233\n",
      "batch 279, loss: 0.0037, instance_loss: 0.0069, weighted_loss: 0.0047, label: 0, bag_size: 9069\n",
      "batch 299, loss: 0.0002, instance_loss: 0.0016, weighted_loss: 0.0006, label: 0, bag_size: 9470\n",
      "batch 319, loss: 0.0012, instance_loss: 0.0017, weighted_loss: 0.0013, label: 0, bag_size: 2091\n",
      "batch 339, loss: 0.0008, instance_loss: 0.0004, weighted_loss: 0.0007, label: 1, bag_size: 11256\n",
      "batch 359, loss: 1.2725, instance_loss: 1.7833, weighted_loss: 1.4257, label: 0, bag_size: 11128\n",
      "batch 379, loss: 0.0048, instance_loss: 0.0885, weighted_loss: 0.0299, label: 1, bag_size: 2412\n",
      "batch 399, loss: 0.2124, instance_loss: 0.2259, weighted_loss: 0.2165, label: 0, bag_size: 11259\n",
      "batch 419, loss: 0.0014, instance_loss: 0.0011, weighted_loss: 0.0013, label: 0, bag_size: 22828\n",
      "batch 439, loss: 0.0013, instance_loss: 0.0014, weighted_loss: 0.0013, label: 1, bag_size: 19039\n",
      "batch 459, loss: 0.3376, instance_loss: 0.3652, weighted_loss: 0.3459, label: 0, bag_size: 14664\n",
      "batch 479, loss: 0.0465, instance_loss: 0.0405, weighted_loss: 0.0447, label: 0, bag_size: 8549\n",
      "batch 499, loss: 0.1007, instance_loss: 0.0998, weighted_loss: 0.1004, label: 1, bag_size: 1038\n",
      "batch 519, loss: 4.2312, instance_loss: 4.5817, weighted_loss: 4.3364, label: 1, bag_size: 9162\n",
      "batch 539, loss: 0.0036, instance_loss: 0.0016, weighted_loss: 0.0030, label: 1, bag_size: 12895\n",
      "batch 559, loss: 0.0063, instance_loss: 0.0042, weighted_loss: 0.0057, label: 1, bag_size: 14681\n",
      "batch 579, loss: 0.0050, instance_loss: 0.0040, weighted_loss: 0.0047, label: 1, bag_size: 6317\n",
      "batch 599, loss: 0.0305, instance_loss: 0.0266, weighted_loss: 0.0293, label: 1, bag_size: 5256\n",
      "batch 619, loss: 3.4731, instance_loss: 3.7441, weighted_loss: 3.5544, label: 1, bag_size: 15185\n",
      "batch 639, loss: 0.3169, instance_loss: 0.3257, weighted_loss: 0.3195, label: 0, bag_size: 6624\n",
      "batch 659, loss: 0.0400, instance_loss: 0.0274, weighted_loss: 0.0362, label: 0, bag_size: 2367\n",
      "batch 679, loss: 0.0214, instance_loss: 0.0200, weighted_loss: 0.0210, label: 0, bag_size: 23791\n",
      "batch 699, loss: 0.7199, instance_loss: 1.0536, weighted_loss: 0.8200, label: 1, bag_size: 1095\n",
      "batch 719, loss: 0.0459, instance_loss: 0.0316, weighted_loss: 0.0416, label: 1, bag_size: 771\n",
      "batch 739, loss: 0.3493, instance_loss: 0.4284, weighted_loss: 0.3730, label: 1, bag_size: 12340\n",
      "batch 759, loss: 0.0042, instance_loss: 0.0014, weighted_loss: 0.0034, label: 1, bag_size: 12603\n",
      "batch 779, loss: 0.0109, instance_loss: 0.0129, weighted_loss: 0.0115, label: 1, bag_size: 2137\n",
      "batch 799, loss: 0.0010, instance_loss: 0.0003, weighted_loss: 0.0008, label: 0, bag_size: 13892\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0185, weighted_loss: 0.0056, label: 1, bag_size: 19039\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9788871951219512: correct 12843/13120\n",
      "class 1 clustering acc 0.9: correct 5904/6560\n",
      "Epoch: 30, train_loss: 0.1834, train_clustering_loss:  0.2078, train_error: 0.0610\n",
      "class 0: acc 0.946078431372549, correct 386/408\n",
      "class 1: acc 0.9320388349514563, correct 384/412\n",
      "\n",
      "Val Set, val_loss: 0.4381, val_error: 0.1727, auc: 0.9738\n",
      "class 0 clustering acc 0.9636363636363636: correct 1696/1760\n",
      "class 1 clustering acc 0.10909090909090909: correct 96/880\n",
      "class 0: acc 0.6730769230769231, correct 35/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0280, instance_loss: 0.0232, weighted_loss: 0.0266, label: 1, bag_size: 1746\n",
      "batch 39, loss: 0.0010, instance_loss: 0.0013, weighted_loss: 0.0011, label: 1, bag_size: 6731\n",
      "batch 59, loss: 0.4061, instance_loss: 0.3890, weighted_loss: 0.4009, label: 0, bag_size: 1508\n",
      "batch 79, loss: 0.0151, instance_loss: 0.0108, weighted_loss: 0.0138, label: 1, bag_size: 3968\n",
      "batch 99, loss: 0.0006, instance_loss: 0.0005, weighted_loss: 0.0006, label: 0, bag_size: 15967\n",
      "batch 119, loss: 0.0006, instance_loss: 0.0006, weighted_loss: 0.0006, label: 1, bag_size: 4259\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 518\n",
      "batch 159, loss: 0.0226, instance_loss: 0.0166, weighted_loss: 0.0208, label: 0, bag_size: 16211\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 10481\n",
      "batch 199, loss: 0.0002, instance_loss: 0.0037, weighted_loss: 0.0012, label: 1, bag_size: 4250\n",
      "batch 219, loss: 0.0339, instance_loss: 0.0257, weighted_loss: 0.0314, label: 0, bag_size: 1483\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 5965\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0009, weighted_loss: 0.0003, label: 0, bag_size: 16782\n",
      "batch 279, loss: 0.8843, instance_loss: 1.0952, weighted_loss: 0.9476, label: 0, bag_size: 4523\n",
      "batch 299, loss: 0.0066, instance_loss: 0.0076, weighted_loss: 0.0069, label: 0, bag_size: 11259\n",
      "batch 319, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 15093\n",
      "batch 339, loss: 0.0010, instance_loss: 0.0009, weighted_loss: 0.0010, label: 0, bag_size: 5009\n",
      "batch 359, loss: 0.0004, instance_loss: 0.0002, weighted_loss: 0.0004, label: 1, bag_size: 3968\n",
      "batch 379, loss: 0.0014, instance_loss: 0.0008, weighted_loss: 0.0012, label: 1, bag_size: 12795\n",
      "batch 399, loss: 0.0002, instance_loss: 0.0006, weighted_loss: 0.0003, label: 0, bag_size: 4465\n",
      "batch 419, loss: 0.0641, instance_loss: 0.0355, weighted_loss: 0.0555, label: 1, bag_size: 18649\n",
      "batch 439, loss: 0.0036, instance_loss: 0.0189, weighted_loss: 0.0082, label: 0, bag_size: 8252\n",
      "batch 459, loss: 0.0047, instance_loss: 0.0051, weighted_loss: 0.0048, label: 1, bag_size: 8438\n",
      "batch 479, loss: 0.0077, instance_loss: 0.0103, weighted_loss: 0.0085, label: 0, bag_size: 9069\n",
      "batch 499, loss: 0.0002, instance_loss: 0.0013, weighted_loss: 0.0005, label: 0, bag_size: 3444\n",
      "batch 519, loss: 0.0039, instance_loss: 0.0017, weighted_loss: 0.0032, label: 1, bag_size: 11160\n",
      "batch 539, loss: 0.0055, instance_loss: 0.0073, weighted_loss: 0.0060, label: 0, bag_size: 11759\n",
      "batch 559, loss: 0.0994, instance_loss: 0.0832, weighted_loss: 0.0945, label: 1, bag_size: 11316\n",
      "batch 579, loss: 0.0247, instance_loss: 0.0204, weighted_loss: 0.0234, label: 1, bag_size: 7669\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0005, label: 0, bag_size: 10481\n",
      "batch 619, loss: 0.0008, instance_loss: 0.0021, weighted_loss: 0.0012, label: 0, bag_size: 9234\n",
      "batch 639, loss: 0.3995, instance_loss: 0.4236, weighted_loss: 0.4068, label: 1, bag_size: 20537\n",
      "batch 659, loss: 0.4441, instance_loss: 0.5330, weighted_loss: 0.4708, label: 0, bag_size: 1714\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0064, weighted_loss: 0.0019, label: 0, bag_size: 8661\n",
      "batch 699, loss: 0.4970, instance_loss: 0.8633, weighted_loss: 0.6069, label: 1, bag_size: 771\n",
      "batch 719, loss: 0.0124, instance_loss: 0.0154, weighted_loss: 0.0133, label: 0, bag_size: 18777\n",
      "batch 739, loss: 0.0345, instance_loss: 0.0795, weighted_loss: 0.0480, label: 1, bag_size: 10033\n",
      "batch 759, loss: 0.5663, instance_loss: 0.7601, weighted_loss: 0.6244, label: 1, bag_size: 6360\n",
      "batch 779, loss: 0.0003, instance_loss: 0.0018, weighted_loss: 0.0008, label: 1, bag_size: 6792\n",
      "batch 799, loss: 0.0068, instance_loss: 0.0062, weighted_loss: 0.0066, label: 1, bag_size: 1483\n",
      "batch 819, loss: 0.0468, instance_loss: 0.1811, weighted_loss: 0.0871, label: 1, bag_size: 10725\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9766006097560975: correct 12813/13120\n",
      "class 1 clustering acc 0.8810975609756098: correct 5780/6560\n",
      "Epoch: 31, train_loss: 0.2064, train_clustering_loss:  0.2380, train_error: 0.0756\n",
      "class 0: acc 0.9236453201970444, correct 375/406\n",
      "class 1: acc 0.9251207729468599, correct 383/414\n",
      "\n",
      "Val Set, val_loss: 0.2617, val_error: 0.1091, auc: 0.9655\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5439, instance_loss: 0.9669, weighted_loss: 0.6708, label: 0, bag_size: 4418\n",
      "batch 39, loss: 0.0206, instance_loss: 0.0223, weighted_loss: 0.0211, label: 1, bag_size: 3856\n",
      "batch 59, loss: 0.0038, instance_loss: 0.0029, weighted_loss: 0.0035, label: 1, bag_size: 4259\n",
      "batch 79, loss: 0.1863, instance_loss: 0.2857, weighted_loss: 0.2161, label: 1, bag_size: 6682\n",
      "batch 99, loss: 2.9232, instance_loss: 3.3735, weighted_loss: 3.0583, label: 0, bag_size: 1701\n",
      "batch 119, loss: 0.0050, instance_loss: 0.0038, weighted_loss: 0.0046, label: 0, bag_size: 2732\n",
      "batch 139, loss: 0.0825, instance_loss: 0.0934, weighted_loss: 0.0858, label: 1, bag_size: 549\n",
      "batch 159, loss: 0.0005, instance_loss: 0.0004, weighted_loss: 0.0005, label: 1, bag_size: 3082\n",
      "batch 179, loss: 0.0258, instance_loss: 0.0230, weighted_loss: 0.0250, label: 0, bag_size: 25814\n",
      "batch 199, loss: 0.0005, instance_loss: 0.0018, weighted_loss: 0.0008, label: 1, bag_size: 8466\n",
      "batch 219, loss: 0.0101, instance_loss: 0.0172, weighted_loss: 0.0122, label: 0, bag_size: 13591\n",
      "batch 239, loss: 0.0004, instance_loss: 0.0005, weighted_loss: 0.0005, label: 0, bag_size: 17630\n",
      "batch 259, loss: 0.5862, instance_loss: 0.6350, weighted_loss: 0.6009, label: 0, bag_size: 1814\n",
      "batch 279, loss: 0.0005, instance_loss: 0.0001, weighted_loss: 0.0004, label: 0, bag_size: 2360\n",
      "batch 299, loss: 0.0834, instance_loss: 0.0484, weighted_loss: 0.0729, label: 1, bag_size: 9610\n",
      "batch 319, loss: 0.5803, instance_loss: 0.7448, weighted_loss: 0.6297, label: 1, bag_size: 10072\n",
      "batch 339, loss: 1.1436, instance_loss: 1.2514, weighted_loss: 1.1759, label: 1, bag_size: 9942\n",
      "batch 359, loss: 0.0063, instance_loss: 0.0059, weighted_loss: 0.0061, label: 1, bag_size: 11981\n",
      "batch 379, loss: 0.6445, instance_loss: 0.9226, weighted_loss: 0.7280, label: 0, bag_size: 1953\n",
      "batch 399, loss: 0.1650, instance_loss: 0.1457, weighted_loss: 0.1593, label: 1, bag_size: 1683\n",
      "batch 419, loss: 0.0016, instance_loss: 0.0019, weighted_loss: 0.0017, label: 1, bag_size: 7798\n",
      "batch 439, loss: 0.1013, instance_loss: 0.0937, weighted_loss: 0.0990, label: 0, bag_size: 11281\n",
      "batch 459, loss: 1.4960, instance_loss: 1.6452, weighted_loss: 1.5408, label: 0, bag_size: 18516\n",
      "batch 479, loss: 0.0010, instance_loss: 0.0012, weighted_loss: 0.0011, label: 0, bag_size: 11125\n",
      "batch 499, loss: 0.0518, instance_loss: 0.0485, weighted_loss: 0.0508, label: 1, bag_size: 2678\n",
      "batch 519, loss: 0.0748, instance_loss: 0.0629, weighted_loss: 0.0712, label: 0, bag_size: 3774\n",
      "batch 539, loss: 0.0354, instance_loss: 0.0275, weighted_loss: 0.0330, label: 1, bag_size: 14887\n",
      "batch 559, loss: 0.0033, instance_loss: 0.0004, weighted_loss: 0.0025, label: 1, bag_size: 7246\n",
      "batch 579, loss: 0.0947, instance_loss: 0.0601, weighted_loss: 0.0843, label: 0, bag_size: 24439\n",
      "batch 599, loss: 0.6476, instance_loss: 0.9048, weighted_loss: 0.7248, label: 0, bag_size: 1437\n",
      "batch 619, loss: 0.7887, instance_loss: 1.2574, weighted_loss: 0.9293, label: 0, bag_size: 18516\n",
      "batch 639, loss: 0.3154, instance_loss: 0.3844, weighted_loss: 0.3361, label: 0, bag_size: 11151\n",
      "batch 659, loss: 0.0237, instance_loss: 0.0125, weighted_loss: 0.0203, label: 0, bag_size: 3198\n",
      "batch 679, loss: 0.0011, instance_loss: 0.0003, weighted_loss: 0.0009, label: 0, bag_size: 11654\n",
      "batch 699, loss: 0.0672, instance_loss: 0.0648, weighted_loss: 0.0665, label: 1, bag_size: 21450\n",
      "batch 719, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 12593\n",
      "batch 739, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 2367\n",
      "batch 759, loss: 0.0267, instance_loss: 0.0168, weighted_loss: 0.0237, label: 1, bag_size: 2678\n",
      "batch 779, loss: 0.0357, instance_loss: 0.0739, weighted_loss: 0.0472, label: 0, bag_size: 2070\n",
      "batch 799, loss: 0.0001, instance_loss: 0.0897, weighted_loss: 0.0270, label: 0, bag_size: 19390\n",
      "batch 819, loss: 0.0188, instance_loss: 0.0085, weighted_loss: 0.0157, label: 0, bag_size: 8252\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9780487804878049: correct 12832/13120\n",
      "class 1 clustering acc 0.9016768292682927: correct 5915/6560\n",
      "Epoch: 32, train_loss: 0.1623, train_clustering_loss:  0.2141, train_error: 0.0634\n",
      "class 0: acc 0.9403341288782816, correct 394/419\n",
      "class 1: acc 0.9326683291770573, correct 374/401\n",
      "\n",
      "Val Set, val_loss: 0.4379, val_error: 0.1909, auc: 0.9692\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.6346153846153846, correct 33/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0105, instance_loss: 0.0386, weighted_loss: 0.0189, label: 0, bag_size: 26271\n",
      "batch 39, loss: 0.0019, instance_loss: 0.0248, weighted_loss: 0.0088, label: 1, bag_size: 14618\n",
      "batch 59, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 23714\n",
      "batch 79, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9949\n",
      "batch 99, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 22870\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 7217\n",
      "batch 139, loss: 0.0006, instance_loss: 0.0020, weighted_loss: 0.0010, label: 0, bag_size: 47866\n",
      "batch 159, loss: 0.0069, instance_loss: 0.0050, weighted_loss: 0.0063, label: 1, bag_size: 2136\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 0, bag_size: 2036\n",
      "batch 199, loss: 0.3898, instance_loss: 0.5762, weighted_loss: 0.4457, label: 0, bag_size: 15071\n",
      "batch 219, loss: 0.0348, instance_loss: 0.0041, weighted_loss: 0.0256, label: 1, bag_size: 6734\n",
      "batch 239, loss: 0.0448, instance_loss: 0.1183, weighted_loss: 0.0668, label: 1, bag_size: 10848\n",
      "batch 259, loss: 0.0082, instance_loss: 0.0079, weighted_loss: 0.0081, label: 0, bag_size: 14266\n",
      "batch 279, loss: 0.0011, instance_loss: 0.0001, weighted_loss: 0.0008, label: 0, bag_size: 3908\n",
      "batch 299, loss: 0.4556, instance_loss: 0.6203, weighted_loss: 0.5050, label: 1, bag_size: 3121\n",
      "batch 319, loss: 0.6327, instance_loss: 0.7314, weighted_loss: 0.6623, label: 1, bag_size: 1683\n",
      "batch 339, loss: 0.0096, instance_loss: 0.0089, weighted_loss: 0.0094, label: 1, bag_size: 11363\n",
      "batch 359, loss: 0.2688, instance_loss: 0.4436, weighted_loss: 0.3213, label: 0, bag_size: 21361\n",
      "batch 379, loss: 0.0297, instance_loss: 0.0269, weighted_loss: 0.0289, label: 0, bag_size: 16720\n",
      "batch 399, loss: 0.0877, instance_loss: 0.0720, weighted_loss: 0.0830, label: 0, bag_size: 26208\n",
      "batch 419, loss: 0.0004, instance_loss: 0.0164, weighted_loss: 0.0052, label: 1, bag_size: 12349\n",
      "batch 439, loss: 0.0018, instance_loss: 0.0015, weighted_loss: 0.0017, label: 0, bag_size: 9930\n",
      "batch 459, loss: 0.0002, instance_loss: 0.0135, weighted_loss: 0.0042, label: 0, bag_size: 2624\n",
      "batch 479, loss: 0.0010, instance_loss: 0.0029, weighted_loss: 0.0015, label: 1, bag_size: 29832\n",
      "batch 499, loss: 0.0006, instance_loss: 0.0005, weighted_loss: 0.0006, label: 1, bag_size: 18468\n",
      "batch 519, loss: 0.0005, instance_loss: 0.0014, weighted_loss: 0.0008, label: 1, bag_size: 13051\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 19039\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0006, weighted_loss: 0.0003, label: 0, bag_size: 14206\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0025, weighted_loss: 0.0008, label: 1, bag_size: 11195\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0003, weighted_loss: 0.0002, label: 0, bag_size: 3557\n",
      "batch 619, loss: 0.0009, instance_loss: 0.0007, weighted_loss: 0.0008, label: 0, bag_size: 18954\n",
      "batch 639, loss: 0.0861, instance_loss: 0.0890, weighted_loss: 0.0870, label: 0, bag_size: 14625\n",
      "batch 659, loss: 0.0115, instance_loss: 0.0035, weighted_loss: 0.0091, label: 0, bag_size: 3321\n",
      "batch 679, loss: 0.0011, instance_loss: 0.0008, weighted_loss: 0.0010, label: 1, bag_size: 16565\n",
      "batch 699, loss: 0.0131, instance_loss: 0.0120, weighted_loss: 0.0128, label: 0, bag_size: 14264\n",
      "batch 719, loss: 0.4824, instance_loss: 0.5934, weighted_loss: 0.5157, label: 0, bag_size: 13992\n",
      "batch 739, loss: 0.0001, instance_loss: 0.0024, weighted_loss: 0.0008, label: 0, bag_size: 9234\n",
      "batch 759, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 10481\n",
      "batch 779, loss: 0.0436, instance_loss: 0.0281, weighted_loss: 0.0390, label: 1, bag_size: 5864\n",
      "batch 799, loss: 0.0320, instance_loss: 0.0138, weighted_loss: 0.0266, label: 0, bag_size: 11527\n",
      "batch 819, loss: 0.0065, instance_loss: 0.0055, weighted_loss: 0.0062, label: 0, bag_size: 9234\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9786585365853658: correct 12840/13120\n",
      "class 1 clustering acc 0.8891768292682927: correct 5833/6560\n",
      "Epoch: 33, train_loss: 0.1750, train_clustering_loss:  0.2022, train_error: 0.0646\n",
      "class 0: acc 0.9363207547169812, correct 397/424\n",
      "class 1: acc 0.9343434343434344, correct 370/396\n",
      "\n",
      "Val Set, val_loss: 0.2160, val_error: 0.1182, auc: 0.9745\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8103448275862069, correct 47/58\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0013, instance_loss: 0.0022, weighted_loss: 0.0015, label: 1, bag_size: 645\n",
      "batch 39, loss: 0.0351, instance_loss: 0.0230, weighted_loss: 0.0315, label: 0, bag_size: 6367\n",
      "batch 59, loss: 0.0463, instance_loss: 0.0407, weighted_loss: 0.0446, label: 0, bag_size: 2382\n",
      "batch 79, loss: 0.1525, instance_loss: 0.1282, weighted_loss: 0.1452, label: 0, bag_size: 10381\n",
      "batch 99, loss: 0.3746, instance_loss: 0.4237, weighted_loss: 0.3893, label: 0, bag_size: 3375\n",
      "batch 119, loss: 0.0012, instance_loss: 0.0013, weighted_loss: 0.0012, label: 0, bag_size: 30751\n",
      "batch 139, loss: 0.0046, instance_loss: 0.0060, weighted_loss: 0.0050, label: 0, bag_size: 15841\n",
      "batch 159, loss: 0.0107, instance_loss: 0.0138, weighted_loss: 0.0117, label: 1, bag_size: 8003\n",
      "batch 179, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 11607\n",
      "batch 199, loss: 0.0075, instance_loss: 0.0171, weighted_loss: 0.0104, label: 1, bag_size: 2137\n",
      "batch 219, loss: 0.0004, instance_loss: 0.0003, weighted_loss: 0.0004, label: 0, bag_size: 8661\n",
      "batch 239, loss: 0.0023, instance_loss: 0.0111, weighted_loss: 0.0049, label: 1, bag_size: 15332\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0017, weighted_loss: 0.0005, label: 0, bag_size: 5551\n",
      "batch 279, loss: 0.8294, instance_loss: 1.5219, weighted_loss: 1.0371, label: 1, bag_size: 898\n",
      "batch 299, loss: 0.0007, instance_loss: 0.0004, weighted_loss: 0.0006, label: 1, bag_size: 6745\n",
      "batch 319, loss: 0.0011, instance_loss: 0.0069, weighted_loss: 0.0028, label: 1, bag_size: 8982\n",
      "batch 339, loss: 0.0171, instance_loss: 0.0079, weighted_loss: 0.0143, label: 1, bag_size: 8019\n",
      "batch 359, loss: 0.0065, instance_loss: 0.0087, weighted_loss: 0.0071, label: 1, bag_size: 9689\n",
      "batch 379, loss: 0.0206, instance_loss: 0.0785, weighted_loss: 0.0380, label: 1, bag_size: 2344\n",
      "batch 399, loss: 0.1485, instance_loss: 0.1120, weighted_loss: 0.1376, label: 0, bag_size: 2004\n",
      "batch 419, loss: 0.1627, instance_loss: 0.2627, weighted_loss: 0.1927, label: 1, bag_size: 1038\n",
      "batch 439, loss: 0.0017, instance_loss: 0.0030, weighted_loss: 0.0021, label: 1, bag_size: 2356\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 19518\n",
      "batch 479, loss: 0.0032, instance_loss: 0.0019, weighted_loss: 0.0028, label: 1, bag_size: 8660\n",
      "batch 499, loss: 0.0045, instance_loss: 0.0118, weighted_loss: 0.0067, label: 0, bag_size: 705\n",
      "batch 519, loss: 0.0035, instance_loss: 0.0068, weighted_loss: 0.0045, label: 0, bag_size: 12731\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0144, weighted_loss: 0.0044, label: 1, bag_size: 10920\n",
      "batch 559, loss: 0.1569, instance_loss: 0.5052, weighted_loss: 0.2614, label: 1, bag_size: 1244\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 19043\n",
      "batch 599, loss: 0.0002, instance_loss: 0.1794, weighted_loss: 0.0540, label: 1, bag_size: 14515\n",
      "batch 619, loss: 0.0730, instance_loss: 0.0333, weighted_loss: 0.0611, label: 1, bag_size: 11256\n",
      "batch 639, loss: 0.0023, instance_loss: 0.0009, weighted_loss: 0.0019, label: 0, bag_size: 10365\n",
      "batch 659, loss: 0.1993, instance_loss: 0.3409, weighted_loss: 0.2418, label: 1, bag_size: 10432\n",
      "batch 679, loss: 0.6284, instance_loss: 0.1857, weighted_loss: 0.4956, label: 1, bag_size: 12712\n",
      "batch 699, loss: 0.0001, instance_loss: 0.0018, weighted_loss: 0.0006, label: 0, bag_size: 21404\n",
      "batch 719, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 0, bag_size: 2457\n",
      "batch 739, loss: 0.1182, instance_loss: 0.2333, weighted_loss: 0.1527, label: 1, bag_size: 6090\n",
      "batch 759, loss: 0.0077, instance_loss: 0.0209, weighted_loss: 0.0117, label: 1, bag_size: 9321\n",
      "batch 779, loss: 0.0092, instance_loss: 0.0092, weighted_loss: 0.0092, label: 0, bag_size: 3908\n",
      "batch 799, loss: 1.0440, instance_loss: 2.9212, weighted_loss: 1.6072, label: 0, bag_size: 3802\n",
      "batch 819, loss: 0.0672, instance_loss: 0.0716, weighted_loss: 0.0685, label: 1, bag_size: 6736\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9734756097560976: correct 12772/13120\n",
      "class 1 clustering acc 0.8788109756097561: correct 5765/6560\n",
      "Epoch: 34, train_loss: 0.1941, train_clustering_loss:  0.2453, train_error: 0.0744\n",
      "class 0: acc 0.9195121951219513, correct 377/410\n",
      "class 1: acc 0.9317073170731708, correct 382/410\n",
      "\n",
      "Val Set, val_loss: 0.5959, val_error: 0.2000, auc: 0.9549\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.00909090909090909: correct 8/880\n",
      "class 0: acc 0.5769230769230769, correct 30/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0675, instance_loss: 0.0650, weighted_loss: 0.0668, label: 0, bag_size: 2242\n",
      "batch 39, loss: 0.0123, instance_loss: 0.0060, weighted_loss: 0.0104, label: 1, bag_size: 9446\n",
      "batch 59, loss: 0.0123, instance_loss: 0.0118, weighted_loss: 0.0122, label: 1, bag_size: 7798\n",
      "batch 79, loss: 0.1995, instance_loss: 0.2198, weighted_loss: 0.2056, label: 0, bag_size: 21361\n",
      "batch 99, loss: 0.0081, instance_loss: 0.0078, weighted_loss: 0.0080, label: 1, bag_size: 2785\n",
      "batch 119, loss: 0.0375, instance_loss: 0.0233, weighted_loss: 0.0332, label: 1, bag_size: 3651\n",
      "batch 139, loss: 0.0097, instance_loss: 0.0078, weighted_loss: 0.0091, label: 1, bag_size: 14433\n",
      "batch 159, loss: 0.2883, instance_loss: 0.3050, weighted_loss: 0.2933, label: 1, bag_size: 1294\n",
      "batch 179, loss: 2.0248, instance_loss: 2.5217, weighted_loss: 2.1739, label: 1, bag_size: 21252\n",
      "batch 199, loss: 0.0077, instance_loss: 0.0200, weighted_loss: 0.0114, label: 1, bag_size: 1101\n",
      "batch 219, loss: 0.0094, instance_loss: 0.0050, weighted_loss: 0.0080, label: 0, bag_size: 23714\n",
      "batch 239, loss: 0.4005, instance_loss: 0.4499, weighted_loss: 0.4154, label: 1, bag_size: 7669\n",
      "batch 259, loss: 0.0019, instance_loss: 0.0020, weighted_loss: 0.0019, label: 1, bag_size: 1919\n",
      "batch 279, loss: 0.0011, instance_loss: 0.0007, weighted_loss: 0.0010, label: 1, bag_size: 10392\n",
      "batch 299, loss: 0.0005, instance_loss: 0.0070, weighted_loss: 0.0025, label: 0, bag_size: 16211\n",
      "batch 319, loss: 0.0034, instance_loss: 0.0092, weighted_loss: 0.0051, label: 0, bag_size: 16690\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0035, weighted_loss: 0.0011, label: 0, bag_size: 3787\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 10898\n",
      "batch 379, loss: 0.0244, instance_loss: 0.0106, weighted_loss: 0.0202, label: 1, bag_size: 1244\n",
      "batch 399, loss: 2.1617, instance_loss: 2.7126, weighted_loss: 2.3270, label: 0, bag_size: 24382\n",
      "batch 419, loss: 0.0011, instance_loss: 0.0008, weighted_loss: 0.0010, label: 0, bag_size: 1881\n",
      "batch 439, loss: 0.0021, instance_loss: 0.0018, weighted_loss: 0.0020, label: 0, bag_size: 2091\n",
      "batch 459, loss: 0.0725, instance_loss: 0.0646, weighted_loss: 0.0702, label: 0, bag_size: 21361\n",
      "batch 479, loss: 0.0007, instance_loss: 0.0235, weighted_loss: 0.0076, label: 1, bag_size: 11032\n",
      "batch 499, loss: 0.0175, instance_loss: 0.0137, weighted_loss: 0.0164, label: 1, bag_size: 7424\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0109, weighted_loss: 0.0033, label: 1, bag_size: 4250\n",
      "batch 539, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 1213\n",
      "batch 559, loss: 0.5180, instance_loss: 0.6498, weighted_loss: 0.5575, label: 1, bag_size: 5723\n",
      "batch 579, loss: 0.0317, instance_loss: 0.0093, weighted_loss: 0.0250, label: 1, bag_size: 7371\n",
      "batch 599, loss: 0.0006, instance_loss: 0.0001, weighted_loss: 0.0005, label: 1, bag_size: 1244\n",
      "batch 619, loss: 0.0003, instance_loss: 0.0015, weighted_loss: 0.0007, label: 0, bag_size: 7011\n",
      "batch 639, loss: 0.2085, instance_loss: 0.2363, weighted_loss: 0.2169, label: 0, bag_size: 2628\n",
      "batch 659, loss: 0.0055, instance_loss: 0.0036, weighted_loss: 0.0049, label: 1, bag_size: 8685\n",
      "batch 679, loss: 0.3633, instance_loss: 0.4905, weighted_loss: 0.4015, label: 1, bag_size: 1867\n",
      "batch 699, loss: 0.0003, instance_loss: 0.0004, weighted_loss: 0.0003, label: 1, bag_size: 10501\n",
      "batch 719, loss: 3.1187, instance_loss: 3.6959, weighted_loss: 3.2919, label: 1, bag_size: 684\n",
      "batch 739, loss: 0.0018, instance_loss: 0.0017, weighted_loss: 0.0018, label: 0, bag_size: 3787\n",
      "batch 759, loss: 0.0045, instance_loss: 0.0034, weighted_loss: 0.0042, label: 0, bag_size: 23796\n",
      "batch 779, loss: 0.0058, instance_loss: 0.0068, weighted_loss: 0.0061, label: 1, bag_size: 1622\n",
      "batch 799, loss: 0.0053, instance_loss: 0.0061, weighted_loss: 0.0055, label: 0, bag_size: 10068\n",
      "batch 819, loss: 0.0002, instance_loss: 0.0005, weighted_loss: 0.0003, label: 1, bag_size: 7119\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.979420731707317: correct 12850/13120\n",
      "class 1 clustering acc 0.8824695121951219: correct 5789/6560\n",
      "Epoch: 35, train_loss: 0.1684, train_clustering_loss:  0.2002, train_error: 0.0780\n",
      "class 0: acc 0.9119804400977995, correct 373/409\n",
      "class 1: acc 0.9318734793187348, correct 383/411\n",
      "\n",
      "Val Set, val_loss: 0.2085, val_error: 0.0727, auc: 0.9735\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0048, instance_loss: 0.0039, weighted_loss: 0.0045, label: 0, bag_size: 8812\n",
      "batch 39, loss: 0.0089, instance_loss: 0.0039, weighted_loss: 0.0074, label: 1, bag_size: 11394\n",
      "batch 59, loss: 0.0001, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 16051\n",
      "batch 79, loss: 0.0336, instance_loss: 0.0161, weighted_loss: 0.0283, label: 1, bag_size: 9322\n",
      "batch 99, loss: 0.2333, instance_loss: 0.3461, weighted_loss: 0.2671, label: 1, bag_size: 6478\n",
      "batch 119, loss: 0.2580, instance_loss: 0.2763, weighted_loss: 0.2635, label: 1, bag_size: 983\n",
      "batch 139, loss: 0.3647, instance_loss: 0.5238, weighted_loss: 0.4125, label: 0, bag_size: 9132\n",
      "batch 159, loss: 0.1344, instance_loss: 0.1416, weighted_loss: 0.1366, label: 0, bag_size: 8420\n",
      "batch 179, loss: 0.0049, instance_loss: 0.0031, weighted_loss: 0.0043, label: 0, bag_size: 7381\n",
      "batch 199, loss: 0.0075, instance_loss: 0.0045, weighted_loss: 0.0066, label: 0, bag_size: 16607\n",
      "batch 219, loss: 0.0084, instance_loss: 0.0084, weighted_loss: 0.0084, label: 0, bag_size: 16052\n",
      "batch 239, loss: 0.2629, instance_loss: 0.3524, weighted_loss: 0.2898, label: 1, bag_size: 12494\n",
      "batch 259, loss: 0.0057, instance_loss: 0.0040, weighted_loss: 0.0052, label: 0, bag_size: 19043\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 65728\n",
      "batch 299, loss: 0.0088, instance_loss: 0.0123, weighted_loss: 0.0099, label: 1, bag_size: 3368\n",
      "batch 319, loss: 0.1033, instance_loss: 0.0932, weighted_loss: 0.1003, label: 0, bag_size: 12840\n",
      "batch 339, loss: 0.1539, instance_loss: 0.1267, weighted_loss: 0.1457, label: 0, bag_size: 15672\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0005, weighted_loss: 0.0002, label: 1, bag_size: 10920\n",
      "batch 379, loss: 0.0085, instance_loss: 0.0177, weighted_loss: 0.0113, label: 1, bag_size: 7110\n",
      "batch 399, loss: 0.0405, instance_loss: 0.0399, weighted_loss: 0.0403, label: 0, bag_size: 9433\n",
      "batch 419, loss: 0.0058, instance_loss: 0.0035, weighted_loss: 0.0051, label: 1, bag_size: 7798\n",
      "batch 439, loss: 0.0008, instance_loss: 0.0003, weighted_loss: 0.0007, label: 1, bag_size: 10281\n",
      "batch 459, loss: 0.2633, instance_loss: 0.2781, weighted_loss: 0.2678, label: 0, bag_size: 11607\n",
      "batch 479, loss: 0.0505, instance_loss: 0.0231, weighted_loss: 0.0422, label: 0, bag_size: 1149\n",
      "batch 499, loss: 0.0019, instance_loss: 0.0046, weighted_loss: 0.0027, label: 0, bag_size: 31106\n",
      "batch 519, loss: 0.0080, instance_loss: 0.0082, weighted_loss: 0.0081, label: 0, bag_size: 1920\n",
      "batch 539, loss: 0.4189, instance_loss: 0.3240, weighted_loss: 0.3904, label: 0, bag_size: 12910\n",
      "batch 559, loss: 0.0017, instance_loss: 0.0007, weighted_loss: 0.0014, label: 1, bag_size: 12931\n",
      "batch 579, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 9060\n",
      "batch 599, loss: 0.0861, instance_loss: 0.0798, weighted_loss: 0.0842, label: 0, bag_size: 9387\n",
      "batch 619, loss: 0.0020, instance_loss: 0.0056, weighted_loss: 0.0031, label: 0, bag_size: 1588\n",
      "batch 639, loss: 0.1981, instance_loss: 0.2520, weighted_loss: 0.2143, label: 0, bag_size: 4418\n",
      "batch 659, loss: 0.0135, instance_loss: 0.0095, weighted_loss: 0.0123, label: 1, bag_size: 11701\n",
      "batch 679, loss: 0.3397, instance_loss: 0.4314, weighted_loss: 0.3672, label: 1, bag_size: 5903\n",
      "batch 699, loss: 0.5942, instance_loss: 0.7745, weighted_loss: 0.6483, label: 1, bag_size: 1493\n",
      "batch 719, loss: 0.0029, instance_loss: 0.0017, weighted_loss: 0.0026, label: 0, bag_size: 9470\n",
      "batch 739, loss: 0.0075, instance_loss: 0.0087, weighted_loss: 0.0078, label: 0, bag_size: 2732\n",
      "batch 759, loss: 0.0082, instance_loss: 0.0024, weighted_loss: 0.0065, label: 0, bag_size: 14377\n",
      "batch 779, loss: 0.0003, instance_loss: 0.0008, weighted_loss: 0.0004, label: 1, bag_size: 5221\n",
      "batch 799, loss: 0.4389, instance_loss: 0.5454, weighted_loss: 0.4708, label: 0, bag_size: 1498\n",
      "batch 819, loss: 0.0003, instance_loss: 0.0028, weighted_loss: 0.0010, label: 1, bag_size: 4877\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9770579268292683: correct 12819/13120\n",
      "class 1 clustering acc 0.8748475609756098: correct 5739/6560\n",
      "Epoch: 36, train_loss: 0.2007, train_clustering_loss:  0.2294, train_error: 0.0805\n",
      "class 0: acc 0.9219512195121952, correct 378/410\n",
      "class 1: acc 0.9170731707317074, correct 376/410\n",
      "\n",
      "Val Set, val_loss: 0.2246, val_error: 0.1000, auc: 0.9708\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1208, instance_loss: 0.1009, weighted_loss: 0.1148, label: 1, bag_size: 14887\n",
      "batch 39, loss: 0.0018, instance_loss: 0.0003, weighted_loss: 0.0014, label: 1, bag_size: 7650\n",
      "batch 59, loss: 0.6114, instance_loss: 0.8166, weighted_loss: 0.6729, label: 1, bag_size: 2937\n",
      "batch 79, loss: 0.0749, instance_loss: 0.0746, weighted_loss: 0.0748, label: 1, bag_size: 1015\n",
      "batch 99, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 8372\n",
      "batch 119, loss: 0.0002, instance_loss: 0.0104, weighted_loss: 0.0032, label: 0, bag_size: 14266\n",
      "batch 139, loss: 0.0006, instance_loss: 0.0014, weighted_loss: 0.0008, label: 1, bag_size: 11884\n",
      "batch 159, loss: 0.0296, instance_loss: 0.0314, weighted_loss: 0.0302, label: 0, bag_size: 8582\n",
      "batch 179, loss: 0.0005, instance_loss: 0.0005, weighted_loss: 0.0005, label: 0, bag_size: 10128\n",
      "batch 199, loss: 0.1655, instance_loss: 0.1880, weighted_loss: 0.1722, label: 0, bag_size: 2296\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0003, label: 0, bag_size: 11383\n",
      "batch 239, loss: 0.2719, instance_loss: 0.2383, weighted_loss: 0.2618, label: 1, bag_size: 4929\n",
      "batch 259, loss: 0.3382, instance_loss: 0.4442, weighted_loss: 0.3700, label: 0, bag_size: 1483\n",
      "batch 279, loss: 0.0022, instance_loss: 0.0016, weighted_loss: 0.0020, label: 0, bag_size: 2654\n",
      "batch 299, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 6898\n",
      "batch 319, loss: 0.0022, instance_loss: 0.0009, weighted_loss: 0.0018, label: 1, bag_size: 13786\n",
      "batch 339, loss: 0.3733, instance_loss: 0.3860, weighted_loss: 0.3771, label: 0, bag_size: 7428\n",
      "batch 359, loss: 0.4657, instance_loss: 0.5529, weighted_loss: 0.4918, label: 0, bag_size: 2367\n",
      "batch 379, loss: 3.0714, instance_loss: 3.5955, weighted_loss: 3.2286, label: 0, bag_size: 2815\n",
      "batch 399, loss: 0.0032, instance_loss: 0.0045, weighted_loss: 0.0036, label: 0, bag_size: 3190\n",
      "batch 419, loss: 0.0646, instance_loss: 0.0397, weighted_loss: 0.0571, label: 1, bag_size: 4039\n",
      "batch 439, loss: 0.3305, instance_loss: 0.3645, weighted_loss: 0.3407, label: 0, bag_size: 12840\n",
      "batch 459, loss: 1.9774, instance_loss: 2.4849, weighted_loss: 2.1297, label: 0, bag_size: 7428\n",
      "batch 479, loss: 0.0002, instance_loss: 0.0008, weighted_loss: 0.0004, label: 0, bag_size: 15313\n",
      "batch 499, loss: 0.0008, instance_loss: 0.0003, weighted_loss: 0.0006, label: 0, bag_size: 18045\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 18738\n",
      "batch 539, loss: 0.1723, instance_loss: 0.1681, weighted_loss: 0.1711, label: 0, bag_size: 17791\n",
      "batch 559, loss: 0.0031, instance_loss: 0.0027, weighted_loss: 0.0030, label: 1, bag_size: 15213\n",
      "batch 579, loss: 0.0052, instance_loss: 0.0029, weighted_loss: 0.0045, label: 1, bag_size: 928\n",
      "batch 599, loss: 0.0169, instance_loss: 0.0089, weighted_loss: 0.0145, label: 0, bag_size: 16720\n",
      "batch 619, loss: 1.5172, instance_loss: 2.0281, weighted_loss: 1.6704, label: 1, bag_size: 2395\n",
      "batch 639, loss: 0.2791, instance_loss: 0.3736, weighted_loss: 0.3075, label: 0, bag_size: 1690\n",
      "batch 659, loss: 0.0063, instance_loss: 0.0070, weighted_loss: 0.0065, label: 1, bag_size: 5025\n",
      "batch 679, loss: 0.0224, instance_loss: 0.0120, weighted_loss: 0.0193, label: 0, bag_size: 12148\n",
      "batch 699, loss: 0.5710, instance_loss: 0.7728, weighted_loss: 0.6315, label: 0, bag_size: 1760\n",
      "batch 719, loss: 0.0114, instance_loss: 0.0077, weighted_loss: 0.0103, label: 1, bag_size: 8191\n",
      "batch 739, loss: 0.0062, instance_loss: 0.0070, weighted_loss: 0.0064, label: 1, bag_size: 19606\n",
      "batch 759, loss: 0.0006, instance_loss: 0.0011, weighted_loss: 0.0007, label: 0, bag_size: 1962\n",
      "batch 779, loss: 0.0040, instance_loss: 0.0128, weighted_loss: 0.0066, label: 1, bag_size: 7513\n",
      "batch 799, loss: 0.1239, instance_loss: 0.1298, weighted_loss: 0.1257, label: 0, bag_size: 2382\n",
      "batch 819, loss: 0.0031, instance_loss: 0.0037, weighted_loss: 0.0032, label: 0, bag_size: 3970\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9772865853658537: correct 12822/13120\n",
      "class 1 clustering acc 0.873780487804878: correct 5732/6560\n",
      "Epoch: 37, train_loss: 0.1880, train_clustering_loss:  0.2187, train_error: 0.0805\n",
      "class 0: acc 0.9213759213759214, correct 375/407\n",
      "class 1: acc 0.9176755447941889, correct 379/413\n",
      "\n",
      "Val Set, val_loss: 0.2085, val_error: 0.1000, auc: 0.9771\n",
      "class 0 clustering acc 0.990909090909091: correct 1744/1760\n",
      "class 1 clustering acc 0.03636363636363636: correct 32/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8448275862068966, correct 49/58\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6107, instance_loss: 0.6885, weighted_loss: 0.6340, label: 0, bag_size: 2070\n",
      "batch 39, loss: 0.0511, instance_loss: 0.0525, weighted_loss: 0.0515, label: 0, bag_size: 9132\n",
      "batch 59, loss: 0.0012, instance_loss: 0.0018, weighted_loss: 0.0014, label: 1, bag_size: 5340\n",
      "batch 79, loss: 0.0229, instance_loss: 0.0186, weighted_loss: 0.0216, label: 1, bag_size: 8680\n",
      "batch 99, loss: 0.0175, instance_loss: 0.0130, weighted_loss: 0.0161, label: 0, bag_size: 13205\n",
      "batch 119, loss: 0.0006, instance_loss: 0.0009, weighted_loss: 0.0007, label: 0, bag_size: 4465\n",
      "batch 139, loss: 0.0091, instance_loss: 0.0073, weighted_loss: 0.0085, label: 0, bag_size: 1415\n",
      "batch 159, loss: 0.3601, instance_loss: 0.3346, weighted_loss: 0.3525, label: 1, bag_size: 983\n",
      "batch 179, loss: 1.1152, instance_loss: 1.1117, weighted_loss: 1.1142, label: 1, bag_size: 3652\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 5317\n",
      "batch 219, loss: 0.0005, instance_loss: 0.0009, weighted_loss: 0.0006, label: 0, bag_size: 2303\n",
      "batch 239, loss: 0.0346, instance_loss: 0.0254, weighted_loss: 0.0318, label: 0, bag_size: 9542\n",
      "batch 259, loss: 0.0055, instance_loss: 0.0017, weighted_loss: 0.0044, label: 0, bag_size: 2195\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0057, weighted_loss: 0.0018, label: 1, bag_size: 15233\n",
      "batch 299, loss: 0.0261, instance_loss: 0.0814, weighted_loss: 0.0427, label: 1, bag_size: 8982\n",
      "batch 319, loss: 0.0096, instance_loss: 0.0034, weighted_loss: 0.0077, label: 1, bag_size: 14515\n",
      "batch 339, loss: 0.0724, instance_loss: 0.0561, weighted_loss: 0.0675, label: 0, bag_size: 2004\n",
      "batch 359, loss: 0.3774, instance_loss: 0.3715, weighted_loss: 0.3756, label: 1, bag_size: 5903\n",
      "batch 379, loss: 0.0434, instance_loss: 0.0404, weighted_loss: 0.0425, label: 0, bag_size: 10942\n",
      "batch 399, loss: 0.0026, instance_loss: 0.0021, weighted_loss: 0.0024, label: 1, bag_size: 9878\n",
      "batch 419, loss: 0.0841, instance_loss: 0.0883, weighted_loss: 0.0853, label: 1, bag_size: 1759\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 6792\n",
      "batch 459, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 1838\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11125\n",
      "batch 499, loss: 0.0009, instance_loss: 0.0004, weighted_loss: 0.0008, label: 1, bag_size: 12931\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 0, bag_size: 15747\n",
      "batch 539, loss: 0.0004, instance_loss: 0.0003, weighted_loss: 0.0004, label: 0, bag_size: 15967\n",
      "batch 559, loss: 0.0562, instance_loss: 0.0377, weighted_loss: 0.0506, label: 1, bag_size: 19832\n",
      "batch 579, loss: 0.0224, instance_loss: 0.0117, weighted_loss: 0.0192, label: 0, bag_size: 22498\n",
      "batch 599, loss: 0.0164, instance_loss: 0.0244, weighted_loss: 0.0188, label: 1, bag_size: 10396\n",
      "batch 619, loss: 0.0008, instance_loss: 0.0013, weighted_loss: 0.0010, label: 1, bag_size: 11875\n",
      "batch 639, loss: 0.0080, instance_loss: 0.0113, weighted_loss: 0.0090, label: 1, bag_size: 1638\n",
      "batch 659, loss: 0.0930, instance_loss: 0.0935, weighted_loss: 0.0932, label: 0, bag_size: 12083\n",
      "batch 679, loss: 1.6171, instance_loss: 1.8343, weighted_loss: 1.6823, label: 0, bag_size: 13332\n",
      "batch 699, loss: 1.7563, instance_loss: 1.9556, weighted_loss: 1.8161, label: 1, bag_size: 2731\n",
      "batch 719, loss: 0.0820, instance_loss: 0.0802, weighted_loss: 0.0815, label: 1, bag_size: 2678\n",
      "batch 739, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 8252\n",
      "batch 759, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 13225\n",
      "batch 779, loss: 0.0214, instance_loss: 0.0210, weighted_loss: 0.0213, label: 0, bag_size: 12793\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0043, weighted_loss: 0.0013, label: 1, bag_size: 10392\n",
      "batch 819, loss: 3.7198, instance_loss: 4.1700, weighted_loss: 3.8548, label: 1, bag_size: 4786\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9811737804878049: correct 12873/13120\n",
      "class 1 clustering acc 0.9051829268292683: correct 5938/6560\n",
      "Epoch: 38, train_loss: 0.1554, train_clustering_loss:  0.1870, train_error: 0.0573\n",
      "class 0: acc 0.9384236453201971, correct 381/406\n",
      "class 1: acc 0.9468599033816425, correct 392/414\n",
      "\n",
      "Val Set, val_loss: 0.7651, val_error: 0.2000, auc: 0.9738\n",
      "class 0 clustering acc 0.9880681818181818: correct 1739/1760\n",
      "class 1 clustering acc 0.03636363636363636: correct 32/880\n",
      "class 0: acc 1.0, correct 52/52\n",
      "class 1: acc 0.6206896551724138, correct 36/58\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1402, instance_loss: 0.0995, weighted_loss: 0.1280, label: 0, bag_size: 19470\n",
      "batch 39, loss: 0.0059, instance_loss: 0.0030, weighted_loss: 0.0051, label: 1, bag_size: 1255\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0071, weighted_loss: 0.0022, label: 0, bag_size: 3725\n",
      "batch 79, loss: 0.0018, instance_loss: 0.0010, weighted_loss: 0.0016, label: 0, bag_size: 19043\n",
      "batch 99, loss: 2.5979, instance_loss: 3.1001, weighted_loss: 2.7486, label: 1, bag_size: 1533\n",
      "batch 119, loss: 0.0295, instance_loss: 0.0314, weighted_loss: 0.0301, label: 0, bag_size: 19808\n",
      "batch 139, loss: 0.0043, instance_loss: 0.0034, weighted_loss: 0.0040, label: 0, bag_size: 7637\n",
      "batch 159, loss: 0.0123, instance_loss: 0.0094, weighted_loss: 0.0114, label: 0, bag_size: 19470\n",
      "batch 179, loss: 0.0016, instance_loss: 0.0006, weighted_loss: 0.0013, label: 1, bag_size: 4394\n",
      "batch 199, loss: 0.0089, instance_loss: 0.0105, weighted_loss: 0.0094, label: 0, bag_size: 2195\n",
      "batch 219, loss: 0.0158, instance_loss: 0.0134, weighted_loss: 0.0150, label: 0, bag_size: 22681\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0266, weighted_loss: 0.0080, label: 1, bag_size: 2385\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 1, bag_size: 8868\n",
      "batch 279, loss: 0.0018, instance_loss: 0.0008, weighted_loss: 0.0015, label: 0, bag_size: 2367\n",
      "batch 299, loss: 0.0027, instance_loss: 0.0010, weighted_loss: 0.0022, label: 0, bag_size: 8025\n",
      "batch 319, loss: 0.0002, instance_loss: 0.0128, weighted_loss: 0.0040, label: 0, bag_size: 7011\n",
      "batch 339, loss: 0.0002, instance_loss: 0.0002, weighted_loss: 0.0002, label: 1, bag_size: 10028\n",
      "batch 359, loss: 0.0014, instance_loss: 0.0008, weighted_loss: 0.0012, label: 0, bag_size: 3502\n",
      "batch 379, loss: 0.0026, instance_loss: 0.0025, weighted_loss: 0.0026, label: 1, bag_size: 10867\n",
      "batch 399, loss: 0.0004, instance_loss: 0.0016, weighted_loss: 0.0008, label: 0, bag_size: 19472\n",
      "batch 419, loss: 0.0012, instance_loss: 0.0004, weighted_loss: 0.0009, label: 1, bag_size: 11518\n",
      "batch 439, loss: 0.0091, instance_loss: 0.0037, weighted_loss: 0.0075, label: 0, bag_size: 12793\n",
      "batch 459, loss: 0.1224, instance_loss: 0.1404, weighted_loss: 0.1278, label: 0, bag_size: 3502\n",
      "batch 479, loss: 0.0002, instance_loss: 0.0014, weighted_loss: 0.0005, label: 0, bag_size: 25814\n",
      "batch 499, loss: 0.0009, instance_loss: 0.0007, weighted_loss: 0.0008, label: 1, bag_size: 10112\n",
      "batch 519, loss: 0.1703, instance_loss: 0.1534, weighted_loss: 0.1653, label: 0, bag_size: 10113\n",
      "batch 539, loss: 0.2074, instance_loss: 0.2555, weighted_loss: 0.2218, label: 1, bag_size: 1746\n",
      "batch 559, loss: 2.1203, instance_loss: 2.5938, weighted_loss: 2.2623, label: 0, bag_size: 2160\n",
      "batch 579, loss: 0.0024, instance_loss: 0.0001, weighted_loss: 0.0017, label: 0, bag_size: 23796\n",
      "batch 599, loss: 0.6957, instance_loss: 0.8659, weighted_loss: 0.7468, label: 1, bag_size: 2937\n",
      "batch 619, loss: 0.0067, instance_loss: 0.0032, weighted_loss: 0.0056, label: 1, bag_size: 10492\n",
      "batch 639, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 19472\n",
      "batch 659, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 7637\n",
      "batch 679, loss: 0.0004, instance_loss: 0.0005, weighted_loss: 0.0004, label: 1, bag_size: 3224\n",
      "batch 699, loss: 0.0069, instance_loss: 0.0053, weighted_loss: 0.0065, label: 1, bag_size: 4821\n",
      "batch 719, loss: 0.1535, instance_loss: 0.1336, weighted_loss: 0.1475, label: 0, bag_size: 2242\n",
      "batch 739, loss: 0.4196, instance_loss: 0.4796, weighted_loss: 0.4376, label: 1, bag_size: 7351\n",
      "batch 759, loss: 0.0037, instance_loss: 0.0024, weighted_loss: 0.0033, label: 1, bag_size: 8448\n",
      "batch 779, loss: 0.0046, instance_loss: 0.0021, weighted_loss: 0.0039, label: 1, bag_size: 11518\n",
      "batch 799, loss: 0.0909, instance_loss: 0.0552, weighted_loss: 0.0802, label: 0, bag_size: 6281\n",
      "batch 819, loss: 0.0003, instance_loss: 0.0002, weighted_loss: 0.0002, label: 0, bag_size: 11735\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9814024390243903: correct 12876/13120\n",
      "class 1 clustering acc 0.8989329268292683: correct 5897/6560\n",
      "Epoch: 39, train_loss: 0.1644, train_clustering_loss:  0.1925, train_error: 0.0646\n",
      "class 0: acc 0.9411764705882353, correct 384/408\n",
      "class 1: acc 0.9296116504854369, correct 383/412\n",
      "\n",
      "Val Set, val_loss: 0.1860, val_error: 0.0818, auc: 0.9781\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "Validation loss decreased (0.199585 --> 0.186012).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5199, instance_loss: 0.6654, weighted_loss: 0.5635, label: 1, bag_size: 9162\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 11032\n",
      "batch 59, loss: 0.0031, instance_loss: 0.0021, weighted_loss: 0.0028, label: 0, bag_size: 23714\n",
      "batch 79, loss: 0.0699, instance_loss: 0.0413, weighted_loss: 0.0613, label: 1, bag_size: 2137\n",
      "batch 99, loss: 0.0030, instance_loss: 0.0060, weighted_loss: 0.0039, label: 0, bag_size: 8252\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 0, bag_size: 2844\n",
      "batch 139, loss: 0.4582, instance_loss: 0.4548, weighted_loss: 0.4572, label: 1, bag_size: 3652\n",
      "batch 159, loss: 3.3832, instance_loss: 4.2763, weighted_loss: 3.6511, label: 0, bag_size: 2290\n",
      "batch 179, loss: 0.0023, instance_loss: 0.0040, weighted_loss: 0.0028, label: 1, bag_size: 8019\n",
      "batch 199, loss: 0.2983, instance_loss: 0.2681, weighted_loss: 0.2892, label: 1, bag_size: 2682\n",
      "batch 219, loss: 0.0003, instance_loss: 0.0013, weighted_loss: 0.0006, label: 0, bag_size: 10898\n",
      "batch 239, loss: 0.0839, instance_loss: 0.0813, weighted_loss: 0.0831, label: 0, bag_size: 3557\n",
      "batch 259, loss: 0.0122, instance_loss: 0.0098, weighted_loss: 0.0115, label: 1, bag_size: 15689\n",
      "batch 279, loss: 0.0533, instance_loss: 0.1553, weighted_loss: 0.0839, label: 1, bag_size: 2137\n",
      "batch 299, loss: 0.1197, instance_loss: 0.1231, weighted_loss: 0.1207, label: 0, bag_size: 1508\n",
      "batch 319, loss: 0.0626, instance_loss: 0.0384, weighted_loss: 0.0553, label: 1, bag_size: 2137\n",
      "batch 339, loss: 0.0471, instance_loss: 0.0367, weighted_loss: 0.0440, label: 0, bag_size: 4598\n",
      "batch 359, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 12460\n",
      "batch 379, loss: 0.1976, instance_loss: 0.2233, weighted_loss: 0.2053, label: 1, bag_size: 1819\n",
      "batch 399, loss: 0.0165, instance_loss: 0.0087, weighted_loss: 0.0142, label: 0, bag_size: 9171\n",
      "batch 419, loss: 0.8033, instance_loss: 1.1388, weighted_loss: 0.9039, label: 0, bag_size: 2918\n",
      "batch 439, loss: 0.0004, instance_loss: 0.0054, weighted_loss: 0.0019, label: 1, bag_size: 11389\n",
      "batch 459, loss: 0.0333, instance_loss: 0.0256, weighted_loss: 0.0310, label: 0, bag_size: 1831\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 11759\n",
      "batch 499, loss: 0.0002, instance_loss: 0.0011, weighted_loss: 0.0005, label: 1, bag_size: 6781\n",
      "batch 519, loss: 0.0009, instance_loss: 0.0006, weighted_loss: 0.0008, label: 0, bag_size: 2652\n",
      "batch 539, loss: 0.0154, instance_loss: 0.0134, weighted_loss: 0.0148, label: 1, bag_size: 16565\n",
      "batch 559, loss: 0.0007, instance_loss: 0.0043, weighted_loss: 0.0018, label: 0, bag_size: 18045\n",
      "batch 579, loss: 0.0106, instance_loss: 0.0016, weighted_loss: 0.0079, label: 1, bag_size: 10432\n",
      "batch 599, loss: 0.0077, instance_loss: 0.0160, weighted_loss: 0.0102, label: 0, bag_size: 2367\n",
      "batch 619, loss: 0.0027, instance_loss: 0.0159, weighted_loss: 0.0067, label: 0, bag_size: 11512\n",
      "batch 639, loss: 0.0802, instance_loss: 0.2508, weighted_loss: 0.1314, label: 0, bag_size: 2918\n",
      "batch 659, loss: 0.1171, instance_loss: 0.2231, weighted_loss: 0.1489, label: 0, bag_size: 8582\n",
      "batch 679, loss: 0.0078, instance_loss: 0.0058, weighted_loss: 0.0072, label: 0, bag_size: 6356\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 15665\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0042, weighted_loss: 0.0013, label: 1, bag_size: 11642\n",
      "batch 739, loss: 1.2657, instance_loss: 1.5017, weighted_loss: 1.3365, label: 0, bag_size: 1592\n",
      "batch 759, loss: 0.0001, instance_loss: 0.0005, weighted_loss: 0.0002, label: 1, bag_size: 12931\n",
      "batch 779, loss: 0.0010, instance_loss: 0.0003, weighted_loss: 0.0008, label: 1, bag_size: 8680\n",
      "batch 799, loss: 0.0045, instance_loss: 0.0004, weighted_loss: 0.0033, label: 0, bag_size: 1651\n",
      "batch 819, loss: 0.0496, instance_loss: 0.0354, weighted_loss: 0.0453, label: 0, bag_size: 1684\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9789634146341464: correct 12844/13120\n",
      "class 1 clustering acc 0.8896341463414634: correct 5836/6560\n",
      "Epoch: 40, train_loss: 0.1746, train_clustering_loss:  0.2081, train_error: 0.0622\n",
      "class 0: acc 0.9393939393939394, correct 372/396\n",
      "class 1: acc 0.9363207547169812, correct 397/424\n",
      "\n",
      "Val Set, val_loss: 0.2359, val_error: 0.1091, auc: 0.9778\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.8076923076923077, correct 42/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.0012, instance_loss: 0.9761, weighted_loss: 0.9937, label: 1, bag_size: 2935\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 1, bag_size: 6966\n",
      "batch 59, loss: 0.0150, instance_loss: 0.0243, weighted_loss: 0.0178, label: 1, bag_size: 5561\n",
      "batch 79, loss: 0.0040, instance_loss: 0.0012, weighted_loss: 0.0032, label: 0, bag_size: 18954\n",
      "batch 99, loss: 0.1631, instance_loss: 0.2537, weighted_loss: 0.1903, label: 0, bag_size: 65728\n",
      "batch 119, loss: 0.0107, instance_loss: 0.0062, weighted_loss: 0.0094, label: 0, bag_size: 2336\n",
      "batch 139, loss: 0.0053, instance_loss: 0.0042, weighted_loss: 0.0050, label: 0, bag_size: 24439\n",
      "batch 159, loss: 0.0240, instance_loss: 0.0189, weighted_loss: 0.0224, label: 0, bag_size: 10068\n",
      "batch 179, loss: 0.1775, instance_loss: 0.1755, weighted_loss: 0.1769, label: 1, bag_size: 5903\n",
      "batch 199, loss: 0.0046, instance_loss: 0.0024, weighted_loss: 0.0040, label: 0, bag_size: 23996\n",
      "batch 219, loss: 0.0016, instance_loss: 0.0001, weighted_loss: 0.0012, label: 0, bag_size: 22828\n",
      "batch 239, loss: 0.0009, instance_loss: 0.0002, weighted_loss: 0.0007, label: 1, bag_size: 9065\n",
      "batch 259, loss: 0.0003, instance_loss: 0.0003, weighted_loss: 0.0003, label: 0, bag_size: 14828\n",
      "batch 279, loss: 0.1328, instance_loss: 0.0957, weighted_loss: 0.1217, label: 0, bag_size: 2266\n",
      "batch 299, loss: 0.0121, instance_loss: 0.0098, weighted_loss: 0.0114, label: 1, bag_size: 15665\n",
      "batch 319, loss: 0.0079, instance_loss: 0.0051, weighted_loss: 0.0070, label: 1, bag_size: 12946\n",
      "batch 339, loss: 0.6089, instance_loss: 0.7266, weighted_loss: 0.6442, label: 0, bag_size: 1714\n",
      "batch 359, loss: 0.0323, instance_loss: 0.0605, weighted_loss: 0.0408, label: 1, bag_size: 6736\n",
      "batch 379, loss: 0.0002, instance_loss: 0.2067, weighted_loss: 0.0621, label: 1, bag_size: 645\n",
      "batch 399, loss: 0.0270, instance_loss: 0.0134, weighted_loss: 0.0229, label: 0, bag_size: 12840\n",
      "batch 419, loss: 0.0061, instance_loss: 0.0035, weighted_loss: 0.0053, label: 0, bag_size: 12201\n",
      "batch 439, loss: 0.0125, instance_loss: 0.0097, weighted_loss: 0.0117, label: 0, bag_size: 1213\n",
      "batch 459, loss: 0.0096, instance_loss: 0.0095, weighted_loss: 0.0095, label: 0, bag_size: 13205\n",
      "batch 479, loss: 0.0552, instance_loss: 0.0643, weighted_loss: 0.0579, label: 0, bag_size: 13205\n",
      "batch 499, loss: 0.0131, instance_loss: 0.0093, weighted_loss: 0.0120, label: 0, bag_size: 1891\n",
      "batch 519, loss: 0.0891, instance_loss: 0.1026, weighted_loss: 0.0931, label: 1, bag_size: 2314\n",
      "batch 539, loss: 0.3152, instance_loss: 0.3955, weighted_loss: 0.3393, label: 1, bag_size: 1759\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 7798\n",
      "batch 579, loss: 0.3075, instance_loss: 0.3918, weighted_loss: 0.3328, label: 0, bag_size: 1651\n",
      "batch 599, loss: 0.0006, instance_loss: 0.0003, weighted_loss: 0.0005, label: 0, bag_size: 9252\n",
      "batch 619, loss: 0.0009, instance_loss: 0.0007, weighted_loss: 0.0008, label: 1, bag_size: 1255\n",
      "batch 639, loss: 0.0141, instance_loss: 0.0164, weighted_loss: 0.0148, label: 1, bag_size: 9519\n",
      "batch 659, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 18154\n",
      "batch 679, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 8145\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 5833\n",
      "batch 719, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 8959\n",
      "batch 739, loss: 0.0146, instance_loss: 0.0111, weighted_loss: 0.0135, label: 0, bag_size: 13339\n",
      "batch 759, loss: 0.1090, instance_loss: 0.1161, weighted_loss: 0.1111, label: 0, bag_size: 21319\n",
      "batch 779, loss: 0.0073, instance_loss: 0.0036, weighted_loss: 0.0062, label: 1, bag_size: 2522\n",
      "batch 799, loss: 0.2267, instance_loss: 0.1884, weighted_loss: 0.2152, label: 0, bag_size: 12083\n",
      "batch 819, loss: 0.0023, instance_loss: 0.0015, weighted_loss: 0.0020, label: 0, bag_size: 2322\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9823932926829269: correct 12889/13120\n",
      "class 1 clustering acc 0.9103658536585366: correct 5972/6560\n",
      "Epoch: 41, train_loss: 0.1492, train_clustering_loss:  0.1706, train_error: 0.0537\n",
      "class 0: acc 0.948019801980198, correct 383/404\n",
      "class 1: acc 0.9447115384615384, correct 393/416\n",
      "\n",
      "Val Set, val_loss: 0.3029, val_error: 0.1455, auc: 0.9741\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.7307692307692307, correct 38/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0013, instance_loss: 0.0069, weighted_loss: 0.0030, label: 1, bag_size: 689\n",
      "batch 39, loss: 0.0025, instance_loss: 0.0029, weighted_loss: 0.0026, label: 1, bag_size: 8003\n",
      "batch 59, loss: 0.0003, instance_loss: 0.0015, weighted_loss: 0.0007, label: 1, bag_size: 9732\n",
      "batch 79, loss: 0.0004, instance_loss: 0.0003, weighted_loss: 0.0004, label: 1, bag_size: 19932\n",
      "batch 99, loss: 0.0005, instance_loss: 0.0037, weighted_loss: 0.0015, label: 1, bag_size: 8019\n",
      "batch 119, loss: 0.0022, instance_loss: 0.0015, weighted_loss: 0.0020, label: 0, bag_size: 19808\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0407, weighted_loss: 0.0122, label: 1, bag_size: 11600\n",
      "batch 159, loss: 0.0600, instance_loss: 0.0290, weighted_loss: 0.0507, label: 1, bag_size: 12611\n",
      "batch 179, loss: 0.0015, instance_loss: 0.0014, weighted_loss: 0.0015, label: 1, bag_size: 12575\n",
      "batch 199, loss: 3.5980, instance_loss: 3.2163, weighted_loss: 3.4835, label: 0, bag_size: 3802\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 13225\n",
      "batch 239, loss: 0.1125, instance_loss: 0.0768, weighted_loss: 0.1018, label: 0, bag_size: 13205\n",
      "batch 259, loss: 0.0905, instance_loss: 0.0663, weighted_loss: 0.0833, label: 1, bag_size: 8982\n",
      "batch 279, loss: 0.4081, instance_loss: 0.5305, weighted_loss: 0.4448, label: 0, bag_size: 4418\n",
      "batch 299, loss: 0.0011, instance_loss: 0.0024, weighted_loss: 0.0015, label: 1, bag_size: 16512\n",
      "batch 319, loss: 0.0494, instance_loss: 0.0375, weighted_loss: 0.0458, label: 1, bag_size: 9147\n",
      "batch 339, loss: 0.0604, instance_loss: 0.0450, weighted_loss: 0.0558, label: 0, bag_size: 22681\n",
      "batch 359, loss: 0.0369, instance_loss: 0.0100, weighted_loss: 0.0288, label: 1, bag_size: 13786\n",
      "batch 379, loss: 0.0345, instance_loss: 0.0225, weighted_loss: 0.0309, label: 1, bag_size: 20161\n",
      "batch 399, loss: 0.0345, instance_loss: 0.0214, weighted_loss: 0.0305, label: 1, bag_size: 3295\n",
      "batch 419, loss: 0.0040, instance_loss: 0.0012, weighted_loss: 0.0031, label: 1, bag_size: 14030\n",
      "batch 439, loss: 0.0017, instance_loss: 0.0026, weighted_loss: 0.0020, label: 0, bag_size: 3160\n",
      "batch 459, loss: 0.0491, instance_loss: 0.0399, weighted_loss: 0.0464, label: 0, bag_size: 10113\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11735\n",
      "batch 499, loss: 0.1767, instance_loss: 0.1602, weighted_loss: 0.1717, label: 1, bag_size: 2308\n",
      "batch 519, loss: 0.0160, instance_loss: 0.0063, weighted_loss: 0.0131, label: 0, bag_size: 19067\n",
      "batch 539, loss: 0.0866, instance_loss: 0.0356, weighted_loss: 0.0713, label: 1, bag_size: 3651\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 11383\n",
      "batch 579, loss: 0.0719, instance_loss: 0.0724, weighted_loss: 0.0720, label: 0, bag_size: 11151\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11113\n",
      "batch 619, loss: 0.0020, instance_loss: 0.0044, weighted_loss: 0.0027, label: 0, bag_size: 1639\n",
      "batch 639, loss: 0.0004, instance_loss: 0.0002, weighted_loss: 0.0003, label: 1, bag_size: 9230\n",
      "batch 659, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 3101\n",
      "batch 679, loss: 0.0087, instance_loss: 0.0010, weighted_loss: 0.0064, label: 0, bag_size: 11194\n",
      "batch 699, loss: 0.0017, instance_loss: 0.0007, weighted_loss: 0.0014, label: 1, bag_size: 5833\n",
      "batch 719, loss: 0.0082, instance_loss: 0.0093, weighted_loss: 0.0085, label: 1, bag_size: 11394\n",
      "batch 739, loss: 0.3009, instance_loss: 0.4020, weighted_loss: 0.3312, label: 0, bag_size: 9616\n",
      "batch 759, loss: 0.0148, instance_loss: 0.0055, weighted_loss: 0.0120, label: 0, bag_size: 18240\n",
      "batch 779, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 1213\n",
      "batch 799, loss: 0.0394, instance_loss: 0.0110, weighted_loss: 0.0309, label: 0, bag_size: 10942\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0033, weighted_loss: 0.0010, label: 1, bag_size: 4877\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9830792682926829: correct 12898/13120\n",
      "class 1 clustering acc 0.9214939024390244: correct 6045/6560\n",
      "Epoch: 42, train_loss: 0.1425, train_clustering_loss:  0.1599, train_error: 0.0488\n",
      "class 0: acc 0.948780487804878, correct 389/410\n",
      "class 1: acc 0.9536585365853658, correct 391/410\n",
      "\n",
      "Val Set, val_loss: 0.2418, val_error: 0.1000, auc: 0.9768\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0065, instance_loss: 0.0079, weighted_loss: 0.0070, label: 1, bag_size: 13477\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 11642\n",
      "batch 59, loss: 0.0086, instance_loss: 0.0091, weighted_loss: 0.0088, label: 1, bag_size: 16154\n",
      "batch 79, loss: 0.0003, instance_loss: 0.0006, weighted_loss: 0.0004, label: 1, bag_size: 12408\n",
      "batch 99, loss: 0.0240, instance_loss: 0.0247, weighted_loss: 0.0242, label: 1, bag_size: 1759\n",
      "batch 119, loss: 0.0022, instance_loss: 0.0017, weighted_loss: 0.0021, label: 0, bag_size: 6281\n",
      "batch 139, loss: 0.0156, instance_loss: 0.0151, weighted_loss: 0.0154, label: 1, bag_size: 865\n",
      "batch 159, loss: 0.0019, instance_loss: 0.0012, weighted_loss: 0.0017, label: 0, bag_size: 1745\n",
      "batch 179, loss: 0.0015, instance_loss: 0.0002, weighted_loss: 0.0011, label: 1, bag_size: 9478\n",
      "batch 199, loss: 0.0005, instance_loss: 0.0310, weighted_loss: 0.0097, label: 1, bag_size: 10112\n",
      "batch 219, loss: 0.0034, instance_loss: 0.0050, weighted_loss: 0.0039, label: 0, bag_size: 17630\n",
      "batch 239, loss: 0.0011, instance_loss: 0.0011, weighted_loss: 0.0011, label: 1, bag_size: 1638\n",
      "batch 259, loss: 0.0047, instance_loss: 0.0038, weighted_loss: 0.0044, label: 1, bag_size: 9230\n",
      "batch 279, loss: 0.3717, instance_loss: 0.2761, weighted_loss: 0.3430, label: 0, bag_size: 3541\n",
      "batch 299, loss: 0.5484, instance_loss: 0.6608, weighted_loss: 0.5821, label: 0, bag_size: 21361\n",
      "batch 319, loss: 0.0286, instance_loss: 0.0156, weighted_loss: 0.0247, label: 0, bag_size: 2654\n",
      "batch 339, loss: 0.0041, instance_loss: 0.0018, weighted_loss: 0.0034, label: 0, bag_size: 3552\n",
      "batch 359, loss: 0.0213, instance_loss: 0.0144, weighted_loss: 0.0192, label: 1, bag_size: 5516\n",
      "batch 379, loss: 0.0062, instance_loss: 0.0058, weighted_loss: 0.0061, label: 1, bag_size: 5894\n",
      "batch 399, loss: 0.0006, instance_loss: 0.0007, weighted_loss: 0.0006, label: 1, bag_size: 16267\n",
      "batch 419, loss: 0.0010, instance_loss: 0.0069, weighted_loss: 0.0028, label: 1, bag_size: 3640\n",
      "batch 439, loss: 0.0118, instance_loss: 0.0140, weighted_loss: 0.0125, label: 0, bag_size: 4345\n",
      "batch 459, loss: 0.0019, instance_loss: 0.0041, weighted_loss: 0.0026, label: 1, bag_size: 2278\n",
      "batch 479, loss: 0.0241, instance_loss: 0.0264, weighted_loss: 0.0248, label: 0, bag_size: 8959\n",
      "batch 499, loss: 0.0143, instance_loss: 0.0115, weighted_loss: 0.0134, label: 1, bag_size: 5256\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0003, weighted_loss: 0.0002, label: 1, bag_size: 5991\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0991, weighted_loss: 0.0297, label: 1, bag_size: 9971\n",
      "batch 559, loss: 0.0035, instance_loss: 0.0007, weighted_loss: 0.0027, label: 0, bag_size: 11194\n",
      "batch 579, loss: 0.0657, instance_loss: 0.0499, weighted_loss: 0.0610, label: 0, bag_size: 1745\n",
      "batch 599, loss: 0.0036, instance_loss: 0.0023, weighted_loss: 0.0032, label: 0, bag_size: 13795\n",
      "batch 619, loss: 0.0084, instance_loss: 0.0078, weighted_loss: 0.0082, label: 0, bag_size: 2511\n",
      "batch 639, loss: 0.6171, instance_loss: 0.7495, weighted_loss: 0.6568, label: 0, bag_size: 2213\n",
      "batch 659, loss: 0.0011, instance_loss: 0.0026, weighted_loss: 0.0016, label: 1, bag_size: 7613\n",
      "batch 679, loss: 0.3700, instance_loss: 0.4708, weighted_loss: 0.4003, label: 1, bag_size: 8103\n",
      "batch 699, loss: 2.3450, instance_loss: 2.8065, weighted_loss: 2.4835, label: 1, bag_size: 13367\n",
      "batch 719, loss: 0.1292, instance_loss: 0.1191, weighted_loss: 0.1262, label: 0, bag_size: 15071\n",
      "batch 739, loss: 0.0009, instance_loss: 0.0002, weighted_loss: 0.0007, label: 0, bag_size: 931\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 17633\n",
      "batch 779, loss: 0.1832, instance_loss: 0.1922, weighted_loss: 0.1859, label: 1, bag_size: 1339\n",
      "batch 799, loss: 0.0035, instance_loss: 0.0047, weighted_loss: 0.0039, label: 0, bag_size: 1760\n",
      "batch 819, loss: 0.0009, instance_loss: 0.0013, weighted_loss: 0.0010, label: 1, bag_size: 9533\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9842987804878048: correct 12914/13120\n",
      "class 1 clustering acc 0.9195121951219513: correct 6032/6560\n",
      "Epoch: 43, train_loss: 0.1244, train_clustering_loss:  0.1470, train_error: 0.0524\n",
      "class 0: acc 0.9507389162561576, correct 386/406\n",
      "class 1: acc 0.9444444444444444, correct 391/414\n",
      "\n",
      "Val Set, val_loss: 0.2366, val_error: 0.1000, auc: 0.9765\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0144, instance_loss: 0.0141, weighted_loss: 0.0143, label: 0, bag_size: 1416\n",
      "batch 39, loss: 0.0751, instance_loss: 0.0605, weighted_loss: 0.0707, label: 0, bag_size: 5639\n",
      "batch 59, loss: 0.4613, instance_loss: 0.5259, weighted_loss: 0.4807, label: 1, bag_size: 1095\n",
      "batch 79, loss: 0.0068, instance_loss: 0.0020, weighted_loss: 0.0054, label: 0, bag_size: 11187\n",
      "batch 99, loss: 0.0020, instance_loss: 0.0015, weighted_loss: 0.0018, label: 1, bag_size: 629\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0002, weighted_loss: 0.0002, label: 1, bag_size: 15716\n",
      "batch 139, loss: 0.0039, instance_loss: 0.0026, weighted_loss: 0.0035, label: 0, bag_size: 10365\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 19472\n",
      "batch 179, loss: 0.0182, instance_loss: 0.0184, weighted_loss: 0.0182, label: 0, bag_size: 14264\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 1, bag_size: 10920\n",
      "batch 219, loss: 0.0006, instance_loss: 0.0016, weighted_loss: 0.0009, label: 1, bag_size: 6875\n",
      "batch 239, loss: 0.0945, instance_loss: 0.0776, weighted_loss: 0.0895, label: 0, bag_size: 11146\n",
      "batch 259, loss: 0.0308, instance_loss: 0.0160, weighted_loss: 0.0263, label: 0, bag_size: 2043\n",
      "batch 279, loss: 0.2259, instance_loss: 0.2166, weighted_loss: 0.2231, label: 1, bag_size: 2308\n",
      "batch 299, loss: 0.0023, instance_loss: 0.0006, weighted_loss: 0.0018, label: 1, bag_size: 16379\n",
      "batch 319, loss: 0.0005, instance_loss: 0.0003, weighted_loss: 0.0004, label: 0, bag_size: 18045\n",
      "batch 339, loss: 0.0168, instance_loss: 0.0170, weighted_loss: 0.0169, label: 0, bag_size: 8420\n",
      "batch 359, loss: 0.0955, instance_loss: 0.2156, weighted_loss: 0.1315, label: 1, bag_size: 684\n",
      "batch 379, loss: 0.0734, instance_loss: 0.0466, weighted_loss: 0.0654, label: 0, bag_size: 12149\n",
      "batch 399, loss: 0.0103, instance_loss: 0.0206, weighted_loss: 0.0134, label: 1, bag_size: 3082\n",
      "batch 419, loss: 0.0003, instance_loss: 0.0285, weighted_loss: 0.0087, label: 0, bag_size: 13225\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0005, weighted_loss: 0.0002, label: 1, bag_size: 8448\n",
      "batch 459, loss: 0.0320, instance_loss: 0.0103, weighted_loss: 0.0254, label: 0, bag_size: 18415\n",
      "batch 479, loss: 0.0112, instance_loss: 0.0016, weighted_loss: 0.0083, label: 1, bag_size: 2136\n",
      "batch 499, loss: 0.0026, instance_loss: 0.0002, weighted_loss: 0.0018, label: 1, bag_size: 16267\n",
      "batch 519, loss: 0.0006, instance_loss: 0.0002, weighted_loss: 0.0005, label: 0, bag_size: 13225\n",
      "batch 539, loss: 0.0020, instance_loss: 0.0105, weighted_loss: 0.0046, label: 0, bag_size: 14319\n",
      "batch 559, loss: 0.0254, instance_loss: 0.0339, weighted_loss: 0.0279, label: 1, bag_size: 5256\n",
      "batch 579, loss: 0.0009, instance_loss: 0.0012, weighted_loss: 0.0010, label: 1, bag_size: 9078\n",
      "batch 599, loss: 0.0002, instance_loss: 0.0002, weighted_loss: 0.0002, label: 1, bag_size: 6317\n",
      "batch 619, loss: 0.0070, instance_loss: 0.0113, weighted_loss: 0.0083, label: 0, bag_size: 14377\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 9471\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 2424\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0016, weighted_loss: 0.0005, label: 1, bag_size: 14223\n",
      "batch 699, loss: 0.0003, instance_loss: 0.0022, weighted_loss: 0.0008, label: 0, bag_size: 24439\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 11512\n",
      "batch 739, loss: 0.0115, instance_loss: 0.0456, weighted_loss: 0.0217, label: 1, bag_size: 13026\n",
      "batch 759, loss: 0.0072, instance_loss: 0.0045, weighted_loss: 0.0064, label: 1, bag_size: 19972\n",
      "batch 779, loss: 4.2290, instance_loss: 4.3549, weighted_loss: 4.2667, label: 0, bag_size: 3802\n",
      "batch 799, loss: 0.0256, instance_loss: 0.0176, weighted_loss: 0.0232, label: 0, bag_size: 2814\n",
      "batch 819, loss: 0.0001, instance_loss: 0.0722, weighted_loss: 0.0217, label: 1, bag_size: 7119\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9839939024390244: correct 12910/13120\n",
      "class 1 clustering acc 0.9149390243902439: correct 6002/6560\n",
      "Epoch: 44, train_loss: 0.1667, train_clustering_loss:  0.1819, train_error: 0.0561\n",
      "class 0: acc 0.9432098765432099, correct 382/405\n",
      "class 1: acc 0.944578313253012, correct 392/415\n",
      "\n",
      "Val Set, val_loss: 0.2377, val_error: 0.0909, auc: 0.9765\n",
      "class 0 clustering acc 0.9460227272727273: correct 1665/1760\n",
      "class 1 clustering acc 0.17272727272727273: correct 152/880\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0058, instance_loss: 0.0041, weighted_loss: 0.0053, label: 1, bag_size: 4821\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0017, weighted_loss: 0.0006, label: 0, bag_size: 9885\n",
      "batch 59, loss: 0.0031, instance_loss: 0.0070, weighted_loss: 0.0042, label: 0, bag_size: 10365\n",
      "batch 79, loss: 0.0015, instance_loss: 0.0019, weighted_loss: 0.0016, label: 1, bag_size: 11387\n",
      "batch 99, loss: 0.7639, instance_loss: 1.0024, weighted_loss: 0.8354, label: 1, bag_size: 898\n",
      "batch 119, loss: 0.0033, instance_loss: 0.0019, weighted_loss: 0.0029, label: 1, bag_size: 5894\n",
      "batch 139, loss: 0.0018, instance_loss: 0.0007, weighted_loss: 0.0015, label: 0, bag_size: 1349\n",
      "batch 159, loss: 0.0005, instance_loss: 0.0003, weighted_loss: 0.0004, label: 1, bag_size: 1838\n",
      "batch 179, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 4250\n",
      "batch 199, loss: 0.0175, instance_loss: 0.0122, weighted_loss: 0.0159, label: 1, bag_size: 13365\n",
      "batch 219, loss: 2.5647, instance_loss: 3.1285, weighted_loss: 2.7338, label: 0, bag_size: 7428\n",
      "batch 239, loss: 0.8019, instance_loss: 1.0370, weighted_loss: 0.8724, label: 0, bag_size: 2653\n",
      "batch 259, loss: 0.0116, instance_loss: 0.0134, weighted_loss: 0.0121, label: 1, bag_size: 11394\n",
      "batch 279, loss: 0.0021, instance_loss: 0.0006, weighted_loss: 0.0017, label: 0, bag_size: 2282\n",
      "batch 299, loss: 0.1105, instance_loss: 0.1237, weighted_loss: 0.1145, label: 1, bag_size: 1525\n",
      "batch 319, loss: 0.0003, instance_loss: 0.0026, weighted_loss: 0.0009, label: 1, bag_size: 11642\n",
      "batch 339, loss: 0.0038, instance_loss: 0.0023, weighted_loss: 0.0034, label: 1, bag_size: 13255\n",
      "batch 359, loss: 1.6135, instance_loss: 1.8781, weighted_loss: 1.6929, label: 0, bag_size: 9597\n",
      "batch 379, loss: 1.2905, instance_loss: 1.1787, weighted_loss: 1.2569, label: 0, bag_size: 3802\n",
      "batch 399, loss: 0.0079, instance_loss: 0.0031, weighted_loss: 0.0065, label: 1, bag_size: 6731\n",
      "batch 419, loss: 0.0151, instance_loss: 0.0076, weighted_loss: 0.0129, label: 0, bag_size: 2511\n",
      "batch 439, loss: 0.0363, instance_loss: 0.0212, weighted_loss: 0.0318, label: 1, bag_size: 1015\n",
      "batch 459, loss: 0.0850, instance_loss: 0.1654, weighted_loss: 0.1091, label: 0, bag_size: 3654\n",
      "batch 479, loss: 0.0071, instance_loss: 0.0015, weighted_loss: 0.0054, label: 1, bag_size: 11600\n",
      "batch 499, loss: 0.0225, instance_loss: 0.0222, weighted_loss: 0.0225, label: 1, bag_size: 1819\n",
      "batch 519, loss: 0.0004, instance_loss: 0.0001, weighted_loss: 0.0003, label: 0, bag_size: 4465\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 17633\n",
      "batch 559, loss: 0.0006, instance_loss: 0.0005, weighted_loss: 0.0005, label: 0, bag_size: 19466\n",
      "batch 579, loss: 0.0820, instance_loss: 0.0832, weighted_loss: 0.0824, label: 0, bag_size: 2628\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 7513\n",
      "batch 619, loss: 0.0114, instance_loss: 0.0081, weighted_loss: 0.0104, label: 0, bag_size: 19518\n",
      "batch 639, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 5409\n",
      "batch 659, loss: 0.0097, instance_loss: 0.0068, weighted_loss: 0.0088, label: 1, bag_size: 5340\n",
      "batch 679, loss: 0.3584, instance_loss: 0.4531, weighted_loss: 0.3868, label: 0, bag_size: 1760\n",
      "batch 699, loss: 0.0045, instance_loss: 0.0018, weighted_loss: 0.0037, label: 1, bag_size: 15233\n",
      "batch 719, loss: 0.3869, instance_loss: 0.5092, weighted_loss: 0.4236, label: 1, bag_size: 1703\n",
      "batch 739, loss: 0.0001, instance_loss: 0.0104, weighted_loss: 0.0032, label: 1, bag_size: 9955\n",
      "batch 759, loss: 0.0009, instance_loss: 0.0001, weighted_loss: 0.0006, label: 0, bag_size: 20796\n",
      "batch 779, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 7381\n",
      "batch 799, loss: 0.0060, instance_loss: 0.0029, weighted_loss: 0.0051, label: 1, bag_size: 7613\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 7823\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9830030487804878: correct 12897/13120\n",
      "class 1 clustering acc 0.910670731707317: correct 5974/6560\n",
      "Epoch: 45, train_loss: 0.1438, train_clustering_loss:  0.1677, train_error: 0.0561\n",
      "class 0: acc 0.9358288770053476, correct 350/374\n",
      "class 1: acc 0.9506726457399103, correct 424/446\n",
      "\n",
      "Val Set, val_loss: 0.3002, val_error: 0.1273, auc: 0.9765\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.7931034482758621, correct 46/58\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0049, instance_loss: 0.0044, weighted_loss: 0.0047, label: 1, bag_size: 34356\n",
      "batch 39, loss: 0.0119, instance_loss: 0.0068, weighted_loss: 0.0104, label: 1, bag_size: 13440\n",
      "batch 59, loss: 0.0786, instance_loss: 0.0656, weighted_loss: 0.0747, label: 0, bag_size: 9616\n",
      "batch 79, loss: 0.1649, instance_loss: 0.1894, weighted_loss: 0.1723, label: 1, bag_size: 5903\n",
      "batch 99, loss: 0.0185, instance_loss: 0.0105, weighted_loss: 0.0161, label: 0, bag_size: 1560\n",
      "batch 119, loss: 0.0159, instance_loss: 0.0212, weighted_loss: 0.0175, label: 1, bag_size: 4039\n",
      "batch 139, loss: 0.9908, instance_loss: 1.1132, weighted_loss: 1.0275, label: 1, bag_size: 6682\n",
      "batch 159, loss: 0.0193, instance_loss: 0.0197, weighted_loss: 0.0194, label: 1, bag_size: 2681\n",
      "batch 179, loss: 0.0028, instance_loss: 0.0040, weighted_loss: 0.0032, label: 0, bag_size: 1614\n",
      "batch 199, loss: 0.0759, instance_loss: 0.0498, weighted_loss: 0.0680, label: 1, bag_size: 7669\n",
      "batch 219, loss: 0.0049, instance_loss: 0.0017, weighted_loss: 0.0039, label: 0, bag_size: 3101\n",
      "batch 239, loss: 0.0068, instance_loss: 0.0043, weighted_loss: 0.0061, label: 0, bag_size: 6652\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 7011\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 23398\n",
      "batch 299, loss: 0.0352, instance_loss: 0.0310, weighted_loss: 0.0340, label: 0, bag_size: 2624\n",
      "batch 319, loss: 0.0003, instance_loss: 0.0077, weighted_loss: 0.0025, label: 1, bag_size: 617\n",
      "batch 339, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 10396\n",
      "batch 359, loss: 0.0000, instance_loss: 0.3488, weighted_loss: 0.1046, label: 1, bag_size: 1781\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9470\n",
      "batch 399, loss: 0.0062, instance_loss: 0.0071, weighted_loss: 0.0065, label: 1, bag_size: 3409\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 14202\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 12575\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15077\n",
      "batch 479, loss: 0.0214, instance_loss: 0.0090, weighted_loss: 0.0177, label: 1, bag_size: 5340\n",
      "batch 499, loss: 1.1091, instance_loss: 1.5379, weighted_loss: 1.2377, label: 0, bag_size: 5120\n",
      "batch 519, loss: 0.0200, instance_loss: 0.0194, weighted_loss: 0.0198, label: 1, bag_size: 15609\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0004, weighted_loss: 0.0004, label: 1, bag_size: 8438\n",
      "batch 559, loss: 0.0341, instance_loss: 0.0312, weighted_loss: 0.0333, label: 0, bag_size: 18954\n",
      "batch 579, loss: 0.0021, instance_loss: 0.0012, weighted_loss: 0.0018, label: 1, bag_size: 19972\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 10867\n",
      "batch 619, loss: 0.0106, instance_loss: 0.0120, weighted_loss: 0.0110, label: 1, bag_size: 7351\n",
      "batch 639, loss: 0.0008, instance_loss: 0.0016, weighted_loss: 0.0010, label: 1, bag_size: 5317\n",
      "batch 659, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 16341\n",
      "batch 679, loss: 0.0032, instance_loss: 0.0025, weighted_loss: 0.0030, label: 1, bag_size: 9078\n",
      "batch 699, loss: 0.0008, instance_loss: 0.0011, weighted_loss: 0.0009, label: 0, bag_size: 13992\n",
      "batch 719, loss: 0.2655, instance_loss: 0.3349, weighted_loss: 0.2863, label: 0, bag_size: 1614\n",
      "batch 739, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 0, bag_size: 4271\n",
      "batch 759, loss: 0.0014, instance_loss: 0.0022, weighted_loss: 0.0017, label: 0, bag_size: 22681\n",
      "batch 779, loss: 5.9844, instance_loss: 4.7661, weighted_loss: 5.6189, label: 1, bag_size: 1497\n",
      "batch 799, loss: 0.2822, instance_loss: 0.1675, weighted_loss: 0.2478, label: 0, bag_size: 1772\n",
      "batch 819, loss: 0.0026, instance_loss: 0.0005, weighted_loss: 0.0020, label: 1, bag_size: 10622\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9836128048780488: correct 12905/13120\n",
      "class 1 clustering acc 0.9280487804878049: correct 6088/6560\n",
      "Epoch: 46, train_loss: 0.1255, train_clustering_loss:  0.1448, train_error: 0.0512\n",
      "class 0: acc 0.9385026737967914, correct 351/374\n",
      "class 1: acc 0.9573991031390134, correct 427/446\n",
      "\n",
      "Val Set, val_loss: 0.2977, val_error: 0.1455, auc: 0.9784\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.011363636363636364: correct 10/880\n",
      "class 0: acc 0.7307692307692307, correct 38/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0048, instance_loss: 0.0046, weighted_loss: 0.0047, label: 0, bag_size: 18415\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 9069\n",
      "batch 59, loss: 0.3134, instance_loss: 0.3552, weighted_loss: 0.3259, label: 1, bag_size: 2314\n",
      "batch 79, loss: 0.0101, instance_loss: 0.0605, weighted_loss: 0.0252, label: 1, bag_size: 8019\n",
      "batch 99, loss: 0.0025, instance_loss: 0.0047, weighted_loss: 0.0032, label: 0, bag_size: 21093\n",
      "batch 119, loss: 0.0034, instance_loss: 0.0046, weighted_loss: 0.0037, label: 0, bag_size: 2628\n",
      "batch 139, loss: 0.0032, instance_loss: 0.0004, weighted_loss: 0.0024, label: 1, bag_size: 7381\n",
      "batch 159, loss: 0.3138, instance_loss: 0.3996, weighted_loss: 0.3395, label: 1, bag_size: 898\n",
      "batch 179, loss: 0.0006, instance_loss: 0.0011, weighted_loss: 0.0008, label: 1, bag_size: 1823\n",
      "batch 199, loss: 0.0048, instance_loss: 0.0022, weighted_loss: 0.0040, label: 0, bag_size: 26271\n",
      "batch 219, loss: 0.0231, instance_loss: 0.0141, weighted_loss: 0.0204, label: 0, bag_size: 17268\n",
      "batch 239, loss: 0.1700, instance_loss: 0.1353, weighted_loss: 0.1596, label: 1, bag_size: 3652\n",
      "batch 259, loss: 0.0005, instance_loss: 0.0001, weighted_loss: 0.0004, label: 1, bag_size: 11642\n",
      "batch 279, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 10128\n",
      "batch 299, loss: 0.0017, instance_loss: 0.0011, weighted_loss: 0.0015, label: 1, bag_size: 3980\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 0, bag_size: 16052\n",
      "batch 339, loss: 0.0038, instance_loss: 0.0044, weighted_loss: 0.0040, label: 0, bag_size: 1884\n",
      "batch 359, loss: 0.0028, instance_loss: 0.0218, weighted_loss: 0.0085, label: 1, bag_size: 7110\n",
      "batch 379, loss: 0.0315, instance_loss: 0.0200, weighted_loss: 0.0281, label: 1, bag_size: 6478\n",
      "batch 399, loss: 0.0117, instance_loss: 0.0030, weighted_loss: 0.0091, label: 1, bag_size: 20161\n",
      "batch 419, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 19932\n",
      "batch 439, loss: 0.0031, instance_loss: 0.0022, weighted_loss: 0.0028, label: 1, bag_size: 4239\n",
      "batch 459, loss: 0.0027, instance_loss: 0.0032, weighted_loss: 0.0029, label: 1, bag_size: 4394\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 2303\n",
      "batch 499, loss: 6.8427, instance_loss: 5.4089, weighted_loss: 6.4126, label: 1, bag_size: 1095\n",
      "batch 519, loss: 0.0282, instance_loss: 0.0134, weighted_loss: 0.0238, label: 0, bag_size: 2043\n",
      "batch 539, loss: 0.2397, instance_loss: 0.3082, weighted_loss: 0.2603, label: 0, bag_size: 1637\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0040, weighted_loss: 0.0013, label: 0, bag_size: 19390\n",
      "batch 579, loss: 0.0786, instance_loss: 0.0592, weighted_loss: 0.0728, label: 0, bag_size: 1772\n",
      "batch 599, loss: 0.0008, instance_loss: 0.0021, weighted_loss: 0.0012, label: 1, bag_size: 6734\n",
      "batch 619, loss: 0.0768, instance_loss: 0.0726, weighted_loss: 0.0755, label: 0, bag_size: 2104\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 1, bag_size: 2495\n",
      "batch 659, loss: 0.0075, instance_loss: 0.0053, weighted_loss: 0.0069, label: 0, bag_size: 10444\n",
      "batch 679, loss: 0.3371, instance_loss: 0.4145, weighted_loss: 0.3603, label: 0, bag_size: 2815\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11389\n",
      "batch 719, loss: 0.0376, instance_loss: 0.0456, weighted_loss: 0.0400, label: 1, bag_size: 11220\n",
      "batch 739, loss: 0.0670, instance_loss: 0.0675, weighted_loss: 0.0671, label: 0, bag_size: 1789\n",
      "batch 759, loss: 0.0326, instance_loss: 0.0441, weighted_loss: 0.0361, label: 1, bag_size: 3211\n",
      "batch 779, loss: 0.0081, instance_loss: 0.0047, weighted_loss: 0.0071, label: 1, bag_size: 10848\n",
      "batch 799, loss: 0.0007, instance_loss: 0.0012, weighted_loss: 0.0008, label: 1, bag_size: 6317\n",
      "batch 819, loss: 0.0005, instance_loss: 0.0016, weighted_loss: 0.0008, label: 1, bag_size: 9533\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9813262195121951: correct 12875/13120\n",
      "class 1 clustering acc 0.9173780487804878: correct 6018/6560\n",
      "Epoch: 47, train_loss: 0.1619, train_clustering_loss:  0.1845, train_error: 0.0610\n",
      "class 0: acc 0.9370277078085643, correct 372/397\n",
      "class 1: acc 0.9408983451536643, correct 398/423\n",
      "\n",
      "Val Set, val_loss: 0.5336, val_error: 0.2000, auc: 0.9738\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.5769230769230769, correct 30/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0058, instance_loss: 0.0052, weighted_loss: 0.0056, label: 1, bag_size: 4250\n",
      "batch 39, loss: 0.0171, instance_loss: 0.0149, weighted_loss: 0.0164, label: 0, bag_size: 25814\n",
      "batch 59, loss: 1.4510, instance_loss: 2.2257, weighted_loss: 1.6834, label: 0, bag_size: 3375\n",
      "batch 79, loss: 0.0083, instance_loss: 0.0058, weighted_loss: 0.0075, label: 1, bag_size: 6736\n",
      "batch 99, loss: 0.0011, instance_loss: 0.0003, weighted_loss: 0.0009, label: 0, bag_size: 21076\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 0, bag_size: 11759\n",
      "batch 139, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 18415\n",
      "batch 159, loss: 0.0014, instance_loss: 0.0005, weighted_loss: 0.0012, label: 0, bag_size: 6356\n",
      "batch 179, loss: 0.0189, instance_loss: 0.0109, weighted_loss: 0.0165, label: 0, bag_size: 26271\n",
      "batch 199, loss: 0.0007, instance_loss: 0.0661, weighted_loss: 0.0203, label: 1, bag_size: 1101\n",
      "batch 219, loss: 0.0062, instance_loss: 0.0082, weighted_loss: 0.0068, label: 0, bag_size: 14828\n",
      "batch 239, loss: 0.0775, instance_loss: 0.0462, weighted_loss: 0.0681, label: 0, bag_size: 9387\n",
      "batch 259, loss: 0.0113, instance_loss: 0.0074, weighted_loss: 0.0102, label: 0, bag_size: 3459\n",
      "batch 279, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 19390\n",
      "batch 299, loss: 0.0139, instance_loss: 0.0103, weighted_loss: 0.0128, label: 1, bag_size: 5345\n",
      "batch 319, loss: 0.0321, instance_loss: 0.0221, weighted_loss: 0.0291, label: 1, bag_size: 20537\n",
      "batch 339, loss: 0.0184, instance_loss: 0.0158, weighted_loss: 0.0176, label: 0, bag_size: 10444\n",
      "batch 359, loss: 0.1597, instance_loss: 0.1903, weighted_loss: 0.1689, label: 0, bag_size: 1732\n",
      "batch 379, loss: 0.0679, instance_loss: 0.0522, weighted_loss: 0.0632, label: 0, bag_size: 12732\n",
      "batch 399, loss: 0.0178, instance_loss: 0.0120, weighted_loss: 0.0160, label: 1, bag_size: 2814\n",
      "batch 419, loss: 0.0018, instance_loss: 0.0021, weighted_loss: 0.0019, label: 0, bag_size: 11125\n",
      "batch 439, loss: 0.0142, instance_loss: 0.0142, weighted_loss: 0.0142, label: 1, bag_size: 2278\n",
      "batch 459, loss: 1.1766, instance_loss: 2.6521, weighted_loss: 1.6193, label: 0, bag_size: 2694\n",
      "batch 479, loss: 0.0227, instance_loss: 0.0174, weighted_loss: 0.0211, label: 0, bag_size: 14333\n",
      "batch 499, loss: 0.0246, instance_loss: 0.0772, weighted_loss: 0.0404, label: 1, bag_size: 689\n",
      "batch 519, loss: 0.0138, instance_loss: 0.0151, weighted_loss: 0.0142, label: 1, bag_size: 9561\n",
      "batch 539, loss: 0.0072, instance_loss: 0.0045, weighted_loss: 0.0064, label: 1, bag_size: 9408\n",
      "batch 559, loss: 0.1239, instance_loss: 0.1441, weighted_loss: 0.1300, label: 1, bag_size: 2179\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 2732\n",
      "batch 599, loss: 0.6702, instance_loss: 0.8392, weighted_loss: 0.7209, label: 0, bag_size: 15898\n",
      "batch 619, loss: 0.0038, instance_loss: 0.0033, weighted_loss: 0.0036, label: 0, bag_size: 3198\n",
      "batch 639, loss: 0.0348, instance_loss: 0.0423, weighted_loss: 0.0371, label: 1, bag_size: 16514\n",
      "batch 659, loss: 0.0123, instance_loss: 0.0069, weighted_loss: 0.0107, label: 0, bag_size: 25420\n",
      "batch 679, loss: 0.0153, instance_loss: 0.0145, weighted_loss: 0.0150, label: 0, bag_size: 3893\n",
      "batch 699, loss: 0.0210, instance_loss: 0.0371, weighted_loss: 0.0258, label: 1, bag_size: 21701\n",
      "batch 719, loss: 0.0004, instance_loss: 0.0003, weighted_loss: 0.0004, label: 0, bag_size: 16936\n",
      "batch 739, loss: 0.0016, instance_loss: 0.0087, weighted_loss: 0.0037, label: 1, bag_size: 6927\n",
      "batch 759, loss: 0.0001, instance_loss: 0.5025, weighted_loss: 0.1508, label: 1, bag_size: 5605\n",
      "batch 779, loss: 0.3889, instance_loss: 0.4078, weighted_loss: 0.3946, label: 0, bag_size: 2104\n",
      "batch 799, loss: 0.0006, instance_loss: 0.0004, weighted_loss: 0.0006, label: 0, bag_size: 6281\n",
      "batch 819, loss: 0.0009, instance_loss: 0.0003, weighted_loss: 0.0008, label: 1, bag_size: 621\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9782774390243902: correct 12835/13120\n",
      "class 1 clustering acc 0.881859756097561: correct 5785/6560\n",
      "Epoch: 48, train_loss: 0.1799, train_clustering_loss:  0.2204, train_error: 0.0744\n",
      "class 0: acc 0.9217171717171717, correct 365/396\n",
      "class 1: acc 0.9292452830188679, correct 394/424\n",
      "\n",
      "Val Set, val_loss: 0.2492, val_error: 0.1000, auc: 0.9771\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0076, instance_loss: 0.0055, weighted_loss: 0.0070, label: 0, bag_size: 3557\n",
      "batch 39, loss: 0.0005, instance_loss: 0.0007, weighted_loss: 0.0005, label: 0, bag_size: 21218\n",
      "batch 59, loss: 0.0004, instance_loss: 0.0001, weighted_loss: 0.0003, label: 1, bag_size: 15213\n",
      "batch 79, loss: 0.6262, instance_loss: 0.8987, weighted_loss: 0.7080, label: 0, bag_size: 2959\n",
      "batch 99, loss: 0.0097, instance_loss: 0.0108, weighted_loss: 0.0100, label: 0, bag_size: 1438\n",
      "batch 119, loss: 0.0086, instance_loss: 0.0080, weighted_loss: 0.0084, label: 0, bag_size: 16211\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0008, weighted_loss: 0.0003, label: 1, bag_size: 4423\n",
      "batch 159, loss: 0.0155, instance_loss: 0.0142, weighted_loss: 0.0151, label: 0, bag_size: 10113\n",
      "batch 179, loss: 1.2158, instance_loss: 1.8365, weighted_loss: 1.4020, label: 0, bag_size: 4418\n",
      "batch 199, loss: 0.0288, instance_loss: 0.0622, weighted_loss: 0.0388, label: 1, bag_size: 1822\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0001, weighted_loss: 0.0002, label: 1, bag_size: 7767\n",
      "batch 239, loss: 0.0212, instance_loss: 0.0123, weighted_loss: 0.0185, label: 1, bag_size: 5629\n",
      "batch 259, loss: 0.7527, instance_loss: 0.8148, weighted_loss: 0.7713, label: 1, bag_size: 2179\n",
      "batch 279, loss: 0.7501, instance_loss: 1.0020, weighted_loss: 0.8257, label: 0, bag_size: 11128\n",
      "batch 299, loss: 0.0039, instance_loss: 0.0028, weighted_loss: 0.0036, label: 1, bag_size: 14681\n",
      "batch 319, loss: 1.6331, instance_loss: 1.7437, weighted_loss: 1.6663, label: 1, bag_size: 1497\n",
      "batch 339, loss: 0.0034, instance_loss: 0.0016, weighted_loss: 0.0028, label: 1, bag_size: 5629\n",
      "batch 359, loss: 0.1137, instance_loss: 0.0628, weighted_loss: 0.0984, label: 1, bag_size: 2480\n",
      "batch 379, loss: 0.0386, instance_loss: 0.0375, weighted_loss: 0.0383, label: 0, bag_size: 9596\n",
      "batch 399, loss: 0.0063, instance_loss: 0.0054, weighted_loss: 0.0060, label: 1, bag_size: 8592\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 1072\n",
      "batch 439, loss: 0.0006, instance_loss: 0.0003, weighted_loss: 0.0005, label: 0, bag_size: 3198\n",
      "batch 459, loss: 0.0017, instance_loss: 0.0020, weighted_loss: 0.0018, label: 1, bag_size: 8410\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0583, weighted_loss: 0.0175, label: 1, bag_size: 645\n",
      "batch 499, loss: 0.0018, instance_loss: 0.0019, weighted_loss: 0.0018, label: 1, bag_size: 7217\n",
      "batch 519, loss: 0.0638, instance_loss: 0.0377, weighted_loss: 0.0560, label: 1, bag_size: 3450\n",
      "batch 539, loss: 0.0036, instance_loss: 0.0018, weighted_loss: 0.0031, label: 1, bag_size: 4423\n",
      "batch 559, loss: 0.0002, instance_loss: 0.0003, weighted_loss: 0.0002, label: 0, bag_size: 1614\n",
      "batch 579, loss: 0.0086, instance_loss: 0.0030, weighted_loss: 0.0069, label: 1, bag_size: 19972\n",
      "batch 599, loss: 0.0131, instance_loss: 0.0068, weighted_loss: 0.0113, label: 0, bag_size: 10751\n",
      "batch 619, loss: 0.0024, instance_loss: 0.0010, weighted_loss: 0.0020, label: 1, bag_size: 11122\n",
      "batch 639, loss: 0.0015, instance_loss: 0.0001, weighted_loss: 0.0011, label: 1, bag_size: 16379\n",
      "batch 659, loss: 0.2668, instance_loss: 0.2839, weighted_loss: 0.2719, label: 0, bag_size: 6624\n",
      "batch 679, loss: 0.0388, instance_loss: 0.0265, weighted_loss: 0.0351, label: 0, bag_size: 1690\n",
      "batch 699, loss: 0.1039, instance_loss: 0.1209, weighted_loss: 0.1090, label: 1, bag_size: 1764\n",
      "batch 719, loss: 0.2955, instance_loss: 0.3229, weighted_loss: 0.3037, label: 0, bag_size: 2266\n",
      "batch 739, loss: 2.0142, instance_loss: 2.5664, weighted_loss: 2.1798, label: 1, bag_size: 2565\n",
      "batch 759, loss: 0.0328, instance_loss: 0.0174, weighted_loss: 0.0282, label: 0, bag_size: 26271\n",
      "batch 779, loss: 0.0032, instance_loss: 0.0010, weighted_loss: 0.0025, label: 0, bag_size: 1213\n",
      "batch 799, loss: 0.2340, instance_loss: 0.0708, weighted_loss: 0.1850, label: 1, bag_size: 9321\n",
      "batch 819, loss: 0.0117, instance_loss: 0.0117, weighted_loss: 0.0117, label: 1, bag_size: 1437\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9797256097560976: correct 12854/13120\n",
      "class 1 clustering acc 0.8702743902439024: correct 5709/6560\n",
      "Epoch: 49, train_loss: 0.1917, train_clustering_loss:  0.2248, train_error: 0.0878\n",
      "class 0: acc 0.9232613908872902, correct 385/417\n",
      "class 1: acc 0.9007444168734491, correct 363/403\n",
      "\n",
      "Val Set, val_loss: 0.1769, val_error: 0.0545, auc: 0.9828\n",
      "class 0 clustering acc 0.990909090909091: correct 1744/1760\n",
      "class 1 clustering acc 0.053409090909090906: correct 47/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "Validation loss decreased (0.186012 --> 0.176909).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0003, instance_loss: 0.0033, weighted_loss: 0.0012, label: 1, bag_size: 8868\n",
      "batch 39, loss: 0.0004, instance_loss: 0.0089, weighted_loss: 0.0029, label: 1, bag_size: 11266\n",
      "batch 59, loss: 0.0427, instance_loss: 0.0368, weighted_loss: 0.0409, label: 1, bag_size: 2278\n",
      "batch 79, loss: 0.0011, instance_loss: 0.0014, weighted_loss: 0.0012, label: 0, bag_size: 24911\n",
      "batch 99, loss: 0.1229, instance_loss: 0.1240, weighted_loss: 0.1232, label: 1, bag_size: 16514\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0007, weighted_loss: 0.0003, label: 0, bag_size: 2534\n",
      "batch 139, loss: 0.0011, instance_loss: 0.0004, weighted_loss: 0.0009, label: 1, bag_size: 9408\n",
      "batch 159, loss: 0.0064, instance_loss: 0.0076, weighted_loss: 0.0067, label: 1, bag_size: 1924\n",
      "batch 179, loss: 0.1218, instance_loss: 0.1503, weighted_loss: 0.1304, label: 0, bag_size: 1701\n",
      "batch 199, loss: 0.0207, instance_loss: 0.0192, weighted_loss: 0.0202, label: 0, bag_size: 705\n",
      "batch 219, loss: 0.0042, instance_loss: 0.0066, weighted_loss: 0.0049, label: 1, bag_size: 10112\n",
      "batch 239, loss: 0.0025, instance_loss: 0.0032, weighted_loss: 0.0027, label: 1, bag_size: 2785\n",
      "batch 259, loss: 0.2600, instance_loss: 0.2704, weighted_loss: 0.2631, label: 0, bag_size: 5297\n",
      "batch 279, loss: 0.0004, instance_loss: 0.0029, weighted_loss: 0.0011, label: 0, bag_size: 6356\n",
      "batch 299, loss: 0.0269, instance_loss: 0.0125, weighted_loss: 0.0226, label: 0, bag_size: 12593\n",
      "batch 319, loss: 0.4983, instance_loss: 0.4446, weighted_loss: 0.4822, label: 1, bag_size: 8103\n",
      "batch 339, loss: 0.0032, instance_loss: 0.0091, weighted_loss: 0.0050, label: 1, bag_size: 5894\n",
      "batch 359, loss: 0.0242, instance_loss: 0.0516, weighted_loss: 0.0324, label: 1, bag_size: 3211\n",
      "batch 379, loss: 0.0395, instance_loss: 0.0403, weighted_loss: 0.0397, label: 0, bag_size: 14681\n",
      "batch 399, loss: 0.0073, instance_loss: 0.0063, weighted_loss: 0.0070, label: 1, bag_size: 9078\n",
      "batch 419, loss: 0.0011, instance_loss: 0.0002, weighted_loss: 0.0009, label: 0, bag_size: 12593\n",
      "batch 439, loss: 0.3546, instance_loss: 0.4053, weighted_loss: 0.3698, label: 1, bag_size: 21450\n",
      "batch 459, loss: 0.0024, instance_loss: 0.0064, weighted_loss: 0.0036, label: 1, bag_size: 8012\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 12795\n",
      "batch 499, loss: 0.0092, instance_loss: 0.0085, weighted_loss: 0.0089, label: 0, bag_size: 14333\n",
      "batch 519, loss: 0.0163, instance_loss: 0.0028, weighted_loss: 0.0123, label: 0, bag_size: 2760\n",
      "batch 539, loss: 0.0002, instance_loss: 0.0002, weighted_loss: 0.0002, label: 0, bag_size: 8372\n",
      "batch 559, loss: 0.0055, instance_loss: 0.0052, weighted_loss: 0.0054, label: 0, bag_size: 14681\n",
      "batch 579, loss: 0.2008, instance_loss: 0.2845, weighted_loss: 0.2259, label: 1, bag_size: 1819\n",
      "batch 599, loss: 0.0256, instance_loss: 0.0152, weighted_loss: 0.0225, label: 0, bag_size: 10444\n",
      "batch 619, loss: 0.0005, instance_loss: 0.0001, weighted_loss: 0.0004, label: 0, bag_size: 18240\n",
      "batch 639, loss: 0.0141, instance_loss: 0.0078, weighted_loss: 0.0122, label: 0, bag_size: 2242\n",
      "batch 659, loss: 0.0154, instance_loss: 0.0131, weighted_loss: 0.0147, label: 0, bag_size: 1438\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 7191\n",
      "batch 699, loss: 0.2271, instance_loss: 0.3146, weighted_loss: 0.2533, label: 0, bag_size: 3321\n",
      "batch 719, loss: 0.0293, instance_loss: 0.0234, weighted_loss: 0.0275, label: 1, bag_size: 2314\n",
      "batch 739, loss: 0.0304, instance_loss: 0.0225, weighted_loss: 0.0280, label: 1, bag_size: 12719\n",
      "batch 759, loss: 0.0120, instance_loss: 0.0117, weighted_loss: 0.0119, label: 1, bag_size: 11266\n",
      "batch 779, loss: 0.0520, instance_loss: 0.0488, weighted_loss: 0.0510, label: 0, bag_size: 1920\n",
      "batch 799, loss: 0.4259, instance_loss: 0.6622, weighted_loss: 0.4968, label: 1, bag_size: 2937\n",
      "batch 819, loss: 0.1393, instance_loss: 0.1020, weighted_loss: 0.1281, label: 1, bag_size: 15609\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.980640243902439: correct 12866/13120\n",
      "class 1 clustering acc 0.9114329268292682: correct 5979/6560\n",
      "Epoch: 50, train_loss: 0.1559, train_clustering_loss:  0.1846, train_error: 0.0573\n",
      "class 0: acc 0.9362745098039216, correct 382/408\n",
      "class 1: acc 0.9490291262135923, correct 391/412\n",
      "\n",
      "Val Set, val_loss: 0.2942, val_error: 0.1273, auc: 0.9791\n",
      "class 0 clustering acc 0.9954545454545455: correct 1752/1760\n",
      "class 1 clustering acc 0.035227272727272725: correct 31/880\n",
      "class 0: acc 0.9807692307692307, correct 51/52\n",
      "class 1: acc 0.7758620689655172, correct 45/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0005, instance_loss: 0.0009, weighted_loss: 0.0006, label: 0, bag_size: 13777\n",
      "batch 39, loss: 0.0016, instance_loss: 0.0001, weighted_loss: 0.0012, label: 0, bag_size: 13225\n",
      "batch 59, loss: 0.0147, instance_loss: 0.0119, weighted_loss: 0.0138, label: 1, bag_size: 3651\n",
      "batch 79, loss: 0.0005, instance_loss: 0.0030, weighted_loss: 0.0013, label: 0, bag_size: 1213\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8330\n",
      "batch 119, loss: 0.0006, instance_loss: 0.0013, weighted_loss: 0.0008, label: 1, bag_size: 5991\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 13892\n",
      "batch 159, loss: 0.0398, instance_loss: 0.0283, weighted_loss: 0.0363, label: 1, bag_size: 5231\n",
      "batch 179, loss: 0.0135, instance_loss: 0.0133, weighted_loss: 0.0134, label: 0, bag_size: 803\n",
      "batch 199, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 5991\n",
      "batch 219, loss: 3.4005, instance_loss: 3.9714, weighted_loss: 3.5717, label: 0, bag_size: 5105\n",
      "batch 239, loss: 0.0391, instance_loss: 0.0243, weighted_loss: 0.0347, label: 1, bag_size: 10492\n",
      "batch 259, loss: 0.0026, instance_loss: 0.0033, weighted_loss: 0.0028, label: 0, bag_size: 23796\n",
      "batch 279, loss: 0.0003, instance_loss: 0.0007, weighted_loss: 0.0005, label: 0, bag_size: 21682\n",
      "batch 299, loss: 0.3868, instance_loss: 0.5028, weighted_loss: 0.4216, label: 1, bag_size: 5366\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 10725\n",
      "batch 339, loss: 0.0025, instance_loss: 0.0000, weighted_loss: 0.0018, label: 1, bag_size: 10501\n",
      "batch 359, loss: 0.0108, instance_loss: 0.0085, weighted_loss: 0.0101, label: 1, bag_size: 13692\n",
      "batch 379, loss: 0.0245, instance_loss: 0.0182, weighted_loss: 0.0226, label: 0, bag_size: 12910\n",
      "batch 399, loss: 0.0006, instance_loss: 0.0006, weighted_loss: 0.0006, label: 0, bag_size: 1213\n",
      "batch 419, loss: 0.0012, instance_loss: 0.0012, weighted_loss: 0.0012, label: 0, bag_size: 23368\n",
      "batch 439, loss: 0.0065, instance_loss: 0.0076, weighted_loss: 0.0068, label: 1, bag_size: 5454\n",
      "batch 459, loss: 0.0000, instance_loss: 0.3404, weighted_loss: 0.1021, label: 1, bag_size: 10867\n",
      "batch 479, loss: 0.0249, instance_loss: 0.0323, weighted_loss: 0.0271, label: 1, bag_size: 21827\n",
      "batch 499, loss: 0.0192, instance_loss: 0.0260, weighted_loss: 0.0213, label: 1, bag_size: 1919\n",
      "batch 519, loss: 0.0637, instance_loss: 0.0603, weighted_loss: 0.0626, label: 0, bag_size: 25814\n",
      "batch 539, loss: 0.0021, instance_loss: 0.0015, weighted_loss: 0.0019, label: 1, bag_size: 14030\n",
      "batch 559, loss: 0.0098, instance_loss: 0.0076, weighted_loss: 0.0092, label: 1, bag_size: 5605\n",
      "batch 579, loss: 0.0346, instance_loss: 0.0207, weighted_loss: 0.0304, label: 1, bag_size: 11223\n",
      "batch 599, loss: 0.5834, instance_loss: 0.6387, weighted_loss: 0.6000, label: 1, bag_size: 1831\n",
      "batch 619, loss: 0.0019, instance_loss: 0.0009, weighted_loss: 0.0016, label: 1, bag_size: 7246\n",
      "batch 639, loss: 0.0015, instance_loss: 0.0026, weighted_loss: 0.0018, label: 0, bag_size: 1202\n",
      "batch 659, loss: 0.0130, instance_loss: 0.0205, weighted_loss: 0.0152, label: 1, bag_size: 3082\n",
      "batch 679, loss: 0.0653, instance_loss: 0.0485, weighted_loss: 0.0602, label: 0, bag_size: 803\n",
      "batch 699, loss: 0.3521, instance_loss: 0.4063, weighted_loss: 0.3684, label: 1, bag_size: 2179\n",
      "batch 719, loss: 0.0175, instance_loss: 0.0104, weighted_loss: 0.0154, label: 1, bag_size: 10281\n",
      "batch 739, loss: 0.0051, instance_loss: 0.0046, weighted_loss: 0.0049, label: 0, bag_size: 14206\n",
      "batch 759, loss: 0.0104, instance_loss: 0.0072, weighted_loss: 0.0095, label: 0, bag_size: 22681\n",
      "batch 779, loss: 0.0032, instance_loss: 0.0018, weighted_loss: 0.0028, label: 0, bag_size: 2814\n",
      "batch 799, loss: 0.0039, instance_loss: 0.0005, weighted_loss: 0.0029, label: 0, bag_size: 10365\n",
      "batch 819, loss: 0.0045, instance_loss: 0.0030, weighted_loss: 0.0041, label: 1, bag_size: 12895\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9789634146341464: correct 12844/13120\n",
      "class 1 clustering acc 0.8807926829268292: correct 5778/6560\n",
      "Epoch: 51, train_loss: 0.1713, train_clustering_loss:  0.1989, train_error: 0.0768\n",
      "class 0: acc 0.9168765743073047, correct 364/397\n",
      "class 1: acc 0.9290780141843972, correct 393/423\n",
      "\n",
      "Val Set, val_loss: 0.3767, val_error: 0.1909, auc: 0.9188\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.01818181818181818: correct 16/880\n",
      "class 0: acc 0.6923076923076923, correct 36/52\n",
      "class 1: acc 0.9137931034482759, correct 53/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5845, instance_loss: 0.7318, weighted_loss: 0.6287, label: 1, bag_size: 8103\n",
      "batch 39, loss: 0.0020, instance_loss: 0.0012, weighted_loss: 0.0017, label: 1, bag_size: 13255\n",
      "batch 59, loss: 0.0068, instance_loss: 0.0040, weighted_loss: 0.0059, label: 0, bag_size: 3893\n",
      "batch 79, loss: 0.0010, instance_loss: 0.0005, weighted_loss: 0.0009, label: 1, bag_size: 5494\n",
      "batch 99, loss: 0.0084, instance_loss: 0.0027, weighted_loss: 0.0067, label: 1, bag_size: 4308\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 0, bag_size: 3552\n",
      "batch 139, loss: 0.0366, instance_loss: 0.0201, weighted_loss: 0.0317, label: 1, bag_size: 1831\n",
      "batch 159, loss: 0.5615, instance_loss: 0.6435, weighted_loss: 0.5861, label: 0, bag_size: 13332\n",
      "batch 179, loss: 0.0004, instance_loss: 0.0126, weighted_loss: 0.0040, label: 1, bag_size: 2140\n",
      "batch 199, loss: 0.0997, instance_loss: 0.1610, weighted_loss: 0.1181, label: 0, bag_size: 1800\n",
      "batch 219, loss: 0.0639, instance_loss: 0.0685, weighted_loss: 0.0653, label: 1, bag_size: 6682\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 15008\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 6356\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0006, label: 1, bag_size: 14681\n",
      "batch 299, loss: 0.1765, instance_loss: 0.1198, weighted_loss: 0.1595, label: 0, bag_size: 4523\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 1, bag_size: 3082\n",
      "batch 339, loss: 0.3002, instance_loss: 0.2675, weighted_loss: 0.2904, label: 1, bag_size: 12712\n",
      "batch 359, loss: 0.0028, instance_loss: 0.0019, weighted_loss: 0.0026, label: 0, bag_size: 13205\n",
      "batch 379, loss: 0.1603, instance_loss: 0.2005, weighted_loss: 0.1724, label: 1, bag_size: 8103\n",
      "batch 399, loss: 0.0041, instance_loss: 0.0024, weighted_loss: 0.0036, label: 1, bag_size: 5516\n",
      "batch 419, loss: 0.0818, instance_loss: 0.0518, weighted_loss: 0.0728, label: 0, bag_size: 10995\n",
      "batch 439, loss: 0.0017, instance_loss: 0.0012, weighted_loss: 0.0016, label: 1, bag_size: 30675\n",
      "batch 459, loss: 1.3121, instance_loss: 1.9054, weighted_loss: 1.4901, label: 1, bag_size: 1703\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 4423\n",
      "batch 499, loss: 0.0438, instance_loss: 0.0321, weighted_loss: 0.0403, label: 0, bag_size: 16690\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 15001\n",
      "batch 539, loss: 3.2860, instance_loss: 3.6366, weighted_loss: 3.3911, label: 0, bag_size: 2815\n",
      "batch 559, loss: 0.0284, instance_loss: 0.0099, weighted_loss: 0.0229, label: 1, bag_size: 9571\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 15967\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 11778\n",
      "batch 619, loss: 0.0080, instance_loss: 0.0060, weighted_loss: 0.0074, label: 1, bag_size: 1255\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0003, weighted_loss: 0.0002, label: 1, bag_size: 9673\n",
      "batch 659, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 13174\n",
      "batch 679, loss: 0.0003, instance_loss: 0.0001, weighted_loss: 0.0002, label: 0, bag_size: 2548\n",
      "batch 699, loss: 0.0054, instance_loss: 0.0053, weighted_loss: 0.0053, label: 1, bag_size: 15125\n",
      "batch 719, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 1, bag_size: 6734\n",
      "batch 739, loss: 0.0090, instance_loss: 0.0113, weighted_loss: 0.0097, label: 0, bag_size: 1814\n",
      "batch 759, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 16052\n",
      "batch 779, loss: 0.2059, instance_loss: 0.1620, weighted_loss: 0.1928, label: 1, bag_size: 2935\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14319\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 21682\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9817835365853659: correct 12881/13120\n",
      "class 1 clustering acc 0.9102134146341463: correct 5971/6560\n",
      "Epoch: 52, train_loss: 0.1470, train_clustering_loss:  0.1740, train_error: 0.0573\n",
      "class 0: acc 0.9458128078817734, correct 384/406\n",
      "class 1: acc 0.9396135265700483, correct 389/414\n",
      "\n",
      "Val Set, val_loss: 0.2236, val_error: 0.0818, auc: 0.9735\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.8653846153846154, correct 45/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1157, instance_loss: 0.1221, weighted_loss: 0.1176, label: 1, bag_size: 5454\n",
      "batch 39, loss: 0.0004, instance_loss: 0.0002, weighted_loss: 0.0004, label: 1, bag_size: 13947\n",
      "batch 59, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 1701\n",
      "batch 79, loss: 0.0306, instance_loss: 0.0104, weighted_loss: 0.0245, label: 1, bag_size: 9561\n",
      "batch 99, loss: 0.0024, instance_loss: 0.0018, weighted_loss: 0.0022, label: 1, bag_size: 2936\n",
      "batch 119, loss: 0.0169, instance_loss: 0.0142, weighted_loss: 0.0161, label: 1, bag_size: 3980\n",
      "batch 139, loss: 0.0165, instance_loss: 0.0122, weighted_loss: 0.0152, label: 0, bag_size: 2336\n",
      "batch 159, loss: 0.0004, instance_loss: 0.0018, weighted_loss: 0.0008, label: 1, bag_size: 16051\n",
      "batch 179, loss: 0.0101, instance_loss: 0.0193, weighted_loss: 0.0129, label: 1, bag_size: 6736\n",
      "batch 199, loss: 0.0042, instance_loss: 0.0021, weighted_loss: 0.0035, label: 0, bag_size: 9949\n",
      "batch 219, loss: 0.4327, instance_loss: 0.5064, weighted_loss: 0.4548, label: 1, bag_size: 5903\n",
      "batch 239, loss: 0.2233, instance_loss: 0.2234, weighted_loss: 0.2233, label: 0, bag_size: 1506\n",
      "batch 259, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 12217\n",
      "batch 279, loss: 0.0020, instance_loss: 0.0008, weighted_loss: 0.0017, label: 1, bag_size: 12758\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 6966\n",
      "batch 319, loss: 0.9704, instance_loss: 1.2579, weighted_loss: 1.0566, label: 1, bag_size: 2731\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 6875\n",
      "batch 359, loss: 0.0006, instance_loss: 0.0001, weighted_loss: 0.0005, label: 0, bag_size: 2098\n",
      "batch 379, loss: 0.0115, instance_loss: 0.0150, weighted_loss: 0.0125, label: 0, bag_size: 12212\n",
      "batch 399, loss: 0.0012, instance_loss: 0.0008, weighted_loss: 0.0011, label: 0, bag_size: 10068\n",
      "batch 419, loss: 1.3395, instance_loss: 1.8247, weighted_loss: 1.4851, label: 1, bag_size: 898\n",
      "batch 439, loss: 0.0020, instance_loss: 0.0006, weighted_loss: 0.0016, label: 1, bag_size: 16267\n",
      "batch 459, loss: 0.9648, instance_loss: 1.0889, weighted_loss: 1.0020, label: 1, bag_size: 7424\n",
      "batch 479, loss: 0.0437, instance_loss: 0.0226, weighted_loss: 0.0373, label: 0, bag_size: 2814\n",
      "batch 499, loss: 0.3202, instance_loss: 0.3486, weighted_loss: 0.3287, label: 1, bag_size: 1609\n",
      "batch 519, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 16267\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 5221\n",
      "batch 559, loss: 0.0163, instance_loss: 0.0089, weighted_loss: 0.0141, label: 0, bag_size: 2998\n",
      "batch 579, loss: 0.0366, instance_loss: 0.0273, weighted_loss: 0.0338, label: 1, bag_size: 8475\n",
      "batch 599, loss: 0.0094, instance_loss: 0.0083, weighted_loss: 0.0091, label: 1, bag_size: 6343\n",
      "batch 619, loss: 0.0403, instance_loss: 0.0419, weighted_loss: 0.0408, label: 1, bag_size: 5340\n",
      "batch 639, loss: 0.0354, instance_loss: 0.0204, weighted_loss: 0.0309, label: 1, bag_size: 1294\n",
      "batch 659, loss: 0.1845, instance_loss: 0.1815, weighted_loss: 0.1836, label: 1, bag_size: 1755\n",
      "batch 679, loss: 0.0038, instance_loss: 0.0019, weighted_loss: 0.0032, label: 1, bag_size: 4250\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 18225\n",
      "batch 719, loss: 0.0008, instance_loss: 0.0004, weighted_loss: 0.0006, label: 0, bag_size: 14305\n",
      "batch 739, loss: 0.0100, instance_loss: 0.0055, weighted_loss: 0.0087, label: 0, bag_size: 1452\n",
      "batch 759, loss: 0.0245, instance_loss: 0.0187, weighted_loss: 0.0227, label: 0, bag_size: 47866\n",
      "batch 779, loss: 0.0099, instance_loss: 0.0115, weighted_loss: 0.0104, label: 1, bag_size: 2278\n",
      "batch 799, loss: 0.6002, instance_loss: 0.7180, weighted_loss: 0.6355, label: 0, bag_size: 1437\n",
      "batch 819, loss: 0.0011, instance_loss: 0.0007, weighted_loss: 0.0010, label: 1, bag_size: 2695\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.978125: correct 12833/13120\n",
      "class 1 clustering acc 0.8899390243902439: correct 5838/6560\n",
      "Epoch: 53, train_loss: 0.1733, train_clustering_loss:  0.2016, train_error: 0.0683\n",
      "class 0: acc 0.9364303178484108, correct 383/409\n",
      "class 1: acc 0.927007299270073, correct 381/411\n",
      "\n",
      "Val Set, val_loss: 0.2234, val_error: 0.1000, auc: 0.9765\n",
      "class 0 clustering acc 0.990909090909091: correct 1744/1760\n",
      "class 1 clustering acc 0.06136363636363636: correct 54/880\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0152, instance_loss: 0.0145, weighted_loss: 0.0149, label: 0, bag_size: 2063\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 1984\n",
      "batch 59, loss: 0.0076, instance_loss: 0.0071, weighted_loss: 0.0075, label: 1, bag_size: 8438\n",
      "batch 79, loss: 0.0031, instance_loss: 0.0019, weighted_loss: 0.0028, label: 1, bag_size: 1014\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9689\n",
      "batch 119, loss: 0.1100, instance_loss: 0.1580, weighted_loss: 0.1244, label: 1, bag_size: 2937\n",
      "batch 139, loss: 0.0003, instance_loss: 0.0007, weighted_loss: 0.0004, label: 1, bag_size: 16565\n",
      "batch 159, loss: 0.1396, instance_loss: 0.1163, weighted_loss: 0.1326, label: 0, bag_size: 705\n",
      "batch 179, loss: 0.0008, instance_loss: 0.0006, weighted_loss: 0.0007, label: 0, bag_size: 1202\n",
      "batch 199, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 1891\n",
      "batch 219, loss: 0.0150, instance_loss: 0.0144, weighted_loss: 0.0148, label: 0, bag_size: 1920\n",
      "batch 239, loss: 0.0989, instance_loss: 0.0827, weighted_loss: 0.0941, label: 1, bag_size: 8040\n",
      "batch 259, loss: 0.0004, instance_loss: 0.0014, weighted_loss: 0.0007, label: 1, bag_size: 2638\n",
      "batch 279, loss: 0.0511, instance_loss: 0.0529, weighted_loss: 0.0517, label: 1, bag_size: 699\n",
      "batch 299, loss: 0.0004, instance_loss: 0.0012, weighted_loss: 0.0006, label: 1, bag_size: 6792\n",
      "batch 319, loss: 0.0033, instance_loss: 0.0085, weighted_loss: 0.0048, label: 0, bag_size: 5297\n",
      "batch 339, loss: 0.2308, instance_loss: 0.2326, weighted_loss: 0.2314, label: 0, bag_size: 4959\n",
      "batch 359, loss: 0.0014, instance_loss: 0.0052, weighted_loss: 0.0025, label: 1, bag_size: 7515\n",
      "batch 379, loss: 0.0078, instance_loss: 0.0079, weighted_loss: 0.0078, label: 0, bag_size: 8788\n",
      "batch 399, loss: 0.0022, instance_loss: 0.0017, weighted_loss: 0.0020, label: 1, bag_size: 4821\n",
      "batch 419, loss: 0.0021, instance_loss: 0.0006, weighted_loss: 0.0016, label: 0, bag_size: 27158\n",
      "batch 439, loss: 0.7836, instance_loss: 0.9654, weighted_loss: 0.8382, label: 1, bag_size: 1683\n",
      "batch 459, loss: 0.3673, instance_loss: 0.3569, weighted_loss: 0.3642, label: 0, bag_size: 1814\n",
      "batch 479, loss: 0.0003, instance_loss: 0.0007, weighted_loss: 0.0004, label: 1, bag_size: 6745\n",
      "batch 499, loss: 0.0032, instance_loss: 0.0087, weighted_loss: 0.0048, label: 1, bag_size: 6090\n",
      "batch 519, loss: 0.0071, instance_loss: 0.0074, weighted_loss: 0.0072, label: 1, bag_size: 3980\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 1, bag_size: 10392\n",
      "batch 559, loss: 0.0115, instance_loss: 0.0175, weighted_loss: 0.0133, label: 1, bag_size: 1512\n",
      "batch 579, loss: 0.0089, instance_loss: 0.0172, weighted_loss: 0.0114, label: 1, bag_size: 8602\n",
      "batch 599, loss: 0.0223, instance_loss: 0.0131, weighted_loss: 0.0196, label: 1, bag_size: 12931\n",
      "batch 619, loss: 0.0578, instance_loss: 0.1330, weighted_loss: 0.0804, label: 1, bag_size: 5454\n",
      "batch 639, loss: 0.3030, instance_loss: 0.5449, weighted_loss: 0.3756, label: 0, bag_size: 2098\n",
      "batch 659, loss: 0.0056, instance_loss: 0.0066, weighted_loss: 0.0059, label: 1, bag_size: 11363\n",
      "batch 679, loss: 0.0056, instance_loss: 0.0067, weighted_loss: 0.0059, label: 1, bag_size: 1294\n",
      "batch 699, loss: 0.0926, instance_loss: 0.0405, weighted_loss: 0.0770, label: 0, bag_size: 2998\n",
      "batch 719, loss: 0.0004, instance_loss: 0.0047, weighted_loss: 0.0017, label: 1, bag_size: 5864\n",
      "batch 739, loss: 0.0047, instance_loss: 0.0035, weighted_loss: 0.0043, label: 1, bag_size: 13947\n",
      "batch 759, loss: 0.0031, instance_loss: 0.0002, weighted_loss: 0.0022, label: 0, bag_size: 11194\n",
      "batch 779, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 4465\n",
      "batch 799, loss: 0.0035, instance_loss: 0.0033, weighted_loss: 0.0034, label: 1, bag_size: 3640\n",
      "batch 819, loss: 0.0147, instance_loss: 0.0090, weighted_loss: 0.0130, label: 0, bag_size: 11390\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9805640243902439: correct 12865/13120\n",
      "class 1 clustering acc 0.8841463414634146: correct 5800/6560\n",
      "Epoch: 54, train_loss: 0.1755, train_clustering_loss:  0.2073, train_error: 0.0707\n",
      "class 0: acc 0.9342723004694836, correct 398/426\n",
      "class 1: acc 0.9238578680203046, correct 364/394\n",
      "\n",
      "Val Set, val_loss: 0.2019, val_error: 0.1091, auc: 0.9814\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.019318181818181818: correct 17/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8275862068965517, correct 48/58\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11113\n",
      "batch 39, loss: 0.0048, instance_loss: 0.0027, weighted_loss: 0.0041, label: 0, bag_size: 3970\n",
      "batch 59, loss: 0.0053, instance_loss: 0.0048, weighted_loss: 0.0051, label: 1, bag_size: 6734\n",
      "batch 79, loss: 0.0681, instance_loss: 0.0482, weighted_loss: 0.0621, label: 1, bag_size: 11316\n",
      "batch 99, loss: 2.9413, instance_loss: 3.6354, weighted_loss: 3.1495, label: 1, bag_size: 8103\n",
      "batch 119, loss: 0.0026, instance_loss: 0.0003, weighted_loss: 0.0019, label: 0, bag_size: 2654\n",
      "batch 139, loss: 0.0266, instance_loss: 0.0184, weighted_loss: 0.0242, label: 0, bag_size: 24439\n",
      "batch 159, loss: 0.0330, instance_loss: 0.0232, weighted_loss: 0.0301, label: 0, bag_size: 2628\n",
      "batch 179, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 13795\n",
      "batch 199, loss: 0.0003, instance_loss: 0.0003, weighted_loss: 0.0003, label: 1, bag_size: 8019\n",
      "batch 219, loss: 0.0017, instance_loss: 0.0020, weighted_loss: 0.0018, label: 1, bag_size: 5907\n",
      "batch 239, loss: 0.0001, instance_loss: 0.0011, weighted_loss: 0.0004, label: 1, bag_size: 8522\n",
      "batch 259, loss: 0.0114, instance_loss: 0.0089, weighted_loss: 0.0107, label: 0, bag_size: 3810\n",
      "batch 279, loss: 2.4122, instance_loss: 2.8441, weighted_loss: 2.5417, label: 0, bag_size: 13332\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15841\n",
      "batch 319, loss: 0.1818, instance_loss: 0.1957, weighted_loss: 0.1860, label: 0, bag_size: 2213\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 18738\n",
      "batch 359, loss: 0.6880, instance_loss: 0.7576, weighted_loss: 0.7089, label: 0, bag_size: 2918\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 22800\n",
      "batch 399, loss: 0.0034, instance_loss: 0.0014, weighted_loss: 0.0028, label: 0, bag_size: 15001\n",
      "batch 419, loss: 0.0068, instance_loss: 0.0150, weighted_loss: 0.0092, label: 1, bag_size: 18095\n",
      "batch 439, loss: 0.9687, instance_loss: 1.1534, weighted_loss: 1.0241, label: 0, bag_size: 2694\n",
      "batch 459, loss: 0.0012, instance_loss: 0.0006, weighted_loss: 0.0010, label: 1, bag_size: 10281\n",
      "batch 479, loss: 0.0439, instance_loss: 0.0628, weighted_loss: 0.0496, label: 1, bag_size: 1038\n",
      "batch 499, loss: 0.5183, instance_loss: 0.5552, weighted_loss: 0.5293, label: 1, bag_size: 7768\n",
      "batch 519, loss: 0.0033, instance_loss: 0.0087, weighted_loss: 0.0050, label: 1, bag_size: 1924\n",
      "batch 539, loss: 0.0028, instance_loss: 0.0012, weighted_loss: 0.0023, label: 1, bag_size: 16565\n",
      "batch 559, loss: 0.0016, instance_loss: 0.0016, weighted_loss: 0.0016, label: 1, bag_size: 10394\n",
      "batch 579, loss: 0.0003, instance_loss: 0.0004, weighted_loss: 0.0004, label: 1, bag_size: 8685\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 1, bag_size: 10392\n",
      "batch 619, loss: 0.0114, instance_loss: 0.0058, weighted_loss: 0.0098, label: 1, bag_size: 10912\n",
      "batch 639, loss: 0.0005, instance_loss: 0.0038, weighted_loss: 0.0015, label: 0, bag_size: 11759\n",
      "batch 659, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 20150\n",
      "batch 679, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 2140\n",
      "batch 699, loss: 1.6605, instance_loss: 2.1824, weighted_loss: 1.8171, label: 1, bag_size: 21450\n",
      "batch 719, loss: 0.0089, instance_loss: 0.0062, weighted_loss: 0.0081, label: 1, bag_size: 10622\n",
      "batch 739, loss: 0.0195, instance_loss: 0.0243, weighted_loss: 0.0210, label: 0, bag_size: 1483\n",
      "batch 759, loss: 0.0013, instance_loss: 0.0007, weighted_loss: 0.0011, label: 1, bag_size: 11964\n",
      "batch 779, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 22828\n",
      "batch 799, loss: 0.0954, instance_loss: 0.0229, weighted_loss: 0.0736, label: 1, bag_size: 2638\n",
      "batch 819, loss: 0.0042, instance_loss: 0.0029, weighted_loss: 0.0038, label: 1, bag_size: 19832\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.979344512195122: correct 12849/13120\n",
      "class 1 clustering acc 0.8972560975609756: correct 5886/6560\n",
      "Epoch: 55, train_loss: 0.1577, train_clustering_loss:  0.1888, train_error: 0.0659\n",
      "class 0: acc 0.9371980676328503, correct 388/414\n",
      "class 1: acc 0.9310344827586207, correct 378/406\n",
      "\n",
      "Val Set, val_loss: 0.1903, val_error: 0.0818, auc: 0.9778\n",
      "class 0 clustering acc 0.9954545454545455: correct 1752/1760\n",
      "class 1 clustering acc 0.08636363636363636: correct 76/880\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0032, instance_loss: 0.0000, weighted_loss: 0.0023, label: 0, bag_size: 9234\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0023, weighted_loss: 0.0007, label: 0, bag_size: 23037\n",
      "batch 59, loss: 0.0004, instance_loss: 0.0003, weighted_loss: 0.0003, label: 0, bag_size: 9888\n",
      "batch 79, loss: 2.1402, instance_loss: 3.0685, weighted_loss: 2.4187, label: 1, bag_size: 898\n",
      "batch 99, loss: 1.2925, instance_loss: 1.1561, weighted_loss: 1.2516, label: 0, bag_size: 2351\n",
      "batch 119, loss: 0.0113, instance_loss: 0.0062, weighted_loss: 0.0098, label: 0, bag_size: 1614\n",
      "batch 139, loss: 0.0332, instance_loss: 0.0328, weighted_loss: 0.0330, label: 1, bag_size: 15689\n",
      "batch 159, loss: 0.0004, instance_loss: 0.0009, weighted_loss: 0.0005, label: 1, bag_size: 2136\n",
      "batch 179, loss: 0.0118, instance_loss: 0.0031, weighted_loss: 0.0092, label: 0, bag_size: 10415\n",
      "batch 199, loss: 0.0499, instance_loss: 0.0469, weighted_loss: 0.0490, label: 0, bag_size: 2694\n",
      "batch 219, loss: 0.0004, instance_loss: 0.0002, weighted_loss: 0.0003, label: 0, bag_size: 12593\n",
      "batch 239, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 22870\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21076\n",
      "batch 279, loss: 0.0719, instance_loss: 0.0580, weighted_loss: 0.0677, label: 0, bag_size: 9132\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0066, weighted_loss: 0.0021, label: 1, bag_size: 865\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 1712\n",
      "batch 339, loss: 0.0004, instance_loss: 0.1294, weighted_loss: 0.0391, label: 1, bag_size: 2146\n",
      "batch 359, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 4271\n",
      "batch 379, loss: 0.6948, instance_loss: 0.8177, weighted_loss: 0.7317, label: 1, bag_size: 7351\n",
      "batch 399, loss: 0.0165, instance_loss: 0.0175, weighted_loss: 0.0168, label: 0, bag_size: 14305\n",
      "batch 419, loss: 0.0011, instance_loss: 0.0016, weighted_loss: 0.0013, label: 1, bag_size: 6736\n",
      "batch 439, loss: 0.0084, instance_loss: 0.0058, weighted_loss: 0.0076, label: 1, bag_size: 21827\n",
      "batch 459, loss: 0.0090, instance_loss: 0.0083, weighted_loss: 0.0088, label: 1, bag_size: 10498\n",
      "batch 479, loss: 1.6048, instance_loss: 2.0891, weighted_loss: 1.7501, label: 1, bag_size: 13089\n",
      "batch 499, loss: 0.1907, instance_loss: 0.2262, weighted_loss: 0.2013, label: 0, bag_size: 11607\n",
      "batch 519, loss: 0.0579, instance_loss: 0.0417, weighted_loss: 0.0530, label: 1, bag_size: 12626\n",
      "batch 539, loss: 0.0154, instance_loss: 0.0140, weighted_loss: 0.0150, label: 0, bag_size: 25420\n",
      "batch 559, loss: 0.0050, instance_loss: 0.0034, weighted_loss: 0.0045, label: 1, bag_size: 11363\n",
      "batch 579, loss: 0.0244, instance_loss: 0.0243, weighted_loss: 0.0243, label: 1, bag_size: 10622\n",
      "batch 599, loss: 0.0287, instance_loss: 0.0335, weighted_loss: 0.0302, label: 0, bag_size: 13992\n",
      "batch 619, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 11654\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 1, bag_size: 8191\n",
      "batch 659, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 3970\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 22828\n",
      "batch 699, loss: 0.0024, instance_loss: 0.0016, weighted_loss: 0.0021, label: 1, bag_size: 2146\n",
      "batch 719, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 18240\n",
      "batch 739, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 1, bag_size: 5864\n",
      "batch 759, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 2036\n",
      "batch 779, loss: 0.1258, instance_loss: 0.0568, weighted_loss: 0.1051, label: 1, bag_size: 19606\n",
      "batch 799, loss: 0.2614, instance_loss: 0.1658, weighted_loss: 0.2327, label: 1, bag_size: 9215\n",
      "batch 819, loss: 0.1643, instance_loss: 0.2160, weighted_loss: 0.1798, label: 1, bag_size: 2480\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9801067073170732: correct 12859/13120\n",
      "class 1 clustering acc 0.9001524390243902: correct 5905/6560\n",
      "Epoch: 56, train_loss: 0.1499, train_clustering_loss:  0.1866, train_error: 0.0610\n",
      "class 0: acc 0.9423963133640553, correct 409/434\n",
      "class 1: acc 0.9352331606217616, correct 361/386\n",
      "\n",
      "Val Set, val_loss: 0.2512, val_error: 0.1000, auc: 0.9768\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.020454545454545454: correct 18/880\n",
      "class 0: acc 0.9807692307692307, correct 51/52\n",
      "class 1: acc 0.8275862068965517, correct 48/58\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0046, instance_loss: 0.0033, weighted_loss: 0.0042, label: 1, bag_size: 3980\n",
      "batch 39, loss: 0.0007, instance_loss: 0.0020, weighted_loss: 0.0011, label: 0, bag_size: 8981\n",
      "batch 59, loss: 0.0204, instance_loss: 0.0163, weighted_loss: 0.0192, label: 0, bag_size: 8549\n",
      "batch 79, loss: 0.0807, instance_loss: 0.0661, weighted_loss: 0.0764, label: 0, bag_size: 9455\n",
      "batch 99, loss: 0.0033, instance_loss: 0.0062, weighted_loss: 0.0042, label: 1, bag_size: 7613\n",
      "batch 119, loss: 0.0144, instance_loss: 0.0112, weighted_loss: 0.0134, label: 0, bag_size: 1639\n",
      "batch 139, loss: 0.8787, instance_loss: 1.0531, weighted_loss: 0.9310, label: 1, bag_size: 3121\n",
      "batch 159, loss: 0.0634, instance_loss: 0.0512, weighted_loss: 0.0597, label: 0, bag_size: 2266\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0005, label: 1, bag_size: 10920\n",
      "batch 199, loss: 0.0892, instance_loss: 0.0889, weighted_loss: 0.0891, label: 1, bag_size: 1444\n",
      "batch 219, loss: 0.0012, instance_loss: 0.0006, weighted_loss: 0.0010, label: 0, bag_size: 19043\n",
      "batch 239, loss: 0.0013, instance_loss: 0.0005, weighted_loss: 0.0011, label: 0, bag_size: 1884\n",
      "batch 259, loss: 0.0994, instance_loss: 0.1063, weighted_loss: 0.1015, label: 1, bag_size: 2455\n",
      "batch 279, loss: 0.0003, instance_loss: 0.0003, weighted_loss: 0.0003, label: 1, bag_size: 11363\n",
      "batch 299, loss: 0.0008, instance_loss: 0.0004, weighted_loss: 0.0007, label: 1, bag_size: 10482\n",
      "batch 319, loss: 0.0164, instance_loss: 0.0137, weighted_loss: 0.0156, label: 0, bag_size: 3670\n",
      "batch 339, loss: 0.0011, instance_loss: 0.0005, weighted_loss: 0.0009, label: 0, bag_size: 4271\n",
      "batch 359, loss: 1.2244, instance_loss: 2.5530, weighted_loss: 1.6230, label: 0, bag_size: 3897\n",
      "batch 379, loss: 0.0057, instance_loss: 0.0092, weighted_loss: 0.0068, label: 1, bag_size: 10432\n",
      "batch 399, loss: 0.0016, instance_loss: 0.0024, weighted_loss: 0.0018, label: 1, bag_size: 1823\n",
      "batch 419, loss: 0.0514, instance_loss: 0.0355, weighted_loss: 0.0466, label: 0, bag_size: 1349\n",
      "batch 439, loss: 0.0106, instance_loss: 0.0066, weighted_loss: 0.0094, label: 1, bag_size: 3640\n",
      "batch 459, loss: 0.0022, instance_loss: 0.0014, weighted_loss: 0.0019, label: 0, bag_size: 10146\n",
      "batch 479, loss: 0.0442, instance_loss: 0.0288, weighted_loss: 0.0396, label: 0, bag_size: 2624\n",
      "batch 499, loss: 0.0024, instance_loss: 0.0006, weighted_loss: 0.0019, label: 0, bag_size: 6898\n",
      "batch 519, loss: 0.6062, instance_loss: 0.7898, weighted_loss: 0.6613, label: 0, bag_size: 1142\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 8868\n",
      "batch 559, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 15213\n",
      "batch 579, loss: 0.0884, instance_loss: 0.1063, weighted_loss: 0.0938, label: 1, bag_size: 13692\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 19932\n",
      "batch 619, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 12178\n",
      "batch 639, loss: 0.0526, instance_loss: 0.0510, weighted_loss: 0.0521, label: 1, bag_size: 1284\n",
      "batch 659, loss: 0.0004, instance_loss: 0.0016, weighted_loss: 0.0008, label: 1, bag_size: 9878\n",
      "batch 679, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 2654\n",
      "batch 699, loss: 0.0013, instance_loss: 0.0014, weighted_loss: 0.0013, label: 1, bag_size: 7371\n",
      "batch 719, loss: 0.0011, instance_loss: 0.0006, weighted_loss: 0.0010, label: 0, bag_size: 11546\n",
      "batch 739, loss: 0.0014, instance_loss: 0.0019, weighted_loss: 0.0016, label: 1, bag_size: 16565\n",
      "batch 759, loss: 0.0234, instance_loss: 0.2845, weighted_loss: 0.1018, label: 1, bag_size: 2904\n",
      "batch 779, loss: 0.0020, instance_loss: 0.0012, weighted_loss: 0.0018, label: 1, bag_size: 2785\n",
      "batch 799, loss: 0.2240, instance_loss: 0.2214, weighted_loss: 0.2232, label: 0, bag_size: 9597\n",
      "batch 819, loss: 0.0373, instance_loss: 0.0311, weighted_loss: 0.0354, label: 0, bag_size: 1127\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9804115853658537: correct 12863/13120\n",
      "class 1 clustering acc 0.9024390243902439: correct 5920/6560\n",
      "Epoch: 57, train_loss: 0.1544, train_clustering_loss:  0.1845, train_error: 0.0659\n",
      "class 0: acc 0.9306666666666666, correct 349/375\n",
      "class 1: acc 0.9370786516853933, correct 417/445\n",
      "\n",
      "Val Set, val_loss: 0.1931, val_error: 0.0818, auc: 0.9808\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9955\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 15636\n",
      "batch 59, loss: 0.0409, instance_loss: 0.0235, weighted_loss: 0.0357, label: 0, bag_size: 2873\n",
      "batch 79, loss: 0.0637, instance_loss: 0.0529, weighted_loss: 0.0604, label: 0, bag_size: 24382\n",
      "batch 99, loss: 0.0248, instance_loss: 0.0194, weighted_loss: 0.0232, label: 1, bag_size: 8754\n",
      "batch 119, loss: 0.0030, instance_loss: 0.0016, weighted_loss: 0.0026, label: 0, bag_size: 10263\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 1888\n",
      "batch 159, loss: 0.0073, instance_loss: 0.0023, weighted_loss: 0.0058, label: 0, bag_size: 31780\n",
      "batch 179, loss: 0.0052, instance_loss: 0.0025, weighted_loss: 0.0044, label: 0, bag_size: 3265\n",
      "batch 199, loss: 0.1596, instance_loss: 0.1649, weighted_loss: 0.1612, label: 1, bag_size: 4929\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 1, bag_size: 9955\n",
      "batch 239, loss: 0.0106, instance_loss: 0.0033, weighted_loss: 0.0084, label: 0, bag_size: 10365\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0003, weighted_loss: 0.0002, label: 1, bag_size: 14230\n",
      "batch 279, loss: 0.4760, instance_loss: 0.3557, weighted_loss: 0.4399, label: 1, bag_size: 699\n",
      "batch 299, loss: 0.0040, instance_loss: 0.0026, weighted_loss: 0.0036, label: 0, bag_size: 12212\n",
      "batch 319, loss: 0.0311, instance_loss: 0.0224, weighted_loss: 0.0285, label: 0, bag_size: 16690\n",
      "batch 339, loss: 2.4170, instance_loss: 3.1098, weighted_loss: 2.6248, label: 1, bag_size: 1095\n",
      "batch 359, loss: 0.1860, instance_loss: 0.2203, weighted_loss: 0.1963, label: 0, bag_size: 1953\n",
      "batch 379, loss: 0.0080, instance_loss: 0.0051, weighted_loss: 0.0071, label: 0, bag_size: 13892\n",
      "batch 399, loss: 0.1500, instance_loss: 0.1528, weighted_loss: 0.1508, label: 0, bag_size: 2006\n",
      "batch 419, loss: 0.1075, instance_loss: 0.1033, weighted_loss: 0.1062, label: 0, bag_size: 1498\n",
      "batch 439, loss: 1.0240, instance_loss: 1.2951, weighted_loss: 1.1054, label: 1, bag_size: 6360\n",
      "batch 459, loss: 0.0103, instance_loss: 0.0067, weighted_loss: 0.0092, label: 1, bag_size: 6665\n",
      "batch 479, loss: 0.2841, instance_loss: 0.4100, weighted_loss: 0.3219, label: 1, bag_size: 1525\n",
      "batch 499, loss: 0.0020, instance_loss: 0.0005, weighted_loss: 0.0016, label: 1, bag_size: 8026\n",
      "batch 519, loss: 0.0003, instance_loss: 0.0001, weighted_loss: 0.0002, label: 0, bag_size: 12793\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 11518\n",
      "batch 559, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 9004\n",
      "batch 579, loss: 0.4487, instance_loss: 0.5733, weighted_loss: 0.4861, label: 1, bag_size: 12180\n",
      "batch 599, loss: 0.0491, instance_loss: 0.0320, weighted_loss: 0.0440, label: 1, bag_size: 10671\n",
      "batch 619, loss: 0.0022, instance_loss: 0.0038, weighted_loss: 0.0027, label: 1, bag_size: 2759\n",
      "batch 639, loss: 0.0055, instance_loss: 0.0116, weighted_loss: 0.0073, label: 0, bag_size: 2091\n",
      "batch 659, loss: 0.0001, instance_loss: 0.0007, weighted_loss: 0.0003, label: 1, bag_size: 9877\n",
      "batch 679, loss: 2.5097, instance_loss: 2.7422, weighted_loss: 2.5795, label: 0, bag_size: 7428\n",
      "batch 699, loss: 0.0033, instance_loss: 0.0029, weighted_loss: 0.0032, label: 1, bag_size: 2146\n",
      "batch 719, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 15464\n",
      "batch 739, loss: 0.1216, instance_loss: 0.1098, weighted_loss: 0.1181, label: 0, bag_size: 1789\n",
      "batch 759, loss: 0.6919, instance_loss: 1.0049, weighted_loss: 0.7858, label: 0, bag_size: 2213\n",
      "batch 779, loss: 0.0010, instance_loss: 0.0009, weighted_loss: 0.0010, label: 1, bag_size: 1638\n",
      "batch 799, loss: 0.0302, instance_loss: 0.0309, weighted_loss: 0.0304, label: 0, bag_size: 9851\n",
      "batch 819, loss: 0.0349, instance_loss: 0.0227, weighted_loss: 0.0312, label: 1, bag_size: 10671\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9855182926829268: correct 12930/13120\n",
      "class 1 clustering acc 0.9152439024390244: correct 6004/6560\n",
      "Epoch: 58, train_loss: 0.1298, train_clustering_loss:  0.1527, train_error: 0.0537\n",
      "class 0: acc 0.9502369668246445, correct 401/422\n",
      "class 1: acc 0.9422110552763819, correct 375/398\n",
      "\n",
      "Val Set, val_loss: 0.1980, val_error: 0.0818, auc: 0.9804\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0036, instance_loss: 0.0026, weighted_loss: 0.0033, label: 1, bag_size: 10072\n",
      "batch 39, loss: 0.0018, instance_loss: 0.0019, weighted_loss: 0.0019, label: 1, bag_size: 4821\n",
      "batch 59, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 3228\n",
      "batch 79, loss: 0.0767, instance_loss: 0.0614, weighted_loss: 0.0721, label: 0, bag_size: 21319\n",
      "batch 99, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 14515\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 3640\n",
      "batch 139, loss: 1.3313, instance_loss: 1.6945, weighted_loss: 1.4403, label: 1, bag_size: 1963\n",
      "batch 159, loss: 1.7698, instance_loss: 2.0587, weighted_loss: 1.8565, label: 0, bag_size: 9597\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 13947\n",
      "batch 199, loss: 1.4339, instance_loss: 1.7313, weighted_loss: 1.5232, label: 1, bag_size: 1703\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11387\n",
      "batch 239, loss: 0.4028, instance_loss: 0.5074, weighted_loss: 0.4342, label: 1, bag_size: 2179\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 5605\n",
      "batch 279, loss: 0.0159, instance_loss: 0.0108, weighted_loss: 0.0144, label: 0, bag_size: 12148\n",
      "batch 299, loss: 0.0024, instance_loss: 0.0018, weighted_loss: 0.0022, label: 0, bag_size: 2511\n",
      "batch 319, loss: 0.0013, instance_loss: 0.0180, weighted_loss: 0.0063, label: 1, bag_size: 6090\n",
      "batch 339, loss: 0.0726, instance_loss: 0.0838, weighted_loss: 0.0760, label: 0, bag_size: 3810\n",
      "batch 359, loss: 0.0020, instance_loss: 0.0012, weighted_loss: 0.0017, label: 0, bag_size: 10304\n",
      "batch 379, loss: 0.0097, instance_loss: 0.0079, weighted_loss: 0.0091, label: 0, bag_size: 2036\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 18225\n",
      "batch 419, loss: 0.0015, instance_loss: 0.0002, weighted_loss: 0.0011, label: 1, bag_size: 11518\n",
      "batch 439, loss: 0.0042, instance_loss: 0.0024, weighted_loss: 0.0036, label: 1, bag_size: 10622\n",
      "batch 459, loss: 0.0023, instance_loss: 0.0080, weighted_loss: 0.0040, label: 0, bag_size: 9433\n",
      "batch 479, loss: 0.0144, instance_loss: 0.0167, weighted_loss: 0.0151, label: 1, bag_size: 29832\n",
      "batch 499, loss: 0.3799, instance_loss: 0.4740, weighted_loss: 0.4081, label: 0, bag_size: 5120\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 15008\n",
      "batch 539, loss: 0.1498, instance_loss: 0.1712, weighted_loss: 0.1562, label: 0, bag_size: 2336\n",
      "batch 559, loss: 0.0008, instance_loss: 0.0004, weighted_loss: 0.0007, label: 0, bag_size: 3725\n",
      "batch 579, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 13786\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 6453\n",
      "batch 619, loss: 0.0663, instance_loss: 0.0518, weighted_loss: 0.0619, label: 1, bag_size: 5231\n",
      "batch 639, loss: 1.5681, instance_loss: 1.5772, weighted_loss: 1.5709, label: 1, bag_size: 1963\n",
      "batch 659, loss: 0.0189, instance_loss: 0.0236, weighted_loss: 0.0203, label: 0, bag_size: 10444\n",
      "batch 679, loss: 0.0198, instance_loss: 0.0135, weighted_loss: 0.0179, label: 1, bag_size: 18603\n",
      "batch 699, loss: 0.4158, instance_loss: 0.3567, weighted_loss: 0.3980, label: 1, bag_size: 5605\n",
      "batch 719, loss: 0.0030, instance_loss: 0.0012, weighted_loss: 0.0025, label: 0, bag_size: 8788\n",
      "batch 739, loss: 0.0008, instance_loss: 0.0010, weighted_loss: 0.0008, label: 1, bag_size: 3640\n",
      "batch 759, loss: 0.0513, instance_loss: 0.0521, weighted_loss: 0.0515, label: 1, bag_size: 1920\n",
      "batch 779, loss: 0.0083, instance_loss: 0.0066, weighted_loss: 0.0078, label: 0, bag_size: 15071\n",
      "batch 799, loss: 0.0035, instance_loss: 0.0033, weighted_loss: 0.0035, label: 1, bag_size: 5561\n",
      "batch 819, loss: 0.0148, instance_loss: 0.0086, weighted_loss: 0.0129, label: 0, bag_size: 3783\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9814786585365853: correct 12877/13120\n",
      "class 1 clustering acc 0.9170731707317074: correct 6016/6560\n",
      "Epoch: 59, train_loss: 0.1361, train_clustering_loss:  0.1619, train_error: 0.0500\n",
      "class 0: acc 0.9568345323741008, correct 399/417\n",
      "class 1: acc 0.9429280397022333, correct 380/403\n",
      "\n",
      "Val Set, val_loss: 0.2205, val_error: 0.1000, auc: 0.9745\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8448275862068966, correct 49/58\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0003, instance_loss: 0.0009, weighted_loss: 0.0005, label: 1, bag_size: 6731\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 1, bag_size: 16051\n",
      "batch 59, loss: 0.0082, instance_loss: 0.0063, weighted_loss: 0.0077, label: 0, bag_size: 19466\n",
      "batch 79, loss: 0.0179, instance_loss: 0.0123, weighted_loss: 0.0162, label: 0, bag_size: 1909\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 1622\n",
      "batch 119, loss: 0.3110, instance_loss: 0.4597, weighted_loss: 0.3556, label: 1, bag_size: 6842\n",
      "batch 139, loss: 0.3695, instance_loss: 0.4376, weighted_loss: 0.3900, label: 0, bag_size: 2290\n",
      "batch 159, loss: 0.0875, instance_loss: 0.1747, weighted_loss: 0.1136, label: 1, bag_size: 10105\n",
      "batch 179, loss: 0.4679, instance_loss: 0.5857, weighted_loss: 0.5032, label: 0, bag_size: 7989\n",
      "batch 199, loss: 1.3367, instance_loss: 1.5315, weighted_loss: 1.3951, label: 1, bag_size: 10848\n",
      "batch 219, loss: 0.0070, instance_loss: 0.0031, weighted_loss: 0.0059, label: 1, bag_size: 12626\n",
      "batch 239, loss: 0.0151, instance_loss: 0.0114, weighted_loss: 0.0140, label: 0, bag_size: 8866\n",
      "batch 259, loss: 0.2820, instance_loss: 0.2995, weighted_loss: 0.2873, label: 0, bag_size: 15071\n",
      "batch 279, loss: 0.9989, instance_loss: 1.2880, weighted_loss: 1.0856, label: 1, bag_size: 12719\n",
      "batch 299, loss: 0.4193, instance_loss: 0.5203, weighted_loss: 0.4496, label: 1, bag_size: 13089\n",
      "batch 319, loss: 0.0006, instance_loss: 0.0001, weighted_loss: 0.0005, label: 0, bag_size: 15841\n",
      "batch 339, loss: 0.0059, instance_loss: 0.0075, weighted_loss: 0.0064, label: 0, bag_size: 1745\n",
      "batch 359, loss: 0.0021, instance_loss: 0.0018, weighted_loss: 0.0020, label: 0, bag_size: 9171\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0038, weighted_loss: 0.0011, label: 1, bag_size: 11701\n",
      "batch 399, loss: 0.0028, instance_loss: 0.0023, weighted_loss: 0.0026, label: 0, bag_size: 7605\n",
      "batch 419, loss: 0.0125, instance_loss: 0.0247, weighted_loss: 0.0162, label: 1, bag_size: 1339\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 1884\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0028, weighted_loss: 0.0008, label: 0, bag_size: 5965\n",
      "batch 479, loss: 0.0015, instance_loss: 0.0015, weighted_loss: 0.0015, label: 1, bag_size: 14230\n",
      "batch 499, loss: 0.0030, instance_loss: 0.0058, weighted_loss: 0.0038, label: 1, bag_size: 3968\n",
      "batch 519, loss: 0.0665, instance_loss: 0.0737, weighted_loss: 0.0687, label: 1, bag_size: 16890\n",
      "batch 539, loss: 0.1137, instance_loss: 0.1236, weighted_loss: 0.1167, label: 1, bag_size: 8040\n",
      "batch 559, loss: 0.0387, instance_loss: 0.0504, weighted_loss: 0.0422, label: 0, bag_size: 2918\n",
      "batch 579, loss: 0.2600, instance_loss: 0.2564, weighted_loss: 0.2590, label: 0, bag_size: 7989\n",
      "batch 599, loss: 0.0974, instance_loss: 0.0755, weighted_loss: 0.0908, label: 0, bag_size: 1789\n",
      "batch 619, loss: 0.0160, instance_loss: 0.0202, weighted_loss: 0.0173, label: 0, bag_size: 3897\n",
      "batch 639, loss: 0.0007, instance_loss: 0.0005, weighted_loss: 0.0006, label: 1, bag_size: 8438\n",
      "batch 659, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 9542\n",
      "batch 679, loss: 0.0045, instance_loss: 0.0035, weighted_loss: 0.0042, label: 0, bag_size: 10751\n",
      "batch 699, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 3541\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0057, weighted_loss: 0.0017, label: 1, bag_size: 4877\n",
      "batch 739, loss: 0.0008, instance_loss: 0.0003, weighted_loss: 0.0006, label: 1, bag_size: 6745\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 1, bag_size: 3437\n",
      "batch 779, loss: 0.0054, instance_loss: 0.0042, weighted_loss: 0.0051, label: 0, bag_size: 3444\n",
      "batch 799, loss: 0.0381, instance_loss: 0.0378, weighted_loss: 0.0381, label: 1, bag_size: 7989\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9971\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9827743902439025: correct 12894/13120\n",
      "class 1 clustering acc 0.9094512195121951: correct 5966/6560\n",
      "Epoch: 60, train_loss: 0.1435, train_clustering_loss:  0.1679, train_error: 0.0598\n",
      "class 0: acc 0.946078431372549, correct 386/408\n",
      "class 1: acc 0.9344660194174758, correct 385/412\n",
      "\n",
      "Val Set, val_loss: 0.3350, val_error: 0.1636, auc: 0.9784\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.6923076923076923, correct 36/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 4271\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 13786\n",
      "batch 59, loss: 0.0108, instance_loss: 0.0077, weighted_loss: 0.0099, label: 0, bag_size: 4523\n",
      "batch 79, loss: 0.0005, instance_loss: 0.0006, weighted_loss: 0.0005, label: 1, bag_size: 4956\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 22828\n",
      "batch 119, loss: 0.0226, instance_loss: 0.0445, weighted_loss: 0.0292, label: 1, bag_size: 1051\n",
      "batch 139, loss: 0.0086, instance_loss: 0.0144, weighted_loss: 0.0103, label: 0, bag_size: 2006\n",
      "batch 159, loss: 0.0130, instance_loss: 0.1819, weighted_loss: 0.0636, label: 1, bag_size: 3368\n",
      "batch 179, loss: 4.2889, instance_loss: 4.1367, weighted_loss: 4.2432, label: 1, bag_size: 15563\n",
      "batch 199, loss: 0.2082, instance_loss: 0.2681, weighted_loss: 0.2262, label: 0, bag_size: 17630\n",
      "batch 219, loss: 0.0214, instance_loss: 0.0069, weighted_loss: 0.0171, label: 1, bag_size: 6731\n",
      "batch 239, loss: 0.7964, instance_loss: 0.6636, weighted_loss: 0.7566, label: 0, bag_size: 10113\n",
      "batch 259, loss: 0.0083, instance_loss: 0.0060, weighted_loss: 0.0076, label: 0, bag_size: 5409\n",
      "batch 279, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 8003\n",
      "batch 299, loss: 0.0098, instance_loss: 0.0167, weighted_loss: 0.0118, label: 1, bag_size: 1459\n",
      "batch 319, loss: 0.2372, instance_loss: 0.3426, weighted_loss: 0.2688, label: 1, bag_size: 1746\n",
      "batch 339, loss: 0.5813, instance_loss: 0.7742, weighted_loss: 0.6392, label: 1, bag_size: 21450\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9885\n",
      "batch 379, loss: 0.0054, instance_loss: 0.0021, weighted_loss: 0.0044, label: 1, bag_size: 2935\n",
      "batch 399, loss: 0.0007, instance_loss: 0.0227, weighted_loss: 0.0073, label: 0, bag_size: 18777\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 5629\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 8602\n",
      "batch 459, loss: 1.4449, instance_loss: 2.1082, weighted_loss: 1.6439, label: 1, bag_size: 7389\n",
      "batch 479, loss: 0.0849, instance_loss: 0.0790, weighted_loss: 0.0831, label: 0, bag_size: 22426\n",
      "batch 499, loss: 0.0253, instance_loss: 0.0343, weighted_loss: 0.0280, label: 1, bag_size: 9561\n",
      "batch 519, loss: 0.2986, instance_loss: 0.5889, weighted_loss: 0.3857, label: 1, bag_size: 2395\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0008, weighted_loss: 0.0003, label: 0, bag_size: 27158\n",
      "batch 559, loss: 0.0011, instance_loss: 0.0006, weighted_loss: 0.0010, label: 0, bag_size: 19518\n",
      "batch 579, loss: 0.1880, instance_loss: 0.1619, weighted_loss: 0.1802, label: 0, bag_size: 3089\n",
      "batch 599, loss: 0.6897, instance_loss: 0.8083, weighted_loss: 0.7253, label: 1, bag_size: 1845\n",
      "batch 619, loss: 0.0057, instance_loss: 0.0082, weighted_loss: 0.0065, label: 0, bag_size: 11199\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0020, weighted_loss: 0.0006, label: 0, bag_size: 4271\n",
      "batch 659, loss: 0.0306, instance_loss: 0.0359, weighted_loss: 0.0322, label: 1, bag_size: 2137\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0017, weighted_loss: 0.0005, label: 1, bag_size: 6343\n",
      "batch 699, loss: 0.0214, instance_loss: 0.0152, weighted_loss: 0.0195, label: 0, bag_size: 705\n",
      "batch 719, loss: 0.2580, instance_loss: 0.3044, weighted_loss: 0.2719, label: 0, bag_size: 3670\n",
      "batch 739, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 24911\n",
      "batch 759, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 5999\n",
      "batch 779, loss: 0.0025, instance_loss: 0.0014, weighted_loss: 0.0022, label: 0, bag_size: 4598\n",
      "batch 799, loss: 5.2481, instance_loss: 4.4957, weighted_loss: 5.0224, label: 0, bag_size: 6281\n",
      "batch 819, loss: 0.0006, instance_loss: 0.0106, weighted_loss: 0.0036, label: 0, bag_size: 19808\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9786585365853658: correct 12840/13120\n",
      "class 1 clustering acc 0.9065548780487804: correct 5947/6560\n",
      "Epoch: 61, train_loss: 0.1786, train_clustering_loss:  0.2041, train_error: 0.0634\n",
      "class 0: acc 0.9387755102040817, correct 414/441\n",
      "class 1: acc 0.9340369393139841, correct 354/379\n",
      "\n",
      "Val Set, val_loss: 0.2965, val_error: 0.1364, auc: 0.9761\n",
      "class 0 clustering acc 0.9818181818181818: correct 1728/1760\n",
      "class 1 clustering acc 0.17613636363636365: correct 155/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.7758620689655172, correct 45/58\n",
      "EarlyStopping counter: 12 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0069, instance_loss: 0.0047, weighted_loss: 0.0063, label: 1, bag_size: 7798\n",
      "batch 39, loss: 0.0111, instance_loss: 0.0064, weighted_loss: 0.0097, label: 1, bag_size: 1512\n",
      "batch 59, loss: 1.2629, instance_loss: 1.6108, weighted_loss: 1.3672, label: 1, bag_size: 898\n",
      "batch 79, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 4877\n",
      "batch 99, loss: 0.0076, instance_loss: 0.0032, weighted_loss: 0.0062, label: 0, bag_size: 11654\n",
      "batch 119, loss: 0.0628, instance_loss: 0.0632, weighted_loss: 0.0629, label: 1, bag_size: 5155\n",
      "batch 139, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0014, label: 1, bag_size: 1622\n",
      "batch 159, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 6652\n",
      "batch 179, loss: 0.0087, instance_loss: 0.0069, weighted_loss: 0.0082, label: 0, bag_size: 1639\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 14319\n",
      "batch 219, loss: 0.0207, instance_loss: 0.0195, weighted_loss: 0.0203, label: 0, bag_size: 5009\n",
      "batch 239, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 6606\n",
      "batch 259, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 10482\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 8003\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 19932\n",
      "batch 319, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 3459\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11701\n",
      "batch 359, loss: 0.0180, instance_loss: 0.0089, weighted_loss: 0.0152, label: 0, bag_size: 12910\n",
      "batch 379, loss: 0.0037, instance_loss: 0.0009, weighted_loss: 0.0028, label: 1, bag_size: 1064\n",
      "batch 399, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 12687\n",
      "batch 419, loss: 0.2797, instance_loss: 0.2983, weighted_loss: 0.2853, label: 0, bag_size: 1760\n",
      "batch 439, loss: 0.2639, instance_loss: 0.2960, weighted_loss: 0.2735, label: 1, bag_size: 12719\n",
      "batch 459, loss: 0.0004, instance_loss: 0.0001, weighted_loss: 0.0003, label: 0, bag_size: 11654\n",
      "batch 479, loss: 0.0439, instance_loss: 0.0223, weighted_loss: 0.0374, label: 1, bag_size: 1339\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11865\n",
      "batch 519, loss: 0.3149, instance_loss: 0.1479, weighted_loss: 0.2648, label: 1, bag_size: 2638\n",
      "batch 539, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 19039\n",
      "batch 559, loss: 1.9603, instance_loss: 2.4108, weighted_loss: 2.0954, label: 0, bag_size: 2290\n",
      "batch 579, loss: 0.0042, instance_loss: 0.0007, weighted_loss: 0.0032, label: 0, bag_size: 6281\n",
      "batch 599, loss: 1.1189, instance_loss: 1.3390, weighted_loss: 1.1849, label: 0, bag_size: 13332\n",
      "batch 619, loss: 0.0493, instance_loss: 0.0390, weighted_loss: 0.0462, label: 0, bag_size: 14828\n",
      "batch 639, loss: 0.0034, instance_loss: 0.0012, weighted_loss: 0.0027, label: 0, bag_size: 13225\n",
      "batch 659, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 9689\n",
      "batch 679, loss: 0.0156, instance_loss: 0.0107, weighted_loss: 0.0141, label: 0, bag_size: 8755\n",
      "batch 699, loss: 0.0593, instance_loss: 0.0510, weighted_loss: 0.0568, label: 0, bag_size: 803\n",
      "batch 719, loss: 0.0451, instance_loss: 0.0469, weighted_loss: 0.0457, label: 0, bag_size: 1483\n",
      "batch 739, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 12425\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 19472\n",
      "batch 779, loss: 0.0151, instance_loss: 0.0173, weighted_loss: 0.0158, label: 1, bag_size: 9470\n",
      "batch 799, loss: 0.0004, instance_loss: 0.0001, weighted_loss: 0.0003, label: 0, bag_size: 13225\n",
      "batch 819, loss: 0.0038, instance_loss: 0.0023, weighted_loss: 0.0033, label: 0, bag_size: 2044\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9842987804878048: correct 12914/13120\n",
      "class 1 clustering acc 0.9178353658536585: correct 6021/6560\n",
      "Epoch: 62, train_loss: 0.1455, train_clustering_loss:  0.1650, train_error: 0.0537\n",
      "class 0: acc 0.945, correct 378/400\n",
      "class 1: acc 0.9476190476190476, correct 398/420\n",
      "\n",
      "Val Set, val_loss: 0.1934, val_error: 0.0818, auc: 0.9781\n",
      "class 0 clustering acc 0.9926136363636363: correct 1747/1760\n",
      "class 1 clustering acc 0.029545454545454545: correct 26/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.896551724137931, correct 52/58\n",
      "EarlyStopping counter: 13 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0607, instance_loss: 0.0678, weighted_loss: 0.0629, label: 1, bag_size: 16890\n",
      "batch 39, loss: 0.0002, instance_loss: 0.0038, weighted_loss: 0.0013, label: 1, bag_size: 9062\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19472\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 5965\n",
      "batch 99, loss: 0.0025, instance_loss: 0.0001, weighted_loss: 0.0018, label: 1, bag_size: 3968\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 5494\n",
      "batch 139, loss: 0.2708, instance_loss: 0.3517, weighted_loss: 0.2951, label: 1, bag_size: 8012\n",
      "batch 159, loss: 0.5170, instance_loss: 0.6739, weighted_loss: 0.5641, label: 0, bag_size: 1953\n",
      "batch 179, loss: 0.0586, instance_loss: 0.0352, weighted_loss: 0.0516, label: 0, bag_size: 1831\n",
      "batch 199, loss: 0.0336, instance_loss: 0.0323, weighted_loss: 0.0332, label: 1, bag_size: 10460\n",
      "batch 219, loss: 0.7605, instance_loss: 1.1024, weighted_loss: 0.8631, label: 0, bag_size: 1637\n",
      "batch 239, loss: 0.0475, instance_loss: 0.0119, weighted_loss: 0.0368, label: 1, bag_size: 699\n",
      "batch 259, loss: 0.0092, instance_loss: 0.0028, weighted_loss: 0.0073, label: 1, bag_size: 9689\n",
      "batch 279, loss: 0.0094, instance_loss: 0.0099, weighted_loss: 0.0095, label: 1, bag_size: 10394\n",
      "batch 299, loss: 0.0066, instance_loss: 0.0039, weighted_loss: 0.0058, label: 0, bag_size: 21093\n",
      "batch 319, loss: 0.0023, instance_loss: 0.0101, weighted_loss: 0.0046, label: 1, bag_size: 9610\n",
      "batch 339, loss: 0.0086, instance_loss: 0.0079, weighted_loss: 0.0084, label: 0, bag_size: 3908\n",
      "batch 359, loss: 0.0239, instance_loss: 0.0293, weighted_loss: 0.0255, label: 1, bag_size: 10671\n",
      "batch 379, loss: 0.0002, instance_loss: 0.0001, weighted_loss: 0.0002, label: 0, bag_size: 2457\n",
      "batch 399, loss: 0.0000, instance_loss: 0.6896, weighted_loss: 0.2069, label: 1, bag_size: 1781\n",
      "batch 419, loss: 0.0572, instance_loss: 0.0746, weighted_loss: 0.0624, label: 1, bag_size: 10105\n",
      "batch 439, loss: 0.0092, instance_loss: 0.0053, weighted_loss: 0.0080, label: 0, bag_size: 1962\n",
      "batch 459, loss: 1.5105, instance_loss: 1.8839, weighted_loss: 1.6225, label: 1, bag_size: 1533\n",
      "batch 479, loss: 0.0041, instance_loss: 0.0023, weighted_loss: 0.0035, label: 1, bag_size: 29832\n",
      "batch 499, loss: 0.0004, instance_loss: 0.0011, weighted_loss: 0.0006, label: 1, bag_size: 1255\n",
      "batch 519, loss: 0.0057, instance_loss: 0.0031, weighted_loss: 0.0049, label: 1, bag_size: 5256\n",
      "batch 539, loss: 0.0019, instance_loss: 0.0004, weighted_loss: 0.0014, label: 0, bag_size: 2732\n",
      "batch 559, loss: 0.0020, instance_loss: 0.0007, weighted_loss: 0.0016, label: 0, bag_size: 9949\n",
      "batch 579, loss: 0.0182, instance_loss: 0.0147, weighted_loss: 0.0172, label: 1, bag_size: 20333\n",
      "batch 599, loss: 0.2127, instance_loss: 0.3077, weighted_loss: 0.2412, label: 1, bag_size: 2455\n",
      "batch 619, loss: 0.0004, instance_loss: 0.0004, weighted_loss: 0.0004, label: 1, bag_size: 4128\n",
      "batch 639, loss: 0.0241, instance_loss: 0.0224, weighted_loss: 0.0236, label: 0, bag_size: 13880\n",
      "batch 659, loss: 0.0490, instance_loss: 0.0423, weighted_loss: 0.0470, label: 0, bag_size: 19067\n",
      "batch 679, loss: 0.0021, instance_loss: 0.0012, weighted_loss: 0.0018, label: 0, bag_size: 21682\n",
      "batch 699, loss: 0.0046, instance_loss: 0.0033, weighted_loss: 0.0042, label: 0, bag_size: 9415\n",
      "batch 719, loss: 0.0258, instance_loss: 0.0266, weighted_loss: 0.0261, label: 0, bag_size: 19518\n",
      "batch 739, loss: 0.1982, instance_loss: 0.2755, weighted_loss: 0.2214, label: 1, bag_size: 8103\n",
      "batch 759, loss: 0.0340, instance_loss: 0.0345, weighted_loss: 0.0342, label: 1, bag_size: 4239\n",
      "batch 779, loss: 0.0329, instance_loss: 0.0385, weighted_loss: 0.0346, label: 1, bag_size: 6736\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 5221\n",
      "batch 819, loss: 0.0182, instance_loss: 0.0165, weighted_loss: 0.0177, label: 1, bag_size: 5454\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9801829268292683: correct 12860/13120\n",
      "class 1 clustering acc 0.9015243902439024: correct 5914/6560\n",
      "Epoch: 63, train_loss: 0.1568, train_clustering_loss:  0.1824, train_error: 0.0646\n",
      "class 0: acc 0.9408983451536643, correct 398/423\n",
      "class 1: acc 0.929471032745592, correct 369/397\n",
      "\n",
      "Val Set, val_loss: 0.2048, val_error: 0.1000, auc: 0.9794\n",
      "class 0 clustering acc 0.9852272727272727: correct 1734/1760\n",
      "class 1 clustering acc 0.125: correct 110/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8448275862068966, correct 49/58\n",
      "EarlyStopping counter: 14 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11195\n",
      "batch 39, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 16052\n",
      "batch 59, loss: 0.0023, instance_loss: 0.0021, weighted_loss: 0.0022, label: 0, bag_size: 9415\n",
      "batch 79, loss: 0.0275, instance_loss: 0.0200, weighted_loss: 0.0252, label: 1, bag_size: 1746\n",
      "batch 99, loss: 0.0381, instance_loss: 0.0329, weighted_loss: 0.0365, label: 0, bag_size: 24439\n",
      "batch 119, loss: 0.0066, instance_loss: 0.0055, weighted_loss: 0.0063, label: 0, bag_size: 11900\n",
      "batch 139, loss: 0.5974, instance_loss: 0.8046, weighted_loss: 0.6595, label: 1, bag_size: 13440\n",
      "batch 159, loss: 0.0087, instance_loss: 0.0091, weighted_loss: 0.0088, label: 0, bag_size: 2351\n",
      "batch 179, loss: 0.0937, instance_loss: 0.0658, weighted_loss: 0.0853, label: 1, bag_size: 11220\n",
      "batch 199, loss: 0.7407, instance_loss: 1.0235, weighted_loss: 0.8256, label: 0, bag_size: 5009\n",
      "batch 219, loss: 0.0287, instance_loss: 0.0156, weighted_loss: 0.0248, label: 0, bag_size: 14681\n",
      "batch 239, loss: 0.0273, instance_loss: 0.0246, weighted_loss: 0.0265, label: 1, bag_size: 9065\n",
      "batch 259, loss: 0.0090, instance_loss: 0.0041, weighted_loss: 0.0075, label: 1, bag_size: 2695\n",
      "batch 279, loss: 0.4018, instance_loss: 0.4018, weighted_loss: 0.4018, label: 0, bag_size: 2219\n",
      "batch 299, loss: 0.2764, instance_loss: 0.2880, weighted_loss: 0.2799, label: 0, bag_size: 20555\n",
      "batch 319, loss: 0.0046, instance_loss: 0.0017, weighted_loss: 0.0037, label: 0, bag_size: 1588\n",
      "batch 339, loss: 0.0043, instance_loss: 0.0012, weighted_loss: 0.0033, label: 1, bag_size: 6343\n",
      "batch 359, loss: 0.0075, instance_loss: 0.0045, weighted_loss: 0.0066, label: 0, bag_size: 1560\n",
      "batch 379, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 2457\n",
      "batch 399, loss: 0.2292, instance_loss: 0.2863, weighted_loss: 0.2463, label: 1, bag_size: 16548\n",
      "batch 419, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 11477\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 17633\n",
      "batch 459, loss: 0.0009, instance_loss: 0.0010, weighted_loss: 0.0010, label: 0, bag_size: 10535\n",
      "batch 479, loss: 0.0046, instance_loss: 0.0026, weighted_loss: 0.0040, label: 0, bag_size: 18777\n",
      "batch 499, loss: 0.0182, instance_loss: 0.0120, weighted_loss: 0.0163, label: 0, bag_size: 1614\n",
      "batch 519, loss: 0.0543, instance_loss: 0.0456, weighted_loss: 0.0517, label: 0, bag_size: 10942\n",
      "batch 539, loss: 0.0752, instance_loss: 0.0799, weighted_loss: 0.0766, label: 0, bag_size: 7612\n",
      "batch 559, loss: 0.0008, instance_loss: 0.0003, weighted_loss: 0.0006, label: 1, bag_size: 8003\n",
      "batch 579, loss: 0.0003, instance_loss: 0.0003, weighted_loss: 0.0003, label: 1, bag_size: 1919\n",
      "batch 599, loss: 0.0255, instance_loss: 0.0172, weighted_loss: 0.0230, label: 0, bag_size: 7989\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0111, weighted_loss: 0.0033, label: 1, bag_size: 9955\n",
      "batch 639, loss: 0.0006, instance_loss: 0.0001, weighted_loss: 0.0004, label: 1, bag_size: 2278\n",
      "batch 659, loss: 3.6951, instance_loss: 4.0968, weighted_loss: 3.8156, label: 0, bag_size: 2694\n",
      "batch 679, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 14206\n",
      "batch 699, loss: 1.1883, instance_loss: 1.4261, weighted_loss: 1.2596, label: 0, bag_size: 7428\n",
      "batch 719, loss: 2.3579, instance_loss: 3.0788, weighted_loss: 2.5742, label: 1, bag_size: 3879\n",
      "batch 739, loss: 0.0098, instance_loss: 0.0066, weighted_loss: 0.0089, label: 0, bag_size: 763\n",
      "batch 759, loss: 0.0304, instance_loss: 0.0164, weighted_loss: 0.0262, label: 1, bag_size: 9519\n",
      "batch 779, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 9644\n",
      "batch 799, loss: 0.2541, instance_loss: 0.3144, weighted_loss: 0.2722, label: 0, bag_size: 1052\n",
      "batch 819, loss: 0.0046, instance_loss: 0.0018, weighted_loss: 0.0037, label: 1, bag_size: 928\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9828506097560976: correct 12895/13120\n",
      "class 1 clustering acc 0.916920731707317: correct 6015/6560\n",
      "Epoch: 64, train_loss: 0.1503, train_clustering_loss:  0.1704, train_error: 0.0549\n",
      "class 0: acc 0.9477434679334917, correct 399/421\n",
      "class 1: acc 0.9423558897243107, correct 376/399\n",
      "\n",
      "Val Set, val_loss: 0.2405, val_error: 0.1091, auc: 0.9791\n",
      "class 0 clustering acc 0.9772727272727273: correct 1720/1760\n",
      "class 1 clustering acc 0.09318181818181819: correct 82/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8275862068965517, correct 48/58\n",
      "EarlyStopping counter: 15 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5198, instance_loss: 0.5833, weighted_loss: 0.5388, label: 0, bag_size: 2098\n",
      "batch 39, loss: 0.0019, instance_loss: 0.0016, weighted_loss: 0.0018, label: 0, bag_size: 22870\n",
      "batch 59, loss: 0.0004, instance_loss: 0.0010, weighted_loss: 0.0006, label: 1, bag_size: 3453\n",
      "batch 79, loss: 0.0018, instance_loss: 0.0015, weighted_loss: 0.0017, label: 1, bag_size: 5025\n",
      "batch 99, loss: 0.0052, instance_loss: 0.0040, weighted_loss: 0.0048, label: 0, bag_size: 13777\n",
      "batch 119, loss: 0.0040, instance_loss: 0.0016, weighted_loss: 0.0033, label: 0, bag_size: 14249\n",
      "batch 139, loss: 0.0135, instance_loss: 0.0085, weighted_loss: 0.0120, label: 0, bag_size: 1370\n",
      "batch 159, loss: 0.0014, instance_loss: 0.0005, weighted_loss: 0.0011, label: 1, bag_size: 16162\n",
      "batch 179, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 3787\n",
      "batch 199, loss: 0.0100, instance_loss: 0.0101, weighted_loss: 0.0100, label: 1, bag_size: 12425\n",
      "batch 219, loss: 0.0006, instance_loss: 0.0002, weighted_loss: 0.0005, label: 0, bag_size: 7191\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 4465\n",
      "batch 259, loss: 0.0155, instance_loss: 0.0106, weighted_loss: 0.0140, label: 0, bag_size: 21319\n",
      "batch 279, loss: 0.0041, instance_loss: 0.0024, weighted_loss: 0.0036, label: 1, bag_size: 11421\n",
      "batch 299, loss: 0.0851, instance_loss: 0.0579, weighted_loss: 0.0770, label: 0, bag_size: 2360\n",
      "batch 319, loss: 0.0186, instance_loss: 0.0201, weighted_loss: 0.0190, label: 0, bag_size: 9387\n",
      "batch 339, loss: 0.0050, instance_loss: 0.0065, weighted_loss: 0.0055, label: 1, bag_size: 20537\n",
      "batch 359, loss: 2.9038, instance_loss: 3.2367, weighted_loss: 3.0036, label: 1, bag_size: 15185\n",
      "batch 379, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 25970\n",
      "batch 399, loss: 0.0010, instance_loss: 0.0006, weighted_loss: 0.0009, label: 0, bag_size: 8661\n",
      "batch 419, loss: 0.0237, instance_loss: 0.0227, weighted_loss: 0.0234, label: 1, bag_size: 8040\n",
      "batch 439, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 928\n",
      "batch 459, loss: 0.1642, instance_loss: 0.1532, weighted_loss: 0.1609, label: 0, bag_size: 6624\n",
      "batch 479, loss: 0.0005, instance_loss: 0.0002, weighted_loss: 0.0004, label: 0, bag_size: 890\n",
      "batch 499, loss: 0.2734, instance_loss: 0.3185, weighted_loss: 0.2869, label: 0, bag_size: 8420\n",
      "batch 519, loss: 0.1244, instance_loss: 0.1028, weighted_loss: 0.1179, label: 0, bag_size: 1814\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 6205\n",
      "batch 559, loss: 0.0223, instance_loss: 0.0225, weighted_loss: 0.0224, label: 1, bag_size: 3651\n",
      "batch 579, loss: 0.0155, instance_loss: 0.0099, weighted_loss: 0.0138, label: 0, bag_size: 12148\n",
      "batch 599, loss: 0.1901, instance_loss: 0.2245, weighted_loss: 0.2004, label: 0, bag_size: 2959\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11778\n",
      "batch 639, loss: 0.0029, instance_loss: 0.0022, weighted_loss: 0.0027, label: 0, bag_size: 19808\n",
      "batch 659, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 20537\n",
      "batch 679, loss: 0.1080, instance_loss: 0.0833, weighted_loss: 0.1006, label: 0, bag_size: 1508\n",
      "batch 699, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 14202\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 4880\n",
      "batch 739, loss: 0.0003, instance_loss: 0.0001, weighted_loss: 0.0002, label: 1, bag_size: 9004\n",
      "batch 759, loss: 0.0029, instance_loss: 0.0016, weighted_loss: 0.0025, label: 1, bag_size: 12697\n",
      "batch 779, loss: 0.0029, instance_loss: 0.0045, weighted_loss: 0.0034, label: 1, bag_size: 689\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8948\n",
      "batch 819, loss: 0.1321, instance_loss: 0.1117, weighted_loss: 0.1260, label: 0, bag_size: 5211\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.984375: correct 12915/13120\n",
      "class 1 clustering acc 0.9155487804878049: correct 6006/6560\n",
      "Epoch: 65, train_loss: 0.1242, train_clustering_loss:  0.1508, train_error: 0.0500\n",
      "class 0: acc 0.9472361809045227, correct 377/398\n",
      "class 1: acc 0.95260663507109, correct 402/422\n",
      "\n",
      "Val Set, val_loss: 0.2323, val_error: 0.1091, auc: 0.9798\n",
      "class 0 clustering acc 0.9994318181818181: correct 1759/1760\n",
      "class 1 clustering acc 0.025: correct 22/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8275862068965517, correct 48/58\n",
      "EarlyStopping counter: 16 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.1814, instance_loss: 1.5018, weighted_loss: 1.2775, label: 0, bag_size: 2653\n",
      "batch 39, loss: 0.0914, instance_loss: 0.1841, weighted_loss: 0.1192, label: 1, bag_size: 2904\n",
      "batch 59, loss: 0.0007, instance_loss: 0.0003, weighted_loss: 0.0006, label: 1, bag_size: 6317\n",
      "batch 79, loss: 0.0189, instance_loss: 0.0100, weighted_loss: 0.0162, label: 1, bag_size: 12340\n",
      "batch 99, loss: 1.7925, instance_loss: 1.4234, weighted_loss: 1.6817, label: 1, bag_size: 2140\n",
      "batch 119, loss: 0.0358, instance_loss: 0.0141, weighted_loss: 0.0293, label: 0, bag_size: 10304\n",
      "batch 139, loss: 0.0454, instance_loss: 0.0546, weighted_loss: 0.0482, label: 0, bag_size: 4523\n",
      "batch 159, loss: 0.0815, instance_loss: 0.0925, weighted_loss: 0.0848, label: 0, bag_size: 14264\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 8866\n",
      "batch 199, loss: 0.0059, instance_loss: 0.0050, weighted_loss: 0.0057, label: 0, bag_size: 14249\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 1789\n",
      "batch 239, loss: 0.4952, instance_loss: 0.7097, weighted_loss: 0.5596, label: 0, bag_size: 2219\n",
      "batch 259, loss: 0.0047, instance_loss: 0.0024, weighted_loss: 0.0040, label: 0, bag_size: 3774\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 10128\n",
      "batch 299, loss: 0.0216, instance_loss: 0.0152, weighted_loss: 0.0197, label: 0, bag_size: 12840\n",
      "batch 319, loss: 0.0062, instance_loss: 0.0037, weighted_loss: 0.0055, label: 0, bag_size: 3198\n",
      "batch 339, loss: 0.0211, instance_loss: 0.0168, weighted_loss: 0.0198, label: 1, bag_size: 12425\n",
      "batch 359, loss: 0.0716, instance_loss: 0.0491, weighted_loss: 0.0648, label: 0, bag_size: 2873\n",
      "batch 379, loss: 1.7768, instance_loss: 2.0091, weighted_loss: 1.8465, label: 1, bag_size: 6360\n",
      "batch 399, loss: 2.8700, instance_loss: 3.4832, weighted_loss: 3.0539, label: 0, bag_size: 8744\n",
      "batch 419, loss: 0.0015, instance_loss: 0.0001, weighted_loss: 0.0011, label: 0, bag_size: 26271\n",
      "batch 439, loss: 0.0330, instance_loss: 0.0334, weighted_loss: 0.0331, label: 1, bag_size: 7768\n",
      "batch 459, loss: 0.0017, instance_loss: 0.0012, weighted_loss: 0.0015, label: 0, bag_size: 14206\n",
      "batch 479, loss: 0.0025, instance_loss: 0.0020, weighted_loss: 0.0023, label: 1, bag_size: 4308\n",
      "batch 499, loss: 0.0005, instance_loss: 0.0004, weighted_loss: 0.0005, label: 0, bag_size: 1202\n",
      "batch 519, loss: 0.5679, instance_loss: 0.7601, weighted_loss: 0.6256, label: 1, bag_size: 9942\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 2511\n",
      "batch 559, loss: 0.0071, instance_loss: 0.0048, weighted_loss: 0.0064, label: 1, bag_size: 20537\n",
      "batch 579, loss: 0.0150, instance_loss: 0.0122, weighted_loss: 0.0142, label: 1, bag_size: 5155\n",
      "batch 599, loss: 0.0000, instance_loss: 0.3125, weighted_loss: 0.0938, label: 1, bag_size: 13174\n",
      "batch 619, loss: 0.0079, instance_loss: 0.0052, weighted_loss: 0.0071, label: 1, bag_size: 689\n",
      "batch 639, loss: 0.8209, instance_loss: 1.0703, weighted_loss: 0.8957, label: 0, bag_size: 1637\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8812\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 3453\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 12524\n",
      "batch 719, loss: 0.0023, instance_loss: 0.0007, weighted_loss: 0.0018, label: 1, bag_size: 22286\n",
      "batch 739, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 5317\n",
      "batch 759, loss: 0.1749, instance_loss: 0.1911, weighted_loss: 0.1797, label: 1, bag_size: 1609\n",
      "batch 779, loss: 0.1676, instance_loss: 0.1682, weighted_loss: 0.1678, label: 1, bag_size: 11729\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 11113\n",
      "batch 819, loss: 0.1445, instance_loss: 0.1610, weighted_loss: 0.1495, label: 1, bag_size: 2842\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9814024390243903: correct 12876/13120\n",
      "class 1 clustering acc 0.9059451219512196: correct 5943/6560\n",
      "Epoch: 66, train_loss: 0.1526, train_clustering_loss:  0.1808, train_error: 0.0610\n",
      "class 0: acc 0.945823927765237, correct 419/443\n",
      "class 1: acc 0.9310344827586207, correct 351/377\n",
      "\n",
      "Val Set, val_loss: 0.1802, val_error: 0.0727, auc: 0.9808\n",
      "class 0 clustering acc 0.9954545454545455: correct 1752/1760\n",
      "class 1 clustering acc 0.013636363636363636: correct 12/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.896551724137931, correct 52/58\n",
      "EarlyStopping counter: 17 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 4.0986, instance_loss: 4.3510, weighted_loss: 4.1743, label: 0, bag_size: 3897\n",
      "batch 39, loss: 0.0048, instance_loss: 0.0012, weighted_loss: 0.0037, label: 0, bag_size: 4497\n",
      "batch 59, loss: 0.1489, instance_loss: 0.1348, weighted_loss: 0.1447, label: 0, bag_size: 15071\n",
      "batch 79, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 1, bag_size: 16565\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0052, weighted_loss: 0.0016, label: 1, bag_size: 10392\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 7011\n",
      "batch 139, loss: 0.0106, instance_loss: 0.0223, weighted_loss: 0.0141, label: 1, bag_size: 3368\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 518\n",
      "batch 179, loss: 0.0045, instance_loss: 0.0057, weighted_loss: 0.0048, label: 1, bag_size: 2344\n",
      "batch 199, loss: 0.0003, instance_loss: 0.0002, weighted_loss: 0.0002, label: 0, bag_size: 9234\n",
      "batch 219, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 7011\n",
      "batch 239, loss: 0.0353, instance_loss: 0.0220, weighted_loss: 0.0313, label: 0, bag_size: 15672\n",
      "batch 259, loss: 0.3436, instance_loss: 0.5502, weighted_loss: 0.4056, label: 0, bag_size: 2624\n",
      "batch 279, loss: 0.0046, instance_loss: 0.0011, weighted_loss: 0.0036, label: 0, bag_size: 2424\n",
      "batch 299, loss: 0.0075, instance_loss: 0.0010, weighted_loss: 0.0056, label: 1, bag_size: 3651\n",
      "batch 319, loss: 0.0100, instance_loss: 0.0054, weighted_loss: 0.0086, label: 1, bag_size: 21827\n",
      "batch 339, loss: 0.2628, instance_loss: 0.3238, weighted_loss: 0.2811, label: 1, bag_size: 5340\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 18225\n",
      "batch 379, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 5221\n",
      "batch 399, loss: 0.0156, instance_loss: 0.0137, weighted_loss: 0.0150, label: 0, bag_size: 3101\n",
      "batch 419, loss: 0.2496, instance_loss: 0.3034, weighted_loss: 0.2658, label: 0, bag_size: 12510\n",
      "batch 439, loss: 0.5177, instance_loss: 0.6852, weighted_loss: 0.5680, label: 1, bag_size: 21252\n",
      "batch 459, loss: 0.6151, instance_loss: 0.7148, weighted_loss: 0.6451, label: 0, bag_size: 1592\n",
      "batch 479, loss: 0.0010, instance_loss: 0.0012, weighted_loss: 0.0011, label: 0, bag_size: 8025\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 8410\n",
      "batch 519, loss: 0.0493, instance_loss: 0.0358, weighted_loss: 0.0452, label: 0, bag_size: 10415\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 10146\n",
      "batch 559, loss: 0.0649, instance_loss: 0.0510, weighted_loss: 0.0607, label: 1, bag_size: 2842\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 9234\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0011, weighted_loss: 0.0004, label: 1, bag_size: 15332\n",
      "batch 619, loss: 0.0310, instance_loss: 0.0222, weighted_loss: 0.0283, label: 0, bag_size: 1349\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 2195\n",
      "batch 659, loss: 0.0033, instance_loss: 0.0047, weighted_loss: 0.0037, label: 0, bag_size: 14266\n",
      "batch 679, loss: 0.0267, instance_loss: 0.0349, weighted_loss: 0.0292, label: 0, bag_size: 1732\n",
      "batch 699, loss: 0.0127, instance_loss: 0.0115, weighted_loss: 0.0123, label: 0, bag_size: 6884\n",
      "batch 719, loss: 0.0558, instance_loss: 0.0532, weighted_loss: 0.0550, label: 1, bag_size: 2842\n",
      "batch 739, loss: 0.0096, instance_loss: 0.0068, weighted_loss: 0.0087, label: 1, bag_size: 3980\n",
      "batch 759, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 2844\n",
      "batch 779, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15003\n",
      "batch 799, loss: 0.1433, instance_loss: 0.1354, weighted_loss: 0.1409, label: 0, bag_size: 1349\n",
      "batch 819, loss: 0.0307, instance_loss: 0.0211, weighted_loss: 0.0279, label: 0, bag_size: 1438\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.978810975609756: correct 12842/13120\n",
      "class 1 clustering acc 0.8946646341463415: correct 5869/6560\n",
      "Epoch: 67, train_loss: 0.1591, train_clustering_loss:  0.1839, train_error: 0.0671\n",
      "class 0: acc 0.927710843373494, correct 385/415\n",
      "class 1: acc 0.9382716049382716, correct 380/405\n",
      "\n",
      "Val Set, val_loss: 0.2579, val_error: 0.1182, auc: 0.9738\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.08068181818181819: correct 71/880\n",
      "class 0: acc 0.7884615384615384, correct 41/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 18 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0156, instance_loss: 0.0089, weighted_loss: 0.0136, label: 1, bag_size: 1525\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 2654\n",
      "batch 59, loss: 0.0738, instance_loss: 0.0775, weighted_loss: 0.0749, label: 1, bag_size: 6478\n",
      "batch 79, loss: 0.0033, instance_loss: 0.0007, weighted_loss: 0.0025, label: 1, bag_size: 13365\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 12217\n",
      "batch 119, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 19039\n",
      "batch 139, loss: 0.0004, instance_loss: 0.0004, weighted_loss: 0.0004, label: 0, bag_size: 24439\n",
      "batch 159, loss: 0.1058, instance_loss: 0.0930, weighted_loss: 0.1019, label: 0, bag_size: 1909\n",
      "batch 179, loss: 1.0994, instance_loss: 1.3205, weighted_loss: 1.1657, label: 1, bag_size: 684\n",
      "batch 199, loss: 0.1106, instance_loss: 0.0954, weighted_loss: 0.1061, label: 0, bag_size: 12732\n",
      "batch 219, loss: 0.0006, instance_loss: 0.0002, weighted_loss: 0.0005, label: 0, bag_size: 9949\n",
      "batch 239, loss: 0.0012, instance_loss: 0.0008, weighted_loss: 0.0011, label: 0, bag_size: 16936\n",
      "batch 259, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 4465\n",
      "batch 279, loss: 0.0003, instance_loss: 0.0001, weighted_loss: 0.0003, label: 0, bag_size: 7381\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 2760\n",
      "batch 319, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 2920\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19390\n",
      "batch 359, loss: 0.0589, instance_loss: 0.1087, weighted_loss: 0.0738, label: 1, bag_size: 7110\n",
      "batch 379, loss: 0.0253, instance_loss: 0.0165, weighted_loss: 0.0227, label: 0, bag_size: 11146\n",
      "batch 399, loss: 0.0038, instance_loss: 0.0011, weighted_loss: 0.0030, label: 1, bag_size: 2785\n",
      "batch 419, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 12840\n",
      "batch 439, loss: 0.0202, instance_loss: 0.0177, weighted_loss: 0.0195, label: 0, bag_size: 8582\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9885\n",
      "batch 479, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 15464\n",
      "batch 499, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 8754\n",
      "batch 519, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 13880\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 2548\n",
      "batch 559, loss: 0.0026, instance_loss: 0.0021, weighted_loss: 0.0025, label: 0, bag_size: 13880\n",
      "batch 579, loss: 0.5062, instance_loss: 0.7443, weighted_loss: 0.5776, label: 0, bag_size: 1127\n",
      "batch 599, loss: 0.0458, instance_loss: 0.0389, weighted_loss: 0.0437, label: 0, bag_size: 4497\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 5894\n",
      "batch 639, loss: 0.0008, instance_loss: 0.0010, weighted_loss: 0.0009, label: 1, bag_size: 13365\n",
      "batch 659, loss: 0.0357, instance_loss: 0.0317, weighted_loss: 0.0345, label: 0, bag_size: 2336\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 10725\n",
      "batch 699, loss: 0.0025, instance_loss: 0.0055, weighted_loss: 0.0034, label: 1, bag_size: 7613\n",
      "batch 719, loss: 0.0260, instance_loss: 0.0329, weighted_loss: 0.0281, label: 0, bag_size: 17268\n",
      "batch 739, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 20796\n",
      "batch 759, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 1614\n",
      "batch 779, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 19972\n",
      "batch 799, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 931\n",
      "batch 819, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 8438\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9864329268292683: correct 12942/13120\n",
      "class 1 clustering acc 0.9164634146341464: correct 6012/6560\n",
      "Epoch: 68, train_loss: 0.1199, train_clustering_loss:  0.1485, train_error: 0.0476\n",
      "class 0: acc 0.9509345794392523, correct 407/428\n",
      "class 1: acc 0.9540816326530612, correct 374/392\n",
      "\n",
      "Val Set, val_loss: 0.3646, val_error: 0.1727, auc: 0.9751\n",
      "class 0 clustering acc 0.9971590909090909: correct 1755/1760\n",
      "class 1 clustering acc 0.3: correct 264/880\n",
      "class 0: acc 0.6538461538461539, correct 34/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 19 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0821, instance_loss: 0.0596, weighted_loss: 0.0754, label: 0, bag_size: 16690\n",
      "batch 39, loss: 0.0433, instance_loss: 0.0303, weighted_loss: 0.0394, label: 1, bag_size: 9561\n",
      "batch 59, loss: 0.0082, instance_loss: 0.0045, weighted_loss: 0.0071, label: 1, bag_size: 3368\n",
      "batch 79, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 617\n",
      "batch 99, loss: 0.0696, instance_loss: 0.0573, weighted_loss: 0.0659, label: 0, bag_size: 2242\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 7011\n",
      "batch 139, loss: 0.0227, instance_loss: 0.0172, weighted_loss: 0.0210, label: 0, bag_size: 2815\n",
      "batch 159, loss: 0.0031, instance_loss: 0.0038, weighted_loss: 0.0033, label: 1, bag_size: 5903\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 14319\n",
      "batch 199, loss: 2.2625, instance_loss: 2.7925, weighted_loss: 2.4215, label: 0, bag_size: 14664\n",
      "batch 219, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 18045\n",
      "batch 239, loss: 0.2188, instance_loss: 0.2488, weighted_loss: 0.2278, label: 0, bag_size: 2918\n",
      "batch 259, loss: 0.0004, instance_loss: 0.0001, weighted_loss: 0.0003, label: 0, bag_size: 9851\n",
      "batch 279, loss: 0.0031, instance_loss: 0.0015, weighted_loss: 0.0026, label: 1, bag_size: 1512\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0054, weighted_loss: 0.0016, label: 0, bag_size: 21218\n",
      "batch 319, loss: 0.0008, instance_loss: 0.0004, weighted_loss: 0.0007, label: 0, bag_size: 2322\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 7110\n",
      "batch 359, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 1, bag_size: 14604\n",
      "batch 379, loss: 0.0445, instance_loss: 0.0312, weighted_loss: 0.0405, label: 1, bag_size: 1483\n",
      "batch 399, loss: 0.0033, instance_loss: 0.0002, weighted_loss: 0.0024, label: 0, bag_size: 5551\n",
      "batch 419, loss: 0.1165, instance_loss: 0.1119, weighted_loss: 0.1151, label: 1, bag_size: 9470\n",
      "batch 439, loss: 0.0008, instance_loss: 0.0005, weighted_loss: 0.0008, label: 0, bag_size: 10410\n",
      "batch 459, loss: 0.0093, instance_loss: 0.0094, weighted_loss: 0.0093, label: 1, bag_size: 4789\n",
      "batch 479, loss: 0.7020, instance_loss: 0.5387, weighted_loss: 0.6530, label: 1, bag_size: 18161\n",
      "batch 499, loss: 0.0029, instance_loss: 0.0010, weighted_loss: 0.0023, label: 0, bag_size: 2760\n",
      "batch 519, loss: 1.8945, instance_loss: 1.9642, weighted_loss: 1.9154, label: 0, bag_size: 11306\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 12201\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11387\n",
      "batch 579, loss: 0.1471, instance_loss: 0.1429, weighted_loss: 0.1458, label: 0, bag_size: 705\n",
      "batch 599, loss: 0.0023, instance_loss: 0.0042, weighted_loss: 0.0029, label: 0, bag_size: 1891\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 18738\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 19832\n",
      "batch 659, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 1891\n",
      "batch 679, loss: 0.0238, instance_loss: 0.0286, weighted_loss: 0.0252, label: 0, bag_size: 7835\n",
      "batch 699, loss: 0.4817, instance_loss: 0.6639, weighted_loss: 0.5363, label: 1, bag_size: 21450\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 25970\n",
      "batch 739, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11113\n",
      "batch 759, loss: 0.0048, instance_loss: 0.0014, weighted_loss: 0.0038, label: 1, bag_size: 16890\n",
      "batch 779, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 6745\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 15332\n",
      "batch 819, loss: 0.0036, instance_loss: 0.0021, weighted_loss: 0.0032, label: 1, bag_size: 1242\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9855182926829268: correct 12930/13120\n",
      "class 1 clustering acc 0.9307926829268293: correct 6106/6560\n",
      "Epoch: 69, train_loss: 0.1181, train_clustering_loss:  0.1372, train_error: 0.0451\n",
      "class 0: acc 0.9560975609756097, correct 392/410\n",
      "class 1: acc 0.9536585365853658, correct 391/410\n",
      "\n",
      "Val Set, val_loss: 0.4640, val_error: 0.1636, auc: 0.9758\n",
      "class 0 clustering acc 0.9454545454545454: correct 1664/1760\n",
      "class 1 clustering acc 0.15681818181818183: correct 138/880\n",
      "class 0: acc 0.6923076923076923, correct 36/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Val error: 0.0545, ROC AUC: 0.9828\n",
      "Test error: 0.0928, ROC AUC: 0.9690\n",
      "class 0: acc 1.0, correct 49/49\n",
      "class 1: acc 0.8125, correct 39/48\n",
      "\n",
      "Training Fold 1!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 822 samples\n",
      "Validating on 109 samples\n",
      "Testing on 96 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Setting tau to 1.0\n",
      "Done!\n",
      "TransformerMIL_SB(\n",
      "  (instance_loss_fn): SmoothTop1SVM()\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (transformer): TransformerEncoder_PerformerAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): SelfAttention(\n",
      "            (fast_attention): FastAttention(\n",
      "              (kernel_fn): ReLU()\n",
      "            )\n",
      "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attention): Attn_Net_Gated(\n",
      "    (attention_a): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_b): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (attention_V2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (attention_U2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (attention_weights2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Total number of parameters: 8406537\n",
      "Total number of trainable parameters: 8406537\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 13.8540, instance_loss: 2.0177, weighted_loss: 10.3031, label: 0, bag_size: 10490\n",
      "batch 39, loss: 2.9101, instance_loss: 3.5007, weighted_loss: 3.0873, label: 1, bag_size: 17769\n",
      "batch 59, loss: 0.5025, instance_loss: 2.2934, weighted_loss: 1.0397, label: 0, bag_size: 15898\n",
      "batch 79, loss: 0.8163, instance_loss: 1.2263, weighted_loss: 0.9393, label: 1, bag_size: 5763\n",
      "batch 99, loss: 1.6909, instance_loss: 1.0409, weighted_loss: 1.4959, label: 1, bag_size: 19932\n",
      "batch 119, loss: 1.7199, instance_loss: 0.7215, weighted_loss: 1.4204, label: 1, bag_size: 8754\n",
      "batch 139, loss: 0.2540, instance_loss: 0.7018, weighted_loss: 0.3883, label: 0, bag_size: 4598\n",
      "batch 159, loss: 0.5316, instance_loss: 0.7752, weighted_loss: 0.6047, label: 0, bag_size: 11199\n",
      "batch 179, loss: 4.4298, instance_loss: 1.4001, weighted_loss: 3.5209, label: 1, bag_size: 13367\n",
      "batch 199, loss: 0.6671, instance_loss: 0.9219, weighted_loss: 0.7435, label: 0, bag_size: 17630\n",
      "batch 219, loss: 7.4307, instance_loss: 2.1818, weighted_loss: 5.8560, label: 0, bag_size: 7235\n",
      "batch 239, loss: 0.4017, instance_loss: 1.1738, weighted_loss: 0.6333, label: 0, bag_size: 18574\n",
      "batch 259, loss: 0.0879, instance_loss: 0.9636, weighted_loss: 0.3506, label: 0, bag_size: 16211\n",
      "batch 279, loss: 0.1007, instance_loss: 1.8828, weighted_loss: 0.6353, label: 0, bag_size: 9387\n",
      "batch 299, loss: 2.5035, instance_loss: 2.5244, weighted_loss: 2.5098, label: 0, bag_size: 12131\n",
      "batch 319, loss: 0.4806, instance_loss: 1.8293, weighted_loss: 0.8852, label: 1, bag_size: 15332\n",
      "batch 339, loss: 0.2967, instance_loss: 1.5911, weighted_loss: 0.6850, label: 1, bag_size: 6599\n",
      "batch 359, loss: 0.1342, instance_loss: 1.5147, weighted_loss: 0.5483, label: 1, bag_size: 11389\n",
      "batch 379, loss: 1.5711, instance_loss: 1.4100, weighted_loss: 1.5228, label: 1, bag_size: 12712\n",
      "batch 399, loss: 0.2426, instance_loss: 0.8386, weighted_loss: 0.4214, label: 0, bag_size: 1438\n",
      "batch 419, loss: 0.3000, instance_loss: 0.7283, weighted_loss: 0.4285, label: 1, bag_size: 11884\n",
      "batch 439, loss: 0.1676, instance_loss: 0.7147, weighted_loss: 0.3317, label: 1, bag_size: 10112\n",
      "batch 459, loss: 0.5857, instance_loss: 0.7023, weighted_loss: 0.6207, label: 1, bag_size: 6752\n",
      "batch 479, loss: 0.2345, instance_loss: 1.2324, weighted_loss: 0.5339, label: 0, bag_size: 1437\n",
      "batch 499, loss: 0.1368, instance_loss: 1.0096, weighted_loss: 0.3986, label: 1, bag_size: 13440\n",
      "batch 519, loss: 0.2071, instance_loss: 0.7427, weighted_loss: 0.3678, label: 0, bag_size: 20150\n",
      "batch 539, loss: 0.0628, instance_loss: 0.7029, weighted_loss: 0.2548, label: 0, bag_size: 11259\n",
      "batch 559, loss: 1.9445, instance_loss: 1.2968, weighted_loss: 1.7502, label: 0, bag_size: 6367\n",
      "batch 579, loss: 0.5421, instance_loss: 1.0071, weighted_loss: 0.6816, label: 1, bag_size: 13089\n",
      "batch 599, loss: 0.0178, instance_loss: 0.7236, weighted_loss: 0.2295, label: 0, bag_size: 2044\n",
      "batch 619, loss: 0.0053, instance_loss: 0.6869, weighted_loss: 0.2098, label: 1, bag_size: 18603\n",
      "batch 639, loss: 0.0861, instance_loss: 0.9780, weighted_loss: 0.3537, label: 1, bag_size: 4480\n",
      "batch 659, loss: 0.0239, instance_loss: 0.6902, weighted_loss: 0.2238, label: 0, bag_size: 17155\n",
      "batch 679, loss: 1.5620, instance_loss: 1.0855, weighted_loss: 1.4191, label: 0, bag_size: 3089\n",
      "batch 699, loss: 0.0771, instance_loss: 0.7364, weighted_loss: 0.2749, label: 0, bag_size: 16782\n",
      "batch 719, loss: 3.3199, instance_loss: 0.9633, weighted_loss: 2.6129, label: 0, bag_size: 12593\n",
      "batch 739, loss: 1.0182, instance_loss: 0.8188, weighted_loss: 0.9584, label: 1, bag_size: 15332\n",
      "batch 759, loss: 0.0482, instance_loss: 0.7670, weighted_loss: 0.2638, label: 1, bag_size: 1786\n",
      "batch 779, loss: 0.0405, instance_loss: 0.7292, weighted_loss: 0.2471, label: 0, bag_size: 14664\n",
      "batch 799, loss: 3.6322, instance_loss: 1.6479, weighted_loss: 3.0369, label: 1, bag_size: 10105\n",
      "batch 819, loss: 0.2218, instance_loss: 0.6269, weighted_loss: 0.3434, label: 1, bag_size: 16267\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.8877737226277372: correct 11676/13152\n",
      "class 1 clustering acc 0.17989659367396593: correct 1183/6576\n",
      "Epoch: 0, train_loss: 1.0022, train_clustering_loss:  1.0962, train_error: 0.3613\n",
      "class 0: acc 0.6234718826405868, correct 255/409\n",
      "class 1: acc 0.6537530266343826, correct 270/413\n",
      "\n",
      "Val Set, val_loss: 0.4338, val_error: 0.1743, auc: 0.9300\n",
      "class 0 clustering acc 0.9868119266055045: correct 1721/1744\n",
      "class 1 clustering acc 0.04128440366972477: correct 36/872\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.7301587301587301, correct 46/63\n",
      "Validation loss decreased (inf --> 0.433782).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0132, instance_loss: 0.6863, weighted_loss: 0.2151, label: 1, bag_size: 15125\n",
      "batch 39, loss: 0.7294, instance_loss: 0.7534, weighted_loss: 0.7366, label: 0, bag_size: 1349\n",
      "batch 59, loss: 0.2611, instance_loss: 0.6626, weighted_loss: 0.3815, label: 1, bag_size: 7935\n",
      "batch 79, loss: 2.0663, instance_loss: 1.3731, weighted_loss: 1.8584, label: 0, bag_size: 8427\n",
      "batch 99, loss: 0.0010, instance_loss: 0.6201, weighted_loss: 0.1867, label: 1, bag_size: 19972\n",
      "batch 119, loss: 0.0140, instance_loss: 0.6902, weighted_loss: 0.2169, label: 0, bag_size: 20555\n",
      "batch 139, loss: 0.2609, instance_loss: 0.6859, weighted_loss: 0.3884, label: 1, bag_size: 2848\n",
      "batch 159, loss: 0.4077, instance_loss: 0.6941, weighted_loss: 0.4936, label: 1, bag_size: 12895\n",
      "batch 179, loss: 0.1092, instance_loss: 0.9059, weighted_loss: 0.3482, label: 0, bag_size: 9171\n",
      "batch 199, loss: 0.0121, instance_loss: 0.6398, weighted_loss: 0.2004, label: 1, bag_size: 8216\n",
      "batch 219, loss: 2.1261, instance_loss: 0.7719, weighted_loss: 1.7199, label: 1, bag_size: 19932\n",
      "batch 239, loss: 0.0591, instance_loss: 0.9284, weighted_loss: 0.3199, label: 1, bag_size: 2904\n",
      "batch 259, loss: 0.0045, instance_loss: 0.6697, weighted_loss: 0.2041, label: 0, bag_size: 12137\n",
      "batch 279, loss: 3.0822, instance_loss: 1.6758, weighted_loss: 2.6603, label: 0, bag_size: 5105\n",
      "batch 299, loss: 0.0125, instance_loss: 0.7515, weighted_loss: 0.2342, label: 1, bag_size: 2405\n",
      "batch 319, loss: 0.2685, instance_loss: 0.9193, weighted_loss: 0.4637, label: 0, bag_size: 2920\n",
      "batch 339, loss: 0.0143, instance_loss: 0.6502, weighted_loss: 0.2051, label: 1, bag_size: 11223\n",
      "batch 359, loss: 0.7271, instance_loss: 0.7033, weighted_loss: 0.7199, label: 1, bag_size: 3437\n",
      "batch 379, loss: 0.4661, instance_loss: 1.5061, weighted_loss: 0.7781, label: 0, bag_size: 10063\n",
      "batch 399, loss: 0.1688, instance_loss: 0.7442, weighted_loss: 0.3414, label: 1, bag_size: 2381\n",
      "batch 419, loss: 0.0001, instance_loss: 0.9734, weighted_loss: 0.2921, label: 1, bag_size: 13194\n",
      "batch 439, loss: 0.0184, instance_loss: 0.7262, weighted_loss: 0.2308, label: 1, bag_size: 5155\n",
      "batch 459, loss: 0.0009, instance_loss: 0.6846, weighted_loss: 0.2060, label: 0, bag_size: 15057\n",
      "batch 479, loss: 6.4928, instance_loss: 2.8327, weighted_loss: 5.3948, label: 0, bag_size: 4692\n",
      "batch 499, loss: 0.0167, instance_loss: 0.7732, weighted_loss: 0.2436, label: 1, bag_size: 8680\n",
      "batch 519, loss: 3.5403, instance_loss: 1.4229, weighted_loss: 2.9051, label: 1, bag_size: 8602\n",
      "batch 539, loss: 0.0107, instance_loss: 0.6639, weighted_loss: 0.2066, label: 0, bag_size: 26271\n",
      "batch 559, loss: 0.3476, instance_loss: 0.6913, weighted_loss: 0.4507, label: 1, bag_size: 1064\n",
      "batch 579, loss: 0.0003, instance_loss: 0.6359, weighted_loss: 0.1910, label: 0, bag_size: 11527\n",
      "batch 599, loss: 2.6735, instance_loss: 2.3829, weighted_loss: 2.5863, label: 1, bag_size: 13440\n",
      "batch 619, loss: 0.9944, instance_loss: 0.8964, weighted_loss: 0.9650, label: 0, bag_size: 11151\n",
      "batch 639, loss: 0.1677, instance_loss: 0.8495, weighted_loss: 0.3722, label: 0, bag_size: 10146\n",
      "batch 659, loss: 0.1737, instance_loss: 0.6964, weighted_loss: 0.3305, label: 1, bag_size: 7186\n",
      "batch 679, loss: 0.1012, instance_loss: 0.7552, weighted_loss: 0.2974, label: 1, bag_size: 10848\n",
      "batch 699, loss: 0.0971, instance_loss: 0.6993, weighted_loss: 0.2778, label: 1, bag_size: 6736\n",
      "batch 719, loss: 0.0642, instance_loss: 0.6048, weighted_loss: 0.2264, label: 0, bag_size: 1984\n",
      "batch 739, loss: 0.0033, instance_loss: 0.8417, weighted_loss: 0.2548, label: 1, bag_size: 2814\n",
      "batch 759, loss: 0.3829, instance_loss: 0.7651, weighted_loss: 0.4975, label: 1, bag_size: 10432\n",
      "batch 779, loss: 0.0013, instance_loss: 0.6426, weighted_loss: 0.1937, label: 1, bag_size: 2522\n",
      "batch 799, loss: 0.0048, instance_loss: 0.6577, weighted_loss: 0.2007, label: 0, bag_size: 14828\n",
      "batch 819, loss: 0.0234, instance_loss: 0.6646, weighted_loss: 0.2158, label: 1, bag_size: 13051\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.8595650851581509: correct 11305/13152\n",
      "class 1 clustering acc 0.30170316301703165: correct 1984/6576\n",
      "Epoch: 1, train_loss: 0.5798, train_clustering_loss:  0.8721, train_error: 0.1983\n",
      "class 0: acc 0.7924050632911392, correct 313/395\n",
      "class 1: acc 0.810304449648712, correct 346/427\n",
      "\n",
      "Val Set, val_loss: 0.3481, val_error: 0.1468, auc: 0.9344\n",
      "class 0 clustering acc 0.8893348623853211: correct 1551/1744\n",
      "class 1 clustering acc 0.26605504587155965: correct 232/872\n",
      "class 0: acc 0.9130434782608695, correct 42/46\n",
      "class 1: acc 0.8095238095238095, correct 51/63\n",
      "Validation loss decreased (0.433782 --> 0.348109).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0595, instance_loss: 0.6902, weighted_loss: 0.2487, label: 0, bag_size: 11390\n",
      "batch 39, loss: 2.8289, instance_loss: 2.2860, weighted_loss: 2.6660, label: 0, bag_size: 10113\n",
      "batch 59, loss: 0.1900, instance_loss: 0.6183, weighted_loss: 0.3185, label: 0, bag_size: 15672\n",
      "batch 79, loss: 0.5835, instance_loss: 0.7324, weighted_loss: 0.6282, label: 1, bag_size: 1015\n",
      "batch 99, loss: 2.9117, instance_loss: 2.0469, weighted_loss: 2.6523, label: 1, bag_size: 12494\n",
      "batch 119, loss: 0.0061, instance_loss: 0.7165, weighted_loss: 0.2192, label: 0, bag_size: 9485\n",
      "batch 139, loss: 0.1161, instance_loss: 1.3455, weighted_loss: 0.4849, label: 0, bag_size: 2609\n",
      "batch 159, loss: 0.0052, instance_loss: 0.6133, weighted_loss: 0.1876, label: 0, bag_size: 2179\n",
      "batch 179, loss: 0.0021, instance_loss: 0.6311, weighted_loss: 0.1908, label: 1, bag_size: 4715\n",
      "batch 199, loss: 1.2667, instance_loss: 1.7341, weighted_loss: 1.4069, label: 0, bag_size: 3783\n",
      "batch 219, loss: 0.0136, instance_loss: 0.6117, weighted_loss: 0.1930, label: 0, bag_size: 22498\n",
      "batch 239, loss: 1.4836, instance_loss: 0.8303, weighted_loss: 1.2876, label: 1, bag_size: 20333\n",
      "batch 259, loss: 0.3663, instance_loss: 1.1634, weighted_loss: 0.6054, label: 0, bag_size: 18215\n",
      "batch 279, loss: 0.3686, instance_loss: 1.0316, weighted_loss: 0.5675, label: 0, bag_size: 8959\n",
      "batch 299, loss: 0.0920, instance_loss: 0.6092, weighted_loss: 0.2472, label: 0, bag_size: 31085\n",
      "batch 319, loss: 0.0085, instance_loss: 0.6080, weighted_loss: 0.1884, label: 1, bag_size: 11701\n",
      "batch 339, loss: 0.2728, instance_loss: 0.6652, weighted_loss: 0.3905, label: 0, bag_size: 7823\n",
      "batch 359, loss: 2.8592, instance_loss: 0.8139, weighted_loss: 2.2456, label: 0, bag_size: 2036\n",
      "batch 379, loss: 1.8166, instance_loss: 2.3799, weighted_loss: 1.9856, label: 1, bag_size: 15192\n",
      "batch 399, loss: 0.0008, instance_loss: 0.6294, weighted_loss: 0.1894, label: 0, bag_size: 18225\n",
      "batch 419, loss: 0.0630, instance_loss: 0.6291, weighted_loss: 0.2329, label: 0, bag_size: 2044\n",
      "batch 439, loss: 0.0051, instance_loss: 0.6634, weighted_loss: 0.2026, label: 0, bag_size: 8252\n",
      "batch 459, loss: 0.8470, instance_loss: 1.0989, weighted_loss: 0.9226, label: 1, bag_size: 13440\n",
      "batch 479, loss: 0.0020, instance_loss: 0.5692, weighted_loss: 0.1721, label: 0, bag_size: 14681\n",
      "batch 499, loss: 0.0001, instance_loss: 0.4861, weighted_loss: 0.1459, label: 1, bag_size: 7078\n",
      "batch 519, loss: 0.1617, instance_loss: 0.8056, weighted_loss: 0.3549, label: 1, bag_size: 2759\n",
      "batch 539, loss: 0.1630, instance_loss: 0.6568, weighted_loss: 0.3111, label: 1, bag_size: 4259\n",
      "batch 559, loss: 0.0657, instance_loss: 0.6965, weighted_loss: 0.2549, label: 0, bag_size: 8420\n",
      "batch 579, loss: 0.2062, instance_loss: 0.6477, weighted_loss: 0.3386, label: 1, bag_size: 2935\n",
      "batch 599, loss: 0.0338, instance_loss: 0.5772, weighted_loss: 0.1968, label: 0, bag_size: 5161\n",
      "batch 619, loss: 1.7410, instance_loss: 0.8511, weighted_loss: 1.4741, label: 0, bag_size: 10029\n",
      "batch 639, loss: 0.0021, instance_loss: 0.3400, weighted_loss: 0.1035, label: 1, bag_size: 10867\n",
      "batch 659, loss: 0.0055, instance_loss: 0.5923, weighted_loss: 0.1815, label: 0, bag_size: 20666\n",
      "batch 679, loss: 0.0544, instance_loss: 0.5873, weighted_loss: 0.2143, label: 0, bag_size: 2534\n",
      "batch 699, loss: 0.0049, instance_loss: 0.4846, weighted_loss: 0.1488, label: 0, bag_size: 11383\n",
      "batch 719, loss: 0.0120, instance_loss: 0.4847, weighted_loss: 0.1538, label: 1, bag_size: 14433\n",
      "batch 739, loss: 0.7694, instance_loss: 0.7110, weighted_loss: 0.7519, label: 0, bag_size: 7917\n",
      "batch 759, loss: 0.0557, instance_loss: 0.5873, weighted_loss: 0.2152, label: 0, bag_size: 19808\n",
      "batch 779, loss: 8.3769, instance_loss: 3.3853, weighted_loss: 6.8794, label: 0, bag_size: 3468\n",
      "batch 799, loss: 0.0243, instance_loss: 0.5256, weighted_loss: 0.1747, label: 0, bag_size: 19466\n",
      "batch 819, loss: 0.0544, instance_loss: 0.6070, weighted_loss: 0.2201, label: 0, bag_size: 29270\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.8967457420924574: correct 11794/13152\n",
      "class 1 clustering acc 0.35933698296836986: correct 2363/6576\n",
      "Epoch: 2, train_loss: 0.4891, train_clustering_loss:  0.8125, train_error: 0.1861\n",
      "class 0: acc 0.8195488721804511, correct 327/399\n",
      "class 1: acc 0.8085106382978723, correct 342/423\n",
      "\n",
      "Val Set, val_loss: 0.3956, val_error: 0.1468, auc: 0.9289\n",
      "class 0 clustering acc 0.7219036697247706: correct 1259/1744\n",
      "class 1 clustering acc 0.6708715596330275: correct 585/872\n",
      "class 0: acc 0.9130434782608695, correct 42/46\n",
      "class 1: acc 0.8095238095238095, correct 51/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0027, instance_loss: 0.5514, weighted_loss: 0.1673, label: 1, bag_size: 10482\n",
      "batch 39, loss: 0.0837, instance_loss: 0.4572, weighted_loss: 0.1957, label: 0, bag_size: 10995\n",
      "batch 59, loss: 0.0174, instance_loss: 0.4453, weighted_loss: 0.1458, label: 0, bag_size: 9885\n",
      "batch 79, loss: 0.0126, instance_loss: 0.6660, weighted_loss: 0.2086, label: 1, bag_size: 7935\n",
      "batch 99, loss: 3.1074, instance_loss: 2.6520, weighted_loss: 2.9708, label: 0, bag_size: 14264\n",
      "batch 119, loss: 0.0324, instance_loss: 0.3933, weighted_loss: 0.1407, label: 0, bag_size: 8866\n",
      "batch 139, loss: 0.1264, instance_loss: 0.4143, weighted_loss: 0.2128, label: 1, bag_size: 8003\n",
      "batch 159, loss: 5.5150, instance_loss: 2.9953, weighted_loss: 4.7591, label: 1, bag_size: 2565\n",
      "batch 179, loss: 0.0627, instance_loss: 0.3875, weighted_loss: 0.1602, label: 1, bag_size: 19500\n",
      "batch 199, loss: 4.9035, instance_loss: 4.8080, weighted_loss: 4.8748, label: 0, bag_size: 5105\n",
      "batch 219, loss: 1.7559, instance_loss: 1.4250, weighted_loss: 1.6566, label: 0, bag_size: 6367\n",
      "batch 239, loss: 0.0978, instance_loss: 0.8845, weighted_loss: 0.3338, label: 1, bag_size: 3295\n",
      "batch 259, loss: 2.2792, instance_loss: 1.9238, weighted_loss: 2.1725, label: 1, bag_size: 21252\n",
      "batch 279, loss: 0.0025, instance_loss: 0.2948, weighted_loss: 0.0902, label: 0, bag_size: 1651\n",
      "batch 299, loss: 0.0637, instance_loss: 0.6439, weighted_loss: 0.2378, label: 1, bag_size: 5605\n",
      "batch 319, loss: 0.0657, instance_loss: 0.5156, weighted_loss: 0.2007, label: 1, bag_size: 1924\n",
      "batch 339, loss: 0.0063, instance_loss: 0.4400, weighted_loss: 0.1364, label: 0, bag_size: 12131\n",
      "batch 359, loss: 0.6204, instance_loss: 1.9609, weighted_loss: 1.0225, label: 0, bag_size: 1953\n",
      "batch 379, loss: 0.3766, instance_loss: 0.5690, weighted_loss: 0.4343, label: 0, bag_size: 20478\n",
      "batch 399, loss: 0.8155, instance_loss: 0.8075, weighted_loss: 0.8131, label: 1, bag_size: 1609\n",
      "batch 419, loss: 0.0659, instance_loss: 0.5088, weighted_loss: 0.1988, label: 1, bag_size: 9065\n",
      "batch 439, loss: 0.0044, instance_loss: 0.2996, weighted_loss: 0.0930, label: 0, bag_size: 9060\n",
      "batch 459, loss: 1.2562, instance_loss: 2.0311, weighted_loss: 1.4886, label: 0, bag_size: 2815\n",
      "batch 479, loss: 0.7805, instance_loss: 1.2655, weighted_loss: 0.9260, label: 1, bag_size: 1703\n",
      "batch 499, loss: 0.0882, instance_loss: 0.6455, weighted_loss: 0.2554, label: 0, bag_size: 9596\n",
      "batch 519, loss: 0.1666, instance_loss: 0.5785, weighted_loss: 0.2902, label: 0, bag_size: 10721\n",
      "batch 539, loss: 0.0252, instance_loss: 0.4185, weighted_loss: 0.1432, label: 1, bag_size: 12697\n",
      "batch 559, loss: 0.0596, instance_loss: 0.1846, weighted_loss: 0.0971, label: 1, bag_size: 7110\n",
      "batch 579, loss: 1.4256, instance_loss: 0.9866, weighted_loss: 1.2939, label: 0, bag_size: 9069\n",
      "batch 599, loss: 0.0002, instance_loss: 0.0448, weighted_loss: 0.0136, label: 0, bag_size: 13225\n",
      "batch 619, loss: 0.1112, instance_loss: 0.4086, weighted_loss: 0.2004, label: 1, bag_size: 12408\n",
      "batch 639, loss: 0.0087, instance_loss: 0.3239, weighted_loss: 0.1032, label: 0, bag_size: 3101\n",
      "batch 659, loss: 0.0018, instance_loss: 0.1534, weighted_loss: 0.0473, label: 0, bag_size: 10995\n",
      "batch 679, loss: 0.0159, instance_loss: 0.4121, weighted_loss: 0.1347, label: 1, bag_size: 8026\n",
      "batch 699, loss: 1.4558, instance_loss: 1.1771, weighted_loss: 1.3722, label: 1, bag_size: 771\n",
      "batch 719, loss: 0.7678, instance_loss: 0.8687, weighted_loss: 0.7981, label: 1, bag_size: 5723\n",
      "batch 739, loss: 0.0282, instance_loss: 0.0849, weighted_loss: 0.0452, label: 0, bag_size: 2628\n",
      "batch 759, loss: 0.6313, instance_loss: 0.7257, weighted_loss: 0.6596, label: 0, bag_size: 18738\n",
      "batch 779, loss: 0.0030, instance_loss: 0.0168, weighted_loss: 0.0071, label: 0, bag_size: 1984\n",
      "batch 799, loss: 0.0015, instance_loss: 0.0167, weighted_loss: 0.0060, label: 0, bag_size: 18574\n",
      "batch 819, loss: 0.0665, instance_loss: 0.6153, weighted_loss: 0.2311, label: 1, bag_size: 15093\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9355991484184915: correct 12305/13152\n",
      "class 1 clustering acc 0.5200729927007299: correct 3420/6576\n",
      "Epoch: 3, train_loss: 0.4328, train_clustering_loss:  0.6713, train_error: 0.1727\n",
      "class 0: acc 0.8426150121065376, correct 348/413\n",
      "class 1: acc 0.8117359413202934, correct 332/409\n",
      "\n",
      "Val Set, val_loss: 0.6494, val_error: 0.1743, auc: 0.9303\n",
      "class 0 clustering acc 1.0: correct 1744/1744\n",
      "class 1 clustering acc 0.0194954128440367: correct 17/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.6984126984126984, correct 44/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2769, instance_loss: 0.9928, weighted_loss: 0.4916, label: 0, bag_size: 11212\n",
      "batch 39, loss: 0.0034, instance_loss: 0.2345, weighted_loss: 0.0727, label: 0, bag_size: 8898\n",
      "batch 59, loss: 0.0148, instance_loss: 0.0939, weighted_loss: 0.0386, label: 0, bag_size: 8981\n",
      "batch 79, loss: 0.0252, instance_loss: 0.1083, weighted_loss: 0.0501, label: 1, bag_size: 12895\n",
      "batch 99, loss: 0.0014, instance_loss: 0.0219, weighted_loss: 0.0076, label: 0, bag_size: 15736\n",
      "batch 119, loss: 0.0102, instance_loss: 0.4081, weighted_loss: 0.1296, label: 0, bag_size: 1825\n",
      "batch 139, loss: 0.0002, instance_loss: 0.0285, weighted_loss: 0.0087, label: 0, bag_size: 12793\n",
      "batch 159, loss: 0.0015, instance_loss: 0.0094, weighted_loss: 0.0038, label: 0, bag_size: 21082\n",
      "batch 179, loss: 0.5414, instance_loss: 2.1500, weighted_loss: 1.0240, label: 0, bag_size: 1800\n",
      "batch 199, loss: 1.0883, instance_loss: 1.1879, weighted_loss: 1.1182, label: 0, bag_size: 15636\n",
      "batch 219, loss: 0.6588, instance_loss: 1.6582, weighted_loss: 0.9586, label: 1, bag_size: 699\n",
      "batch 239, loss: 0.0376, instance_loss: 0.3001, weighted_loss: 0.1164, label: 0, bag_size: 3502\n",
      "batch 259, loss: 0.0540, instance_loss: 0.9480, weighted_loss: 0.3222, label: 0, bag_size: 1797\n",
      "batch 279, loss: 0.0093, instance_loss: 0.4743, weighted_loss: 0.1488, label: 1, bag_size: 5494\n",
      "batch 299, loss: 1.3205, instance_loss: 1.2349, weighted_loss: 1.2948, label: 1, bag_size: 1230\n",
      "batch 319, loss: 0.0223, instance_loss: 0.3672, weighted_loss: 0.1258, label: 1, bag_size: 9533\n",
      "batch 339, loss: 0.1530, instance_loss: 0.5903, weighted_loss: 0.2842, label: 1, bag_size: 15665\n",
      "batch 359, loss: 0.0008, instance_loss: 0.1736, weighted_loss: 0.0526, label: 0, bag_size: 21385\n",
      "batch 379, loss: 0.2173, instance_loss: 0.4049, weighted_loss: 0.2736, label: 1, bag_size: 8438\n",
      "batch 399, loss: 0.0975, instance_loss: 0.4976, weighted_loss: 0.2175, label: 0, bag_size: 2004\n",
      "batch 419, loss: 0.0028, instance_loss: 0.6703, weighted_loss: 0.2030, label: 1, bag_size: 3640\n",
      "batch 439, loss: 0.1294, instance_loss: 0.4605, weighted_loss: 0.2288, label: 1, bag_size: 12697\n",
      "batch 459, loss: 0.0709, instance_loss: 0.4565, weighted_loss: 0.1866, label: 1, bag_size: 8103\n",
      "batch 479, loss: 0.4007, instance_loss: 0.3554, weighted_loss: 0.3871, label: 0, bag_size: 2091\n",
      "batch 499, loss: 0.0125, instance_loss: 0.0328, weighted_loss: 0.0186, label: 0, bag_size: 18045\n",
      "batch 519, loss: 0.4000, instance_loss: 0.6047, weighted_loss: 0.4614, label: 0, bag_size: 18516\n",
      "batch 539, loss: 0.1795, instance_loss: 0.2672, weighted_loss: 0.2058, label: 0, bag_size: 2748\n",
      "batch 559, loss: 0.0086, instance_loss: 0.8353, weighted_loss: 0.2566, label: 1, bag_size: 7110\n",
      "batch 579, loss: 0.0005, instance_loss: 0.4720, weighted_loss: 0.1420, label: 1, bag_size: 9955\n",
      "batch 599, loss: 0.0012, instance_loss: 0.0080, weighted_loss: 0.0032, label: 0, bag_size: 10535\n",
      "batch 619, loss: 0.0836, instance_loss: 0.4314, weighted_loss: 0.1879, label: 0, bag_size: 9930\n",
      "batch 639, loss: 0.0440, instance_loss: 0.2825, weighted_loss: 0.1155, label: 1, bag_size: 4128\n",
      "batch 659, loss: 0.1002, instance_loss: 0.4799, weighted_loss: 0.2141, label: 1, bag_size: 629\n",
      "batch 679, loss: 0.0072, instance_loss: 0.0745, weighted_loss: 0.0274, label: 0, bag_size: 9060\n",
      "batch 699, loss: 0.0009, instance_loss: 0.0042, weighted_loss: 0.0019, label: 0, bag_size: 18132\n",
      "batch 719, loss: 0.0029, instance_loss: 0.2893, weighted_loss: 0.0889, label: 1, bag_size: 13015\n",
      "batch 739, loss: 0.1912, instance_loss: 0.5924, weighted_loss: 0.3116, label: 0, bag_size: 1891\n",
      "batch 759, loss: 0.0817, instance_loss: 0.2468, weighted_loss: 0.1312, label: 0, bag_size: 14739\n",
      "batch 779, loss: 0.0182, instance_loss: 0.2500, weighted_loss: 0.0878, label: 1, bag_size: 12425\n",
      "batch 799, loss: 0.0095, instance_loss: 0.1026, weighted_loss: 0.0374, label: 1, bag_size: 1249\n",
      "batch 819, loss: 0.0181, instance_loss: 0.1083, weighted_loss: 0.0452, label: 0, bag_size: 19466\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9421380778588808: correct 12391/13152\n",
      "class 1 clustering acc 0.572992700729927: correct 3768/6576\n",
      "Epoch: 4, train_loss: 0.4244, train_clustering_loss:  0.6228, train_error: 0.1545\n",
      "class 0: acc 0.8459715639810427, correct 357/422\n",
      "class 1: acc 0.845, correct 338/400\n",
      "\n",
      "Val Set, val_loss: 0.3851, val_error: 0.1651, auc: 0.9306\n",
      "class 0 clustering acc 0.7958715596330275: correct 1388/1744\n",
      "class 1 clustering acc 0.4334862385321101: correct 378/872\n",
      "class 0: acc 0.9130434782608695, correct 42/46\n",
      "class 1: acc 0.7777777777777778, correct 49/63\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0015, instance_loss: 0.0064, weighted_loss: 0.0030, label: 0, bag_size: 13795\n",
      "batch 39, loss: 0.0012, instance_loss: 0.0001, weighted_loss: 0.0009, label: 0, bag_size: 31085\n",
      "batch 59, loss: 4.8017, instance_loss: 3.8693, weighted_loss: 4.5220, label: 0, bag_size: 17279\n",
      "batch 79, loss: 0.1417, instance_loss: 0.7942, weighted_loss: 0.3374, label: 0, bag_size: 6281\n",
      "batch 99, loss: 0.0036, instance_loss: 0.2362, weighted_loss: 0.0733, label: 1, bag_size: 15716\n",
      "batch 119, loss: 0.2926, instance_loss: 0.6157, weighted_loss: 0.3895, label: 0, bag_size: 14893\n",
      "batch 139, loss: 0.0205, instance_loss: 0.8137, weighted_loss: 0.2585, label: 0, bag_size: 1639\n",
      "batch 159, loss: 0.0085, instance_loss: 0.0628, weighted_loss: 0.0248, label: 0, bag_size: 13225\n",
      "batch 179, loss: 0.0049, instance_loss: 0.2429, weighted_loss: 0.0763, label: 1, bag_size: 3549\n",
      "batch 199, loss: 1.1876, instance_loss: 1.0005, weighted_loss: 1.1315, label: 0, bag_size: 1701\n",
      "batch 219, loss: 0.0167, instance_loss: 0.3022, weighted_loss: 0.1023, label: 0, bag_size: 14377\n",
      "batch 239, loss: 0.7694, instance_loss: 0.9899, weighted_loss: 0.8356, label: 1, bag_size: 1038\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python main.py --drop_out --early_stopping --lr 2e-4 --k 5 --label_frac 1 \\\n",
    "--exp_code cptac_lung_100_level0_transformer_adam --weighted_sample --bag_loss ce --inst_loss svm \\\n",
    "--task task_2_tumor_subtyping --model_type transmil --log_data --data_root_dir /home/sci/Disk2/CPTAC-LUNG/FEATURES_level0 \\\n",
    "--split_dir /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/cptac_lung_100 --subtyping \\\n",
    "--csv_path dataset_csv/cptac_lung_subtyping.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCBAT Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load Dataset\n",
      "label column: label\n",
      "label dictionary: {'LUAD': 0, 'LUSC': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 0    507\n",
      "1    520\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 164\n",
      "Slide-LVL; Number of samples registered in class 0: 507\n",
      "Patient-LVL; Number of samples registered in class 1: 157\n",
      "Slide-LVL; Number of samples registered in class 1: 520\n",
      "split_dir:  /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/cptac_lung_100\n",
      "################# Settings ###################\n",
      "num_splits:  5\n",
      "k_start:  -1\n",
      "k_end:  -1\n",
      "task:  task_2_tumor_subtyping\n",
      "max_epochs:  200\n",
      "results_dir:  ./results\n",
      "lr:  0.0002\n",
      "experiment:  cptac_lung_100_level02_mcbat_sb_depth2_adam_FLASH\n",
      "reg:  1e-05\n",
      "label_frac:  1.0\n",
      "bag_loss:  ce\n",
      "seed:  1\n",
      "model_type:  mcbat_sb\n",
      "model_size:  small\n",
      "use_drop_out:  True\n",
      "weighted_sample:  True\n",
      "opt:  adam\n",
      "split_dir:  /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/cptac_lung_100\n",
      "\n",
      "Training Fold 0!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 820 samples\n",
      "Validating on 110 samples\n",
      "Testing on 97 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Setting tau to 1.0\n",
      "Done!\n",
      "MCBAT_SB(\n",
      "  (instance_loss_fn): SmoothTop1SVM()\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (transformer_low): TransformerEncoder_FLASH(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FLASH(\n",
      "            (attn_fn): LaplacianAttnFn()\n",
      "            (rel_pos_bias): T5RelativePositionBias(\n",
      "              (relative_attention_bias): Embedding(32, 1)\n",
      "            )\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (to_hidden): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (to_qk): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (qk_offset_scale): OffsetScale()\n",
      "            (to_out): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer_high): TransformerEncoder_FLASH(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FLASH(\n",
      "            (attn_fn): LaplacianAttnFn()\n",
      "            (rel_pos_bias): T5RelativePositionBias(\n",
      "              (relative_attention_bias): Embedding(32, 1)\n",
      "            )\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (to_hidden): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (to_qk): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (qk_offset_scale): OffsetScale()\n",
      "            (to_out): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attention): Attn_Net_Gated(\n",
      "    (attention_a): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_b): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (fusion_encoder): FusionEncoder()\n",
      "  (attention_V2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (attention_U2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (attention_weights2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Total number of parameters: 17086091\n",
      "Total number of trainable parameters: 17086091\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... 2\n",
      "Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7405, instance_loss: 1.5560, weighted_loss: 0.9852, label: 0, bag_size: 23398\n",
      "batch 39, loss: 0.4631, instance_loss: 1.1555, weighted_loss: 0.6708, label: 1, bag_size: 1703\n",
      "batch 59, loss: 0.7423, instance_loss: 1.1122, weighted_loss: 0.8533, label: 1, bag_size: 11981\n",
      "batch 79, loss: 0.7744, instance_loss: 0.6046, weighted_loss: 0.7235, label: 0, bag_size: 1745\n",
      "batch 99, loss: 0.6867, instance_loss: 0.6260, weighted_loss: 0.6685, label: 1, bag_size: 2140\n",
      "batch 119, loss: 0.5824, instance_loss: 0.8125, weighted_loss: 0.6514, label: 1, bag_size: 3368\n",
      "batch 139, loss: 0.9939, instance_loss: 2.3977, weighted_loss: 1.4150, label: 0, bag_size: 1592\n",
      "batch 159, loss: 0.9806, instance_loss: 0.5052, weighted_loss: 0.8380, label: 0, bag_size: 3557\n",
      "batch 179, loss: 0.5790, instance_loss: 0.6848, weighted_loss: 0.6108, label: 1, bag_size: 2937\n",
      "batch 199, loss: 0.8284, instance_loss: 0.9722, weighted_loss: 0.8715, label: 0, bag_size: 1800\n",
      "batch 219, loss: 0.6383, instance_loss: 1.8208, weighted_loss: 0.9930, label: 1, bag_size: 3968\n",
      "batch 239, loss: 0.4812, instance_loss: 2.0833, weighted_loss: 0.9618, label: 1, bag_size: 3450\n",
      "batch 259, loss: 0.6166, instance_loss: 0.9248, weighted_loss: 0.7091, label: 1, bag_size: 12340\n",
      "batch 279, loss: 0.6827, instance_loss: 0.4580, weighted_loss: 0.6153, label: 1, bag_size: 6164\n",
      "batch 299, loss: 0.6208, instance_loss: 0.3771, weighted_loss: 0.5477, label: 1, bag_size: 11981\n",
      "batch 319, loss: 0.7256, instance_loss: 1.6255, weighted_loss: 0.9955, label: 0, bag_size: 19390\n",
      "batch 339, loss: 0.4394, instance_loss: 1.7856, weighted_loss: 0.8433, label: 0, bag_size: 3375\n",
      "batch 359, loss: 1.0241, instance_loss: 2.4182, weighted_loss: 1.4424, label: 1, bag_size: 3879\n",
      "batch 379, loss: 0.4554, instance_loss: 0.6667, weighted_loss: 0.5188, label: 0, bag_size: 10898\n",
      "batch 399, loss: 0.9176, instance_loss: 2.4127, weighted_loss: 1.3661, label: 1, bag_size: 14681\n",
      "batch 419, loss: 0.6716, instance_loss: 0.6616, weighted_loss: 0.6686, label: 1, bag_size: 4250\n",
      "batch 439, loss: 0.7290, instance_loss: 0.8346, weighted_loss: 0.7607, label: 1, bag_size: 2731\n",
      "batch 459, loss: 0.8355, instance_loss: 0.6603, weighted_loss: 0.7829, label: 0, bag_size: 9252\n",
      "batch 479, loss: 0.8006, instance_loss: 0.3865, weighted_loss: 0.6764, label: 1, bag_size: 9955\n",
      "batch 499, loss: 0.6062, instance_loss: 0.6320, weighted_loss: 0.6140, label: 0, bag_size: 2336\n",
      "batch 519, loss: 0.6699, instance_loss: 0.6873, weighted_loss: 0.6751, label: 1, bag_size: 12626\n",
      "batch 539, loss: 0.5364, instance_loss: 0.5171, weighted_loss: 0.5307, label: 1, bag_size: 13786\n",
      "batch 559, loss: 0.5036, instance_loss: 0.1784, weighted_loss: 0.4060, label: 1, bag_size: 16034\n",
      "batch 579, loss: 0.6561, instance_loss: 0.0560, weighted_loss: 0.4761, label: 1, bag_size: 5454\n",
      "batch 599, loss: 0.4630, instance_loss: 0.0335, weighted_loss: 0.3341, label: 1, bag_size: 11701\n",
      "batch 619, loss: 0.5034, instance_loss: 0.0807, weighted_loss: 0.3766, label: 1, bag_size: 11701\n",
      "batch 639, loss: 0.4788, instance_loss: 0.1447, weighted_loss: 0.3785, label: 1, bag_size: 9446\n",
      "batch 659, loss: 0.8029, instance_loss: 1.6713, weighted_loss: 1.0635, label: 0, bag_size: 1732\n",
      "batch 679, loss: 0.7411, instance_loss: 0.8919, weighted_loss: 0.7863, label: 1, bag_size: 10392\n",
      "batch 699, loss: 0.7681, instance_loss: 0.1434, weighted_loss: 0.5807, label: 0, bag_size: 14319\n",
      "batch 719, loss: 0.8692, instance_loss: 0.3923, weighted_loss: 0.7261, label: 0, bag_size: 2006\n",
      "batch 739, loss: 0.4632, instance_loss: 0.0116, weighted_loss: 0.3277, label: 0, bag_size: 14266\n",
      "batch 759, loss: 0.3598, instance_loss: 1.1612, weighted_loss: 0.6002, label: 0, bag_size: 12524\n",
      "batch 779, loss: 0.9849, instance_loss: 0.2333, weighted_loss: 0.7594, label: 1, bag_size: 8660\n",
      "batch 799, loss: 0.8383, instance_loss: 0.9970, weighted_loss: 0.8859, label: 0, bag_size: 3198\n",
      "batch 819, loss: 0.7881, instance_loss: 0.7772, weighted_loss: 0.7849, label: 0, bag_size: 21138\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.8885670731707317: correct 11658/13120\n",
      "class 1 clustering acc 0.374390243902439: correct 2456/6560\n",
      "Epoch: 0, train_loss: 0.7067, train_clustering_loss:  1.0968, train_error: 0.4841\n",
      "class 0: acc 0.37531486146095716, correct 149/397\n",
      "class 1: acc 0.6477541371158393, correct 274/423\n",
      "\n",
      "Val Set, val_loss: 0.6919, val_error: 0.4727, auc: 0.4383\n",
      "class 0 clustering acc 0.8926136363636363: correct 1571/1760\n",
      "class 1 clustering acc 0.5102272727272728: correct 449/880\n",
      "class 0: acc 0.0, correct 0/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "Validation loss decreased (inf --> 0.691851).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6732, instance_loss: 0.3271, weighted_loss: 0.5694, label: 0, bag_size: 16936\n",
      "batch 39, loss: 0.6818, instance_loss: 0.1191, weighted_loss: 0.5130, label: 1, bag_size: 3224\n",
      "batch 59, loss: 0.9120, instance_loss: 3.9281, weighted_loss: 1.8168, label: 1, bag_size: 1533\n",
      "batch 79, loss: 0.9436, instance_loss: 0.5115, weighted_loss: 0.8140, label: 1, bag_size: 13947\n",
      "batch 99, loss: 0.7178, instance_loss: 0.8051, weighted_loss: 0.7440, label: 0, bag_size: 8755\n",
      "batch 119, loss: 1.0680, instance_loss: 0.0303, weighted_loss: 0.7567, label: 1, bag_size: 14202\n",
      "batch 139, loss: 0.7781, instance_loss: 4.2210, weighted_loss: 1.8110, label: 0, bag_size: 1701\n",
      "batch 159, loss: 0.6975, instance_loss: 0.2402, weighted_loss: 0.5603, label: 0, bag_size: 2195\n",
      "batch 179, loss: 0.6243, instance_loss: 1.0562, weighted_loss: 0.7538, label: 0, bag_size: 1072\n",
      "batch 199, loss: 0.6626, instance_loss: 1.7677, weighted_loss: 0.9941, label: 1, bag_size: 1533\n",
      "batch 219, loss: 0.4967, instance_loss: 0.6938, weighted_loss: 0.5558, label: 0, bag_size: 16936\n",
      "batch 239, loss: 0.6548, instance_loss: 1.0415, weighted_loss: 0.7708, label: 0, bag_size: 1438\n",
      "batch 259, loss: 0.6457, instance_loss: 0.6994, weighted_loss: 0.6618, label: 0, bag_size: 1416\n",
      "batch 279, loss: 0.7415, instance_loss: 0.3028, weighted_loss: 0.6099, label: 0, bag_size: 1508\n",
      "batch 299, loss: 0.7123, instance_loss: 0.3979, weighted_loss: 0.6180, label: 1, bag_size: 7767\n",
      "batch 319, loss: 0.6552, instance_loss: 0.9893, weighted_loss: 0.7554, label: 1, bag_size: 16512\n",
      "batch 339, loss: 0.7302, instance_loss: 0.6694, weighted_loss: 0.7120, label: 1, bag_size: 10394\n",
      "batch 359, loss: 0.6845, instance_loss: 1.2018, weighted_loss: 0.8397, label: 1, bag_size: 9971\n",
      "batch 379, loss: 0.6397, instance_loss: 1.2278, weighted_loss: 0.8161, label: 1, bag_size: 9878\n",
      "batch 399, loss: 0.8752, instance_loss: 1.8206, weighted_loss: 1.1588, label: 1, bag_size: 2356\n",
      "batch 419, loss: 0.7535, instance_loss: 0.7109, weighted_loss: 0.7407, label: 1, bag_size: 14202\n",
      "batch 439, loss: 0.4405, instance_loss: 0.5335, weighted_loss: 0.4684, label: 1, bag_size: 15609\n",
      "batch 459, loss: 0.9363, instance_loss: 0.4148, weighted_loss: 0.7799, label: 0, bag_size: 2534\n",
      "batch 479, loss: 0.6269, instance_loss: 0.4056, weighted_loss: 0.5605, label: 0, bag_size: 2548\n",
      "batch 499, loss: 0.3992, instance_loss: 0.4401, weighted_loss: 0.4115, label: 0, bag_size: 2760\n",
      "batch 519, loss: 0.8452, instance_loss: 1.4813, weighted_loss: 1.0361, label: 1, bag_size: 1051\n",
      "batch 539, loss: 0.5774, instance_loss: 0.4001, weighted_loss: 0.5242, label: 0, bag_size: 705\n",
      "batch 559, loss: 0.3860, instance_loss: 0.5428, weighted_loss: 0.4330, label: 0, bag_size: 1814\n",
      "batch 579, loss: 0.8672, instance_loss: 1.5012, weighted_loss: 1.0574, label: 1, bag_size: 7424\n",
      "batch 599, loss: 0.7649, instance_loss: 0.0501, weighted_loss: 0.5504, label: 0, bag_size: 22498\n",
      "batch 619, loss: 0.9550, instance_loss: 0.3509, weighted_loss: 0.7738, label: 1, bag_size: 6966\n",
      "batch 639, loss: 0.8781, instance_loss: 0.8811, weighted_loss: 0.8790, label: 1, bag_size: 12460\n",
      "batch 659, loss: 0.6206, instance_loss: 0.3802, weighted_loss: 0.5485, label: 1, bag_size: 1822\n",
      "batch 679, loss: 0.6238, instance_loss: 0.6908, weighted_loss: 0.6439, label: 1, bag_size: 2731\n",
      "batch 699, loss: 0.7841, instance_loss: 0.3370, weighted_loss: 0.6500, label: 0, bag_size: 2044\n",
      "batch 719, loss: 0.8074, instance_loss: 1.0593, weighted_loss: 0.8829, label: 1, bag_size: 11195\n",
      "batch 739, loss: 0.8284, instance_loss: 0.1626, weighted_loss: 0.6286, label: 1, bag_size: 12626\n",
      "batch 759, loss: 0.7518, instance_loss: 0.2796, weighted_loss: 0.6102, label: 0, bag_size: 1458\n",
      "batch 779, loss: 0.6789, instance_loss: 3.6405, weighted_loss: 1.5673, label: 1, bag_size: 3211\n",
      "batch 799, loss: 0.5656, instance_loss: 0.0093, weighted_loss: 0.3987, label: 0, bag_size: 27158\n",
      "batch 819, loss: 0.6736, instance_loss: 0.7335, weighted_loss: 0.6916, label: 1, bag_size: 2682\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9294969512195121: correct 12195/13120\n",
      "class 1 clustering acc 0.5533536585365854: correct 3630/6560\n",
      "Epoch: 1, train_loss: 0.7128, train_clustering_loss:  0.7298, train_error: 0.5037\n",
      "class 0: acc 0.6088992974238876, correct 260/427\n",
      "class 1: acc 0.37404580152671757, correct 147/393\n",
      "\n",
      "Val Set, val_loss: 0.6944, val_error: 0.5273, auc: 0.6185\n",
      "class 0 clustering acc 0.9835227272727273: correct 1731/1760\n",
      "class 1 clustering acc 0.40795454545454546: correct 359/880\n",
      "class 0: acc 1.0, correct 52/52\n",
      "class 1: acc 0.0, correct 0/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.9695, instance_loss: 0.7497, weighted_loss: 0.9036, label: 0, bag_size: 6624\n",
      "batch 39, loss: 0.4506, instance_loss: 0.6462, weighted_loss: 0.5093, label: 1, bag_size: 699\n",
      "batch 59, loss: 0.9431, instance_loss: 1.2968, weighted_loss: 1.0492, label: 1, bag_size: 1339\n",
      "batch 79, loss: 0.4916, instance_loss: 0.1548, weighted_loss: 0.3906, label: 0, bag_size: 2382\n",
      "batch 99, loss: 0.5135, instance_loss: 0.1870, weighted_loss: 0.4155, label: 0, bag_size: 13892\n",
      "batch 119, loss: 0.5862, instance_loss: 0.1263, weighted_loss: 0.4482, label: 0, bag_size: 3444\n",
      "batch 139, loss: 1.0148, instance_loss: 2.5202, weighted_loss: 1.4665, label: 1, bag_size: 13367\n",
      "batch 159, loss: 0.4101, instance_loss: 0.5177, weighted_loss: 0.4424, label: 0, bag_size: 11607\n",
      "batch 179, loss: 0.8352, instance_loss: 0.7981, weighted_loss: 0.8241, label: 1, bag_size: 5292\n",
      "batch 199, loss: 1.1384, instance_loss: 1.1132, weighted_loss: 1.1309, label: 1, bag_size: 21701\n",
      "batch 219, loss: 0.3534, instance_loss: 0.2130, weighted_loss: 0.3112, label: 0, bag_size: 16690\n",
      "batch 239, loss: 0.4591, instance_loss: 0.1520, weighted_loss: 0.3670, label: 0, bag_size: 1349\n",
      "batch 259, loss: 0.4254, instance_loss: 0.0649, weighted_loss: 0.3173, label: 0, bag_size: 1052\n",
      "batch 279, loss: 0.4517, instance_loss: 0.1309, weighted_loss: 0.3554, label: 0, bag_size: 9470\n",
      "batch 299, loss: 0.5448, instance_loss: 0.1121, weighted_loss: 0.4150, label: 0, bag_size: 10365\n",
      "batch 319, loss: 0.6630, instance_loss: 1.4862, weighted_loss: 0.9100, label: 1, bag_size: 1683\n",
      "batch 339, loss: 0.8053, instance_loss: 0.2048, weighted_loss: 0.6252, label: 1, bag_size: 9971\n",
      "batch 359, loss: 0.4625, instance_loss: 0.0834, weighted_loss: 0.3487, label: 1, bag_size: 4880\n",
      "batch 379, loss: 0.4769, instance_loss: 0.0119, weighted_loss: 0.3374, label: 1, bag_size: 9230\n",
      "batch 399, loss: 1.0291, instance_loss: 0.0037, weighted_loss: 0.7215, label: 0, bag_size: 11654\n",
      "batch 419, loss: 0.4837, instance_loss: 0.3337, weighted_loss: 0.4387, label: 1, bag_size: 18161\n",
      "batch 439, loss: 0.7139, instance_loss: 0.2952, weighted_loss: 0.5883, label: 0, bag_size: 5297\n",
      "batch 459, loss: 0.8513, instance_loss: 0.2590, weighted_loss: 0.6736, label: 1, bag_size: 3856\n",
      "batch 479, loss: 0.7063, instance_loss: 0.5003, weighted_loss: 0.6445, label: 0, bag_size: 1127\n",
      "batch 499, loss: 0.7414, instance_loss: 0.0834, weighted_loss: 0.5440, label: 1, bag_size: 10622\n",
      "batch 519, loss: 0.7719, instance_loss: 0.0290, weighted_loss: 0.5490, label: 0, bag_size: 4523\n",
      "batch 539, loss: 0.5244, instance_loss: 0.4764, weighted_loss: 0.5100, label: 0, bag_size: 9171\n",
      "batch 559, loss: 0.6438, instance_loss: 0.0156, weighted_loss: 0.4553, label: 0, bag_size: 23791\n",
      "batch 579, loss: 0.6623, instance_loss: 0.7596, weighted_loss: 0.6915, label: 1, bag_size: 4039\n",
      "batch 599, loss: 0.4793, instance_loss: 1.5028, weighted_loss: 0.7864, label: 1, bag_size: 1819\n",
      "batch 619, loss: 0.6055, instance_loss: 0.0257, weighted_loss: 0.4316, label: 1, bag_size: 10028\n",
      "batch 639, loss: 0.7576, instance_loss: 0.3157, weighted_loss: 0.6250, label: 0, bag_size: 6624\n",
      "batch 659, loss: 0.6933, instance_loss: 0.0000, weighted_loss: 0.4853, label: 1, bag_size: 22264\n",
      "batch 679, loss: 0.6225, instance_loss: 0.0681, weighted_loss: 0.4562, label: 0, bag_size: 1684\n",
      "batch 699, loss: 0.7182, instance_loss: 0.0179, weighted_loss: 0.5081, label: 1, bag_size: 4239\n",
      "batch 719, loss: 0.4024, instance_loss: 0.0218, weighted_loss: 0.2882, label: 1, bag_size: 12714\n",
      "batch 739, loss: 0.3892, instance_loss: 0.0445, weighted_loss: 0.2858, label: 1, bag_size: 8522\n",
      "batch 759, loss: 0.5892, instance_loss: 0.2366, weighted_loss: 0.4834, label: 1, bag_size: 7873\n",
      "batch 779, loss: 0.6814, instance_loss: 0.0135, weighted_loss: 0.4810, label: 0, bag_size: 16782\n",
      "batch 799, loss: 0.5042, instance_loss: 0.1719, weighted_loss: 0.4045, label: 1, bag_size: 11729\n",
      "batch 819, loss: 0.5604, instance_loss: 0.0174, weighted_loss: 0.3975, label: 1, bag_size: 15093\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.953734756097561: correct 12513/13120\n",
      "class 1 clustering acc 0.7006097560975609: correct 4596/6560\n",
      "Epoch: 2, train_loss: 0.6990, train_clustering_loss:  0.5049, train_error: 0.4549\n",
      "class 0: acc 0.5966183574879227, correct 247/414\n",
      "class 1: acc 0.49261083743842365, correct 200/406\n",
      "\n",
      "Val Set, val_loss: 0.7065, val_error: 0.4727, auc: 0.8924\n",
      "class 0 clustering acc 0.9568181818181818: correct 1684/1760\n",
      "class 1 clustering acc 0.7227272727272728: correct 636/880\n",
      "class 0: acc 0.0, correct 0/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4954, instance_loss: 0.1968, weighted_loss: 0.4058, label: 1, bag_size: 18794\n",
      "batch 39, loss: 0.8062, instance_loss: 0.0006, weighted_loss: 0.5646, label: 0, bag_size: 21319\n",
      "batch 59, loss: 1.1245, instance_loss: 0.0480, weighted_loss: 0.8016, label: 1, bag_size: 4880\n",
      "batch 79, loss: 0.4631, instance_loss: 0.5585, weighted_loss: 0.4917, label: 0, bag_size: 1701\n",
      "batch 99, loss: 0.5452, instance_loss: 0.0735, weighted_loss: 0.4037, label: 0, bag_size: 13339\n",
      "batch 119, loss: 0.4998, instance_loss: 0.3747, weighted_loss: 0.4623, label: 0, bag_size: 3970\n",
      "batch 139, loss: 0.4542, instance_loss: 0.1350, weighted_loss: 0.3585, label: 0, bag_size: 65728\n",
      "batch 159, loss: 0.6629, instance_loss: 0.1368, weighted_loss: 0.5051, label: 0, bag_size: 18215\n",
      "batch 179, loss: 0.7293, instance_loss: 0.4120, weighted_loss: 0.6341, label: 0, bag_size: 1437\n",
      "batch 199, loss: 0.7700, instance_loss: 0.2776, weighted_loss: 0.6223, label: 0, bag_size: 2244\n",
      "batch 219, loss: 0.4499, instance_loss: 0.0032, weighted_loss: 0.3159, label: 1, bag_size: 7351\n",
      "batch 239, loss: 0.6443, instance_loss: 0.1505, weighted_loss: 0.4961, label: 1, bag_size: 9955\n",
      "batch 259, loss: 0.8695, instance_loss: 0.0187, weighted_loss: 0.6142, label: 1, bag_size: 13051\n",
      "batch 279, loss: 0.5677, instance_loss: 0.5795, weighted_loss: 0.5712, label: 1, bag_size: 1123\n",
      "batch 299, loss: 0.4532, instance_loss: 0.0123, weighted_loss: 0.3209, label: 1, bag_size: 11389\n",
      "batch 319, loss: 0.7212, instance_loss: 0.0326, weighted_loss: 0.5146, label: 0, bag_size: 11512\n",
      "batch 339, loss: 0.6203, instance_loss: 0.6103, weighted_loss: 0.6173, label: 0, bag_size: 13880\n",
      "batch 359, loss: 0.7292, instance_loss: 0.5131, weighted_loss: 0.6643, label: 0, bag_size: 1438\n",
      "batch 379, loss: 0.5287, instance_loss: 1.2346, weighted_loss: 0.7405, label: 0, bag_size: 17279\n",
      "batch 399, loss: 0.6000, instance_loss: 0.0322, weighted_loss: 0.4297, label: 0, bag_size: 22681\n",
      "batch 419, loss: 0.8071, instance_loss: 0.1514, weighted_loss: 0.6103, label: 1, bag_size: 11220\n",
      "batch 439, loss: 0.5104, instance_loss: 0.3011, weighted_loss: 0.4476, label: 0, bag_size: 3893\n",
      "batch 459, loss: 0.5160, instance_loss: 0.0860, weighted_loss: 0.3870, label: 0, bag_size: 11259\n",
      "batch 479, loss: 1.0754, instance_loss: 0.3450, weighted_loss: 0.8563, label: 1, bag_size: 14030\n",
      "batch 499, loss: 0.3669, instance_loss: 0.0018, weighted_loss: 0.2574, label: 0, bag_size: 21404\n",
      "batch 519, loss: 0.4881, instance_loss: 0.0203, weighted_loss: 0.3477, label: 0, bag_size: 3908\n",
      "batch 539, loss: 0.6346, instance_loss: 0.0331, weighted_loss: 0.4542, label: 1, bag_size: 15008\n",
      "batch 559, loss: 0.5292, instance_loss: 0.1494, weighted_loss: 0.4152, label: 1, bag_size: 20537\n",
      "batch 579, loss: 0.6415, instance_loss: 0.0982, weighted_loss: 0.4785, label: 1, bag_size: 12180\n",
      "batch 599, loss: 0.5152, instance_loss: 0.1167, weighted_loss: 0.3956, label: 1, bag_size: 12626\n",
      "batch 619, loss: 0.4505, instance_loss: 2.0026, weighted_loss: 0.9161, label: 1, bag_size: 2731\n",
      "batch 639, loss: 0.8851, instance_loss: 0.2354, weighted_loss: 0.6902, label: 0, bag_size: 10410\n",
      "batch 659, loss: 0.6904, instance_loss: 0.1860, weighted_loss: 0.5391, label: 1, bag_size: 3980\n",
      "batch 679, loss: 0.6520, instance_loss: 0.0115, weighted_loss: 0.4599, label: 1, bag_size: 13732\n",
      "batch 699, loss: 0.7111, instance_loss: 0.1511, weighted_loss: 0.5431, label: 0, bag_size: 1483\n",
      "batch 719, loss: 0.7646, instance_loss: 0.1005, weighted_loss: 0.5653, label: 0, bag_size: 22870\n",
      "batch 739, loss: 0.4169, instance_loss: 1.0089, weighted_loss: 0.5945, label: 0, bag_size: 8372\n",
      "batch 759, loss: 0.5245, instance_loss: 0.3325, weighted_loss: 0.4669, label: 0, bag_size: 7612\n",
      "batch 779, loss: 0.9531, instance_loss: 0.9424, weighted_loss: 0.9499, label: 1, bag_size: 1437\n",
      "batch 799, loss: 0.6066, instance_loss: 0.0005, weighted_loss: 0.4248, label: 1, bag_size: 5833\n",
      "batch 819, loss: 0.7569, instance_loss: 0.5699, weighted_loss: 0.7008, label: 0, bag_size: 1772\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.952515243902439: correct 12497/13120\n",
      "class 1 clustering acc 0.7391768292682926: correct 4849/6560\n",
      "Epoch: 3, train_loss: 0.6991, train_clustering_loss:  0.4494, train_error: 0.4683\n",
      "class 0: acc 0.5882352941176471, correct 250/425\n",
      "class 1: acc 0.4708860759493671, correct 186/395\n",
      "\n",
      "Val Set, val_loss: 0.6893, val_error: 0.4727, auc: 0.9436\n",
      "class 0 clustering acc 0.9517045454545454: correct 1675/1760\n",
      "class 1 clustering acc 0.6420454545454546: correct 565/880\n",
      "class 0: acc 0.0, correct 0/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "Validation loss decreased (0.691851 --> 0.689336).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7266, instance_loss: 0.2829, weighted_loss: 0.5935, label: 1, bag_size: 11160\n",
      "batch 39, loss: 0.4533, instance_loss: 0.0609, weighted_loss: 0.3356, label: 0, bag_size: 2382\n",
      "batch 59, loss: 0.4669, instance_loss: 0.0093, weighted_loss: 0.3296, label: 0, bag_size: 11199\n",
      "batch 79, loss: 1.0331, instance_loss: 0.0248, weighted_loss: 0.7306, label: 1, bag_size: 13365\n",
      "batch 99, loss: 0.9946, instance_loss: 0.0046, weighted_loss: 0.6976, label: 1, bag_size: 13051\n",
      "batch 119, loss: 0.5565, instance_loss: 0.0097, weighted_loss: 0.3924, label: 0, bag_size: 24911\n",
      "batch 139, loss: 0.5983, instance_loss: 0.0048, weighted_loss: 0.4202, label: 0, bag_size: 12593\n",
      "batch 159, loss: 0.6733, instance_loss: 0.0001, weighted_loss: 0.4713, label: 0, bag_size: 26271\n",
      "batch 179, loss: 0.6149, instance_loss: 0.1511, weighted_loss: 0.4758, label: 0, bag_size: 10415\n",
      "batch 199, loss: 0.6455, instance_loss: 0.2306, weighted_loss: 0.5210, label: 1, bag_size: 15125\n",
      "batch 219, loss: 0.8538, instance_loss: 1.1452, weighted_loss: 0.9412, label: 0, bag_size: 7239\n",
      "batch 239, loss: 0.8688, instance_loss: 0.0426, weighted_loss: 0.6210, label: 0, bag_size: 7191\n",
      "batch 259, loss: 0.6357, instance_loss: 3.8256, weighted_loss: 1.5927, label: 0, bag_size: 3810\n",
      "batch 279, loss: 0.4553, instance_loss: 0.3169, weighted_loss: 0.4138, label: 0, bag_size: 3228\n",
      "batch 299, loss: 0.6915, instance_loss: 0.0979, weighted_loss: 0.5134, label: 0, bag_size: 11654\n",
      "batch 319, loss: 0.5702, instance_loss: 0.1840, weighted_loss: 0.4543, label: 0, bag_size: 2044\n",
      "batch 339, loss: 0.6749, instance_loss: 0.5283, weighted_loss: 0.6309, label: 1, bag_size: 1764\n",
      "batch 359, loss: 0.9562, instance_loss: 0.0831, weighted_loss: 0.6942, label: 0, bag_size: 26208\n",
      "batch 379, loss: 0.5793, instance_loss: 1.1008, weighted_loss: 0.7357, label: 1, bag_size: 3856\n",
      "batch 399, loss: 0.7285, instance_loss: 0.4938, weighted_loss: 0.6581, label: 1, bag_size: 15665\n",
      "batch 419, loss: 1.0001, instance_loss: 0.0723, weighted_loss: 0.7218, label: 1, bag_size: 5494\n",
      "batch 439, loss: 0.6531, instance_loss: 0.0511, weighted_loss: 0.4725, label: 1, bag_size: 9561\n",
      "batch 459, loss: 0.8027, instance_loss: 0.5047, weighted_loss: 0.7133, label: 0, bag_size: 14249\n",
      "batch 479, loss: 0.9529, instance_loss: 1.7055, weighted_loss: 1.1787, label: 1, bag_size: 1764\n",
      "batch 499, loss: 0.5837, instance_loss: 0.0000, weighted_loss: 0.4086, label: 1, bag_size: 19932\n",
      "batch 519, loss: 0.9138, instance_loss: 0.3455, weighted_loss: 0.7433, label: 0, bag_size: 2091\n",
      "batch 539, loss: 0.7417, instance_loss: 0.0018, weighted_loss: 0.5197, label: 1, bag_size: 11884\n",
      "batch 559, loss: 0.8292, instance_loss: 0.3523, weighted_loss: 0.6861, label: 1, bag_size: 1459\n",
      "batch 579, loss: 0.5292, instance_loss: 0.3529, weighted_loss: 0.4763, label: 0, bag_size: 763\n",
      "batch 599, loss: 0.8991, instance_loss: 0.2327, weighted_loss: 0.6992, label: 1, bag_size: 2137\n",
      "batch 619, loss: 0.7838, instance_loss: 1.8965, weighted_loss: 1.1176, label: 1, bag_size: 684\n",
      "batch 639, loss: 0.7038, instance_loss: 0.0292, weighted_loss: 0.5014, label: 0, bag_size: 15636\n",
      "batch 659, loss: 0.7607, instance_loss: 0.4837, weighted_loss: 0.6776, label: 0, bag_size: 1760\n",
      "batch 679, loss: 0.6915, instance_loss: 0.0052, weighted_loss: 0.4856, label: 0, bag_size: 18154\n",
      "batch 699, loss: 0.8403, instance_loss: 0.0010, weighted_loss: 0.5885, label: 1, bag_size: 25695\n",
      "batch 719, loss: 0.5948, instance_loss: 0.0250, weighted_loss: 0.4239, label: 1, bag_size: 10105\n",
      "batch 739, loss: 0.6790, instance_loss: 0.3152, weighted_loss: 0.5699, label: 0, bag_size: 26208\n",
      "batch 759, loss: 0.7021, instance_loss: 0.5161, weighted_loss: 0.6463, label: 0, bag_size: 8755\n",
      "batch 779, loss: 0.7366, instance_loss: 0.0916, weighted_loss: 0.5431, label: 1, bag_size: 8216\n",
      "batch 799, loss: 0.7631, instance_loss: 0.3130, weighted_loss: 0.6281, label: 1, bag_size: 2695\n",
      "batch 819, loss: 0.5049, instance_loss: 0.0277, weighted_loss: 0.3617, label: 0, bag_size: 11199\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9583079268292682: correct 12573/13120\n",
      "class 1 clustering acc 0.7445121951219512: correct 4884/6560\n",
      "Epoch: 4, train_loss: 0.7006, train_clustering_loss:  0.4354, train_error: 0.4841\n",
      "class 0: acc 0.641860465116279, correct 276/430\n",
      "class 1: acc 0.3769230769230769, correct 147/390\n",
      "\n",
      "Val Set, val_loss: 0.7030, val_error: 0.5273, auc: 0.9453\n",
      "class 0 clustering acc 0.9221590909090909: correct 1623/1760\n",
      "class 1 clustering acc 0.7909090909090909: correct 696/880\n",
      "class 0: acc 1.0, correct 52/52\n",
      "class 1: acc 0.0, correct 0/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.8151, instance_loss: 0.1224, weighted_loss: 0.6073, label: 1, bag_size: 1838\n",
      "batch 39, loss: 0.7130, instance_loss: 2.5719, weighted_loss: 1.2707, label: 1, bag_size: 684\n",
      "batch 59, loss: 0.6943, instance_loss: 1.0797, weighted_loss: 0.8099, label: 0, bag_size: 16690\n",
      "batch 79, loss: 0.6836, instance_loss: 0.1910, weighted_loss: 0.5358, label: 1, bag_size: 3224\n",
      "batch 99, loss: 0.6433, instance_loss: 0.6766, weighted_loss: 0.6533, label: 1, bag_size: 4929\n",
      "batch 119, loss: 0.8316, instance_loss: 0.0786, weighted_loss: 0.6057, label: 0, bag_size: 12510\n",
      "batch 139, loss: 0.5141, instance_loss: 0.0059, weighted_loss: 0.3617, label: 1, bag_size: 3082\n",
      "batch 159, loss: 0.7977, instance_loss: 3.1463, weighted_loss: 1.5023, label: 1, bag_size: 12494\n",
      "batch 179, loss: 0.6586, instance_loss: 0.0003, weighted_loss: 0.4611, label: 1, bag_size: 11389\n",
      "batch 199, loss: 0.6327, instance_loss: 0.0042, weighted_loss: 0.4441, label: 1, bag_size: 5921\n",
      "batch 219, loss: 0.8675, instance_loss: 1.2360, weighted_loss: 0.9781, label: 0, bag_size: 1142\n",
      "batch 239, loss: 0.5959, instance_loss: 0.0842, weighted_loss: 0.4424, label: 1, bag_size: 12575\n",
      "batch 259, loss: 0.5081, instance_loss: 0.7699, weighted_loss: 0.5866, label: 1, bag_size: 1822\n",
      "batch 279, loss: 0.6437, instance_loss: 0.0336, weighted_loss: 0.4607, label: 0, bag_size: 12793\n",
      "batch 299, loss: 0.5786, instance_loss: 1.3436, weighted_loss: 0.8081, label: 0, bag_size: 7835\n",
      "batch 319, loss: 0.7581, instance_loss: 0.0568, weighted_loss: 0.5477, label: 0, bag_size: 9471\n",
      "batch 339, loss: 0.6348, instance_loss: 0.5829, weighted_loss: 0.6192, label: 0, bag_size: 8420\n",
      "batch 359, loss: 0.6090, instance_loss: 0.2306, weighted_loss: 0.4955, label: 1, bag_size: 10501\n",
      "batch 379, loss: 0.9900, instance_loss: 0.1381, weighted_loss: 0.7344, label: 0, bag_size: 5551\n",
      "batch 399, loss: 0.5817, instance_loss: 0.0045, weighted_loss: 0.4085, label: 0, bag_size: 14266\n",
      "batch 419, loss: 0.6919, instance_loss: 0.0087, weighted_loss: 0.4870, label: 1, bag_size: 1838\n",
      "batch 439, loss: 0.5091, instance_loss: 0.5072, weighted_loss: 0.5085, label: 1, bag_size: 1244\n",
      "batch 459, loss: 0.9103, instance_loss: 0.4115, weighted_loss: 0.7606, label: 0, bag_size: 9415\n",
      "batch 479, loss: 0.6814, instance_loss: 0.0070, weighted_loss: 0.4791, label: 0, bag_size: 22800\n",
      "batch 499, loss: 0.6433, instance_loss: 0.0003, weighted_loss: 0.4504, label: 1, bag_size: 10725\n",
      "batch 519, loss: 0.4683, instance_loss: 0.6583, weighted_loss: 0.5253, label: 1, bag_size: 3856\n",
      "batch 539, loss: 0.9827, instance_loss: 0.5598, weighted_loss: 0.8558, label: 0, bag_size: 2290\n",
      "batch 559, loss: 0.5381, instance_loss: 0.6765, weighted_loss: 0.5796, label: 1, bag_size: 1339\n",
      "batch 579, loss: 0.8054, instance_loss: 0.3766, weighted_loss: 0.6768, label: 0, bag_size: 9597\n",
      "batch 599, loss: 0.7007, instance_loss: 0.1166, weighted_loss: 0.5255, label: 0, bag_size: 3444\n",
      "batch 619, loss: 0.5643, instance_loss: 2.3449, weighted_loss: 1.0985, label: 1, bag_size: 13367\n",
      "batch 639, loss: 0.4895, instance_loss: 0.4735, weighted_loss: 0.4847, label: 1, bag_size: 3368\n",
      "batch 659, loss: 0.4657, instance_loss: 0.0225, weighted_loss: 0.3327, label: 1, bag_size: 9321\n",
      "batch 679, loss: 0.8400, instance_loss: 0.6249, weighted_loss: 0.7755, label: 0, bag_size: 1789\n",
      "batch 699, loss: 0.6265, instance_loss: 0.3773, weighted_loss: 0.5517, label: 0, bag_size: 8661\n",
      "batch 719, loss: 0.5127, instance_loss: 0.3048, weighted_loss: 0.4503, label: 0, bag_size: 1234\n",
      "batch 739, loss: 0.8899, instance_loss: 0.1309, weighted_loss: 0.6622, label: 1, bag_size: 13692\n",
      "batch 759, loss: 0.5277, instance_loss: 0.2944, weighted_loss: 0.4577, label: 0, bag_size: 2918\n",
      "batch 779, loss: 0.5379, instance_loss: 1.9467, weighted_loss: 0.9605, label: 0, bag_size: 1437\n",
      "batch 799, loss: 0.4598, instance_loss: 0.1741, weighted_loss: 0.3741, label: 0, bag_size: 1052\n",
      "batch 819, loss: 0.7877, instance_loss: 0.0518, weighted_loss: 0.5669, label: 1, bag_size: 699\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9594512195121951: correct 12588/13120\n",
      "class 1 clustering acc 0.7496951219512196: correct 4918/6560\n",
      "Epoch: 5, train_loss: 0.6810, train_clustering_loss:  0.4202, train_error: 0.4476\n",
      "class 0: acc 0.44529262086513993, correct 175/393\n",
      "class 1: acc 0.6510538641686182, correct 278/427\n",
      "\n",
      "Val Set, val_loss: 0.6613, val_error: 0.5273, auc: 0.9450\n",
      "class 0 clustering acc 0.9318181818181818: correct 1640/1760\n",
      "class 1 clustering acc 0.5386363636363637: correct 474/880\n",
      "class 0: acc 1.0, correct 52/52\n",
      "class 1: acc 0.0, correct 0/58\n",
      "Validation loss decreased (0.689336 --> 0.661322).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3999, instance_loss: 0.0295, weighted_loss: 0.2888, label: 0, bag_size: 7381\n",
      "batch 39, loss: 0.4930, instance_loss: 1.0892, weighted_loss: 0.6718, label: 0, bag_size: 2091\n",
      "batch 59, loss: 0.5452, instance_loss: 0.0033, weighted_loss: 0.3826, label: 1, bag_size: 3409\n",
      "batch 79, loss: 0.3472, instance_loss: 0.1188, weighted_loss: 0.2787, label: 1, bag_size: 6734\n",
      "batch 99, loss: 0.5271, instance_loss: 0.0624, weighted_loss: 0.3877, label: 1, bag_size: 6205\n",
      "batch 119, loss: 0.6589, instance_loss: 0.0153, weighted_loss: 0.4658, label: 1, bag_size: 10920\n",
      "batch 139, loss: 0.5493, instance_loss: 0.0104, weighted_loss: 0.3876, label: 1, bag_size: 15464\n",
      "batch 159, loss: 0.6511, instance_loss: 0.0132, weighted_loss: 0.4597, label: 0, bag_size: 10898\n",
      "batch 179, loss: 0.6481, instance_loss: 0.0003, weighted_loss: 0.4538, label: 0, bag_size: 19470\n",
      "batch 199, loss: 0.5880, instance_loss: 0.0832, weighted_loss: 0.4366, label: 0, bag_size: 7191\n",
      "batch 219, loss: 0.8424, instance_loss: 0.9539, weighted_loss: 0.8758, label: 0, bag_size: 2360\n",
      "batch 239, loss: 0.7295, instance_loss: 0.0002, weighted_loss: 0.5107, label: 1, bag_size: 9732\n",
      "batch 259, loss: 0.4063, instance_loss: 0.0056, weighted_loss: 0.2861, label: 0, bag_size: 22800\n",
      "batch 279, loss: 0.5051, instance_loss: 0.4701, weighted_loss: 0.4946, label: 0, bag_size: 9542\n",
      "batch 299, loss: 0.4182, instance_loss: 0.4002, weighted_loss: 0.4128, label: 1, bag_size: 11386\n",
      "batch 319, loss: 0.4936, instance_loss: 0.0066, weighted_loss: 0.3475, label: 0, bag_size: 10791\n",
      "batch 339, loss: 0.3143, instance_loss: 0.0290, weighted_loss: 0.2287, label: 0, bag_size: 1891\n",
      "batch 359, loss: 0.6915, instance_loss: 0.0484, weighted_loss: 0.4985, label: 1, bag_size: 12611\n",
      "batch 379, loss: 0.6105, instance_loss: 0.0034, weighted_loss: 0.4284, label: 1, bag_size: 3368\n",
      "batch 399, loss: 0.7172, instance_loss: 0.0032, weighted_loss: 0.5030, label: 1, bag_size: 5612\n",
      "batch 419, loss: 0.4969, instance_loss: 0.8632, weighted_loss: 0.6068, label: 0, bag_size: 6356\n",
      "batch 439, loss: 0.6152, instance_loss: 0.0087, weighted_loss: 0.4333, label: 1, bag_size: 7110\n",
      "batch 459, loss: 0.6849, instance_loss: 0.0000, weighted_loss: 0.4794, label: 0, bag_size: 17155\n",
      "batch 479, loss: 0.6925, instance_loss: 0.0000, weighted_loss: 0.4847, label: 0, bag_size: 17633\n",
      "batch 499, loss: 0.4025, instance_loss: 0.0174, weighted_loss: 0.2870, label: 1, bag_size: 7513\n",
      "batch 519, loss: 0.9335, instance_loss: 0.1511, weighted_loss: 0.6988, label: 1, bag_size: 9147\n",
      "batch 539, loss: 0.5527, instance_loss: 1.1621, weighted_loss: 0.7355, label: 0, bag_size: 2219\n",
      "batch 559, loss: 1.1072, instance_loss: 0.5680, weighted_loss: 0.9454, label: 1, bag_size: 1533\n",
      "batch 579, loss: 0.6441, instance_loss: 0.0419, weighted_loss: 0.4634, label: 1, bag_size: 2904\n",
      "batch 599, loss: 0.6207, instance_loss: 0.0039, weighted_loss: 0.4357, label: 1, bag_size: 15665\n",
      "batch 619, loss: 0.6582, instance_loss: 0.0366, weighted_loss: 0.4717, label: 0, bag_size: 9234\n",
      "batch 639, loss: 0.5863, instance_loss: 0.0343, weighted_loss: 0.4207, label: 0, bag_size: 23618\n",
      "batch 659, loss: 0.7773, instance_loss: 0.0000, weighted_loss: 0.5441, label: 1, bag_size: 14618\n",
      "batch 679, loss: 0.6860, instance_loss: 2.1273, weighted_loss: 1.1184, label: 0, bag_size: 2070\n",
      "batch 699, loss: 0.7645, instance_loss: 0.0457, weighted_loss: 0.5489, label: 0, bag_size: 16936\n",
      "batch 719, loss: 0.4998, instance_loss: 0.3157, weighted_loss: 0.4446, label: 1, bag_size: 5292\n",
      "batch 739, loss: 0.3362, instance_loss: 0.0112, weighted_loss: 0.2387, label: 0, bag_size: 11151\n",
      "batch 759, loss: 0.7206, instance_loss: 0.0022, weighted_loss: 0.5051, label: 1, bag_size: 10501\n",
      "batch 779, loss: 0.5136, instance_loss: 0.3604, weighted_loss: 0.4676, label: 1, bag_size: 1781\n",
      "batch 799, loss: 0.5912, instance_loss: 0.0374, weighted_loss: 0.4250, label: 1, bag_size: 11875\n",
      "batch 819, loss: 0.4538, instance_loss: 0.1672, weighted_loss: 0.3678, label: 0, bag_size: 4497\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9618140243902439: correct 12619/13120\n",
      "class 1 clustering acc 0.7557926829268292: correct 4958/6560\n",
      "Epoch: 6, train_loss: 0.6415, train_clustering_loss:  0.4211, train_error: 0.3561\n",
      "class 0: acc 0.7162790697674418, correct 308/430\n",
      "class 1: acc 0.5641025641025641, correct 220/390\n",
      "\n",
      "Val Set, val_loss: 0.5859, val_error: 0.1818, auc: 0.9446\n",
      "class 0 clustering acc 0.9721590909090909: correct 1711/1760\n",
      "class 1 clustering acc 0.42045454545454547: correct 370/880\n",
      "class 0: acc 0.9807692307692307, correct 51/52\n",
      "class 1: acc 0.6724137931034483, correct 39/58\n",
      "Validation loss decreased (0.661322 --> 0.585935).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4096, instance_loss: 0.0092, weighted_loss: 0.2895, label: 1, bag_size: 11122\n",
      "batch 39, loss: 0.6420, instance_loss: 0.2226, weighted_loss: 0.5162, label: 0, bag_size: 2036\n",
      "batch 59, loss: 0.5087, instance_loss: 0.3230, weighted_loss: 0.4530, label: 0, bag_size: 13332\n",
      "batch 79, loss: 0.4422, instance_loss: 0.8962, weighted_loss: 0.5784, label: 1, bag_size: 7389\n",
      "batch 99, loss: 0.7347, instance_loss: 2.6137, weighted_loss: 1.2984, label: 1, bag_size: 21252\n",
      "batch 119, loss: 0.2418, instance_loss: 0.0028, weighted_loss: 0.1701, label: 0, bag_size: 23037\n",
      "batch 139, loss: 0.8436, instance_loss: 0.5962, weighted_loss: 0.7694, label: 0, bag_size: 25814\n",
      "batch 159, loss: 0.5781, instance_loss: 0.3930, weighted_loss: 0.5226, label: 0, bag_size: 9888\n",
      "batch 179, loss: 0.4006, instance_loss: 0.6025, weighted_loss: 0.4612, label: 0, bag_size: 8661\n",
      "batch 199, loss: 0.2830, instance_loss: 0.0163, weighted_loss: 0.2030, label: 1, bag_size: 11363\n",
      "batch 219, loss: 0.2431, instance_loss: 0.0371, weighted_loss: 0.1813, label: 1, bag_size: 4821\n",
      "batch 239, loss: 0.4909, instance_loss: 0.0031, weighted_loss: 0.3445, label: 1, bag_size: 5833\n",
      "batch 259, loss: 0.5561, instance_loss: 0.3478, weighted_loss: 0.4936, label: 1, bag_size: 1512\n",
      "batch 279, loss: 0.6909, instance_loss: 0.1590, weighted_loss: 0.5314, label: 1, bag_size: 21252\n",
      "batch 299, loss: 0.7446, instance_loss: 0.2508, weighted_loss: 0.5964, label: 0, bag_size: 2654\n",
      "batch 319, loss: 0.5951, instance_loss: 0.4775, weighted_loss: 0.5598, label: 1, bag_size: 10072\n",
      "batch 339, loss: 0.2878, instance_loss: 0.2291, weighted_loss: 0.2702, label: 1, bag_size: 4039\n",
      "batch 359, loss: 0.3364, instance_loss: 2.2239, weighted_loss: 0.9026, label: 1, bag_size: 11600\n",
      "batch 379, loss: 0.4430, instance_loss: 0.7171, weighted_loss: 0.5252, label: 0, bag_size: 13591\n",
      "batch 399, loss: 0.6446, instance_loss: 0.0013, weighted_loss: 0.4516, label: 1, bag_size: 15213\n",
      "batch 419, loss: 0.3818, instance_loss: 0.0884, weighted_loss: 0.2938, label: 0, bag_size: 14266\n",
      "batch 439, loss: 0.9802, instance_loss: 0.0473, weighted_loss: 0.7004, label: 0, bag_size: 2652\n",
      "batch 459, loss: 0.4998, instance_loss: 0.0034, weighted_loss: 0.3509, label: 1, bag_size: 20161\n",
      "batch 479, loss: 0.3733, instance_loss: 0.0024, weighted_loss: 0.2620, label: 0, bag_size: 19466\n",
      "batch 499, loss: 0.2682, instance_loss: 0.3090, weighted_loss: 0.2804, label: 0, bag_size: 8372\n",
      "batch 519, loss: 0.3816, instance_loss: 0.0848, weighted_loss: 0.2925, label: 0, bag_size: 10898\n",
      "batch 539, loss: 0.6025, instance_loss: 0.4483, weighted_loss: 0.5562, label: 0, bag_size: 2815\n",
      "batch 559, loss: 0.6206, instance_loss: 0.0027, weighted_loss: 0.4352, label: 1, bag_size: 8602\n",
      "batch 579, loss: 0.7239, instance_loss: 1.6377, weighted_loss: 0.9981, label: 1, bag_size: 1339\n",
      "batch 599, loss: 0.7226, instance_loss: 0.3022, weighted_loss: 0.5965, label: 1, bag_size: 2695\n",
      "batch 619, loss: 0.6244, instance_loss: 0.0380, weighted_loss: 0.4485, label: 1, bag_size: 9322\n",
      "batch 639, loss: 0.7226, instance_loss: 0.5312, weighted_loss: 0.6652, label: 0, bag_size: 12840\n",
      "batch 659, loss: 0.2585, instance_loss: 0.0011, weighted_loss: 0.1813, label: 1, bag_size: 10392\n",
      "batch 679, loss: 0.4721, instance_loss: 0.0000, weighted_loss: 0.3304, label: 1, bag_size: 15689\n",
      "batch 699, loss: 0.3590, instance_loss: 0.0639, weighted_loss: 0.2704, label: 0, bag_size: 12910\n",
      "batch 719, loss: 0.4904, instance_loss: 0.5456, weighted_loss: 0.5069, label: 0, bag_size: 2609\n",
      "batch 739, loss: 0.6998, instance_loss: 0.2841, weighted_loss: 0.5750, label: 0, bag_size: 2070\n",
      "batch 759, loss: 0.6492, instance_loss: 0.1174, weighted_loss: 0.4896, label: 0, bag_size: 6850\n",
      "batch 779, loss: 0.3221, instance_loss: 0.3443, weighted_loss: 0.3287, label: 0, bag_size: 1438\n",
      "batch 799, loss: 0.2845, instance_loss: 0.0132, weighted_loss: 0.2031, label: 1, bag_size: 15464\n",
      "batch 819, loss: 0.2522, instance_loss: 0.0057, weighted_loss: 0.1782, label: 1, bag_size: 8685\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9586128048780488: correct 12577/13120\n",
      "class 1 clustering acc 0.7507621951219512: correct 4925/6560\n",
      "Epoch: 7, train_loss: 0.5706, train_clustering_loss:  0.4232, train_error: 0.2500\n",
      "class 0: acc 0.7356608478802993, correct 295/401\n",
      "class 1: acc 0.7637231503579952, correct 320/419\n",
      "\n",
      "Val Set, val_loss: 0.4866, val_error: 0.2000, auc: 0.9479\n",
      "class 0 clustering acc 0.9505681818181818: correct 1673/1760\n",
      "class 1 clustering acc 0.6568181818181819: correct 578/880\n",
      "class 0: acc 0.6346153846153846, correct 33/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "Validation loss decreased (0.585935 --> 0.486583).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.9247, instance_loss: 0.0365, weighted_loss: 0.6583, label: 1, bag_size: 10482\n",
      "batch 39, loss: 0.2482, instance_loss: 0.0120, weighted_loss: 0.1773, label: 1, bag_size: 10033\n",
      "batch 59, loss: 0.5310, instance_loss: 0.1562, weighted_loss: 0.4186, label: 0, bag_size: 10791\n",
      "batch 79, loss: 0.4499, instance_loss: 0.0229, weighted_loss: 0.3218, label: 0, bag_size: 11212\n",
      "batch 99, loss: 0.4708, instance_loss: 0.1770, weighted_loss: 0.3827, label: 0, bag_size: 2814\n",
      "batch 119, loss: 0.4120, instance_loss: 0.0369, weighted_loss: 0.2994, label: 1, bag_size: 5160\n",
      "batch 139, loss: 0.5887, instance_loss: 0.3604, weighted_loss: 0.5202, label: 1, bag_size: 18649\n",
      "batch 159, loss: 0.4331, instance_loss: 0.2943, weighted_loss: 0.3915, label: 1, bag_size: 1412\n",
      "batch 179, loss: 0.5005, instance_loss: 0.1534, weighted_loss: 0.3963, label: 0, bag_size: 11390\n",
      "batch 199, loss: 0.4459, instance_loss: 1.4761, weighted_loss: 0.7550, label: 1, bag_size: 983\n",
      "batch 219, loss: 0.4104, instance_loss: 0.0011, weighted_loss: 0.2876, label: 0, bag_size: 14305\n",
      "batch 239, loss: 0.8512, instance_loss: 1.7578, weighted_loss: 1.1232, label: 1, bag_size: 2935\n",
      "batch 259, loss: 0.1817, instance_loss: 0.1372, weighted_loss: 0.1683, label: 0, bag_size: 12217\n",
      "batch 279, loss: 0.8543, instance_loss: 0.0731, weighted_loss: 0.6200, label: 0, bag_size: 10113\n",
      "batch 299, loss: 0.4876, instance_loss: 0.4218, weighted_loss: 0.4679, label: 1, bag_size: 4054\n",
      "batch 319, loss: 0.1987, instance_loss: 0.0716, weighted_loss: 0.1606, label: 1, bag_size: 7110\n",
      "batch 339, loss: 0.1947, instance_loss: 0.0000, weighted_loss: 0.1363, label: 0, bag_size: 20796\n",
      "batch 359, loss: 0.4303, instance_loss: 0.0016, weighted_loss: 0.3017, label: 1, bag_size: 8012\n",
      "batch 379, loss: 0.3424, instance_loss: 1.0131, weighted_loss: 0.5437, label: 1, bag_size: 15125\n",
      "batch 399, loss: 0.6893, instance_loss: 0.3980, weighted_loss: 0.6019, label: 0, bag_size: 2360\n",
      "batch 419, loss: 0.2113, instance_loss: 0.0003, weighted_loss: 0.1480, label: 0, bag_size: 11759\n",
      "batch 439, loss: 0.2708, instance_loss: 0.1432, weighted_loss: 0.2325, label: 1, bag_size: 4250\n",
      "batch 459, loss: 0.2708, instance_loss: 0.0216, weighted_loss: 0.1960, label: 0, bag_size: 27158\n",
      "batch 479, loss: 0.3534, instance_loss: 0.0444, weighted_loss: 0.2607, label: 0, bag_size: 7557\n",
      "batch 499, loss: 0.2687, instance_loss: 0.2567, weighted_loss: 0.2651, label: 0, bag_size: 1052\n",
      "batch 519, loss: 0.2251, instance_loss: 0.0000, weighted_loss: 0.1576, label: 0, bag_size: 16607\n",
      "batch 539, loss: 0.4046, instance_loss: 0.0218, weighted_loss: 0.2897, label: 0, bag_size: 10898\n",
      "batch 559, loss: 0.5618, instance_loss: 0.1751, weighted_loss: 0.4458, label: 0, bag_size: 2998\n",
      "batch 579, loss: 0.6880, instance_loss: 1.0412, weighted_loss: 0.7940, label: 1, bag_size: 1242\n",
      "batch 599, loss: 0.6478, instance_loss: 0.5224, weighted_loss: 0.6102, label: 1, bag_size: 16162\n",
      "batch 619, loss: 0.5321, instance_loss: 0.8597, weighted_loss: 0.6303, label: 0, bag_size: 705\n",
      "batch 639, loss: 0.1556, instance_loss: 0.0313, weighted_loss: 0.1183, label: 0, bag_size: 13339\n",
      "batch 659, loss: 0.6614, instance_loss: 0.0013, weighted_loss: 0.4634, label: 1, bag_size: 11642\n",
      "batch 679, loss: 0.0915, instance_loss: 0.0002, weighted_loss: 0.0641, label: 0, bag_size: 10535\n",
      "batch 699, loss: 0.1259, instance_loss: 0.0004, weighted_loss: 0.0883, label: 0, bag_size: 19466\n",
      "batch 719, loss: 0.1595, instance_loss: 0.0001, weighted_loss: 0.1117, label: 1, bag_size: 13732\n",
      "batch 739, loss: 0.1815, instance_loss: 0.0008, weighted_loss: 0.1273, label: 0, bag_size: 15001\n",
      "batch 759, loss: 0.5718, instance_loss: 0.1876, weighted_loss: 0.4566, label: 1, bag_size: 1746\n",
      "batch 779, loss: 0.9162, instance_loss: 1.5399, weighted_loss: 1.1033, label: 1, bag_size: 1963\n",
      "batch 799, loss: 0.0918, instance_loss: 0.0000, weighted_loss: 0.0643, label: 1, bag_size: 19039\n",
      "batch 819, loss: 0.1080, instance_loss: 0.0504, weighted_loss: 0.0907, label: 1, bag_size: 1781\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9650914634146341: correct 12662/13120\n",
      "class 1 clustering acc 0.7952743902439025: correct 5217/6560\n",
      "Epoch: 8, train_loss: 0.4573, train_clustering_loss:  0.3510, train_error: 0.1756\n",
      "class 0: acc 0.7908163265306123, correct 310/392\n",
      "class 1: acc 0.8551401869158879, correct 366/428\n",
      "\n",
      "Val Set, val_loss: 0.3715, val_error: 0.1182, auc: 0.9523\n",
      "class 0 clustering acc 0.9880681818181818: correct 1739/1760\n",
      "class 1 clustering acc 0.45227272727272727: correct 398/880\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "Validation loss decreased (0.486583 --> 0.371477).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3743, instance_loss: 0.3646, weighted_loss: 0.3714, label: 0, bag_size: 1416\n",
      "batch 39, loss: 0.2631, instance_loss: 0.0149, weighted_loss: 0.1886, label: 0, bag_size: 6652\n",
      "batch 59, loss: 0.2981, instance_loss: 0.0092, weighted_loss: 0.2115, label: 1, bag_size: 6726\n",
      "batch 79, loss: 0.2573, instance_loss: 0.0053, weighted_loss: 0.1817, label: 0, bag_size: 12148\n",
      "batch 99, loss: 0.7527, instance_loss: 0.2010, weighted_loss: 0.5872, label: 1, bag_size: 2662\n",
      "batch 119, loss: 0.6537, instance_loss: 1.0389, weighted_loss: 0.7692, label: 0, bag_size: 11306\n",
      "batch 139, loss: 0.0506, instance_loss: 0.0224, weighted_loss: 0.0422, label: 0, bag_size: 8372\n",
      "batch 159, loss: 0.8891, instance_loss: 0.5171, weighted_loss: 0.7775, label: 0, bag_size: 3444\n",
      "batch 179, loss: 0.3144, instance_loss: 0.2140, weighted_loss: 0.2843, label: 0, bag_size: 8959\n",
      "batch 199, loss: 0.1993, instance_loss: 0.0262, weighted_loss: 0.1474, label: 1, bag_size: 15689\n",
      "batch 219, loss: 0.4496, instance_loss: 0.0109, weighted_loss: 0.3180, label: 0, bag_size: 4345\n",
      "batch 239, loss: 0.2062, instance_loss: 0.4997, weighted_loss: 0.2943, label: 1, bag_size: 16890\n",
      "batch 259, loss: 0.7624, instance_loss: 0.3191, weighted_loss: 0.6294, label: 0, bag_size: 9069\n",
      "batch 279, loss: 2.5961, instance_loss: 2.9718, weighted_loss: 2.7088, label: 1, bag_size: 3121\n",
      "batch 299, loss: 0.5039, instance_loss: 0.6647, weighted_loss: 0.5521, label: 1, bag_size: 1831\n",
      "batch 319, loss: 0.2118, instance_loss: 1.5027, weighted_loss: 0.5991, label: 1, bag_size: 9404\n",
      "batch 339, loss: 1.0949, instance_loss: 0.0301, weighted_loss: 0.7754, label: 1, bag_size: 9942\n",
      "batch 359, loss: 1.4725, instance_loss: 2.5276, weighted_loss: 1.7891, label: 0, bag_size: 6281\n",
      "batch 379, loss: 0.0915, instance_loss: 0.0005, weighted_loss: 0.0642, label: 1, bag_size: 13368\n",
      "batch 399, loss: 0.2917, instance_loss: 0.0029, weighted_loss: 0.2051, label: 1, bag_size: 13732\n",
      "batch 419, loss: 0.4148, instance_loss: 0.3274, weighted_loss: 0.3886, label: 1, bag_size: 18649\n",
      "batch 439, loss: 0.3303, instance_loss: 0.4342, weighted_loss: 0.3615, label: 1, bag_size: 10848\n",
      "batch 459, loss: 0.0749, instance_loss: 0.0724, weighted_loss: 0.0742, label: 1, bag_size: 10725\n",
      "batch 479, loss: 0.3729, instance_loss: 0.1803, weighted_loss: 0.3151, label: 0, bag_size: 9866\n",
      "batch 499, loss: 0.1735, instance_loss: 0.1922, weighted_loss: 0.1791, label: 0, bag_size: 2063\n",
      "batch 519, loss: 0.4831, instance_loss: 2.5262, weighted_loss: 1.0960, label: 1, bag_size: 2904\n",
      "batch 539, loss: 0.4168, instance_loss: 0.7013, weighted_loss: 0.5022, label: 0, bag_size: 2296\n",
      "batch 559, loss: 0.1298, instance_loss: 0.6166, weighted_loss: 0.2759, label: 1, bag_size: 34356\n",
      "batch 579, loss: 0.9312, instance_loss: 0.2162, weighted_loss: 0.7167, label: 1, bag_size: 1459\n",
      "batch 599, loss: 0.2238, instance_loss: 0.4960, weighted_loss: 0.3054, label: 1, bag_size: 4128\n",
      "batch 619, loss: 0.2079, instance_loss: 0.2887, weighted_loss: 0.2321, label: 0, bag_size: 9616\n",
      "batch 639, loss: 0.2127, instance_loss: 0.1203, weighted_loss: 0.1850, label: 0, bag_size: 11122\n",
      "batch 659, loss: 0.8778, instance_loss: 2.1545, weighted_loss: 1.2608, label: 1, bag_size: 15185\n",
      "batch 679, loss: 0.5267, instance_loss: 0.3197, weighted_loss: 0.4646, label: 1, bag_size: 1746\n",
      "batch 699, loss: 0.2751, instance_loss: 0.0682, weighted_loss: 0.2130, label: 0, bag_size: 1797\n",
      "batch 719, loss: 0.7668, instance_loss: 2.6701, weighted_loss: 1.3378, label: 0, bag_size: 2219\n",
      "batch 739, loss: 0.9731, instance_loss: 1.8870, weighted_loss: 1.2473, label: 1, bag_size: 2937\n",
      "batch 759, loss: 0.2086, instance_loss: 0.3235, weighted_loss: 0.2431, label: 0, bag_size: 2873\n",
      "batch 779, loss: 0.1045, instance_loss: 0.0263, weighted_loss: 0.0810, label: 0, bag_size: 9885\n",
      "batch 799, loss: 0.1317, instance_loss: 0.0595, weighted_loss: 0.1101, label: 0, bag_size: 10995\n",
      "batch 819, loss: 0.3578, instance_loss: 0.1507, weighted_loss: 0.2957, label: 0, bag_size: 8582\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9551829268292683: correct 12532/13120\n",
      "class 1 clustering acc 0.7594512195121951: correct 4982/6560\n",
      "Epoch: 9, train_loss: 0.4189, train_clustering_loss:  0.4248, train_error: 0.1707\n",
      "class 0: acc 0.8483412322274881, correct 358/422\n",
      "class 1: acc 0.8090452261306532, correct 322/398\n",
      "\n",
      "Val Set, val_loss: 0.4116, val_error: 0.2273, auc: 0.9576\n",
      "class 0 clustering acc 0.9431818181818182: correct 1660/1760\n",
      "class 1 clustering acc 0.6511363636363636: correct 573/880\n",
      "class 0: acc 0.5384615384615384, correct 28/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1233, instance_loss: 0.0051, weighted_loss: 0.0878, label: 1, bag_size: 19972\n",
      "batch 39, loss: 1.4869, instance_loss: 0.4838, weighted_loss: 1.1859, label: 0, bag_size: 11306\n",
      "batch 59, loss: 0.0896, instance_loss: 0.1911, weighted_loss: 0.1200, label: 0, bag_size: 10995\n",
      "batch 79, loss: 0.6368, instance_loss: 1.0748, weighted_loss: 0.7682, label: 1, bag_size: 2759\n",
      "batch 99, loss: 0.0751, instance_loss: 0.0008, weighted_loss: 0.0528, label: 1, bag_size: 14433\n",
      "batch 119, loss: 0.2681, instance_loss: 0.1009, weighted_loss: 0.2179, label: 0, bag_size: 8549\n",
      "batch 139, loss: 0.6563, instance_loss: 1.0441, weighted_loss: 0.7727, label: 0, bag_size: 2290\n",
      "batch 159, loss: 0.1637, instance_loss: 0.2933, weighted_loss: 0.2026, label: 0, bag_size: 1052\n",
      "batch 179, loss: 0.1164, instance_loss: 0.0087, weighted_loss: 0.0841, label: 0, bag_size: 19067\n",
      "batch 199, loss: 0.0503, instance_loss: 0.0002, weighted_loss: 0.0353, label: 1, bag_size: 13174\n",
      "batch 219, loss: 0.1383, instance_loss: 0.0000, weighted_loss: 0.0968, label: 1, bag_size: 9878\n",
      "batch 239, loss: 0.2383, instance_loss: 0.0066, weighted_loss: 0.1688, label: 0, bag_size: 4959\n",
      "batch 259, loss: 0.1991, instance_loss: 0.0003, weighted_loss: 0.1395, label: 1, bag_size: 19932\n",
      "batch 279, loss: 0.0986, instance_loss: 0.0284, weighted_loss: 0.0775, label: 1, bag_size: 7798\n",
      "batch 299, loss: 0.4854, instance_loss: 0.2031, weighted_loss: 0.4007, label: 0, bag_size: 19043\n",
      "batch 319, loss: 0.4602, instance_loss: 0.3191, weighted_loss: 0.4179, label: 0, bag_size: 2004\n",
      "batch 339, loss: 0.2190, instance_loss: 0.1066, weighted_loss: 0.1853, label: 0, bag_size: 7557\n",
      "batch 359, loss: 0.0872, instance_loss: 0.1348, weighted_loss: 0.1015, label: 1, bag_size: 18794\n",
      "batch 379, loss: 0.1641, instance_loss: 0.1146, weighted_loss: 0.1493, label: 1, bag_size: 6736\n",
      "batch 399, loss: 0.2101, instance_loss: 0.0247, weighted_loss: 0.1545, label: 1, bag_size: 16548\n",
      "batch 419, loss: 0.1273, instance_loss: 0.0189, weighted_loss: 0.0948, label: 1, bag_size: 11363\n",
      "batch 439, loss: 0.1556, instance_loss: 0.2093, weighted_loss: 0.1717, label: 1, bag_size: 928\n",
      "batch 459, loss: 1.0936, instance_loss: 1.4573, weighted_loss: 1.2027, label: 0, bag_size: 2815\n",
      "batch 479, loss: 0.0626, instance_loss: 0.1108, weighted_loss: 0.0770, label: 0, bag_size: 14319\n",
      "batch 499, loss: 0.0596, instance_loss: 0.0006, weighted_loss: 0.0419, label: 0, bag_size: 11900\n",
      "batch 519, loss: 3.4138, instance_loss: 8.5742, weighted_loss: 4.9619, label: 0, bag_size: 3468\n",
      "batch 539, loss: 0.1245, instance_loss: 0.4459, weighted_loss: 0.2209, label: 1, bag_size: 3453\n",
      "batch 559, loss: 0.3196, instance_loss: 0.7280, weighted_loss: 0.4421, label: 1, bag_size: 7873\n",
      "batch 579, loss: 0.3423, instance_loss: 0.2025, weighted_loss: 0.3004, label: 0, bag_size: 10381\n",
      "batch 599, loss: 0.0571, instance_loss: 0.2265, weighted_loss: 0.1079, label: 1, bag_size: 5317\n",
      "batch 619, loss: 0.2659, instance_loss: 0.5025, weighted_loss: 0.3368, label: 0, bag_size: 6624\n",
      "batch 639, loss: 0.2366, instance_loss: 0.1518, weighted_loss: 0.2111, label: 0, bag_size: 21319\n",
      "batch 659, loss: 0.6096, instance_loss: 0.6537, weighted_loss: 0.6228, label: 0, bag_size: 2820\n",
      "batch 679, loss: 0.1842, instance_loss: 0.4137, weighted_loss: 0.2531, label: 1, bag_size: 1412\n",
      "batch 699, loss: 0.2243, instance_loss: 0.1328, weighted_loss: 0.1968, label: 1, bag_size: 1683\n",
      "batch 719, loss: 0.0608, instance_loss: 0.0252, weighted_loss: 0.0501, label: 1, bag_size: 8003\n",
      "batch 739, loss: 1.4619, instance_loss: 2.9487, weighted_loss: 1.9079, label: 1, bag_size: 1845\n",
      "batch 759, loss: 0.4108, instance_loss: 0.4389, weighted_loss: 0.4192, label: 0, bag_size: 2195\n",
      "batch 779, loss: 0.2078, instance_loss: 0.1072, weighted_loss: 0.1776, label: 0, bag_size: 22498\n",
      "batch 799, loss: 0.2268, instance_loss: 0.0959, weighted_loss: 0.1875, label: 1, bag_size: 11220\n",
      "batch 819, loss: 0.0270, instance_loss: 0.0000, weighted_loss: 0.0189, label: 1, bag_size: 7513\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9696646341463414: correct 12722/13120\n",
      "class 1 clustering acc 0.8097560975609757: correct 5312/6560\n",
      "Epoch: 10, train_loss: 0.3505, train_clustering_loss:  0.3274, train_error: 0.1354\n",
      "class 0: acc 0.8497409326424871, correct 328/386\n",
      "class 1: acc 0.8778801843317973, correct 381/434\n",
      "\n",
      "Val Set, val_loss: 0.3157, val_error: 0.1364, auc: 0.9556\n",
      "class 0 clustering acc 0.9681818181818181: correct 1704/1760\n",
      "class 1 clustering acc 0.7863636363636364: correct 692/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.7758620689655172, correct 45/58\n",
      "Validation loss decreased (0.371477 --> 0.315673).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0685, instance_loss: 0.0002, weighted_loss: 0.0480, label: 1, bag_size: 18794\n",
      "batch 39, loss: 0.3016, instance_loss: 0.0644, weighted_loss: 0.2304, label: 0, bag_size: 1884\n",
      "batch 59, loss: 0.1481, instance_loss: 0.5244, weighted_loss: 0.2610, label: 0, bag_size: 2270\n",
      "batch 79, loss: 0.0581, instance_loss: 0.0057, weighted_loss: 0.0424, label: 1, bag_size: 13026\n",
      "batch 99, loss: 0.0159, instance_loss: 0.0041, weighted_loss: 0.0124, label: 0, bag_size: 8372\n",
      "batch 119, loss: 0.3880, instance_loss: 0.0409, weighted_loss: 0.2839, label: 1, bag_size: 3450\n",
      "batch 139, loss: 1.0588, instance_loss: 0.1206, weighted_loss: 0.7773, label: 0, bag_size: 25814\n",
      "batch 159, loss: 0.0351, instance_loss: 0.0256, weighted_loss: 0.0323, label: 1, bag_size: 3437\n",
      "batch 179, loss: 0.0949, instance_loss: 0.0083, weighted_loss: 0.0689, label: 1, bag_size: 12865\n",
      "batch 199, loss: 1.0549, instance_loss: 0.4533, weighted_loss: 0.8744, label: 0, bag_size: 1831\n",
      "batch 219, loss: 4.5376, instance_loss: 3.4115, weighted_loss: 4.1998, label: 0, bag_size: 2694\n",
      "batch 239, loss: 0.1051, instance_loss: 0.1776, weighted_loss: 0.1268, label: 1, bag_size: 617\n",
      "batch 259, loss: 0.2266, instance_loss: 0.0329, weighted_loss: 0.1685, label: 0, bag_size: 14377\n",
      "batch 279, loss: 0.5563, instance_loss: 0.0514, weighted_loss: 0.4048, label: 1, bag_size: 2179\n",
      "batch 299, loss: 0.0343, instance_loss: 0.0187, weighted_loss: 0.0296, label: 0, bag_size: 8252\n",
      "batch 319, loss: 0.0730, instance_loss: 0.0358, weighted_loss: 0.0619, label: 0, bag_size: 2624\n",
      "batch 339, loss: 0.1468, instance_loss: 0.0006, weighted_loss: 0.1029, label: 1, bag_size: 9408\n",
      "batch 359, loss: 0.0305, instance_loss: 0.0000, weighted_loss: 0.0214, label: 1, bag_size: 11701\n",
      "batch 379, loss: 0.1945, instance_loss: 0.0009, weighted_loss: 0.1364, label: 0, bag_size: 21319\n",
      "batch 399, loss: 0.0329, instance_loss: 0.0000, weighted_loss: 0.0230, label: 1, bag_size: 7798\n",
      "batch 419, loss: 0.2315, instance_loss: 0.2695, weighted_loss: 0.2429, label: 1, bag_size: 4259\n",
      "batch 439, loss: 0.2153, instance_loss: 0.1152, weighted_loss: 0.1853, label: 1, bag_size: 5454\n",
      "batch 459, loss: 0.0585, instance_loss: 0.0000, weighted_loss: 0.0409, label: 1, bag_size: 19832\n",
      "batch 479, loss: 0.3430, instance_loss: 0.0532, weighted_loss: 0.2561, label: 0, bag_size: 11122\n",
      "batch 499, loss: 0.0696, instance_loss: 0.1658, weighted_loss: 0.0985, label: 1, bag_size: 2385\n",
      "batch 519, loss: 0.3143, instance_loss: 0.1083, weighted_loss: 0.2525, label: 0, bag_size: 6356\n",
      "batch 539, loss: 0.0691, instance_loss: 0.0073, weighted_loss: 0.0505, label: 1, bag_size: 8438\n",
      "batch 559, loss: 0.3589, instance_loss: 0.0237, weighted_loss: 0.2583, label: 0, bag_size: 9415\n",
      "batch 579, loss: 1.2723, instance_loss: 0.4430, weighted_loss: 1.0235, label: 0, bag_size: 11128\n",
      "batch 599, loss: 0.2686, instance_loss: 0.6727, weighted_loss: 0.3898, label: 1, bag_size: 18468\n",
      "batch 619, loss: 1.8607, instance_loss: 1.6082, weighted_loss: 1.7849, label: 1, bag_size: 9162\n",
      "batch 639, loss: 0.7570, instance_loss: 0.3234, weighted_loss: 0.6269, label: 0, bag_size: 10146\n",
      "batch 659, loss: 0.0952, instance_loss: 0.3140, weighted_loss: 0.1608, label: 0, bag_size: 2628\n",
      "batch 679, loss: 0.1303, instance_loss: 0.1708, weighted_loss: 0.1424, label: 1, bag_size: 22264\n",
      "batch 699, loss: 0.1491, instance_loss: 0.2176, weighted_loss: 0.1696, label: 0, bag_size: 3474\n",
      "batch 719, loss: 0.0231, instance_loss: 0.2419, weighted_loss: 0.0887, label: 0, bag_size: 12687\n",
      "batch 739, loss: 0.0720, instance_loss: 0.0401, weighted_loss: 0.0625, label: 0, bag_size: 3228\n",
      "batch 759, loss: 0.0801, instance_loss: 0.0737, weighted_loss: 0.0782, label: 1, bag_size: 11223\n",
      "batch 779, loss: 0.1031, instance_loss: 0.0000, weighted_loss: 0.0722, label: 1, bag_size: 15716\n",
      "batch 799, loss: 0.0702, instance_loss: 0.0224, weighted_loss: 0.0559, label: 0, bag_size: 17633\n",
      "batch 819, loss: 0.3491, instance_loss: 0.9528, weighted_loss: 0.5302, label: 1, bag_size: 1123\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.968140243902439: correct 12702/13120\n",
      "class 1 clustering acc 0.8157012195121951: correct 5351/6560\n",
      "Epoch: 11, train_loss: 0.3397, train_clustering_loss:  0.3202, train_error: 0.1146\n",
      "class 0: acc 0.8902743142144638, correct 357/401\n",
      "class 1: acc 0.8806682577565632, correct 369/419\n",
      "\n",
      "Val Set, val_loss: 0.2871, val_error: 0.1182, auc: 0.9566\n",
      "class 0 clustering acc 0.9539772727272727: correct 1679/1760\n",
      "class 1 clustering acc 0.6420454545454546: correct 565/880\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "Validation loss decreased (0.315673 --> 0.287096).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0826, instance_loss: 0.7198, weighted_loss: 0.2738, label: 0, bag_size: 931\n",
      "batch 39, loss: 0.1078, instance_loss: 0.0213, weighted_loss: 0.0819, label: 1, bag_size: 6164\n",
      "batch 59, loss: 0.7612, instance_loss: 0.4497, weighted_loss: 0.6678, label: 1, bag_size: 7989\n",
      "batch 79, loss: 0.2996, instance_loss: 0.2835, weighted_loss: 0.2947, label: 0, bag_size: 1202\n",
      "batch 99, loss: 0.1109, instance_loss: 0.1813, weighted_loss: 0.1320, label: 0, bag_size: 17268\n",
      "batch 119, loss: 0.1448, instance_loss: 0.1354, weighted_loss: 0.1420, label: 1, bag_size: 5256\n",
      "batch 139, loss: 0.3274, instance_loss: 0.3138, weighted_loss: 0.3233, label: 0, bag_size: 2628\n",
      "batch 159, loss: 0.0513, instance_loss: 0.5701, weighted_loss: 0.2070, label: 1, bag_size: 3651\n",
      "batch 179, loss: 0.1488, instance_loss: 0.0297, weighted_loss: 0.1130, label: 1, bag_size: 10920\n",
      "batch 199, loss: 0.9120, instance_loss: 0.6770, weighted_loss: 0.8415, label: 0, bag_size: 1437\n",
      "batch 219, loss: 1.5490, instance_loss: 0.7401, weighted_loss: 1.3063, label: 0, bag_size: 2179\n",
      "batch 239, loss: 0.2594, instance_loss: 0.0000, weighted_loss: 0.1816, label: 1, bag_size: 7389\n",
      "batch 259, loss: 0.0504, instance_loss: 0.0081, weighted_loss: 0.0377, label: 1, bag_size: 6606\n",
      "batch 279, loss: 0.0559, instance_loss: 0.0017, weighted_loss: 0.0396, label: 1, bag_size: 14202\n",
      "batch 299, loss: 0.4842, instance_loss: 0.0366, weighted_loss: 0.3499, label: 0, bag_size: 3089\n",
      "batch 319, loss: 0.1412, instance_loss: 0.0002, weighted_loss: 0.0989, label: 1, bag_size: 8868\n",
      "batch 339, loss: 0.0988, instance_loss: 0.0000, weighted_loss: 0.0691, label: 1, bag_size: 6606\n",
      "batch 359, loss: 0.2138, instance_loss: 0.0000, weighted_loss: 0.1497, label: 1, bag_size: 18649\n",
      "batch 379, loss: 0.0750, instance_loss: 0.1174, weighted_loss: 0.0877, label: 1, bag_size: 16565\n",
      "batch 399, loss: 0.0726, instance_loss: 0.0035, weighted_loss: 0.0519, label: 1, bag_size: 12349\n",
      "batch 419, loss: 0.2348, instance_loss: 0.0034, weighted_loss: 0.1654, label: 1, bag_size: 9561\n",
      "batch 439, loss: 0.7072, instance_loss: 1.2506, weighted_loss: 0.8702, label: 0, bag_size: 9616\n",
      "batch 459, loss: 0.0193, instance_loss: 0.0399, weighted_loss: 0.0255, label: 0, bag_size: 10365\n",
      "batch 479, loss: 0.0186, instance_loss: 0.0107, weighted_loss: 0.0162, label: 0, bag_size: 13892\n",
      "batch 499, loss: 0.1415, instance_loss: 0.3540, weighted_loss: 0.2053, label: 0, bag_size: 1909\n",
      "batch 519, loss: 0.0589, instance_loss: 0.0013, weighted_loss: 0.0416, label: 0, bag_size: 18045\n",
      "batch 539, loss: 0.9908, instance_loss: 5.1477, weighted_loss: 2.2379, label: 0, bag_size: 2653\n",
      "batch 559, loss: 0.1521, instance_loss: 0.1564, weighted_loss: 0.1534, label: 1, bag_size: 3683\n",
      "batch 579, loss: 0.1161, instance_loss: 0.6245, weighted_loss: 0.2686, label: 1, bag_size: 699\n",
      "batch 599, loss: 0.2325, instance_loss: 1.4546, weighted_loss: 0.5991, label: 0, bag_size: 10415\n",
      "batch 619, loss: 0.0842, instance_loss: 0.8894, weighted_loss: 0.3258, label: 0, bag_size: 11759\n",
      "batch 639, loss: 0.4994, instance_loss: 2.1016, weighted_loss: 0.9801, label: 1, bag_size: 11386\n",
      "batch 659, loss: 0.7254, instance_loss: 0.3420, weighted_loss: 0.6104, label: 0, bag_size: 4418\n",
      "batch 679, loss: 0.4798, instance_loss: 0.0185, weighted_loss: 0.3414, label: 1, bag_size: 6343\n",
      "batch 699, loss: 0.0853, instance_loss: 0.0000, weighted_loss: 0.0597, label: 1, bag_size: 18794\n",
      "batch 719, loss: 1.5771, instance_loss: 0.6053, weighted_loss: 1.2855, label: 0, bag_size: 8420\n",
      "batch 739, loss: 0.3455, instance_loss: 0.5526, weighted_loss: 0.4077, label: 0, bag_size: 2457\n",
      "batch 759, loss: 0.2139, instance_loss: 0.3724, weighted_loss: 0.2614, label: 1, bag_size: 3651\n",
      "batch 779, loss: 0.2568, instance_loss: 0.4187, weighted_loss: 0.3054, label: 1, bag_size: 13732\n",
      "batch 799, loss: 0.0793, instance_loss: 0.0081, weighted_loss: 0.0579, label: 0, bag_size: 2873\n",
      "batch 819, loss: 1.6550, instance_loss: 1.3631, weighted_loss: 1.5674, label: 1, bag_size: 13440\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9591463414634146: correct 12584/13120\n",
      "class 1 clustering acc 0.7628048780487805: correct 5004/6560\n",
      "Epoch: 12, train_loss: 0.3727, train_clustering_loss:  0.4070, train_error: 0.1524\n",
      "class 0: acc 0.8428927680798005, correct 338/401\n",
      "class 1: acc 0.8520286396181385, correct 357/419\n",
      "\n",
      "Val Set, val_loss: 0.2818, val_error: 0.1273, auc: 0.9579\n",
      "class 0 clustering acc 0.94375: correct 1661/1760\n",
      "class 1 clustering acc 0.775: correct 682/880\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "Validation loss decreased (0.287096 --> 0.281812).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0770, instance_loss: 0.0252, weighted_loss: 0.0615, label: 0, bag_size: 11527\n",
      "batch 39, loss: 0.3516, instance_loss: 1.5343, weighted_loss: 0.7064, label: 0, bag_size: 7835\n",
      "batch 59, loss: 1.8024, instance_loss: 2.1352, weighted_loss: 1.9023, label: 1, bag_size: 2395\n",
      "batch 79, loss: 0.1047, instance_loss: 0.0085, weighted_loss: 0.0759, label: 1, bag_size: 9610\n",
      "batch 99, loss: 0.0702, instance_loss: 0.0142, weighted_loss: 0.0534, label: 1, bag_size: 5731\n",
      "batch 119, loss: 0.1353, instance_loss: 0.3190, weighted_loss: 0.1904, label: 0, bag_size: 1149\n",
      "batch 139, loss: 0.1429, instance_loss: 0.0121, weighted_loss: 0.1036, label: 1, bag_size: 5256\n",
      "batch 159, loss: 0.1930, instance_loss: 0.2147, weighted_loss: 0.1995, label: 1, bag_size: 2662\n",
      "batch 179, loss: 0.2118, instance_loss: 0.3704, weighted_loss: 0.2594, label: 0, bag_size: 2628\n",
      "batch 199, loss: 0.3150, instance_loss: 0.0915, weighted_loss: 0.2480, label: 0, bag_size: 18777\n",
      "batch 219, loss: 0.0755, instance_loss: 0.0064, weighted_loss: 0.0548, label: 0, bag_size: 2322\n",
      "batch 239, loss: 0.4138, instance_loss: 0.1390, weighted_loss: 0.3314, label: 1, bag_size: 1823\n",
      "batch 259, loss: 0.1049, instance_loss: 0.0369, weighted_loss: 0.0845, label: 1, bag_size: 10028\n",
      "batch 279, loss: 0.1313, instance_loss: 0.0296, weighted_loss: 0.1008, label: 1, bag_size: 12719\n",
      "batch 299, loss: 0.0024, instance_loss: 0.0071, weighted_loss: 0.0038, label: 1, bag_size: 6792\n",
      "batch 319, loss: 1.0004, instance_loss: 0.1983, weighted_loss: 0.7598, label: 0, bag_size: 18738\n",
      "batch 339, loss: 0.0576, instance_loss: 0.0033, weighted_loss: 0.0413, label: 0, bag_size: 8898\n",
      "batch 359, loss: 0.6286, instance_loss: 0.9828, weighted_loss: 0.7349, label: 1, bag_size: 1294\n",
      "batch 379, loss: 0.0524, instance_loss: 0.1548, weighted_loss: 0.0831, label: 0, bag_size: 12687\n",
      "batch 399, loss: 0.1829, instance_loss: 0.0039, weighted_loss: 0.1292, label: 0, bag_size: 18215\n",
      "batch 419, loss: 0.7875, instance_loss: 0.1497, weighted_loss: 0.5961, label: 1, bag_size: 8475\n",
      "batch 439, loss: 0.1381, instance_loss: 0.0076, weighted_loss: 0.0989, label: 1, bag_size: 2662\n",
      "batch 459, loss: 0.0634, instance_loss: 0.0029, weighted_loss: 0.0453, label: 1, bag_size: 18603\n",
      "batch 479, loss: 0.0331, instance_loss: 0.0011, weighted_loss: 0.0235, label: 1, bag_size: 16512\n",
      "batch 499, loss: 0.1152, instance_loss: 0.0289, weighted_loss: 0.0894, label: 1, bag_size: 6665\n",
      "batch 519, loss: 0.1151, instance_loss: 0.1300, weighted_loss: 0.1196, label: 0, bag_size: 1712\n",
      "batch 539, loss: 0.1308, instance_loss: 0.0385, weighted_loss: 0.1031, label: 0, bag_size: 1712\n",
      "batch 559, loss: 0.0870, instance_loss: 0.0323, weighted_loss: 0.0706, label: 1, bag_size: 10112\n",
      "batch 579, loss: 0.0639, instance_loss: 0.0131, weighted_loss: 0.0486, label: 0, bag_size: 9415\n",
      "batch 599, loss: 0.0530, instance_loss: 0.0000, weighted_loss: 0.0371, label: 1, bag_size: 19832\n",
      "batch 619, loss: 0.2872, instance_loss: 0.1437, weighted_loss: 0.2441, label: 0, bag_size: 2609\n",
      "batch 639, loss: 0.0836, instance_loss: 0.0713, weighted_loss: 0.0799, label: 0, bag_size: 1881\n",
      "batch 659, loss: 2.3988, instance_loss: 1.6529, weighted_loss: 2.1750, label: 1, bag_size: 898\n",
      "batch 679, loss: 0.0627, instance_loss: 0.1291, weighted_loss: 0.0826, label: 1, bag_size: 3224\n",
      "batch 699, loss: 0.1812, instance_loss: 0.0932, weighted_loss: 0.1548, label: 1, bag_size: 15689\n",
      "batch 719, loss: 0.5081, instance_loss: 0.0000, weighted_loss: 0.3557, label: 0, bag_size: 5297\n",
      "batch 739, loss: 4.6777, instance_loss: 1.6091, weighted_loss: 3.7572, label: 0, bag_size: 3897\n",
      "batch 759, loss: 0.1092, instance_loss: 0.0000, weighted_loss: 0.0765, label: 1, bag_size: 14515\n",
      "batch 779, loss: 0.0207, instance_loss: 0.0003, weighted_loss: 0.0146, label: 0, bag_size: 23398\n",
      "batch 799, loss: 0.0459, instance_loss: 0.0050, weighted_loss: 0.0336, label: 0, bag_size: 8898\n",
      "batch 819, loss: 1.4513, instance_loss: 1.6930, weighted_loss: 1.5238, label: 1, bag_size: 15563\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9665396341463415: correct 12681/13120\n",
      "class 1 clustering acc 0.8019817073170732: correct 5261/6560\n",
      "Epoch: 13, train_loss: 0.3406, train_clustering_loss:  0.3445, train_error: 0.1256\n",
      "class 0: acc 0.8896882494004796, correct 371/417\n",
      "class 1: acc 0.858560794044665, correct 346/403\n",
      "\n",
      "Val Set, val_loss: 0.2776, val_error: 0.1273, auc: 0.9586\n",
      "class 0 clustering acc 0.9619318181818182: correct 1693/1760\n",
      "class 1 clustering acc 0.8136363636363636: correct 716/880\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "Validation loss decreased (0.281812 --> 0.277632).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2073, instance_loss: 0.1139, weighted_loss: 0.1793, label: 1, bag_size: 4259\n",
      "batch 39, loss: 0.0663, instance_loss: 0.0291, weighted_loss: 0.0551, label: 1, bag_size: 4239\n",
      "batch 59, loss: 1.2592, instance_loss: 1.3066, weighted_loss: 1.2734, label: 0, bag_size: 3375\n",
      "batch 79, loss: 0.0242, instance_loss: 0.0030, weighted_loss: 0.0179, label: 1, bag_size: 5612\n",
      "batch 99, loss: 0.0309, instance_loss: 0.0221, weighted_loss: 0.0283, label: 0, bag_size: 11122\n",
      "batch 119, loss: 0.1282, instance_loss: 0.2538, weighted_loss: 0.1659, label: 1, bag_size: 10396\n",
      "batch 139, loss: 0.1539, instance_loss: 0.0587, weighted_loss: 0.1254, label: 0, bag_size: 13591\n",
      "batch 159, loss: 0.0679, instance_loss: 0.0125, weighted_loss: 0.0513, label: 0, bag_size: 11512\n",
      "batch 179, loss: 0.0532, instance_loss: 0.1425, weighted_loss: 0.0800, label: 1, bag_size: 4423\n",
      "batch 199, loss: 0.0170, instance_loss: 0.0142, weighted_loss: 0.0162, label: 0, bag_size: 15636\n",
      "batch 219, loss: 0.1720, instance_loss: 0.0654, weighted_loss: 0.1400, label: 0, bag_size: 10415\n",
      "batch 239, loss: 0.2173, instance_loss: 0.0337, weighted_loss: 0.1622, label: 1, bag_size: 5903\n",
      "batch 259, loss: 1.8402, instance_loss: 0.4240, weighted_loss: 1.4153, label: 0, bag_size: 1831\n",
      "batch 279, loss: 0.1385, instance_loss: 2.0263, weighted_loss: 0.7048, label: 0, bag_size: 2091\n",
      "batch 299, loss: 0.1464, instance_loss: 1.2211, weighted_loss: 0.4688, label: 0, bag_size: 705\n",
      "batch 319, loss: 0.1198, instance_loss: 0.0002, weighted_loss: 0.0839, label: 0, bag_size: 14377\n",
      "batch 339, loss: 0.0195, instance_loss: 0.0072, weighted_loss: 0.0158, label: 1, bag_size: 5221\n",
      "batch 359, loss: 0.0354, instance_loss: 0.0000, weighted_loss: 0.0248, label: 0, bag_size: 22498\n",
      "batch 379, loss: 0.0413, instance_loss: 1.6027, weighted_loss: 0.5097, label: 1, bag_size: 11981\n",
      "batch 399, loss: 0.1863, instance_loss: 0.0838, weighted_loss: 0.1555, label: 0, bag_size: 2006\n",
      "batch 419, loss: 0.1857, instance_loss: 0.1859, weighted_loss: 0.1858, label: 1, bag_size: 8191\n",
      "batch 439, loss: 0.0533, instance_loss: 0.0550, weighted_loss: 0.0538, label: 0, bag_size: 9851\n",
      "batch 459, loss: 0.0150, instance_loss: 0.2938, weighted_loss: 0.0987, label: 0, bag_size: 10068\n",
      "batch 479, loss: 1.3917, instance_loss: 1.6592, weighted_loss: 1.4720, label: 1, bag_size: 684\n",
      "batch 499, loss: 1.1156, instance_loss: 0.7194, weighted_loss: 0.9967, label: 0, bag_size: 1370\n",
      "batch 519, loss: 0.0770, instance_loss: 0.3337, weighted_loss: 0.1540, label: 1, bag_size: 1014\n",
      "batch 539, loss: 0.2911, instance_loss: 0.0078, weighted_loss: 0.2061, label: 1, bag_size: 13365\n",
      "batch 559, loss: 0.5370, instance_loss: 0.0454, weighted_loss: 0.3895, label: 0, bag_size: 21361\n",
      "batch 579, loss: 0.2262, instance_loss: 0.0632, weighted_loss: 0.1773, label: 1, bag_size: 5894\n",
      "batch 599, loss: 0.1693, instance_loss: 0.0291, weighted_loss: 0.1272, label: 0, bag_size: 10029\n",
      "batch 619, loss: 0.1266, instance_loss: 0.0080, weighted_loss: 0.0911, label: 0, bag_size: 14377\n",
      "batch 639, loss: 0.3274, instance_loss: 0.4752, weighted_loss: 0.3718, label: 1, bag_size: 1255\n",
      "batch 659, loss: 0.0096, instance_loss: 0.0004, weighted_loss: 0.0068, label: 0, bag_size: 7191\n",
      "batch 679, loss: 0.3850, instance_loss: 0.0948, weighted_loss: 0.2979, label: 1, bag_size: 2695\n",
      "batch 699, loss: 0.0550, instance_loss: 0.0103, weighted_loss: 0.0416, label: 0, bag_size: 15636\n",
      "batch 719, loss: 0.0125, instance_loss: 0.0002, weighted_loss: 0.0088, label: 0, bag_size: 11735\n",
      "batch 739, loss: 0.0612, instance_loss: 0.0013, weighted_loss: 0.0432, label: 0, bag_size: 12732\n",
      "batch 759, loss: 0.4106, instance_loss: 0.0002, weighted_loss: 0.2875, label: 0, bag_size: 10113\n",
      "batch 779, loss: 0.9338, instance_loss: 0.5538, weighted_loss: 0.8198, label: 1, bag_size: 1095\n",
      "batch 799, loss: 0.1295, instance_loss: 0.0093, weighted_loss: 0.0934, label: 0, bag_size: 19067\n",
      "batch 819, loss: 0.0104, instance_loss: 0.0079, weighted_loss: 0.0097, label: 1, bag_size: 5317\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9701981707317073: correct 12729/13120\n",
      "class 1 clustering acc 0.8230182926829268: correct 5399/6560\n",
      "Epoch: 14, train_loss: 0.3245, train_clustering_loss:  0.3116, train_error: 0.1159\n",
      "class 0: acc 0.8888888888888888, correct 368/414\n",
      "class 1: acc 0.8793103448275862, correct 357/406\n",
      "\n",
      "Val Set, val_loss: 0.2716, val_error: 0.1273, auc: 0.9595\n",
      "class 0 clustering acc 0.9420454545454545: correct 1658/1760\n",
      "class 1 clustering acc 0.7272727272727273: correct 640/880\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "Validation loss decreased (0.277632 --> 0.271571).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0269, instance_loss: 0.0086, weighted_loss: 0.0214, label: 1, bag_size: 8522\n",
      "batch 39, loss: 0.1045, instance_loss: 0.0097, weighted_loss: 0.0760, label: 1, bag_size: 12603\n",
      "batch 59, loss: 0.0095, instance_loss: 0.0047, weighted_loss: 0.0081, label: 1, bag_size: 11122\n",
      "batch 79, loss: 0.4995, instance_loss: 0.7020, weighted_loss: 0.5603, label: 1, bag_size: 1230\n",
      "batch 99, loss: 0.0271, instance_loss: 0.0076, weighted_loss: 0.0212, label: 0, bag_size: 13591\n",
      "batch 119, loss: 0.0476, instance_loss: 0.0072, weighted_loss: 0.0355, label: 1, bag_size: 5991\n",
      "batch 139, loss: 0.0136, instance_loss: 0.0380, weighted_loss: 0.0209, label: 0, bag_size: 26271\n",
      "batch 159, loss: 0.0120, instance_loss: 0.0000, weighted_loss: 0.0084, label: 0, bag_size: 19472\n",
      "batch 179, loss: 0.5426, instance_loss: 0.2999, weighted_loss: 0.4698, label: 1, bag_size: 7669\n",
      "batch 199, loss: 0.1195, instance_loss: 0.0616, weighted_loss: 0.1022, label: 0, bag_size: 10490\n",
      "batch 219, loss: 0.0531, instance_loss: 0.0008, weighted_loss: 0.0374, label: 0, bag_size: 15313\n",
      "batch 239, loss: 0.1773, instance_loss: 0.2933, weighted_loss: 0.2121, label: 1, bag_size: 4442\n",
      "batch 259, loss: 0.2554, instance_loss: 0.1699, weighted_loss: 0.2297, label: 0, bag_size: 1690\n",
      "batch 279, loss: 0.1034, instance_loss: 0.6481, weighted_loss: 0.2668, label: 0, bag_size: 11512\n",
      "batch 299, loss: 1.6757, instance_loss: 1.3244, weighted_loss: 1.5703, label: 0, bag_size: 1701\n",
      "batch 319, loss: 2.1572, instance_loss: 6.2496, weighted_loss: 3.3849, label: 1, bag_size: 2731\n",
      "batch 339, loss: 0.7582, instance_loss: 0.2268, weighted_loss: 0.5988, label: 1, bag_size: 2179\n",
      "batch 359, loss: 0.1708, instance_loss: 0.0322, weighted_loss: 0.1292, label: 0, bag_size: 10490\n",
      "batch 379, loss: 0.2174, instance_loss: 0.0184, weighted_loss: 0.1577, label: 0, bag_size: 16087\n",
      "batch 399, loss: 0.3590, instance_loss: 0.0726, weighted_loss: 0.2731, label: 0, bag_size: 3321\n",
      "batch 419, loss: 2.2205, instance_loss: 2.3631, weighted_loss: 2.2633, label: 1, bag_size: 1703\n",
      "batch 439, loss: 0.0932, instance_loss: 0.0000, weighted_loss: 0.0652, label: 0, bag_size: 14333\n",
      "batch 459, loss: 0.7176, instance_loss: 1.2947, weighted_loss: 0.8907, label: 0, bag_size: 1142\n",
      "batch 479, loss: 0.2452, instance_loss: 0.3574, weighted_loss: 0.2789, label: 1, bag_size: 12460\n",
      "batch 499, loss: 0.0411, instance_loss: 0.0393, weighted_loss: 0.0406, label: 1, bag_size: 11363\n",
      "batch 519, loss: 0.5862, instance_loss: 0.1939, weighted_loss: 0.4685, label: 1, bag_size: 6682\n",
      "batch 539, loss: 1.2780, instance_loss: 2.0103, weighted_loss: 1.4977, label: 1, bag_size: 1867\n",
      "batch 559, loss: 0.0315, instance_loss: 0.0409, weighted_loss: 0.0343, label: 1, bag_size: 6731\n",
      "batch 579, loss: 0.2320, instance_loss: 0.0416, weighted_loss: 0.1749, label: 1, bag_size: 5561\n",
      "batch 599, loss: 0.1303, instance_loss: 0.0642, weighted_loss: 0.1104, label: 0, bag_size: 1452\n",
      "batch 619, loss: 0.1372, instance_loss: 0.0079, weighted_loss: 0.0984, label: 0, bag_size: 2457\n",
      "batch 639, loss: 0.0554, instance_loss: 0.0118, weighted_loss: 0.0423, label: 0, bag_size: 19518\n",
      "batch 659, loss: 0.0344, instance_loss: 0.0000, weighted_loss: 0.0241, label: 1, bag_size: 11600\n",
      "batch 679, loss: 0.5287, instance_loss: 0.3069, weighted_loss: 0.4622, label: 0, bag_size: 2160\n",
      "batch 699, loss: 0.0228, instance_loss: 0.0000, weighted_loss: 0.0160, label: 0, bag_size: 11187\n",
      "batch 719, loss: 0.0945, instance_loss: 0.0362, weighted_loss: 0.0770, label: 0, bag_size: 14956\n",
      "batch 739, loss: 0.1519, instance_loss: 0.0009, weighted_loss: 0.1066, label: 0, bag_size: 16087\n",
      "batch 759, loss: 0.0296, instance_loss: 0.0698, weighted_loss: 0.0416, label: 1, bag_size: 3640\n",
      "batch 779, loss: 0.1032, instance_loss: 0.0471, weighted_loss: 0.0864, label: 0, bag_size: 7557\n",
      "batch 799, loss: 0.0255, instance_loss: 0.0128, weighted_loss: 0.0217, label: 1, bag_size: 11981\n",
      "batch 819, loss: 0.0715, instance_loss: 0.0128, weighted_loss: 0.0539, label: 0, bag_size: 21093\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9670731707317073: correct 12688/13120\n",
      "class 1 clustering acc 0.8358231707317073: correct 5483/6560\n",
      "Epoch: 15, train_loss: 0.2688, train_clustering_loss:  0.3233, train_error: 0.1049\n",
      "class 0: acc 0.9029126213592233, correct 372/412\n",
      "class 1: acc 0.8872549019607843, correct 362/408\n",
      "\n",
      "Val Set, val_loss: 0.2616, val_error: 0.1091, auc: 0.9622\n",
      "class 0 clustering acc 0.9556818181818182: correct 1682/1760\n",
      "class 1 clustering acc 0.7670454545454546: correct 675/880\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "Validation loss decreased (0.271571 --> 0.261555).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2849, instance_loss: 3.6628, weighted_loss: 1.2983, label: 1, bag_size: 8191\n",
      "batch 39, loss: 1.5678, instance_loss: 1.9368, weighted_loss: 1.6785, label: 0, bag_size: 21361\n",
      "batch 59, loss: 0.0238, instance_loss: 0.0303, weighted_loss: 0.0257, label: 0, bag_size: 2282\n",
      "batch 79, loss: 1.4741, instance_loss: 1.1869, weighted_loss: 1.3880, label: 1, bag_size: 15185\n",
      "batch 99, loss: 0.0962, instance_loss: 0.0002, weighted_loss: 0.0674, label: 0, bag_size: 8582\n",
      "batch 119, loss: 0.1080, instance_loss: 0.3406, weighted_loss: 0.1778, label: 1, bag_size: 4308\n",
      "batch 139, loss: 0.1421, instance_loss: 0.0777, weighted_loss: 0.1228, label: 1, bag_size: 2495\n",
      "batch 159, loss: 0.0247, instance_loss: 0.0000, weighted_loss: 0.0173, label: 0, bag_size: 8948\n",
      "batch 179, loss: 0.0091, instance_loss: 0.0000, weighted_loss: 0.0064, label: 0, bag_size: 16782\n",
      "batch 199, loss: 0.3743, instance_loss: 2.2414, weighted_loss: 0.9344, label: 1, bag_size: 21450\n",
      "batch 219, loss: 0.0190, instance_loss: 0.0038, weighted_loss: 0.0145, label: 1, bag_size: 8216\n",
      "batch 239, loss: 0.4516, instance_loss: 1.0944, weighted_loss: 0.6445, label: 1, bag_size: 4956\n",
      "batch 259, loss: 0.2585, instance_loss: 0.1689, weighted_loss: 0.2316, label: 1, bag_size: 4956\n",
      "batch 279, loss: 0.0611, instance_loss: 0.0150, weighted_loss: 0.0473, label: 0, bag_size: 10791\n",
      "batch 299, loss: 0.0416, instance_loss: 0.0045, weighted_loss: 0.0305, label: 0, bag_size: 4497\n",
      "batch 319, loss: 4.7301, instance_loss: 3.4854, weighted_loss: 4.3567, label: 0, bag_size: 5105\n",
      "batch 339, loss: 0.1161, instance_loss: 0.2161, weighted_loss: 0.1461, label: 0, bag_size: 1452\n",
      "batch 359, loss: 2.6981, instance_loss: 4.8292, weighted_loss: 3.3374, label: 0, bag_size: 14264\n",
      "batch 379, loss: 0.1261, instance_loss: 0.0018, weighted_loss: 0.0888, label: 0, bag_size: 18215\n",
      "batch 399, loss: 0.4417, instance_loss: 0.0052, weighted_loss: 0.3107, label: 1, bag_size: 16034\n",
      "batch 419, loss: 0.0518, instance_loss: 0.0001, weighted_loss: 0.0363, label: 1, bag_size: 5991\n",
      "batch 439, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 3787\n",
      "batch 459, loss: 0.0027, instance_loss: 0.0000, weighted_loss: 0.0019, label: 0, bag_size: 23996\n",
      "batch 479, loss: 0.1177, instance_loss: 0.0000, weighted_loss: 0.0824, label: 1, bag_size: 16162\n",
      "batch 499, loss: 0.0108, instance_loss: 0.0002, weighted_loss: 0.0076, label: 0, bag_size: 9433\n",
      "batch 519, loss: 0.1273, instance_loss: 0.3566, weighted_loss: 0.1961, label: 1, bag_size: 2146\n",
      "batch 539, loss: 0.1233, instance_loss: 0.0632, weighted_loss: 0.1053, label: 0, bag_size: 8788\n",
      "batch 559, loss: 2.5762, instance_loss: 2.2894, weighted_loss: 2.4902, label: 0, bag_size: 11306\n",
      "batch 579, loss: 0.1826, instance_loss: 0.5356, weighted_loss: 0.2885, label: 1, bag_size: 8982\n",
      "batch 599, loss: 0.1949, instance_loss: 0.0666, weighted_loss: 0.1564, label: 0, bag_size: 7011\n",
      "batch 619, loss: 0.4085, instance_loss: 0.8129, weighted_loss: 0.5299, label: 1, bag_size: 1038\n",
      "batch 639, loss: 1.9624, instance_loss: 1.9135, weighted_loss: 1.9477, label: 0, bag_size: 2179\n",
      "batch 659, loss: 0.3377, instance_loss: 0.0030, weighted_loss: 0.2373, label: 0, bag_size: 2548\n",
      "batch 679, loss: 0.0344, instance_loss: 0.1187, weighted_loss: 0.0597, label: 0, bag_size: 13795\n",
      "batch 699, loss: 0.5260, instance_loss: 0.8059, weighted_loss: 0.6100, label: 0, bag_size: 6624\n",
      "batch 719, loss: 0.1404, instance_loss: 0.2961, weighted_loss: 0.1871, label: 0, bag_size: 2382\n",
      "batch 739, loss: 0.0132, instance_loss: 0.0000, weighted_loss: 0.0092, label: 0, bag_size: 14681\n",
      "batch 759, loss: 0.1472, instance_loss: 0.1335, weighted_loss: 0.1431, label: 1, bag_size: 5690\n",
      "batch 779, loss: 0.6155, instance_loss: 0.0519, weighted_loss: 0.4464, label: 1, bag_size: 13365\n",
      "batch 799, loss: 0.0322, instance_loss: 0.0409, weighted_loss: 0.0348, label: 0, bag_size: 11654\n",
      "batch 819, loss: 0.2609, instance_loss: 0.0217, weighted_loss: 0.1892, label: 1, bag_size: 2678\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9692073170731708: correct 12716/13120\n",
      "class 1 clustering acc 0.8263719512195122: correct 5421/6560\n",
      "Epoch: 16, train_loss: 0.3069, train_clustering_loss:  0.3102, train_error: 0.1280\n",
      "class 0: acc 0.8823529411764706, correct 375/425\n",
      "class 1: acc 0.8607594936708861, correct 340/395\n",
      "\n",
      "Val Set, val_loss: 0.2933, val_error: 0.1273, auc: 0.9622\n",
      "class 0 clustering acc 0.9568181818181818: correct 1684/1760\n",
      "class 1 clustering acc 0.7795454545454545: correct 686/880\n",
      "class 0: acc 0.7692307692307693, correct 40/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0574, instance_loss: 0.0021, weighted_loss: 0.0408, label: 0, bag_size: 14681\n",
      "batch 39, loss: 0.0578, instance_loss: 0.1715, weighted_loss: 0.0919, label: 1, bag_size: 5137\n",
      "batch 59, loss: 0.0889, instance_loss: 0.0585, weighted_loss: 0.0798, label: 0, bag_size: 1072\n",
      "batch 79, loss: 0.4882, instance_loss: 0.8457, weighted_loss: 0.5955, label: 0, bag_size: 13332\n",
      "batch 99, loss: 0.0485, instance_loss: 0.1005, weighted_loss: 0.0641, label: 1, bag_size: 3683\n",
      "batch 119, loss: 0.1445, instance_loss: 0.4424, weighted_loss: 0.2339, label: 1, bag_size: 8982\n",
      "batch 139, loss: 0.7364, instance_loss: 0.7996, weighted_loss: 0.7554, label: 0, bag_size: 3670\n",
      "batch 159, loss: 0.0118, instance_loss: 0.1724, weighted_loss: 0.0600, label: 1, bag_size: 10033\n",
      "batch 179, loss: 0.4120, instance_loss: 0.0000, weighted_loss: 0.2884, label: 0, bag_size: 65728\n",
      "batch 199, loss: 0.2320, instance_loss: 0.5039, weighted_loss: 0.3136, label: 0, bag_size: 5999\n",
      "batch 219, loss: 0.0188, instance_loss: 0.0000, weighted_loss: 0.0132, label: 0, bag_size: 22426\n",
      "batch 239, loss: 0.2616, instance_loss: 0.3503, weighted_loss: 0.2882, label: 0, bag_size: 1684\n",
      "batch 259, loss: 0.0683, instance_loss: 0.0000, weighted_loss: 0.0478, label: 1, bag_size: 2146\n",
      "batch 279, loss: 0.0361, instance_loss: 0.1475, weighted_loss: 0.0695, label: 1, bag_size: 2385\n",
      "batch 299, loss: 0.7411, instance_loss: 0.8497, weighted_loss: 0.7737, label: 1, bag_size: 1437\n",
      "batch 319, loss: 0.0168, instance_loss: 0.5639, weighted_loss: 0.1809, label: 1, bag_size: 4442\n",
      "batch 339, loss: 0.7817, instance_loss: 0.1214, weighted_loss: 0.5836, label: 0, bag_size: 26208\n",
      "batch 359, loss: 0.0905, instance_loss: 0.0317, weighted_loss: 0.0729, label: 1, bag_size: 14887\n",
      "batch 379, loss: 0.0311, instance_loss: 0.0859, weighted_loss: 0.0475, label: 0, bag_size: 11259\n",
      "batch 399, loss: 0.0908, instance_loss: 0.1743, weighted_loss: 0.1159, label: 0, bag_size: 14249\n",
      "batch 419, loss: 0.1159, instance_loss: 0.1526, weighted_loss: 0.1269, label: 1, bag_size: 12408\n",
      "batch 439, loss: 0.0715, instance_loss: 0.1248, weighted_loss: 0.0875, label: 0, bag_size: 11125\n",
      "batch 459, loss: 0.8816, instance_loss: 0.6553, weighted_loss: 0.8137, label: 0, bag_size: 1684\n",
      "batch 479, loss: 0.0816, instance_loss: 0.2554, weighted_loss: 0.1338, label: 1, bag_size: 1822\n",
      "batch 499, loss: 0.2023, instance_loss: 0.5414, weighted_loss: 0.3040, label: 0, bag_size: 2844\n",
      "batch 519, loss: 0.0914, instance_loss: 0.3243, weighted_loss: 0.1612, label: 0, bag_size: 1797\n",
      "batch 539, loss: 0.0691, instance_loss: 2.1273, weighted_loss: 0.6866, label: 0, bag_size: 2091\n",
      "batch 559, loss: 0.0665, instance_loss: 0.2652, weighted_loss: 0.1261, label: 0, bag_size: 1458\n",
      "batch 579, loss: 0.2349, instance_loss: 0.0839, weighted_loss: 0.1896, label: 1, bag_size: 21450\n",
      "batch 599, loss: 0.7617, instance_loss: 0.2872, weighted_loss: 0.6193, label: 0, bag_size: 15898\n",
      "batch 619, loss: 0.0336, instance_loss: 0.0008, weighted_loss: 0.0238, label: 0, bag_size: 8959\n",
      "batch 639, loss: 0.0906, instance_loss: 0.0010, weighted_loss: 0.0637, label: 0, bag_size: 8959\n",
      "batch 659, loss: 0.0186, instance_loss: 0.0025, weighted_loss: 0.0138, label: 1, bag_size: 8216\n",
      "batch 679, loss: 0.0108, instance_loss: 0.0011, weighted_loss: 0.0079, label: 1, bag_size: 11875\n",
      "batch 699, loss: 0.3086, instance_loss: 0.0033, weighted_loss: 0.2170, label: 0, bag_size: 9597\n",
      "batch 719, loss: 0.0532, instance_loss: 0.8675, weighted_loss: 0.2975, label: 0, bag_size: 1149\n",
      "batch 739, loss: 0.1441, instance_loss: 0.0212, weighted_loss: 0.1072, label: 0, bag_size: 8330\n",
      "batch 759, loss: 0.7567, instance_loss: 1.8478, weighted_loss: 1.0840, label: 1, bag_size: 1493\n",
      "batch 779, loss: 0.3066, instance_loss: 0.4375, weighted_loss: 0.3458, label: 1, bag_size: 4956\n",
      "batch 799, loss: 0.0631, instance_loss: 0.1961, weighted_loss: 0.1030, label: 0, bag_size: 4497\n",
      "batch 819, loss: 4.6000, instance_loss: 1.4348, weighted_loss: 3.6505, label: 1, bag_size: 2565\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9642530487804878: correct 12651/13120\n",
      "class 1 clustering acc 0.7708841463414634: correct 5057/6560\n",
      "Epoch: 17, train_loss: 0.2978, train_clustering_loss:  0.4098, train_error: 0.1110\n",
      "class 0: acc 0.8948655256723717, correct 366/409\n",
      "class 1: acc 0.8832116788321168, correct 363/411\n",
      "\n",
      "Val Set, val_loss: 0.2785, val_error: 0.1091, auc: 0.9629\n",
      "class 0 clustering acc 0.95625: correct 1683/1760\n",
      "class 1 clustering acc 0.7340909090909091: correct 646/880\n",
      "class 0: acc 0.8076923076923077, correct 42/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0846, instance_loss: 0.1751, weighted_loss: 0.1118, label: 0, bag_size: 1614\n",
      "batch 39, loss: 0.0527, instance_loss: 0.0632, weighted_loss: 0.0558, label: 1, bag_size: 5833\n",
      "batch 59, loss: 1.0934, instance_loss: 0.4583, weighted_loss: 0.9028, label: 1, bag_size: 1819\n",
      "batch 79, loss: 0.3184, instance_loss: 0.3964, weighted_loss: 0.3418, label: 1, bag_size: 1746\n",
      "batch 99, loss: 0.0149, instance_loss: 0.0003, weighted_loss: 0.0106, label: 0, bag_size: 10898\n",
      "batch 119, loss: 0.0535, instance_loss: 0.0563, weighted_loss: 0.0544, label: 1, bag_size: 9078\n",
      "batch 139, loss: 0.4757, instance_loss: 0.5731, weighted_loss: 0.5049, label: 0, bag_size: 6356\n",
      "batch 159, loss: 0.1291, instance_loss: 0.0714, weighted_loss: 0.1118, label: 1, bag_size: 2356\n",
      "batch 179, loss: 0.0088, instance_loss: 0.0010, weighted_loss: 0.0065, label: 1, bag_size: 2638\n",
      "batch 199, loss: 0.0409, instance_loss: 0.1949, weighted_loss: 0.0871, label: 1, bag_size: 19606\n",
      "batch 219, loss: 0.0465, instance_loss: 0.0720, weighted_loss: 0.0542, label: 0, bag_size: 2624\n",
      "batch 239, loss: 0.2606, instance_loss: 0.1815, weighted_loss: 0.2369, label: 0, bag_size: 10381\n",
      "batch 259, loss: 0.2419, instance_loss: 0.0413, weighted_loss: 0.1817, label: 1, bag_size: 8103\n",
      "batch 279, loss: 0.0931, instance_loss: 0.1740, weighted_loss: 0.1174, label: 0, bag_size: 2628\n",
      "batch 299, loss: 0.0741, instance_loss: 0.1565, weighted_loss: 0.0988, label: 1, bag_size: 11223\n",
      "batch 319, loss: 0.0320, instance_loss: 0.2798, weighted_loss: 0.1064, label: 0, bag_size: 9866\n",
      "batch 339, loss: 0.0783, instance_loss: 0.1302, weighted_loss: 0.0939, label: 0, bag_size: 2043\n",
      "batch 359, loss: 0.0555, instance_loss: 0.3913, weighted_loss: 0.1562, label: 0, bag_size: 19043\n",
      "batch 379, loss: 0.0688, instance_loss: 0.0009, weighted_loss: 0.0484, label: 1, bag_size: 8003\n",
      "batch 399, loss: 0.0701, instance_loss: 0.3088, weighted_loss: 0.1417, label: 0, bag_size: 12149\n",
      "batch 419, loss: 0.6398, instance_loss: 0.3368, weighted_loss: 0.5489, label: 1, bag_size: 1764\n",
      "batch 439, loss: 0.0447, instance_loss: 0.0013, weighted_loss: 0.0317, label: 0, bag_size: 7823\n",
      "batch 459, loss: 0.1257, instance_loss: 0.0000, weighted_loss: 0.0880, label: 1, bag_size: 2308\n",
      "batch 479, loss: 0.0458, instance_loss: 0.0403, weighted_loss: 0.0442, label: 0, bag_size: 19067\n",
      "batch 499, loss: 0.0282, instance_loss: 1.0075, weighted_loss: 0.3220, label: 0, bag_size: 1639\n",
      "batch 519, loss: 1.5299, instance_loss: 1.3050, weighted_loss: 1.4624, label: 1, bag_size: 771\n",
      "batch 539, loss: 0.3125, instance_loss: 1.0936, weighted_loss: 0.5469, label: 0, bag_size: 9616\n",
      "batch 559, loss: 0.0197, instance_loss: 0.5986, weighted_loss: 0.1934, label: 1, bag_size: 699\n",
      "batch 579, loss: 0.0938, instance_loss: 0.0143, weighted_loss: 0.0700, label: 0, bag_size: 2367\n",
      "batch 599, loss: 0.3544, instance_loss: 0.5635, weighted_loss: 0.4171, label: 0, bag_size: 1714\n",
      "batch 619, loss: 0.0224, instance_loss: 0.0013, weighted_loss: 0.0161, label: 1, bag_size: 15689\n",
      "batch 639, loss: 0.1463, instance_loss: 0.0115, weighted_loss: 0.1059, label: 0, bag_size: 18954\n",
      "batch 659, loss: 0.0232, instance_loss: 0.0044, weighted_loss: 0.0176, label: 1, bag_size: 5629\n",
      "batch 679, loss: 0.2088, instance_loss: 0.4520, weighted_loss: 0.2818, label: 0, bag_size: 9596\n",
      "batch 699, loss: 0.0303, instance_loss: 0.0150, weighted_loss: 0.0257, label: 1, bag_size: 3683\n",
      "batch 719, loss: 2.4236, instance_loss: 1.1407, weighted_loss: 2.0387, label: 0, bag_size: 2242\n",
      "batch 739, loss: 0.7838, instance_loss: 0.7532, weighted_loss: 0.7747, label: 1, bag_size: 2731\n",
      "batch 759, loss: 0.0621, instance_loss: 0.0482, weighted_loss: 0.0579, label: 0, bag_size: 13880\n",
      "batch 779, loss: 0.0401, instance_loss: 0.0026, weighted_loss: 0.0288, label: 0, bag_size: 13591\n",
      "batch 799, loss: 0.2115, instance_loss: 0.0000, weighted_loss: 0.1481, label: 0, bag_size: 6281\n",
      "batch 819, loss: 0.4029, instance_loss: 1.0633, weighted_loss: 0.6010, label: 0, bag_size: 9132\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9714939024390243: correct 12746/13120\n",
      "class 1 clustering acc 0.7945121951219513: correct 5212/6560\n",
      "Epoch: 18, train_loss: 0.2458, train_clustering_loss:  0.3299, train_error: 0.0939\n",
      "class 0: acc 0.9051282051282051, correct 353/390\n",
      "class 1: acc 0.9069767441860465, correct 390/430\n",
      "\n",
      "Val Set, val_loss: 0.2510, val_error: 0.1091, auc: 0.9645\n",
      "class 0 clustering acc 0.946590909090909: correct 1666/1760\n",
      "class 1 clustering acc 0.7113636363636363: correct 626/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8275862068965517, correct 48/58\n",
      "Validation loss decreased (0.261555 --> 0.250978).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0615, instance_loss: 0.1774, weighted_loss: 0.0963, label: 0, bag_size: 8582\n",
      "batch 39, loss: 5.4148, instance_loss: 3.1674, weighted_loss: 4.7406, label: 0, bag_size: 5105\n",
      "batch 59, loss: 0.0864, instance_loss: 0.0052, weighted_loss: 0.0620, label: 1, bag_size: 19606\n",
      "batch 79, loss: 0.0293, instance_loss: 0.1304, weighted_loss: 0.0596, label: 0, bag_size: 9471\n",
      "batch 99, loss: 0.0363, instance_loss: 0.1459, weighted_loss: 0.0692, label: 1, bag_size: 2904\n",
      "batch 119, loss: 0.0055, instance_loss: 0.0012, weighted_loss: 0.0042, label: 1, bag_size: 16417\n",
      "batch 139, loss: 0.1600, instance_loss: 1.0764, weighted_loss: 0.4349, label: 1, bag_size: 1255\n",
      "batch 159, loss: 0.0078, instance_loss: 0.0013, weighted_loss: 0.0058, label: 0, bag_size: 32227\n",
      "batch 179, loss: 0.4060, instance_loss: 1.3613, weighted_loss: 0.6926, label: 0, bag_size: 10410\n",
      "batch 199, loss: 1.5015, instance_loss: 2.2383, weighted_loss: 1.7225, label: 1, bag_size: 15185\n",
      "batch 219, loss: 0.1978, instance_loss: 0.4309, weighted_loss: 0.2677, label: 1, bag_size: 5292\n",
      "batch 239, loss: 0.2548, instance_loss: 0.0033, weighted_loss: 0.1793, label: 0, bag_size: 2548\n",
      "batch 259, loss: 0.0138, instance_loss: 0.0006, weighted_loss: 0.0098, label: 0, bag_size: 11735\n",
      "batch 279, loss: 0.3827, instance_loss: 0.8320, weighted_loss: 0.5175, label: 0, bag_size: 6356\n",
      "batch 299, loss: 4.6469, instance_loss: 1.0717, weighted_loss: 3.5744, label: 0, bag_size: 5105\n",
      "batch 319, loss: 1.0996, instance_loss: 0.3504, weighted_loss: 0.8748, label: 1, bag_size: 1123\n",
      "batch 339, loss: 0.0214, instance_loss: 0.0000, weighted_loss: 0.0150, label: 1, bag_size: 12575\n",
      "batch 359, loss: 0.3372, instance_loss: 1.6143, weighted_loss: 0.7204, label: 1, bag_size: 1051\n",
      "batch 379, loss: 4.1021, instance_loss: 0.4987, weighted_loss: 3.0211, label: 1, bag_size: 2565\n",
      "batch 399, loss: 0.2928, instance_loss: 0.0035, weighted_loss: 0.2060, label: 0, bag_size: 47866\n",
      "batch 419, loss: 1.3204, instance_loss: 1.6826, weighted_loss: 1.4290, label: 1, bag_size: 9162\n",
      "batch 439, loss: 0.0819, instance_loss: 1.0673, weighted_loss: 0.3775, label: 0, bag_size: 3474\n",
      "batch 459, loss: 0.0595, instance_loss: 0.0031, weighted_loss: 0.0426, label: 0, bag_size: 12687\n",
      "batch 479, loss: 0.0176, instance_loss: 0.0000, weighted_loss: 0.0123, label: 1, bag_size: 17486\n",
      "batch 499, loss: 0.0148, instance_loss: 0.2010, weighted_loss: 0.0706, label: 0, bag_size: 1234\n",
      "batch 519, loss: 0.0161, instance_loss: 0.0051, weighted_loss: 0.0128, label: 1, bag_size: 5025\n",
      "batch 539, loss: 0.0279, instance_loss: 0.1106, weighted_loss: 0.0527, label: 0, bag_size: 1458\n",
      "batch 559, loss: 0.6998, instance_loss: 2.3887, weighted_loss: 1.2064, label: 1, bag_size: 10072\n",
      "batch 579, loss: 0.0083, instance_loss: 0.0029, weighted_loss: 0.0067, label: 1, bag_size: 5833\n",
      "batch 599, loss: 0.0602, instance_loss: 0.3419, weighted_loss: 0.1447, label: 1, bag_size: 4394\n",
      "batch 619, loss: 0.1371, instance_loss: 0.0017, weighted_loss: 0.0965, label: 0, bag_size: 6898\n",
      "batch 639, loss: 0.0452, instance_loss: 0.2790, weighted_loss: 0.1153, label: 0, bag_size: 1458\n",
      "batch 659, loss: 0.0065, instance_loss: 0.0007, weighted_loss: 0.0047, label: 0, bag_size: 11778\n",
      "batch 679, loss: 0.3924, instance_loss: 0.3285, weighted_loss: 0.3732, label: 0, bag_size: 1438\n",
      "batch 699, loss: 0.1075, instance_loss: 0.1859, weighted_loss: 0.1311, label: 1, bag_size: 6736\n",
      "batch 719, loss: 2.0769, instance_loss: 1.6279, weighted_loss: 1.9422, label: 1, bag_size: 3879\n",
      "batch 739, loss: 0.0239, instance_loss: 0.0515, weighted_loss: 0.0322, label: 0, bag_size: 10263\n",
      "batch 759, loss: 0.0162, instance_loss: 0.0003, weighted_loss: 0.0115, label: 0, bag_size: 11727\n",
      "batch 779, loss: 0.1688, instance_loss: 0.3835, weighted_loss: 0.2332, label: 1, bag_size: 2356\n",
      "batch 799, loss: 0.1924, instance_loss: 0.4105, weighted_loss: 0.2578, label: 0, bag_size: 8959\n",
      "batch 819, loss: 0.0084, instance_loss: 0.4076, weighted_loss: 0.1282, label: 0, bag_size: 1712\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9634908536585366: correct 12641/13120\n",
      "class 1 clustering acc 0.7669207317073171: correct 5031/6560\n",
      "Epoch: 19, train_loss: 0.3286, train_clustering_loss:  0.3691, train_error: 0.1207\n",
      "class 0: acc 0.8835443037974684, correct 349/395\n",
      "class 1: acc 0.8752941176470588, correct 372/425\n",
      "\n",
      "Val Set, val_loss: 0.2659, val_error: 0.1182, auc: 0.9645\n",
      "class 0 clustering acc 0.9636363636363636: correct 1696/1760\n",
      "class 1 clustering acc 0.6875: correct 605/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8103448275862069, correct 47/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5282, instance_loss: 0.2345, weighted_loss: 0.4401, label: 1, bag_size: 2842\n",
      "batch 39, loss: 0.6872, instance_loss: 0.1555, weighted_loss: 0.5277, label: 0, bag_size: 3654\n",
      "batch 59, loss: 0.7327, instance_loss: 3.7635, weighted_loss: 1.6419, label: 0, bag_size: 14664\n",
      "batch 79, loss: 0.0713, instance_loss: 0.2619, weighted_loss: 0.1285, label: 1, bag_size: 7935\n",
      "batch 99, loss: 0.0654, instance_loss: 0.2123, weighted_loss: 0.1095, label: 1, bag_size: 9689\n",
      "batch 119, loss: 0.0704, instance_loss: 0.4124, weighted_loss: 0.1730, label: 1, bag_size: 12460\n",
      "batch 139, loss: 0.1025, instance_loss: 0.1336, weighted_loss: 0.1118, label: 1, bag_size: 12408\n",
      "batch 159, loss: 0.0662, instance_loss: 0.2487, weighted_loss: 0.1210, label: 0, bag_size: 2624\n",
      "batch 179, loss: 0.5587, instance_loss: 0.2375, weighted_loss: 0.4623, label: 0, bag_size: 3654\n",
      "batch 199, loss: 0.0480, instance_loss: 0.3331, weighted_loss: 0.1335, label: 1, bag_size: 621\n",
      "batch 219, loss: 0.5605, instance_loss: 0.1136, weighted_loss: 0.4264, label: 0, bag_size: 9069\n",
      "batch 239, loss: 0.0094, instance_loss: 0.0839, weighted_loss: 0.0317, label: 0, bag_size: 11527\n",
      "batch 259, loss: 0.9920, instance_loss: 0.2598, weighted_loss: 0.7723, label: 0, bag_size: 9616\n",
      "batch 279, loss: 0.0243, instance_loss: 0.3629, weighted_loss: 0.1259, label: 1, bag_size: 621\n",
      "batch 299, loss: 0.0192, instance_loss: 0.1741, weighted_loss: 0.0657, label: 0, bag_size: 9930\n",
      "batch 319, loss: 0.0631, instance_loss: 0.0116, weighted_loss: 0.0477, label: 1, bag_size: 4394\n",
      "batch 339, loss: 0.0676, instance_loss: 0.0000, weighted_loss: 0.0473, label: 1, bag_size: 11729\n",
      "batch 359, loss: 0.8825, instance_loss: 1.4397, weighted_loss: 1.0496, label: 1, bag_size: 1963\n",
      "batch 379, loss: 0.0242, instance_loss: 0.0003, weighted_loss: 0.0170, label: 1, bag_size: 16154\n",
      "batch 399, loss: 0.0688, instance_loss: 0.1518, weighted_loss: 0.0937, label: 1, bag_size: 5894\n",
      "batch 419, loss: 0.0308, instance_loss: 0.1726, weighted_loss: 0.0733, label: 0, bag_size: 2920\n",
      "batch 439, loss: 0.0773, instance_loss: 0.2545, weighted_loss: 0.1305, label: 0, bag_size: 19518\n",
      "batch 459, loss: 0.1205, instance_loss: 0.1266, weighted_loss: 0.1223, label: 1, bag_size: 7613\n",
      "batch 479, loss: 0.0055, instance_loss: 0.6590, weighted_loss: 0.2016, label: 0, bag_size: 15077\n",
      "batch 499, loss: 0.5345, instance_loss: 0.1885, weighted_loss: 0.4307, label: 0, bag_size: 2548\n",
      "batch 519, loss: 0.9257, instance_loss: 0.3231, weighted_loss: 0.7449, label: 0, bag_size: 3654\n",
      "batch 539, loss: 0.5192, instance_loss: 0.3573, weighted_loss: 0.4706, label: 1, bag_size: 5160\n",
      "batch 559, loss: 0.8396, instance_loss: 0.0042, weighted_loss: 0.5890, label: 0, bag_size: 5211\n",
      "batch 579, loss: 2.0152, instance_loss: 2.1367, weighted_loss: 2.0516, label: 1, bag_size: 12712\n",
      "batch 599, loss: 0.0936, instance_loss: 0.0454, weighted_loss: 0.0792, label: 0, bag_size: 23714\n",
      "batch 619, loss: 1.7517, instance_loss: 0.4212, weighted_loss: 1.3525, label: 0, bag_size: 2219\n",
      "batch 639, loss: 0.8267, instance_loss: 0.9810, weighted_loss: 0.8730, label: 0, bag_size: 3375\n",
      "batch 659, loss: 0.1290, instance_loss: 0.0061, weighted_loss: 0.0921, label: 0, bag_size: 10381\n",
      "batch 679, loss: 0.0738, instance_loss: 0.0682, weighted_loss: 0.0721, label: 0, bag_size: 11187\n",
      "batch 699, loss: 0.0049, instance_loss: 0.2597, weighted_loss: 0.0813, label: 0, bag_size: 15747\n",
      "batch 719, loss: 0.0010, instance_loss: 0.0739, weighted_loss: 0.0229, label: 0, bag_size: 16782\n",
      "batch 739, loss: 0.3118, instance_loss: 0.0833, weighted_loss: 0.2432, label: 0, bag_size: 1684\n",
      "batch 759, loss: 0.0559, instance_loss: 0.2248, weighted_loss: 0.1065, label: 0, bag_size: 3474\n",
      "batch 779, loss: 0.0259, instance_loss: 0.0758, weighted_loss: 0.0409, label: 0, bag_size: 15967\n",
      "batch 799, loss: 0.0543, instance_loss: 0.0008, weighted_loss: 0.0382, label: 1, bag_size: 13732\n",
      "batch 819, loss: 0.0944, instance_loss: 0.1185, weighted_loss: 0.1016, label: 0, bag_size: 5551\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9612042682926829: correct 12611/13120\n",
      "class 1 clustering acc 0.8217987804878049: correct 5391/6560\n",
      "Epoch: 20, train_loss: 0.2884, train_clustering_loss:  0.3676, train_error: 0.1110\n",
      "class 0: acc 0.8768844221105527, correct 349/398\n",
      "class 1: acc 0.9004739336492891, correct 380/422\n",
      "\n",
      "Val Set, val_loss: 0.2546, val_error: 0.1000, auc: 0.9662\n",
      "class 0 clustering acc 0.94375: correct 1661/1760\n",
      "class 1 clustering acc 0.7625: correct 671/880\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0345, instance_loss: 0.0484, weighted_loss: 0.0387, label: 1, bag_size: 14202\n",
      "batch 39, loss: 0.0331, instance_loss: 0.0000, weighted_loss: 0.0232, label: 1, bag_size: 18468\n",
      "batch 59, loss: 0.0048, instance_loss: 0.0439, weighted_loss: 0.0165, label: 1, bag_size: 629\n",
      "batch 79, loss: 0.0433, instance_loss: 0.0365, weighted_loss: 0.0412, label: 0, bag_size: 11917\n",
      "batch 99, loss: 0.0252, instance_loss: 1.2287, weighted_loss: 0.3862, label: 1, bag_size: 7119\n",
      "batch 119, loss: 0.2586, instance_loss: 0.4487, weighted_loss: 0.3156, label: 0, bag_size: 2351\n",
      "batch 139, loss: 0.0319, instance_loss: 0.2776, weighted_loss: 0.1056, label: 0, bag_size: 9252\n",
      "batch 159, loss: 0.4648, instance_loss: 1.1515, weighted_loss: 0.6708, label: 1, bag_size: 1746\n",
      "batch 179, loss: 0.0067, instance_loss: 0.0227, weighted_loss: 0.0115, label: 0, bag_size: 16992\n",
      "batch 199, loss: 0.0704, instance_loss: 0.0860, weighted_loss: 0.0751, label: 0, bag_size: 17155\n",
      "batch 219, loss: 0.0792, instance_loss: 0.0791, weighted_loss: 0.0792, label: 0, bag_size: 1760\n",
      "batch 239, loss: 1.4358, instance_loss: 2.4021, weighted_loss: 1.7257, label: 1, bag_size: 12494\n",
      "batch 259, loss: 0.0039, instance_loss: 0.2845, weighted_loss: 0.0881, label: 0, bag_size: 14266\n",
      "batch 279, loss: 0.0407, instance_loss: 0.0039, weighted_loss: 0.0297, label: 1, bag_size: 9062\n",
      "batch 299, loss: 0.1144, instance_loss: 0.0372, weighted_loss: 0.0912, label: 0, bag_size: 2652\n",
      "batch 319, loss: 0.0499, instance_loss: 0.0018, weighted_loss: 0.0355, label: 1, bag_size: 4259\n",
      "batch 339, loss: 1.4135, instance_loss: 3.0341, weighted_loss: 1.8997, label: 1, bag_size: 1497\n",
      "batch 359, loss: 0.0139, instance_loss: 0.0014, weighted_loss: 0.0101, label: 1, bag_size: 10105\n",
      "batch 379, loss: 0.0124, instance_loss: 0.0062, weighted_loss: 0.0105, label: 1, bag_size: 3224\n",
      "batch 399, loss: 0.0073, instance_loss: 1.8548, weighted_loss: 0.5615, label: 1, bag_size: 2140\n",
      "batch 419, loss: 0.1323, instance_loss: 0.6693, weighted_loss: 0.2934, label: 1, bag_size: 5160\n",
      "batch 439, loss: 0.0245, instance_loss: 0.0001, weighted_loss: 0.0172, label: 1, bag_size: 9408\n",
      "batch 459, loss: 0.0712, instance_loss: 0.0062, weighted_loss: 0.0517, label: 1, bag_size: 10105\n",
      "batch 479, loss: 0.0062, instance_loss: 0.3479, weighted_loss: 0.1087, label: 1, bag_size: 629\n",
      "batch 499, loss: 0.0043, instance_loss: 0.0015, weighted_loss: 0.0034, label: 1, bag_size: 13015\n",
      "batch 519, loss: 0.0246, instance_loss: 0.0090, weighted_loss: 0.0199, label: 0, bag_size: 5225\n",
      "batch 539, loss: 0.1458, instance_loss: 0.0009, weighted_loss: 0.1023, label: 1, bag_size: 7389\n",
      "batch 559, loss: 1.3898, instance_loss: 1.5926, weighted_loss: 1.4507, label: 0, bag_size: 2653\n",
      "batch 579, loss: 0.0440, instance_loss: 0.1198, weighted_loss: 0.0667, label: 0, bag_size: 3474\n",
      "batch 599, loss: 0.0157, instance_loss: 0.1131, weighted_loss: 0.0449, label: 0, bag_size: 2360\n",
      "batch 619, loss: 0.0100, instance_loss: 0.1781, weighted_loss: 0.0604, label: 0, bag_size: 890\n",
      "batch 639, loss: 0.3696, instance_loss: 0.0930, weighted_loss: 0.2866, label: 0, bag_size: 9387\n",
      "batch 659, loss: 0.0307, instance_loss: 0.0014, weighted_loss: 0.0219, label: 1, bag_size: 11964\n",
      "batch 679, loss: 0.3495, instance_loss: 0.0397, weighted_loss: 0.2566, label: 1, bag_size: 16514\n",
      "batch 699, loss: 0.1118, instance_loss: 0.0065, weighted_loss: 0.0802, label: 1, bag_size: 5256\n",
      "batch 719, loss: 0.0837, instance_loss: 0.0032, weighted_loss: 0.0595, label: 1, bag_size: 2522\n",
      "batch 739, loss: 0.3562, instance_loss: 0.0230, weighted_loss: 0.2563, label: 1, bag_size: 10460\n",
      "batch 759, loss: 0.0030, instance_loss: 0.0027, weighted_loss: 0.0029, label: 1, bag_size: 3453\n",
      "batch 779, loss: 0.0061, instance_loss: 0.0195, weighted_loss: 0.0101, label: 1, bag_size: 8522\n",
      "batch 799, loss: 0.0064, instance_loss: 0.0307, weighted_loss: 0.0137, label: 0, bag_size: 9433\n",
      "batch 819, loss: 0.4551, instance_loss: 0.1501, weighted_loss: 0.3636, label: 0, bag_size: 10029\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9655487804878049: correct 12668/13120\n",
      "class 1 clustering acc 0.848170731707317: correct 5564/6560\n",
      "Epoch: 21, train_loss: 0.2618, train_clustering_loss:  0.3105, train_error: 0.0963\n",
      "class 0: acc 0.913151364764268, correct 368/403\n",
      "class 1: acc 0.894484412470024, correct 373/417\n",
      "\n",
      "Val Set, val_loss: 0.2578, val_error: 0.1091, auc: 0.9678\n",
      "class 0 clustering acc 0.9346590909090909: correct 1645/1760\n",
      "class 1 clustering acc 0.7795454545454545: correct 686/880\n",
      "class 0: acc 0.8076923076923077, correct 42/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5495, instance_loss: 0.5187, weighted_loss: 0.5403, label: 0, bag_size: 9132\n",
      "batch 39, loss: 4.1361, instance_loss: 0.9539, weighted_loss: 3.1814, label: 0, bag_size: 5105\n",
      "batch 59, loss: 1.1079, instance_loss: 0.0610, weighted_loss: 0.7938, label: 1, bag_size: 2455\n",
      "batch 79, loss: 0.0426, instance_loss: 0.0064, weighted_loss: 0.0317, label: 1, bag_size: 12603\n",
      "batch 99, loss: 0.2403, instance_loss: 0.0613, weighted_loss: 0.1866, label: 1, bag_size: 11220\n",
      "batch 119, loss: 0.0133, instance_loss: 0.0001, weighted_loss: 0.0094, label: 1, bag_size: 11266\n",
      "batch 139, loss: 1.1366, instance_loss: 0.0166, weighted_loss: 0.8006, label: 0, bag_size: 18516\n",
      "batch 159, loss: 0.0473, instance_loss: 0.7302, weighted_loss: 0.2522, label: 0, bag_size: 803\n",
      "batch 179, loss: 1.8122, instance_loss: 2.1673, weighted_loss: 1.9187, label: 1, bag_size: 6360\n",
      "batch 199, loss: 0.3588, instance_loss: 0.0518, weighted_loss: 0.2667, label: 1, bag_size: 7669\n",
      "batch 219, loss: 0.1153, instance_loss: 0.0054, weighted_loss: 0.0824, label: 0, bag_size: 8330\n",
      "batch 239, loss: 0.2885, instance_loss: 0.0859, weighted_loss: 0.2277, label: 1, bag_size: 1920\n",
      "batch 259, loss: 0.0095, instance_loss: 0.0000, weighted_loss: 0.0067, label: 0, bag_size: 27158\n",
      "batch 279, loss: 0.0857, instance_loss: 0.1689, weighted_loss: 0.1107, label: 0, bag_size: 2104\n",
      "batch 299, loss: 0.4371, instance_loss: 0.2444, weighted_loss: 0.3793, label: 0, bag_size: 8420\n",
      "batch 319, loss: 0.0445, instance_loss: 0.0069, weighted_loss: 0.0332, label: 1, bag_size: 1459\n",
      "batch 339, loss: 1.5046, instance_loss: 0.8680, weighted_loss: 1.3136, label: 0, bag_size: 1592\n",
      "batch 359, loss: 1.0195, instance_loss: 0.1842, weighted_loss: 0.7689, label: 0, bag_size: 1732\n",
      "batch 379, loss: 0.2215, instance_loss: 0.0028, weighted_loss: 0.1559, label: 1, bag_size: 7613\n",
      "batch 399, loss: 0.0217, instance_loss: 0.0810, weighted_loss: 0.0395, label: 0, bag_size: 10068\n",
      "batch 419, loss: 0.0487, instance_loss: 0.8011, weighted_loss: 0.2744, label: 1, bag_size: 4308\n",
      "batch 439, loss: 0.0419, instance_loss: 0.0553, weighted_loss: 0.0459, label: 0, bag_size: 12212\n",
      "batch 459, loss: 0.0565, instance_loss: 0.0558, weighted_loss: 0.0563, label: 1, bag_size: 18095\n",
      "batch 479, loss: 0.3700, instance_loss: 0.1785, weighted_loss: 0.3125, label: 0, bag_size: 2160\n",
      "batch 499, loss: 0.1982, instance_loss: 0.0787, weighted_loss: 0.1624, label: 1, bag_size: 2137\n",
      "batch 519, loss: 0.0026, instance_loss: 0.0000, weighted_loss: 0.0018, label: 1, bag_size: 12349\n",
      "batch 539, loss: 1.0702, instance_loss: 0.1048, weighted_loss: 0.7806, label: 1, bag_size: 10460\n",
      "batch 559, loss: 0.0571, instance_loss: 0.0902, weighted_loss: 0.0670, label: 1, bag_size: 1015\n",
      "batch 579, loss: 0.1591, instance_loss: 0.6254, weighted_loss: 0.2990, label: 1, bag_size: 1963\n",
      "batch 599, loss: 0.1031, instance_loss: 0.4924, weighted_loss: 0.2199, label: 1, bag_size: 1822\n",
      "batch 619, loss: 0.3784, instance_loss: 0.6099, weighted_loss: 0.4479, label: 1, bag_size: 4786\n",
      "batch 639, loss: 0.0809, instance_loss: 0.9665, weighted_loss: 0.3466, label: 0, bag_size: 12910\n",
      "batch 659, loss: 0.0492, instance_loss: 1.4245, weighted_loss: 0.4618, label: 0, bag_size: 10415\n",
      "batch 679, loss: 0.0277, instance_loss: 0.0178, weighted_loss: 0.0247, label: 1, bag_size: 14433\n",
      "batch 699, loss: 0.0172, instance_loss: 0.0015, weighted_loss: 0.0125, label: 0, bag_size: 22426\n",
      "batch 719, loss: 0.0392, instance_loss: 0.0017, weighted_loss: 0.0280, label: 1, bag_size: 14202\n",
      "batch 739, loss: 0.0454, instance_loss: 0.1556, weighted_loss: 0.0785, label: 0, bag_size: 9415\n",
      "batch 759, loss: 0.7928, instance_loss: 2.6045, weighted_loss: 1.3363, label: 1, bag_size: 9404\n",
      "batch 779, loss: 0.0050, instance_loss: 0.0000, weighted_loss: 0.0035, label: 1, bag_size: 19039\n",
      "batch 799, loss: 0.0393, instance_loss: 0.2244, weighted_loss: 0.0948, label: 0, bag_size: 2609\n",
      "batch 819, loss: 0.9125, instance_loss: 1.8864, weighted_loss: 1.2047, label: 0, bag_size: 1701\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9646341463414634: correct 12656/13120\n",
      "class 1 clustering acc 0.8157012195121951: correct 5351/6560\n",
      "Epoch: 22, train_loss: 0.2911, train_clustering_loss:  0.3444, train_error: 0.1183\n",
      "class 0: acc 0.8817733990147784, correct 358/406\n",
      "class 1: acc 0.8816425120772947, correct 365/414\n",
      "\n",
      "Val Set, val_loss: 0.2385, val_error: 0.1000, auc: 0.9715\n",
      "class 0 clustering acc 0.9340909090909091: correct 1644/1760\n",
      "class 1 clustering acc 0.7659090909090909: correct 674/880\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "Validation loss decreased (0.250978 --> 0.238487).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0372, instance_loss: 0.0424, weighted_loss: 0.0388, label: 1, bag_size: 16565\n",
      "batch 39, loss: 0.0133, instance_loss: 0.0252, weighted_loss: 0.0168, label: 0, bag_size: 20796\n",
      "batch 59, loss: 0.3958, instance_loss: 0.1442, weighted_loss: 0.3203, label: 1, bag_size: 1746\n",
      "batch 79, loss: 0.0336, instance_loss: 0.0008, weighted_loss: 0.0237, label: 0, bag_size: 3228\n",
      "batch 99, loss: 0.0223, instance_loss: 1.9717, weighted_loss: 0.6071, label: 0, bag_size: 1202\n",
      "batch 119, loss: 2.1389, instance_loss: 1.1107, weighted_loss: 1.8304, label: 0, bag_size: 14664\n",
      "batch 139, loss: 0.0789, instance_loss: 0.1135, weighted_loss: 0.0893, label: 0, bag_size: 8788\n",
      "batch 159, loss: 1.0367, instance_loss: 1.1544, weighted_loss: 1.0720, label: 0, bag_size: 2213\n",
      "batch 179, loss: 0.3457, instance_loss: 1.5266, weighted_loss: 0.6999, label: 1, bag_size: 2935\n",
      "batch 199, loss: 0.6877, instance_loss: 0.6865, weighted_loss: 0.6873, label: 0, bag_size: 1142\n",
      "batch 219, loss: 0.4716, instance_loss: 0.0573, weighted_loss: 0.3473, label: 0, bag_size: 23618\n",
      "batch 239, loss: 0.0324, instance_loss: 0.1345, weighted_loss: 0.0630, label: 0, bag_size: 3198\n",
      "batch 259, loss: 0.0496, instance_loss: 0.0006, weighted_loss: 0.0349, label: 1, bag_size: 5340\n",
      "batch 279, loss: 0.0369, instance_loss: 0.0006, weighted_loss: 0.0260, label: 1, bag_size: 29832\n",
      "batch 299, loss: 0.0804, instance_loss: 0.0086, weighted_loss: 0.0589, label: 1, bag_size: 3082\n",
      "batch 319, loss: 0.0034, instance_loss: 0.0007, weighted_loss: 0.0026, label: 0, bag_size: 15914\n",
      "batch 339, loss: 0.0294, instance_loss: 0.0184, weighted_loss: 0.0261, label: 0, bag_size: 16720\n",
      "batch 359, loss: 0.0635, instance_loss: 0.0027, weighted_loss: 0.0453, label: 1, bag_size: 13692\n",
      "batch 379, loss: 0.0047, instance_loss: 0.0000, weighted_loss: 0.0033, label: 1, bag_size: 11884\n",
      "batch 399, loss: 0.0501, instance_loss: 0.0002, weighted_loss: 0.0351, label: 1, bag_size: 7935\n",
      "batch 419, loss: 0.0067, instance_loss: 0.0000, weighted_loss: 0.0047, label: 0, bag_size: 11546\n",
      "batch 439, loss: 0.0575, instance_loss: 0.2801, weighted_loss: 0.1243, label: 0, bag_size: 13777\n",
      "batch 459, loss: 0.0115, instance_loss: 0.0014, weighted_loss: 0.0085, label: 1, bag_size: 699\n",
      "batch 479, loss: 0.3240, instance_loss: 0.2704, weighted_loss: 0.3079, label: 1, bag_size: 8040\n",
      "batch 499, loss: 0.0380, instance_loss: 0.0002, weighted_loss: 0.0267, label: 1, bag_size: 12626\n",
      "batch 519, loss: 0.0055, instance_loss: 0.0000, weighted_loss: 0.0038, label: 0, bag_size: 13892\n",
      "batch 539, loss: 0.0594, instance_loss: 0.0104, weighted_loss: 0.0447, label: 1, bag_size: 12603\n",
      "batch 559, loss: 0.0397, instance_loss: 0.0000, weighted_loss: 0.0278, label: 1, bag_size: 11363\n",
      "batch 579, loss: 0.2713, instance_loss: 0.6457, weighted_loss: 0.3836, label: 1, bag_size: 1525\n",
      "batch 599, loss: 0.8222, instance_loss: 0.7702, weighted_loss: 0.8066, label: 0, bag_size: 1953\n",
      "batch 619, loss: 0.5059, instance_loss: 0.0928, weighted_loss: 0.3820, label: 0, bag_size: 2534\n",
      "batch 639, loss: 0.0024, instance_loss: 0.0019, weighted_loss: 0.0023, label: 0, bag_size: 18225\n",
      "batch 659, loss: 0.0472, instance_loss: 0.0008, weighted_loss: 0.0333, label: 0, bag_size: 6850\n",
      "batch 679, loss: 0.1030, instance_loss: 0.0015, weighted_loss: 0.0725, label: 1, bag_size: 11684\n",
      "batch 699, loss: 0.0113, instance_loss: 0.0000, weighted_loss: 0.0079, label: 0, bag_size: 14319\n",
      "batch 719, loss: 0.0057, instance_loss: 0.0000, weighted_loss: 0.0040, label: 0, bag_size: 11900\n",
      "batch 739, loss: 0.7779, instance_loss: 1.4642, weighted_loss: 0.9838, label: 1, bag_size: 9162\n",
      "batch 759, loss: 0.1209, instance_loss: 0.0282, weighted_loss: 0.0931, label: 0, bag_size: 8959\n",
      "batch 779, loss: 0.1198, instance_loss: 0.1215, weighted_loss: 0.1203, label: 0, bag_size: 7612\n",
      "batch 799, loss: 0.0660, instance_loss: 0.0592, weighted_loss: 0.0640, label: 0, bag_size: 1349\n",
      "batch 819, loss: 0.8007, instance_loss: 0.4045, weighted_loss: 0.6818, label: 0, bag_size: 2653\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9709603658536585: correct 12739/13120\n",
      "class 1 clustering acc 0.8477134146341463: correct 5561/6560\n",
      "Epoch: 23, train_loss: 0.2797, train_clustering_loss:  0.2736, train_error: 0.1146\n",
      "class 0: acc 0.8943661971830986, correct 381/426\n",
      "class 1: acc 0.8756345177664975, correct 345/394\n",
      "\n",
      "Val Set, val_loss: 0.2211, val_error: 0.0909, auc: 0.9735\n",
      "class 0 clustering acc 0.9636363636363636: correct 1696/1760\n",
      "class 1 clustering acc 0.7931818181818182: correct 698/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "Validation loss decreased (0.238487 --> 0.221089).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2506, instance_loss: 0.3665, weighted_loss: 0.2854, label: 1, bag_size: 1822\n",
      "batch 39, loss: 1.3129, instance_loss: 0.7627, weighted_loss: 1.1478, label: 0, bag_size: 3375\n",
      "batch 59, loss: 0.0103, instance_loss: 0.0005, weighted_loss: 0.0074, label: 0, bag_size: 10304\n",
      "batch 79, loss: 0.0265, instance_loss: 0.0017, weighted_loss: 0.0190, label: 0, bag_size: 15313\n",
      "batch 99, loss: 0.1693, instance_loss: 0.0004, weighted_loss: 0.1186, label: 0, bag_size: 21093\n",
      "batch 119, loss: 0.1654, instance_loss: 0.0584, weighted_loss: 0.1333, label: 0, bag_size: 12910\n",
      "batch 139, loss: 0.0539, instance_loss: 0.0018, weighted_loss: 0.0383, label: 1, bag_size: 3980\n",
      "batch 159, loss: 0.1087, instance_loss: 0.0000, weighted_loss: 0.0761, label: 1, bag_size: 29832\n",
      "batch 179, loss: 0.1030, instance_loss: 0.0000, weighted_loss: 0.0721, label: 1, bag_size: 11223\n",
      "batch 199, loss: 0.1057, instance_loss: 0.0010, weighted_loss: 0.0743, label: 0, bag_size: 2760\n",
      "batch 219, loss: 0.0109, instance_loss: 0.0000, weighted_loss: 0.0076, label: 1, bag_size: 14202\n",
      "batch 239, loss: 0.0639, instance_loss: 0.0030, weighted_loss: 0.0457, label: 0, bag_size: 3541\n",
      "batch 259, loss: 0.1396, instance_loss: 0.0015, weighted_loss: 0.0982, label: 0, bag_size: 15071\n",
      "batch 279, loss: 0.3428, instance_loss: 0.0053, weighted_loss: 0.2416, label: 0, bag_size: 10113\n",
      "batch 299, loss: 1.9055, instance_loss: 3.0397, weighted_loss: 2.2458, label: 0, bag_size: 17279\n",
      "batch 319, loss: 0.0028, instance_loss: 0.0009, weighted_loss: 0.0022, label: 0, bag_size: 3459\n",
      "batch 339, loss: 0.0622, instance_loss: 0.0002, weighted_loss: 0.0436, label: 0, bag_size: 14681\n",
      "batch 359, loss: 1.3245, instance_loss: 0.0258, weighted_loss: 0.9349, label: 0, bag_size: 5211\n",
      "batch 379, loss: 0.1300, instance_loss: 0.0304, weighted_loss: 0.1001, label: 0, bag_size: 8582\n",
      "batch 399, loss: 2.5173, instance_loss: 0.4469, weighted_loss: 1.8962, label: 0, bag_size: 1800\n",
      "batch 419, loss: 0.0219, instance_loss: 0.0158, weighted_loss: 0.0201, label: 0, bag_size: 8744\n",
      "batch 439, loss: 0.8095, instance_loss: 1.5776, weighted_loss: 1.0399, label: 0, bag_size: 1714\n",
      "batch 459, loss: 0.1501, instance_loss: 0.0222, weighted_loss: 0.1117, label: 1, bag_size: 6734\n",
      "batch 479, loss: 0.3402, instance_loss: 0.2460, weighted_loss: 0.3119, label: 0, bag_size: 2098\n",
      "batch 499, loss: 1.4832, instance_loss: 2.7865, weighted_loss: 1.8742, label: 1, bag_size: 2731\n",
      "batch 519, loss: 0.1030, instance_loss: 0.3828, weighted_loss: 0.1869, label: 1, bag_size: 865\n",
      "batch 539, loss: 1.4895, instance_loss: 0.7106, weighted_loss: 1.2558, label: 0, bag_size: 1506\n",
      "batch 559, loss: 0.0404, instance_loss: 0.0464, weighted_loss: 0.0422, label: 0, bag_size: 4845\n",
      "batch 579, loss: 0.2068, instance_loss: 0.0009, weighted_loss: 0.1450, label: 1, bag_size: 7389\n",
      "batch 599, loss: 0.0156, instance_loss: 0.0744, weighted_loss: 0.0332, label: 0, bag_size: 1234\n",
      "batch 619, loss: 0.0247, instance_loss: 0.0019, weighted_loss: 0.0179, label: 0, bag_size: 4902\n",
      "batch 639, loss: 0.0283, instance_loss: 0.0000, weighted_loss: 0.0198, label: 0, bag_size: 9866\n",
      "batch 659, loss: 0.0025, instance_loss: 0.0027, weighted_loss: 0.0025, label: 1, bag_size: 4880\n",
      "batch 679, loss: 0.4330, instance_loss: 0.0131, weighted_loss: 0.3070, label: 1, bag_size: 1493\n",
      "batch 699, loss: 0.0067, instance_loss: 0.0006, weighted_loss: 0.0049, label: 1, bag_size: 9955\n",
      "batch 719, loss: 0.6553, instance_loss: 0.3118, weighted_loss: 0.5522, label: 0, bag_size: 2242\n",
      "batch 739, loss: 0.2422, instance_loss: 0.0518, weighted_loss: 0.1851, label: 1, bag_size: 10848\n",
      "batch 759, loss: 0.1565, instance_loss: 0.1033, weighted_loss: 0.1406, label: 1, bag_size: 2495\n",
      "batch 779, loss: 0.1326, instance_loss: 0.0012, weighted_loss: 0.0932, label: 1, bag_size: 16154\n",
      "batch 799, loss: 0.0091, instance_loss: 0.0003, weighted_loss: 0.0065, label: 1, bag_size: 13947\n",
      "batch 819, loss: 0.0268, instance_loss: 0.0781, weighted_loss: 0.0422, label: 0, bag_size: 14333\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9804115853658537: correct 12863/13120\n",
      "class 1 clustering acc 0.8792682926829268: correct 5768/6560\n",
      "Epoch: 24, train_loss: 0.2483, train_clustering_loss:  0.2075, train_error: 0.1049\n",
      "class 0: acc 0.8823529411764706, correct 345/391\n",
      "class 1: acc 0.9067599067599068, correct 389/429\n",
      "\n",
      "Val Set, val_loss: 0.2223, val_error: 0.0818, auc: 0.9731\n",
      "class 0 clustering acc 0.9357954545454545: correct 1647/1760\n",
      "class 1 clustering acc 0.8079545454545455: correct 711/880\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4884, instance_loss: 0.5142, weighted_loss: 0.4962, label: 1, bag_size: 7981\n",
      "batch 39, loss: 0.0352, instance_loss: 0.0579, weighted_loss: 0.0420, label: 0, bag_size: 763\n",
      "batch 59, loss: 0.0014, instance_loss: 0.0529, weighted_loss: 0.0169, label: 0, bag_size: 20150\n",
      "batch 79, loss: 0.0481, instance_loss: 0.0097, weighted_loss: 0.0365, label: 1, bag_size: 7246\n",
      "batch 99, loss: 2.1979, instance_loss: 3.8094, weighted_loss: 2.6813, label: 1, bag_size: 12180\n",
      "batch 119, loss: 0.2610, instance_loss: 0.1743, weighted_loss: 0.2350, label: 1, bag_size: 6726\n",
      "batch 139, loss: 1.1079, instance_loss: 1.1323, weighted_loss: 1.1152, label: 1, bag_size: 9162\n",
      "batch 159, loss: 0.2111, instance_loss: 0.0256, weighted_loss: 0.1555, label: 1, bag_size: 1483\n",
      "batch 179, loss: 1.4110, instance_loss: 3.5034, weighted_loss: 2.0387, label: 1, bag_size: 15185\n",
      "batch 199, loss: 0.0042, instance_loss: 0.0000, weighted_loss: 0.0030, label: 1, bag_size: 9078\n",
      "batch 219, loss: 0.0613, instance_loss: 0.0182, weighted_loss: 0.0484, label: 0, bag_size: 15967\n",
      "batch 239, loss: 0.2307, instance_loss: 0.3550, weighted_loss: 0.2680, label: 0, bag_size: 2195\n",
      "batch 259, loss: 0.2787, instance_loss: 0.0004, weighted_loss: 0.1952, label: 1, bag_size: 12946\n",
      "batch 279, loss: 0.0435, instance_loss: 0.0000, weighted_loss: 0.0305, label: 1, bag_size: 9230\n",
      "batch 299, loss: 0.1128, instance_loss: 0.1214, weighted_loss: 0.1154, label: 0, bag_size: 1452\n",
      "batch 319, loss: 0.2484, instance_loss: 0.0998, weighted_loss: 0.2038, label: 0, bag_size: 1438\n",
      "batch 339, loss: 0.1210, instance_loss: 0.2758, weighted_loss: 0.1674, label: 1, bag_size: 13026\n",
      "batch 359, loss: 0.0192, instance_loss: 0.0012, weighted_loss: 0.0138, label: 1, bag_size: 18603\n",
      "batch 379, loss: 0.0640, instance_loss: 0.1743, weighted_loss: 0.0971, label: 0, bag_size: 10751\n",
      "batch 399, loss: 0.4754, instance_loss: 0.3341, weighted_loss: 0.4330, label: 0, bag_size: 6898\n",
      "batch 419, loss: 0.1312, instance_loss: 0.1714, weighted_loss: 0.1433, label: 0, bag_size: 5409\n",
      "batch 439, loss: 0.0272, instance_loss: 0.0722, weighted_loss: 0.0407, label: 0, bag_size: 13992\n",
      "batch 459, loss: 1.6995, instance_loss: 2.0157, weighted_loss: 1.7943, label: 1, bag_size: 15563\n",
      "batch 479, loss: 0.0045, instance_loss: 0.0000, weighted_loss: 0.0031, label: 0, bag_size: 31780\n",
      "batch 499, loss: 0.5294, instance_loss: 0.0733, weighted_loss: 0.3926, label: 0, bag_size: 1592\n",
      "batch 519, loss: 0.1438, instance_loss: 0.0042, weighted_loss: 0.1020, label: 1, bag_size: 7389\n",
      "batch 539, loss: 0.0289, instance_loss: 0.0750, weighted_loss: 0.0427, label: 0, bag_size: 10942\n",
      "batch 559, loss: 0.0060, instance_loss: 0.0017, weighted_loss: 0.0047, label: 1, bag_size: 2936\n",
      "batch 579, loss: 0.0031, instance_loss: 0.0000, weighted_loss: 0.0021, label: 1, bag_size: 7650\n",
      "batch 599, loss: 0.3853, instance_loss: 0.1952, weighted_loss: 0.3283, label: 0, bag_size: 6898\n",
      "batch 619, loss: 0.2591, instance_loss: 0.0360, weighted_loss: 0.1922, label: 1, bag_size: 10671\n",
      "batch 639, loss: 0.0018, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 8372\n",
      "batch 659, loss: 0.0410, instance_loss: 0.0662, weighted_loss: 0.0486, label: 1, bag_size: 549\n",
      "batch 679, loss: 0.1143, instance_loss: 0.0172, weighted_loss: 0.0852, label: 1, bag_size: 5561\n",
      "batch 699, loss: 0.0122, instance_loss: 0.0000, weighted_loss: 0.0086, label: 1, bag_size: 8522\n",
      "batch 719, loss: 0.0460, instance_loss: 0.0078, weighted_loss: 0.0345, label: 1, bag_size: 1823\n",
      "batch 739, loss: 0.0067, instance_loss: 0.0003, weighted_loss: 0.0048, label: 1, bag_size: 9321\n",
      "batch 759, loss: 0.0309, instance_loss: 0.0354, weighted_loss: 0.0322, label: 1, bag_size: 1622\n",
      "batch 779, loss: 0.0259, instance_loss: 0.0010, weighted_loss: 0.0184, label: 0, bag_size: 4902\n",
      "batch 799, loss: 0.0168, instance_loss: 0.6399, weighted_loss: 0.2038, label: 1, bag_size: 2759\n",
      "batch 819, loss: 0.7194, instance_loss: 1.8109, weighted_loss: 1.0468, label: 1, bag_size: 21252\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9740091463414634: correct 12779/13120\n",
      "class 1 clustering acc 0.8634146341463415: correct 5664/6560\n",
      "Epoch: 25, train_loss: 0.2511, train_clustering_loss:  0.2466, train_error: 0.1000\n",
      "class 0: acc 0.905852417302799, correct 356/393\n",
      "class 1: acc 0.8946135831381733, correct 382/427\n",
      "\n",
      "Val Set, val_loss: 0.2557, val_error: 0.1182, auc: 0.9745\n",
      "class 0 clustering acc 0.9488636363636364: correct 1670/1760\n",
      "class 1 clustering acc 0.8306818181818182: correct 731/880\n",
      "class 0: acc 0.7884615384615384, correct 41/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0637, instance_loss: 0.0000, weighted_loss: 0.0446, label: 1, bag_size: 19606\n",
      "batch 39, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 9851\n",
      "batch 59, loss: 0.0041, instance_loss: 0.0000, weighted_loss: 0.0029, label: 1, bag_size: 15213\n",
      "batch 79, loss: 0.0303, instance_loss: 0.0000, weighted_loss: 0.0212, label: 0, bag_size: 16341\n",
      "batch 99, loss: 0.0084, instance_loss: 0.0000, weighted_loss: 0.0059, label: 1, bag_size: 18794\n",
      "batch 119, loss: 0.0059, instance_loss: 0.0079, weighted_loss: 0.0065, label: 1, bag_size: 4250\n",
      "batch 139, loss: 0.1697, instance_loss: 0.0034, weighted_loss: 0.1198, label: 1, bag_size: 6736\n",
      "batch 159, loss: 0.0369, instance_loss: 0.0023, weighted_loss: 0.0265, label: 1, bag_size: 15464\n",
      "batch 179, loss: 0.0687, instance_loss: 0.0609, weighted_loss: 0.0663, label: 0, bag_size: 10029\n",
      "batch 199, loss: 0.0555, instance_loss: 0.0000, weighted_loss: 0.0389, label: 1, bag_size: 6343\n",
      "batch 219, loss: 0.0704, instance_loss: 0.0046, weighted_loss: 0.0507, label: 0, bag_size: 3541\n",
      "batch 239, loss: 0.1708, instance_loss: 0.4835, weighted_loss: 0.2646, label: 0, bag_size: 6624\n",
      "batch 259, loss: 0.1515, instance_loss: 0.0000, weighted_loss: 0.1061, label: 0, bag_size: 10146\n",
      "batch 279, loss: 0.0262, instance_loss: 0.1934, weighted_loss: 0.0763, label: 1, bag_size: 2759\n",
      "batch 299, loss: 0.1989, instance_loss: 0.0887, weighted_loss: 0.1658, label: 0, bag_size: 1684\n",
      "batch 319, loss: 0.1160, instance_loss: 0.2461, weighted_loss: 0.1550, label: 0, bag_size: 14333\n",
      "batch 339, loss: 0.5439, instance_loss: 1.0145, weighted_loss: 0.6851, label: 0, bag_size: 1637\n",
      "batch 359, loss: 0.3508, instance_loss: 0.2547, weighted_loss: 0.3220, label: 1, bag_size: 1339\n",
      "batch 379, loss: 1.7014, instance_loss: 4.1708, weighted_loss: 2.4422, label: 1, bag_size: 15563\n",
      "batch 399, loss: 0.0107, instance_loss: 0.0000, weighted_loss: 0.0075, label: 1, bag_size: 14779\n",
      "batch 419, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 13225\n",
      "batch 439, loss: 0.1252, instance_loss: 0.0268, weighted_loss: 0.0957, label: 0, bag_size: 2006\n",
      "batch 459, loss: 0.4592, instance_loss: 0.0113, weighted_loss: 0.3249, label: 0, bag_size: 4997\n",
      "batch 479, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 8372\n",
      "batch 499, loss: 0.0123, instance_loss: 0.0023, weighted_loss: 0.0093, label: 0, bag_size: 15747\n",
      "batch 519, loss: 0.0910, instance_loss: 0.0000, weighted_loss: 0.0637, label: 0, bag_size: 11259\n",
      "batch 539, loss: 0.9897, instance_loss: 0.2339, weighted_loss: 0.7630, label: 1, bag_size: 2842\n",
      "batch 559, loss: 0.0309, instance_loss: 1.8239, weighted_loss: 0.5688, label: 1, bag_size: 2785\n",
      "batch 579, loss: 0.3437, instance_loss: 2.2775, weighted_loss: 0.9238, label: 0, bag_size: 1714\n",
      "batch 599, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 9533\n",
      "batch 619, loss: 0.5118, instance_loss: 1.4482, weighted_loss: 0.7927, label: 1, bag_size: 6682\n",
      "batch 639, loss: 0.0086, instance_loss: 0.0000, weighted_loss: 0.0060, label: 0, bag_size: 24911\n",
      "batch 659, loss: 0.0894, instance_loss: 0.0043, weighted_loss: 0.0638, label: 0, bag_size: 1920\n",
      "batch 679, loss: 0.0835, instance_loss: 0.0009, weighted_loss: 0.0587, label: 0, bag_size: 14681\n",
      "batch 699, loss: 0.0387, instance_loss: 0.4845, weighted_loss: 0.1725, label: 0, bag_size: 1760\n",
      "batch 719, loss: 0.0168, instance_loss: 0.0004, weighted_loss: 0.0119, label: 0, bag_size: 22828\n",
      "batch 739, loss: 0.9830, instance_loss: 0.1935, weighted_loss: 0.7462, label: 0, bag_size: 1637\n",
      "batch 759, loss: 0.0088, instance_loss: 0.0396, weighted_loss: 0.0181, label: 0, bag_size: 4902\n",
      "batch 779, loss: 0.0042, instance_loss: 0.0031, weighted_loss: 0.0039, label: 1, bag_size: 13255\n",
      "batch 799, loss: 0.0725, instance_loss: 0.0046, weighted_loss: 0.0521, label: 1, bag_size: 12178\n",
      "batch 819, loss: 0.0343, instance_loss: 0.8205, weighted_loss: 0.2702, label: 0, bag_size: 2336\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9751524390243902: correct 12794/13120\n",
      "class 1 clustering acc 0.8626524390243903: correct 5659/6560\n",
      "Epoch: 26, train_loss: 0.2691, train_clustering_loss:  0.2479, train_error: 0.1110\n",
      "class 0: acc 0.8954545454545455, correct 394/440\n",
      "class 1: acc 0.881578947368421, correct 335/380\n",
      "\n",
      "Val Set, val_loss: 0.2172, val_error: 0.0818, auc: 0.9755\n",
      "class 0 clustering acc 0.9619318181818182: correct 1693/1760\n",
      "class 1 clustering acc 0.775: correct 682/880\n",
      "class 0: acc 0.8653846153846154, correct 45/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "Validation loss decreased (0.221089 --> 0.217235).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1379, instance_loss: 0.0341, weighted_loss: 0.1067, label: 1, bag_size: 6726\n",
      "batch 39, loss: 0.0049, instance_loss: 0.0196, weighted_loss: 0.0093, label: 0, bag_size: 18240\n",
      "batch 59, loss: 0.0658, instance_loss: 0.0230, weighted_loss: 0.0529, label: 1, bag_size: 689\n",
      "batch 79, loss: 0.2082, instance_loss: 0.0265, weighted_loss: 0.1537, label: 0, bag_size: 4523\n",
      "batch 99, loss: 0.0413, instance_loss: 0.0394, weighted_loss: 0.0407, label: 1, bag_size: 1512\n",
      "batch 119, loss: 0.1079, instance_loss: 0.0577, weighted_loss: 0.0928, label: 1, bag_size: 5256\n",
      "batch 139, loss: 0.6078, instance_loss: 0.9217, weighted_loss: 0.7020, label: 1, bag_size: 2935\n",
      "batch 159, loss: 0.0849, instance_loss: 0.4265, weighted_loss: 0.1874, label: 0, bag_size: 9252\n",
      "batch 179, loss: 0.0481, instance_loss: 0.0000, weighted_loss: 0.0337, label: 0, bag_size: 7381\n",
      "batch 199, loss: 0.0746, instance_loss: 0.1785, weighted_loss: 0.1058, label: 1, bag_size: 1064\n",
      "batch 219, loss: 0.0224, instance_loss: 0.0000, weighted_loss: 0.0157, label: 1, bag_size: 18794\n",
      "batch 239, loss: 0.0026, instance_loss: 0.0485, weighted_loss: 0.0164, label: 1, bag_size: 7767\n",
      "batch 259, loss: 0.0015, instance_loss: 0.0089, weighted_loss: 0.0038, label: 1, bag_size: 4877\n",
      "batch 279, loss: 0.0053, instance_loss: 0.0000, weighted_loss: 0.0037, label: 0, bag_size: 8981\n",
      "batch 299, loss: 0.0430, instance_loss: 0.1146, weighted_loss: 0.0645, label: 1, bag_size: 8003\n",
      "batch 319, loss: 0.0112, instance_loss: 0.0013, weighted_loss: 0.0082, label: 1, bag_size: 9078\n",
      "batch 339, loss: 0.0446, instance_loss: 0.0203, weighted_loss: 0.0373, label: 1, bag_size: 8602\n",
      "batch 359, loss: 0.0162, instance_loss: 0.6583, weighted_loss: 0.2089, label: 1, bag_size: 2140\n",
      "batch 379, loss: 0.0129, instance_loss: 0.0866, weighted_loss: 0.0350, label: 1, bag_size: 4423\n",
      "batch 399, loss: 0.1235, instance_loss: 0.0729, weighted_loss: 0.1083, label: 1, bag_size: 16890\n",
      "batch 419, loss: 0.0087, instance_loss: 0.0039, weighted_loss: 0.0072, label: 1, bag_size: 15093\n",
      "batch 439, loss: 0.1417, instance_loss: 0.0116, weighted_loss: 0.1027, label: 0, bag_size: 1213\n",
      "batch 459, loss: 0.0007, instance_loss: 0.0005, weighted_loss: 0.0006, label: 1, bag_size: 5612\n",
      "batch 479, loss: 0.0045, instance_loss: 0.0000, weighted_loss: 0.0031, label: 0, bag_size: 30751\n",
      "batch 499, loss: 0.1978, instance_loss: 0.0063, weighted_loss: 0.1403, label: 1, bag_size: 7669\n",
      "batch 519, loss: 0.0270, instance_loss: 0.0000, weighted_loss: 0.0189, label: 0, bag_size: 21864\n",
      "batch 539, loss: 0.0154, instance_loss: 0.0259, weighted_loss: 0.0185, label: 1, bag_size: 629\n",
      "batch 559, loss: 0.0204, instance_loss: 0.0000, weighted_loss: 0.0143, label: 0, bag_size: 2873\n",
      "batch 579, loss: 0.0367, instance_loss: 0.3204, weighted_loss: 0.1218, label: 1, bag_size: 1412\n",
      "batch 599, loss: 0.0840, instance_loss: 0.0055, weighted_loss: 0.0605, label: 1, bag_size: 2356\n",
      "batch 619, loss: 0.0117, instance_loss: 0.0080, weighted_loss: 0.0106, label: 1, bag_size: 5561\n",
      "batch 639, loss: 0.9990, instance_loss: 1.0873, weighted_loss: 1.0255, label: 1, bag_size: 1497\n",
      "batch 659, loss: 0.0567, instance_loss: 0.1900, weighted_loss: 0.0967, label: 1, bag_size: 7246\n",
      "batch 679, loss: 0.0349, instance_loss: 0.0415, weighted_loss: 0.0369, label: 1, bag_size: 2146\n",
      "batch 699, loss: 0.0942, instance_loss: 0.7393, weighted_loss: 0.2877, label: 0, bag_size: 1213\n",
      "batch 719, loss: 0.0137, instance_loss: 0.0027, weighted_loss: 0.0104, label: 1, bag_size: 8466\n",
      "batch 739, loss: 0.0280, instance_loss: 0.0504, weighted_loss: 0.0347, label: 0, bag_size: 4902\n",
      "batch 759, loss: 0.0256, instance_loss: 0.0000, weighted_loss: 0.0179, label: 0, bag_size: 16720\n",
      "batch 779, loss: 0.0117, instance_loss: 0.0000, weighted_loss: 0.0082, label: 0, bag_size: 15001\n",
      "batch 799, loss: 0.1362, instance_loss: 0.6799, weighted_loss: 0.2993, label: 1, bag_size: 8103\n",
      "batch 819, loss: 0.0193, instance_loss: 0.0024, weighted_loss: 0.0142, label: 1, bag_size: 2140\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9765243902439025: correct 12812/13120\n",
      "class 1 clustering acc 0.8615853658536585: correct 5652/6560\n",
      "Epoch: 27, train_loss: 0.2485, train_clustering_loss:  0.2433, train_error: 0.0902\n",
      "class 0: acc 0.8989071038251366, correct 329/366\n",
      "class 1: acc 0.9185022026431718, correct 417/454\n",
      "\n",
      "Val Set, val_loss: 0.2764, val_error: 0.1273, auc: 0.9761\n",
      "class 0 clustering acc 0.9585227272727272: correct 1687/1760\n",
      "class 1 clustering acc 0.8238636363636364: correct 725/880\n",
      "class 0: acc 0.75, correct 39/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0346, instance_loss: 0.0885, weighted_loss: 0.0507, label: 1, bag_size: 2759\n",
      "batch 39, loss: 0.0863, instance_loss: 0.0002, weighted_loss: 0.0605, label: 0, bag_size: 1202\n",
      "batch 59, loss: 0.8551, instance_loss: 0.0553, weighted_loss: 0.6152, label: 0, bag_size: 7835\n",
      "batch 79, loss: 0.0218, instance_loss: 0.0000, weighted_loss: 0.0153, label: 0, bag_size: 12687\n",
      "batch 99, loss: 0.1228, instance_loss: 0.0489, weighted_loss: 0.1006, label: 1, bag_size: 6731\n",
      "batch 119, loss: 0.0094, instance_loss: 0.0000, weighted_loss: 0.0066, label: 0, bag_size: 23368\n",
      "batch 139, loss: 0.0281, instance_loss: 0.5837, weighted_loss: 0.1948, label: 1, bag_size: 13174\n",
      "batch 159, loss: 0.0595, instance_loss: 0.0043, weighted_loss: 0.0429, label: 1, bag_size: 7613\n",
      "batch 179, loss: 0.0088, instance_loss: 0.0000, weighted_loss: 0.0061, label: 0, bag_size: 21082\n",
      "batch 199, loss: 1.1668, instance_loss: 3.9081, weighted_loss: 1.9892, label: 1, bag_size: 12340\n",
      "batch 219, loss: 0.0348, instance_loss: 0.0005, weighted_loss: 0.0245, label: 1, bag_size: 18794\n",
      "batch 239, loss: 3.2147, instance_loss: 2.4517, weighted_loss: 2.9858, label: 1, bag_size: 3879\n",
      "batch 259, loss: 0.0251, instance_loss: 0.0403, weighted_loss: 0.0297, label: 0, bag_size: 2360\n",
      "batch 279, loss: 1.2553, instance_loss: 1.0890, weighted_loss: 1.2054, label: 1, bag_size: 1284\n",
      "batch 299, loss: 0.0018, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 17633\n",
      "batch 319, loss: 0.0310, instance_loss: 0.0003, weighted_loss: 0.0218, label: 1, bag_size: 9230\n",
      "batch 339, loss: 0.1238, instance_loss: 0.0326, weighted_loss: 0.0964, label: 0, bag_size: 3502\n",
      "batch 359, loss: 0.0440, instance_loss: 0.0856, weighted_loss: 0.0565, label: 0, bag_size: 2367\n",
      "batch 379, loss: 0.0272, instance_loss: 0.0459, weighted_loss: 0.0328, label: 0, bag_size: 13591\n",
      "batch 399, loss: 0.2129, instance_loss: 0.0420, weighted_loss: 0.1616, label: 1, bag_size: 7351\n",
      "batch 419, loss: 0.0072, instance_loss: 0.0001, weighted_loss: 0.0051, label: 1, bag_size: 7119\n",
      "batch 439, loss: 0.0267, instance_loss: 0.0069, weighted_loss: 0.0207, label: 0, bag_size: 9930\n",
      "batch 459, loss: 0.0224, instance_loss: 0.0149, weighted_loss: 0.0202, label: 0, bag_size: 1213\n",
      "batch 479, loss: 0.2946, instance_loss: 0.7050, weighted_loss: 0.4177, label: 0, bag_size: 2918\n",
      "batch 499, loss: 0.0370, instance_loss: 0.0000, weighted_loss: 0.0259, label: 0, bag_size: 9252\n",
      "batch 519, loss: 0.0025, instance_loss: 0.0000, weighted_loss: 0.0017, label: 0, bag_size: 19466\n",
      "batch 539, loss: 0.0018, instance_loss: 0.0007, weighted_loss: 0.0014, label: 0, bag_size: 17791\n",
      "batch 559, loss: 0.0190, instance_loss: 0.0342, weighted_loss: 0.0236, label: 0, bag_size: 10304\n",
      "batch 579, loss: 0.0078, instance_loss: 0.0000, weighted_loss: 0.0054, label: 0, bag_size: 21682\n",
      "batch 599, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 15914\n",
      "batch 619, loss: 0.0154, instance_loss: 0.0001, weighted_loss: 0.0108, label: 1, bag_size: 9571\n",
      "batch 639, loss: 0.0676, instance_loss: 0.1966, weighted_loss: 0.1063, label: 0, bag_size: 12687\n",
      "batch 659, loss: 0.3693, instance_loss: 0.1634, weighted_loss: 0.3075, label: 1, bag_size: 1444\n",
      "batch 679, loss: 0.5724, instance_loss: 1.6997, weighted_loss: 0.9106, label: 1, bag_size: 8103\n",
      "batch 699, loss: 0.0038, instance_loss: 0.0000, weighted_loss: 0.0027, label: 1, bag_size: 6752\n",
      "batch 719, loss: 0.0789, instance_loss: 0.0004, weighted_loss: 0.0554, label: 0, bag_size: 12149\n",
      "batch 739, loss: 0.2054, instance_loss: 0.0052, weighted_loss: 0.1453, label: 0, bag_size: 2998\n",
      "batch 759, loss: 0.0185, instance_loss: 0.2019, weighted_loss: 0.0735, label: 1, bag_size: 4308\n",
      "batch 779, loss: 0.0179, instance_loss: 0.0449, weighted_loss: 0.0260, label: 0, bag_size: 2457\n",
      "batch 799, loss: 0.1135, instance_loss: 0.0076, weighted_loss: 0.0817, label: 1, bag_size: 1459\n",
      "batch 819, loss: 0.3921, instance_loss: 0.0000, weighted_loss: 0.2745, label: 1, bag_size: 9942\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9798780487804878: correct 12856/13120\n",
      "class 1 clustering acc 0.8989329268292683: correct 5897/6560\n",
      "Epoch: 28, train_loss: 0.2024, train_clustering_loss:  0.2088, train_error: 0.0756\n",
      "class 0: acc 0.9230769230769231, correct 384/416\n",
      "class 1: acc 0.9257425742574258, correct 374/404\n",
      "\n",
      "Val Set, val_loss: 0.2108, val_error: 0.0909, auc: 0.9788\n",
      "class 0 clustering acc 0.9443181818181818: correct 1662/1760\n",
      "class 1 clustering acc 0.8363636363636363: correct 736/880\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "Validation loss decreased (0.217235 --> 0.210814).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0016, instance_loss: 0.0218, weighted_loss: 0.0077, label: 1, bag_size: 9877\n",
      "batch 39, loss: 0.1339, instance_loss: 0.0347, weighted_loss: 0.1042, label: 1, bag_size: 3368\n",
      "batch 59, loss: 0.0595, instance_loss: 0.0016, weighted_loss: 0.0422, label: 1, bag_size: 7389\n",
      "batch 79, loss: 0.5257, instance_loss: 0.1496, weighted_loss: 0.4129, label: 1, bag_size: 4939\n",
      "batch 99, loss: 0.1116, instance_loss: 0.0608, weighted_loss: 0.0963, label: 0, bag_size: 6367\n",
      "batch 119, loss: 0.7909, instance_loss: 1.8149, weighted_loss: 1.0981, label: 1, bag_size: 2455\n",
      "batch 139, loss: 0.0176, instance_loss: 0.0023, weighted_loss: 0.0130, label: 0, bag_size: 17633\n",
      "batch 159, loss: 0.0221, instance_loss: 0.0119, weighted_loss: 0.0190, label: 1, bag_size: 16267\n",
      "batch 179, loss: 0.0045, instance_loss: 0.0006, weighted_loss: 0.0034, label: 0, bag_size: 11383\n",
      "batch 199, loss: 0.0461, instance_loss: 0.0293, weighted_loss: 0.0411, label: 0, bag_size: 5409\n",
      "batch 219, loss: 0.0099, instance_loss: 0.0028, weighted_loss: 0.0078, label: 1, bag_size: 3082\n",
      "batch 239, loss: 6.2532, instance_loss: 4.7122, weighted_loss: 5.7909, label: 0, bag_size: 3468\n",
      "batch 259, loss: 0.0158, instance_loss: 0.0171, weighted_loss: 0.0162, label: 1, bag_size: 4821\n",
      "batch 279, loss: 0.8732, instance_loss: 0.0894, weighted_loss: 0.6380, label: 0, bag_size: 13332\n",
      "batch 299, loss: 0.8678, instance_loss: 0.0010, weighted_loss: 0.6078, label: 0, bag_size: 1732\n",
      "batch 319, loss: 0.6040, instance_loss: 0.1510, weighted_loss: 0.4681, label: 1, bag_size: 2681\n",
      "batch 339, loss: 0.0299, instance_loss: 0.0720, weighted_loss: 0.0426, label: 1, bag_size: 3224\n",
      "batch 359, loss: 0.0306, instance_loss: 0.0000, weighted_loss: 0.0214, label: 1, bag_size: 21827\n",
      "batch 379, loss: 0.1695, instance_loss: 0.0100, weighted_loss: 0.1217, label: 0, bag_size: 9069\n",
      "batch 399, loss: 0.3962, instance_loss: 0.0612, weighted_loss: 0.2957, label: 1, bag_size: 7468\n",
      "batch 419, loss: 0.1276, instance_loss: 0.0742, weighted_loss: 0.1116, label: 0, bag_size: 2290\n",
      "batch 439, loss: 0.0098, instance_loss: 0.0057, weighted_loss: 0.0086, label: 1, bag_size: 9446\n",
      "batch 459, loss: 0.0358, instance_loss: 0.1650, weighted_loss: 0.0746, label: 0, bag_size: 2360\n",
      "batch 479, loss: 0.0032, instance_loss: 0.0000, weighted_loss: 0.0022, label: 1, bag_size: 6453\n",
      "batch 499, loss: 0.2206, instance_loss: 0.0085, weighted_loss: 0.1570, label: 0, bag_size: 9597\n",
      "batch 519, loss: 0.0170, instance_loss: 0.0169, weighted_loss: 0.0170, label: 0, bag_size: 16607\n",
      "batch 539, loss: 0.7076, instance_loss: 0.0529, weighted_loss: 0.5112, label: 0, bag_size: 24382\n",
      "batch 559, loss: 0.1472, instance_loss: 0.4107, weighted_loss: 0.2263, label: 1, bag_size: 3409\n",
      "batch 579, loss: 0.0056, instance_loss: 0.0237, weighted_loss: 0.0111, label: 0, bag_size: 23714\n",
      "batch 599, loss: 0.0378, instance_loss: 0.0183, weighted_loss: 0.0319, label: 0, bag_size: 21864\n",
      "batch 619, loss: 0.0034, instance_loss: 0.0333, weighted_loss: 0.0124, label: 0, bag_size: 890\n",
      "batch 639, loss: 0.0013, instance_loss: 0.0031, weighted_loss: 0.0018, label: 0, bag_size: 11477\n",
      "batch 659, loss: 0.0930, instance_loss: 0.3350, weighted_loss: 0.1656, label: 0, bag_size: 13992\n",
      "batch 679, loss: 0.0182, instance_loss: 0.0001, weighted_loss: 0.0128, label: 1, bag_size: 6736\n",
      "batch 699, loss: 0.0455, instance_loss: 0.0413, weighted_loss: 0.0442, label: 1, bag_size: 2522\n",
      "batch 719, loss: 0.2807, instance_loss: 0.2069, weighted_loss: 0.2585, label: 1, bag_size: 1051\n",
      "batch 739, loss: 0.0048, instance_loss: 0.0108, weighted_loss: 0.0066, label: 0, bag_size: 16992\n",
      "batch 759, loss: 0.0022, instance_loss: 0.0009, weighted_loss: 0.0018, label: 1, bag_size: 11421\n",
      "batch 779, loss: 0.0021, instance_loss: 0.0061, weighted_loss: 0.0033, label: 1, bag_size: 1101\n",
      "batch 799, loss: 0.0020, instance_loss: 0.0002, weighted_loss: 0.0015, label: 1, bag_size: 7767\n",
      "batch 819, loss: 0.1291, instance_loss: 0.0291, weighted_loss: 0.0991, label: 1, bag_size: 7669\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9778201219512195: correct 12829/13120\n",
      "class 1 clustering acc 0.9166158536585366: correct 6013/6560\n",
      "Epoch: 29, train_loss: 0.2180, train_clustering_loss:  0.2025, train_error: 0.0817\n",
      "class 0: acc 0.9174757281553398, correct 378/412\n",
      "class 1: acc 0.9191176470588235, correct 375/408\n",
      "\n",
      "Val Set, val_loss: 0.2179, val_error: 0.0909, auc: 0.9781\n",
      "class 0 clustering acc 0.9653409090909091: correct 1699/1760\n",
      "class 1 clustering acc 0.821590909090909: correct 723/880\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0012, instance_loss: 0.0160, weighted_loss: 0.0056, label: 0, bag_size: 9851\n",
      "batch 39, loss: 0.0068, instance_loss: 0.0147, weighted_loss: 0.0092, label: 0, bag_size: 22870\n",
      "batch 59, loss: 0.0085, instance_loss: 0.1093, weighted_loss: 0.0387, label: 0, bag_size: 3474\n",
      "batch 79, loss: 1.4181, instance_loss: 2.4482, weighted_loss: 1.7271, label: 1, bag_size: 2681\n",
      "batch 99, loss: 0.0256, instance_loss: 0.0662, weighted_loss: 0.0378, label: 0, bag_size: 12796\n",
      "batch 119, loss: 0.1360, instance_loss: 0.1241, weighted_loss: 0.1324, label: 0, bag_size: 4418\n",
      "batch 139, loss: 0.0016, instance_loss: 0.0101, weighted_loss: 0.0041, label: 0, bag_size: 9485\n",
      "batch 159, loss: 0.6118, instance_loss: 0.0620, weighted_loss: 0.4469, label: 1, bag_size: 11316\n",
      "batch 179, loss: 0.1434, instance_loss: 0.2161, weighted_loss: 0.1652, label: 1, bag_size: 10591\n",
      "batch 199, loss: 0.0104, instance_loss: 0.0877, weighted_loss: 0.0336, label: 0, bag_size: 21076\n",
      "batch 219, loss: 0.1115, instance_loss: 0.0349, weighted_loss: 0.0885, label: 1, bag_size: 11316\n",
      "batch 239, loss: 0.3125, instance_loss: 0.0129, weighted_loss: 0.2226, label: 1, bag_size: 1493\n",
      "batch 259, loss: 0.0652, instance_loss: 0.2092, weighted_loss: 0.1084, label: 0, bag_size: 4345\n",
      "batch 279, loss: 0.6156, instance_loss: 0.6638, weighted_loss: 0.6301, label: 0, bag_size: 1701\n",
      "batch 299, loss: 0.0136, instance_loss: 0.0000, weighted_loss: 0.0095, label: 1, bag_size: 13947\n",
      "batch 319, loss: 1.0395, instance_loss: 1.3832, weighted_loss: 1.1426, label: 0, bag_size: 2070\n",
      "batch 339, loss: 0.0648, instance_loss: 0.0384, weighted_loss: 0.0569, label: 0, bag_size: 11281\n",
      "batch 359, loss: 0.0363, instance_loss: 0.0069, weighted_loss: 0.0275, label: 0, bag_size: 11199\n",
      "batch 379, loss: 0.0225, instance_loss: 0.0002, weighted_loss: 0.0158, label: 0, bag_size: 4271\n",
      "batch 399, loss: 0.0836, instance_loss: 0.0167, weighted_loss: 0.0635, label: 1, bag_size: 6736\n",
      "batch 419, loss: 0.0908, instance_loss: 0.9495, weighted_loss: 0.3484, label: 0, bag_size: 8549\n",
      "batch 439, loss: 0.2556, instance_loss: 0.4780, weighted_loss: 0.3223, label: 1, bag_size: 1746\n",
      "batch 459, loss: 4.0653, instance_loss: 3.3626, weighted_loss: 3.8544, label: 0, bag_size: 4692\n",
      "batch 479, loss: 0.0055, instance_loss: 0.0037, weighted_loss: 0.0050, label: 1, bag_size: 7513\n",
      "batch 499, loss: 0.0785, instance_loss: 0.0517, weighted_loss: 0.0704, label: 1, bag_size: 11964\n",
      "batch 519, loss: 0.2498, instance_loss: 0.7384, weighted_loss: 0.3964, label: 0, bag_size: 7989\n",
      "batch 539, loss: 0.0481, instance_loss: 0.4963, weighted_loss: 0.1826, label: 0, bag_size: 1416\n",
      "batch 559, loss: 0.0799, instance_loss: 0.1034, weighted_loss: 0.0869, label: 1, bag_size: 6736\n",
      "batch 579, loss: 0.0873, instance_loss: 0.0570, weighted_loss: 0.0782, label: 1, bag_size: 5160\n",
      "batch 599, loss: 0.1300, instance_loss: 0.0007, weighted_loss: 0.0912, label: 0, bag_size: 16607\n",
      "batch 619, loss: 0.0511, instance_loss: 0.0021, weighted_loss: 0.0364, label: 1, bag_size: 20161\n",
      "batch 639, loss: 0.0175, instance_loss: 0.0072, weighted_loss: 0.0144, label: 1, bag_size: 14433\n",
      "batch 659, loss: 0.0156, instance_loss: 0.0001, weighted_loss: 0.0110, label: 0, bag_size: 16992\n",
      "batch 679, loss: 0.0003, instance_loss: 0.0016, weighted_loss: 0.0007, label: 1, bag_size: 3437\n",
      "batch 699, loss: 0.0153, instance_loss: 0.0006, weighted_loss: 0.0109, label: 0, bag_size: 18415\n",
      "batch 719, loss: 0.9950, instance_loss: 0.1481, weighted_loss: 0.7409, label: 0, bag_size: 11212\n",
      "batch 739, loss: 0.9790, instance_loss: 0.4551, weighted_loss: 0.8219, label: 0, bag_size: 2959\n",
      "batch 759, loss: 0.0063, instance_loss: 0.0462, weighted_loss: 0.0183, label: 0, bag_size: 22800\n",
      "batch 779, loss: 0.6624, instance_loss: 0.4420, weighted_loss: 0.5963, label: 1, bag_size: 12712\n",
      "batch 799, loss: 0.0784, instance_loss: 0.0029, weighted_loss: 0.0558, label: 1, bag_size: 21701\n",
      "batch 819, loss: 0.8914, instance_loss: 0.1382, weighted_loss: 0.6654, label: 1, bag_size: 1294\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9727134146341463: correct 12762/13120\n",
      "class 1 clustering acc 0.8597560975609756: correct 5640/6560\n",
      "Epoch: 30, train_loss: 0.2619, train_clustering_loss:  0.2771, train_error: 0.0927\n",
      "class 0: acc 0.9064039408866995, correct 368/406\n",
      "class 1: acc 0.9082125603864735, correct 376/414\n",
      "\n",
      "Val Set, val_loss: 0.2043, val_error: 0.1000, auc: 0.9784\n",
      "class 0 clustering acc 0.9323863636363636: correct 1641/1760\n",
      "class 1 clustering acc 0.8181818181818182: correct 720/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "Validation loss decreased (0.210814 --> 0.204325).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.8923, instance_loss: 0.3257, weighted_loss: 0.7223, label: 0, bag_size: 9132\n",
      "batch 39, loss: 1.5756, instance_loss: 3.1782, weighted_loss: 2.0564, label: 1, bag_size: 1242\n",
      "batch 59, loss: 0.0059, instance_loss: 0.0076, weighted_loss: 0.0064, label: 0, bag_size: 13964\n",
      "batch 79, loss: 0.0274, instance_loss: 0.0177, weighted_loss: 0.0245, label: 0, bag_size: 9485\n",
      "batch 99, loss: 0.4983, instance_loss: 0.2014, weighted_loss: 0.4092, label: 1, bag_size: 5903\n",
      "batch 119, loss: 0.1594, instance_loss: 0.3207, weighted_loss: 0.2078, label: 1, bag_size: 20767\n",
      "batch 139, loss: 0.0209, instance_loss: 0.0274, weighted_loss: 0.0228, label: 0, bag_size: 8788\n",
      "batch 159, loss: 0.0362, instance_loss: 0.0359, weighted_loss: 0.0361, label: 0, bag_size: 17155\n",
      "batch 179, loss: 2.0575, instance_loss: 5.4421, weighted_loss: 3.0728, label: 0, bag_size: 7239\n",
      "batch 199, loss: 0.0066, instance_loss: 0.0269, weighted_loss: 0.0127, label: 0, bag_size: 10263\n",
      "batch 219, loss: 0.0061, instance_loss: 0.0005, weighted_loss: 0.0044, label: 1, bag_size: 11642\n",
      "batch 239, loss: 0.0225, instance_loss: 0.0087, weighted_loss: 0.0184, label: 0, bag_size: 16607\n",
      "batch 259, loss: 0.0063, instance_loss: 0.0081, weighted_loss: 0.0068, label: 1, bag_size: 13015\n",
      "batch 279, loss: 6.2912, instance_loss: 2.2020, weighted_loss: 5.0644, label: 0, bag_size: 3468\n",
      "batch 299, loss: 0.0022, instance_loss: 0.0000, weighted_loss: 0.0015, label: 1, bag_size: 11195\n",
      "batch 319, loss: 0.0015, instance_loss: 0.0130, weighted_loss: 0.0049, label: 0, bag_size: 20796\n",
      "batch 339, loss: 0.0556, instance_loss: 0.0225, weighted_loss: 0.0456, label: 1, bag_size: 2278\n",
      "batch 359, loss: 0.0062, instance_loss: 0.0978, weighted_loss: 0.0337, label: 0, bag_size: 19472\n",
      "batch 379, loss: 0.0113, instance_loss: 0.0765, weighted_loss: 0.0309, label: 0, bag_size: 18415\n",
      "batch 399, loss: 0.0002, instance_loss: 0.0338, weighted_loss: 0.0103, label: 0, bag_size: 3787\n",
      "batch 419, loss: 3.8647, instance_loss: 1.9827, weighted_loss: 3.3001, label: 1, bag_size: 3879\n",
      "batch 439, loss: 0.4162, instance_loss: 1.3267, weighted_loss: 0.6893, label: 1, bag_size: 9404\n",
      "batch 459, loss: 0.6661, instance_loss: 0.5143, weighted_loss: 0.6206, label: 0, bag_size: 3783\n",
      "batch 479, loss: 0.0010, instance_loss: 0.2183, weighted_loss: 0.0662, label: 0, bag_size: 16936\n",
      "batch 499, loss: 1.0465, instance_loss: 1.5481, weighted_loss: 1.1970, label: 0, bag_size: 1831\n",
      "batch 519, loss: 0.0745, instance_loss: 0.2670, weighted_loss: 0.1323, label: 0, bag_size: 8330\n",
      "batch 539, loss: 0.0211, instance_loss: 0.0170, weighted_loss: 0.0199, label: 1, bag_size: 4239\n",
      "batch 559, loss: 1.2882, instance_loss: 0.0322, weighted_loss: 0.9114, label: 0, bag_size: 5120\n",
      "batch 579, loss: 0.0022, instance_loss: 0.0125, weighted_loss: 0.0053, label: 1, bag_size: 5833\n",
      "batch 599, loss: 0.1816, instance_loss: 0.1798, weighted_loss: 0.1810, label: 0, bag_size: 5009\n",
      "batch 619, loss: 0.0058, instance_loss: 0.0147, weighted_loss: 0.0085, label: 0, bag_size: 16341\n",
      "batch 639, loss: 0.0379, instance_loss: 0.0009, weighted_loss: 0.0268, label: 1, bag_size: 10432\n",
      "batch 659, loss: 0.5885, instance_loss: 0.0249, weighted_loss: 0.4194, label: 0, bag_size: 1690\n",
      "batch 679, loss: 0.0329, instance_loss: 0.0029, weighted_loss: 0.0239, label: 1, bag_size: 8216\n",
      "batch 699, loss: 0.0129, instance_loss: 0.0022, weighted_loss: 0.0097, label: 1, bag_size: 12931\n",
      "batch 719, loss: 2.5597, instance_loss: 1.2193, weighted_loss: 2.1576, label: 1, bag_size: 15185\n",
      "batch 739, loss: 0.0164, instance_loss: 0.0035, weighted_loss: 0.0125, label: 1, bag_size: 2495\n",
      "batch 759, loss: 0.0356, instance_loss: 0.0845, weighted_loss: 0.0503, label: 1, bag_size: 6205\n",
      "batch 779, loss: 0.0118, instance_loss: 0.1407, weighted_loss: 0.0505, label: 1, bag_size: 4128\n",
      "batch 799, loss: 0.2165, instance_loss: 0.2744, weighted_loss: 0.2339, label: 0, bag_size: 1498\n",
      "batch 819, loss: 0.1315, instance_loss: 0.3632, weighted_loss: 0.2010, label: 0, bag_size: 1052\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.972484756097561: correct 12759/13120\n",
      "class 1 clustering acc 0.8881097560975609: correct 5826/6560\n",
      "Epoch: 31, train_loss: 0.2353, train_clustering_loss:  0.2404, train_error: 0.1000\n",
      "class 0: acc 0.8921568627450981, correct 364/408\n",
      "class 1: acc 0.9077669902912622, correct 374/412\n",
      "\n",
      "Val Set, val_loss: 0.1913, val_error: 0.0545, auc: 0.9804\n",
      "class 0 clustering acc 0.9460227272727273: correct 1665/1760\n",
      "class 1 clustering acc 0.7693181818181818: correct 677/880\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "Validation loss decreased (0.204325 --> 0.191287).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0107, instance_loss: 0.0002, weighted_loss: 0.0076, label: 1, bag_size: 7119\n",
      "batch 39, loss: 0.0221, instance_loss: 0.0558, weighted_loss: 0.0322, label: 0, bag_size: 14333\n",
      "batch 59, loss: 0.0216, instance_loss: 0.0082, weighted_loss: 0.0176, label: 0, bag_size: 2920\n",
      "batch 79, loss: 0.0188, instance_loss: 0.0000, weighted_loss: 0.0132, label: 1, bag_size: 16051\n",
      "batch 99, loss: 0.0455, instance_loss: 0.1831, weighted_loss: 0.0868, label: 0, bag_size: 5409\n",
      "batch 119, loss: 0.0086, instance_loss: 0.0142, weighted_loss: 0.0103, label: 0, bag_size: 2244\n",
      "batch 139, loss: 0.0293, instance_loss: 0.0849, weighted_loss: 0.0460, label: 0, bag_size: 11199\n",
      "batch 159, loss: 0.7575, instance_loss: 0.2096, weighted_loss: 0.5931, label: 0, bag_size: 1701\n",
      "batch 179, loss: 0.1133, instance_loss: 0.0581, weighted_loss: 0.0968, label: 0, bag_size: 2511\n",
      "batch 199, loss: 0.0290, instance_loss: 0.1123, weighted_loss: 0.0540, label: 1, bag_size: 1622\n",
      "batch 219, loss: 0.1721, instance_loss: 0.0476, weighted_loss: 0.1348, label: 1, bag_size: 2137\n",
      "batch 239, loss: 0.0777, instance_loss: 0.0022, weighted_loss: 0.0551, label: 0, bag_size: 3541\n",
      "batch 259, loss: 0.0049, instance_loss: 0.1751, weighted_loss: 0.0560, label: 1, bag_size: 8019\n",
      "batch 279, loss: 0.0005, instance_loss: 0.0014, weighted_loss: 0.0007, label: 1, bag_size: 4877\n",
      "batch 299, loss: 0.7858, instance_loss: 0.1014, weighted_loss: 0.5805, label: 1, bag_size: 21450\n",
      "batch 319, loss: 1.6642, instance_loss: 1.6596, weighted_loss: 1.6628, label: 0, bag_size: 2815\n",
      "batch 339, loss: 0.0013, instance_loss: 0.0655, weighted_loss: 0.0206, label: 0, bag_size: 9885\n",
      "batch 359, loss: 0.0013, instance_loss: 0.0332, weighted_loss: 0.0109, label: 0, bag_size: 11546\n",
      "batch 379, loss: 0.0606, instance_loss: 0.3601, weighted_loss: 0.1504, label: 1, bag_size: 3980\n",
      "batch 399, loss: 0.0720, instance_loss: 0.1878, weighted_loss: 0.1067, label: 0, bag_size: 11281\n",
      "batch 419, loss: 0.0361, instance_loss: 0.0008, weighted_loss: 0.0255, label: 1, bag_size: 11223\n",
      "batch 439, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 1, bag_size: 13051\n",
      "batch 459, loss: 0.2134, instance_loss: 0.0178, weighted_loss: 0.1547, label: 0, bag_size: 3657\n",
      "batch 479, loss: 0.0167, instance_loss: 0.0267, weighted_loss: 0.0197, label: 0, bag_size: 1651\n",
      "batch 499, loss: 0.0082, instance_loss: 0.0076, weighted_loss: 0.0080, label: 1, bag_size: 10112\n",
      "batch 519, loss: 0.0083, instance_loss: 0.1440, weighted_loss: 0.0490, label: 0, bag_size: 12201\n",
      "batch 539, loss: 0.6683, instance_loss: 0.0345, weighted_loss: 0.4782, label: 0, bag_size: 2918\n",
      "batch 559, loss: 0.0328, instance_loss: 0.0014, weighted_loss: 0.0234, label: 1, bag_size: 5345\n",
      "batch 579, loss: 0.0133, instance_loss: 0.0001, weighted_loss: 0.0093, label: 1, bag_size: 3968\n",
      "batch 599, loss: 0.2172, instance_loss: 0.7566, weighted_loss: 0.3790, label: 0, bag_size: 11922\n",
      "batch 619, loss: 0.0004, instance_loss: 0.0055, weighted_loss: 0.0019, label: 0, bag_size: 2424\n",
      "batch 639, loss: 0.0552, instance_loss: 0.0535, weighted_loss: 0.0547, label: 1, bag_size: 1822\n",
      "batch 659, loss: 1.5040, instance_loss: 1.8246, weighted_loss: 1.6001, label: 1, bag_size: 1533\n",
      "batch 679, loss: 0.3020, instance_loss: 0.1024, weighted_loss: 0.2421, label: 0, bag_size: 10113\n",
      "batch 699, loss: 0.0225, instance_loss: 0.0544, weighted_loss: 0.0321, label: 0, bag_size: 2360\n",
      "batch 719, loss: 0.4177, instance_loss: 0.0397, weighted_loss: 0.3043, label: 1, bag_size: 1819\n",
      "batch 739, loss: 0.1060, instance_loss: 0.6114, weighted_loss: 0.2576, label: 1, bag_size: 5256\n",
      "batch 759, loss: 0.0751, instance_loss: 0.2326, weighted_loss: 0.1224, label: 0, bag_size: 9415\n",
      "batch 779, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 9644\n",
      "batch 799, loss: 0.0162, instance_loss: 0.0732, weighted_loss: 0.0333, label: 0, bag_size: 22426\n",
      "batch 819, loss: 0.7987, instance_loss: 0.0983, weighted_loss: 0.5886, label: 0, bag_size: 2242\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9763719512195121: correct 12810/13120\n",
      "class 1 clustering acc 0.8763719512195122: correct 5749/6560\n",
      "Epoch: 32, train_loss: 0.2241, train_clustering_loss:  0.2236, train_error: 0.0878\n",
      "class 0: acc 0.905852417302799, correct 356/393\n",
      "class 1: acc 0.9180327868852459, correct 392/427\n",
      "\n",
      "Val Set, val_loss: 0.2713, val_error: 0.1273, auc: 0.9794\n",
      "class 0 clustering acc 0.9522727272727273: correct 1676/1760\n",
      "class 1 clustering acc 0.8147727272727273: correct 717/880\n",
      "class 0: acc 0.75, correct 39/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0034, instance_loss: 0.0131, weighted_loss: 0.0063, label: 0, bag_size: 14319\n",
      "batch 39, loss: 0.1901, instance_loss: 0.0013, weighted_loss: 0.1334, label: 1, bag_size: 5137\n",
      "batch 59, loss: 0.0008, instance_loss: 0.0730, weighted_loss: 0.0225, label: 0, bag_size: 2179\n",
      "batch 79, loss: 0.0062, instance_loss: 0.0113, weighted_loss: 0.0077, label: 0, bag_size: 22828\n",
      "batch 99, loss: 0.0158, instance_loss: 0.0445, weighted_loss: 0.0244, label: 0, bag_size: 10995\n",
      "batch 119, loss: 0.2626, instance_loss: 0.8898, weighted_loss: 0.4508, label: 1, bag_size: 1294\n",
      "batch 139, loss: 0.2951, instance_loss: 0.0805, weighted_loss: 0.2307, label: 0, bag_size: 2219\n",
      "batch 159, loss: 0.0137, instance_loss: 0.0443, weighted_loss: 0.0229, label: 0, bag_size: 9930\n",
      "batch 179, loss: 1.1871, instance_loss: 2.0335, weighted_loss: 1.4410, label: 0, bag_size: 7835\n",
      "batch 199, loss: 0.0301, instance_loss: 0.0000, weighted_loss: 0.0211, label: 1, bag_size: 2695\n",
      "batch 219, loss: 0.0127, instance_loss: 0.0458, weighted_loss: 0.0226, label: 0, bag_size: 3893\n",
      "batch 239, loss: 0.3673, instance_loss: 0.0012, weighted_loss: 0.2575, label: 1, bag_size: 5723\n",
      "batch 259, loss: 0.0317, instance_loss: 0.0131, weighted_loss: 0.0261, label: 0, bag_size: 8582\n",
      "batch 279, loss: 0.0009, instance_loss: 0.0231, weighted_loss: 0.0075, label: 0, bag_size: 16992\n",
      "batch 299, loss: 0.0070, instance_loss: 0.0000, weighted_loss: 0.0049, label: 1, bag_size: 19039\n",
      "batch 319, loss: 0.0376, instance_loss: 0.0148, weighted_loss: 0.0308, label: 1, bag_size: 2308\n",
      "batch 339, loss: 0.0768, instance_loss: 0.0221, weighted_loss: 0.0604, label: 1, bag_size: 5561\n",
      "batch 359, loss: 0.0127, instance_loss: 0.5999, weighted_loss: 0.1889, label: 0, bag_size: 3265\n",
      "batch 379, loss: 0.7882, instance_loss: 0.0217, weighted_loss: 0.5582, label: 0, bag_size: 7835\n",
      "batch 399, loss: 0.0249, instance_loss: 0.0534, weighted_loss: 0.0334, label: 0, bag_size: 1909\n",
      "batch 419, loss: 0.0174, instance_loss: 0.0000, weighted_loss: 0.0122, label: 1, bag_size: 19832\n",
      "batch 439, loss: 0.0058, instance_loss: 0.0144, weighted_loss: 0.0084, label: 0, bag_size: 16720\n",
      "batch 459, loss: 0.0087, instance_loss: 0.0173, weighted_loss: 0.0112, label: 0, bag_size: 15636\n",
      "batch 479, loss: 0.0082, instance_loss: 0.0703, weighted_loss: 0.0268, label: 0, bag_size: 22870\n",
      "batch 499, loss: 0.1334, instance_loss: 0.1721, weighted_loss: 0.1450, label: 1, bag_size: 1437\n",
      "batch 519, loss: 0.0752, instance_loss: 0.0402, weighted_loss: 0.0647, label: 0, bag_size: 1891\n",
      "batch 539, loss: 0.0470, instance_loss: 0.1975, weighted_loss: 0.0922, label: 1, bag_size: 8660\n",
      "batch 559, loss: 0.0017, instance_loss: 0.0004, weighted_loss: 0.0013, label: 1, bag_size: 6453\n",
      "batch 579, loss: 0.0017, instance_loss: 0.0097, weighted_loss: 0.0041, label: 0, bag_size: 9060\n",
      "batch 599, loss: 0.0392, instance_loss: 0.0018, weighted_loss: 0.0280, label: 0, bag_size: 18215\n",
      "batch 619, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 1, bag_size: 7371\n",
      "batch 639, loss: 0.0243, instance_loss: 0.0000, weighted_loss: 0.0170, label: 1, bag_size: 1838\n",
      "batch 659, loss: 0.0059, instance_loss: 0.0053, weighted_loss: 0.0057, label: 0, bag_size: 20796\n",
      "batch 679, loss: 0.0393, instance_loss: 0.0505, weighted_loss: 0.0427, label: 0, bag_size: 19518\n",
      "batch 699, loss: 0.0048, instance_loss: 0.0293, weighted_loss: 0.0122, label: 1, bag_size: 14030\n",
      "batch 719, loss: 0.0006, instance_loss: 0.0001, weighted_loss: 0.0005, label: 1, bag_size: 1781\n",
      "batch 739, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0015, label: 1, bag_size: 5221\n",
      "batch 759, loss: 0.3938, instance_loss: 0.0751, weighted_loss: 0.2982, label: 0, bag_size: 26208\n",
      "batch 779, loss: 0.0130, instance_loss: 0.0161, weighted_loss: 0.0139, label: 0, bag_size: 1452\n",
      "batch 799, loss: 0.0550, instance_loss: 0.3879, weighted_loss: 0.1549, label: 0, bag_size: 14249\n",
      "batch 819, loss: 0.1089, instance_loss: 0.8647, weighted_loss: 0.3356, label: 0, bag_size: 1772\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9778963414634146: correct 12830/13120\n",
      "class 1 clustering acc 0.9004573170731708: correct 5907/6560\n",
      "Epoch: 33, train_loss: 0.1810, train_clustering_loss:  0.2108, train_error: 0.0585\n",
      "class 0: acc 0.9346246973365617, correct 386/413\n",
      "class 1: acc 0.9484029484029484, correct 386/407\n",
      "\n",
      "Val Set, val_loss: 0.2671, val_error: 0.1273, auc: 0.9828\n",
      "class 0 clustering acc 0.9357954545454545: correct 1647/1760\n",
      "class 1 clustering acc 0.8272727272727273: correct 728/880\n",
      "class 0: acc 0.75, correct 39/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0019, instance_loss: 0.0077, weighted_loss: 0.0037, label: 0, bag_size: 21076\n",
      "batch 39, loss: 0.0337, instance_loss: 0.5852, weighted_loss: 0.1992, label: 1, bag_size: 9533\n",
      "batch 59, loss: 0.2124, instance_loss: 0.1483, weighted_loss: 0.1932, label: 0, bag_size: 4598\n",
      "batch 79, loss: 0.0002, instance_loss: 0.0393, weighted_loss: 0.0119, label: 0, bag_size: 11735\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0081, weighted_loss: 0.0025, label: 1, bag_size: 6792\n",
      "batch 119, loss: 0.0007, instance_loss: 0.0011, weighted_loss: 0.0008, label: 1, bag_size: 7650\n",
      "batch 139, loss: 0.0028, instance_loss: 0.0000, weighted_loss: 0.0019, label: 1, bag_size: 13194\n",
      "batch 159, loss: 0.0040, instance_loss: 0.0023, weighted_loss: 0.0035, label: 1, bag_size: 6317\n",
      "batch 179, loss: 0.0610, instance_loss: 0.0556, weighted_loss: 0.0594, label: 0, bag_size: 2296\n",
      "batch 199, loss: 0.0024, instance_loss: 0.0000, weighted_loss: 0.0017, label: 1, bag_size: 7515\n",
      "batch 219, loss: 0.3893, instance_loss: 0.0289, weighted_loss: 0.2812, label: 1, bag_size: 3450\n",
      "batch 239, loss: 0.0600, instance_loss: 0.0502, weighted_loss: 0.0570, label: 0, bag_size: 1052\n",
      "batch 259, loss: 0.0482, instance_loss: 0.0007, weighted_loss: 0.0340, label: 1, bag_size: 5345\n",
      "batch 279, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 9644\n",
      "batch 299, loss: 0.0375, instance_loss: 0.0228, weighted_loss: 0.0331, label: 0, bag_size: 11122\n",
      "batch 319, loss: 0.0040, instance_loss: 0.0000, weighted_loss: 0.0028, label: 1, bag_size: 15332\n",
      "batch 339, loss: 0.0748, instance_loss: 0.0000, weighted_loss: 0.0524, label: 1, bag_size: 13786\n",
      "batch 359, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 6966\n",
      "batch 379, loss: 0.0195, instance_loss: 0.0664, weighted_loss: 0.0336, label: 0, bag_size: 20796\n",
      "batch 399, loss: 0.0197, instance_loss: 0.1418, weighted_loss: 0.0563, label: 0, bag_size: 2624\n",
      "batch 419, loss: 0.0135, instance_loss: 0.0005, weighted_loss: 0.0096, label: 1, bag_size: 21827\n",
      "batch 439, loss: 0.0073, instance_loss: 0.0261, weighted_loss: 0.0129, label: 1, bag_size: 11518\n",
      "batch 459, loss: 0.0035, instance_loss: 0.0000, weighted_loss: 0.0025, label: 1, bag_size: 11266\n",
      "batch 479, loss: 0.2439, instance_loss: 0.2915, weighted_loss: 0.2582, label: 1, bag_size: 1339\n",
      "batch 499, loss: 0.0139, instance_loss: 0.0237, weighted_loss: 0.0168, label: 0, bag_size: 12148\n",
      "batch 519, loss: 0.1018, instance_loss: 0.0650, weighted_loss: 0.0907, label: 1, bag_size: 10591\n",
      "batch 539, loss: 0.0046, instance_loss: 0.0323, weighted_loss: 0.0129, label: 0, bag_size: 3190\n",
      "batch 559, loss: 0.0286, instance_loss: 0.2189, weighted_loss: 0.0857, label: 0, bag_size: 10365\n",
      "batch 579, loss: 0.0409, instance_loss: 0.0041, weighted_loss: 0.0299, label: 1, bag_size: 14887\n",
      "batch 599, loss: 0.3142, instance_loss: 0.1331, weighted_loss: 0.2598, label: 0, bag_size: 10029\n",
      "batch 619, loss: 0.0368, instance_loss: 0.1281, weighted_loss: 0.0642, label: 1, bag_size: 20767\n",
      "batch 639, loss: 0.2935, instance_loss: 0.0754, weighted_loss: 0.2281, label: 1, bag_size: 4939\n",
      "batch 659, loss: 0.0041, instance_loss: 0.0000, weighted_loss: 0.0029, label: 1, bag_size: 13194\n",
      "batch 679, loss: 0.0484, instance_loss: 0.0008, weighted_loss: 0.0341, label: 1, bag_size: 6731\n",
      "batch 699, loss: 0.2546, instance_loss: 0.0079, weighted_loss: 0.1806, label: 0, bag_size: 10113\n",
      "batch 719, loss: 0.1137, instance_loss: 0.2263, weighted_loss: 0.1475, label: 0, bag_size: 2043\n",
      "batch 739, loss: 0.0002, instance_loss: 0.0124, weighted_loss: 0.0038, label: 0, bag_size: 17791\n",
      "batch 759, loss: 0.5451, instance_loss: 0.3972, weighted_loss: 0.5008, label: 1, bag_size: 1683\n",
      "batch 779, loss: 0.0936, instance_loss: 0.0733, weighted_loss: 0.0875, label: 0, bag_size: 2624\n",
      "batch 799, loss: 0.0096, instance_loss: 2.5284, weighted_loss: 0.7652, label: 0, bag_size: 5225\n",
      "batch 819, loss: 3.1073, instance_loss: 1.6335, weighted_loss: 2.6651, label: 1, bag_size: 898\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.973094512195122: correct 12767/13120\n",
      "class 1 clustering acc 0.8911585365853658: correct 5846/6560\n",
      "Epoch: 34, train_loss: 0.2100, train_clustering_loss:  0.2571, train_error: 0.0744\n",
      "class 0: acc 0.915, correct 366/400\n",
      "class 1: acc 0.9357142857142857, correct 393/420\n",
      "\n",
      "Val Set, val_loss: 0.1947, val_error: 0.0727, auc: 0.9841\n",
      "class 0 clustering acc 0.9511363636363637: correct 1674/1760\n",
      "class 1 clustering acc 0.7988636363636363: correct 703/880\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0436, instance_loss: 0.7564, weighted_loss: 0.2574, label: 1, bag_size: 9004\n",
      "batch 39, loss: 0.1663, instance_loss: 0.5279, weighted_loss: 0.2748, label: 0, bag_size: 2098\n",
      "batch 59, loss: 0.0180, instance_loss: 1.0343, weighted_loss: 0.3229, label: 1, bag_size: 5907\n",
      "batch 79, loss: 0.0193, instance_loss: 0.0002, weighted_loss: 0.0136, label: 1, bag_size: 5345\n",
      "batch 99, loss: 0.0416, instance_loss: 0.0224, weighted_loss: 0.0358, label: 1, bag_size: 5921\n",
      "batch 119, loss: 0.0686, instance_loss: 0.1135, weighted_loss: 0.0820, label: 1, bag_size: 5256\n",
      "batch 139, loss: 3.8102, instance_loss: 1.2484, weighted_loss: 3.0416, label: 1, bag_size: 898\n",
      "batch 159, loss: 0.0601, instance_loss: 0.1494, weighted_loss: 0.0869, label: 0, bag_size: 3557\n",
      "batch 179, loss: 0.0049, instance_loss: 0.0385, weighted_loss: 0.0150, label: 0, bag_size: 11125\n",
      "batch 199, loss: 0.4684, instance_loss: 0.2492, weighted_loss: 0.4027, label: 0, bag_size: 26208\n",
      "batch 219, loss: 0.0495, instance_loss: 0.0859, weighted_loss: 0.0604, label: 0, bag_size: 1438\n",
      "batch 239, loss: 0.2376, instance_loss: 0.0784, weighted_loss: 0.1898, label: 0, bag_size: 21361\n",
      "batch 259, loss: 0.0376, instance_loss: 0.0057, weighted_loss: 0.0280, label: 0, bag_size: 3783\n",
      "batch 279, loss: 0.2102, instance_loss: 0.0162, weighted_loss: 0.1520, label: 1, bag_size: 3652\n",
      "batch 299, loss: 0.0249, instance_loss: 1.3760, weighted_loss: 0.4302, label: 0, bag_size: 3265\n",
      "batch 319, loss: 0.0081, instance_loss: 0.0294, weighted_loss: 0.0145, label: 1, bag_size: 2344\n",
      "batch 339, loss: 0.0150, instance_loss: 0.1367, weighted_loss: 0.0515, label: 0, bag_size: 1588\n",
      "batch 359, loss: 0.1090, instance_loss: 0.2110, weighted_loss: 0.1396, label: 1, bag_size: 6682\n",
      "batch 379, loss: 0.0022, instance_loss: 0.0056, weighted_loss: 0.0032, label: 1, bag_size: 8003\n",
      "batch 399, loss: 0.0035, instance_loss: 0.0939, weighted_loss: 0.0306, label: 1, bag_size: 3980\n",
      "batch 419, loss: 0.0154, instance_loss: 0.0336, weighted_loss: 0.0209, label: 0, bag_size: 21682\n",
      "batch 439, loss: 0.0006, instance_loss: 0.0231, weighted_loss: 0.0073, label: 0, bag_size: 11865\n",
      "batch 459, loss: 0.0205, instance_loss: 0.0172, weighted_loss: 0.0195, label: 0, bag_size: 9542\n",
      "batch 479, loss: 0.0217, instance_loss: 0.0021, weighted_loss: 0.0158, label: 0, bag_size: 8866\n",
      "batch 499, loss: 0.0263, instance_loss: 0.4152, weighted_loss: 0.1430, label: 0, bag_size: 1797\n",
      "batch 519, loss: 0.0392, instance_loss: 0.0008, weighted_loss: 0.0277, label: 1, bag_size: 9689\n",
      "batch 539, loss: 0.4104, instance_loss: 0.0915, weighted_loss: 0.3147, label: 0, bag_size: 2160\n",
      "batch 559, loss: 0.6242, instance_loss: 0.4048, weighted_loss: 0.5584, label: 1, bag_size: 2681\n",
      "batch 579, loss: 0.0867, instance_loss: 0.0937, weighted_loss: 0.0888, label: 0, bag_size: 2548\n",
      "batch 599, loss: 0.0313, instance_loss: 0.1833, weighted_loss: 0.0769, label: 1, bag_size: 4054\n",
      "batch 619, loss: 5.2610, instance_loss: 2.8313, weighted_loss: 4.5321, label: 0, bag_size: 3897\n",
      "batch 639, loss: 0.0028, instance_loss: 0.0068, weighted_loss: 0.0040, label: 1, bag_size: 11266\n",
      "batch 659, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 21404\n",
      "batch 679, loss: 0.1472, instance_loss: 0.5174, weighted_loss: 0.2583, label: 1, bag_size: 12340\n",
      "batch 699, loss: 0.0041, instance_loss: 0.0005, weighted_loss: 0.0030, label: 0, bag_size: 12796\n",
      "batch 719, loss: 0.0090, instance_loss: 0.0000, weighted_loss: 0.0063, label: 0, bag_size: 24911\n",
      "batch 739, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 31780\n",
      "batch 759, loss: 0.1201, instance_loss: 0.0257, weighted_loss: 0.0918, label: 1, bag_size: 11964\n",
      "batch 779, loss: 0.0027, instance_loss: 0.0251, weighted_loss: 0.0094, label: 0, bag_size: 10263\n",
      "batch 799, loss: 0.0250, instance_loss: 0.1444, weighted_loss: 0.0608, label: 1, bag_size: 10912\n",
      "batch 819, loss: 0.3024, instance_loss: 0.8577, weighted_loss: 0.4690, label: 0, bag_size: 6624\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9727134146341463: correct 12762/13120\n",
      "class 1 clustering acc 0.8611280487804878: correct 5649/6560\n",
      "Epoch: 35, train_loss: 0.1779, train_clustering_loss:  0.2687, train_error: 0.0634\n",
      "class 0: acc 0.9433497536945813, correct 383/406\n",
      "class 1: acc 0.9299516908212561, correct 385/414\n",
      "\n",
      "Val Set, val_loss: 0.3490, val_error: 0.1727, auc: 0.9841\n",
      "class 0 clustering acc 0.884090909090909: correct 1556/1760\n",
      "class 1 clustering acc 0.7: correct 616/880\n",
      "class 0: acc 0.6346153846153846, correct 33/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1131, instance_loss: 0.3493, weighted_loss: 0.1839, label: 1, bag_size: 5921\n",
      "batch 39, loss: 0.0095, instance_loss: 0.0047, weighted_loss: 0.0081, label: 0, bag_size: 4902\n",
      "batch 59, loss: 0.3144, instance_loss: 0.9616, weighted_loss: 0.5085, label: 0, bag_size: 4598\n",
      "batch 79, loss: 0.1115, instance_loss: 0.5976, weighted_loss: 0.2573, label: 0, bag_size: 1142\n",
      "batch 99, loss: 0.0247, instance_loss: 0.7507, weighted_loss: 0.2425, label: 0, bag_size: 19808\n",
      "batch 119, loss: 0.0032, instance_loss: 0.0000, weighted_loss: 0.0022, label: 1, bag_size: 19039\n",
      "batch 139, loss: 0.0312, instance_loss: 0.0000, weighted_loss: 0.0218, label: 1, bag_size: 21827\n",
      "batch 159, loss: 0.1393, instance_loss: 0.5428, weighted_loss: 0.2604, label: 1, bag_size: 8026\n",
      "batch 179, loss: 0.0119, instance_loss: 0.0082, weighted_loss: 0.0108, label: 0, bag_size: 24439\n",
      "batch 199, loss: 0.1439, instance_loss: 0.0005, weighted_loss: 0.1009, label: 1, bag_size: 8592\n",
      "batch 219, loss: 0.0636, instance_loss: 1.3628, weighted_loss: 0.4534, label: 1, bag_size: 1064\n",
      "batch 239, loss: 0.0127, instance_loss: 0.0010, weighted_loss: 0.0092, label: 1, bag_size: 9571\n",
      "batch 259, loss: 0.1802, instance_loss: 0.8640, weighted_loss: 0.3853, label: 0, bag_size: 1142\n",
      "batch 279, loss: 0.0081, instance_loss: 1.0129, weighted_loss: 0.3095, label: 1, bag_size: 2785\n",
      "batch 299, loss: 0.1522, instance_loss: 1.4002, weighted_loss: 0.5266, label: 0, bag_size: 1483\n",
      "batch 319, loss: 0.1026, instance_loss: 0.0080, weighted_loss: 0.0742, label: 1, bag_size: 13692\n",
      "batch 339, loss: 0.0071, instance_loss: 0.0000, weighted_loss: 0.0050, label: 1, bag_size: 19832\n",
      "batch 359, loss: 0.0011, instance_loss: 0.1468, weighted_loss: 0.0448, label: 0, bag_size: 21076\n",
      "batch 379, loss: 0.0010, instance_loss: 0.0130, weighted_loss: 0.0046, label: 0, bag_size: 8948\n",
      "batch 399, loss: 1.2087, instance_loss: 1.1709, weighted_loss: 1.1973, label: 1, bag_size: 9404\n",
      "batch 419, loss: 0.0066, instance_loss: 0.0000, weighted_loss: 0.0046, label: 1, bag_size: 10498\n",
      "batch 439, loss: 0.0060, instance_loss: 0.2590, weighted_loss: 0.0819, label: 0, bag_size: 15636\n",
      "batch 459, loss: 0.0003, instance_loss: 0.0001, weighted_loss: 0.0002, label: 0, bag_size: 23037\n",
      "batch 479, loss: 0.3941, instance_loss: 0.7681, weighted_loss: 0.5063, label: 0, bag_size: 1592\n",
      "batch 499, loss: 0.0502, instance_loss: 0.0116, weighted_loss: 0.0386, label: 0, bag_size: 7989\n",
      "batch 519, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 9673\n",
      "batch 539, loss: 0.0763, instance_loss: 0.0083, weighted_loss: 0.0559, label: 0, bag_size: 10415\n",
      "batch 559, loss: 0.0058, instance_loss: 0.0136, weighted_loss: 0.0081, label: 0, bag_size: 1639\n",
      "batch 579, loss: 0.0026, instance_loss: 0.0000, weighted_loss: 0.0018, label: 1, bag_size: 11600\n",
      "batch 599, loss: 0.0357, instance_loss: 0.0868, weighted_loss: 0.0510, label: 1, bag_size: 12626\n",
      "batch 619, loss: 0.4197, instance_loss: 0.6825, weighted_loss: 0.4986, label: 0, bag_size: 3783\n",
      "batch 639, loss: 0.0155, instance_loss: 0.0532, weighted_loss: 0.0268, label: 1, bag_size: 6734\n",
      "batch 659, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 14515\n",
      "batch 679, loss: 0.0074, instance_loss: 0.0002, weighted_loss: 0.0052, label: 1, bag_size: 18794\n",
      "batch 699, loss: 0.5366, instance_loss: 0.5774, weighted_loss: 0.5489, label: 0, bag_size: 2179\n",
      "batch 719, loss: 0.0061, instance_loss: 0.0329, weighted_loss: 0.0141, label: 0, bag_size: 1881\n",
      "batch 739, loss: 0.0542, instance_loss: 0.0034, weighted_loss: 0.0390, label: 0, bag_size: 8755\n",
      "batch 759, loss: 0.0398, instance_loss: 0.0007, weighted_loss: 0.0281, label: 0, bag_size: 11281\n",
      "batch 779, loss: 0.0421, instance_loss: 0.0002, weighted_loss: 0.0296, label: 1, bag_size: 1888\n",
      "batch 799, loss: 0.0047, instance_loss: 0.0000, weighted_loss: 0.0033, label: 0, bag_size: 12148\n",
      "batch 819, loss: 0.0034, instance_loss: 0.0000, weighted_loss: 0.0024, label: 0, bag_size: 12796\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9778201219512195: correct 12829/13120\n",
      "class 1 clustering acc 0.8736280487804878: correct 5731/6560\n",
      "Epoch: 36, train_loss: 0.1799, train_clustering_loss:  0.2207, train_error: 0.0646\n",
      "class 0: acc 0.928921568627451, correct 379/408\n",
      "class 1: acc 0.941747572815534, correct 388/412\n",
      "\n",
      "Val Set, val_loss: 0.2030, val_error: 0.1000, auc: 0.9824\n",
      "class 0 clustering acc 0.9443181818181818: correct 1662/1760\n",
      "class 1 clustering acc 0.803409090909091: correct 707/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8448275862068966, correct 49/58\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0208, weighted_loss: 0.0064, label: 0, bag_size: 2179\n",
      "batch 39, loss: 0.0205, instance_loss: 0.0000, weighted_loss: 0.0144, label: 1, bag_size: 12575\n",
      "batch 59, loss: 0.0868, instance_loss: 0.0192, weighted_loss: 0.0666, label: 0, bag_size: 4418\n",
      "batch 79, loss: 0.0078, instance_loss: 0.0004, weighted_loss: 0.0056, label: 0, bag_size: 9949\n",
      "batch 99, loss: 0.1107, instance_loss: 0.0020, weighted_loss: 0.0781, label: 0, bag_size: 9069\n",
      "batch 119, loss: 0.0339, instance_loss: 0.0000, weighted_loss: 0.0237, label: 1, bag_size: 11122\n",
      "batch 139, loss: 0.0085, instance_loss: 0.0018, weighted_loss: 0.0065, label: 0, bag_size: 18215\n",
      "batch 159, loss: 0.4918, instance_loss: 0.5157, weighted_loss: 0.4990, label: 1, bag_size: 2681\n",
      "batch 179, loss: 0.1108, instance_loss: 3.0594, weighted_loss: 0.9954, label: 0, bag_size: 19808\n",
      "batch 199, loss: 0.0011, instance_loss: 0.0010, weighted_loss: 0.0011, label: 0, bag_size: 9433\n",
      "batch 219, loss: 0.3348, instance_loss: 0.5706, weighted_loss: 0.4055, label: 1, bag_size: 2455\n",
      "batch 239, loss: 0.0101, instance_loss: 0.0136, weighted_loss: 0.0111, label: 0, bag_size: 2628\n",
      "batch 259, loss: 0.0034, instance_loss: 0.0005, weighted_loss: 0.0025, label: 0, bag_size: 12149\n",
      "batch 279, loss: 0.0330, instance_loss: 0.9385, weighted_loss: 0.3046, label: 0, bag_size: 2091\n",
      "batch 299, loss: 0.0045, instance_loss: 0.0000, weighted_loss: 0.0031, label: 1, bag_size: 6752\n",
      "batch 319, loss: 0.1472, instance_loss: 0.0838, weighted_loss: 0.1282, label: 0, bag_size: 2242\n",
      "batch 339, loss: 0.7973, instance_loss: 0.5383, weighted_loss: 0.7196, label: 0, bag_size: 6884\n",
      "batch 359, loss: 0.0298, instance_loss: 0.0000, weighted_loss: 0.0209, label: 1, bag_size: 3968\n",
      "batch 379, loss: 0.0712, instance_loss: 0.1948, weighted_loss: 0.1083, label: 0, bag_size: 1052\n",
      "batch 399, loss: 0.3090, instance_loss: 0.1209, weighted_loss: 0.2526, label: 0, bag_size: 1142\n",
      "batch 419, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 20150\n",
      "batch 439, loss: 0.1406, instance_loss: 0.3320, weighted_loss: 0.1980, label: 0, bag_size: 15071\n",
      "batch 459, loss: 0.0055, instance_loss: 0.0000, weighted_loss: 0.0038, label: 0, bag_size: 12212\n",
      "batch 479, loss: 0.0318, instance_loss: 0.0000, weighted_loss: 0.0222, label: 1, bag_size: 3211\n",
      "batch 499, loss: 0.0053, instance_loss: 0.0187, weighted_loss: 0.0093, label: 1, bag_size: 1459\n",
      "batch 519, loss: 0.0275, instance_loss: 0.0000, weighted_loss: 0.0192, label: 1, bag_size: 9983\n",
      "batch 539, loss: 0.2024, instance_loss: 0.5595, weighted_loss: 0.3095, label: 1, bag_size: 2455\n",
      "batch 559, loss: 0.0751, instance_loss: 0.0001, weighted_loss: 0.0526, label: 1, bag_size: 15689\n",
      "batch 579, loss: 0.0018, instance_loss: 0.0015, weighted_loss: 0.0017, label: 0, bag_size: 2624\n",
      "batch 599, loss: 0.1150, instance_loss: 0.4210, weighted_loss: 0.2068, label: 0, bag_size: 2098\n",
      "batch 619, loss: 0.0907, instance_loss: 0.5182, weighted_loss: 0.2189, label: 0, bag_size: 1370\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0797, weighted_loss: 0.0240, label: 1, bag_size: 1781\n",
      "batch 659, loss: 0.0178, instance_loss: 0.0057, weighted_loss: 0.0142, label: 1, bag_size: 7217\n",
      "batch 679, loss: 0.5681, instance_loss: 0.7161, weighted_loss: 0.6125, label: 1, bag_size: 2681\n",
      "batch 699, loss: 0.0122, instance_loss: 0.0000, weighted_loss: 0.0085, label: 1, bag_size: 13365\n",
      "batch 719, loss: 0.0007, instance_loss: 0.2038, weighted_loss: 0.0616, label: 1, bag_size: 11981\n",
      "batch 739, loss: 0.0884, instance_loss: 1.1968, weighted_loss: 0.4210, label: 0, bag_size: 1772\n",
      "batch 759, loss: 0.0528, instance_loss: 0.0022, weighted_loss: 0.0376, label: 1, bag_size: 19606\n",
      "batch 779, loss: 0.0024, instance_loss: 0.0909, weighted_loss: 0.0290, label: 1, bag_size: 10281\n",
      "batch 799, loss: 0.0698, instance_loss: 0.0030, weighted_loss: 0.0497, label: 1, bag_size: 10432\n",
      "batch 819, loss: 0.0028, instance_loss: 0.0000, weighted_loss: 0.0020, label: 0, bag_size: 16992\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9816310975609757: correct 12879/13120\n",
      "class 1 clustering acc 0.8707317073170732: correct 5712/6560\n",
      "Epoch: 37, train_loss: 0.1874, train_clustering_loss:  0.1975, train_error: 0.0646\n",
      "class 0: acc 0.9333333333333333, correct 378/405\n",
      "class 1: acc 0.9373493975903614, correct 389/415\n",
      "\n",
      "Val Set, val_loss: 0.2030, val_error: 0.1000, auc: 0.9831\n",
      "class 0 clustering acc 0.9670454545454545: correct 1702/1760\n",
      "class 1 clustering acc 0.8409090909090909: correct 740/880\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0062, instance_loss: 0.0000, weighted_loss: 0.0043, label: 0, bag_size: 20796\n",
      "batch 39, loss: 0.0172, instance_loss: 0.0257, weighted_loss: 0.0197, label: 1, bag_size: 1888\n",
      "batch 59, loss: 0.0481, instance_loss: 0.0262, weighted_loss: 0.0415, label: 1, bag_size: 6736\n",
      "batch 79, loss: 0.0037, instance_loss: 0.0006, weighted_loss: 0.0028, label: 0, bag_size: 9234\n",
      "batch 99, loss: 0.0020, instance_loss: 0.0366, weighted_loss: 0.0124, label: 0, bag_size: 16341\n",
      "batch 119, loss: 0.0183, instance_loss: 1.1997, weighted_loss: 0.3727, label: 1, bag_size: 2356\n",
      "batch 139, loss: 0.0030, instance_loss: 0.0186, weighted_loss: 0.0077, label: 0, bag_size: 2820\n",
      "batch 159, loss: 0.0059, instance_loss: 0.0000, weighted_loss: 0.0041, label: 0, bag_size: 12201\n",
      "batch 179, loss: 0.0182, instance_loss: 1.8714, weighted_loss: 0.5742, label: 0, bag_size: 3474\n",
      "batch 199, loss: 0.1578, instance_loss: 0.0294, weighted_loss: 0.1193, label: 1, bag_size: 4939\n",
      "batch 219, loss: 0.0047, instance_loss: 0.9728, weighted_loss: 0.2951, label: 0, bag_size: 1234\n",
      "batch 239, loss: 0.0030, instance_loss: 0.0122, weighted_loss: 0.0057, label: 1, bag_size: 4250\n",
      "batch 259, loss: 0.1476, instance_loss: 0.0000, weighted_loss: 0.1033, label: 1, bag_size: 4039\n",
      "batch 279, loss: 0.0022, instance_loss: 0.0000, weighted_loss: 0.0015, label: 0, bag_size: 10068\n",
      "batch 299, loss: 0.0501, instance_loss: 0.0140, weighted_loss: 0.0393, label: 0, bag_size: 22681\n",
      "batch 319, loss: 0.0387, instance_loss: 0.0000, weighted_loss: 0.0271, label: 1, bag_size: 4956\n",
      "batch 339, loss: 0.1173, instance_loss: 0.0109, weighted_loss: 0.0854, label: 0, bag_size: 2918\n",
      "batch 359, loss: 0.0135, instance_loss: 0.0065, weighted_loss: 0.0114, label: 0, bag_size: 9069\n",
      "batch 379, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 30751\n",
      "batch 399, loss: 0.0071, instance_loss: 0.0000, weighted_loss: 0.0050, label: 1, bag_size: 16379\n",
      "batch 419, loss: 0.1761, instance_loss: 0.0165, weighted_loss: 0.1282, label: 0, bag_size: 2098\n",
      "batch 439, loss: 0.1304, instance_loss: 0.0000, weighted_loss: 0.0913, label: 1, bag_size: 25695\n",
      "batch 459, loss: 0.0261, instance_loss: 0.0004, weighted_loss: 0.0184, label: 0, bag_size: 17630\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0007, weighted_loss: 0.0002, label: 1, bag_size: 1781\n",
      "batch 499, loss: 0.0038, instance_loss: 0.0501, weighted_loss: 0.0177, label: 0, bag_size: 4465\n",
      "batch 519, loss: 0.0366, instance_loss: 0.1631, weighted_loss: 0.0746, label: 0, bag_size: 1458\n",
      "batch 539, loss: 0.1424, instance_loss: 0.0022, weighted_loss: 0.1003, label: 1, bag_size: 2179\n",
      "batch 559, loss: 0.0066, instance_loss: 0.0000, weighted_loss: 0.0046, label: 1, bag_size: 10969\n",
      "batch 579, loss: 0.0008, instance_loss: 0.0073, weighted_loss: 0.0027, label: 0, bag_size: 890\n",
      "batch 599, loss: 0.1395, instance_loss: 0.2058, weighted_loss: 0.1594, label: 0, bag_size: 1690\n",
      "batch 619, loss: 0.3104, instance_loss: 0.4600, weighted_loss: 0.3553, label: 0, bag_size: 2043\n",
      "batch 639, loss: 0.0729, instance_loss: 3.7525, weighted_loss: 1.1768, label: 0, bag_size: 47866\n",
      "batch 659, loss: 0.0074, instance_loss: 0.3162, weighted_loss: 0.1000, label: 0, bag_size: 1234\n",
      "batch 679, loss: 0.0002, instance_loss: 0.0204, weighted_loss: 0.0063, label: 1, bag_size: 9673\n",
      "batch 699, loss: 0.0038, instance_loss: 0.0242, weighted_loss: 0.0099, label: 1, bag_size: 6090\n",
      "batch 719, loss: 0.0238, instance_loss: 0.0002, weighted_loss: 0.0167, label: 1, bag_size: 12575\n",
      "batch 739, loss: 0.0016, instance_loss: 0.0391, weighted_loss: 0.0128, label: 0, bag_size: 2179\n",
      "batch 759, loss: 0.0029, instance_loss: 0.0408, weighted_loss: 0.0143, label: 1, bag_size: 12611\n",
      "batch 779, loss: 0.0022, instance_loss: 0.1280, weighted_loss: 0.0400, label: 1, bag_size: 1412\n",
      "batch 799, loss: 0.0026, instance_loss: 0.0015, weighted_loss: 0.0023, label: 1, bag_size: 2662\n",
      "batch 819, loss: 0.1607, instance_loss: 0.0218, weighted_loss: 0.1191, label: 0, bag_size: 6884\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9766768292682927: correct 12814/13120\n",
      "class 1 clustering acc 0.8849085365853658: correct 5805/6560\n",
      "Epoch: 38, train_loss: 0.2008, train_clustering_loss:  0.2230, train_error: 0.0683\n",
      "class 0: acc 0.9219143576826196, correct 366/397\n",
      "class 1: acc 0.9408983451536643, correct 398/423\n",
      "\n",
      "Val Set, val_loss: 0.1882, val_error: 0.0818, auc: 0.9834\n",
      "class 0 clustering acc 0.9494318181818182: correct 1671/1760\n",
      "class 1 clustering acc 0.8579545454545454: correct 755/880\n",
      "class 0: acc 0.8653846153846154, correct 45/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "Validation loss decreased (0.191287 --> 0.188214).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0152, instance_loss: 0.0094, weighted_loss: 0.0134, label: 0, bag_size: 19518\n",
      "batch 39, loss: 0.0919, instance_loss: 0.4319, weighted_loss: 0.1939, label: 0, bag_size: 14249\n",
      "batch 59, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 7191\n",
      "batch 79, loss: 0.0428, instance_loss: 0.0004, weighted_loss: 0.0301, label: 0, bag_size: 24439\n",
      "batch 99, loss: 0.0228, instance_loss: 0.0045, weighted_loss: 0.0173, label: 0, bag_size: 2063\n",
      "batch 119, loss: 0.0050, instance_loss: 0.0028, weighted_loss: 0.0044, label: 1, bag_size: 2412\n",
      "batch 139, loss: 0.0072, instance_loss: 0.0101, weighted_loss: 0.0080, label: 1, bag_size: 9955\n",
      "batch 159, loss: 0.0443, instance_loss: 0.0172, weighted_loss: 0.0362, label: 1, bag_size: 5894\n",
      "batch 179, loss: 0.0965, instance_loss: 0.0637, weighted_loss: 0.0866, label: 1, bag_size: 5231\n",
      "batch 199, loss: 1.1316, instance_loss: 0.3395, weighted_loss: 0.8940, label: 0, bag_size: 8420\n",
      "batch 219, loss: 0.3645, instance_loss: 0.1345, weighted_loss: 0.2955, label: 0, bag_size: 11151\n",
      "batch 239, loss: 0.0065, instance_loss: 0.0000, weighted_loss: 0.0045, label: 1, bag_size: 12931\n",
      "batch 259, loss: 0.0027, instance_loss: 0.0000, weighted_loss: 0.0019, label: 0, bag_size: 30751\n",
      "batch 279, loss: 0.0108, instance_loss: 0.0085, weighted_loss: 0.0101, label: 1, bag_size: 8754\n",
      "batch 299, loss: 0.0029, instance_loss: 0.0001, weighted_loss: 0.0021, label: 1, bag_size: 5340\n",
      "batch 319, loss: 0.0544, instance_loss: 0.3328, weighted_loss: 0.1379, label: 1, bag_size: 8982\n",
      "batch 339, loss: 0.5284, instance_loss: 0.2338, weighted_loss: 0.4400, label: 1, bag_size: 1284\n",
      "batch 359, loss: 0.0412, instance_loss: 0.0200, weighted_loss: 0.0348, label: 1, bag_size: 4789\n",
      "batch 379, loss: 0.0052, instance_loss: 0.1596, weighted_loss: 0.0515, label: 0, bag_size: 3190\n",
      "batch 399, loss: 0.0500, instance_loss: 0.0093, weighted_loss: 0.0378, label: 0, bag_size: 9069\n",
      "batch 419, loss: 0.0581, instance_loss: 0.0005, weighted_loss: 0.0408, label: 1, bag_size: 15609\n",
      "batch 439, loss: 0.0004, instance_loss: 0.0847, weighted_loss: 0.0257, label: 0, bag_size: 9433\n",
      "batch 459, loss: 0.1262, instance_loss: 0.2009, weighted_loss: 0.1486, label: 0, bag_size: 1760\n",
      "batch 479, loss: 0.0099, instance_loss: 0.0282, weighted_loss: 0.0154, label: 0, bag_size: 8959\n",
      "batch 499, loss: 0.0066, instance_loss: 0.0388, weighted_loss: 0.0162, label: 0, bag_size: 9415\n",
      "batch 519, loss: 0.0044, instance_loss: 0.0785, weighted_loss: 0.0266, label: 1, bag_size: 9955\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0004, weighted_loss: 0.0003, label: 0, bag_size: 18154\n",
      "batch 559, loss: 0.0004, instance_loss: 0.0053, weighted_loss: 0.0019, label: 1, bag_size: 9610\n",
      "batch 579, loss: 0.1917, instance_loss: 0.0130, weighted_loss: 0.1381, label: 1, bag_size: 6205\n",
      "batch 599, loss: 0.0023, instance_loss: 0.0935, weighted_loss: 0.0297, label: 1, bag_size: 1412\n",
      "batch 619, loss: 0.2949, instance_loss: 0.0177, weighted_loss: 0.2118, label: 0, bag_size: 12840\n",
      "batch 639, loss: 0.1251, instance_loss: 0.0814, weighted_loss: 0.1120, label: 0, bag_size: 6624\n",
      "batch 659, loss: 0.0459, instance_loss: 0.0675, weighted_loss: 0.0524, label: 0, bag_size: 18777\n",
      "batch 679, loss: 2.5854, instance_loss: 0.5583, weighted_loss: 1.9773, label: 1, bag_size: 9215\n",
      "batch 699, loss: 0.0011, instance_loss: 0.0006, weighted_loss: 0.0010, label: 0, bag_size: 32227\n",
      "batch 719, loss: 0.0011, instance_loss: 0.0006, weighted_loss: 0.0009, label: 0, bag_size: 16341\n",
      "batch 739, loss: 0.0128, instance_loss: 0.0108, weighted_loss: 0.0122, label: 0, bag_size: 14956\n",
      "batch 759, loss: 0.1943, instance_loss: 1.5806, weighted_loss: 0.6102, label: 1, bag_size: 1963\n",
      "batch 779, loss: 0.0133, instance_loss: 0.0035, weighted_loss: 0.0104, label: 1, bag_size: 6745\n",
      "batch 799, loss: 0.0051, instance_loss: 0.0000, weighted_loss: 0.0036, label: 0, bag_size: 18045\n",
      "batch 819, loss: 0.0418, instance_loss: 0.0008, weighted_loss: 0.0295, label: 0, bag_size: 14377\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.980030487804878: correct 12858/13120\n",
      "class 1 clustering acc 0.9015243902439024: correct 5914/6560\n",
      "Epoch: 39, train_loss: 0.1936, train_clustering_loss:  0.2030, train_error: 0.0756\n",
      "class 0: acc 0.9113924050632911, correct 360/395\n",
      "class 1: acc 0.9364705882352942, correct 398/425\n",
      "\n",
      "Val Set, val_loss: 0.2417, val_error: 0.1182, auc: 0.9854\n",
      "class 0 clustering acc 0.9744318181818182: correct 1715/1760\n",
      "class 1 clustering acc 0.8715909090909091: correct 767/880\n",
      "class 0: acc 0.7692307692307693, correct 40/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0051, instance_loss: 0.0075, weighted_loss: 0.0058, label: 1, bag_size: 15464\n",
      "batch 39, loss: 0.0207, instance_loss: 0.0012, weighted_loss: 0.0148, label: 1, bag_size: 19606\n",
      "batch 59, loss: 0.0027, instance_loss: 0.0011, weighted_loss: 0.0022, label: 0, bag_size: 4465\n",
      "batch 79, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 23398\n",
      "batch 99, loss: 0.0020, instance_loss: 0.0003, weighted_loss: 0.0015, label: 0, bag_size: 15001\n",
      "batch 119, loss: 0.1109, instance_loss: 0.1915, weighted_loss: 0.1351, label: 1, bag_size: 11394\n",
      "batch 139, loss: 1.5599, instance_loss: 0.4033, weighted_loss: 1.2129, label: 1, bag_size: 9215\n",
      "batch 159, loss: 0.1533, instance_loss: 1.5977, weighted_loss: 0.5867, label: 0, bag_size: 931\n",
      "batch 179, loss: 0.0022, instance_loss: 0.3231, weighted_loss: 0.0985, label: 1, bag_size: 9533\n",
      "batch 199, loss: 0.0425, instance_loss: 0.0664, weighted_loss: 0.0496, label: 1, bag_size: 12719\n",
      "batch 219, loss: 0.3669, instance_loss: 0.8307, weighted_loss: 0.5061, label: 1, bag_size: 983\n",
      "batch 239, loss: 0.0398, instance_loss: 0.0033, weighted_loss: 0.0289, label: 0, bag_size: 8788\n",
      "batch 259, loss: 0.7027, instance_loss: 2.3477, weighted_loss: 1.1962, label: 1, bag_size: 2935\n",
      "batch 279, loss: 0.0005, instance_loss: 0.0267, weighted_loss: 0.0083, label: 1, bag_size: 9321\n",
      "batch 299, loss: 0.0155, instance_loss: 0.0540, weighted_loss: 0.0270, label: 0, bag_size: 2360\n",
      "batch 319, loss: 0.0156, instance_loss: 0.0136, weighted_loss: 0.0150, label: 0, bag_size: 13339\n",
      "batch 339, loss: 1.8863, instance_loss: 0.4004, weighted_loss: 1.4405, label: 1, bag_size: 3121\n",
      "batch 359, loss: 0.0140, instance_loss: 0.0163, weighted_loss: 0.0147, label: 0, bag_size: 22870\n",
      "batch 379, loss: 0.0152, instance_loss: 0.0000, weighted_loss: 0.0107, label: 1, bag_size: 6205\n",
      "batch 399, loss: 0.1249, instance_loss: 0.0054, weighted_loss: 0.0891, label: 0, bag_size: 1508\n",
      "batch 419, loss: 0.9410, instance_loss: 0.1312, weighted_loss: 0.6980, label: 0, bag_size: 3375\n",
      "batch 439, loss: 0.0094, instance_loss: 0.0230, weighted_loss: 0.0135, label: 0, bag_size: 2360\n",
      "batch 459, loss: 0.0241, instance_loss: 0.0133, weighted_loss: 0.0208, label: 1, bag_size: 11394\n",
      "batch 479, loss: 0.0494, instance_loss: 0.0080, weighted_loss: 0.0370, label: 0, bag_size: 2548\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9971\n",
      "batch 519, loss: 1.0606, instance_loss: 0.4869, weighted_loss: 0.8885, label: 0, bag_size: 2653\n",
      "batch 539, loss: 0.6653, instance_loss: 0.5932, weighted_loss: 0.6437, label: 1, bag_size: 1339\n",
      "batch 559, loss: 0.0050, instance_loss: 0.0011, weighted_loss: 0.0038, label: 1, bag_size: 14604\n",
      "batch 579, loss: 0.0045, instance_loss: 0.0000, weighted_loss: 0.0032, label: 0, bag_size: 13795\n",
      "batch 599, loss: 0.0107, instance_loss: 0.0016, weighted_loss: 0.0080, label: 0, bag_size: 18954\n",
      "batch 619, loss: 0.0069, instance_loss: 0.0000, weighted_loss: 0.0048, label: 1, bag_size: 5025\n",
      "batch 639, loss: 0.0245, instance_loss: 0.0018, weighted_loss: 0.0177, label: 1, bag_size: 3683\n",
      "batch 659, loss: 0.0331, instance_loss: 0.0047, weighted_loss: 0.0246, label: 0, bag_size: 22498\n",
      "batch 679, loss: 0.0120, instance_loss: 0.0000, weighted_loss: 0.0084, label: 0, bag_size: 19043\n",
      "batch 699, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 1, bag_size: 19932\n",
      "batch 719, loss: 0.0363, instance_loss: 0.0017, weighted_loss: 0.0259, label: 1, bag_size: 5454\n",
      "batch 739, loss: 0.0032, instance_loss: 0.0829, weighted_loss: 0.0271, label: 1, bag_size: 10671\n",
      "batch 759, loss: 0.3539, instance_loss: 0.2442, weighted_loss: 0.3210, label: 1, bag_size: 4786\n",
      "batch 779, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 16052\n",
      "batch 799, loss: 0.0050, instance_loss: 0.3467, weighted_loss: 0.1075, label: 0, bag_size: 2360\n",
      "batch 819, loss: 0.0262, instance_loss: 0.0016, weighted_loss: 0.0188, label: 1, bag_size: 17769\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9801067073170732: correct 12859/13120\n",
      "class 1 clustering acc 0.9035060975609757: correct 5927/6560\n",
      "Epoch: 40, train_loss: 0.1490, train_clustering_loss:  0.1757, train_error: 0.0585\n",
      "class 0: acc 0.9385342789598109, correct 397/423\n",
      "class 1: acc 0.9445843828715366, correct 375/397\n",
      "\n",
      "Val Set, val_loss: 0.2231, val_error: 0.0909, auc: 0.9857\n",
      "class 0 clustering acc 0.9443181818181818: correct 1662/1760\n",
      "class 1 clustering acc 0.8125: correct 715/880\n",
      "class 0: acc 1.0, correct 52/52\n",
      "class 1: acc 0.8275862068965517, correct 48/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0059, instance_loss: 0.0024, weighted_loss: 0.0048, label: 0, bag_size: 1213\n",
      "batch 39, loss: 1.6511, instance_loss: 0.3870, weighted_loss: 1.2719, label: 0, bag_size: 18516\n",
      "batch 59, loss: 0.2496, instance_loss: 0.0045, weighted_loss: 0.1761, label: 1, bag_size: 10460\n",
      "batch 79, loss: 0.0093, instance_loss: 0.0481, weighted_loss: 0.0209, label: 1, bag_size: 9519\n",
      "batch 99, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 15077\n",
      "batch 119, loss: 0.0074, instance_loss: 0.0000, weighted_loss: 0.0052, label: 0, bag_size: 8145\n",
      "batch 139, loss: 0.0197, instance_loss: 0.0000, weighted_loss: 0.0138, label: 1, bag_size: 18603\n",
      "batch 159, loss: 0.0169, instance_loss: 0.0927, weighted_loss: 0.0397, label: 0, bag_size: 4598\n",
      "batch 179, loss: 0.0024, instance_loss: 0.0000, weighted_loss: 0.0017, label: 1, bag_size: 7515\n",
      "batch 199, loss: 0.0103, instance_loss: 0.0555, weighted_loss: 0.0239, label: 1, bag_size: 5605\n",
      "batch 219, loss: 0.7238, instance_loss: 0.9801, weighted_loss: 0.8007, label: 1, bag_size: 9404\n",
      "batch 239, loss: 0.0001, instance_loss: 0.0025, weighted_loss: 0.0008, label: 1, bag_size: 9078\n",
      "batch 259, loss: 0.0168, instance_loss: 0.0000, weighted_loss: 0.0118, label: 0, bag_size: 9542\n",
      "batch 279, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 9971\n",
      "batch 299, loss: 0.0085, instance_loss: 0.0571, weighted_loss: 0.0231, label: 1, bag_size: 29832\n",
      "batch 319, loss: 0.6645, instance_loss: 0.0303, weighted_loss: 0.4743, label: 0, bag_size: 2160\n",
      "batch 339, loss: 0.0031, instance_loss: 0.0001, weighted_loss: 0.0022, label: 1, bag_size: 11122\n",
      "batch 359, loss: 0.0767, instance_loss: 0.0909, weighted_loss: 0.0809, label: 0, bag_size: 9069\n",
      "batch 379, loss: 0.0006, instance_loss: 0.0023, weighted_loss: 0.0011, label: 0, bag_size: 9851\n",
      "batch 399, loss: 5.1999, instance_loss: 3.7545, weighted_loss: 4.7662, label: 0, bag_size: 5105\n",
      "batch 419, loss: 0.4544, instance_loss: 0.0005, weighted_loss: 0.3182, label: 1, bag_size: 8592\n",
      "batch 439, loss: 0.0122, instance_loss: 0.5424, weighted_loss: 0.1713, label: 1, bag_size: 8660\n",
      "batch 459, loss: 0.0163, instance_loss: 0.0586, weighted_loss: 0.0290, label: 0, bag_size: 1213\n",
      "batch 479, loss: 0.0557, instance_loss: 0.2859, weighted_loss: 0.1247, label: 0, bag_size: 1149\n",
      "batch 499, loss: 0.0042, instance_loss: 0.0101, weighted_loss: 0.0060, label: 0, bag_size: 2282\n",
      "batch 519, loss: 0.0031, instance_loss: 0.0054, weighted_loss: 0.0038, label: 1, bag_size: 13174\n",
      "batch 539, loss: 0.0418, instance_loss: 0.1154, weighted_loss: 0.0639, label: 1, bag_size: 7381\n",
      "batch 559, loss: 0.7237, instance_loss: 0.4327, weighted_loss: 0.6364, label: 0, bag_size: 7428\n",
      "batch 579, loss: 0.0176, instance_loss: 0.0191, weighted_loss: 0.0181, label: 0, bag_size: 7637\n",
      "batch 599, loss: 0.5459, instance_loss: 0.0144, weighted_loss: 0.3864, label: 0, bag_size: 13619\n",
      "batch 619, loss: 0.3382, instance_loss: 0.3507, weighted_loss: 0.3419, label: 1, bag_size: 5516\n",
      "batch 639, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 14319\n",
      "batch 659, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 14515\n",
      "batch 679, loss: 0.0036, instance_loss: 0.0000, weighted_loss: 0.0025, label: 0, bag_size: 2322\n",
      "batch 699, loss: 0.3471, instance_loss: 0.0474, weighted_loss: 0.2572, label: 0, bag_size: 9616\n",
      "batch 719, loss: 0.0372, instance_loss: 0.0000, weighted_loss: 0.0261, label: 1, bag_size: 21827\n",
      "batch 739, loss: 0.0084, instance_loss: 0.0000, weighted_loss: 0.0059, label: 1, bag_size: 8019\n",
      "batch 759, loss: 0.2602, instance_loss: 1.0963, weighted_loss: 0.5111, label: 0, bag_size: 2098\n",
      "batch 779, loss: 0.0477, instance_loss: 0.7389, weighted_loss: 0.2551, label: 1, bag_size: 1064\n",
      "batch 799, loss: 0.0033, instance_loss: 0.0014, weighted_loss: 0.0028, label: 1, bag_size: 2356\n",
      "batch 819, loss: 0.0504, instance_loss: 0.0555, weighted_loss: 0.0519, label: 0, bag_size: 3657\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9818597560975609: correct 12882/13120\n",
      "class 1 clustering acc 0.8847560975609756: correct 5804/6560\n",
      "Epoch: 41, train_loss: 0.1959, train_clustering_loss:  0.1915, train_error: 0.0768\n",
      "class 0: acc 0.9243902439024391, correct 379/410\n",
      "class 1: acc 0.9219512195121952, correct 378/410\n",
      "\n",
      "Val Set, val_loss: 0.1978, val_error: 0.1000, auc: 0.9867\n",
      "class 0 clustering acc 0.9642045454545455: correct 1697/1760\n",
      "class 1 clustering acc 0.8159090909090909: correct 718/880\n",
      "class 0: acc 0.8076923076923077, correct 42/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0020, instance_loss: 0.0007, weighted_loss: 0.0016, label: 1, bag_size: 4128\n",
      "batch 39, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 14202\n",
      "batch 59, loss: 0.0586, instance_loss: 0.1270, weighted_loss: 0.0791, label: 0, bag_size: 22498\n",
      "batch 79, loss: 0.0364, instance_loss: 0.0199, weighted_loss: 0.0314, label: 1, bag_size: 8040\n",
      "batch 99, loss: 0.1125, instance_loss: 0.0748, weighted_loss: 0.1012, label: 0, bag_size: 1800\n",
      "batch 119, loss: 0.5946, instance_loss: 0.4421, weighted_loss: 0.5488, label: 1, bag_size: 2681\n",
      "batch 139, loss: 7.9299, instance_loss: 0.6439, weighted_loss: 5.7441, label: 0, bag_size: 3897\n",
      "batch 159, loss: 0.0097, instance_loss: 0.0386, weighted_loss: 0.0183, label: 0, bag_size: 16720\n",
      "batch 179, loss: 0.7098, instance_loss: 2.2409, weighted_loss: 1.1691, label: 1, bag_size: 1242\n",
      "batch 199, loss: 0.0246, instance_loss: 0.0638, weighted_loss: 0.0364, label: 0, bag_size: 17268\n",
      "batch 219, loss: 0.0365, instance_loss: 0.0947, weighted_loss: 0.0540, label: 0, bag_size: 3774\n",
      "batch 239, loss: 0.0729, instance_loss: 0.0369, weighted_loss: 0.0621, label: 0, bag_size: 3670\n",
      "batch 259, loss: 0.0098, instance_loss: 0.0586, weighted_loss: 0.0244, label: 1, bag_size: 2385\n",
      "batch 279, loss: 0.5026, instance_loss: 3.5002, weighted_loss: 1.4019, label: 0, bag_size: 1714\n",
      "batch 299, loss: 0.0867, instance_loss: 0.8553, weighted_loss: 0.3173, label: 0, bag_size: 2098\n",
      "batch 319, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 14779\n",
      "batch 339, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 15332\n",
      "batch 359, loss: 0.0531, instance_loss: 0.2938, weighted_loss: 0.1253, label: 0, bag_size: 9387\n",
      "batch 379, loss: 0.1282, instance_loss: 0.0119, weighted_loss: 0.0933, label: 1, bag_size: 12180\n",
      "batch 399, loss: 0.0018, instance_loss: 0.0059, weighted_loss: 0.0031, label: 0, bag_size: 9455\n",
      "batch 419, loss: 0.0583, instance_loss: 1.0359, weighted_loss: 0.3516, label: 0, bag_size: 1772\n",
      "batch 439, loss: 0.1168, instance_loss: 0.0120, weighted_loss: 0.0854, label: 1, bag_size: 13089\n",
      "batch 459, loss: 0.0438, instance_loss: 0.0273, weighted_loss: 0.0388, label: 0, bag_size: 10415\n",
      "batch 479, loss: 0.0476, instance_loss: 0.0799, weighted_loss: 0.0573, label: 0, bag_size: 5009\n",
      "batch 499, loss: 0.0407, instance_loss: 0.0023, weighted_loss: 0.0292, label: 0, bag_size: 17268\n",
      "batch 519, loss: 0.0022, instance_loss: 0.0017, weighted_loss: 0.0021, label: 1, bag_size: 7513\n",
      "batch 539, loss: 0.0104, instance_loss: 0.0030, weighted_loss: 0.0082, label: 1, bag_size: 10969\n",
      "batch 559, loss: 0.3452, instance_loss: 1.2761, weighted_loss: 0.6245, label: 1, bag_size: 6726\n",
      "batch 579, loss: 0.0064, instance_loss: 0.0000, weighted_loss: 0.0045, label: 1, bag_size: 6781\n",
      "batch 599, loss: 0.2435, instance_loss: 1.1060, weighted_loss: 0.5023, label: 1, bag_size: 12626\n",
      "batch 619, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 11884\n",
      "batch 639, loss: 0.1285, instance_loss: 0.0272, weighted_loss: 0.0981, label: 0, bag_size: 1458\n",
      "batch 659, loss: 0.0308, instance_loss: 0.0000, weighted_loss: 0.0216, label: 0, bag_size: 18777\n",
      "batch 679, loss: 0.0199, instance_loss: 0.0000, weighted_loss: 0.0139, label: 1, bag_size: 12408\n",
      "batch 699, loss: 0.0065, instance_loss: 0.0002, weighted_loss: 0.0046, label: 0, bag_size: 16087\n",
      "batch 719, loss: 0.0831, instance_loss: 0.1427, weighted_loss: 0.1010, label: 0, bag_size: 11922\n",
      "batch 739, loss: 0.0339, instance_loss: 0.0421, weighted_loss: 0.0364, label: 1, bag_size: 6731\n",
      "batch 759, loss: 3.1243, instance_loss: 1.8261, weighted_loss: 2.7348, label: 1, bag_size: 898\n",
      "batch 779, loss: 0.0122, instance_loss: 0.0234, weighted_loss: 0.0156, label: 1, bag_size: 5256\n",
      "batch 799, loss: 0.3854, instance_loss: 0.0467, weighted_loss: 0.2838, label: 0, bag_size: 24382\n",
      "batch 819, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 12524\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9783536585365854: correct 12836/13120\n",
      "class 1 clustering acc 0.8853658536585366: correct 5808/6560\n",
      "Epoch: 42, train_loss: 0.2216, train_clustering_loss:  0.2210, train_error: 0.0732\n",
      "class 0: acc 0.9349397590361446, correct 388/415\n",
      "class 1: acc 0.9185185185185185, correct 372/405\n",
      "\n",
      "Val Set, val_loss: 0.1607, val_error: 0.0455, auc: 0.9891\n",
      "class 0 clustering acc 0.9653409090909091: correct 1699/1760\n",
      "class 1 clustering acc 0.8011363636363636: correct 705/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "Validation loss decreased (0.188214 --> 0.160687).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0033, instance_loss: 0.0000, weighted_loss: 0.0023, label: 1, bag_size: 8019\n",
      "batch 39, loss: 0.0027, instance_loss: 0.0001, weighted_loss: 0.0019, label: 1, bag_size: 6950\n",
      "batch 59, loss: 0.0041, instance_loss: 0.0272, weighted_loss: 0.0110, label: 0, bag_size: 24911\n",
      "batch 79, loss: 0.0193, instance_loss: 0.2091, weighted_loss: 0.0762, label: 0, bag_size: 763\n",
      "batch 99, loss: 0.0239, instance_loss: 0.0182, weighted_loss: 0.0222, label: 0, bag_size: 8025\n",
      "batch 119, loss: 0.0858, instance_loss: 0.2109, weighted_loss: 0.1234, label: 1, bag_size: 12946\n",
      "batch 139, loss: 0.0334, instance_loss: 0.0154, weighted_loss: 0.0280, label: 1, bag_size: 5907\n",
      "batch 159, loss: 0.0052, instance_loss: 0.0906, weighted_loss: 0.0308, label: 0, bag_size: 1824\n",
      "batch 179, loss: 4.0648, instance_loss: 1.6044, weighted_loss: 3.3266, label: 1, bag_size: 898\n",
      "batch 199, loss: 0.0048, instance_loss: 0.0000, weighted_loss: 0.0033, label: 1, bag_size: 5340\n",
      "batch 219, loss: 0.0831, instance_loss: 0.0000, weighted_loss: 0.0582, label: 0, bag_size: 24439\n",
      "batch 239, loss: 0.0048, instance_loss: 0.0000, weighted_loss: 0.0033, label: 0, bag_size: 12593\n",
      "batch 259, loss: 0.0391, instance_loss: 0.0042, weighted_loss: 0.0286, label: 1, bag_size: 11220\n",
      "batch 279, loss: 0.0007, instance_loss: 0.0088, weighted_loss: 0.0031, label: 1, bag_size: 16512\n",
      "batch 299, loss: 0.0051, instance_loss: 0.0011, weighted_loss: 0.0039, label: 1, bag_size: 8019\n",
      "batch 319, loss: 0.0018, instance_loss: 0.0000, weighted_loss: 0.0013, label: 0, bag_size: 14956\n",
      "batch 339, loss: 0.0389, instance_loss: 0.0000, weighted_loss: 0.0272, label: 0, bag_size: 7381\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 4442\n",
      "batch 379, loss: 0.0191, instance_loss: 0.0037, weighted_loss: 0.0145, label: 0, bag_size: 14377\n",
      "batch 399, loss: 0.0144, instance_loss: 0.0002, weighted_loss: 0.0101, label: 1, bag_size: 5561\n",
      "batch 419, loss: 0.0797, instance_loss: 0.0500, weighted_loss: 0.0708, label: 1, bag_size: 5921\n",
      "batch 439, loss: 0.0006, instance_loss: 0.0086, weighted_loss: 0.0030, label: 0, bag_size: 14206\n",
      "batch 459, loss: 0.0128, instance_loss: 0.0010, weighted_loss: 0.0093, label: 0, bag_size: 1508\n",
      "batch 479, loss: 0.5169, instance_loss: 0.0028, weighted_loss: 0.3627, label: 1, bag_size: 15689\n",
      "batch 499, loss: 1.4979, instance_loss: 2.7031, weighted_loss: 1.8595, label: 1, bag_size: 2731\n",
      "batch 519, loss: 0.0022, instance_loss: 0.1716, weighted_loss: 0.0530, label: 1, bag_size: 8019\n",
      "batch 539, loss: 0.0905, instance_loss: 0.0000, weighted_loss: 0.0634, label: 1, bag_size: 9230\n",
      "batch 559, loss: 0.1780, instance_loss: 0.0191, weighted_loss: 0.1303, label: 0, bag_size: 11212\n",
      "batch 579, loss: 0.0188, instance_loss: 0.0145, weighted_loss: 0.0175, label: 1, bag_size: 1255\n",
      "batch 599, loss: 0.0049, instance_loss: 0.0000, weighted_loss: 0.0034, label: 0, bag_size: 8898\n",
      "batch 619, loss: 0.0016, instance_loss: 0.0063, weighted_loss: 0.0030, label: 0, bag_size: 2628\n",
      "batch 639, loss: 0.0004, instance_loss: 0.0063, weighted_loss: 0.0021, label: 1, bag_size: 7767\n",
      "batch 659, loss: 0.0276, instance_loss: 0.0922, weighted_loss: 0.0470, label: 1, bag_size: 1051\n",
      "batch 679, loss: 0.0966, instance_loss: 0.0002, weighted_loss: 0.0677, label: 1, bag_size: 6842\n",
      "batch 699, loss: 0.1606, instance_loss: 0.1819, weighted_loss: 0.1670, label: 0, bag_size: 2270\n",
      "batch 719, loss: 0.9086, instance_loss: 0.3946, weighted_loss: 0.7544, label: 1, bag_size: 1920\n",
      "batch 739, loss: 0.0024, instance_loss: 0.0000, weighted_loss: 0.0017, label: 0, bag_size: 8372\n",
      "batch 759, loss: 1.4917, instance_loss: 0.7665, weighted_loss: 1.2741, label: 1, bag_size: 2395\n",
      "batch 779, loss: 0.1173, instance_loss: 0.1629, weighted_loss: 0.1310, label: 1, bag_size: 3674\n",
      "batch 799, loss: 0.0099, instance_loss: 0.0010, weighted_loss: 0.0073, label: 0, bag_size: 11259\n",
      "batch 819, loss: 0.0020, instance_loss: 0.0080, weighted_loss: 0.0038, label: 0, bag_size: 2179\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9864329268292683: correct 12942/13120\n",
      "class 1 clustering acc 0.9224085365853658: correct 6051/6560\n",
      "Epoch: 43, train_loss: 0.1680, train_clustering_loss:  0.1461, train_error: 0.0549\n",
      "class 0: acc 0.9310344827586207, correct 378/406\n",
      "class 1: acc 0.9589371980676329, correct 397/414\n",
      "\n",
      "Val Set, val_loss: 0.2094, val_error: 0.1000, auc: 0.9884\n",
      "class 0 clustering acc 0.9522727272727273: correct 1676/1760\n",
      "class 1 clustering acc 0.8454545454545455: correct 744/880\n",
      "class 0: acc 0.7884615384615384, correct 41/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 5221\n",
      "batch 39, loss: 0.3555, instance_loss: 0.0032, weighted_loss: 0.2498, label: 0, bag_size: 9132\n",
      "batch 59, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 18240\n",
      "batch 79, loss: 0.0005, instance_loss: 0.0364, weighted_loss: 0.0113, label: 1, bag_size: 14030\n",
      "batch 99, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 14223\n",
      "batch 119, loss: 0.0137, instance_loss: 2.0947, weighted_loss: 0.6380, label: 0, bag_size: 2242\n",
      "batch 139, loss: 0.0673, instance_loss: 0.6332, weighted_loss: 0.2371, label: 0, bag_size: 2351\n",
      "batch 159, loss: 0.0096, instance_loss: 0.6483, weighted_loss: 0.2012, label: 0, bag_size: 9888\n",
      "batch 179, loss: 0.1691, instance_loss: 0.5226, weighted_loss: 0.2752, label: 0, bag_size: 2814\n",
      "batch 199, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 14515\n",
      "batch 219, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 6606\n",
      "batch 239, loss: 0.0668, instance_loss: 0.5423, weighted_loss: 0.2094, label: 0, bag_size: 12732\n",
      "batch 259, loss: 0.0063, instance_loss: 0.0000, weighted_loss: 0.0044, label: 0, bag_size: 12793\n",
      "batch 279, loss: 0.2213, instance_loss: 0.0110, weighted_loss: 0.1582, label: 1, bag_size: 19972\n",
      "batch 299, loss: 0.1990, instance_loss: 0.0000, weighted_loss: 0.1393, label: 0, bag_size: 25814\n",
      "batch 319, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 16087\n",
      "batch 339, loss: 0.0463, instance_loss: 0.2837, weighted_loss: 0.1175, label: 0, bag_size: 2006\n",
      "batch 359, loss: 0.0052, instance_loss: 0.4452, weighted_loss: 0.1372, label: 1, bag_size: 2455\n",
      "batch 379, loss: 0.0759, instance_loss: 0.0001, weighted_loss: 0.0532, label: 1, bag_size: 5454\n",
      "batch 399, loss: 0.0041, instance_loss: 0.0102, weighted_loss: 0.0059, label: 0, bag_size: 1588\n",
      "batch 419, loss: 0.0037, instance_loss: 0.0003, weighted_loss: 0.0027, label: 0, bag_size: 1560\n",
      "batch 439, loss: 0.7398, instance_loss: 0.3842, weighted_loss: 0.6331, label: 1, bag_size: 1095\n",
      "batch 459, loss: 0.0169, instance_loss: 0.0865, weighted_loss: 0.0378, label: 1, bag_size: 4039\n",
      "batch 479, loss: 0.0239, instance_loss: 0.0111, weighted_loss: 0.0201, label: 0, bag_size: 4997\n",
      "batch 499, loss: 0.2448, instance_loss: 0.6519, weighted_loss: 0.3669, label: 0, bag_size: 2213\n",
      "batch 519, loss: 0.0044, instance_loss: 0.0016, weighted_loss: 0.0036, label: 1, bag_size: 10396\n",
      "batch 539, loss: 0.0147, instance_loss: 0.3211, weighted_loss: 0.1067, label: 1, bag_size: 1746\n",
      "batch 559, loss: 0.0252, instance_loss: 0.0885, weighted_loss: 0.0442, label: 0, bag_size: 1149\n",
      "batch 579, loss: 0.0669, instance_loss: 0.0010, weighted_loss: 0.0472, label: 1, bag_size: 6736\n",
      "batch 599, loss: 0.0243, instance_loss: 0.0142, weighted_loss: 0.0213, label: 1, bag_size: 16162\n",
      "batch 619, loss: 0.5006, instance_loss: 0.4132, weighted_loss: 0.4744, label: 0, bag_size: 11212\n",
      "batch 639, loss: 0.0011, instance_loss: 0.0005, weighted_loss: 0.0009, label: 0, bag_size: 518\n",
      "batch 659, loss: 0.1627, instance_loss: 0.0000, weighted_loss: 0.1139, label: 1, bag_size: 34356\n",
      "batch 679, loss: 0.1035, instance_loss: 0.0000, weighted_loss: 0.0725, label: 1, bag_size: 13732\n",
      "batch 699, loss: 0.0103, instance_loss: 0.0000, weighted_loss: 0.0072, label: 0, bag_size: 2457\n",
      "batch 719, loss: 0.0205, instance_loss: 0.3670, weighted_loss: 0.1244, label: 0, bag_size: 1416\n",
      "batch 739, loss: 0.0080, instance_loss: 0.0019, weighted_loss: 0.0061, label: 1, bag_size: 12611\n",
      "batch 759, loss: 0.0091, instance_loss: 0.0000, weighted_loss: 0.0064, label: 1, bag_size: 12611\n",
      "batch 779, loss: 0.0083, instance_loss: 0.0006, weighted_loss: 0.0060, label: 0, bag_size: 9234\n",
      "batch 799, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 13225\n",
      "batch 819, loss: 0.0129, instance_loss: 0.0008, weighted_loss: 0.0093, label: 1, bag_size: 6090\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9756859756097561: correct 12801/13120\n",
      "class 1 clustering acc 0.8814024390243902: correct 5782/6560\n",
      "Epoch: 44, train_loss: 0.1785, train_clustering_loss:  0.2389, train_error: 0.0598\n",
      "class 0: acc 0.9458823529411765, correct 402/425\n",
      "class 1: acc 0.9341772151898734, correct 369/395\n",
      "\n",
      "Val Set, val_loss: 0.1690, val_error: 0.0727, auc: 0.9884\n",
      "class 0 clustering acc 0.9670454545454545: correct 1702/1760\n",
      "class 1 clustering acc 0.8056818181818182: correct 709/880\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0595, instance_loss: 0.0123, weighted_loss: 0.0454, label: 1, bag_size: 4939\n",
      "batch 39, loss: 0.0054, instance_loss: 0.0271, weighted_loss: 0.0119, label: 1, bag_size: 7381\n",
      "batch 59, loss: 0.0043, instance_loss: 0.0000, weighted_loss: 0.0030, label: 0, bag_size: 4902\n",
      "batch 79, loss: 0.0249, instance_loss: 0.0002, weighted_loss: 0.0175, label: 0, bag_size: 10444\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 10920\n",
      "batch 119, loss: 0.0977, instance_loss: 0.0000, weighted_loss: 0.0684, label: 0, bag_size: 3541\n",
      "batch 139, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 19472\n",
      "batch 159, loss: 0.0123, instance_loss: 0.0000, weighted_loss: 0.0086, label: 1, bag_size: 6745\n",
      "batch 179, loss: 0.0037, instance_loss: 0.0000, weighted_loss: 0.0026, label: 0, bag_size: 14266\n",
      "batch 199, loss: 0.0333, instance_loss: 0.0699, weighted_loss: 0.0443, label: 0, bag_size: 7605\n",
      "batch 219, loss: 0.1784, instance_loss: 0.1192, weighted_loss: 0.1606, label: 0, bag_size: 8744\n",
      "batch 239, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 15665\n",
      "batch 259, loss: 0.1506, instance_loss: 0.8006, weighted_loss: 0.3456, label: 1, bag_size: 12626\n",
      "batch 279, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 3228\n",
      "batch 299, loss: 0.3612, instance_loss: 0.0920, weighted_loss: 0.2804, label: 1, bag_size: 4786\n",
      "batch 319, loss: 0.0184, instance_loss: 0.9537, weighted_loss: 0.2990, label: 0, bag_size: 2367\n",
      "batch 339, loss: 0.0002, instance_loss: 0.0255, weighted_loss: 0.0078, label: 0, bag_size: 10898\n",
      "batch 359, loss: 0.0004, instance_loss: 0.0236, weighted_loss: 0.0074, label: 0, bag_size: 31106\n",
      "batch 379, loss: 0.0271, instance_loss: 0.0559, weighted_loss: 0.0358, label: 0, bag_size: 4902\n",
      "batch 399, loss: 1.0898, instance_loss: 0.0589, weighted_loss: 0.7805, label: 0, bag_size: 21361\n",
      "batch 419, loss: 0.0328, instance_loss: 2.6506, weighted_loss: 0.8182, label: 1, bag_size: 2455\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 9610\n",
      "batch 459, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 12865\n",
      "batch 479, loss: 0.0038, instance_loss: 0.0925, weighted_loss: 0.0305, label: 0, bag_size: 15967\n",
      "batch 499, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0011, label: 1, bag_size: 6950\n",
      "batch 519, loss: 0.1266, instance_loss: 0.0069, weighted_loss: 0.0907, label: 1, bag_size: 1493\n",
      "batch 539, loss: 0.0137, instance_loss: 0.2728, weighted_loss: 0.0915, label: 0, bag_size: 4497\n",
      "batch 559, loss: 0.0130, instance_loss: 0.0103, weighted_loss: 0.0122, label: 0, bag_size: 16607\n",
      "batch 579, loss: 0.0044, instance_loss: 0.0030, weighted_loss: 0.0040, label: 1, bag_size: 2662\n",
      "batch 599, loss: 0.0737, instance_loss: 0.0030, weighted_loss: 0.0525, label: 1, bag_size: 19972\n",
      "batch 619, loss: 0.0018, instance_loss: 0.0334, weighted_loss: 0.0113, label: 0, bag_size: 890\n",
      "batch 639, loss: 0.0443, instance_loss: 0.0816, weighted_loss: 0.0555, label: 0, bag_size: 6898\n",
      "batch 659, loss: 0.1873, instance_loss: 0.3352, weighted_loss: 0.2316, label: 0, bag_size: 65728\n",
      "batch 679, loss: 0.0056, instance_loss: 0.0149, weighted_loss: 0.0084, label: 1, bag_size: 8660\n",
      "batch 699, loss: 0.4338, instance_loss: 0.6306, weighted_loss: 0.4928, label: 1, bag_size: 5366\n",
      "batch 719, loss: 0.1188, instance_loss: 0.8641, weighted_loss: 0.3424, label: 0, bag_size: 4598\n",
      "batch 739, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 31106\n",
      "batch 759, loss: 0.2278, instance_loss: 0.2032, weighted_loss: 0.2204, label: 0, bag_size: 6356\n",
      "batch 779, loss: 0.0155, instance_loss: 0.0036, weighted_loss: 0.0119, label: 0, bag_size: 20555\n",
      "batch 799, loss: 0.0019, instance_loss: 0.0015, weighted_loss: 0.0018, label: 1, bag_size: 6734\n",
      "batch 819, loss: 0.0105, instance_loss: 0.0056, weighted_loss: 0.0091, label: 0, bag_size: 16607\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9798018292682927: correct 12855/13120\n",
      "class 1 clustering acc 0.905640243902439: correct 5941/6560\n",
      "Epoch: 45, train_loss: 0.1839, train_clustering_loss:  0.1789, train_error: 0.0720\n",
      "class 0: acc 0.9123711340206185, correct 354/388\n",
      "class 1: acc 0.9421296296296297, correct 407/432\n",
      "\n",
      "Val Set, val_loss: 0.1545, val_error: 0.0727, auc: 0.9894\n",
      "class 0 clustering acc 0.9392045454545455: correct 1653/1760\n",
      "class 1 clustering acc 0.8443181818181819: correct 743/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.896551724137931, correct 52/58\n",
      "Validation loss decreased (0.160687 --> 0.154479).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0034, instance_loss: 0.0141, weighted_loss: 0.0066, label: 0, bag_size: 8252\n",
      "batch 39, loss: 0.1457, instance_loss: 0.0302, weighted_loss: 0.1110, label: 0, bag_size: 3502\n",
      "batch 59, loss: 0.0026, instance_loss: 0.0000, weighted_loss: 0.0018, label: 0, bag_size: 4271\n",
      "batch 79, loss: 0.0610, instance_loss: 1.8952, weighted_loss: 0.6112, label: 1, bag_size: 865\n",
      "batch 99, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 1781\n",
      "batch 119, loss: 0.0176, instance_loss: 0.0781, weighted_loss: 0.0357, label: 1, bag_size: 5907\n",
      "batch 139, loss: 0.1569, instance_loss: 0.0487, weighted_loss: 0.1244, label: 0, bag_size: 12083\n",
      "batch 159, loss: 0.0406, instance_loss: 0.0029, weighted_loss: 0.0293, label: 1, bag_size: 16162\n",
      "batch 179, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 5225\n",
      "batch 199, loss: 0.0099, instance_loss: 0.0000, weighted_loss: 0.0069, label: 0, bag_size: 22681\n",
      "batch 219, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 10725\n",
      "batch 239, loss: 0.0284, instance_loss: 0.0282, weighted_loss: 0.0283, label: 1, bag_size: 11394\n",
      "batch 259, loss: 0.0695, instance_loss: 0.0001, weighted_loss: 0.0487, label: 1, bag_size: 16565\n",
      "batch 279, loss: 0.0099, instance_loss: 0.0582, weighted_loss: 0.0244, label: 0, bag_size: 1416\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 9644\n",
      "batch 319, loss: 0.0010, instance_loss: 0.0286, weighted_loss: 0.0093, label: 1, bag_size: 11387\n",
      "batch 339, loss: 0.0010, instance_loss: 0.0051, weighted_loss: 0.0022, label: 1, bag_size: 2936\n",
      "batch 359, loss: 0.0706, instance_loss: 0.8586, weighted_loss: 0.3070, label: 1, bag_size: 1525\n",
      "batch 379, loss: 0.0039, instance_loss: 0.0001, weighted_loss: 0.0028, label: 1, bag_size: 14433\n",
      "batch 399, loss: 0.0822, instance_loss: 0.0023, weighted_loss: 0.0582, label: 1, bag_size: 6731\n",
      "batch 419, loss: 0.0050, instance_loss: 0.0000, weighted_loss: 0.0035, label: 0, bag_size: 14681\n",
      "batch 439, loss: 1.6659, instance_loss: 3.7861, weighted_loss: 2.3019, label: 1, bag_size: 1095\n",
      "batch 459, loss: 0.0063, instance_loss: 0.0033, weighted_loss: 0.0054, label: 1, bag_size: 2356\n",
      "batch 479, loss: 0.0129, instance_loss: 0.1259, weighted_loss: 0.0468, label: 0, bag_size: 2382\n",
      "batch 499, loss: 0.0072, instance_loss: 0.2094, weighted_loss: 0.0678, label: 1, bag_size: 8448\n",
      "batch 519, loss: 0.0086, instance_loss: 0.1789, weighted_loss: 0.0597, label: 0, bag_size: 2360\n",
      "batch 539, loss: 0.0092, instance_loss: 0.0002, weighted_loss: 0.0065, label: 1, bag_size: 17769\n",
      "batch 559, loss: 0.0053, instance_loss: 0.0000, weighted_loss: 0.0037, label: 1, bag_size: 13194\n",
      "batch 579, loss: 0.0197, instance_loss: 0.0048, weighted_loss: 0.0152, label: 1, bag_size: 7246\n",
      "batch 599, loss: 0.0379, instance_loss: 0.0591, weighted_loss: 0.0443, label: 1, bag_size: 1838\n",
      "batch 619, loss: 0.0028, instance_loss: 0.2204, weighted_loss: 0.0681, label: 0, bag_size: 3101\n",
      "batch 639, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 18225\n",
      "batch 659, loss: 0.0205, instance_loss: 0.4230, weighted_loss: 0.1412, label: 1, bag_size: 3856\n",
      "batch 679, loss: 0.0447, instance_loss: 0.0000, weighted_loss: 0.0313, label: 1, bag_size: 21701\n",
      "batch 699, loss: 0.0294, instance_loss: 0.1495, weighted_loss: 0.0654, label: 0, bag_size: 2382\n",
      "batch 719, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 9471\n",
      "batch 739, loss: 0.0057, instance_loss: 0.0948, weighted_loss: 0.0324, label: 1, bag_size: 2759\n",
      "batch 759, loss: 0.0437, instance_loss: 0.0000, weighted_loss: 0.0306, label: 0, bag_size: 11122\n",
      "batch 779, loss: 0.1934, instance_loss: 0.8603, weighted_loss: 0.3935, label: 0, bag_size: 9132\n",
      "batch 799, loss: 0.0379, instance_loss: 0.2554, weighted_loss: 0.1032, label: 1, bag_size: 1014\n",
      "batch 819, loss: 0.2517, instance_loss: 0.0442, weighted_loss: 0.1894, label: 0, bag_size: 11128\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9786585365853658: correct 12840/13120\n",
      "class 1 clustering acc 0.8940548780487805: correct 5865/6560\n",
      "Epoch: 46, train_loss: 0.1753, train_clustering_loss:  0.1972, train_error: 0.0610\n",
      "class 0: acc 0.9326683291770573, correct 374/401\n",
      "class 1: acc 0.9451073985680191, correct 396/419\n",
      "\n",
      "Val Set, val_loss: 0.1571, val_error: 0.0818, auc: 0.9887\n",
      "class 0 clustering acc 0.9454545454545454: correct 1664/1760\n",
      "class 1 clustering acc 0.8409090909090909: correct 740/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0831, instance_loss: 0.1413, weighted_loss: 0.1006, label: 1, bag_size: 4054\n",
      "batch 39, loss: 0.0003, instance_loss: 0.0507, weighted_loss: 0.0154, label: 1, bag_size: 1412\n",
      "batch 59, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 10068\n",
      "batch 79, loss: 0.3727, instance_loss: 0.5097, weighted_loss: 0.4138, label: 1, bag_size: 8982\n",
      "batch 99, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 6734\n",
      "batch 119, loss: 0.0174, instance_loss: 0.0225, weighted_loss: 0.0190, label: 1, bag_size: 13786\n",
      "batch 139, loss: 0.0063, instance_loss: 0.0012, weighted_loss: 0.0048, label: 1, bag_size: 7935\n",
      "batch 159, loss: 0.0015, instance_loss: 0.0002, weighted_loss: 0.0011, label: 0, bag_size: 10263\n",
      "batch 179, loss: 0.0700, instance_loss: 0.2232, weighted_loss: 0.1159, label: 1, bag_size: 12712\n",
      "batch 199, loss: 0.0118, instance_loss: 0.0654, weighted_loss: 0.0278, label: 1, bag_size: 8660\n",
      "batch 219, loss: 0.0058, instance_loss: 0.0000, weighted_loss: 0.0041, label: 0, bag_size: 18215\n",
      "batch 239, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 5551\n",
      "batch 259, loss: 0.0030, instance_loss: 0.0137, weighted_loss: 0.0062, label: 0, bag_size: 12687\n",
      "batch 279, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 15914\n",
      "batch 299, loss: 0.2114, instance_loss: 0.7953, weighted_loss: 0.3866, label: 1, bag_size: 1963\n",
      "batch 319, loss: 0.3394, instance_loss: 0.1543, weighted_loss: 0.2839, label: 0, bag_size: 1690\n",
      "batch 339, loss: 0.2339, instance_loss: 0.3130, weighted_loss: 0.2576, label: 1, bag_size: 6682\n",
      "batch 359, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 31780\n",
      "batch 379, loss: 0.0785, instance_loss: 0.2832, weighted_loss: 0.1399, label: 1, bag_size: 2522\n",
      "batch 399, loss: 0.0148, instance_loss: 0.0000, weighted_loss: 0.0103, label: 0, bag_size: 9866\n",
      "batch 419, loss: 0.0090, instance_loss: 0.0158, weighted_loss: 0.0111, label: 1, bag_size: 9533\n",
      "batch 439, loss: 0.0022, instance_loss: 0.0450, weighted_loss: 0.0150, label: 0, bag_size: 3190\n",
      "batch 459, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 15008\n",
      "batch 479, loss: 0.0834, instance_loss: 0.1040, weighted_loss: 0.0896, label: 0, bag_size: 1690\n",
      "batch 499, loss: 0.0004, instance_loss: 0.2069, weighted_loss: 0.0624, label: 0, bag_size: 15313\n",
      "batch 519, loss: 0.0041, instance_loss: 0.0289, weighted_loss: 0.0116, label: 0, bag_size: 12212\n",
      "batch 539, loss: 0.0139, instance_loss: 0.0077, weighted_loss: 0.0120, label: 0, bag_size: 8755\n",
      "batch 559, loss: 1.0161, instance_loss: 0.6150, weighted_loss: 0.8957, label: 1, bag_size: 1683\n",
      "batch 579, loss: 0.0840, instance_loss: 0.0880, weighted_loss: 0.0852, label: 0, bag_size: 16607\n",
      "batch 599, loss: 0.0507, instance_loss: 0.0020, weighted_loss: 0.0361, label: 1, bag_size: 19606\n",
      "batch 619, loss: 0.0017, instance_loss: 0.5736, weighted_loss: 0.1732, label: 0, bag_size: 2820\n",
      "batch 639, loss: 0.0273, instance_loss: 0.1938, weighted_loss: 0.0772, label: 1, bag_size: 14604\n",
      "batch 659, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 6966\n",
      "batch 679, loss: 0.2801, instance_loss: 0.0216, weighted_loss: 0.2026, label: 0, bag_size: 15898\n",
      "batch 699, loss: 0.2468, instance_loss: 0.5163, weighted_loss: 0.3276, label: 1, bag_size: 1920\n",
      "batch 719, loss: 3.4350, instance_loss: 2.7741, weighted_loss: 3.2367, label: 0, bag_size: 2815\n",
      "batch 739, loss: 0.0063, instance_loss: 0.0282, weighted_loss: 0.0128, label: 0, bag_size: 17155\n",
      "batch 759, loss: 0.0076, instance_loss: 0.0243, weighted_loss: 0.0126, label: 1, bag_size: 4862\n",
      "batch 779, loss: 0.0103, instance_loss: 0.0181, weighted_loss: 0.0126, label: 1, bag_size: 1759\n",
      "batch 799, loss: 0.0091, instance_loss: 0.0000, weighted_loss: 0.0064, label: 1, bag_size: 13786\n",
      "batch 819, loss: 0.0362, instance_loss: 0.0148, weighted_loss: 0.0298, label: 0, bag_size: 19470\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9770579268292683: correct 12819/13120\n",
      "class 1 clustering acc 0.8922256097560975: correct 5853/6560\n",
      "Epoch: 47, train_loss: 0.1800, train_clustering_loss:  0.2067, train_error: 0.0646\n",
      "class 0: acc 0.930952380952381, correct 391/420\n",
      "class 1: acc 0.94, correct 376/400\n",
      "\n",
      "Val Set, val_loss: 0.2870, val_error: 0.1455, auc: 0.9901\n",
      "class 0 clustering acc 0.9460227272727273: correct 1665/1760\n",
      "class 1 clustering acc 0.7613636363636364: correct 670/880\n",
      "class 0: acc 0.6923076923076923, correct 36/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0003, instance_loss: 0.0057, weighted_loss: 0.0019, label: 0, bag_size: 16936\n",
      "batch 39, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 11195\n",
      "batch 59, loss: 0.0092, instance_loss: 0.0790, weighted_loss: 0.0301, label: 0, bag_size: 4959\n",
      "batch 79, loss: 0.2313, instance_loss: 0.6986, weighted_loss: 0.3715, label: 1, bag_size: 4929\n",
      "batch 99, loss: 0.0774, instance_loss: 0.0747, weighted_loss: 0.0766, label: 0, bag_size: 2242\n",
      "batch 119, loss: 0.0023, instance_loss: 0.0618, weighted_loss: 0.0201, label: 0, bag_size: 3190\n",
      "batch 139, loss: 0.0009, instance_loss: 0.0006, weighted_loss: 0.0008, label: 1, bag_size: 8522\n",
      "batch 159, loss: 0.0179, instance_loss: 0.0023, weighted_loss: 0.0132, label: 1, bag_size: 12697\n",
      "batch 179, loss: 0.0037, instance_loss: 0.1217, weighted_loss: 0.0391, label: 0, bag_size: 6850\n",
      "batch 199, loss: 0.2228, instance_loss: 0.1027, weighted_loss: 0.1868, label: 0, bag_size: 7835\n",
      "batch 219, loss: 0.0086, instance_loss: 0.0000, weighted_loss: 0.0060, label: 1, bag_size: 14681\n",
      "batch 239, loss: 0.0155, instance_loss: 1.2464, weighted_loss: 0.3848, label: 1, bag_size: 549\n",
      "batch 259, loss: 0.2864, instance_loss: 0.0639, weighted_loss: 0.2196, label: 0, bag_size: 18738\n",
      "batch 279, loss: 0.0334, instance_loss: 0.6620, weighted_loss: 0.2219, label: 0, bag_size: 2043\n",
      "batch 299, loss: 0.0109, instance_loss: 0.0831, weighted_loss: 0.0326, label: 0, bag_size: 25420\n",
      "batch 319, loss: 0.0168, instance_loss: 0.0038, weighted_loss: 0.0129, label: 1, bag_size: 3674\n",
      "batch 339, loss: 1.6785, instance_loss: 0.8663, weighted_loss: 1.4348, label: 0, bag_size: 11128\n",
      "batch 359, loss: 0.1453, instance_loss: 0.0000, weighted_loss: 0.1017, label: 0, bag_size: 25814\n",
      "batch 379, loss: 0.2474, instance_loss: 1.9031, weighted_loss: 0.7441, label: 1, bag_size: 1242\n",
      "batch 399, loss: 0.0066, instance_loss: 0.0201, weighted_loss: 0.0106, label: 0, bag_size: 2036\n",
      "batch 419, loss: 0.0746, instance_loss: 0.6410, weighted_loss: 0.2445, label: 0, bag_size: 1498\n",
      "batch 439, loss: 0.0171, instance_loss: 0.0002, weighted_loss: 0.0120, label: 1, bag_size: 16379\n",
      "batch 459, loss: 1.5288, instance_loss: 1.5288, weighted_loss: 1.5288, label: 0, bag_size: 1637\n",
      "batch 479, loss: 0.0165, instance_loss: 0.6768, weighted_loss: 0.2146, label: 0, bag_size: 1772\n",
      "batch 499, loss: 0.0630, instance_loss: 0.3702, weighted_loss: 0.1552, label: 1, bag_size: 7424\n",
      "batch 519, loss: 0.0125, instance_loss: 0.0026, weighted_loss: 0.0096, label: 0, bag_size: 14681\n",
      "batch 539, loss: 0.0764, instance_loss: 0.0247, weighted_loss: 0.0609, label: 0, bag_size: 1831\n",
      "batch 559, loss: 0.0004, instance_loss: 0.1864, weighted_loss: 0.0562, label: 0, bag_size: 9885\n",
      "batch 579, loss: 0.1981, instance_loss: 1.8370, weighted_loss: 0.6898, label: 1, bag_size: 1609\n",
      "batch 599, loss: 0.0044, instance_loss: 0.1011, weighted_loss: 0.0334, label: 0, bag_size: 1745\n",
      "batch 619, loss: 0.0812, instance_loss: 0.0238, weighted_loss: 0.0640, label: 0, bag_size: 3502\n",
      "batch 639, loss: 0.0044, instance_loss: 0.0181, weighted_loss: 0.0085, label: 0, bag_size: 9470\n",
      "batch 659, loss: 0.0038, instance_loss: 0.0519, weighted_loss: 0.0183, label: 0, bag_size: 2548\n",
      "batch 679, loss: 0.0166, instance_loss: 0.0005, weighted_loss: 0.0118, label: 0, bag_size: 17630\n",
      "batch 699, loss: 2.3201, instance_loss: 0.0202, weighted_loss: 1.6302, label: 0, bag_size: 11212\n",
      "batch 719, loss: 0.0072, instance_loss: 0.0003, weighted_loss: 0.0051, label: 1, bag_size: 2385\n",
      "batch 739, loss: 0.0179, instance_loss: 0.0023, weighted_loss: 0.0132, label: 1, bag_size: 20161\n",
      "batch 759, loss: 0.0186, instance_loss: 0.0000, weighted_loss: 0.0131, label: 0, bag_size: 23791\n",
      "batch 779, loss: 0.0635, instance_loss: 0.0008, weighted_loss: 0.0447, label: 1, bag_size: 13026\n",
      "batch 799, loss: 0.0091, instance_loss: 0.0000, weighted_loss: 0.0064, label: 0, bag_size: 21864\n",
      "batch 819, loss: 1.0544, instance_loss: 0.2307, weighted_loss: 0.8073, label: 1, bag_size: 9215\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9762195121951219: correct 12808/13120\n",
      "class 1 clustering acc 0.8785060975609756: correct 5763/6560\n",
      "Epoch: 48, train_loss: 0.1871, train_clustering_loss:  0.2245, train_error: 0.0707\n",
      "class 0: acc 0.9328703703703703, correct 403/432\n",
      "class 1: acc 0.9252577319587629, correct 359/388\n",
      "\n",
      "Val Set, val_loss: 0.1619, val_error: 0.0818, auc: 0.9904\n",
      "class 0 clustering acc 0.9346590909090909: correct 1645/1760\n",
      "class 1 clustering acc 0.8136363636363636: correct 716/880\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0244, instance_loss: 0.0679, weighted_loss: 0.0375, label: 0, bag_size: 2760\n",
      "batch 39, loss: 1.1739, instance_loss: 2.5366, weighted_loss: 1.5827, label: 0, bag_size: 7239\n",
      "batch 59, loss: 0.0004, instance_loss: 0.0150, weighted_loss: 0.0048, label: 0, bag_size: 9949\n",
      "batch 79, loss: 0.0707, instance_loss: 0.1153, weighted_loss: 0.0841, label: 0, bag_size: 11390\n",
      "batch 99, loss: 0.0168, instance_loss: 0.1441, weighted_loss: 0.0550, label: 1, bag_size: 4039\n",
      "batch 119, loss: 0.0003, instance_loss: 0.0961, weighted_loss: 0.0290, label: 0, bag_size: 9885\n",
      "batch 139, loss: 0.0052, instance_loss: 0.0022, weighted_loss: 0.0043, label: 1, bag_size: 14030\n",
      "batch 159, loss: 0.0974, instance_loss: 0.0217, weighted_loss: 0.0747, label: 0, bag_size: 2732\n",
      "batch 179, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 14515\n",
      "batch 199, loss: 0.1658, instance_loss: 0.1394, weighted_loss: 0.1578, label: 1, bag_size: 2681\n",
      "batch 219, loss: 0.0387, instance_loss: 0.0270, weighted_loss: 0.0352, label: 0, bag_size: 1438\n",
      "batch 239, loss: 0.0326, instance_loss: 0.0000, weighted_loss: 0.0228, label: 1, bag_size: 9230\n",
      "batch 259, loss: 0.2625, instance_loss: 1.2488, weighted_loss: 0.5584, label: 0, bag_size: 2213\n",
      "batch 279, loss: 0.0041, instance_loss: 0.0000, weighted_loss: 0.0028, label: 1, bag_size: 9571\n",
      "batch 299, loss: 0.1129, instance_loss: 0.0639, weighted_loss: 0.0982, label: 0, bag_size: 3657\n",
      "batch 319, loss: 0.0035, instance_loss: 0.0019, weighted_loss: 0.0030, label: 0, bag_size: 23796\n",
      "batch 339, loss: 0.0114, instance_loss: 0.0000, weighted_loss: 0.0080, label: 1, bag_size: 9571\n",
      "batch 359, loss: 0.0060, instance_loss: 0.0558, weighted_loss: 0.0210, label: 1, bag_size: 6453\n",
      "batch 379, loss: 0.8217, instance_loss: 0.8648, weighted_loss: 0.8346, label: 1, bag_size: 1683\n",
      "batch 399, loss: 0.6107, instance_loss: 2.0820, weighted_loss: 1.0521, label: 1, bag_size: 1831\n",
      "batch 419, loss: 0.0007, instance_loss: 0.0207, weighted_loss: 0.0067, label: 0, bag_size: 11512\n",
      "batch 439, loss: 0.0015, instance_loss: 0.0053, weighted_loss: 0.0026, label: 0, bag_size: 21076\n",
      "batch 459, loss: 0.9953, instance_loss: 0.0119, weighted_loss: 0.7003, label: 0, bag_size: 11212\n",
      "batch 479, loss: 0.0025, instance_loss: 0.0080, weighted_loss: 0.0041, label: 1, bag_size: 9446\n",
      "batch 499, loss: 0.0168, instance_loss: 0.0000, weighted_loss: 0.0118, label: 1, bag_size: 11600\n",
      "batch 519, loss: 0.0107, instance_loss: 0.0042, weighted_loss: 0.0088, label: 1, bag_size: 4239\n",
      "batch 539, loss: 0.0001, instance_loss: 0.2227, weighted_loss: 0.0669, label: 0, bag_size: 9060\n",
      "batch 559, loss: 0.0115, instance_loss: 0.1016, weighted_loss: 0.0386, label: 0, bag_size: 2322\n",
      "batch 579, loss: 0.6733, instance_loss: 0.9840, weighted_loss: 0.7665, label: 1, bag_size: 3121\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 10592\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0207, weighted_loss: 0.0063, label: 0, bag_size: 14206\n",
      "batch 639, loss: 0.0004, instance_loss: 0.0253, weighted_loss: 0.0079, label: 0, bag_size: 13225\n",
      "batch 659, loss: 0.0250, instance_loss: 0.0805, weighted_loss: 0.0417, label: 1, bag_size: 10072\n",
      "batch 679, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 14202\n",
      "batch 699, loss: 0.1646, instance_loss: 0.1482, weighted_loss: 0.1597, label: 0, bag_size: 1690\n",
      "batch 719, loss: 0.0821, instance_loss: 2.4390, weighted_loss: 0.7892, label: 0, bag_size: 14249\n",
      "batch 739, loss: 0.0130, instance_loss: 0.0813, weighted_loss: 0.0335, label: 0, bag_size: 1884\n",
      "batch 759, loss: 0.0064, instance_loss: 1.5340, weighted_loss: 0.4646, label: 1, bag_size: 22264\n",
      "batch 779, loss: 0.0015, instance_loss: 0.0819, weighted_loss: 0.0257, label: 0, bag_size: 12524\n",
      "batch 799, loss: 0.0727, instance_loss: 0.0000, weighted_loss: 0.0509, label: 1, bag_size: 34356\n",
      "batch 819, loss: 0.1844, instance_loss: 0.1523, weighted_loss: 0.1747, label: 1, bag_size: 5160\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9757621951219512: correct 12802/13120\n",
      "class 1 clustering acc 0.8958841463414634: correct 5877/6560\n",
      "Epoch: 49, train_loss: 0.1667, train_clustering_loss:  0.2202, train_error: 0.0585\n",
      "class 0: acc 0.9364303178484108, correct 383/409\n",
      "class 1: acc 0.9464720194647201, correct 389/411\n",
      "\n",
      "Val Set, val_loss: 0.1516, val_error: 0.0636, auc: 0.9914\n",
      "class 0 clustering acc 0.9301136363636363: correct 1637/1760\n",
      "class 1 clustering acc 0.7681818181818182: correct 676/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9137931034482759, correct 53/58\n",
      "Validation loss decreased (0.154479 --> 0.151606).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0049, instance_loss: 0.0000, weighted_loss: 0.0034, label: 1, bag_size: 10482\n",
      "batch 39, loss: 0.0658, instance_loss: 0.4903, weighted_loss: 0.1931, label: 1, bag_size: 2179\n",
      "batch 59, loss: 0.1274, instance_loss: 0.0004, weighted_loss: 0.0893, label: 1, bag_size: 7613\n",
      "batch 79, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 12931\n",
      "batch 99, loss: 0.0660, instance_loss: 0.0000, weighted_loss: 0.0462, label: 1, bag_size: 5629\n",
      "batch 119, loss: 0.0024, instance_loss: 0.0128, weighted_loss: 0.0055, label: 0, bag_size: 14206\n",
      "batch 139, loss: 0.0240, instance_loss: 0.2039, weighted_loss: 0.0780, label: 1, bag_size: 1123\n",
      "batch 159, loss: 0.0019, instance_loss: 0.0000, weighted_loss: 0.0013, label: 1, bag_size: 16034\n",
      "batch 179, loss: 0.0012, instance_loss: 0.0440, weighted_loss: 0.0140, label: 0, bag_size: 11477\n",
      "batch 199, loss: 0.0023, instance_loss: 0.0502, weighted_loss: 0.0167, label: 0, bag_size: 10068\n",
      "batch 219, loss: 0.0104, instance_loss: 0.0000, weighted_loss: 0.0073, label: 1, bag_size: 10501\n",
      "batch 239, loss: 0.0064, instance_loss: 0.0554, weighted_loss: 0.0211, label: 0, bag_size: 1234\n",
      "batch 259, loss: 0.0304, instance_loss: 0.0113, weighted_loss: 0.0246, label: 1, bag_size: 8003\n",
      "batch 279, loss: 0.0011, instance_loss: 0.0540, weighted_loss: 0.0170, label: 0, bag_size: 1824\n",
      "batch 299, loss: 0.0041, instance_loss: 0.3135, weighted_loss: 0.0969, label: 0, bag_size: 2457\n",
      "batch 319, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 16341\n",
      "batch 339, loss: 0.0071, instance_loss: 0.0000, weighted_loss: 0.0050, label: 0, bag_size: 22828\n",
      "batch 359, loss: 0.0009, instance_loss: 0.0001, weighted_loss: 0.0007, label: 1, bag_size: 14433\n",
      "batch 379, loss: 0.0233, instance_loss: 0.0064, weighted_loss: 0.0182, label: 0, bag_size: 24439\n",
      "batch 399, loss: 0.0795, instance_loss: 0.2422, weighted_loss: 0.1283, label: 0, bag_size: 2104\n",
      "batch 419, loss: 0.0032, instance_loss: 0.0000, weighted_loss: 0.0022, label: 0, bag_size: 21682\n",
      "batch 439, loss: 0.0033, instance_loss: 0.0036, weighted_loss: 0.0034, label: 0, bag_size: 19518\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0129, weighted_loss: 0.0040, label: 0, bag_size: 3459\n",
      "batch 479, loss: 0.1570, instance_loss: 0.2294, weighted_loss: 0.1787, label: 1, bag_size: 1755\n",
      "batch 499, loss: 0.0047, instance_loss: 0.0000, weighted_loss: 0.0033, label: 1, bag_size: 12460\n",
      "batch 519, loss: 0.0714, instance_loss: 0.5775, weighted_loss: 0.2233, label: 1, bag_size: 4821\n",
      "batch 539, loss: 0.0027, instance_loss: 0.2740, weighted_loss: 0.0841, label: 1, bag_size: 17769\n",
      "batch 559, loss: 4.7701, instance_loss: 0.0130, weighted_loss: 3.3430, label: 0, bag_size: 3897\n",
      "batch 579, loss: 0.0072, instance_loss: 0.0004, weighted_loss: 0.0051, label: 0, bag_size: 2873\n",
      "batch 599, loss: 0.0497, instance_loss: 0.1575, weighted_loss: 0.0821, label: 1, bag_size: 13732\n",
      "batch 619, loss: 0.0053, instance_loss: 0.0034, weighted_loss: 0.0047, label: 0, bag_size: 1072\n",
      "batch 639, loss: 0.1877, instance_loss: 0.0000, weighted_loss: 0.1314, label: 1, bag_size: 6731\n",
      "batch 659, loss: 0.2971, instance_loss: 0.1432, weighted_loss: 0.2509, label: 0, bag_size: 3321\n",
      "batch 679, loss: 0.7824, instance_loss: 0.0197, weighted_loss: 0.5536, label: 0, bag_size: 1953\n",
      "batch 699, loss: 0.0442, instance_loss: 0.0019, weighted_loss: 0.0315, label: 1, bag_size: 12719\n",
      "batch 719, loss: 0.0139, instance_loss: 0.0000, weighted_loss: 0.0098, label: 0, bag_size: 5965\n",
      "batch 739, loss: 0.0267, instance_loss: 0.1500, weighted_loss: 0.0637, label: 1, bag_size: 3224\n",
      "batch 759, loss: 0.0107, instance_loss: 0.0001, weighted_loss: 0.0075, label: 0, bag_size: 2044\n",
      "batch 779, loss: 0.0321, instance_loss: 0.2852, weighted_loss: 0.1080, label: 0, bag_size: 931\n",
      "batch 799, loss: 0.0156, instance_loss: 0.4411, weighted_loss: 0.1432, label: 0, bag_size: 3908\n",
      "batch 819, loss: 0.1184, instance_loss: 0.0136, weighted_loss: 0.0870, label: 0, bag_size: 20555\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.984375: correct 12915/13120\n",
      "class 1 clustering acc 0.9150914634146341: correct 6003/6560\n",
      "Epoch: 50, train_loss: 0.1328, train_clustering_loss:  0.1511, train_error: 0.0512\n",
      "class 0: acc 0.9444444444444444, correct 391/414\n",
      "class 1: acc 0.9532019704433498, correct 387/406\n",
      "\n",
      "Val Set, val_loss: 0.1787, val_error: 0.0727, auc: 0.9917\n",
      "class 0 clustering acc 0.9647727272727272: correct 1698/1760\n",
      "class 1 clustering acc 0.8568181818181818: correct 754/880\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0604, instance_loss: 0.0025, weighted_loss: 0.0430, label: 1, bag_size: 15125\n",
      "batch 39, loss: 0.1103, instance_loss: 0.0005, weighted_loss: 0.0774, label: 1, bag_size: 3683\n",
      "batch 59, loss: 0.0024, instance_loss: 0.0000, weighted_loss: 0.0017, label: 0, bag_size: 12731\n",
      "batch 79, loss: 0.0130, instance_loss: 0.0000, weighted_loss: 0.0091, label: 1, bag_size: 3450\n",
      "batch 99, loss: 0.0152, instance_loss: 0.0020, weighted_loss: 0.0112, label: 1, bag_size: 16162\n",
      "batch 119, loss: 1.1152, instance_loss: 0.5828, weighted_loss: 0.9555, label: 0, bag_size: 11306\n",
      "batch 139, loss: 0.0438, instance_loss: 0.3494, weighted_loss: 0.1355, label: 0, bag_size: 65728\n",
      "batch 159, loss: 0.0842, instance_loss: 0.5920, weighted_loss: 0.2366, label: 0, bag_size: 4997\n",
      "batch 179, loss: 0.0387, instance_loss: 0.0039, weighted_loss: 0.0283, label: 1, bag_size: 6736\n",
      "batch 199, loss: 0.0003, instance_loss: 0.0148, weighted_loss: 0.0046, label: 0, bag_size: 11477\n",
      "batch 219, loss: 0.0113, instance_loss: 0.1684, weighted_loss: 0.0584, label: 1, bag_size: 1437\n",
      "batch 239, loss: 0.0037, instance_loss: 0.0000, weighted_loss: 0.0026, label: 1, bag_size: 2638\n",
      "batch 259, loss: 0.0510, instance_loss: 0.0799, weighted_loss: 0.0596, label: 0, bag_size: 2654\n",
      "batch 279, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 4394\n",
      "batch 299, loss: 0.0108, instance_loss: 0.0000, weighted_loss: 0.0076, label: 0, bag_size: 1072\n",
      "batch 319, loss: 0.0310, instance_loss: 0.0000, weighted_loss: 0.0217, label: 1, bag_size: 34356\n",
      "batch 339, loss: 0.0009, instance_loss: 0.0144, weighted_loss: 0.0049, label: 0, bag_size: 890\n",
      "batch 359, loss: 0.0396, instance_loss: 0.9886, weighted_loss: 0.3243, label: 0, bag_size: 2367\n",
      "batch 379, loss: 0.0143, instance_loss: 0.0000, weighted_loss: 0.0100, label: 0, bag_size: 9252\n",
      "batch 399, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 1, bag_size: 9321\n",
      "batch 419, loss: 0.0639, instance_loss: 0.0008, weighted_loss: 0.0450, label: 1, bag_size: 6090\n",
      "batch 439, loss: 0.0054, instance_loss: 1.4581, weighted_loss: 0.4412, label: 1, bag_size: 2785\n",
      "batch 459, loss: 0.8969, instance_loss: 0.0000, weighted_loss: 0.6279, label: 0, bag_size: 23618\n",
      "batch 479, loss: 0.3970, instance_loss: 0.0001, weighted_loss: 0.2779, label: 1, bag_size: 11701\n",
      "batch 499, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 13880\n",
      "batch 519, loss: 0.0531, instance_loss: 0.0009, weighted_loss: 0.0374, label: 1, bag_size: 1823\n",
      "batch 539, loss: 0.0985, instance_loss: 0.0406, weighted_loss: 0.0811, label: 0, bag_size: 763\n",
      "batch 559, loss: 0.0148, instance_loss: 0.0565, weighted_loss: 0.0273, label: 1, bag_size: 10969\n",
      "batch 579, loss: 0.0813, instance_loss: 0.1251, weighted_loss: 0.0944, label: 1, bag_size: 9519\n",
      "batch 599, loss: 0.6087, instance_loss: 0.1973, weighted_loss: 0.4853, label: 1, bag_size: 6665\n",
      "batch 619, loss: 0.0022, instance_loss: 0.0000, weighted_loss: 0.0016, label: 1, bag_size: 16034\n",
      "batch 639, loss: 0.0167, instance_loss: 0.0000, weighted_loss: 0.0117, label: 1, bag_size: 18095\n",
      "batch 659, loss: 0.0154, instance_loss: 0.5227, weighted_loss: 0.1676, label: 1, bag_size: 8466\n",
      "batch 679, loss: 2.7033, instance_loss: 0.6375, weighted_loss: 2.0835, label: 0, bag_size: 2815\n",
      "batch 699, loss: 0.0097, instance_loss: 2.6107, weighted_loss: 0.7900, label: 0, bag_size: 2242\n",
      "batch 719, loss: 0.0147, instance_loss: 0.0000, weighted_loss: 0.0103, label: 0, bag_size: 3552\n",
      "batch 739, loss: 0.0151, instance_loss: 0.0134, weighted_loss: 0.0146, label: 0, bag_size: 14681\n",
      "batch 759, loss: 0.1508, instance_loss: 0.2743, weighted_loss: 0.1879, label: 1, bag_size: 1764\n",
      "batch 779, loss: 2.7277, instance_loss: 0.0171, weighted_loss: 1.9146, label: 0, bag_size: 5211\n",
      "batch 799, loss: 0.0014, instance_loss: 0.0261, weighted_loss: 0.0088, label: 0, bag_size: 1797\n",
      "batch 819, loss: 0.4435, instance_loss: 0.0639, weighted_loss: 0.3296, label: 1, bag_size: 4939\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9751524390243902: correct 12794/13120\n",
      "class 1 clustering acc 0.8782012195121951: correct 5761/6560\n",
      "Epoch: 51, train_loss: 0.1621, train_clustering_loss:  0.2477, train_error: 0.0573\n",
      "class 0: acc 0.9351851851851852, correct 404/432\n",
      "class 1: acc 0.9510309278350515, correct 369/388\n",
      "\n",
      "Val Set, val_loss: 0.1475, val_error: 0.0818, auc: 0.9934\n",
      "class 0 clustering acc 0.9607954545454546: correct 1691/1760\n",
      "class 1 clustering acc 0.821590909090909: correct 723/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "Validation loss decreased (0.151606 --> 0.147488).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0283, instance_loss: 0.1443, weighted_loss: 0.0631, label: 0, bag_size: 1483\n",
      "batch 39, loss: 0.0140, instance_loss: 0.0007, weighted_loss: 0.0100, label: 1, bag_size: 12758\n",
      "batch 59, loss: 0.0436, instance_loss: 0.0020, weighted_loss: 0.0311, label: 1, bag_size: 12425\n",
      "batch 79, loss: 0.0024, instance_loss: 0.0523, weighted_loss: 0.0174, label: 0, bag_size: 11146\n",
      "batch 99, loss: 0.0031, instance_loss: 0.7822, weighted_loss: 0.2369, label: 0, bag_size: 803\n",
      "batch 119, loss: 0.0795, instance_loss: 0.0036, weighted_loss: 0.0567, label: 1, bag_size: 5605\n",
      "batch 139, loss: 0.5542, instance_loss: 0.2838, weighted_loss: 0.4731, label: 0, bag_size: 2653\n",
      "batch 159, loss: 0.0019, instance_loss: 0.1039, weighted_loss: 0.0325, label: 0, bag_size: 1824\n",
      "batch 179, loss: 0.5090, instance_loss: 1.0354, weighted_loss: 0.6669, label: 1, bag_size: 5723\n",
      "batch 199, loss: 0.0122, instance_loss: 0.0451, weighted_loss: 0.0221, label: 1, bag_size: 5160\n",
      "batch 219, loss: 0.0003, instance_loss: 0.0093, weighted_loss: 0.0030, label: 1, bag_size: 1244\n",
      "batch 239, loss: 0.0002, instance_loss: 0.0009, weighted_loss: 0.0004, label: 1, bag_size: 6966\n",
      "batch 259, loss: 0.0337, instance_loss: 0.0198, weighted_loss: 0.0295, label: 1, bag_size: 2356\n",
      "batch 279, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 15332\n",
      "batch 299, loss: 0.0107, instance_loss: 0.0000, weighted_loss: 0.0075, label: 1, bag_size: 34356\n",
      "batch 319, loss: 0.0065, instance_loss: 0.0000, weighted_loss: 0.0045, label: 0, bag_size: 5551\n",
      "batch 339, loss: 0.0035, instance_loss: 0.0000, weighted_loss: 0.0025, label: 0, bag_size: 12201\n",
      "batch 359, loss: 0.7077, instance_loss: 0.7925, weighted_loss: 0.7331, label: 1, bag_size: 1831\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0214, weighted_loss: 0.0064, label: 1, bag_size: 9078\n",
      "batch 399, loss: 0.4295, instance_loss: 0.6718, weighted_loss: 0.5022, label: 1, bag_size: 771\n",
      "batch 419, loss: 0.0094, instance_loss: 0.0007, weighted_loss: 0.0068, label: 1, bag_size: 12758\n",
      "batch 439, loss: 0.0023, instance_loss: 0.0026, weighted_loss: 0.0024, label: 1, bag_size: 2278\n",
      "batch 459, loss: 0.0427, instance_loss: 0.0163, weighted_loss: 0.0348, label: 1, bag_size: 1512\n",
      "batch 479, loss: 0.0278, instance_loss: 1.7238, weighted_loss: 0.5366, label: 0, bag_size: 2609\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0085, weighted_loss: 0.0026, label: 1, bag_size: 3295\n",
      "batch 519, loss: 0.0003, instance_loss: 0.0001, weighted_loss: 0.0002, label: 1, bag_size: 3640\n",
      "batch 539, loss: 0.0887, instance_loss: 0.3591, weighted_loss: 0.1698, label: 0, bag_size: 2290\n",
      "batch 559, loss: 0.0244, instance_loss: 0.7865, weighted_loss: 0.2530, label: 1, bag_size: 549\n",
      "batch 579, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 14206\n",
      "batch 599, loss: 0.0442, instance_loss: 0.0274, weighted_loss: 0.0392, label: 1, bag_size: 8685\n",
      "batch 619, loss: 0.0066, instance_loss: 0.0339, weighted_loss: 0.0148, label: 0, bag_size: 3474\n",
      "batch 639, loss: 0.0069, instance_loss: 0.0011, weighted_loss: 0.0052, label: 0, bag_size: 11917\n",
      "batch 659, loss: 0.0036, instance_loss: 0.0419, weighted_loss: 0.0151, label: 0, bag_size: 518\n",
      "batch 679, loss: 0.0081, instance_loss: 0.0729, weighted_loss: 0.0276, label: 1, bag_size: 689\n",
      "batch 699, loss: 0.0036, instance_loss: 0.0111, weighted_loss: 0.0058, label: 0, bag_size: 2360\n",
      "batch 719, loss: 0.0001, instance_loss: 0.0154, weighted_loss: 0.0047, label: 1, bag_size: 6875\n",
      "batch 739, loss: 0.0003, instance_loss: 0.0001, weighted_loss: 0.0002, label: 0, bag_size: 9433\n",
      "batch 759, loss: 0.1586, instance_loss: 1.2404, weighted_loss: 0.4831, label: 0, bag_size: 1142\n",
      "batch 779, loss: 0.0226, instance_loss: 0.0076, weighted_loss: 0.0181, label: 1, bag_size: 7246\n",
      "batch 799, loss: 0.0038, instance_loss: 0.0492, weighted_loss: 0.0174, label: 1, bag_size: 3450\n",
      "batch 819, loss: 0.0496, instance_loss: 0.0076, weighted_loss: 0.0370, label: 1, bag_size: 7424\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9814024390243903: correct 12876/13120\n",
      "class 1 clustering acc 0.9077743902439024: correct 5955/6560\n",
      "Epoch: 52, train_loss: 0.1770, train_clustering_loss:  0.1789, train_error: 0.0720\n",
      "class 0: acc 0.927536231884058, correct 384/414\n",
      "class 1: acc 0.9285714285714286, correct 377/406\n",
      "\n",
      "Val Set, val_loss: 0.1939, val_error: 0.0909, auc: 0.9930\n",
      "class 0 clustering acc 0.9579545454545455: correct 1686/1760\n",
      "class 1 clustering acc 0.8102272727272727: correct 713/880\n",
      "class 0: acc 0.8076923076923077, correct 42/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 19472\n",
      "batch 39, loss: 0.6076, instance_loss: 0.3983, weighted_loss: 0.5448, label: 0, bag_size: 6624\n",
      "batch 59, loss: 0.0070, instance_loss: 0.0007, weighted_loss: 0.0051, label: 0, bag_size: 2920\n",
      "batch 79, loss: 0.3509, instance_loss: 0.5916, weighted_loss: 0.4231, label: 1, bag_size: 4929\n",
      "batch 99, loss: 0.0532, instance_loss: 0.1006, weighted_loss: 0.0674, label: 0, bag_size: 15464\n",
      "batch 119, loss: 0.1709, instance_loss: 0.0260, weighted_loss: 0.1274, label: 0, bag_size: 4418\n",
      "batch 139, loss: 0.0892, instance_loss: 0.0248, weighted_loss: 0.0699, label: 0, bag_size: 10381\n",
      "batch 159, loss: 0.0091, instance_loss: 0.0001, weighted_loss: 0.0064, label: 1, bag_size: 13194\n",
      "batch 179, loss: 0.8652, instance_loss: 0.2574, weighted_loss: 0.6829, label: 0, bag_size: 2070\n",
      "batch 199, loss: 0.2994, instance_loss: 0.0014, weighted_loss: 0.2100, label: 1, bag_size: 3652\n",
      "batch 219, loss: 0.4521, instance_loss: 0.0541, weighted_loss: 0.3327, label: 0, bag_size: 2160\n",
      "batch 239, loss: 0.1457, instance_loss: 0.0001, weighted_loss: 0.1020, label: 1, bag_size: 5921\n",
      "batch 259, loss: 0.0049, instance_loss: 0.1443, weighted_loss: 0.0467, label: 0, bag_size: 15636\n",
      "batch 279, loss: 0.0591, instance_loss: 0.1023, weighted_loss: 0.0720, label: 0, bag_size: 2732\n",
      "batch 299, loss: 0.0058, instance_loss: 0.0000, weighted_loss: 0.0040, label: 0, bag_size: 15464\n",
      "batch 319, loss: 0.0110, instance_loss: 0.0497, weighted_loss: 0.0226, label: 1, bag_size: 20161\n",
      "batch 339, loss: 1.7946, instance_loss: 0.3564, weighted_loss: 1.3631, label: 0, bag_size: 7239\n",
      "batch 359, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0015, label: 1, bag_size: 18468\n",
      "batch 379, loss: 0.0060, instance_loss: 1.3694, weighted_loss: 0.4150, label: 1, bag_size: 2785\n",
      "batch 399, loss: 0.0169, instance_loss: 0.0063, weighted_loss: 0.0137, label: 1, bag_size: 12626\n",
      "batch 419, loss: 0.0144, instance_loss: 0.0121, weighted_loss: 0.0137, label: 0, bag_size: 10751\n",
      "batch 439, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 5991\n",
      "batch 459, loss: 1.3368, instance_loss: 0.0823, weighted_loss: 0.9605, label: 0, bag_size: 21361\n",
      "batch 479, loss: 0.0322, instance_loss: 0.1062, weighted_loss: 0.0544, label: 0, bag_size: 2382\n",
      "batch 499, loss: 0.0029, instance_loss: 0.0000, weighted_loss: 0.0020, label: 1, bag_size: 11389\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 15716\n",
      "batch 539, loss: 0.0094, instance_loss: 0.1782, weighted_loss: 0.0600, label: 0, bag_size: 2534\n",
      "batch 559, loss: 0.0061, instance_loss: 0.3179, weighted_loss: 0.0996, label: 1, bag_size: 1437\n",
      "batch 579, loss: 0.0839, instance_loss: 2.1115, weighted_loss: 0.6922, label: 1, bag_size: 898\n",
      "batch 599, loss: 0.0250, instance_loss: 0.3868, weighted_loss: 0.1335, label: 1, bag_size: 3674\n",
      "batch 619, loss: 0.0009, instance_loss: 0.0216, weighted_loss: 0.0071, label: 0, bag_size: 11512\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0085, weighted_loss: 0.0026, label: 0, bag_size: 19390\n",
      "batch 659, loss: 0.0067, instance_loss: 0.0310, weighted_loss: 0.0140, label: 0, bag_size: 11259\n",
      "batch 679, loss: 0.0014, instance_loss: 0.1919, weighted_loss: 0.0586, label: 1, bag_size: 11600\n",
      "batch 699, loss: 0.0320, instance_loss: 0.3058, weighted_loss: 0.1142, label: 0, bag_size: 2063\n",
      "batch 719, loss: 0.2712, instance_loss: 0.0272, weighted_loss: 0.1980, label: 1, bag_size: 5723\n",
      "batch 739, loss: 0.0121, instance_loss: 0.1499, weighted_loss: 0.0534, label: 1, bag_size: 1437\n",
      "batch 759, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 16512\n",
      "batch 779, loss: 0.0116, instance_loss: 0.0154, weighted_loss: 0.0127, label: 1, bag_size: 7246\n",
      "batch 799, loss: 0.0011, instance_loss: 0.0093, weighted_loss: 0.0036, label: 0, bag_size: 11383\n",
      "batch 819, loss: 0.0037, instance_loss: 0.0000, weighted_loss: 0.0026, label: 1, bag_size: 9689\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9809451219512195: correct 12870/13120\n",
      "class 1 clustering acc 0.8975609756097561: correct 5888/6560\n",
      "Epoch: 53, train_loss: 0.1306, train_clustering_loss:  0.1901, train_error: 0.0500\n",
      "class 0: acc 0.9492753623188406, correct 393/414\n",
      "class 1: acc 0.9507389162561576, correct 386/406\n",
      "\n",
      "Val Set, val_loss: 0.1532, val_error: 0.0455, auc: 0.9924\n",
      "class 0 clustering acc 0.9681818181818181: correct 1704/1760\n",
      "class 1 clustering acc 0.8625: correct 759/880\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 8602\n",
      "batch 39, loss: 0.0008, instance_loss: 0.4033, weighted_loss: 0.1215, label: 1, bag_size: 3450\n",
      "batch 59, loss: 0.0296, instance_loss: 2.4567, weighted_loss: 0.7578, label: 0, bag_size: 803\n",
      "batch 79, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 11518\n",
      "batch 99, loss: 0.0259, instance_loss: 0.1637, weighted_loss: 0.0672, label: 0, bag_size: 3783\n",
      "batch 119, loss: 0.1594, instance_loss: 0.0398, weighted_loss: 0.1235, label: 1, bag_size: 1294\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 13964\n",
      "batch 159, loss: 1.0520, instance_loss: 1.3589, weighted_loss: 1.1441, label: 1, bag_size: 684\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 12408\n",
      "batch 199, loss: 0.0278, instance_loss: 0.2055, weighted_loss: 0.0811, label: 0, bag_size: 3198\n",
      "batch 219, loss: 0.0999, instance_loss: 0.0000, weighted_loss: 0.0699, label: 0, bag_size: 26208\n",
      "batch 239, loss: 0.0014, instance_loss: 0.0001, weighted_loss: 0.0010, label: 0, bag_size: 23796\n",
      "batch 259, loss: 0.1108, instance_loss: 0.0000, weighted_loss: 0.0776, label: 1, bag_size: 21827\n",
      "batch 279, loss: 0.0033, instance_loss: 0.0365, weighted_loss: 0.0133, label: 0, bag_size: 1483\n",
      "batch 299, loss: 0.0075, instance_loss: 0.0000, weighted_loss: 0.0053, label: 0, bag_size: 23368\n",
      "batch 319, loss: 0.1779, instance_loss: 0.0004, weighted_loss: 0.1247, label: 1, bag_size: 9561\n",
      "batch 339, loss: 0.0156, instance_loss: 0.1927, weighted_loss: 0.0687, label: 0, bag_size: 7989\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 14266\n",
      "batch 379, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 11387\n",
      "batch 399, loss: 0.0334, instance_loss: 0.0471, weighted_loss: 0.0375, label: 0, bag_size: 5639\n",
      "batch 419, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 14433\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9885\n",
      "batch 459, loss: 0.0007, instance_loss: 0.0010, weighted_loss: 0.0008, label: 0, bag_size: 14956\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14319\n",
      "batch 499, loss: 0.0027, instance_loss: 0.0000, weighted_loss: 0.0019, label: 0, bag_size: 13591\n",
      "batch 519, loss: 0.0042, instance_loss: 0.0188, weighted_loss: 0.0085, label: 1, bag_size: 2480\n",
      "batch 539, loss: 0.0036, instance_loss: 0.0761, weighted_loss: 0.0253, label: 0, bag_size: 3474\n",
      "batch 559, loss: 0.0026, instance_loss: 0.0374, weighted_loss: 0.0130, label: 1, bag_size: 928\n",
      "batch 579, loss: 0.0244, instance_loss: 0.0000, weighted_loss: 0.0171, label: 0, bag_size: 3552\n",
      "batch 599, loss: 0.0062, instance_loss: 0.0038, weighted_loss: 0.0055, label: 0, bag_size: 12148\n",
      "batch 619, loss: 0.2253, instance_loss: 0.0074, weighted_loss: 0.1599, label: 1, bag_size: 4939\n",
      "batch 639, loss: 0.3119, instance_loss: 0.0093, weighted_loss: 0.2211, label: 1, bag_size: 7468\n",
      "batch 659, loss: 0.0139, instance_loss: 0.0000, weighted_loss: 0.0097, label: 0, bag_size: 2044\n",
      "batch 679, loss: 0.1422, instance_loss: 0.3588, weighted_loss: 0.2072, label: 0, bag_size: 2104\n",
      "batch 699, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 9408\n",
      "batch 719, loss: 0.0070, instance_loss: 0.4054, weighted_loss: 0.1265, label: 0, bag_size: 1797\n",
      "batch 739, loss: 0.0037, instance_loss: 0.0000, weighted_loss: 0.0026, label: 1, bag_size: 19832\n",
      "batch 759, loss: 0.0069, instance_loss: 0.0296, weighted_loss: 0.0137, label: 0, bag_size: 9471\n",
      "batch 779, loss: 1.4550, instance_loss: 1.8444, weighted_loss: 1.5718, label: 1, bag_size: 2935\n",
      "batch 799, loss: 0.1233, instance_loss: 0.0091, weighted_loss: 0.0890, label: 1, bag_size: 15125\n",
      "batch 819, loss: 0.0043, instance_loss: 0.0000, weighted_loss: 0.0030, label: 1, bag_size: 22264\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9825457317073171: correct 12891/13120\n",
      "class 1 clustering acc 0.9208841463414634: correct 6041/6560\n",
      "Epoch: 54, train_loss: 0.1848, train_clustering_loss:  0.1556, train_error: 0.0768\n",
      "class 0: acc 0.9142156862745098, correct 373/408\n",
      "class 1: acc 0.9320388349514563, correct 384/412\n",
      "\n",
      "Val Set, val_loss: 0.1788, val_error: 0.0727, auc: 0.9920\n",
      "class 0 clustering acc 0.9636363636363636: correct 1696/1760\n",
      "class 1 clustering acc 0.8397727272727272: correct 739/880\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0204, instance_loss: 0.0250, weighted_loss: 0.0218, label: 0, bag_size: 2303\n",
      "batch 39, loss: 0.0075, instance_loss: 0.0001, weighted_loss: 0.0053, label: 0, bag_size: 12687\n",
      "batch 59, loss: 0.0736, instance_loss: 0.0328, weighted_loss: 0.0614, label: 1, bag_size: 2522\n",
      "batch 79, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 23398\n",
      "batch 99, loss: 0.0014, instance_loss: 0.0031, weighted_loss: 0.0019, label: 1, bag_size: 14604\n",
      "batch 119, loss: 0.1040, instance_loss: 1.1837, weighted_loss: 0.4279, label: 0, bag_size: 2609\n",
      "batch 139, loss: 1.9394, instance_loss: 0.0000, weighted_loss: 1.3576, label: 0, bag_size: 5211\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 10481\n",
      "batch 179, loss: 0.1069, instance_loss: 0.0076, weighted_loss: 0.0771, label: 1, bag_size: 9983\n",
      "batch 199, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 1, bag_size: 14202\n",
      "batch 219, loss: 0.5490, instance_loss: 3.7415, weighted_loss: 1.5068, label: 1, bag_size: 12712\n",
      "batch 239, loss: 0.0080, instance_loss: 0.0558, weighted_loss: 0.0223, label: 1, bag_size: 4239\n",
      "batch 259, loss: 0.0123, instance_loss: 0.0000, weighted_loss: 0.0086, label: 1, bag_size: 16051\n",
      "batch 279, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 15636\n",
      "batch 299, loss: 0.0001, instance_loss: 0.1104, weighted_loss: 0.0332, label: 1, bag_size: 629\n",
      "batch 319, loss: 0.0268, instance_loss: 0.0161, weighted_loss: 0.0236, label: 1, bag_size: 5894\n",
      "batch 339, loss: 0.0077, instance_loss: 0.0798, weighted_loss: 0.0293, label: 0, bag_size: 2360\n",
      "batch 359, loss: 0.0042, instance_loss: 0.1025, weighted_loss: 0.0337, label: 0, bag_size: 1909\n",
      "batch 379, loss: 0.4112, instance_loss: 0.7711, weighted_loss: 0.5192, label: 1, bag_size: 1242\n",
      "batch 399, loss: 0.0020, instance_loss: 0.0181, weighted_loss: 0.0068, label: 1, bag_size: 1622\n",
      "batch 419, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 11778\n",
      "batch 439, loss: 0.0069, instance_loss: 0.5020, weighted_loss: 0.1554, label: 0, bag_size: 19808\n",
      "batch 459, loss: 0.8131, instance_loss: 0.0179, weighted_loss: 0.5746, label: 0, bag_size: 3375\n",
      "batch 479, loss: 0.0361, instance_loss: 0.0115, weighted_loss: 0.0287, label: 1, bag_size: 16154\n",
      "batch 499, loss: 0.0314, instance_loss: 0.0031, weighted_loss: 0.0229, label: 1, bag_size: 2179\n",
      "batch 519, loss: 0.1311, instance_loss: 0.7650, weighted_loss: 0.3213, label: 1, bag_size: 15689\n",
      "batch 539, loss: 0.0040, instance_loss: 0.0258, weighted_loss: 0.0105, label: 0, bag_size: 1614\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23996\n",
      "batch 579, loss: 0.0631, instance_loss: 0.0009, weighted_loss: 0.0444, label: 1, bag_size: 7468\n",
      "batch 599, loss: 0.0026, instance_loss: 0.0000, weighted_loss: 0.0018, label: 1, bag_size: 4423\n",
      "batch 619, loss: 0.0093, instance_loss: 0.0000, weighted_loss: 0.0065, label: 1, bag_size: 6745\n",
      "batch 639, loss: 0.0003, instance_loss: 0.0044, weighted_loss: 0.0015, label: 1, bag_size: 9971\n",
      "batch 659, loss: 0.1282, instance_loss: 2.0364, weighted_loss: 0.7007, label: 1, bag_size: 549\n",
      "batch 679, loss: 0.0013, instance_loss: 0.4073, weighted_loss: 0.1231, label: 0, bag_size: 1234\n",
      "batch 699, loss: 0.0036, instance_loss: 0.0465, weighted_loss: 0.0165, label: 1, bag_size: 2146\n",
      "batch 719, loss: 0.0119, instance_loss: 0.0186, weighted_loss: 0.0139, label: 0, bag_size: 3541\n",
      "batch 739, loss: 0.0295, instance_loss: 0.0134, weighted_loss: 0.0247, label: 0, bag_size: 6367\n",
      "batch 759, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 10392\n",
      "batch 779, loss: 0.0709, instance_loss: 0.0837, weighted_loss: 0.0748, label: 0, bag_size: 2382\n",
      "batch 799, loss: 0.0270, instance_loss: 0.0239, weighted_loss: 0.0260, label: 0, bag_size: 14377\n",
      "batch 819, loss: 0.0103, instance_loss: 0.0200, weighted_loss: 0.0132, label: 0, bag_size: 763\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.984375: correct 12915/13120\n",
      "class 1 clustering acc 0.9196646341463415: correct 6033/6560\n",
      "Epoch: 55, train_loss: 0.1377, train_clustering_loss:  0.1534, train_error: 0.0524\n",
      "class 0: acc 0.9404466501240695, correct 379/403\n",
      "class 1: acc 0.9544364508393285, correct 398/417\n",
      "\n",
      "Val Set, val_loss: 0.1273, val_error: 0.0364, auc: 0.9924\n",
      "class 0 clustering acc 0.9181818181818182: correct 1616/1760\n",
      "class 1 clustering acc 0.7522727272727273: correct 662/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "Validation loss decreased (0.147488 --> 0.127319).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0031, instance_loss: 0.0372, weighted_loss: 0.0133, label: 0, bag_size: 15001\n",
      "batch 39, loss: 0.0316, instance_loss: 1.0793, weighted_loss: 0.3459, label: 0, bag_size: 6898\n",
      "batch 59, loss: 0.7193, instance_loss: 0.0886, weighted_loss: 0.5301, label: 0, bag_size: 2098\n",
      "batch 79, loss: 0.3036, instance_loss: 0.0017, weighted_loss: 0.2130, label: 1, bag_size: 14887\n",
      "batch 99, loss: 0.1148, instance_loss: 0.0020, weighted_loss: 0.0809, label: 1, bag_size: 7613\n",
      "batch 119, loss: 0.0015, instance_loss: 0.2407, weighted_loss: 0.0732, label: 0, bag_size: 8866\n",
      "batch 139, loss: 0.1646, instance_loss: 0.0008, weighted_loss: 0.1154, label: 1, bag_size: 10432\n",
      "batch 159, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 7110\n",
      "batch 179, loss: 0.3586, instance_loss: 0.0066, weighted_loss: 0.2530, label: 0, bag_size: 5120\n",
      "batch 199, loss: 0.0003, instance_loss: 0.4324, weighted_loss: 0.1299, label: 1, bag_size: 2136\n",
      "batch 219, loss: 0.0028, instance_loss: 0.0037, weighted_loss: 0.0031, label: 0, bag_size: 4465\n",
      "batch 239, loss: 0.0005, instance_loss: 0.0542, weighted_loss: 0.0166, label: 0, bag_size: 8252\n",
      "batch 259, loss: 0.0215, instance_loss: 0.9240, weighted_loss: 0.2922, label: 1, bag_size: 4039\n",
      "batch 279, loss: 0.0146, instance_loss: 0.0077, weighted_loss: 0.0125, label: 1, bag_size: 3224\n",
      "batch 299, loss: 0.0227, instance_loss: 0.0047, weighted_loss: 0.0173, label: 0, bag_size: 11259\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14319\n",
      "batch 339, loss: 0.0146, instance_loss: 0.0038, weighted_loss: 0.0113, label: 1, bag_size: 2278\n",
      "batch 359, loss: 0.0016, instance_loss: 0.0339, weighted_loss: 0.0113, label: 0, bag_size: 14956\n",
      "batch 379, loss: 3.3831, instance_loss: 5.4517, weighted_loss: 4.0037, label: 0, bag_size: 17279\n",
      "batch 399, loss: 0.0046, instance_loss: 0.0000, weighted_loss: 0.0032, label: 0, bag_size: 11383\n",
      "batch 419, loss: 0.1144, instance_loss: 0.0889, weighted_loss: 0.1067, label: 1, bag_size: 1867\n",
      "batch 439, loss: 0.0007, instance_loss: 0.0422, weighted_loss: 0.0131, label: 1, bag_size: 15233\n",
      "batch 459, loss: 0.0015, instance_loss: 0.0881, weighted_loss: 0.0275, label: 0, bag_size: 12217\n",
      "batch 479, loss: 0.0097, instance_loss: 0.0396, weighted_loss: 0.0186, label: 1, bag_size: 2412\n",
      "batch 499, loss: 0.0665, instance_loss: 0.0846, weighted_loss: 0.0719, label: 0, bag_size: 24382\n",
      "batch 519, loss: 0.0018, instance_loss: 0.0752, weighted_loss: 0.0238, label: 0, bag_size: 2548\n",
      "batch 539, loss: 0.8497, instance_loss: 2.0647, weighted_loss: 1.2142, label: 1, bag_size: 2731\n",
      "batch 559, loss: 0.0802, instance_loss: 1.0607, weighted_loss: 0.3743, label: 0, bag_size: 47866\n",
      "batch 579, loss: 0.0583, instance_loss: 0.1392, weighted_loss: 0.0825, label: 0, bag_size: 1760\n",
      "batch 599, loss: 0.0102, instance_loss: 2.5270, weighted_loss: 0.7652, label: 1, bag_size: 865\n",
      "batch 619, loss: 1.3398, instance_loss: 0.2114, weighted_loss: 1.0013, label: 1, bag_size: 9215\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 10867\n",
      "batch 659, loss: 0.0014, instance_loss: 0.0416, weighted_loss: 0.0135, label: 0, bag_size: 21076\n",
      "batch 679, loss: 1.2376, instance_loss: 0.2845, weighted_loss: 0.9517, label: 1, bag_size: 3121\n",
      "batch 699, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 4394\n",
      "batch 719, loss: 0.0365, instance_loss: 0.5425, weighted_loss: 0.1883, label: 0, bag_size: 11922\n",
      "batch 739, loss: 0.0050, instance_loss: 0.0059, weighted_loss: 0.0053, label: 1, bag_size: 8216\n",
      "batch 759, loss: 0.0083, instance_loss: 0.0046, weighted_loss: 0.0072, label: 0, bag_size: 11194\n",
      "batch 779, loss: 0.0047, instance_loss: 0.0097, weighted_loss: 0.0062, label: 1, bag_size: 10498\n",
      "batch 799, loss: 0.0285, instance_loss: 0.8722, weighted_loss: 0.2816, label: 1, bag_size: 12626\n",
      "batch 819, loss: 0.0942, instance_loss: 0.0580, weighted_loss: 0.0833, label: 1, bag_size: 11223\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.975: correct 12792/13120\n",
      "class 1 clustering acc 0.8647865853658536: correct 5673/6560\n",
      "Epoch: 56, train_loss: 0.1522, train_clustering_loss:  0.2333, train_error: 0.0598\n",
      "class 0: acc 0.9326424870466321, correct 360/386\n",
      "class 1: acc 0.9470046082949308, correct 411/434\n",
      "\n",
      "Val Set, val_loss: 0.1251, val_error: 0.0364, auc: 0.9924\n",
      "class 0 clustering acc 0.9505681818181818: correct 1673/1760\n",
      "class 1 clustering acc 0.7840909090909091: correct 690/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "Validation loss decreased (0.127319 --> 0.125128).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0003, instance_loss: 0.0293, weighted_loss: 0.0090, label: 0, bag_size: 2179\n",
      "batch 39, loss: 0.0247, instance_loss: 0.0537, weighted_loss: 0.0334, label: 1, bag_size: 10460\n",
      "batch 59, loss: 0.0224, instance_loss: 0.0000, weighted_loss: 0.0157, label: 0, bag_size: 19067\n",
      "batch 79, loss: 0.0089, instance_loss: 0.4127, weighted_loss: 0.1300, label: 0, bag_size: 3725\n",
      "batch 99, loss: 0.0067, instance_loss: 0.0000, weighted_loss: 0.0047, label: 0, bag_size: 5965\n",
      "batch 119, loss: 0.0197, instance_loss: 0.0025, weighted_loss: 0.0145, label: 1, bag_size: 9470\n",
      "batch 139, loss: 0.0092, instance_loss: 0.0072, weighted_loss: 0.0086, label: 1, bag_size: 5454\n",
      "batch 159, loss: 0.0007, instance_loss: 0.0002, weighted_loss: 0.0006, label: 1, bag_size: 16417\n",
      "batch 179, loss: 0.0010, instance_loss: 0.1052, weighted_loss: 0.0323, label: 0, bag_size: 1824\n",
      "batch 199, loss: 0.0035, instance_loss: 0.0113, weighted_loss: 0.0059, label: 0, bag_size: 21093\n",
      "batch 219, loss: 0.1719, instance_loss: 1.7316, weighted_loss: 0.6398, label: 1, bag_size: 983\n",
      "batch 239, loss: 0.0010, instance_loss: 0.2058, weighted_loss: 0.0624, label: 0, bag_size: 8661\n",
      "batch 259, loss: 0.0025, instance_loss: 0.0028, weighted_loss: 0.0025, label: 1, bag_size: 7515\n",
      "batch 279, loss: 0.0123, instance_loss: 0.1553, weighted_loss: 0.0552, label: 1, bag_size: 16051\n",
      "batch 299, loss: 2.8551, instance_loss: 1.4567, weighted_loss: 2.4356, label: 1, bag_size: 2731\n",
      "batch 319, loss: 0.6723, instance_loss: 0.0677, weighted_loss: 0.4909, label: 0, bag_size: 9597\n",
      "batch 339, loss: 0.0019, instance_loss: 0.0274, weighted_loss: 0.0096, label: 0, bag_size: 11125\n",
      "batch 359, loss: 0.5462, instance_loss: 2.8176, weighted_loss: 1.2276, label: 0, bag_size: 10410\n",
      "batch 379, loss: 0.0276, instance_loss: 0.1325, weighted_loss: 0.0591, label: 0, bag_size: 3321\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0396, weighted_loss: 0.0119, label: 0, bag_size: 9060\n",
      "batch 419, loss: 0.0015, instance_loss: 0.0267, weighted_loss: 0.0091, label: 1, bag_size: 2662\n",
      "batch 439, loss: 0.0024, instance_loss: 0.9085, weighted_loss: 0.2742, label: 1, bag_size: 16034\n",
      "batch 459, loss: 0.1387, instance_loss: 0.0126, weighted_loss: 0.1009, label: 1, bag_size: 10591\n",
      "batch 479, loss: 0.0004, instance_loss: 0.0028, weighted_loss: 0.0011, label: 1, bag_size: 18794\n",
      "batch 499, loss: 0.0021, instance_loss: 0.0046, weighted_loss: 0.0028, label: 0, bag_size: 9949\n",
      "batch 519, loss: 0.0003, instance_loss: 0.0123, weighted_loss: 0.0039, label: 0, bag_size: 13225\n",
      "batch 539, loss: 0.0009, instance_loss: 0.0010, weighted_loss: 0.0009, label: 0, bag_size: 21076\n",
      "batch 559, loss: 0.0002, instance_loss: 0.0064, weighted_loss: 0.0021, label: 1, bag_size: 20333\n",
      "batch 579, loss: 0.0230, instance_loss: 0.0166, weighted_loss: 0.0211, label: 1, bag_size: 7798\n",
      "batch 599, loss: 0.0023, instance_loss: 0.0015, weighted_loss: 0.0021, label: 0, bag_size: 11125\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0053, weighted_loss: 0.0017, label: 1, bag_size: 7650\n",
      "batch 639, loss: 0.0989, instance_loss: 2.8660, weighted_loss: 0.9291, label: 1, bag_size: 18161\n",
      "batch 659, loss: 0.0010, instance_loss: 0.3410, weighted_loss: 0.1030, label: 0, bag_size: 2360\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 21218\n",
      "batch 699, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 13225\n",
      "batch 719, loss: 0.2417, instance_loss: 0.2413, weighted_loss: 0.2416, label: 0, bag_size: 1953\n",
      "batch 739, loss: 0.0286, instance_loss: 0.0293, weighted_loss: 0.0288, label: 1, bag_size: 9470\n",
      "batch 759, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9147\n",
      "batch 779, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 11865\n",
      "batch 799, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 15008\n",
      "batch 819, loss: 0.0005, instance_loss: 0.0162, weighted_loss: 0.0052, label: 1, bag_size: 18649\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9744664634146342: correct 12785/13120\n",
      "class 1 clustering acc 0.8759146341463414: correct 5746/6560\n",
      "Epoch: 57, train_loss: 0.1374, train_clustering_loss:  0.2387, train_error: 0.0451\n",
      "class 0: acc 0.9423558897243107, correct 376/399\n",
      "class 1: acc 0.9667458432304038, correct 407/421\n",
      "\n",
      "Val Set, val_loss: 0.1728, val_error: 0.0727, auc: 0.9904\n",
      "class 0 clustering acc 0.9670454545454545: correct 1702/1760\n",
      "class 1 clustering acc 0.8465909090909091: correct 745/880\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0806, instance_loss: 0.4816, weighted_loss: 0.2009, label: 0, bag_size: 1052\n",
      "batch 39, loss: 0.0243, instance_loss: 0.2351, weighted_loss: 0.0876, label: 0, bag_size: 1415\n",
      "batch 59, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 8145\n",
      "batch 79, loss: 0.0308, instance_loss: 0.0026, weighted_loss: 0.0223, label: 0, bag_size: 10898\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 10898\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0208, weighted_loss: 0.0063, label: 1, bag_size: 1412\n",
      "batch 139, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 18240\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 3228\n",
      "batch 179, loss: 0.0379, instance_loss: 0.2019, weighted_loss: 0.0871, label: 1, bag_size: 1051\n",
      "batch 199, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 10995\n",
      "batch 219, loss: 0.4050, instance_loss: 1.1052, weighted_loss: 0.6151, label: 0, bag_size: 15898\n",
      "batch 239, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 18154\n",
      "batch 259, loss: 0.0314, instance_loss: 0.3297, weighted_loss: 0.1209, label: 0, bag_size: 3321\n",
      "batch 279, loss: 0.0289, instance_loss: 0.1006, weighted_loss: 0.0504, label: 0, bag_size: 3670\n",
      "batch 299, loss: 0.0004, instance_loss: 0.0006, weighted_loss: 0.0004, label: 0, bag_size: 10068\n",
      "batch 319, loss: 0.0601, instance_loss: 0.5296, weighted_loss: 0.2010, label: 1, bag_size: 2842\n",
      "batch 339, loss: 0.0087, instance_loss: 0.0709, weighted_loss: 0.0274, label: 1, bag_size: 2814\n",
      "batch 359, loss: 0.1242, instance_loss: 0.0402, weighted_loss: 0.0990, label: 1, bag_size: 1867\n",
      "batch 379, loss: 0.0016, instance_loss: 0.0381, weighted_loss: 0.0125, label: 1, bag_size: 18603\n",
      "batch 399, loss: 0.0006, instance_loss: 0.0353, weighted_loss: 0.0110, label: 1, bag_size: 13194\n",
      "batch 419, loss: 0.0312, instance_loss: 0.1751, weighted_loss: 0.0743, label: 0, bag_size: 1370\n",
      "batch 439, loss: 0.0028, instance_loss: 0.0004, weighted_loss: 0.0021, label: 0, bag_size: 9949\n",
      "batch 459, loss: 0.0553, instance_loss: 0.0055, weighted_loss: 0.0404, label: 0, bag_size: 2732\n",
      "batch 479, loss: 0.0405, instance_loss: 0.0579, weighted_loss: 0.0457, label: 1, bag_size: 1759\n",
      "batch 499, loss: 0.0860, instance_loss: 0.3379, weighted_loss: 0.1616, label: 0, bag_size: 7989\n",
      "batch 519, loss: 0.0380, instance_loss: 2.7072, weighted_loss: 0.8387, label: 0, bag_size: 803\n",
      "batch 539, loss: 0.0116, instance_loss: 0.0033, weighted_loss: 0.0091, label: 0, bag_size: 13777\n",
      "batch 559, loss: 0.0112, instance_loss: 0.0000, weighted_loss: 0.0078, label: 0, bag_size: 10942\n",
      "batch 579, loss: 0.0096, instance_loss: 0.1683, weighted_loss: 0.0572, label: 0, bag_size: 3908\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0025, weighted_loss: 0.0008, label: 1, bag_size: 1781\n",
      "batch 619, loss: 0.0026, instance_loss: 0.0000, weighted_loss: 0.0018, label: 0, bag_size: 10791\n",
      "batch 639, loss: 0.1563, instance_loss: 0.0918, weighted_loss: 0.1370, label: 0, bag_size: 11151\n",
      "batch 659, loss: 0.0087, instance_loss: 0.0000, weighted_loss: 0.0061, label: 0, bag_size: 24911\n",
      "batch 679, loss: 0.0029, instance_loss: 0.0015, weighted_loss: 0.0025, label: 0, bag_size: 2457\n",
      "batch 699, loss: 0.0035, instance_loss: 0.0005, weighted_loss: 0.0026, label: 0, bag_size: 2760\n",
      "batch 719, loss: 0.1594, instance_loss: 0.0102, weighted_loss: 0.1146, label: 1, bag_size: 7389\n",
      "batch 739, loss: 0.0388, instance_loss: 0.1595, weighted_loss: 0.0750, label: 1, bag_size: 1924\n",
      "batch 759, loss: 0.0012, instance_loss: 0.0014, weighted_loss: 0.0013, label: 0, bag_size: 14377\n",
      "batch 779, loss: 0.2009, instance_loss: 1.4294, weighted_loss: 0.5694, label: 1, bag_size: 1242\n",
      "batch 799, loss: 0.0483, instance_loss: 0.0692, weighted_loss: 0.0546, label: 1, bag_size: 1493\n",
      "batch 819, loss: 0.0161, instance_loss: 0.0000, weighted_loss: 0.0112, label: 0, bag_size: 18954\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.979344512195122: correct 12849/13120\n",
      "class 1 clustering acc 0.9010670731707318: correct 5911/6560\n",
      "Epoch: 58, train_loss: 0.1395, train_clustering_loss:  0.2085, train_error: 0.0585\n",
      "class 0: acc 0.9405940594059405, correct 380/404\n",
      "class 1: acc 0.9423076923076923, correct 392/416\n",
      "\n",
      "Val Set, val_loss: 0.1319, val_error: 0.0455, auc: 0.9920\n",
      "class 0 clustering acc 0.9630681818181818: correct 1695/1760\n",
      "class 1 clustering acc 0.8363636363636363: correct 736/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0014, instance_loss: 0.0003, weighted_loss: 0.0011, label: 1, bag_size: 5221\n",
      "batch 39, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 2036\n",
      "batch 59, loss: 0.0236, instance_loss: 0.1523, weighted_loss: 0.0622, label: 0, bag_size: 3160\n",
      "batch 79, loss: 0.0020, instance_loss: 0.0013, weighted_loss: 0.0018, label: 1, bag_size: 4239\n",
      "batch 99, loss: 0.3421, instance_loss: 0.8698, weighted_loss: 0.5004, label: 0, bag_size: 11281\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 10481\n",
      "batch 139, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 12178\n",
      "batch 159, loss: 0.0409, instance_loss: 0.0424, weighted_loss: 0.0414, label: 0, bag_size: 4845\n",
      "batch 179, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 19932\n",
      "batch 199, loss: 0.1444, instance_loss: 0.0022, weighted_loss: 0.1017, label: 0, bag_size: 12510\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 10592\n",
      "batch 239, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 3552\n",
      "batch 259, loss: 0.0121, instance_loss: 0.0171, weighted_loss: 0.0136, label: 1, bag_size: 6950\n",
      "batch 279, loss: 0.0112, instance_loss: 0.0759, weighted_loss: 0.0306, label: 0, bag_size: 6281\n",
      "batch 299, loss: 0.0513, instance_loss: 0.0239, weighted_loss: 0.0431, label: 1, bag_size: 12340\n",
      "batch 319, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 11113\n",
      "batch 339, loss: 0.0022, instance_loss: 0.0010, weighted_loss: 0.0018, label: 0, bag_size: 1508\n",
      "batch 359, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 9234\n",
      "batch 379, loss: 0.0261, instance_loss: 0.0088, weighted_loss: 0.0209, label: 1, bag_size: 6090\n",
      "batch 399, loss: 0.0026, instance_loss: 0.3760, weighted_loss: 0.1146, label: 0, bag_size: 9252\n",
      "batch 419, loss: 0.0081, instance_loss: 0.0419, weighted_loss: 0.0182, label: 1, bag_size: 1525\n",
      "batch 439, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 10068\n",
      "batch 459, loss: 0.0008, instance_loss: 0.0026, weighted_loss: 0.0014, label: 1, bag_size: 8410\n",
      "batch 479, loss: 0.0028, instance_loss: 0.0013, weighted_loss: 0.0024, label: 1, bag_size: 8868\n",
      "batch 499, loss: 0.1677, instance_loss: 0.8914, weighted_loss: 0.3848, label: 1, bag_size: 2314\n",
      "batch 519, loss: 0.0005, instance_loss: 0.0108, weighted_loss: 0.0036, label: 0, bag_size: 2424\n",
      "batch 539, loss: 0.1706, instance_loss: 0.8963, weighted_loss: 0.3884, label: 1, bag_size: 13477\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23996\n",
      "batch 579, loss: 0.0618, instance_loss: 0.0693, weighted_loss: 0.0641, label: 1, bag_size: 8680\n",
      "batch 599, loss: 0.6249, instance_loss: 0.0320, weighted_loss: 0.4470, label: 1, bag_size: 7351\n",
      "batch 619, loss: 0.0115, instance_loss: 0.0322, weighted_loss: 0.0177, label: 1, bag_size: 13015\n",
      "batch 639, loss: 0.4557, instance_loss: 0.0147, weighted_loss: 0.3234, label: 1, bag_size: 1512\n",
      "batch 659, loss: 0.0052, instance_loss: 0.0198, weighted_loss: 0.0096, label: 1, bag_size: 1483\n",
      "batch 679, loss: 0.0027, instance_loss: 0.0000, weighted_loss: 0.0019, label: 0, bag_size: 2036\n",
      "batch 699, loss: 0.0001, instance_loss: 0.0038, weighted_loss: 0.0012, label: 1, bag_size: 9644\n",
      "batch 719, loss: 0.2699, instance_loss: 0.1086, weighted_loss: 0.2215, label: 1, bag_size: 983\n",
      "batch 739, loss: 0.0645, instance_loss: 0.0141, weighted_loss: 0.0494, label: 1, bag_size: 3674\n",
      "batch 759, loss: 0.0338, instance_loss: 0.1394, weighted_loss: 0.0654, label: 1, bag_size: 1525\n",
      "batch 779, loss: 0.0080, instance_loss: 0.0005, weighted_loss: 0.0058, label: 1, bag_size: 18095\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21218\n",
      "batch 819, loss: 0.0004, instance_loss: 0.0001, weighted_loss: 0.0003, label: 1, bag_size: 9147\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.983765243902439: correct 12907/13120\n",
      "class 1 clustering acc 0.9323170731707318: correct 6116/6560\n",
      "Epoch: 59, train_loss: 0.1331, train_clustering_loss:  0.1430, train_error: 0.0524\n",
      "class 0: acc 0.9359605911330049, correct 380/406\n",
      "class 1: acc 0.9589371980676329, correct 397/414\n",
      "\n",
      "Val Set, val_loss: 0.1346, val_error: 0.0364, auc: 0.9910\n",
      "class 0 clustering acc 0.9556818181818182: correct 1682/1760\n",
      "class 1 clustering acc 0.8897727272727273: correct 783/880\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0006, instance_loss: 0.0742, weighted_loss: 0.0227, label: 1, bag_size: 6734\n",
      "batch 39, loss: 0.0253, instance_loss: 0.0319, weighted_loss: 0.0273, label: 1, bag_size: 4239\n",
      "batch 59, loss: 0.1996, instance_loss: 0.1745, weighted_loss: 0.1921, label: 1, bag_size: 1919\n",
      "batch 79, loss: 0.0002, instance_loss: 0.0049, weighted_loss: 0.0016, label: 1, bag_size: 9408\n",
      "batch 99, loss: 0.0197, instance_loss: 0.3386, weighted_loss: 0.1153, label: 0, bag_size: 931\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0037, weighted_loss: 0.0011, label: 0, bag_size: 2424\n",
      "batch 139, loss: 0.0924, instance_loss: 0.1686, weighted_loss: 0.1153, label: 1, bag_size: 6726\n",
      "batch 159, loss: 0.0015, instance_loss: 0.0108, weighted_loss: 0.0043, label: 1, bag_size: 1924\n",
      "batch 179, loss: 0.0685, instance_loss: 1.1741, weighted_loss: 0.4002, label: 1, bag_size: 5723\n",
      "batch 199, loss: 0.1367, instance_loss: 0.0051, weighted_loss: 0.0972, label: 1, bag_size: 9230\n",
      "batch 219, loss: 0.0264, instance_loss: 0.0093, weighted_loss: 0.0213, label: 0, bag_size: 5999\n",
      "batch 239, loss: 0.0001, instance_loss: 0.2309, weighted_loss: 0.0694, label: 0, bag_size: 9851\n",
      "batch 259, loss: 0.1346, instance_loss: 0.2802, weighted_loss: 0.1783, label: 1, bag_size: 1339\n",
      "batch 279, loss: 0.0219, instance_loss: 0.3541, weighted_loss: 0.1215, label: 0, bag_size: 1772\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 31106\n",
      "batch 319, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 10146\n",
      "batch 339, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 2548\n",
      "batch 359, loss: 0.0027, instance_loss: 0.0028, weighted_loss: 0.0027, label: 1, bag_size: 11266\n",
      "batch 379, loss: 0.0017, instance_loss: 0.0597, weighted_loss: 0.0191, label: 0, bag_size: 9252\n",
      "batch 399, loss: 0.0004, instance_loss: 0.0368, weighted_loss: 0.0113, label: 1, bag_size: 6453\n",
      "batch 419, loss: 0.0005, instance_loss: 0.4522, weighted_loss: 0.1360, label: 1, bag_size: 621\n",
      "batch 439, loss: 0.0035, instance_loss: 0.0044, weighted_loss: 0.0038, label: 0, bag_size: 1202\n",
      "batch 459, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 17268\n",
      "batch 479, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 2920\n",
      "batch 499, loss: 0.0471, instance_loss: 0.0182, weighted_loss: 0.0384, label: 0, bag_size: 2998\n",
      "batch 519, loss: 0.0490, instance_loss: 0.4767, weighted_loss: 0.1773, label: 0, bag_size: 2760\n",
      "batch 539, loss: 0.0470, instance_loss: 0.0150, weighted_loss: 0.0374, label: 0, bag_size: 763\n",
      "batch 559, loss: 0.0079, instance_loss: 0.0000, weighted_loss: 0.0055, label: 0, bag_size: 12732\n",
      "batch 579, loss: 0.0020, instance_loss: 0.0002, weighted_loss: 0.0014, label: 0, bag_size: 2091\n",
      "batch 599, loss: 0.1948, instance_loss: 0.1774, weighted_loss: 0.1896, label: 1, bag_size: 1444\n",
      "batch 619, loss: 0.0671, instance_loss: 0.0000, weighted_loss: 0.0470, label: 0, bag_size: 18777\n",
      "batch 639, loss: 0.2039, instance_loss: 0.0169, weighted_loss: 0.1478, label: 1, bag_size: 1963\n",
      "batch 659, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 15914\n",
      "batch 679, loss: 0.4544, instance_loss: 0.0211, weighted_loss: 0.3244, label: 1, bag_size: 1963\n",
      "batch 699, loss: 0.0020, instance_loss: 0.0021, weighted_loss: 0.0020, label: 0, bag_size: 1349\n",
      "batch 719, loss: 0.7540, instance_loss: 1.8798, weighted_loss: 1.0918, label: 0, bag_size: 9132\n",
      "batch 739, loss: 0.3376, instance_loss: 0.1618, weighted_loss: 0.2848, label: 0, bag_size: 1498\n",
      "batch 759, loss: 0.0264, instance_loss: 0.0337, weighted_loss: 0.0286, label: 0, bag_size: 7989\n",
      "batch 779, loss: 0.0008, instance_loss: 0.1405, weighted_loss: 0.0427, label: 0, bag_size: 1560\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11735\n",
      "batch 819, loss: 0.0503, instance_loss: 0.0001, weighted_loss: 0.0352, label: 1, bag_size: 13786\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9820884146341463: correct 12885/13120\n",
      "class 1 clustering acc 0.9321646341463414: correct 6115/6560\n",
      "Epoch: 60, train_loss: 0.1795, train_clustering_loss:  0.1916, train_error: 0.0768\n",
      "class 0: acc 0.9093137254901961, correct 371/408\n",
      "class 1: acc 0.9368932038834952, correct 386/412\n",
      "\n",
      "Val Set, val_loss: 0.1623, val_error: 0.0636, auc: 0.9887\n",
      "class 0 clustering acc 0.9363636363636364: correct 1648/1760\n",
      "class 1 clustering acc 0.8147727272727273: correct 717/880\n",
      "class 0: acc 0.8653846153846154, correct 45/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0007, instance_loss: 0.1315, weighted_loss: 0.0399, label: 0, bag_size: 1213\n",
      "batch 39, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 23368\n",
      "batch 59, loss: 0.0202, instance_loss: 0.0139, weighted_loss: 0.0183, label: 0, bag_size: 2213\n",
      "batch 79, loss: 0.0185, instance_loss: 0.0195, weighted_loss: 0.0188, label: 1, bag_size: 16267\n",
      "batch 99, loss: 0.0048, instance_loss: 0.0062, weighted_loss: 0.0052, label: 1, bag_size: 7110\n",
      "batch 119, loss: 0.0003, instance_loss: 0.0105, weighted_loss: 0.0034, label: 1, bag_size: 5561\n",
      "batch 139, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 12149\n",
      "batch 159, loss: 2.5887, instance_loss: 2.1045, weighted_loss: 2.4435, label: 1, bag_size: 684\n",
      "batch 179, loss: 0.0153, instance_loss: 0.0672, weighted_loss: 0.0309, label: 0, bag_size: 8959\n",
      "batch 199, loss: 0.0036, instance_loss: 0.0101, weighted_loss: 0.0056, label: 0, bag_size: 10942\n",
      "batch 219, loss: 0.0086, instance_loss: 0.0000, weighted_loss: 0.0060, label: 0, bag_size: 15636\n",
      "batch 239, loss: 0.7237, instance_loss: 0.6422, weighted_loss: 0.6993, label: 1, bag_size: 1703\n",
      "batch 259, loss: 0.0092, instance_loss: 0.0302, weighted_loss: 0.0155, label: 1, bag_size: 7246\n",
      "batch 279, loss: 0.4040, instance_loss: 0.0034, weighted_loss: 0.2838, label: 1, bag_size: 5137\n",
      "batch 299, loss: 0.1772, instance_loss: 0.0103, weighted_loss: 0.1271, label: 1, bag_size: 2179\n",
      "batch 319, loss: 0.8427, instance_loss: 0.0832, weighted_loss: 0.6148, label: 1, bag_size: 9162\n",
      "batch 339, loss: 0.2776, instance_loss: 2.5011, weighted_loss: 0.9446, label: 1, bag_size: 2731\n",
      "batch 359, loss: 0.0528, instance_loss: 0.0510, weighted_loss: 0.0523, label: 1, bag_size: 10460\n",
      "batch 379, loss: 0.0022, instance_loss: 0.0000, weighted_loss: 0.0016, label: 1, bag_size: 6606\n",
      "batch 399, loss: 0.0052, instance_loss: 0.1751, weighted_loss: 0.0561, label: 1, bag_size: 3651\n",
      "batch 419, loss: 0.1020, instance_loss: 0.0000, weighted_loss: 0.0714, label: 0, bag_size: 21319\n",
      "batch 439, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0015, label: 0, bag_size: 19470\n",
      "batch 459, loss: 0.0207, instance_loss: 0.0100, weighted_loss: 0.0175, label: 1, bag_size: 6205\n",
      "batch 479, loss: 0.2615, instance_loss: 0.0145, weighted_loss: 0.1874, label: 0, bag_size: 1953\n",
      "batch 499, loss: 0.0019, instance_loss: 0.0025, weighted_loss: 0.0021, label: 1, bag_size: 9147\n",
      "batch 519, loss: 0.0073, instance_loss: 0.0000, weighted_loss: 0.0051, label: 0, bag_size: 18777\n",
      "batch 539, loss: 0.0086, instance_loss: 0.0000, weighted_loss: 0.0060, label: 0, bag_size: 13892\n",
      "batch 559, loss: 0.0594, instance_loss: 0.7215, weighted_loss: 0.2580, label: 0, bag_size: 1772\n",
      "batch 579, loss: 0.0009, instance_loss: 0.0002, weighted_loss: 0.0007, label: 0, bag_size: 2748\n",
      "batch 599, loss: 0.4316, instance_loss: 0.1394, weighted_loss: 0.3440, label: 0, bag_size: 11281\n",
      "batch 619, loss: 0.1188, instance_loss: 0.0053, weighted_loss: 0.0848, label: 0, bag_size: 13619\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 32227\n",
      "batch 659, loss: 0.0106, instance_loss: 0.0083, weighted_loss: 0.0099, label: 1, bag_size: 2308\n",
      "batch 679, loss: 0.0014, instance_loss: 0.0012, weighted_loss: 0.0013, label: 0, bag_size: 8959\n",
      "batch 699, loss: 0.0118, instance_loss: 0.0063, weighted_loss: 0.0101, label: 0, bag_size: 2290\n",
      "batch 719, loss: 0.0003, instance_loss: 0.2952, weighted_loss: 0.0887, label: 0, bag_size: 705\n",
      "batch 739, loss: 0.0149, instance_loss: 0.9798, weighted_loss: 0.3044, label: 0, bag_size: 4598\n",
      "batch 759, loss: 1.0083, instance_loss: 0.0003, weighted_loss: 0.7059, label: 0, bag_size: 5211\n",
      "batch 779, loss: 0.0067, instance_loss: 0.0001, weighted_loss: 0.0047, label: 0, bag_size: 4997\n",
      "batch 799, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 1, bag_size: 14618\n",
      "batch 819, loss: 0.0078, instance_loss: 0.0000, weighted_loss: 0.0054, label: 0, bag_size: 2044\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9814786585365853: correct 12877/13120\n",
      "class 1 clustering acc 0.9221036585365854: correct 6049/6560\n",
      "Epoch: 61, train_loss: 0.1473, train_clustering_loss:  0.1647, train_error: 0.0512\n",
      "class 0: acc 0.9418886198547215, correct 389/413\n",
      "class 1: acc 0.9557739557739557, correct 389/407\n",
      "\n",
      "Val Set, val_loss: 0.2541, val_error: 0.1273, auc: 0.9920\n",
      "class 0 clustering acc 0.9409090909090909: correct 1656/1760\n",
      "class 1 clustering acc 0.8363636363636363: correct 736/880\n",
      "class 0: acc 0.7307692307692307, correct 38/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0044, instance_loss: 0.0000, weighted_loss: 0.0031, label: 0, bag_size: 9949\n",
      "batch 39, loss: 0.0091, instance_loss: 0.0259, weighted_loss: 0.0141, label: 1, bag_size: 2695\n",
      "batch 59, loss: 0.0052, instance_loss: 0.0080, weighted_loss: 0.0061, label: 0, bag_size: 12731\n",
      "batch 79, loss: 0.0491, instance_loss: 0.0098, weighted_loss: 0.0373, label: 1, bag_size: 7468\n",
      "batch 99, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 23714\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8948\n",
      "batch 139, loss: 0.0359, instance_loss: 0.0303, weighted_loss: 0.0342, label: 0, bag_size: 2360\n",
      "batch 159, loss: 0.0356, instance_loss: 0.0904, weighted_loss: 0.0520, label: 1, bag_size: 2140\n",
      "batch 179, loss: 0.0598, instance_loss: 0.0000, weighted_loss: 0.0419, label: 1, bag_size: 34356\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 20150\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0250, weighted_loss: 0.0076, label: 1, bag_size: 10112\n",
      "batch 239, loss: 0.0382, instance_loss: 0.0015, weighted_loss: 0.0272, label: 0, bag_size: 10490\n",
      "batch 259, loss: 0.2835, instance_loss: 0.3356, weighted_loss: 0.2991, label: 0, bag_size: 2219\n",
      "batch 279, loss: 0.3418, instance_loss: 0.0007, weighted_loss: 0.2394, label: 1, bag_size: 8592\n",
      "batch 299, loss: 0.0173, instance_loss: 0.0011, weighted_loss: 0.0124, label: 1, bag_size: 21701\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 21682\n",
      "batch 339, loss: 0.0303, instance_loss: 0.0022, weighted_loss: 0.0219, label: 1, bag_size: 10028\n",
      "batch 359, loss: 0.0008, instance_loss: 0.0156, weighted_loss: 0.0053, label: 1, bag_size: 7650\n",
      "batch 379, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 23368\n",
      "batch 399, loss: 0.0013, instance_loss: 0.0545, weighted_loss: 0.0173, label: 1, bag_size: 4128\n",
      "batch 419, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 21404\n",
      "batch 439, loss: 0.0064, instance_loss: 1.1009, weighted_loss: 0.3347, label: 1, bag_size: 2140\n",
      "batch 459, loss: 2.0040, instance_loss: 0.0236, weighted_loss: 1.4099, label: 0, bag_size: 3897\n",
      "batch 479, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 21404\n",
      "batch 499, loss: 0.0009, instance_loss: 0.0698, weighted_loss: 0.0216, label: 1, bag_size: 3651\n",
      "batch 519, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 2844\n",
      "batch 539, loss: 0.0021, instance_loss: 0.0578, weighted_loss: 0.0188, label: 1, bag_size: 3450\n",
      "batch 559, loss: 0.0067, instance_loss: 0.0998, weighted_loss: 0.0346, label: 0, bag_size: 8866\n",
      "batch 579, loss: 0.0406, instance_loss: 0.5601, weighted_loss: 0.1965, label: 0, bag_size: 2213\n",
      "batch 599, loss: 0.0023, instance_loss: 0.0044, weighted_loss: 0.0029, label: 0, bag_size: 12148\n",
      "batch 619, loss: 0.0052, instance_loss: 0.0310, weighted_loss: 0.0129, label: 1, bag_size: 1759\n",
      "batch 639, loss: 0.0009, instance_loss: 0.0235, weighted_loss: 0.0077, label: 1, bag_size: 689\n",
      "batch 659, loss: 0.0334, instance_loss: 0.1881, weighted_loss: 0.0798, label: 0, bag_size: 2382\n",
      "batch 679, loss: 0.0530, instance_loss: 0.0019, weighted_loss: 0.0377, label: 1, bag_size: 3683\n",
      "batch 699, loss: 0.5974, instance_loss: 0.3416, weighted_loss: 0.5207, label: 1, bag_size: 2842\n",
      "batch 719, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 12793\n",
      "batch 739, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 3228\n",
      "batch 759, loss: 0.0028, instance_loss: 0.0000, weighted_loss: 0.0020, label: 1, bag_size: 12178\n",
      "batch 779, loss: 0.2560, instance_loss: 1.0443, weighted_loss: 0.4925, label: 0, bag_size: 4418\n",
      "batch 799, loss: 0.0032, instance_loss: 0.1101, weighted_loss: 0.0353, label: 0, bag_size: 2091\n",
      "batch 819, loss: 0.0039, instance_loss: 0.0086, weighted_loss: 0.0053, label: 1, bag_size: 13174\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9829268292682927: correct 12896/13120\n",
      "class 1 clustering acc 0.9228658536585366: correct 6054/6560\n",
      "Epoch: 62, train_loss: 0.1306, train_clustering_loss:  0.1581, train_error: 0.0537\n",
      "class 0: acc 0.9292682926829269, correct 381/410\n",
      "class 1: acc 0.9634146341463414, correct 395/410\n",
      "\n",
      "Val Set, val_loss: 0.1438, val_error: 0.0364, auc: 0.9897\n",
      "class 0 clustering acc 0.9482954545454545: correct 1669/1760\n",
      "class 1 clustering acc 0.8340909090909091: correct 734/880\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 15332\n",
      "batch 39, loss: 0.0584, instance_loss: 0.0187, weighted_loss: 0.0465, label: 1, bag_size: 16890\n",
      "batch 59, loss: 0.0062, instance_loss: 0.0000, weighted_loss: 0.0043, label: 0, bag_size: 10490\n",
      "batch 79, loss: 0.1814, instance_loss: 0.1320, weighted_loss: 0.1666, label: 1, bag_size: 5231\n",
      "batch 99, loss: 0.0086, instance_loss: 0.0002, weighted_loss: 0.0060, label: 0, bag_size: 7605\n",
      "batch 119, loss: 0.0809, instance_loss: 0.0003, weighted_loss: 0.0567, label: 1, bag_size: 6205\n",
      "batch 139, loss: 0.0554, instance_loss: 0.0022, weighted_loss: 0.0394, label: 1, bag_size: 1525\n",
      "batch 159, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 11383\n",
      "batch 179, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 11875\n",
      "batch 199, loss: 0.0006, instance_loss: 0.0007, weighted_loss: 0.0006, label: 1, bag_size: 5221\n",
      "batch 219, loss: 0.0007, instance_loss: 0.0026, weighted_loss: 0.0013, label: 1, bag_size: 10725\n",
      "batch 239, loss: 0.0048, instance_loss: 0.0059, weighted_loss: 0.0051, label: 1, bag_size: 1746\n",
      "batch 259, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 3228\n",
      "batch 279, loss: 0.1992, instance_loss: 0.1484, weighted_loss: 0.1840, label: 0, bag_size: 1149\n",
      "batch 299, loss: 0.0016, instance_loss: 0.0024, weighted_loss: 0.0018, label: 1, bag_size: 12408\n",
      "batch 319, loss: 0.0003, instance_loss: 0.0330, weighted_loss: 0.0101, label: 1, bag_size: 621\n",
      "batch 339, loss: 0.0075, instance_loss: 0.0004, weighted_loss: 0.0054, label: 1, bag_size: 16051\n",
      "batch 359, loss: 0.0424, instance_loss: 0.2507, weighted_loss: 0.1049, label: 0, bag_size: 14249\n",
      "batch 379, loss: 0.0973, instance_loss: 0.0030, weighted_loss: 0.0690, label: 1, bag_size: 6205\n",
      "batch 399, loss: 0.0196, instance_loss: 0.1412, weighted_loss: 0.0561, label: 1, bag_size: 8754\n",
      "batch 419, loss: 0.0007, instance_loss: 0.0010, weighted_loss: 0.0008, label: 1, bag_size: 16417\n",
      "batch 439, loss: 0.7306, instance_loss: 0.9696, weighted_loss: 0.8023, label: 1, bag_size: 1963\n",
      "batch 459, loss: 0.0003, instance_loss: 0.0007, weighted_loss: 0.0004, label: 1, bag_size: 7515\n",
      "batch 479, loss: 0.0158, instance_loss: 0.0000, weighted_loss: 0.0111, label: 0, bag_size: 9415\n",
      "batch 499, loss: 0.0244, instance_loss: 0.1695, weighted_loss: 0.0680, label: 0, bag_size: 1831\n",
      "batch 519, loss: 0.0004, instance_loss: 0.0396, weighted_loss: 0.0122, label: 1, bag_size: 3453\n",
      "batch 539, loss: 0.0006, instance_loss: 0.0001, weighted_loss: 0.0004, label: 1, bag_size: 5991\n",
      "batch 559, loss: 0.2739, instance_loss: 0.0154, weighted_loss: 0.1963, label: 1, bag_size: 1819\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9673\n",
      "batch 599, loss: 0.0072, instance_loss: 0.3395, weighted_loss: 0.1069, label: 0, bag_size: 9930\n",
      "batch 619, loss: 0.0006, instance_loss: 0.2909, weighted_loss: 0.0877, label: 1, bag_size: 10281\n",
      "batch 639, loss: 0.0108, instance_loss: 0.0004, weighted_loss: 0.0077, label: 1, bag_size: 12758\n",
      "batch 659, loss: 0.1289, instance_loss: 0.0497, weighted_loss: 0.1051, label: 1, bag_size: 13440\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0033, weighted_loss: 0.0010, label: 1, bag_size: 10394\n",
      "batch 699, loss: 0.0192, instance_loss: 0.0092, weighted_loss: 0.0162, label: 1, bag_size: 8191\n",
      "batch 719, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 10146\n",
      "batch 739, loss: 0.0873, instance_loss: 0.0453, weighted_loss: 0.0747, label: 1, bag_size: 1242\n",
      "batch 759, loss: 0.0003, instance_loss: 0.0092, weighted_loss: 0.0030, label: 1, bag_size: 1609\n",
      "batch 779, loss: 0.0084, instance_loss: 0.0001, weighted_loss: 0.0059, label: 0, bag_size: 6652\n",
      "batch 799, loss: 0.0144, instance_loss: 0.0073, weighted_loss: 0.0123, label: 1, bag_size: 3968\n",
      "batch 819, loss: 0.4904, instance_loss: 0.1181, weighted_loss: 0.3787, label: 1, bag_size: 2395\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9854420731707317: correct 12929/13120\n",
      "class 1 clustering acc 0.9243902439024391: correct 6064/6560\n",
      "Epoch: 63, train_loss: 0.0945, train_clustering_loss:  0.1416, train_error: 0.0402\n",
      "class 0: acc 0.9491978609625669, correct 355/374\n",
      "class 1: acc 0.968609865470852, correct 432/446\n",
      "\n",
      "Val Set, val_loss: 0.1219, val_error: 0.0182, auc: 0.9904\n",
      "class 0 clustering acc 0.9556818181818182: correct 1682/1760\n",
      "class 1 clustering acc 0.8477272727272728: correct 746/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "Validation loss decreased (0.125128 --> 0.121896).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0183, instance_loss: 0.0600, weighted_loss: 0.0308, label: 0, bag_size: 18954\n",
      "batch 39, loss: 0.1986, instance_loss: 0.1318, weighted_loss: 0.1786, label: 0, bag_size: 2098\n",
      "batch 59, loss: 0.0058, instance_loss: 0.0000, weighted_loss: 0.0040, label: 0, bag_size: 17268\n",
      "batch 79, loss: 0.0053, instance_loss: 0.0042, weighted_loss: 0.0050, label: 1, bag_size: 11386\n",
      "batch 99, loss: 0.0139, instance_loss: 0.1006, weighted_loss: 0.0399, label: 0, bag_size: 2382\n",
      "batch 119, loss: 1.8974, instance_loss: 0.5895, weighted_loss: 1.5051, label: 1, bag_size: 3121\n",
      "batch 139, loss: 0.0088, instance_loss: 0.2256, weighted_loss: 0.0738, label: 0, bag_size: 10128\n",
      "batch 159, loss: 0.0654, instance_loss: 0.0004, weighted_loss: 0.0459, label: 1, bag_size: 5025\n",
      "batch 179, loss: 0.0042, instance_loss: 0.3987, weighted_loss: 0.1225, label: 0, bag_size: 2296\n",
      "batch 199, loss: 0.0003, instance_loss: 0.0118, weighted_loss: 0.0038, label: 0, bag_size: 11527\n",
      "batch 219, loss: 0.0108, instance_loss: 0.0132, weighted_loss: 0.0115, label: 0, bag_size: 13205\n",
      "batch 239, loss: 0.0066, instance_loss: 1.1010, weighted_loss: 0.3349, label: 0, bag_size: 2367\n",
      "batch 259, loss: 0.0009, instance_loss: 0.0858, weighted_loss: 0.0264, label: 0, bag_size: 14206\n",
      "batch 279, loss: 0.0004, instance_loss: 0.0093, weighted_loss: 0.0030, label: 1, bag_size: 15093\n",
      "batch 299, loss: 0.1012, instance_loss: 2.5992, weighted_loss: 0.8506, label: 1, bag_size: 20767\n",
      "batch 319, loss: 0.0472, instance_loss: 0.0888, weighted_loss: 0.0597, label: 1, bag_size: 2522\n",
      "batch 339, loss: 0.0003, instance_loss: 0.0734, weighted_loss: 0.0222, label: 0, bag_size: 12212\n",
      "batch 359, loss: 0.0685, instance_loss: 0.0021, weighted_loss: 0.0486, label: 1, bag_size: 8466\n",
      "batch 379, loss: 0.0258, instance_loss: 0.0000, weighted_loss: 0.0181, label: 1, bag_size: 20161\n",
      "batch 399, loss: 0.0007, instance_loss: 0.7078, weighted_loss: 0.2128, label: 1, bag_size: 9732\n",
      "batch 419, loss: 0.0044, instance_loss: 0.0531, weighted_loss: 0.0190, label: 1, bag_size: 1622\n",
      "batch 439, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 1962\n",
      "batch 459, loss: 0.0356, instance_loss: 0.6664, weighted_loss: 0.2249, label: 0, bag_size: 1789\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 3228\n",
      "batch 499, loss: 0.0006, instance_loss: 0.5830, weighted_loss: 0.1753, label: 1, bag_size: 2136\n",
      "batch 519, loss: 0.0873, instance_loss: 0.0000, weighted_loss: 0.0611, label: 0, bag_size: 65728\n",
      "batch 539, loss: 0.0000, instance_loss: 0.2920, weighted_loss: 0.0876, label: 1, bag_size: 4442\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11727\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 2244\n",
      "batch 599, loss: 0.4601, instance_loss: 0.0217, weighted_loss: 0.3286, label: 0, bag_size: 9616\n",
      "batch 619, loss: 0.0155, instance_loss: 0.0000, weighted_loss: 0.0108, label: 0, bag_size: 10365\n",
      "batch 639, loss: 0.0022, instance_loss: 0.0620, weighted_loss: 0.0201, label: 0, bag_size: 1884\n",
      "batch 659, loss: 0.0018, instance_loss: 0.2280, weighted_loss: 0.0697, label: 1, bag_size: 8003\n",
      "batch 679, loss: 0.0079, instance_loss: 0.0010, weighted_loss: 0.0058, label: 1, bag_size: 19606\n",
      "batch 699, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 11387\n",
      "batch 719, loss: 2.3286, instance_loss: 1.9108, weighted_loss: 2.2032, label: 1, bag_size: 5292\n",
      "batch 739, loss: 0.0001, instance_loss: 0.0016, weighted_loss: 0.0006, label: 0, bag_size: 12212\n",
      "batch 759, loss: 0.0042, instance_loss: 0.3130, weighted_loss: 0.0968, label: 1, bag_size: 9877\n",
      "batch 779, loss: 0.0020, instance_loss: 0.1322, weighted_loss: 0.0410, label: 0, bag_size: 1884\n",
      "batch 799, loss: 0.0059, instance_loss: 0.2167, weighted_loss: 0.0691, label: 0, bag_size: 1416\n",
      "batch 819, loss: 0.0088, instance_loss: 0.0245, weighted_loss: 0.0135, label: 1, bag_size: 689\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.973170731707317: correct 12768/13120\n",
      "class 1 clustering acc 0.8614329268292683: correct 5651/6560\n",
      "Epoch: 64, train_loss: 0.1143, train_clustering_loss:  0.2459, train_error: 0.0402\n",
      "class 0: acc 0.95, correct 399/420\n",
      "class 1: acc 0.97, correct 388/400\n",
      "\n",
      "Val Set, val_loss: 0.1519, val_error: 0.0818, auc: 0.9917\n",
      "class 0 clustering acc 0.9414772727272728: correct 1657/1760\n",
      "class 1 clustering acc 0.8386363636363636: correct 738/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8948\n",
      "batch 39, loss: 0.0001, instance_loss: 0.3738, weighted_loss: 0.1122, label: 1, bag_size: 12865\n",
      "batch 59, loss: 0.0004, instance_loss: 0.0029, weighted_loss: 0.0012, label: 0, bag_size: 15313\n",
      "batch 79, loss: 0.0083, instance_loss: 0.1448, weighted_loss: 0.0493, label: 1, bag_size: 2308\n",
      "batch 99, loss: 0.0000, instance_loss: 0.6511, weighted_loss: 0.1954, label: 0, bag_size: 10068\n",
      "batch 119, loss: 0.0030, instance_loss: 0.0374, weighted_loss: 0.0133, label: 1, bag_size: 18603\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 10481\n",
      "batch 159, loss: 0.1002, instance_loss: 0.0231, weighted_loss: 0.0771, label: 0, bag_size: 18738\n",
      "batch 179, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 19043\n",
      "batch 199, loss: 0.1072, instance_loss: 2.6214, weighted_loss: 0.8614, label: 1, bag_size: 2937\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14266\n",
      "batch 239, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 6652\n",
      "batch 259, loss: 0.0020, instance_loss: 0.0532, weighted_loss: 0.0173, label: 1, bag_size: 15464\n",
      "batch 279, loss: 0.0007, instance_loss: 0.0125, weighted_loss: 0.0042, label: 1, bag_size: 11032\n",
      "batch 299, loss: 0.1864, instance_loss: 0.1658, weighted_loss: 0.1802, label: 1, bag_size: 5231\n",
      "batch 319, loss: 0.0042, instance_loss: 0.0000, weighted_loss: 0.0030, label: 1, bag_size: 22286\n",
      "batch 339, loss: 0.0182, instance_loss: 0.0000, weighted_loss: 0.0127, label: 0, bag_size: 17630\n",
      "batch 359, loss: 0.0195, instance_loss: 0.0122, weighted_loss: 0.0173, label: 1, bag_size: 3211\n",
      "batch 379, loss: 0.0202, instance_loss: 0.0065, weighted_loss: 0.0161, label: 0, bag_size: 3552\n",
      "batch 399, loss: 0.2594, instance_loss: 0.0610, weighted_loss: 0.1999, label: 1, bag_size: 8438\n",
      "batch 419, loss: 0.0660, instance_loss: 0.0961, weighted_loss: 0.0751, label: 0, bag_size: 13332\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 23398\n",
      "batch 459, loss: 0.0010, instance_loss: 0.0204, weighted_loss: 0.0068, label: 1, bag_size: 5256\n",
      "batch 479, loss: 0.0262, instance_loss: 0.0030, weighted_loss: 0.0192, label: 1, bag_size: 8685\n",
      "batch 499, loss: 0.0009, instance_loss: 0.0204, weighted_loss: 0.0067, label: 0, bag_size: 5409\n",
      "batch 519, loss: 0.0371, instance_loss: 0.0362, weighted_loss: 0.0368, label: 1, bag_size: 16154\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0127, weighted_loss: 0.0040, label: 1, bag_size: 18468\n",
      "batch 559, loss: 0.0004, instance_loss: 0.0672, weighted_loss: 0.0204, label: 0, bag_size: 1891\n",
      "batch 579, loss: 0.0498, instance_loss: 0.0058, weighted_loss: 0.0366, label: 1, bag_size: 1493\n",
      "batch 599, loss: 0.0003, instance_loss: 0.0042, weighted_loss: 0.0014, label: 0, bag_size: 2044\n",
      "batch 619, loss: 0.2663, instance_loss: 0.0568, weighted_loss: 0.2034, label: 0, bag_size: 1127\n",
      "batch 639, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0015, label: 0, bag_size: 12593\n",
      "batch 659, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 9234\n",
      "batch 679, loss: 0.0098, instance_loss: 0.0006, weighted_loss: 0.0071, label: 1, bag_size: 1638\n",
      "batch 699, loss: 0.1717, instance_loss: 0.1579, weighted_loss: 0.1676, label: 0, bag_size: 7612\n",
      "batch 719, loss: 0.3885, instance_loss: 0.0107, weighted_loss: 0.2752, label: 0, bag_size: 13992\n",
      "batch 739, loss: 0.1477, instance_loss: 0.8015, weighted_loss: 0.3438, label: 1, bag_size: 21252\n",
      "batch 759, loss: 0.1001, instance_loss: 0.0009, weighted_loss: 0.0703, label: 1, bag_size: 2179\n",
      "batch 779, loss: 0.0112, instance_loss: 0.0003, weighted_loss: 0.0079, label: 1, bag_size: 2638\n",
      "batch 799, loss: 0.0895, instance_loss: 4.7662, weighted_loss: 1.4925, label: 0, bag_size: 11607\n",
      "batch 819, loss: 0.0137, instance_loss: 1.1395, weighted_loss: 0.3514, label: 0, bag_size: 8755\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.978734756097561: correct 12841/13120\n",
      "class 1 clustering acc 0.9047256097560976: correct 5935/6560\n",
      "Epoch: 65, train_loss: 0.1409, train_clustering_loss:  0.1939, train_error: 0.0585\n",
      "class 0: acc 0.9314720812182741, correct 367/394\n",
      "class 1: acc 0.9507042253521126, correct 405/426\n",
      "\n",
      "Val Set, val_loss: 0.2884, val_error: 0.1273, auc: 0.9904\n",
      "class 0 clustering acc 0.9142045454545454: correct 1609/1760\n",
      "class 1 clustering acc 0.7897727272727273: correct 695/880\n",
      "class 0: acc 0.7307692307692307, correct 38/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.8842, instance_loss: 0.0383, weighted_loss: 0.6305, label: 1, bag_size: 19972\n",
      "batch 39, loss: 0.0002, instance_loss: 0.0016, weighted_loss: 0.0006, label: 0, bag_size: 13591\n",
      "batch 59, loss: 0.0001, instance_loss: 0.0471, weighted_loss: 0.0142, label: 1, bag_size: 3980\n",
      "batch 79, loss: 0.0011, instance_loss: 0.0601, weighted_loss: 0.0188, label: 0, bag_size: 3190\n",
      "batch 99, loss: 0.9630, instance_loss: 0.0578, weighted_loss: 0.6914, label: 0, bag_size: 3802\n",
      "batch 119, loss: 0.0223, instance_loss: 0.3644, weighted_loss: 0.1249, label: 0, bag_size: 1684\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0419, weighted_loss: 0.0127, label: 0, bag_size: 22828\n",
      "batch 159, loss: 0.0001, instance_loss: 0.1302, weighted_loss: 0.0392, label: 0, bag_size: 13591\n",
      "batch 179, loss: 0.0018, instance_loss: 0.3807, weighted_loss: 0.1155, label: 1, bag_size: 3640\n",
      "batch 199, loss: 0.0001, instance_loss: 0.1484, weighted_loss: 0.0446, label: 0, bag_size: 1072\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23714\n",
      "batch 239, loss: 0.0892, instance_loss: 0.9405, weighted_loss: 0.3446, label: 1, bag_size: 983\n",
      "batch 259, loss: 0.3523, instance_loss: 1.0700, weighted_loss: 0.5676, label: 1, bag_size: 5292\n",
      "batch 279, loss: 1.7823, instance_loss: 0.7687, weighted_loss: 1.4782, label: 0, bag_size: 2694\n",
      "batch 299, loss: 0.0839, instance_loss: 1.4814, weighted_loss: 0.5032, label: 1, bag_size: 1095\n",
      "batch 319, loss: 0.2101, instance_loss: 0.6832, weighted_loss: 0.3520, label: 0, bag_size: 2653\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0302, weighted_loss: 0.0091, label: 1, bag_size: 6966\n",
      "batch 359, loss: 0.6354, instance_loss: 0.0464, weighted_loss: 0.4587, label: 0, bag_size: 13619\n",
      "batch 379, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 11113\n",
      "batch 399, loss: 0.0822, instance_loss: 1.4553, weighted_loss: 0.4941, label: 1, bag_size: 13089\n",
      "batch 419, loss: 0.0016, instance_loss: 0.0341, weighted_loss: 0.0114, label: 1, bag_size: 4128\n",
      "batch 439, loss: 3.0015, instance_loss: 1.6406, weighted_loss: 2.5932, label: 1, bag_size: 15563\n",
      "batch 459, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 6898\n",
      "batch 479, loss: 0.0026, instance_loss: 0.0579, weighted_loss: 0.0192, label: 1, bag_size: 14433\n",
      "batch 499, loss: 0.0151, instance_loss: 0.0123, weighted_loss: 0.0142, label: 1, bag_size: 12697\n",
      "batch 519, loss: 0.0048, instance_loss: 0.0642, weighted_loss: 0.0226, label: 1, bag_size: 2278\n",
      "batch 539, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 11865\n",
      "batch 559, loss: 0.0007, instance_loss: 0.0005, weighted_loss: 0.0006, label: 0, bag_size: 8898\n",
      "batch 579, loss: 0.0168, instance_loss: 1.0697, weighted_loss: 0.3327, label: 1, bag_size: 20767\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0034, weighted_loss: 0.0011, label: 1, bag_size: 5221\n",
      "batch 619, loss: 0.0249, instance_loss: 0.8523, weighted_loss: 0.2731, label: 0, bag_size: 1416\n",
      "batch 639, loss: 0.0493, instance_loss: 0.1976, weighted_loss: 0.0938, label: 1, bag_size: 1051\n",
      "batch 659, loss: 0.0201, instance_loss: 4.2723, weighted_loss: 1.2957, label: 0, bag_size: 2336\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8252\n",
      "batch 699, loss: 0.0008, instance_loss: 0.2535, weighted_loss: 0.0766, label: 0, bag_size: 1639\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 31106\n",
      "batch 739, loss: 0.9167, instance_loss: 0.0000, weighted_loss: 0.6417, label: 0, bag_size: 18516\n",
      "batch 759, loss: 0.0744, instance_loss: 0.2074, weighted_loss: 0.1143, label: 0, bag_size: 3783\n",
      "batch 779, loss: 0.0210, instance_loss: 0.0000, weighted_loss: 0.0147, label: 0, bag_size: 8661\n",
      "batch 799, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 9171\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 10791\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9833841463414634: correct 12902/13120\n",
      "class 1 clustering acc 0.9019817073170732: correct 5917/6560\n",
      "Epoch: 66, train_loss: 0.1215, train_clustering_loss:  0.1733, train_error: 0.0512\n",
      "class 0: acc 0.944578313253012, correct 392/415\n",
      "class 1: acc 0.9530864197530864, correct 386/405\n",
      "\n",
      "Val Set, val_loss: 0.1722, val_error: 0.0727, auc: 0.9917\n",
      "class 0 clustering acc 0.9534090909090909: correct 1678/1760\n",
      "class 1 clustering acc 0.8670454545454546: correct 763/880\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.0008, weighted_loss: 0.0003, label: 1, bag_size: 11875\n",
      "batch 39, loss: 0.0111, instance_loss: 0.0018, weighted_loss: 0.0083, label: 1, bag_size: 3224\n",
      "batch 59, loss: 0.0007, instance_loss: 0.0006, weighted_loss: 0.0006, label: 0, bag_size: 2511\n",
      "batch 79, loss: 0.0892, instance_loss: 0.0081, weighted_loss: 0.0649, label: 1, bag_size: 6731\n",
      "batch 99, loss: 0.0545, instance_loss: 0.0000, weighted_loss: 0.0382, label: 1, bag_size: 34356\n",
      "batch 119, loss: 0.0078, instance_loss: 0.0024, weighted_loss: 0.0062, label: 0, bag_size: 14333\n",
      "batch 139, loss: 0.3683, instance_loss: 0.0753, weighted_loss: 0.2804, label: 1, bag_size: 9942\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 10898\n",
      "batch 179, loss: 0.0012, instance_loss: 0.0011, weighted_loss: 0.0012, label: 0, bag_size: 2548\n",
      "batch 199, loss: 0.0275, instance_loss: 0.0070, weighted_loss: 0.0214, label: 1, bag_size: 7371\n",
      "batch 219, loss: 0.3437, instance_loss: 0.0005, weighted_loss: 0.2407, label: 0, bag_size: 5120\n",
      "batch 239, loss: 0.0005, instance_loss: 0.0020, weighted_loss: 0.0010, label: 1, bag_size: 12865\n",
      "batch 259, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 19832\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0019, weighted_loss: 0.0006, label: 1, bag_size: 10592\n",
      "batch 299, loss: 0.0052, instance_loss: 0.0025, weighted_loss: 0.0044, label: 1, bag_size: 16051\n",
      "batch 319, loss: 0.0005, instance_loss: 0.0038, weighted_loss: 0.0015, label: 1, bag_size: 5894\n",
      "batch 339, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 15093\n",
      "batch 359, loss: 0.0112, instance_loss: 0.0002, weighted_loss: 0.0079, label: 0, bag_size: 6624\n",
      "batch 379, loss: 1.4751, instance_loss: 3.9711, weighted_loss: 2.2239, label: 1, bag_size: 1845\n",
      "batch 399, loss: 0.0069, instance_loss: 0.0157, weighted_loss: 0.0096, label: 1, bag_size: 8466\n",
      "batch 419, loss: 0.0042, instance_loss: 0.0347, weighted_loss: 0.0134, label: 1, bag_size: 2308\n",
      "batch 439, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 8812\n",
      "batch 459, loss: 0.1056, instance_loss: 0.3786, weighted_loss: 0.1875, label: 1, bag_size: 1242\n",
      "batch 479, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 9234\n",
      "batch 499, loss: 0.6299, instance_loss: 0.0539, weighted_loss: 0.4571, label: 1, bag_size: 1284\n",
      "batch 519, loss: 0.0034, instance_loss: 0.0046, weighted_loss: 0.0038, label: 1, bag_size: 5317\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0039, weighted_loss: 0.0014, label: 0, bag_size: 2844\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21218\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 2511\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0429, weighted_loss: 0.0130, label: 1, bag_size: 9732\n",
      "batch 619, loss: 0.0421, instance_loss: 0.0015, weighted_loss: 0.0299, label: 1, bag_size: 9561\n",
      "batch 639, loss: 0.0128, instance_loss: 0.0077, weighted_loss: 0.0113, label: 1, bag_size: 12758\n",
      "batch 659, loss: 0.0017, instance_loss: 0.5865, weighted_loss: 0.1772, label: 0, bag_size: 19808\n",
      "batch 679, loss: 0.0858, instance_loss: 0.5793, weighted_loss: 0.2339, label: 1, bag_size: 2681\n",
      "batch 699, loss: 0.0025, instance_loss: 0.0317, weighted_loss: 0.0113, label: 0, bag_size: 3321\n",
      "batch 719, loss: 1.1671, instance_loss: 0.1203, weighted_loss: 0.8530, label: 0, bag_size: 1437\n",
      "batch 739, loss: 0.1969, instance_loss: 0.0247, weighted_loss: 0.1453, label: 1, bag_size: 14887\n",
      "batch 759, loss: 0.0217, instance_loss: 0.0009, weighted_loss: 0.0155, label: 1, bag_size: 13786\n",
      "batch 779, loss: 0.0324, instance_loss: 0.0007, weighted_loss: 0.0229, label: 0, bag_size: 3502\n",
      "batch 799, loss: 0.0012, instance_loss: 0.0506, weighted_loss: 0.0160, label: 0, bag_size: 2920\n",
      "batch 819, loss: 0.0710, instance_loss: 0.0612, weighted_loss: 0.0680, label: 1, bag_size: 1822\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9827743902439025: correct 12894/13120\n",
      "class 1 clustering acc 0.9237804878048781: correct 6060/6560\n",
      "Epoch: 67, train_loss: 0.1368, train_clustering_loss:  0.1604, train_error: 0.0573\n",
      "class 0: acc 0.9348837209302325, correct 402/430\n",
      "class 1: acc 0.9512820512820512, correct 371/390\n",
      "\n",
      "Val Set, val_loss: 0.1088, val_error: 0.0364, auc: 0.9934\n",
      "class 0 clustering acc 0.9482954545454545: correct 1669/1760\n",
      "class 1 clustering acc 0.7954545454545454: correct 700/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "Validation loss decreased (0.121896 --> 0.108828).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 23714\n",
      "batch 39, loss: 0.0283, instance_loss: 0.0719, weighted_loss: 0.0413, label: 0, bag_size: 763\n",
      "batch 59, loss: 0.3836, instance_loss: 0.2880, weighted_loss: 0.3549, label: 0, bag_size: 11151\n",
      "batch 79, loss: 0.0074, instance_loss: 0.0120, weighted_loss: 0.0088, label: 1, bag_size: 10028\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 13964\n",
      "batch 119, loss: 0.1235, instance_loss: 0.2188, weighted_loss: 0.1521, label: 0, bag_size: 1052\n",
      "batch 139, loss: 0.0030, instance_loss: 0.0000, weighted_loss: 0.0021, label: 0, bag_size: 9949\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 19470\n",
      "batch 179, loss: 0.0006, instance_loss: 0.0641, weighted_loss: 0.0197, label: 0, bag_size: 2748\n",
      "batch 199, loss: 1.7331, instance_loss: 0.0003, weighted_loss: 1.2133, label: 0, bag_size: 14664\n",
      "batch 219, loss: 0.1273, instance_loss: 0.0474, weighted_loss: 0.1034, label: 1, bag_size: 6665\n",
      "batch 239, loss: 0.0024, instance_loss: 0.0000, weighted_loss: 0.0017, label: 0, bag_size: 26271\n",
      "batch 259, loss: 0.0105, instance_loss: 0.0503, weighted_loss: 0.0224, label: 1, bag_size: 14030\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14319\n",
      "batch 299, loss: 0.0260, instance_loss: 0.0000, weighted_loss: 0.0182, label: 0, bag_size: 10365\n",
      "batch 319, loss: 0.0424, instance_loss: 0.5539, weighted_loss: 0.1959, label: 1, bag_size: 13477\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0074, weighted_loss: 0.0023, label: 0, bag_size: 4465\n",
      "batch 359, loss: 0.0754, instance_loss: 0.0062, weighted_loss: 0.0546, label: 1, bag_size: 11964\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0053, weighted_loss: 0.0016, label: 1, bag_size: 4394\n",
      "batch 399, loss: 0.0018, instance_loss: 0.0000, weighted_loss: 0.0013, label: 0, bag_size: 12148\n",
      "batch 419, loss: 0.0429, instance_loss: 0.0181, weighted_loss: 0.0354, label: 1, bag_size: 8680\n",
      "batch 439, loss: 0.1036, instance_loss: 0.0006, weighted_loss: 0.0727, label: 0, bag_size: 1701\n",
      "batch 459, loss: 0.0023, instance_loss: 0.2059, weighted_loss: 0.0634, label: 0, bag_size: 6850\n",
      "batch 479, loss: 0.6053, instance_loss: 0.1671, weighted_loss: 0.4739, label: 1, bag_size: 1284\n",
      "batch 499, loss: 0.0093, instance_loss: 0.0040, weighted_loss: 0.0077, label: 1, bag_size: 16890\n",
      "batch 519, loss: 0.8078, instance_loss: 0.1223, weighted_loss: 0.6022, label: 0, bag_size: 3375\n",
      "batch 539, loss: 0.0013, instance_loss: 0.0047, weighted_loss: 0.0023, label: 1, bag_size: 2695\n",
      "batch 559, loss: 1.4523, instance_loss: 0.0037, weighted_loss: 1.0177, label: 0, bag_size: 11212\n",
      "batch 579, loss: 0.7839, instance_loss: 4.0851, weighted_loss: 1.7743, label: 0, bag_size: 10381\n",
      "batch 599, loss: 0.0033, instance_loss: 0.0006, weighted_loss: 0.0025, label: 0, bag_size: 13880\n",
      "batch 619, loss: 0.0009, instance_loss: 0.0029, weighted_loss: 0.0015, label: 1, bag_size: 10028\n",
      "batch 639, loss: 0.7461, instance_loss: 0.0244, weighted_loss: 0.5296, label: 1, bag_size: 9215\n",
      "batch 659, loss: 0.0028, instance_loss: 0.0000, weighted_loss: 0.0019, label: 0, bag_size: 3893\n",
      "batch 679, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 4497\n",
      "batch 699, loss: 0.0745, instance_loss: 0.0816, weighted_loss: 0.0766, label: 1, bag_size: 8680\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15077\n",
      "batch 739, loss: 0.0017, instance_loss: 0.6464, weighted_loss: 0.1951, label: 0, bag_size: 7381\n",
      "batch 759, loss: 0.0131, instance_loss: 0.3037, weighted_loss: 0.1003, label: 1, bag_size: 1242\n",
      "batch 779, loss: 0.4475, instance_loss: 0.4207, weighted_loss: 0.4395, label: 0, bag_size: 11306\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0024, weighted_loss: 0.0007, label: 1, bag_size: 17486\n",
      "batch 819, loss: 0.0007, instance_loss: 0.7780, weighted_loss: 0.2339, label: 0, bag_size: 14377\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9823932926829269: correct 12889/13120\n",
      "class 1 clustering acc 0.911890243902439: correct 5982/6560\n",
      "Epoch: 68, train_loss: 0.1214, train_clustering_loss:  0.1689, train_error: 0.0500\n",
      "class 0: acc 0.9451371571072319, correct 379/401\n",
      "class 1: acc 0.954653937947494, correct 400/419\n",
      "\n",
      "Val Set, val_loss: 0.1741, val_error: 0.0636, auc: 0.9937\n",
      "class 0 clustering acc 0.9335227272727272: correct 1643/1760\n",
      "class 1 clustering acc 0.7613636363636364: correct 670/880\n",
      "class 0: acc 0.8653846153846154, correct 45/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0282, instance_loss: 0.2400, weighted_loss: 0.0918, label: 0, bag_size: 2043\n",
      "batch 39, loss: 0.0007, instance_loss: 1.5834, weighted_loss: 0.4755, label: 0, bag_size: 11607\n",
      "batch 59, loss: 0.0327, instance_loss: 0.0211, weighted_loss: 0.0292, label: 1, bag_size: 9519\n",
      "batch 79, loss: 0.0137, instance_loss: 0.0102, weighted_loss: 0.0127, label: 1, bag_size: 15609\n",
      "batch 99, loss: 0.0009, instance_loss: 0.5686, weighted_loss: 0.1712, label: 0, bag_size: 7011\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0112, weighted_loss: 0.0034, label: 1, bag_size: 20333\n",
      "batch 139, loss: 0.0002, instance_loss: 0.0001, weighted_loss: 0.0002, label: 0, bag_size: 11113\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 8948\n",
      "batch 179, loss: 0.0002, instance_loss: 0.0249, weighted_loss: 0.0076, label: 1, bag_size: 689\n",
      "batch 199, loss: 0.0829, instance_loss: 0.4306, weighted_loss: 0.1872, label: 1, bag_size: 2522\n",
      "batch 219, loss: 0.3809, instance_loss: 0.0775, weighted_loss: 0.2899, label: 1, bag_size: 7768\n",
      "batch 239, loss: 0.0156, instance_loss: 0.0330, weighted_loss: 0.0208, label: 1, bag_size: 18603\n",
      "batch 259, loss: 0.0005, instance_loss: 0.0010, weighted_loss: 0.0006, label: 1, bag_size: 10482\n",
      "batch 279, loss: 0.0036, instance_loss: 0.0024, weighted_loss: 0.0033, label: 1, bag_size: 11032\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11865\n",
      "batch 319, loss: 3.9547, instance_loss: 1.2174, weighted_loss: 3.1335, label: 1, bag_size: 13367\n",
      "batch 339, loss: 0.1383, instance_loss: 0.0843, weighted_loss: 0.1221, label: 1, bag_size: 9404\n",
      "batch 359, loss: 0.0001, instance_loss: 0.1139, weighted_loss: 0.0342, label: 1, bag_size: 3980\n",
      "batch 379, loss: 0.0002, instance_loss: 0.0029, weighted_loss: 0.0010, label: 1, bag_size: 15213\n",
      "batch 399, loss: 0.0034, instance_loss: 0.0710, weighted_loss: 0.0237, label: 0, bag_size: 9387\n",
      "batch 419, loss: 0.1093, instance_loss: 0.5642, weighted_loss: 0.2457, label: 0, bag_size: 7835\n",
      "batch 439, loss: 0.0067, instance_loss: 0.0000, weighted_loss: 0.0047, label: 1, bag_size: 21701\n",
      "batch 459, loss: 0.0003, instance_loss: 0.0016, weighted_loss: 0.0007, label: 1, bag_size: 2278\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15967\n",
      "batch 499, loss: 0.0005, instance_loss: 0.0160, weighted_loss: 0.0052, label: 1, bag_size: 12611\n",
      "batch 519, loss: 0.0139, instance_loss: 0.0036, weighted_loss: 0.0108, label: 0, bag_size: 21319\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 10481\n",
      "batch 559, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 15313\n",
      "batch 579, loss: 0.1817, instance_loss: 0.8393, weighted_loss: 0.3790, label: 0, bag_size: 1127\n",
      "batch 599, loss: 0.0002, instance_loss: 0.0057, weighted_loss: 0.0019, label: 1, bag_size: 621\n",
      "batch 619, loss: 0.0091, instance_loss: 0.0153, weighted_loss: 0.0109, label: 0, bag_size: 14249\n",
      "batch 639, loss: 0.0568, instance_loss: 0.5275, weighted_loss: 0.1980, label: 0, bag_size: 3089\n",
      "batch 659, loss: 0.0024, instance_loss: 0.0073, weighted_loss: 0.0038, label: 1, bag_size: 9322\n",
      "batch 679, loss: 0.0520, instance_loss: 0.2919, weighted_loss: 0.1240, label: 0, bag_size: 2628\n",
      "batch 699, loss: 0.0003, instance_loss: 0.0054, weighted_loss: 0.0018, label: 1, bag_size: 6606\n",
      "batch 719, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 18215\n",
      "batch 739, loss: 0.0014, instance_loss: 0.0156, weighted_loss: 0.0056, label: 1, bag_size: 8216\n",
      "batch 759, loss: 0.0005, instance_loss: 0.1755, weighted_loss: 0.0530, label: 1, bag_size: 16034\n",
      "batch 779, loss: 0.6204, instance_loss: 0.4057, weighted_loss: 0.5560, label: 0, bag_size: 3375\n",
      "batch 799, loss: 0.0339, instance_loss: 0.0068, weighted_loss: 0.0258, label: 1, bag_size: 20767\n",
      "batch 819, loss: 0.0313, instance_loss: 1.6347, weighted_loss: 0.5123, label: 0, bag_size: 22498\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9785060975609756: correct 12838/13120\n",
      "class 1 clustering acc 0.8914634146341464: correct 5848/6560\n",
      "Epoch: 69, train_loss: 0.1149, train_clustering_loss:  0.1891, train_error: 0.0463\n",
      "class 0: acc 0.9464720194647201, correct 389/411\n",
      "class 1: acc 0.960880195599022, correct 393/409\n",
      "\n",
      "Val Set, val_loss: 0.0959, val_error: 0.0273, auc: 0.9950\n",
      "class 0 clustering acc 0.9375: correct 1650/1760\n",
      "class 1 clustering acc 0.8352272727272727: correct 735/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "Validation loss decreased (0.108828 --> 0.095925).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0016, instance_loss: 0.0001, weighted_loss: 0.0012, label: 0, bag_size: 19808\n",
      "batch 39, loss: 1.4524, instance_loss: 2.1001, weighted_loss: 1.6467, label: 1, bag_size: 15185\n",
      "batch 59, loss: 0.0002, instance_loss: 0.0176, weighted_loss: 0.0054, label: 1, bag_size: 9408\n",
      "batch 79, loss: 0.0317, instance_loss: 1.0031, weighted_loss: 0.3231, label: 0, bag_size: 5009\n",
      "batch 99, loss: 0.0015, instance_loss: 0.1436, weighted_loss: 0.0441, label: 1, bag_size: 12946\n",
      "batch 119, loss: 3.2934, instance_loss: 4.3828, weighted_loss: 3.6202, label: 0, bag_size: 4692\n",
      "batch 139, loss: 0.0063, instance_loss: 0.1240, weighted_loss: 0.0416, label: 1, bag_size: 2278\n",
      "batch 159, loss: 0.0012, instance_loss: 0.0101, weighted_loss: 0.0039, label: 1, bag_size: 1022\n",
      "batch 179, loss: 0.0002, instance_loss: 0.2097, weighted_loss: 0.0631, label: 1, bag_size: 8040\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0667, weighted_loss: 0.0200, label: 1, bag_size: 4442\n",
      "batch 219, loss: 0.0595, instance_loss: 0.0870, weighted_loss: 0.0678, label: 1, bag_size: 10622\n",
      "batch 239, loss: 0.0069, instance_loss: 0.0017, weighted_loss: 0.0053, label: 1, bag_size: 1888\n",
      "batch 259, loss: 0.0011, instance_loss: 0.0014, weighted_loss: 0.0012, label: 1, bag_size: 2936\n",
      "batch 279, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 1, bag_size: 1838\n",
      "batch 299, loss: 0.0015, instance_loss: 0.0003, weighted_loss: 0.0011, label: 0, bag_size: 5409\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0284, weighted_loss: 0.0085, label: 1, bag_size: 10112\n",
      "batch 339, loss: 0.0076, instance_loss: 0.8669, weighted_loss: 0.2654, label: 0, bag_size: 5639\n",
      "batch 359, loss: 0.0016, instance_loss: 0.0022, weighted_loss: 0.0018, label: 1, bag_size: 16267\n",
      "batch 379, loss: 0.0002, instance_loss: 0.0545, weighted_loss: 0.0165, label: 1, bag_size: 1064\n",
      "batch 399, loss: 0.0004, instance_loss: 0.0786, weighted_loss: 0.0239, label: 0, bag_size: 1213\n",
      "batch 419, loss: 0.0000, instance_loss: 0.1397, weighted_loss: 0.0419, label: 1, bag_size: 6792\n",
      "batch 439, loss: 0.0009, instance_loss: 0.0012, weighted_loss: 0.0010, label: 1, bag_size: 8040\n",
      "batch 459, loss: 0.0013, instance_loss: 0.0110, weighted_loss: 0.0043, label: 1, bag_size: 7381\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 12217\n",
      "batch 499, loss: 0.0564, instance_loss: 0.0093, weighted_loss: 0.0423, label: 1, bag_size: 6090\n",
      "batch 519, loss: 0.0042, instance_loss: 0.0000, weighted_loss: 0.0030, label: 0, bag_size: 3552\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 15464\n",
      "batch 559, loss: 0.1566, instance_loss: 0.2629, weighted_loss: 0.1885, label: 1, bag_size: 11223\n",
      "batch 579, loss: 0.0691, instance_loss: 0.0202, weighted_loss: 0.0545, label: 1, bag_size: 6682\n",
      "batch 599, loss: 0.0005, instance_loss: 0.0500, weighted_loss: 0.0154, label: 1, bag_size: 16034\n",
      "batch 619, loss: 0.0133, instance_loss: 0.0308, weighted_loss: 0.0185, label: 0, bag_size: 12510\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 3228\n",
      "batch 659, loss: 0.3859, instance_loss: 0.0671, weighted_loss: 0.2902, label: 0, bag_size: 11390\n",
      "batch 679, loss: 0.5260, instance_loss: 0.5018, weighted_loss: 0.5187, label: 0, bag_size: 6356\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9885\n",
      "batch 719, loss: 0.0283, instance_loss: 0.0053, weighted_loss: 0.0214, label: 1, bag_size: 11386\n",
      "batch 739, loss: 0.0046, instance_loss: 0.0791, weighted_loss: 0.0269, label: 0, bag_size: 7557\n",
      "batch 759, loss: 0.0102, instance_loss: 0.0074, weighted_loss: 0.0094, label: 0, bag_size: 10113\n",
      "batch 779, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 3228\n",
      "batch 799, loss: 0.0112, instance_loss: 0.5066, weighted_loss: 0.1598, label: 0, bag_size: 13339\n",
      "batch 819, loss: 3.6572, instance_loss: 0.5381, weighted_loss: 2.7215, label: 0, bag_size: 2815\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9775914634146341: correct 12826/13120\n",
      "class 1 clustering acc 0.8911585365853658: correct 5846/6560\n",
      "Epoch: 70, train_loss: 0.1204, train_clustering_loss:  0.2170, train_error: 0.0488\n",
      "class 0: acc 0.9455958549222798, correct 365/386\n",
      "class 1: acc 0.956221198156682, correct 415/434\n",
      "\n",
      "Val Set, val_loss: 0.1197, val_error: 0.0182, auc: 0.9907\n",
      "class 0 clustering acc 0.9431818181818182: correct 1660/1760\n",
      "class 1 clustering acc 0.8125: correct 715/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0019, weighted_loss: 0.0007, label: 0, bag_size: 14681\n",
      "batch 39, loss: 0.0015, instance_loss: 0.0168, weighted_loss: 0.0061, label: 1, bag_size: 4821\n",
      "batch 59, loss: 0.0065, instance_loss: 0.1144, weighted_loss: 0.0389, label: 0, bag_size: 2534\n",
      "batch 79, loss: 0.0001, instance_loss: 0.1722, weighted_loss: 0.0517, label: 1, bag_size: 11981\n",
      "batch 99, loss: 0.0293, instance_loss: 0.5291, weighted_loss: 0.1792, label: 1, bag_size: 5723\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 1, bag_size: 14223\n",
      "batch 139, loss: 0.0016, instance_loss: 0.0981, weighted_loss: 0.0306, label: 1, bag_size: 4308\n",
      "batch 159, loss: 0.0236, instance_loss: 0.2561, weighted_loss: 0.0934, label: 0, bag_size: 2609\n",
      "batch 179, loss: 0.0050, instance_loss: 0.0768, weighted_loss: 0.0265, label: 1, bag_size: 3640\n",
      "batch 199, loss: 0.0178, instance_loss: 0.0039, weighted_loss: 0.0137, label: 0, bag_size: 8866\n",
      "batch 219, loss: 0.0019, instance_loss: 0.0030, weighted_loss: 0.0022, label: 0, bag_size: 6898\n",
      "batch 239, loss: 0.0054, instance_loss: 0.0214, weighted_loss: 0.0102, label: 0, bag_size: 2382\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0102, weighted_loss: 0.0031, label: 1, bag_size: 689\n",
      "batch 279, loss: 0.0004, instance_loss: 0.0044, weighted_loss: 0.0016, label: 0, bag_size: 9234\n",
      "batch 299, loss: 0.0104, instance_loss: 0.4756, weighted_loss: 0.1500, label: 0, bag_size: 2004\n",
      "batch 319, loss: 0.0010, instance_loss: 0.0159, weighted_loss: 0.0054, label: 1, bag_size: 7246\n",
      "batch 339, loss: 0.0138, instance_loss: 0.0318, weighted_loss: 0.0192, label: 1, bag_size: 5723\n",
      "batch 359, loss: 0.0015, instance_loss: 0.2366, weighted_loss: 0.0721, label: 0, bag_size: 1881\n",
      "batch 379, loss: 0.0015, instance_loss: 0.0246, weighted_loss: 0.0084, label: 1, bag_size: 2278\n",
      "batch 399, loss: 0.0002, instance_loss: 0.0021, weighted_loss: 0.0008, label: 0, bag_size: 2920\n",
      "batch 419, loss: 0.0310, instance_loss: 0.9271, weighted_loss: 0.2998, label: 1, bag_size: 2759\n",
      "batch 439, loss: 0.0280, instance_loss: 0.1708, weighted_loss: 0.0708, label: 0, bag_size: 11281\n",
      "batch 459, loss: 0.4304, instance_loss: 0.2012, weighted_loss: 0.3617, label: 1, bag_size: 8103\n",
      "batch 479, loss: 0.0006, instance_loss: 0.0096, weighted_loss: 0.0033, label: 0, bag_size: 14956\n",
      "batch 499, loss: 0.0011, instance_loss: 0.0099, weighted_loss: 0.0037, label: 0, bag_size: 7989\n",
      "batch 519, loss: 0.0002, instance_loss: 0.0249, weighted_loss: 0.0076, label: 1, bag_size: 12931\n",
      "batch 539, loss: 0.0069, instance_loss: 0.0305, weighted_loss: 0.0140, label: 1, bag_size: 16514\n",
      "batch 559, loss: 0.0006, instance_loss: 0.0708, weighted_loss: 0.0217, label: 0, bag_size: 4902\n",
      "batch 579, loss: 0.0006, instance_loss: 0.0147, weighted_loss: 0.0048, label: 1, bag_size: 5340\n",
      "batch 599, loss: 0.0477, instance_loss: 0.0004, weighted_loss: 0.0335, label: 0, bag_size: 19067\n",
      "batch 619, loss: 0.0023, instance_loss: 0.0261, weighted_loss: 0.0094, label: 0, bag_size: 8755\n",
      "batch 639, loss: 0.0002, instance_loss: 0.0008, weighted_loss: 0.0004, label: 0, bag_size: 9470\n",
      "batch 659, loss: 0.1376, instance_loss: 0.0021, weighted_loss: 0.0970, label: 0, bag_size: 2219\n",
      "batch 679, loss: 0.0003, instance_loss: 0.0141, weighted_loss: 0.0045, label: 1, bag_size: 4877\n",
      "batch 699, loss: 0.0006, instance_loss: 0.0003, weighted_loss: 0.0005, label: 0, bag_size: 12212\n",
      "batch 719, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 9234\n",
      "batch 739, loss: 0.0015, instance_loss: 0.0018, weighted_loss: 0.0016, label: 1, bag_size: 5345\n",
      "batch 759, loss: 0.0426, instance_loss: 0.0051, weighted_loss: 0.0314, label: 1, bag_size: 11964\n",
      "batch 779, loss: 0.0022, instance_loss: 0.0062, weighted_loss: 0.0034, label: 1, bag_size: 1101\n",
      "batch 799, loss: 0.0040, instance_loss: 0.0250, weighted_loss: 0.0103, label: 1, bag_size: 4308\n",
      "batch 819, loss: 0.0193, instance_loss: 0.0436, weighted_loss: 0.0266, label: 1, bag_size: 1493\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9788871951219512: correct 12843/13120\n",
      "class 1 clustering acc 0.9048780487804878: correct 5936/6560\n",
      "Epoch: 71, train_loss: 0.1384, train_clustering_loss:  0.2026, train_error: 0.0500\n",
      "class 0: acc 0.9447004608294931, correct 410/434\n",
      "class 1: acc 0.9559585492227979, correct 369/386\n",
      "\n",
      "Val Set, val_loss: 0.1179, val_error: 0.0182, auc: 0.9914\n",
      "class 0 clustering acc 0.9420454545454545: correct 1658/1760\n",
      "class 1 clustering acc 0.8147727272727273: correct 717/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0211, weighted_loss: 0.0064, label: 1, bag_size: 9321\n",
      "batch 39, loss: 0.3296, instance_loss: 0.6027, weighted_loss: 0.4115, label: 1, bag_size: 3121\n",
      "batch 59, loss: 0.1599, instance_loss: 1.0522, weighted_loss: 0.4276, label: 0, bag_size: 2815\n",
      "batch 79, loss: 0.0019, instance_loss: 0.0509, weighted_loss: 0.0166, label: 0, bag_size: 9387\n",
      "batch 99, loss: 0.0009, instance_loss: 0.0839, weighted_loss: 0.0258, label: 0, bag_size: 11146\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 1, bag_size: 7078\n",
      "batch 139, loss: 0.0006, instance_loss: 0.8385, weighted_loss: 0.2520, label: 1, bag_size: 2904\n",
      "batch 159, loss: 0.0079, instance_loss: 1.4067, weighted_loss: 0.4275, label: 0, bag_size: 2457\n",
      "batch 179, loss: 0.2140, instance_loss: 2.7809, weighted_loss: 0.9841, label: 0, bag_size: 26208\n",
      "batch 199, loss: 0.0203, instance_loss: 0.3044, weighted_loss: 0.1055, label: 0, bag_size: 7612\n",
      "batch 219, loss: 0.0086, instance_loss: 0.2868, weighted_loss: 0.0920, label: 0, bag_size: 15003\n",
      "batch 239, loss: 0.0272, instance_loss: 0.0274, weighted_loss: 0.0273, label: 1, bag_size: 4821\n",
      "batch 259, loss: 0.0816, instance_loss: 0.1878, weighted_loss: 0.1135, label: 1, bag_size: 5921\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0258, weighted_loss: 0.0078, label: 1, bag_size: 11122\n",
      "batch 299, loss: 0.0112, instance_loss: 0.0752, weighted_loss: 0.0304, label: 1, bag_size: 3368\n",
      "batch 319, loss: 0.0012, instance_loss: 0.3040, weighted_loss: 0.0921, label: 0, bag_size: 1149\n",
      "batch 339, loss: 0.3223, instance_loss: 0.2819, weighted_loss: 0.3102, label: 1, bag_size: 7989\n",
      "batch 359, loss: 0.0009, instance_loss: 0.3913, weighted_loss: 0.1180, label: 1, bag_size: 4423\n",
      "batch 379, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 13964\n",
      "batch 399, loss: 0.1515, instance_loss: 0.0517, weighted_loss: 0.1215, label: 1, bag_size: 8680\n",
      "batch 419, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 10146\n",
      "batch 439, loss: 0.0060, instance_loss: 0.0009, weighted_loss: 0.0044, label: 1, bag_size: 13174\n",
      "batch 459, loss: 0.0121, instance_loss: 0.0008, weighted_loss: 0.0087, label: 1, bag_size: 8466\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0026, weighted_loss: 0.0008, label: 1, bag_size: 5731\n",
      "batch 499, loss: 0.1574, instance_loss: 0.0031, weighted_loss: 0.1111, label: 1, bag_size: 7873\n",
      "batch 519, loss: 0.0125, instance_loss: 0.0000, weighted_loss: 0.0088, label: 0, bag_size: 12201\n",
      "batch 539, loss: 0.1158, instance_loss: 0.0121, weighted_loss: 0.0847, label: 1, bag_size: 1819\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 27158\n",
      "batch 579, loss: 0.0223, instance_loss: 0.0556, weighted_loss: 0.0323, label: 1, bag_size: 1920\n",
      "batch 599, loss: 0.0033, instance_loss: 0.0144, weighted_loss: 0.0066, label: 0, bag_size: 2296\n",
      "batch 619, loss: 0.2950, instance_loss: 0.2168, weighted_loss: 0.2715, label: 1, bag_size: 898\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0346, weighted_loss: 0.0105, label: 1, bag_size: 3980\n",
      "batch 659, loss: 0.0018, instance_loss: 0.0003, weighted_loss: 0.0013, label: 0, bag_size: 8582\n",
      "batch 679, loss: 0.0154, instance_loss: 0.0258, weighted_loss: 0.0185, label: 0, bag_size: 4845\n",
      "batch 699, loss: 0.0014, instance_loss: 0.0126, weighted_loss: 0.0048, label: 0, bag_size: 1831\n",
      "batch 719, loss: 0.1682, instance_loss: 0.0439, weighted_loss: 0.1309, label: 0, bag_size: 6367\n",
      "batch 739, loss: 0.0747, instance_loss: 0.5350, weighted_loss: 0.2128, label: 0, bag_size: 10113\n",
      "batch 759, loss: 0.0018, instance_loss: 0.3204, weighted_loss: 0.0974, label: 0, bag_size: 1891\n",
      "batch 779, loss: 0.0104, instance_loss: 0.0190, weighted_loss: 0.0129, label: 0, bag_size: 1508\n",
      "batch 799, loss: 0.0444, instance_loss: 0.1913, weighted_loss: 0.0885, label: 1, bag_size: 1920\n",
      "batch 819, loss: 0.0173, instance_loss: 0.0183, weighted_loss: 0.0176, label: 1, bag_size: 4821\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9772865853658537: correct 12822/13120\n",
      "class 1 clustering acc 0.8916158536585366: correct 5849/6560\n",
      "Epoch: 72, train_loss: 0.1158, train_clustering_loss:  0.2052, train_error: 0.0476\n",
      "class 0: acc 0.9440389294403893, correct 388/411\n",
      "class 1: acc 0.960880195599022, correct 393/409\n",
      "\n",
      "Val Set, val_loss: 0.1563, val_error: 0.0545, auc: 0.9920\n",
      "class 0 clustering acc 0.95625: correct 1683/1760\n",
      "class 1 clustering acc 0.8409090909090909: correct 740/880\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 12201\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 2920\n",
      "batch 59, loss: 0.0413, instance_loss: 0.0000, weighted_loss: 0.0289, label: 1, bag_size: 1022\n",
      "batch 79, loss: 0.0001, instance_loss: 0.3956, weighted_loss: 0.1187, label: 0, bag_size: 19466\n",
      "batch 99, loss: 1.0604, instance_loss: 0.2108, weighted_loss: 0.8055, label: 1, bag_size: 9162\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0098, weighted_loss: 0.0030, label: 0, bag_size: 11527\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 6851\n",
      "batch 159, loss: 0.0044, instance_loss: 0.0043, weighted_loss: 0.0043, label: 1, bag_size: 11684\n",
      "batch 179, loss: 0.0012, instance_loss: 0.0202, weighted_loss: 0.0069, label: 0, bag_size: 10751\n",
      "batch 199, loss: 0.0008, instance_loss: 0.0024, weighted_loss: 0.0013, label: 0, bag_size: 10995\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0064, weighted_loss: 0.0019, label: 1, bag_size: 9673\n",
      "batch 239, loss: 0.0084, instance_loss: 0.0553, weighted_loss: 0.0224, label: 1, bag_size: 7424\n",
      "batch 259, loss: 0.0092, instance_loss: 0.0141, weighted_loss: 0.0107, label: 1, bag_size: 5723\n",
      "batch 279, loss: 0.0022, instance_loss: 0.1314, weighted_loss: 0.0410, label: 0, bag_size: 1416\n",
      "batch 299, loss: 0.0615, instance_loss: 0.0111, weighted_loss: 0.0464, label: 1, bag_size: 14887\n",
      "batch 319, loss: 0.1141, instance_loss: 0.0108, weighted_loss: 0.0831, label: 0, bag_size: 8744\n",
      "batch 339, loss: 0.0910, instance_loss: 0.0390, weighted_loss: 0.0754, label: 1, bag_size: 1242\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0320, weighted_loss: 0.0096, label: 1, bag_size: 629\n",
      "batch 379, loss: 0.0007, instance_loss: 0.0580, weighted_loss: 0.0179, label: 0, bag_size: 9171\n",
      "batch 399, loss: 0.0047, instance_loss: 0.1485, weighted_loss: 0.0479, label: 1, bag_size: 7119\n",
      "batch 419, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 21404\n",
      "batch 439, loss: 0.0277, instance_loss: 0.0567, weighted_loss: 0.0364, label: 1, bag_size: 8191\n",
      "batch 459, loss: 0.4216, instance_loss: 0.1134, weighted_loss: 0.3291, label: 1, bag_size: 12494\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0625, weighted_loss: 0.0188, label: 1, bag_size: 9078\n",
      "batch 499, loss: 0.0302, instance_loss: 0.0398, weighted_loss: 0.0331, label: 1, bag_size: 2678\n",
      "batch 519, loss: 0.2509, instance_loss: 0.0081, weighted_loss: 0.1781, label: 0, bag_size: 2918\n",
      "batch 539, loss: 0.0277, instance_loss: 0.7165, weighted_loss: 0.2344, label: 0, bag_size: 9866\n",
      "batch 559, loss: 0.4191, instance_loss: 1.8209, weighted_loss: 0.8397, label: 1, bag_size: 13367\n",
      "batch 579, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 3541\n",
      "batch 599, loss: 0.0013, instance_loss: 0.5386, weighted_loss: 0.1625, label: 1, bag_size: 9533\n",
      "batch 619, loss: 0.0162, instance_loss: 0.0003, weighted_loss: 0.0114, label: 0, bag_size: 3810\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 27158\n",
      "batch 659, loss: 0.0005, instance_loss: 0.0002, weighted_loss: 0.0004, label: 1, bag_size: 14618\n",
      "batch 679, loss: 0.1533, instance_loss: 0.0580, weighted_loss: 0.1247, label: 1, bag_size: 6842\n",
      "batch 699, loss: 0.0018, instance_loss: 0.0038, weighted_loss: 0.0024, label: 0, bag_size: 2652\n",
      "batch 719, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 2820\n",
      "batch 739, loss: 0.0001, instance_loss: 0.0281, weighted_loss: 0.0085, label: 1, bag_size: 5221\n",
      "batch 759, loss: 0.0000, instance_loss: 0.2950, weighted_loss: 0.0885, label: 1, bag_size: 3295\n",
      "batch 779, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 8755\n",
      "batch 799, loss: 0.0004, instance_loss: 0.0361, weighted_loss: 0.0111, label: 0, bag_size: 23368\n",
      "batch 819, loss: 0.0150, instance_loss: 0.0238, weighted_loss: 0.0176, label: 1, bag_size: 5292\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9772865853658537: correct 12822/13120\n",
      "class 1 clustering acc 0.9108231707317073: correct 5975/6560\n",
      "Epoch: 73, train_loss: 0.1329, train_clustering_loss:  0.2121, train_error: 0.0512\n",
      "class 0: acc 0.9456264775413712, correct 400/423\n",
      "class 1: acc 0.9521410579345088, correct 378/397\n",
      "\n",
      "Val Set, val_loss: 0.3692, val_error: 0.1727, auc: 0.9917\n",
      "class 0 clustering acc 0.9488636363636364: correct 1670/1760\n",
      "class 1 clustering acc 0.8363636363636363: correct 736/880\n",
      "class 0: acc 0.6346153846153846, correct 33/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0253, instance_loss: 0.2389, weighted_loss: 0.0894, label: 0, bag_size: 20555\n",
      "batch 39, loss: 0.0099, instance_loss: 0.4953, weighted_loss: 0.1555, label: 1, bag_size: 1867\n",
      "batch 59, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 11865\n",
      "batch 79, loss: 0.4015, instance_loss: 0.0298, weighted_loss: 0.2900, label: 1, bag_size: 11256\n",
      "batch 99, loss: 0.0502, instance_loss: 0.3060, weighted_loss: 0.1269, label: 1, bag_size: 11160\n",
      "batch 119, loss: 0.0006, instance_loss: 0.0370, weighted_loss: 0.0115, label: 1, bag_size: 7381\n",
      "batch 139, loss: 0.0016, instance_loss: 0.0071, weighted_loss: 0.0032, label: 1, bag_size: 10072\n",
      "batch 159, loss: 1.3823, instance_loss: 0.0165, weighted_loss: 0.9725, label: 0, bag_size: 5120\n",
      "batch 179, loss: 1.5231, instance_loss: 0.1477, weighted_loss: 1.1105, label: 1, bag_size: 1703\n",
      "batch 199, loss: 0.1128, instance_loss: 0.0088, weighted_loss: 0.0816, label: 0, bag_size: 22498\n",
      "batch 219, loss: 0.0010, instance_loss: 0.0634, weighted_loss: 0.0197, label: 1, bag_size: 3856\n",
      "batch 239, loss: 0.0002, instance_loss: 0.0006, weighted_loss: 0.0004, label: 1, bag_size: 15213\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0055, weighted_loss: 0.0017, label: 1, bag_size: 15213\n",
      "batch 279, loss: 0.0054, instance_loss: 0.0000, weighted_loss: 0.0038, label: 0, bag_size: 13880\n",
      "batch 299, loss: 0.0056, instance_loss: 0.7608, weighted_loss: 0.2322, label: 0, bag_size: 5639\n",
      "batch 319, loss: 0.3717, instance_loss: 1.0498, weighted_loss: 0.5751, label: 1, bag_size: 9404\n",
      "batch 339, loss: 0.0019, instance_loss: 0.1162, weighted_loss: 0.0362, label: 0, bag_size: 2336\n",
      "batch 359, loss: 0.0338, instance_loss: 0.1204, weighted_loss: 0.0598, label: 1, bag_size: 12895\n",
      "batch 379, loss: 0.0764, instance_loss: 0.0390, weighted_loss: 0.0652, label: 1, bag_size: 14887\n",
      "batch 399, loss: 0.0017, instance_loss: 0.0584, weighted_loss: 0.0187, label: 1, bag_size: 3409\n",
      "batch 419, loss: 0.0045, instance_loss: 0.0896, weighted_loss: 0.0300, label: 1, bag_size: 15464\n",
      "batch 439, loss: 0.0035, instance_loss: 0.0000, weighted_loss: 0.0025, label: 0, bag_size: 24439\n",
      "batch 459, loss: 0.0026, instance_loss: 0.0856, weighted_loss: 0.0275, label: 0, bag_size: 2382\n",
      "batch 479, loss: 0.0032, instance_loss: 0.0161, weighted_loss: 0.0071, label: 0, bag_size: 19067\n",
      "batch 499, loss: 0.0268, instance_loss: 0.0061, weighted_loss: 0.0206, label: 1, bag_size: 6731\n",
      "batch 519, loss: 0.0884, instance_loss: 0.0693, weighted_loss: 0.0826, label: 0, bag_size: 2098\n",
      "batch 539, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0015, label: 0, bag_size: 2844\n",
      "batch 559, loss: 0.0142, instance_loss: 0.0554, weighted_loss: 0.0265, label: 0, bag_size: 1508\n",
      "batch 579, loss: 0.0260, instance_loss: 0.0154, weighted_loss: 0.0228, label: 1, bag_size: 2140\n",
      "batch 599, loss: 0.0005, instance_loss: 0.0215, weighted_loss: 0.0068, label: 0, bag_size: 9455\n",
      "batch 619, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 11546\n",
      "batch 639, loss: 0.0021, instance_loss: 0.0041, weighted_loss: 0.0027, label: 1, bag_size: 13255\n",
      "batch 659, loss: 0.0107, instance_loss: 0.0008, weighted_loss: 0.0077, label: 1, bag_size: 16267\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0010, weighted_loss: 0.0004, label: 1, bag_size: 12611\n",
      "batch 699, loss: 0.0034, instance_loss: 0.0004, weighted_loss: 0.0025, label: 0, bag_size: 8582\n",
      "batch 719, loss: 0.0319, instance_loss: 0.1811, weighted_loss: 0.0767, label: 1, bag_size: 4039\n",
      "batch 739, loss: 0.0032, instance_loss: 0.0001, weighted_loss: 0.0023, label: 0, bag_size: 19067\n",
      "batch 759, loss: 0.0309, instance_loss: 0.0114, weighted_loss: 0.0251, label: 0, bag_size: 2624\n",
      "batch 779, loss: 0.0000, instance_loss: 0.0684, weighted_loss: 0.0206, label: 1, bag_size: 19932\n",
      "batch 799, loss: 0.0001, instance_loss: 0.0044, weighted_loss: 0.0014, label: 1, bag_size: 4862\n",
      "batch 819, loss: 0.0060, instance_loss: 0.0000, weighted_loss: 0.0042, label: 0, bag_size: 19067\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9807164634146341: correct 12867/13120\n",
      "class 1 clustering acc 0.9172256097560976: correct 6017/6560\n",
      "Epoch: 74, train_loss: 0.1308, train_clustering_loss:  0.1844, train_error: 0.0537\n",
      "class 0: acc 0.9451073985680191, correct 396/419\n",
      "class 1: acc 0.9476309226932669, correct 380/401\n",
      "\n",
      "Val Set, val_loss: 0.1204, val_error: 0.0273, auc: 0.9927\n",
      "class 0 clustering acc 0.9585227272727272: correct 1687/1760\n",
      "class 1 clustering acc 0.875: correct 770/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0428, instance_loss: 0.0032, weighted_loss: 0.0309, label: 0, bag_size: 9069\n",
      "batch 39, loss: 0.0962, instance_loss: 0.0889, weighted_loss: 0.0940, label: 1, bag_size: 1123\n",
      "batch 59, loss: 0.8223, instance_loss: 0.1294, weighted_loss: 0.6144, label: 0, bag_size: 2918\n",
      "batch 79, loss: 0.0004, instance_loss: 0.0564, weighted_loss: 0.0172, label: 1, bag_size: 5256\n",
      "batch 99, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0015, label: 0, bag_size: 11917\n",
      "batch 119, loss: 0.0027, instance_loss: 0.0958, weighted_loss: 0.0306, label: 1, bag_size: 18794\n",
      "batch 139, loss: 0.1231, instance_loss: 4.6290, weighted_loss: 1.4748, label: 0, bag_size: 2213\n",
      "batch 159, loss: 0.0005, instance_loss: 0.1509, weighted_loss: 0.0456, label: 1, bag_size: 2455\n",
      "batch 179, loss: 0.0385, instance_loss: 0.0215, weighted_loss: 0.0334, label: 1, bag_size: 5629\n",
      "batch 199, loss: 0.0024, instance_loss: 0.0345, weighted_loss: 0.0120, label: 1, bag_size: 4039\n",
      "batch 219, loss: 0.0008, instance_loss: 0.0001, weighted_loss: 0.0006, label: 0, bag_size: 15636\n",
      "batch 239, loss: 0.0879, instance_loss: 0.5428, weighted_loss: 0.2244, label: 0, bag_size: 1637\n",
      "batch 259, loss: 0.3658, instance_loss: 0.1958, weighted_loss: 0.3148, label: 1, bag_size: 771\n",
      "batch 279, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 11125\n",
      "batch 299, loss: 0.0124, instance_loss: 0.0633, weighted_loss: 0.0277, label: 0, bag_size: 6624\n",
      "batch 319, loss: 0.0065, instance_loss: 0.0000, weighted_loss: 0.0045, label: 0, bag_size: 2004\n",
      "batch 339, loss: 0.0250, instance_loss: 0.0527, weighted_loss: 0.0333, label: 0, bag_size: 2609\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0004, label: 1, bag_size: 14202\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0157, weighted_loss: 0.0047, label: 1, bag_size: 11981\n",
      "batch 399, loss: 0.0001, instance_loss: 0.0048, weighted_loss: 0.0015, label: 1, bag_size: 6734\n",
      "batch 419, loss: 0.0001, instance_loss: 0.0770, weighted_loss: 0.0232, label: 0, bag_size: 23398\n",
      "batch 439, loss: 0.0036, instance_loss: 0.4448, weighted_loss: 0.1359, label: 0, bag_size: 9888\n",
      "batch 459, loss: 0.1362, instance_loss: 0.0953, weighted_loss: 0.1239, label: 1, bag_size: 6665\n",
      "batch 479, loss: 0.0008, instance_loss: 0.0111, weighted_loss: 0.0039, label: 1, bag_size: 6950\n",
      "batch 499, loss: 0.0002, instance_loss: 0.0038, weighted_loss: 0.0013, label: 1, bag_size: 10725\n",
      "batch 519, loss: 0.0007, instance_loss: 0.0577, weighted_loss: 0.0178, label: 1, bag_size: 6453\n",
      "batch 539, loss: 0.1293, instance_loss: 0.1566, weighted_loss: 0.1375, label: 1, bag_size: 8026\n",
      "batch 559, loss: 1.0872, instance_loss: 0.4410, weighted_loss: 0.8933, label: 0, bag_size: 3375\n",
      "batch 579, loss: 0.0007, instance_loss: 0.0086, weighted_loss: 0.0031, label: 0, bag_size: 2652\n",
      "batch 599, loss: 0.0010, instance_loss: 0.0050, weighted_loss: 0.0022, label: 0, bag_size: 11654\n",
      "batch 619, loss: 0.2234, instance_loss: 0.6812, weighted_loss: 0.3607, label: 1, bag_size: 10848\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15077\n",
      "batch 659, loss: 0.0351, instance_loss: 0.0224, weighted_loss: 0.0313, label: 1, bag_size: 1963\n",
      "batch 679, loss: 0.1544, instance_loss: 0.3546, weighted_loss: 0.2145, label: 0, bag_size: 4845\n",
      "batch 699, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 11259\n",
      "batch 719, loss: 0.0151, instance_loss: 0.0007, weighted_loss: 0.0108, label: 1, bag_size: 6745\n",
      "batch 739, loss: 0.0001, instance_loss: 0.0055, weighted_loss: 0.0017, label: 0, bag_size: 11546\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15841\n",
      "batch 779, loss: 0.0018, instance_loss: 0.0444, weighted_loss: 0.0145, label: 0, bag_size: 1415\n",
      "batch 799, loss: 0.0209, instance_loss: 0.3143, weighted_loss: 0.1090, label: 1, bag_size: 1919\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 0, bag_size: 3228\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9810213414634147: correct 12871/13120\n",
      "class 1 clustering acc 0.9217987804878048: correct 6047/6560\n",
      "Epoch: 75, train_loss: 0.1007, train_clustering_loss:  0.1678, train_error: 0.0390\n",
      "class 0: acc 0.9541062801932367, correct 395/414\n",
      "class 1: acc 0.9679802955665024, correct 393/406\n",
      "\n",
      "Val Set, val_loss: 0.1219, val_error: 0.0273, auc: 0.9924\n",
      "class 0 clustering acc 0.9602272727272727: correct 1690/1760\n",
      "class 1 clustering acc 0.8727272727272727: correct 768/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 17633\n",
      "batch 39, loss: 0.0102, instance_loss: 0.0263, weighted_loss: 0.0150, label: 0, bag_size: 1639\n",
      "batch 59, loss: 0.0328, instance_loss: 0.0008, weighted_loss: 0.0232, label: 1, bag_size: 16565\n",
      "batch 79, loss: 0.0024, instance_loss: 0.0345, weighted_loss: 0.0120, label: 1, bag_size: 15464\n",
      "batch 99, loss: 0.0003, instance_loss: 0.0020, weighted_loss: 0.0008, label: 1, bag_size: 5345\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0104, weighted_loss: 0.0031, label: 0, bag_size: 27158\n",
      "batch 139, loss: 0.0002, instance_loss: 0.0253, weighted_loss: 0.0078, label: 0, bag_size: 14305\n",
      "batch 159, loss: 0.0027, instance_loss: 0.0748, weighted_loss: 0.0243, label: 1, bag_size: 8754\n",
      "batch 179, loss: 0.0016, instance_loss: 0.1086, weighted_loss: 0.0337, label: 1, bag_size: 8216\n",
      "batch 199, loss: 0.9224, instance_loss: 0.0409, weighted_loss: 0.6580, label: 0, bag_size: 18516\n",
      "batch 219, loss: 0.0571, instance_loss: 0.0141, weighted_loss: 0.0442, label: 1, bag_size: 2179\n",
      "batch 239, loss: 0.0027, instance_loss: 0.0057, weighted_loss: 0.0036, label: 1, bag_size: 5345\n",
      "batch 259, loss: 0.0250, instance_loss: 0.0100, weighted_loss: 0.0205, label: 0, bag_size: 10898\n",
      "batch 279, loss: 0.0059, instance_loss: 0.0000, weighted_loss: 0.0041, label: 0, bag_size: 3725\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0052, weighted_loss: 0.0016, label: 0, bag_size: 11865\n",
      "batch 319, loss: 0.0520, instance_loss: 0.0179, weighted_loss: 0.0418, label: 1, bag_size: 1919\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23996\n",
      "batch 359, loss: 0.3154, instance_loss: 0.0817, weighted_loss: 0.2453, label: 1, bag_size: 2681\n",
      "batch 379, loss: 0.0008, instance_loss: 0.0116, weighted_loss: 0.0041, label: 1, bag_size: 14887\n",
      "batch 399, loss: 0.0141, instance_loss: 0.0005, weighted_loss: 0.0100, label: 0, bag_size: 2063\n",
      "batch 419, loss: 0.0018, instance_loss: 0.0051, weighted_loss: 0.0028, label: 0, bag_size: 1234\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 32227\n",
      "batch 459, loss: 0.0007, instance_loss: 0.0800, weighted_loss: 0.0245, label: 1, bag_size: 3980\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 16720\n",
      "batch 499, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 2652\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 3228\n",
      "batch 539, loss: 0.0095, instance_loss: 0.0037, weighted_loss: 0.0078, label: 1, bag_size: 5907\n",
      "batch 559, loss: 0.0109, instance_loss: 0.0184, weighted_loss: 0.0132, label: 1, bag_size: 2146\n",
      "batch 579, loss: 0.0131, instance_loss: 0.0034, weighted_loss: 0.0102, label: 1, bag_size: 16890\n",
      "batch 599, loss: 0.0814, instance_loss: 0.0224, weighted_loss: 0.0637, label: 0, bag_size: 7557\n",
      "batch 619, loss: 0.0060, instance_loss: 0.0122, weighted_loss: 0.0078, label: 0, bag_size: 15001\n",
      "batch 639, loss: 0.0049, instance_loss: 0.0214, weighted_loss: 0.0099, label: 1, bag_size: 2814\n",
      "batch 659, loss: 0.0010, instance_loss: 0.0007, weighted_loss: 0.0009, label: 1, bag_size: 9571\n",
      "batch 679, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 8145\n",
      "batch 699, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 10444\n",
      "batch 719, loss: 0.0039, instance_loss: 0.0000, weighted_loss: 0.0027, label: 0, bag_size: 15003\n",
      "batch 739, loss: 0.0005, instance_loss: 0.0007, weighted_loss: 0.0006, label: 1, bag_size: 11875\n",
      "batch 759, loss: 0.0117, instance_loss: 0.0017, weighted_loss: 0.0087, label: 1, bag_size: 8868\n",
      "batch 779, loss: 0.0001, instance_loss: 0.0017, weighted_loss: 0.0006, label: 1, bag_size: 6966\n",
      "batch 799, loss: 0.0001, instance_loss: 0.0005, weighted_loss: 0.0002, label: 1, bag_size: 15093\n",
      "batch 819, loss: 0.0013, instance_loss: 0.0064, weighted_loss: 0.0028, label: 0, bag_size: 10535\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9852134146341464: correct 12926/13120\n",
      "class 1 clustering acc 0.9384146341463414: correct 6156/6560\n",
      "Epoch: 76, train_loss: 0.1007, train_clustering_loss:  0.1357, train_error: 0.0390\n",
      "class 0: acc 0.9569620253164557, correct 378/395\n",
      "class 1: acc 0.9647058823529412, correct 410/425\n",
      "\n",
      "Val Set, val_loss: 0.3273, val_error: 0.1455, auc: 0.9930\n",
      "class 0 clustering acc 0.9232954545454546: correct 1625/1760\n",
      "class 1 clustering acc 0.8011363636363636: correct 705/880\n",
      "class 0: acc 0.6923076923076923, correct 36/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0091, instance_loss: 0.1537, weighted_loss: 0.0525, label: 0, bag_size: 6367\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0028, weighted_loss: 0.0009, label: 0, bag_size: 9930\n",
      "batch 59, loss: 0.0040, instance_loss: 0.0422, weighted_loss: 0.0155, label: 0, bag_size: 4523\n",
      "batch 79, loss: 0.0010, instance_loss: 0.1462, weighted_loss: 0.0445, label: 0, bag_size: 1891\n",
      "batch 99, loss: 0.0010, instance_loss: 0.0368, weighted_loss: 0.0117, label: 0, bag_size: 2424\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0082, weighted_loss: 0.0025, label: 1, bag_size: 14202\n",
      "batch 139, loss: 0.0239, instance_loss: 0.0016, weighted_loss: 0.0172, label: 1, bag_size: 1838\n",
      "batch 159, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 9234\n",
      "batch 179, loss: 0.0376, instance_loss: 0.1835, weighted_loss: 0.0814, label: 0, bag_size: 1760\n",
      "batch 199, loss: 0.0096, instance_loss: 0.0329, weighted_loss: 0.0166, label: 0, bag_size: 1438\n",
      "batch 219, loss: 0.0326, instance_loss: 0.0132, weighted_loss: 0.0267, label: 1, bag_size: 8438\n",
      "batch 239, loss: 0.0679, instance_loss: 0.0027, weighted_loss: 0.0483, label: 1, bag_size: 9561\n",
      "batch 259, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 11125\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 1, bag_size: 10482\n",
      "batch 299, loss: 0.7774, instance_loss: 1.2294, weighted_loss: 0.9130, label: 0, bag_size: 1714\n",
      "batch 319, loss: 0.3951, instance_loss: 0.2372, weighted_loss: 0.3477, label: 1, bag_size: 7351\n",
      "batch 339, loss: 0.0092, instance_loss: 0.0842, weighted_loss: 0.0317, label: 1, bag_size: 5907\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19472\n",
      "batch 379, loss: 0.4709, instance_loss: 0.0084, weighted_loss: 0.3321, label: 1, bag_size: 1284\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0026, weighted_loss: 0.0008, label: 1, bag_size: 18649\n",
      "batch 419, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 17633\n",
      "batch 439, loss: 0.0055, instance_loss: 0.3695, weighted_loss: 0.1147, label: 1, bag_size: 7468\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0116, weighted_loss: 0.0035, label: 1, bag_size: 15716\n",
      "batch 479, loss: 1.8305, instance_loss: 0.0839, weighted_loss: 1.3065, label: 1, bag_size: 3879\n",
      "batch 499, loss: 0.0004, instance_loss: 0.1660, weighted_loss: 0.0500, label: 1, bag_size: 8680\n",
      "batch 519, loss: 2.3144, instance_loss: 0.0834, weighted_loss: 1.6451, label: 0, bag_size: 11390\n",
      "batch 539, loss: 1.4333, instance_loss: 0.5711, weighted_loss: 1.1746, label: 1, bag_size: 11729\n",
      "batch 559, loss: 0.0083, instance_loss: 0.0461, weighted_loss: 0.0197, label: 1, bag_size: 1339\n",
      "batch 579, loss: 0.0188, instance_loss: 2.6131, weighted_loss: 0.7971, label: 1, bag_size: 7669\n",
      "batch 599, loss: 0.0002, instance_loss: 0.2618, weighted_loss: 0.0787, label: 1, bag_size: 9878\n",
      "batch 619, loss: 0.0005, instance_loss: 0.0686, weighted_loss: 0.0209, label: 1, bag_size: 2495\n",
      "batch 639, loss: 0.0034, instance_loss: 0.0205, weighted_loss: 0.0085, label: 0, bag_size: 2382\n",
      "batch 659, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 22870\n",
      "batch 679, loss: 0.0050, instance_loss: 0.0009, weighted_loss: 0.0038, label: 1, bag_size: 1022\n",
      "batch 699, loss: 0.0885, instance_loss: 0.0004, weighted_loss: 0.0621, label: 1, bag_size: 7217\n",
      "batch 719, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 14828\n",
      "batch 739, loss: 0.3521, instance_loss: 0.2496, weighted_loss: 0.3214, label: 0, bag_size: 2653\n",
      "batch 759, loss: 0.0668, instance_loss: 0.1233, weighted_loss: 0.0837, label: 1, bag_size: 5256\n",
      "batch 779, loss: 0.1220, instance_loss: 0.0023, weighted_loss: 0.0861, label: 1, bag_size: 15689\n",
      "batch 799, loss: 0.0002, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 8981\n",
      "batch 819, loss: 0.0067, instance_loss: 0.0032, weighted_loss: 0.0056, label: 1, bag_size: 5345\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9778201219512195: correct 12829/13120\n",
      "class 1 clustering acc 0.9103658536585366: correct 5972/6560\n",
      "Epoch: 77, train_loss: 0.1441, train_clustering_loss:  0.2429, train_error: 0.0524\n",
      "class 0: acc 0.9393203883495146, correct 387/412\n",
      "class 1: acc 0.9558823529411765, correct 390/408\n",
      "\n",
      "Val Set, val_loss: 0.1149, val_error: 0.0273, auc: 0.9930\n",
      "class 0 clustering acc 0.9522727272727273: correct 1676/1760\n",
      "class 1 clustering acc 0.8431818181818181: correct 742/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0109, weighted_loss: 0.0033, label: 1, bag_size: 6875\n",
      "batch 39, loss: 0.0002, instance_loss: 0.0985, weighted_loss: 0.0297, label: 1, bag_size: 10671\n",
      "batch 59, loss: 0.0104, instance_loss: 0.2966, weighted_loss: 0.0963, label: 0, bag_size: 7612\n",
      "batch 79, loss: 0.3098, instance_loss: 1.4611, weighted_loss: 0.6552, label: 0, bag_size: 15898\n",
      "batch 99, loss: 0.0034, instance_loss: 0.0003, weighted_loss: 0.0025, label: 0, bag_size: 2351\n",
      "batch 119, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 10146\n",
      "batch 139, loss: 0.3232, instance_loss: 0.2534, weighted_loss: 0.3023, label: 0, bag_size: 9597\n",
      "batch 159, loss: 0.2974, instance_loss: 0.0197, weighted_loss: 0.2141, label: 1, bag_size: 7351\n",
      "batch 179, loss: 0.0008, instance_loss: 0.0025, weighted_loss: 0.0013, label: 0, bag_size: 15001\n",
      "batch 199, loss: 0.0320, instance_loss: 0.0005, weighted_loss: 0.0226, label: 1, bag_size: 7873\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 5612\n",
      "batch 239, loss: 0.0002, instance_loss: 0.0050, weighted_loss: 0.0016, label: 0, bag_size: 7381\n",
      "batch 259, loss: 0.0204, instance_loss: 0.0019, weighted_loss: 0.0149, label: 0, bag_size: 9387\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0877, weighted_loss: 0.0264, label: 1, bag_size: 22264\n",
      "batch 299, loss: 0.0023, instance_loss: 0.0222, weighted_loss: 0.0083, label: 1, bag_size: 11032\n",
      "batch 319, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 20161\n",
      "batch 339, loss: 0.0115, instance_loss: 0.1820, weighted_loss: 0.0626, label: 1, bag_size: 7389\n",
      "batch 359, loss: 0.1373, instance_loss: 0.3894, weighted_loss: 0.2129, label: 1, bag_size: 4054\n",
      "batch 379, loss: 1.4532, instance_loss: 0.0025, weighted_loss: 1.0180, label: 0, bag_size: 5211\n",
      "batch 399, loss: 0.1167, instance_loss: 0.2907, weighted_loss: 0.1689, label: 1, bag_size: 1963\n",
      "batch 419, loss: 0.0002, instance_loss: 0.0020, weighted_loss: 0.0007, label: 1, bag_size: 13174\n",
      "batch 439, loss: 0.8504, instance_loss: 0.7140, weighted_loss: 0.8095, label: 0, bag_size: 2959\n",
      "batch 459, loss: 0.0527, instance_loss: 0.1043, weighted_loss: 0.0682, label: 0, bag_size: 3198\n",
      "batch 479, loss: 0.0003, instance_loss: 0.0571, weighted_loss: 0.0173, label: 1, bag_size: 5340\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0265, weighted_loss: 0.0080, label: 1, bag_size: 18468\n",
      "batch 519, loss: 0.1087, instance_loss: 0.0336, weighted_loss: 0.0862, label: 1, bag_size: 9404\n",
      "batch 539, loss: 0.0054, instance_loss: 0.0341, weighted_loss: 0.0140, label: 1, bag_size: 4821\n",
      "batch 559, loss: 0.0124, instance_loss: 0.0570, weighted_loss: 0.0258, label: 0, bag_size: 10365\n",
      "batch 579, loss: 0.4151, instance_loss: 0.7774, weighted_loss: 0.5238, label: 0, bag_size: 14264\n",
      "batch 599, loss: 0.0007, instance_loss: 0.0016, weighted_loss: 0.0010, label: 0, bag_size: 11259\n",
      "batch 619, loss: 0.0019, instance_loss: 0.0023, weighted_loss: 0.0020, label: 1, bag_size: 12575\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0830, weighted_loss: 0.0250, label: 1, bag_size: 4259\n",
      "batch 659, loss: 0.0007, instance_loss: 0.0002, weighted_loss: 0.0005, label: 1, bag_size: 11875\n",
      "batch 679, loss: 0.0024, instance_loss: 0.0115, weighted_loss: 0.0051, label: 0, bag_size: 8788\n",
      "batch 699, loss: 0.0016, instance_loss: 0.0795, weighted_loss: 0.0249, label: 1, bag_size: 5256\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21404\n",
      "batch 739, loss: 0.0001, instance_loss: 0.0018, weighted_loss: 0.0006, label: 1, bag_size: 8522\n",
      "batch 759, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 19466\n",
      "batch 779, loss: 0.0028, instance_loss: 0.0008, weighted_loss: 0.0022, label: 0, bag_size: 22681\n",
      "batch 799, loss: 0.8627, instance_loss: 0.4841, weighted_loss: 0.7491, label: 0, bag_size: 14264\n",
      "batch 819, loss: 0.0030, instance_loss: 0.1399, weighted_loss: 0.0441, label: 0, bag_size: 3557\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9797256097560976: correct 12854/13120\n",
      "class 1 clustering acc 0.9047256097560976: correct 5935/6560\n",
      "Epoch: 78, train_loss: 0.1124, train_clustering_loss:  0.1872, train_error: 0.0500\n",
      "class 0: acc 0.9458128078817734, correct 384/406\n",
      "class 1: acc 0.9541062801932367, correct 395/414\n",
      "\n",
      "Val Set, val_loss: 0.1097, val_error: 0.0364, auc: 0.9934\n",
      "class 0 clustering acc 0.9386363636363636: correct 1652/1760\n",
      "class 1 clustering acc 0.8022727272727272: correct 706/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0016, instance_loss: 0.0164, weighted_loss: 0.0061, label: 1, bag_size: 16267\n",
      "batch 39, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 9485\n",
      "batch 59, loss: 0.0299, instance_loss: 0.1578, weighted_loss: 0.0683, label: 0, bag_size: 1506\n",
      "batch 79, loss: 0.0006, instance_loss: 0.0109, weighted_loss: 0.0037, label: 1, bag_size: 11421\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 14377\n",
      "batch 119, loss: 0.0644, instance_loss: 0.0426, weighted_loss: 0.0578, label: 0, bag_size: 11281\n",
      "batch 139, loss: 0.2403, instance_loss: 0.0567, weighted_loss: 0.1852, label: 1, bag_size: 3879\n",
      "batch 159, loss: 0.0074, instance_loss: 0.0001, weighted_loss: 0.0052, label: 0, bag_size: 19067\n",
      "batch 179, loss: 0.0106, instance_loss: 0.0015, weighted_loss: 0.0079, label: 1, bag_size: 5629\n",
      "batch 199, loss: 0.0007, instance_loss: 0.0675, weighted_loss: 0.0208, label: 1, bag_size: 11884\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 18154\n",
      "batch 239, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 14956\n",
      "batch 259, loss: 0.0001, instance_loss: 1.1136, weighted_loss: 0.3341, label: 1, bag_size: 5256\n",
      "batch 279, loss: 0.0056, instance_loss: 0.0208, weighted_loss: 0.0102, label: 0, bag_size: 7605\n",
      "batch 299, loss: 0.0034, instance_loss: 0.0071, weighted_loss: 0.0045, label: 1, bag_size: 5317\n",
      "batch 319, loss: 0.0008, instance_loss: 0.0164, weighted_loss: 0.0055, label: 1, bag_size: 10392\n",
      "batch 339, loss: 0.0010, instance_loss: 0.1945, weighted_loss: 0.0590, label: 0, bag_size: 2063\n",
      "batch 359, loss: 0.2307, instance_loss: 2.8147, weighted_loss: 1.0059, label: 1, bag_size: 29832\n",
      "batch 379, loss: 0.0005, instance_loss: 0.1414, weighted_loss: 0.0427, label: 1, bag_size: 3453\n",
      "batch 399, loss: 0.6522, instance_loss: 0.1079, weighted_loss: 0.4889, label: 1, bag_size: 2565\n",
      "batch 419, loss: 0.1212, instance_loss: 0.0029, weighted_loss: 0.0857, label: 1, bag_size: 3674\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 9471\n",
      "batch 459, loss: 0.0006, instance_loss: 0.0924, weighted_loss: 0.0281, label: 1, bag_size: 3453\n",
      "batch 479, loss: 0.0177, instance_loss: 0.0008, weighted_loss: 0.0126, label: 1, bag_size: 4821\n",
      "batch 499, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 24911\n",
      "batch 519, loss: 0.0052, instance_loss: 1.1219, weighted_loss: 0.3402, label: 0, bag_size: 2004\n",
      "batch 539, loss: 0.0423, instance_loss: 1.0965, weighted_loss: 0.3585, label: 1, bag_size: 2137\n",
      "batch 559, loss: 0.0154, instance_loss: 0.0172, weighted_loss: 0.0159, label: 1, bag_size: 12895\n",
      "batch 579, loss: 0.0045, instance_loss: 0.2734, weighted_loss: 0.0852, label: 0, bag_size: 18738\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14956\n",
      "batch 619, loss: 0.0164, instance_loss: 0.1214, weighted_loss: 0.0479, label: 0, bag_size: 1909\n",
      "batch 639, loss: 0.0003, instance_loss: 0.0660, weighted_loss: 0.0200, label: 0, bag_size: 3970\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 18154\n",
      "batch 679, loss: 0.3056, instance_loss: 0.4796, weighted_loss: 0.3578, label: 0, bag_size: 11128\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 15233\n",
      "batch 719, loss: 0.0597, instance_loss: 0.0025, weighted_loss: 0.0426, label: 1, bag_size: 5454\n",
      "batch 739, loss: 0.7886, instance_loss: 0.0715, weighted_loss: 0.5735, label: 1, bag_size: 11729\n",
      "batch 759, loss: 0.0086, instance_loss: 0.3810, weighted_loss: 0.1203, label: 0, bag_size: 4997\n",
      "batch 779, loss: 0.0035, instance_loss: 0.0014, weighted_loss: 0.0029, label: 1, bag_size: 1015\n",
      "batch 799, loss: 0.0002, instance_loss: 0.0134, weighted_loss: 0.0042, label: 1, bag_size: 10105\n",
      "batch 819, loss: 0.0060, instance_loss: 0.0004, weighted_loss: 0.0043, label: 1, bag_size: 9230\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.979954268292683: correct 12857/13120\n",
      "class 1 clustering acc 0.894359756097561: correct 5867/6560\n",
      "Epoch: 79, train_loss: 0.1061, train_clustering_loss:  0.1891, train_error: 0.0439\n",
      "class 0: acc 0.9465648854961832, correct 372/393\n",
      "class 1: acc 0.9648711943793911, correct 412/427\n",
      "\n",
      "Val Set, val_loss: 0.3366, val_error: 0.1545, auc: 0.9917\n",
      "class 0 clustering acc 0.9380681818181819: correct 1651/1760\n",
      "class 1 clustering acc 0.8170454545454545: correct 719/880\n",
      "class 0: acc 0.6730769230769231, correct 35/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.8103, instance_loss: 0.6092, weighted_loss: 0.7500, label: 0, bag_size: 1909\n",
      "batch 39, loss: 0.0004, instance_loss: 0.3863, weighted_loss: 0.1161, label: 0, bag_size: 3101\n",
      "batch 59, loss: 0.0000, instance_loss: 0.4289, weighted_loss: 0.1287, label: 0, bag_size: 2244\n",
      "batch 79, loss: 0.0051, instance_loss: 0.0045, weighted_loss: 0.0049, label: 1, bag_size: 7768\n",
      "batch 99, loss: 0.4756, instance_loss: 0.3551, weighted_loss: 0.4395, label: 1, bag_size: 898\n",
      "batch 119, loss: 0.0688, instance_loss: 0.0898, weighted_loss: 0.0751, label: 1, bag_size: 1022\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 13591\n",
      "batch 159, loss: 0.0029, instance_loss: 0.0000, weighted_loss: 0.0021, label: 0, bag_size: 12593\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 4465\n",
      "batch 199, loss: 0.0061, instance_loss: 0.0169, weighted_loss: 0.0094, label: 1, bag_size: 1493\n",
      "batch 219, loss: 0.0010, instance_loss: 0.0003, weighted_loss: 0.0008, label: 0, bag_size: 12148\n",
      "batch 239, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 5551\n",
      "batch 259, loss: 0.0247, instance_loss: 0.0378, weighted_loss: 0.0286, label: 1, bag_size: 13015\n",
      "batch 279, loss: 0.0013, instance_loss: 0.0304, weighted_loss: 0.0100, label: 0, bag_size: 1508\n",
      "batch 299, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 11259\n",
      "batch 319, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 19518\n",
      "batch 339, loss: 0.1126, instance_loss: 0.0274, weighted_loss: 0.0870, label: 0, bag_size: 3670\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 1, bag_size: 5612\n",
      "batch 379, loss: 0.0035, instance_loss: 0.0307, weighted_loss: 0.0116, label: 0, bag_size: 1483\n",
      "batch 399, loss: 0.0005, instance_loss: 0.0161, weighted_loss: 0.0052, label: 1, bag_size: 3640\n",
      "batch 419, loss: 1.0263, instance_loss: 2.8219, weighted_loss: 1.5650, label: 1, bag_size: 1845\n",
      "batch 439, loss: 0.0016, instance_loss: 0.0059, weighted_loss: 0.0029, label: 0, bag_size: 20796\n",
      "batch 459, loss: 0.0192, instance_loss: 0.2578, weighted_loss: 0.0908, label: 0, bag_size: 10898\n",
      "batch 479, loss: 0.0008, instance_loss: 0.1657, weighted_loss: 0.0502, label: 1, bag_size: 6453\n",
      "batch 499, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 9542\n",
      "batch 519, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 2457\n",
      "batch 539, loss: 0.0194, instance_loss: 0.0018, weighted_loss: 0.0141, label: 1, bag_size: 5629\n",
      "batch 559, loss: 0.0005, instance_loss: 1.2269, weighted_loss: 0.3684, label: 0, bag_size: 2814\n",
      "batch 579, loss: 0.1364, instance_loss: 0.2022, weighted_loss: 0.1562, label: 1, bag_size: 5907\n",
      "batch 599, loss: 2.0893, instance_loss: 5.2558, weighted_loss: 3.0392, label: 0, bag_size: 1714\n",
      "batch 619, loss: 0.0005, instance_loss: 0.1170, weighted_loss: 0.0355, label: 1, bag_size: 12178\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23714\n",
      "batch 659, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 26271\n",
      "batch 679, loss: 0.0134, instance_loss: 0.3226, weighted_loss: 0.1061, label: 0, bag_size: 763\n",
      "batch 699, loss: 0.0052, instance_loss: 1.7298, weighted_loss: 0.5226, label: 0, bag_size: 2004\n",
      "batch 719, loss: 0.3914, instance_loss: 0.0519, weighted_loss: 0.2895, label: 1, bag_size: 1284\n",
      "batch 739, loss: 0.0002, instance_loss: 0.0010, weighted_loss: 0.0004, label: 0, bag_size: 3228\n",
      "batch 759, loss: 0.0351, instance_loss: 0.2062, weighted_loss: 0.0865, label: 0, bag_size: 1760\n",
      "batch 779, loss: 0.0000, instance_loss: 0.0624, weighted_loss: 0.0187, label: 0, bag_size: 3459\n",
      "batch 799, loss: 0.0193, instance_loss: 0.0000, weighted_loss: 0.0135, label: 0, bag_size: 47866\n",
      "batch 819, loss: 0.2851, instance_loss: 0.0401, weighted_loss: 0.2116, label: 0, bag_size: 14264\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9824695121951219: correct 12890/13120\n",
      "class 1 clustering acc 0.9149390243902439: correct 6002/6560\n",
      "Epoch: 80, train_loss: 0.1097, train_clustering_loss:  0.1720, train_error: 0.0402\n",
      "class 0: acc 0.9566265060240964, correct 397/415\n",
      "class 1: acc 0.9629629629629629, correct 390/405\n",
      "\n",
      "Val Set, val_loss: 0.1049, val_error: 0.0273, auc: 0.9940\n",
      "class 0 clustering acc 0.9409090909090909: correct 1656/1760\n",
      "class 1 clustering acc 0.8227272727272728: correct 724/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0894, instance_loss: 0.0103, weighted_loss: 0.0657, label: 0, bag_size: 1760\n",
      "batch 39, loss: 2.1047, instance_loss: 2.7413, weighted_loss: 2.2957, label: 1, bag_size: 6360\n",
      "batch 59, loss: 0.0002, instance_loss: 0.0484, weighted_loss: 0.0146, label: 1, bag_size: 1255\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0025, weighted_loss: 0.0008, label: 1, bag_size: 19932\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0028, weighted_loss: 0.0009, label: 1, bag_size: 12931\n",
      "batch 119, loss: 0.0176, instance_loss: 0.4882, weighted_loss: 0.1588, label: 1, bag_size: 5160\n",
      "batch 139, loss: 0.0007, instance_loss: 0.0033, weighted_loss: 0.0015, label: 0, bag_size: 9234\n",
      "batch 159, loss: 0.0145, instance_loss: 0.1718, weighted_loss: 0.0617, label: 0, bag_size: 2652\n",
      "batch 179, loss: 0.0110, instance_loss: 0.0323, weighted_loss: 0.0174, label: 1, bag_size: 7513\n",
      "batch 199, loss: 0.0013, instance_loss: 0.0333, weighted_loss: 0.0109, label: 1, bag_size: 2695\n",
      "batch 219, loss: 1.2444, instance_loss: 0.3955, weighted_loss: 0.9898, label: 1, bag_size: 5292\n",
      "batch 239, loss: 0.0004, instance_loss: 0.0482, weighted_loss: 0.0148, label: 1, bag_size: 689\n",
      "batch 259, loss: 0.0012, instance_loss: 0.0001, weighted_loss: 0.0009, label: 0, bag_size: 8025\n",
      "batch 279, loss: 0.0006, instance_loss: 0.0001, weighted_loss: 0.0005, label: 0, bag_size: 20796\n",
      "batch 299, loss: 0.1842, instance_loss: 0.0384, weighted_loss: 0.1405, label: 1, bag_size: 8592\n",
      "batch 319, loss: 0.0010, instance_loss: 0.0017, weighted_loss: 0.0013, label: 0, bag_size: 9596\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0033, weighted_loss: 0.0010, label: 1, bag_size: 13947\n",
      "batch 359, loss: 0.0058, instance_loss: 0.5410, weighted_loss: 0.1664, label: 0, bag_size: 1560\n",
      "batch 379, loss: 0.0178, instance_loss: 0.2970, weighted_loss: 0.1016, label: 0, bag_size: 2004\n",
      "batch 399, loss: 0.0986, instance_loss: 1.2362, weighted_loss: 0.4399, label: 1, bag_size: 6726\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 18154\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 9851\n",
      "batch 459, loss: 0.0630, instance_loss: 0.0391, weighted_loss: 0.0558, label: 1, bag_size: 7468\n",
      "batch 479, loss: 0.3401, instance_loss: 0.0005, weighted_loss: 0.2382, label: 1, bag_size: 10432\n",
      "batch 499, loss: 0.0024, instance_loss: 0.0000, weighted_loss: 0.0017, label: 0, bag_size: 21138\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0097, weighted_loss: 0.0029, label: 1, bag_size: 9971\n",
      "batch 539, loss: 0.0092, instance_loss: 0.0021, weighted_loss: 0.0070, label: 1, bag_size: 4250\n",
      "batch 559, loss: 0.0004, instance_loss: 0.0004, weighted_loss: 0.0004, label: 0, bag_size: 10444\n",
      "batch 579, loss: 0.0019, instance_loss: 0.0000, weighted_loss: 0.0013, label: 0, bag_size: 21138\n",
      "batch 599, loss: 0.0052, instance_loss: 0.0000, weighted_loss: 0.0036, label: 0, bag_size: 15003\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0137, weighted_loss: 0.0041, label: 1, bag_size: 15233\n",
      "batch 639, loss: 0.0179, instance_loss: 0.1296, weighted_loss: 0.0514, label: 0, bag_size: 13332\n",
      "batch 659, loss: 0.1679, instance_loss: 0.3607, weighted_loss: 0.2257, label: 0, bag_size: 3375\n",
      "batch 679, loss: 0.0003, instance_loss: 0.0225, weighted_loss: 0.0070, label: 1, bag_size: 10482\n",
      "batch 699, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 15967\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0758, weighted_loss: 0.0227, label: 1, bag_size: 9732\n",
      "batch 739, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11383\n",
      "batch 759, loss: 0.0884, instance_loss: 0.6917, weighted_loss: 0.2694, label: 0, bag_size: 1592\n",
      "batch 779, loss: 1.1371, instance_loss: 0.0000, weighted_loss: 0.7960, label: 0, bag_size: 3468\n",
      "batch 799, loss: 0.0016, instance_loss: 0.5595, weighted_loss: 0.1690, label: 0, bag_size: 1909\n",
      "batch 819, loss: 0.1462, instance_loss: 0.0053, weighted_loss: 0.1039, label: 1, bag_size: 8592\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9825457317073171: correct 12891/13120\n",
      "class 1 clustering acc 0.9167682926829268: correct 6014/6560\n",
      "Epoch: 81, train_loss: 0.1321, train_clustering_loss:  0.1618, train_error: 0.0622\n",
      "class 0: acc 0.9334916864608076, correct 393/421\n",
      "class 1: acc 0.9423558897243107, correct 376/399\n",
      "\n",
      "Val Set, val_loss: 0.2183, val_error: 0.0909, auc: 0.9924\n",
      "class 0 clustering acc 0.9460227272727273: correct 1665/1760\n",
      "class 1 clustering acc 0.8363636363636363: correct 736/880\n",
      "class 0: acc 0.8076923076923077, correct 42/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 12 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 23796\n",
      "batch 39, loss: 0.0059, instance_loss: 0.0205, weighted_loss: 0.0102, label: 0, bag_size: 2457\n",
      "batch 59, loss: 0.1065, instance_loss: 0.0000, weighted_loss: 0.0746, label: 0, bag_size: 12840\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14377\n",
      "batch 99, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 4902\n",
      "batch 119, loss: 0.0156, instance_loss: 0.0005, weighted_loss: 0.0111, label: 1, bag_size: 16565\n",
      "batch 139, loss: 0.0549, instance_loss: 1.6780, weighted_loss: 0.5418, label: 0, bag_size: 931\n",
      "batch 159, loss: 0.0012, instance_loss: 0.0661, weighted_loss: 0.0207, label: 1, bag_size: 7515\n",
      "batch 179, loss: 0.2047, instance_loss: 1.2009, weighted_loss: 0.5036, label: 0, bag_size: 15898\n",
      "batch 199, loss: 0.0002, instance_loss: 0.0247, weighted_loss: 0.0075, label: 1, bag_size: 12611\n",
      "batch 219, loss: 0.0019, instance_loss: 0.0037, weighted_loss: 0.0024, label: 1, bag_size: 8475\n",
      "batch 239, loss: 0.0071, instance_loss: 0.0144, weighted_loss: 0.0093, label: 1, bag_size: 5494\n",
      "batch 259, loss: 0.0022, instance_loss: 0.0084, weighted_loss: 0.0041, label: 1, bag_size: 30675\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11727\n",
      "batch 299, loss: 0.0557, instance_loss: 0.0261, weighted_loss: 0.0468, label: 1, bag_size: 1831\n",
      "batch 319, loss: 0.0019, instance_loss: 0.0244, weighted_loss: 0.0086, label: 1, bag_size: 1638\n",
      "batch 339, loss: 0.0000, instance_loss: 0.1315, weighted_loss: 0.0395, label: 1, bag_size: 10112\n",
      "batch 359, loss: 0.0008, instance_loss: 0.0027, weighted_loss: 0.0014, label: 0, bag_size: 8755\n",
      "batch 379, loss: 0.0003, instance_loss: 0.0024, weighted_loss: 0.0010, label: 1, bag_size: 621\n",
      "batch 399, loss: 0.0277, instance_loss: 0.0106, weighted_loss: 0.0226, label: 0, bag_size: 7557\n",
      "batch 419, loss: 0.0028, instance_loss: 0.0000, weighted_loss: 0.0020, label: 1, bag_size: 9877\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15077\n",
      "batch 459, loss: 0.1096, instance_loss: 0.1060, weighted_loss: 0.1085, label: 1, bag_size: 16548\n",
      "batch 479, loss: 0.0099, instance_loss: 0.0003, weighted_loss: 0.0070, label: 0, bag_size: 9415\n",
      "batch 499, loss: 0.0165, instance_loss: 0.0570, weighted_loss: 0.0287, label: 1, bag_size: 8466\n",
      "batch 519, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 3228\n",
      "batch 539, loss: 0.0148, instance_loss: 0.2406, weighted_loss: 0.0826, label: 0, bag_size: 2290\n",
      "batch 559, loss: 0.0050, instance_loss: 0.0379, weighted_loss: 0.0149, label: 1, bag_size: 8754\n",
      "batch 579, loss: 0.0083, instance_loss: 0.0296, weighted_loss: 0.0147, label: 1, bag_size: 7119\n",
      "batch 599, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 2036\n",
      "batch 619, loss: 0.0029, instance_loss: 0.0000, weighted_loss: 0.0020, label: 0, bag_size: 1962\n",
      "batch 639, loss: 0.0036, instance_loss: 0.0042, weighted_loss: 0.0038, label: 1, bag_size: 12895\n",
      "batch 659, loss: 0.0242, instance_loss: 0.1178, weighted_loss: 0.0523, label: 0, bag_size: 1416\n",
      "batch 679, loss: 0.0165, instance_loss: 0.2433, weighted_loss: 0.0845, label: 0, bag_size: 1370\n",
      "batch 699, loss: 0.4349, instance_loss: 0.5467, weighted_loss: 0.4684, label: 0, bag_size: 13619\n",
      "batch 719, loss: 0.1010, instance_loss: 0.0346, weighted_loss: 0.0811, label: 1, bag_size: 7989\n",
      "batch 739, loss: 0.0003, instance_loss: 0.0046, weighted_loss: 0.0016, label: 0, bag_size: 5409\n",
      "batch 759, loss: 0.0212, instance_loss: 0.0354, weighted_loss: 0.0254, label: 1, bag_size: 18603\n",
      "batch 779, loss: 0.1336, instance_loss: 0.4333, weighted_loss: 0.2235, label: 0, bag_size: 2918\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11727\n",
      "batch 819, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 7381\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9856707317073171: correct 12932/13120\n",
      "class 1 clustering acc 0.9338414634146341: correct 6126/6560\n",
      "Epoch: 82, train_loss: 0.0884, train_clustering_loss:  0.1331, train_error: 0.0280\n",
      "class 0: acc 0.9706601466992665, correct 397/409\n",
      "class 1: acc 0.9732360097323601, correct 400/411\n",
      "\n",
      "Val Set, val_loss: 0.1653, val_error: 0.0545, auc: 0.9924\n",
      "class 0 clustering acc 0.9386363636363636: correct 1652/1760\n",
      "class 1 clustering acc 0.8454545454545455: correct 744/880\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 13 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0010, instance_loss: 0.0020, weighted_loss: 0.0013, label: 1, bag_size: 8003\n",
      "batch 39, loss: 0.0025, instance_loss: 0.0000, weighted_loss: 0.0018, label: 0, bag_size: 13880\n",
      "batch 59, loss: 0.0121, instance_loss: 0.8096, weighted_loss: 0.2513, label: 0, bag_size: 2043\n",
      "batch 79, loss: 0.0267, instance_loss: 2.7215, weighted_loss: 0.8351, label: 0, bag_size: 47866\n",
      "batch 99, loss: 0.0000, instance_loss: 0.2191, weighted_loss: 0.0657, label: 0, bag_size: 10146\n",
      "batch 119, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 2424\n",
      "batch 139, loss: 0.0115, instance_loss: 0.0836, weighted_loss: 0.0331, label: 0, bag_size: 9888\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14956\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 23796\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0138, weighted_loss: 0.0042, label: 1, bag_size: 699\n",
      "batch 219, loss: 1.7239, instance_loss: 0.0003, weighted_loss: 1.2068, label: 0, bag_size: 5120\n",
      "batch 239, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 10751\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 16052\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 17486\n",
      "batch 299, loss: 0.0259, instance_loss: 0.0004, weighted_loss: 0.0183, label: 0, bag_size: 7557\n",
      "batch 319, loss: 0.0241, instance_loss: 0.0000, weighted_loss: 0.0169, label: 0, bag_size: 19067\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0135, weighted_loss: 0.0041, label: 1, bag_size: 19932\n",
      "batch 359, loss: 0.0238, instance_loss: 0.0247, weighted_loss: 0.0240, label: 1, bag_size: 5454\n",
      "batch 379, loss: 0.0007, instance_loss: 0.0089, weighted_loss: 0.0032, label: 1, bag_size: 5894\n",
      "batch 399, loss: 0.0002, instance_loss: 0.0041, weighted_loss: 0.0014, label: 1, bag_size: 12611\n",
      "batch 419, loss: 0.0071, instance_loss: 0.0000, weighted_loss: 0.0049, label: 0, bag_size: 13892\n",
      "batch 439, loss: 0.0328, instance_loss: 0.0240, weighted_loss: 0.0302, label: 1, bag_size: 2814\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 13591\n",
      "batch 479, loss: 0.0034, instance_loss: 0.0000, weighted_loss: 0.0024, label: 0, bag_size: 4902\n",
      "batch 499, loss: 0.0110, instance_loss: 0.0026, weighted_loss: 0.0085, label: 1, bag_size: 11032\n",
      "batch 519, loss: 0.8331, instance_loss: 0.0000, weighted_loss: 0.5831, label: 0, bag_size: 5211\n",
      "batch 539, loss: 0.0044, instance_loss: 0.0036, weighted_loss: 0.0041, label: 0, bag_size: 763\n",
      "batch 559, loss: 0.0062, instance_loss: 0.0206, weighted_loss: 0.0105, label: 1, bag_size: 13692\n",
      "batch 579, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 13964\n",
      "batch 599, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 1483\n",
      "batch 619, loss: 0.0033, instance_loss: 0.0068, weighted_loss: 0.0043, label: 1, bag_size: 645\n",
      "batch 639, loss: 0.0031, instance_loss: 0.4094, weighted_loss: 0.1250, label: 0, bag_size: 2367\n",
      "batch 659, loss: 0.0597, instance_loss: 1.1424, weighted_loss: 0.3845, label: 1, bag_size: 3968\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0008, weighted_loss: 0.0003, label: 1, bag_size: 11389\n",
      "batch 699, loss: 0.1735, instance_loss: 0.0349, weighted_loss: 0.1320, label: 0, bag_size: 9616\n",
      "batch 719, loss: 0.0028, instance_loss: 0.0000, weighted_loss: 0.0020, label: 0, bag_size: 2998\n",
      "batch 739, loss: 0.0002, instance_loss: 0.0449, weighted_loss: 0.0136, label: 0, bag_size: 15747\n",
      "batch 759, loss: 0.0001, instance_loss: 0.0747, weighted_loss: 0.0224, label: 0, bag_size: 1712\n",
      "batch 779, loss: 0.0013, instance_loss: 1.2250, weighted_loss: 0.3684, label: 1, bag_size: 2785\n",
      "batch 799, loss: 0.1172, instance_loss: 0.2128, weighted_loss: 0.1459, label: 1, bag_size: 1963\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 32227\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9826981707317073: correct 12893/13120\n",
      "class 1 clustering acc 0.9338414634146341: correct 6126/6560\n",
      "Epoch: 83, train_loss: 0.1164, train_clustering_loss:  0.1622, train_error: 0.0488\n",
      "class 0: acc 0.9516129032258065, correct 413/434\n",
      "class 1: acc 0.9507772020725389, correct 367/386\n",
      "\n",
      "Val Set, val_loss: 0.0992, val_error: 0.0182, auc: 0.9947\n",
      "class 0 clustering acc 0.9545454545454546: correct 1680/1760\n",
      "class 1 clustering acc 0.8011363636363636: correct 705/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 14 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.0041, weighted_loss: 0.0013, label: 1, bag_size: 16417\n",
      "batch 39, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 10995\n",
      "batch 59, loss: 0.0214, instance_loss: 0.0019, weighted_loss: 0.0155, label: 0, bag_size: 18516\n",
      "batch 79, loss: 0.0059, instance_loss: 0.0000, weighted_loss: 0.0041, label: 0, bag_size: 11122\n",
      "batch 99, loss: 0.0182, instance_loss: 0.0015, weighted_loss: 0.0131, label: 0, bag_size: 2043\n",
      "batch 119, loss: 0.0049, instance_loss: 0.0995, weighted_loss: 0.0333, label: 0, bag_size: 1458\n",
      "batch 139, loss: 0.0002, instance_loss: 0.0012, weighted_loss: 0.0005, label: 0, bag_size: 16992\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0521, weighted_loss: 0.0156, label: 1, bag_size: 7650\n",
      "batch 179, loss: 0.0038, instance_loss: 0.0057, weighted_loss: 0.0044, label: 0, bag_size: 3557\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0506, weighted_loss: 0.0153, label: 0, bag_size: 4523\n",
      "batch 219, loss: 0.0008, instance_loss: 0.0245, weighted_loss: 0.0079, label: 0, bag_size: 3101\n",
      "batch 239, loss: 0.0043, instance_loss: 0.0717, weighted_loss: 0.0246, label: 0, bag_size: 11607\n",
      "batch 259, loss: 0.0099, instance_loss: 0.0109, weighted_loss: 0.0102, label: 1, bag_size: 3211\n",
      "batch 279, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 9234\n",
      "batch 299, loss: 0.0008, instance_loss: 0.0012, weighted_loss: 0.0009, label: 1, bag_size: 8602\n",
      "batch 319, loss: 0.0013, instance_loss: 0.0005, weighted_loss: 0.0011, label: 1, bag_size: 3640\n",
      "batch 339, loss: 0.0035, instance_loss: 0.0000, weighted_loss: 0.0025, label: 0, bag_size: 3893\n",
      "batch 359, loss: 0.0219, instance_loss: 0.0001, weighted_loss: 0.0153, label: 1, bag_size: 21701\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 1, bag_size: 12931\n",
      "batch 399, loss: 0.0001, instance_loss: 0.0051, weighted_loss: 0.0016, label: 1, bag_size: 10482\n",
      "batch 419, loss: 0.0002, instance_loss: 0.0002, weighted_loss: 0.0002, label: 1, bag_size: 4862\n",
      "batch 439, loss: 0.0563, instance_loss: 0.0041, weighted_loss: 0.0406, label: 0, bag_size: 13619\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0024, weighted_loss: 0.0007, label: 1, bag_size: 6317\n",
      "batch 479, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 19466\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0108, weighted_loss: 0.0033, label: 1, bag_size: 11518\n",
      "batch 519, loss: 0.0051, instance_loss: 0.0051, weighted_loss: 0.0051, label: 0, bag_size: 9596\n",
      "batch 539, loss: 0.0029, instance_loss: 0.0000, weighted_loss: 0.0021, label: 0, bag_size: 19470\n",
      "batch 559, loss: 0.0003, instance_loss: 0.0111, weighted_loss: 0.0036, label: 0, bag_size: 4465\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0589, weighted_loss: 0.0177, label: 0, bag_size: 2351\n",
      "batch 599, loss: 0.0144, instance_loss: 0.0822, weighted_loss: 0.0348, label: 1, bag_size: 16514\n",
      "batch 619, loss: 0.0021, instance_loss: 0.2240, weighted_loss: 0.0687, label: 1, bag_size: 11394\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0048, weighted_loss: 0.0014, label: 1, bag_size: 1412\n",
      "batch 659, loss: 0.0192, instance_loss: 0.0713, weighted_loss: 0.0348, label: 1, bag_size: 11386\n",
      "batch 679, loss: 0.3492, instance_loss: 0.0000, weighted_loss: 0.2444, label: 0, bag_size: 3468\n",
      "batch 699, loss: 0.0824, instance_loss: 0.2180, weighted_loss: 0.1231, label: 0, bag_size: 7835\n",
      "batch 719, loss: 0.0241, instance_loss: 0.0186, weighted_loss: 0.0225, label: 0, bag_size: 9616\n",
      "batch 739, loss: 0.0004, instance_loss: 0.0024, weighted_loss: 0.0010, label: 0, bag_size: 2351\n",
      "batch 759, loss: 0.3504, instance_loss: 0.1305, weighted_loss: 0.2844, label: 0, bag_size: 2918\n",
      "batch 779, loss: 0.0061, instance_loss: 0.0672, weighted_loss: 0.0244, label: 0, bag_size: 21864\n",
      "batch 799, loss: 0.0040, instance_loss: 0.2062, weighted_loss: 0.0646, label: 1, bag_size: 18603\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 9147\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9850609756097561: correct 12924/13120\n",
      "class 1 clustering acc 0.932469512195122: correct 6117/6560\n",
      "Epoch: 84, train_loss: 0.1150, train_clustering_loss:  0.1507, train_error: 0.0451\n",
      "class 0: acc 0.95, correct 399/420\n",
      "class 1: acc 0.96, correct 384/400\n",
      "\n",
      "Val Set, val_loss: 0.1255, val_error: 0.0364, auc: 0.9924\n",
      "class 0 clustering acc 0.9579545454545455: correct 1686/1760\n",
      "class 1 clustering acc 0.8647727272727272: correct 761/880\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 15 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.2283, weighted_loss: 0.0686, label: 1, bag_size: 3453\n",
      "batch 39, loss: 0.0005, instance_loss: 0.0131, weighted_loss: 0.0043, label: 0, bag_size: 2351\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0703, weighted_loss: 0.0211, label: 1, bag_size: 10112\n",
      "batch 79, loss: 0.0622, instance_loss: 0.8820, weighted_loss: 0.3081, label: 1, bag_size: 898\n",
      "batch 99, loss: 0.0000, instance_loss: 0.9495, weighted_loss: 0.2849, label: 0, bag_size: 518\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0161, weighted_loss: 0.0049, label: 0, bag_size: 1588\n",
      "batch 139, loss: 0.9953, instance_loss: 0.0005, weighted_loss: 0.6969, label: 0, bag_size: 5211\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0283, weighted_loss: 0.0085, label: 0, bag_size: 9485\n",
      "batch 179, loss: 0.8515, instance_loss: 0.3793, weighted_loss: 0.7099, label: 0, bag_size: 1732\n",
      "batch 199, loss: 0.0002, instance_loss: 0.0009, weighted_loss: 0.0005, label: 1, bag_size: 5894\n",
      "batch 219, loss: 0.0327, instance_loss: 0.0002, weighted_loss: 0.0230, label: 1, bag_size: 21827\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0058, weighted_loss: 0.0018, label: 1, bag_size: 12931\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 11735\n",
      "batch 279, loss: 0.1555, instance_loss: 0.0710, weighted_loss: 0.1301, label: 1, bag_size: 10848\n",
      "batch 299, loss: 0.0156, instance_loss: 0.4328, weighted_loss: 0.1408, label: 1, bag_size: 1230\n",
      "batch 319, loss: 0.0000, instance_loss: 0.3336, weighted_loss: 0.1001, label: 0, bag_size: 518\n",
      "batch 339, loss: 0.0031, instance_loss: 0.0684, weighted_loss: 0.0227, label: 1, bag_size: 8216\n",
      "batch 359, loss: 0.0009, instance_loss: 0.0735, weighted_loss: 0.0227, label: 0, bag_size: 14249\n",
      "batch 379, loss: 0.0005, instance_loss: 0.0083, weighted_loss: 0.0028, label: 1, bag_size: 10671\n",
      "batch 399, loss: 0.0077, instance_loss: 0.0089, weighted_loss: 0.0080, label: 1, bag_size: 1015\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0016, weighted_loss: 0.0005, label: 0, bag_size: 11735\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0002, weighted_loss: 0.0002, label: 0, bag_size: 2179\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 17486\n",
      "batch 479, loss: 0.0009, instance_loss: 0.0036, weighted_loss: 0.0017, label: 1, bag_size: 9062\n",
      "batch 499, loss: 0.0586, instance_loss: 0.0010, weighted_loss: 0.0413, label: 1, bag_size: 5903\n",
      "batch 519, loss: 0.0000, instance_loss: 0.1183, weighted_loss: 0.0355, label: 1, bag_size: 7767\n",
      "batch 539, loss: 0.0174, instance_loss: 0.0001, weighted_loss: 0.0122, label: 0, bag_size: 13880\n",
      "batch 559, loss: 0.0009, instance_loss: 0.0756, weighted_loss: 0.0233, label: 0, bag_size: 2654\n",
      "batch 579, loss: 0.0181, instance_loss: 0.0024, weighted_loss: 0.0134, label: 1, bag_size: 5137\n",
      "batch 599, loss: 0.0084, instance_loss: 0.0138, weighted_loss: 0.0100, label: 0, bag_size: 1415\n",
      "batch 619, loss: 1.0247, instance_loss: 0.0355, weighted_loss: 0.7279, label: 1, bag_size: 1284\n",
      "batch 639, loss: 0.0002, instance_loss: 0.1350, weighted_loss: 0.0406, label: 1, bag_size: 865\n",
      "batch 659, loss: 0.0020, instance_loss: 0.0000, weighted_loss: 0.0014, label: 0, bag_size: 9455\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0036, weighted_loss: 0.0011, label: 1, bag_size: 11642\n",
      "batch 699, loss: 0.0578, instance_loss: 0.0131, weighted_loss: 0.0444, label: 1, bag_size: 2522\n",
      "batch 719, loss: 0.0019, instance_loss: 0.0014, weighted_loss: 0.0017, label: 1, bag_size: 2344\n",
      "batch 739, loss: 0.0023, instance_loss: 0.0057, weighted_loss: 0.0033, label: 1, bag_size: 21701\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 7191\n",
      "batch 779, loss: 0.0198, instance_loss: 0.0100, weighted_loss: 0.0168, label: 0, bag_size: 7557\n",
      "batch 799, loss: 0.0018, instance_loss: 0.1042, weighted_loss: 0.0325, label: 0, bag_size: 2004\n",
      "batch 819, loss: 0.0061, instance_loss: 2.9487, weighted_loss: 0.8888, label: 0, bag_size: 2367\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9852896341463414: correct 12927/13120\n",
      "class 1 clustering acc 0.9379573170731708: correct 6153/6560\n",
      "Epoch: 85, train_loss: 0.0939, train_clustering_loss:  0.1375, train_error: 0.0341\n",
      "class 0: acc 0.9698492462311558, correct 386/398\n",
      "class 1: acc 0.9620853080568721, correct 406/422\n",
      "\n",
      "Val Set, val_loss: 0.1635, val_error: 0.0455, auc: 0.9920\n",
      "class 0 clustering acc 0.9534090909090909: correct 1678/1760\n",
      "class 1 clustering acc 0.8568181818181818: correct 754/880\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 16 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 2.1153, instance_loss: 0.9143, weighted_loss: 1.7550, label: 1, bag_size: 1497\n",
      "batch 39, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 7011\n",
      "batch 59, loss: 0.0820, instance_loss: 0.0016, weighted_loss: 0.0578, label: 1, bag_size: 10432\n",
      "batch 79, loss: 0.5468, instance_loss: 0.0386, weighted_loss: 0.3944, label: 0, bag_size: 11390\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23037\n",
      "batch 119, loss: 0.1041, instance_loss: 0.0018, weighted_loss: 0.0734, label: 0, bag_size: 24382\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8948\n",
      "batch 159, loss: 0.3541, instance_loss: 0.0106, weighted_loss: 0.2511, label: 0, bag_size: 1498\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0020, weighted_loss: 0.0006, label: 1, bag_size: 4862\n",
      "batch 199, loss: 0.0843, instance_loss: 0.0034, weighted_loss: 0.0600, label: 1, bag_size: 14887\n",
      "batch 219, loss: 0.0239, instance_loss: 0.0042, weighted_loss: 0.0180, label: 1, bag_size: 10460\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0061, weighted_loss: 0.0019, label: 0, bag_size: 2920\n",
      "batch 259, loss: 0.3723, instance_loss: 0.0180, weighted_loss: 0.2660, label: 0, bag_size: 11212\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 5833\n",
      "batch 299, loss: 0.0001, instance_loss: 0.3881, weighted_loss: 0.1165, label: 0, bag_size: 19808\n",
      "batch 319, loss: 0.0019, instance_loss: 0.0271, weighted_loss: 0.0094, label: 1, bag_size: 1123\n",
      "batch 339, loss: 0.0004, instance_loss: 0.0730, weighted_loss: 0.0222, label: 1, bag_size: 4308\n",
      "batch 359, loss: 0.0036, instance_loss: 0.0155, weighted_loss: 0.0072, label: 0, bag_size: 18215\n",
      "batch 379, loss: 0.0001, instance_loss: 0.0277, weighted_loss: 0.0084, label: 0, bag_size: 1588\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 16341\n",
      "batch 419, loss: 0.0007, instance_loss: 0.0029, weighted_loss: 0.0014, label: 1, bag_size: 10492\n",
      "batch 439, loss: 0.0315, instance_loss: 0.0000, weighted_loss: 0.0221, label: 0, bag_size: 12510\n",
      "batch 459, loss: 0.0002, instance_loss: 0.0021, weighted_loss: 0.0008, label: 1, bag_size: 12178\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0406, weighted_loss: 0.0122, label: 1, bag_size: 2278\n",
      "batch 499, loss: 0.0620, instance_loss: 0.0138, weighted_loss: 0.0475, label: 1, bag_size: 1493\n",
      "batch 519, loss: 0.0005, instance_loss: 0.0710, weighted_loss: 0.0216, label: 0, bag_size: 705\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 10898\n",
      "batch 559, loss: 0.0277, instance_loss: 0.0135, weighted_loss: 0.0234, label: 1, bag_size: 11394\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15841\n",
      "batch 599, loss: 0.0031, instance_loss: 0.0000, weighted_loss: 0.0022, label: 0, bag_size: 8025\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 3459\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0114, weighted_loss: 0.0035, label: 1, bag_size: 1339\n",
      "batch 659, loss: 0.2518, instance_loss: 1.5828, weighted_loss: 0.6511, label: 0, bag_size: 25420\n",
      "batch 679, loss: 0.0030, instance_loss: 0.0000, weighted_loss: 0.0021, label: 0, bag_size: 8025\n",
      "batch 699, loss: 0.0027, instance_loss: 0.2587, weighted_loss: 0.0795, label: 0, bag_size: 2628\n",
      "batch 719, loss: 0.1625, instance_loss: 2.0118, weighted_loss: 0.7173, label: 1, bag_size: 3968\n",
      "batch 739, loss: 0.0062, instance_loss: 0.0986, weighted_loss: 0.0339, label: 1, bag_size: 1759\n",
      "batch 759, loss: 0.0006, instance_loss: 0.0015, weighted_loss: 0.0009, label: 0, bag_size: 10751\n",
      "batch 779, loss: 0.0967, instance_loss: 0.0027, weighted_loss: 0.0685, label: 0, bag_size: 12510\n",
      "batch 799, loss: 0.0267, instance_loss: 0.4057, weighted_loss: 0.1404, label: 1, bag_size: 8982\n",
      "batch 819, loss: 0.0032, instance_loss: 0.3516, weighted_loss: 0.1077, label: 1, bag_size: 25695\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9836890243902439: correct 12906/13120\n",
      "class 1 clustering acc 0.927439024390244: correct 6084/6560\n",
      "Epoch: 86, train_loss: 0.1384, train_clustering_loss:  0.1515, train_error: 0.0537\n",
      "class 0: acc 0.948905109489051, correct 390/411\n",
      "class 1: acc 0.9437652811735942, correct 386/409\n",
      "\n",
      "Val Set, val_loss: 0.2020, val_error: 0.0909, auc: 0.9930\n",
      "class 0 clustering acc 0.9522727272727273: correct 1676/1760\n",
      "class 1 clustering acc 0.825: correct 726/880\n",
      "class 0: acc 0.8076923076923077, correct 42/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 17 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0011, instance_loss: 0.0487, weighted_loss: 0.0154, label: 1, bag_size: 25695\n",
      "batch 39, loss: 0.8119, instance_loss: 0.1423, weighted_loss: 0.6110, label: 0, bag_size: 1732\n",
      "batch 59, loss: 0.0036, instance_loss: 0.0003, weighted_loss: 0.0026, label: 1, bag_size: 10028\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0047, weighted_loss: 0.0014, label: 1, bag_size: 4394\n",
      "batch 99, loss: 0.1688, instance_loss: 0.0034, weighted_loss: 0.1192, label: 0, bag_size: 9597\n",
      "batch 119, loss: 0.0035, instance_loss: 0.0217, weighted_loss: 0.0090, label: 1, bag_size: 15464\n",
      "batch 139, loss: 0.2845, instance_loss: 0.7194, weighted_loss: 0.4150, label: 1, bag_size: 2681\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0232, weighted_loss: 0.0070, label: 1, bag_size: 689\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0510, weighted_loss: 0.0153, label: 1, bag_size: 18095\n",
      "batch 199, loss: 0.0720, instance_loss: 0.0789, weighted_loss: 0.0741, label: 0, bag_size: 2270\n",
      "batch 219, loss: 0.0128, instance_loss: 0.0142, weighted_loss: 0.0132, label: 1, bag_size: 18603\n",
      "batch 239, loss: 0.0430, instance_loss: 0.1365, weighted_loss: 0.0710, label: 1, bag_size: 6736\n",
      "batch 259, loss: 0.0002, instance_loss: 0.0269, weighted_loss: 0.0082, label: 1, bag_size: 1255\n",
      "batch 279, loss: 0.1941, instance_loss: 0.0146, weighted_loss: 0.1402, label: 1, bag_size: 1284\n",
      "batch 299, loss: 0.0679, instance_loss: 0.0141, weighted_loss: 0.0517, label: 0, bag_size: 11151\n",
      "batch 319, loss: 0.1308, instance_loss: 0.0010, weighted_loss: 0.0918, label: 0, bag_size: 2628\n",
      "batch 339, loss: 0.0401, instance_loss: 0.0000, weighted_loss: 0.0280, label: 1, bag_size: 29832\n",
      "batch 359, loss: 0.0964, instance_loss: 0.0010, weighted_loss: 0.0677, label: 1, bag_size: 13440\n",
      "batch 379, loss: 0.0117, instance_loss: 0.7463, weighted_loss: 0.2321, label: 1, bag_size: 18161\n",
      "batch 399, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 10535\n",
      "batch 419, loss: 0.0003, instance_loss: 0.0326, weighted_loss: 0.0100, label: 1, bag_size: 8660\n",
      "batch 439, loss: 0.0960, instance_loss: 0.0154, weighted_loss: 0.0718, label: 1, bag_size: 12180\n",
      "batch 459, loss: 0.0463, instance_loss: 0.0613, weighted_loss: 0.0508, label: 0, bag_size: 4418\n",
      "batch 479, loss: 0.0109, instance_loss: 0.0096, weighted_loss: 0.0105, label: 1, bag_size: 6736\n",
      "batch 499, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 15464\n",
      "batch 519, loss: 0.0040, instance_loss: 0.0430, weighted_loss: 0.0157, label: 1, bag_size: 1920\n",
      "batch 539, loss: 0.2057, instance_loss: 0.0447, weighted_loss: 0.1574, label: 1, bag_size: 11386\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8661\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 0, bag_size: 5225\n",
      "batch 599, loss: 0.4422, instance_loss: 0.2931, weighted_loss: 0.3975, label: 0, bag_size: 1732\n",
      "batch 619, loss: 0.1089, instance_loss: 0.0052, weighted_loss: 0.0778, label: 1, bag_size: 13440\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0133, weighted_loss: 0.0040, label: 1, bag_size: 14515\n",
      "batch 659, loss: 0.0054, instance_loss: 0.0446, weighted_loss: 0.0171, label: 1, bag_size: 6781\n",
      "batch 679, loss: 0.0049, instance_loss: 0.0000, weighted_loss: 0.0035, label: 0, bag_size: 13339\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9433\n",
      "batch 719, loss: 0.0001, instance_loss: 0.1018, weighted_loss: 0.0306, label: 1, bag_size: 3651\n",
      "batch 739, loss: 0.0110, instance_loss: 0.0017, weighted_loss: 0.0082, label: 1, bag_size: 9942\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0039, weighted_loss: 0.0012, label: 1, bag_size: 5991\n",
      "batch 779, loss: 0.8316, instance_loss: 0.0000, weighted_loss: 0.5822, label: 0, bag_size: 15672\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0039, weighted_loss: 0.0012, label: 1, bag_size: 3295\n",
      "batch 819, loss: 0.0039, instance_loss: 0.0013, weighted_loss: 0.0031, label: 1, bag_size: 5160\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9864329268292683: correct 12942/13120\n",
      "class 1 clustering acc 0.9388719512195122: correct 6159/6560\n",
      "Epoch: 87, train_loss: 0.0869, train_clustering_loss:  0.1317, train_error: 0.0366\n",
      "class 0: acc 0.9583333333333334, correct 391/408\n",
      "class 1: acc 0.9684466019417476, correct 399/412\n",
      "\n",
      "Val Set, val_loss: 0.1400, val_error: 0.0364, auc: 0.9917\n",
      "class 0 clustering acc 0.9613636363636363: correct 1692/1760\n",
      "class 1 clustering acc 0.8738636363636364: correct 769/880\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 18 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21218\n",
      "batch 39, loss: 0.0774, instance_loss: 0.0018, weighted_loss: 0.0547, label: 0, bag_size: 2918\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 15213\n",
      "batch 79, loss: 0.0020, instance_loss: 0.0029, weighted_loss: 0.0022, label: 1, bag_size: 4821\n",
      "batch 99, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 14377\n",
      "batch 119, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 9485\n",
      "batch 139, loss: 0.0139, instance_loss: 0.0015, weighted_loss: 0.0102, label: 1, bag_size: 5137\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21218\n",
      "batch 179, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 10995\n",
      "batch 199, loss: 0.0002, instance_loss: 0.0126, weighted_loss: 0.0039, label: 0, bag_size: 9252\n",
      "batch 219, loss: 0.0817, instance_loss: 0.0091, weighted_loss: 0.0599, label: 1, bag_size: 9942\n",
      "batch 239, loss: 0.0028, instance_loss: 0.0057, weighted_loss: 0.0036, label: 0, bag_size: 1831\n",
      "batch 259, loss: 0.0003, instance_loss: 0.0151, weighted_loss: 0.0047, label: 1, bag_size: 7119\n",
      "batch 279, loss: 0.0397, instance_loss: 0.6225, weighted_loss: 0.2145, label: 1, bag_size: 2314\n",
      "batch 299, loss: 0.2715, instance_loss: 2.1918, weighted_loss: 0.8476, label: 0, bag_size: 15898\n",
      "batch 319, loss: 0.0182, instance_loss: 0.0001, weighted_loss: 0.0128, label: 0, bag_size: 19518\n",
      "batch 339, loss: 0.0044, instance_loss: 2.0863, weighted_loss: 0.6290, label: 1, bag_size: 7468\n",
      "batch 359, loss: 0.0977, instance_loss: 3.2947, weighted_loss: 1.0568, label: 0, bag_size: 2179\n",
      "batch 379, loss: 0.0002, instance_loss: 0.2788, weighted_loss: 0.0838, label: 0, bag_size: 17268\n",
      "batch 399, loss: 0.0114, instance_loss: 0.0177, weighted_loss: 0.0133, label: 0, bag_size: 11122\n",
      "batch 419, loss: 0.0747, instance_loss: 0.0715, weighted_loss: 0.0737, label: 1, bag_size: 3879\n",
      "batch 439, loss: 0.0072, instance_loss: 0.0126, weighted_loss: 0.0088, label: 1, bag_size: 10492\n",
      "batch 459, loss: 0.1030, instance_loss: 0.0035, weighted_loss: 0.0732, label: 0, bag_size: 11390\n",
      "batch 479, loss: 1.9245, instance_loss: 2.8849, weighted_loss: 2.2126, label: 0, bag_size: 7239\n",
      "batch 499, loss: 0.0031, instance_loss: 0.3254, weighted_loss: 0.0998, label: 1, bag_size: 1339\n",
      "batch 519, loss: 0.0110, instance_loss: 0.0172, weighted_loss: 0.0129, label: 1, bag_size: 17769\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0302, weighted_loss: 0.0091, label: 1, bag_size: 3082\n",
      "batch 559, loss: 0.0148, instance_loss: 0.0685, weighted_loss: 0.0309, label: 0, bag_size: 1438\n",
      "batch 579, loss: 0.0006, instance_loss: 0.0074, weighted_loss: 0.0026, label: 1, bag_size: 9878\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0198, weighted_loss: 0.0060, label: 1, bag_size: 9533\n",
      "batch 619, loss: 0.1274, instance_loss: 0.2009, weighted_loss: 0.1494, label: 0, bag_size: 3783\n",
      "batch 639, loss: 1.1259, instance_loss: 0.7429, weighted_loss: 1.0110, label: 1, bag_size: 1038\n",
      "batch 659, loss: 0.1383, instance_loss: 0.7358, weighted_loss: 0.3175, label: 1, bag_size: 1845\n",
      "batch 679, loss: 0.0042, instance_loss: 0.0001, weighted_loss: 0.0029, label: 0, bag_size: 12148\n",
      "batch 699, loss: 0.0139, instance_loss: 0.0338, weighted_loss: 0.0199, label: 1, bag_size: 9470\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0342, weighted_loss: 0.0103, label: 1, bag_size: 4442\n",
      "batch 739, loss: 1.4449, instance_loss: 0.1592, weighted_loss: 1.0592, label: 0, bag_size: 11306\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 19039\n",
      "batch 779, loss: 0.0013, instance_loss: 0.0143, weighted_loss: 0.0052, label: 1, bag_size: 5894\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8812\n",
      "batch 819, loss: 0.0014, instance_loss: 0.0056, weighted_loss: 0.0026, label: 0, bag_size: 2844\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9795731707317074: correct 12852/13120\n",
      "class 1 clustering acc 0.9126524390243902: correct 5987/6560\n",
      "Epoch: 88, train_loss: 0.1048, train_clustering_loss:  0.1836, train_error: 0.0451\n",
      "class 0: acc 0.947103274559194, correct 376/397\n",
      "class 1: acc 0.9621749408983451, correct 407/423\n",
      "\n",
      "Val Set, val_loss: 0.1984, val_error: 0.0909, auc: 0.9924\n",
      "class 0 clustering acc 0.928409090909091: correct 1634/1760\n",
      "class 1 clustering acc 0.803409090909091: correct 707/880\n",
      "class 0: acc 0.8076923076923077, correct 42/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 19 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.0259, instance_loss: 0.2459, weighted_loss: 0.7919, label: 0, bag_size: 11390\n",
      "batch 39, loss: 0.1526, instance_loss: 0.0747, weighted_loss: 0.1293, label: 1, bag_size: 12626\n",
      "batch 59, loss: 0.0019, instance_loss: 0.0442, weighted_loss: 0.0146, label: 0, bag_size: 15071\n",
      "batch 79, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 1, bag_size: 5690\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0019, weighted_loss: 0.0006, label: 1, bag_size: 9321\n",
      "batch 119, loss: 0.0072, instance_loss: 0.2846, weighted_loss: 0.0904, label: 0, bag_size: 1458\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0028, weighted_loss: 0.0009, label: 1, bag_size: 18468\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0255, weighted_loss: 0.0077, label: 1, bag_size: 617\n",
      "batch 179, loss: 0.0003, instance_loss: 0.0046, weighted_loss: 0.0016, label: 0, bag_size: 17155\n",
      "batch 199, loss: 0.4212, instance_loss: 0.0732, weighted_loss: 0.3168, label: 0, bag_size: 2070\n",
      "batch 219, loss: 0.0098, instance_loss: 0.0001, weighted_loss: 0.0069, label: 1, bag_size: 6205\n",
      "batch 239, loss: 0.0014, instance_loss: 0.0001, weighted_loss: 0.0010, label: 0, bag_size: 9542\n",
      "batch 259, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 19466\n",
      "batch 279, loss: 0.0042, instance_loss: 0.0000, weighted_loss: 0.0029, label: 0, bag_size: 19067\n",
      "batch 299, loss: 0.6475, instance_loss: 0.1265, weighted_loss: 0.4912, label: 1, bag_size: 3121\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 10481\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0502, weighted_loss: 0.0151, label: 0, bag_size: 8661\n",
      "batch 359, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 18215\n",
      "batch 379, loss: 0.0007, instance_loss: 0.0010, weighted_loss: 0.0008, label: 1, bag_size: 30675\n",
      "batch 399, loss: 0.0058, instance_loss: 0.0000, weighted_loss: 0.0040, label: 0, bag_size: 18738\n",
      "batch 419, loss: 0.0017, instance_loss: 0.0008, weighted_loss: 0.0014, label: 0, bag_size: 7381\n",
      "batch 439, loss: 0.0340, instance_loss: 0.0303, weighted_loss: 0.0329, label: 1, bag_size: 1038\n",
      "batch 459, loss: 0.0771, instance_loss: 0.0314, weighted_loss: 0.0634, label: 1, bag_size: 1038\n",
      "batch 479, loss: 0.0003, instance_loss: 0.0066, weighted_loss: 0.0022, label: 1, bag_size: 2495\n",
      "batch 499, loss: 0.0408, instance_loss: 0.0008, weighted_loss: 0.0288, label: 1, bag_size: 13440\n",
      "batch 519, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 15747\n",
      "batch 539, loss: 0.0202, instance_loss: 0.0225, weighted_loss: 0.0209, label: 0, bag_size: 3670\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 9470\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0036, weighted_loss: 0.0011, label: 1, bag_size: 18794\n",
      "batch 599, loss: 0.0042, instance_loss: 0.0002, weighted_loss: 0.0030, label: 1, bag_size: 5629\n",
      "batch 619, loss: 0.0026, instance_loss: 0.0028, weighted_loss: 0.0027, label: 1, bag_size: 10072\n",
      "batch 639, loss: 0.0087, instance_loss: 0.0175, weighted_loss: 0.0113, label: 0, bag_size: 16607\n",
      "batch 659, loss: 0.0007, instance_loss: 0.0112, weighted_loss: 0.0038, label: 0, bag_size: 1891\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0165, weighted_loss: 0.0050, label: 1, bag_size: 3856\n",
      "batch 699, loss: 0.1584, instance_loss: 2.3946, weighted_loss: 0.8293, label: 1, bag_size: 3121\n",
      "batch 719, loss: 0.0001, instance_loss: 0.0069, weighted_loss: 0.0021, label: 0, bag_size: 3459\n",
      "batch 739, loss: 0.0000, instance_loss: 0.1271, weighted_loss: 0.0381, label: 1, bag_size: 2136\n",
      "batch 759, loss: 0.0013, instance_loss: 0.0807, weighted_loss: 0.0251, label: 0, bag_size: 7612\n",
      "batch 779, loss: 0.0704, instance_loss: 0.0002, weighted_loss: 0.0493, label: 0, bag_size: 2998\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0081, weighted_loss: 0.0024, label: 1, bag_size: 928\n",
      "batch 819, loss: 0.0147, instance_loss: 0.0028, weighted_loss: 0.0111, label: 1, bag_size: 1823\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9872713414634147: correct 12953/13120\n",
      "class 1 clustering acc 0.9446646341463415: correct 6197/6560\n",
      "Epoch: 89, train_loss: 0.0865, train_clustering_loss:  0.1216, train_error: 0.0427\n",
      "class 0: acc 0.9489795918367347, correct 372/392\n",
      "class 1: acc 0.9649532710280374, correct 413/428\n",
      "\n",
      "Val Set, val_loss: 0.1078, val_error: 0.0364, auc: 0.9920\n",
      "class 0 clustering acc 0.9590909090909091: correct 1688/1760\n",
      "class 1 clustering acc 0.8772727272727273: correct 772/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Val error: 0.0273, ROC AUC: 0.9950\n",
      "Test error: 0.0825, ROC AUC: 0.9685\n",
      "class 0: acc 1.0, correct 49/49\n",
      "class 1: acc 0.8333333333333334, correct 40/48\n",
      "\n",
      "Training Fold 1!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 822 samples\n",
      "Validating on 109 samples\n",
      "Testing on 96 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Setting tau to 1.0\n",
      "Done!\n",
      "MCBAT_SB(\n",
      "  (instance_loss_fn): SmoothTop1SVM()\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (transformer_low): TransformerEncoder_FLASH(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FLASH(\n",
      "            (attn_fn): LaplacianAttnFn()\n",
      "            (rel_pos_bias): T5RelativePositionBias(\n",
      "              (relative_attention_bias): Embedding(32, 1)\n",
      "            )\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (to_hidden): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (to_qk): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (qk_offset_scale): OffsetScale()\n",
      "            (to_out): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer_high): TransformerEncoder_FLASH(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FLASH(\n",
      "            (attn_fn): LaplacianAttnFn()\n",
      "            (rel_pos_bias): T5RelativePositionBias(\n",
      "              (relative_attention_bias): Embedding(32, 1)\n",
      "            )\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (to_hidden): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (to_qk): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (qk_offset_scale): OffsetScale()\n",
      "            (to_out): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attention): Attn_Net_Gated(\n",
      "    (attention_a): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_b): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (fusion_encoder): FusionEncoder()\n",
      "  (attention_V2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (attention_U2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (attention_weights2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Total number of parameters: 17086091\n",
      "Total number of trainable parameters: 17086091\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... 2\n",
      "Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6569, instance_loss: 2.0661, weighted_loss: 1.0797, label: 0, bag_size: 10481\n",
      "batch 39, loss: 0.4087, instance_loss: 2.5175, weighted_loss: 1.0414, label: 1, bag_size: 1755\n",
      "batch 59, loss: 0.6415, instance_loss: 3.3407, weighted_loss: 1.4512, label: 1, bag_size: 9215\n",
      "batch 79, loss: 0.7487, instance_loss: 0.5605, weighted_loss: 0.6922, label: 0, bag_size: 2104\n",
      "batch 99, loss: 0.6949, instance_loss: 0.8785, weighted_loss: 0.7499, label: 1, bag_size: 1888\n",
      "batch 119, loss: 0.5562, instance_loss: 0.6763, weighted_loss: 0.5922, label: 1, bag_size: 9230\n",
      "batch 139, loss: 0.8216, instance_loss: 3.3315, weighted_loss: 1.5746, label: 0, bag_size: 2959\n",
      "batch 159, loss: 0.8851, instance_loss: 0.9443, weighted_loss: 0.9029, label: 0, bag_size: 20230\n",
      "batch 179, loss: 0.6863, instance_loss: 0.6956, weighted_loss: 0.6891, label: 1, bag_size: 2314\n",
      "batch 199, loss: 0.7591, instance_loss: 0.4232, weighted_loss: 0.6584, label: 0, bag_size: 29270\n",
      "batch 219, loss: 0.6921, instance_loss: 1.1596, weighted_loss: 0.8324, label: 1, bag_size: 1875\n",
      "batch 239, loss: 0.4672, instance_loss: 0.4725, weighted_loss: 0.4688, label: 1, bag_size: 1683\n",
      "batch 259, loss: 0.6374, instance_loss: 1.0599, weighted_loss: 0.7641, label: 1, bag_size: 7989\n",
      "batch 279, loss: 0.6955, instance_loss: 0.8224, weighted_loss: 0.7336, label: 1, bag_size: 8410\n",
      "batch 299, loss: 0.5895, instance_loss: 1.3948, weighted_loss: 0.8311, label: 1, bag_size: 9215\n",
      "batch 319, loss: 0.6513, instance_loss: 0.6847, weighted_loss: 0.6613, label: 0, bag_size: 19466\n",
      "batch 339, loss: 0.4216, instance_loss: 0.5904, weighted_loss: 0.4722, label: 0, bag_size: 2548\n",
      "batch 359, loss: 0.9270, instance_loss: 2.1500, weighted_loss: 1.2939, label: 1, bag_size: 1924\n",
      "batch 379, loss: 0.5567, instance_loss: 0.1866, weighted_loss: 0.4456, label: 0, bag_size: 4497\n",
      "batch 399, loss: 0.8785, instance_loss: 2.1599, weighted_loss: 1.2629, label: 1, bag_size: 7583\n",
      "batch 419, loss: 0.8013, instance_loss: 0.9214, weighted_loss: 0.8373, label: 1, bag_size: 4862\n",
      "batch 439, loss: 0.6995, instance_loss: 0.8996, weighted_loss: 0.7595, label: 1, bag_size: 2731\n",
      "batch 459, loss: 1.0376, instance_loss: 0.1140, weighted_loss: 0.7606, label: 0, bag_size: 21682\n",
      "batch 479, loss: 0.7771, instance_loss: 0.6084, weighted_loss: 0.7265, label: 1, bag_size: 11122\n",
      "batch 499, loss: 0.5059, instance_loss: 0.8509, weighted_loss: 0.6094, label: 0, bag_size: 7637\n",
      "batch 519, loss: 0.7614, instance_loss: 0.3032, weighted_loss: 0.6240, label: 1, bag_size: 14618\n",
      "batch 539, loss: 0.5316, instance_loss: 0.5891, weighted_loss: 0.5489, label: 1, bag_size: 4880\n",
      "batch 559, loss: 0.5639, instance_loss: 0.5197, weighted_loss: 0.5507, label: 1, bag_size: 4317\n",
      "batch 579, loss: 0.6475, instance_loss: 0.3285, weighted_loss: 0.5518, label: 1, bag_size: 7873\n",
      "batch 599, loss: 0.4920, instance_loss: 0.0369, weighted_loss: 0.3554, label: 1, bag_size: 4239\n",
      "batch 619, loss: 0.4077, instance_loss: 0.4409, weighted_loss: 0.4176, label: 1, bag_size: 8012\n",
      "batch 639, loss: 0.4362, instance_loss: 2.2834, weighted_loss: 0.9903, label: 1, bag_size: 28527\n",
      "batch 659, loss: 1.0318, instance_loss: 0.7256, weighted_loss: 0.9399, label: 0, bag_size: 6727\n",
      "batch 679, loss: 0.6184, instance_loss: 1.0911, weighted_loss: 0.7602, label: 1, bag_size: 4367\n",
      "batch 699, loss: 0.6993, instance_loss: 1.2174, weighted_loss: 0.8548, label: 0, bag_size: 11146\n",
      "batch 719, loss: 0.8257, instance_loss: 0.7401, weighted_loss: 0.8000, label: 0, bag_size: 5120\n",
      "batch 739, loss: 0.4435, instance_loss: 0.4550, weighted_loss: 0.4470, label: 0, bag_size: 19518\n",
      "batch 759, loss: 0.4745, instance_loss: 2.5728, weighted_loss: 1.1040, label: 0, bag_size: 18777\n",
      "batch 779, loss: 0.8538, instance_loss: 0.5231, weighted_loss: 0.7546, label: 1, bag_size: 18699\n",
      "batch 799, loss: 0.8658, instance_loss: 0.6465, weighted_loss: 0.8000, label: 0, bag_size: 14266\n",
      "batch 819, loss: 0.8940, instance_loss: 0.6857, weighted_loss: 0.8315, label: 0, bag_size: 14625\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.8883059610705596: correct 11683/13152\n",
      "class 1 clustering acc 0.3451946472019465: correct 2270/6576\n",
      "Epoch: 0, train_loss: 0.7017, train_clustering_loss:  1.1107, train_error: 0.4878\n",
      "class 0: acc 0.3743718592964824, correct 149/398\n",
      "class 1: acc 0.6415094339622641, correct 272/424\n",
      "\n",
      "Val Set, val_loss: 0.6850, val_error: 0.4220, auc: 0.5588\n",
      "class 0 clustering acc 0.9678899082568807: correct 1688/1744\n",
      "class 1 clustering acc 0.591743119266055: correct 516/872\n",
      "class 0: acc 0.0, correct 0/46\n",
      "class 1: acc 1.0, correct 63/63\n",
      "Validation loss decreased (inf --> 0.685044).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6950, instance_loss: 0.3791, weighted_loss: 0.6003, label: 1, bag_size: 12575\n",
      "batch 39, loss: 0.5673, instance_loss: 0.5191, weighted_loss: 0.5528, label: 0, bag_size: 16341\n",
      "batch 59, loss: 0.6058, instance_loss: 0.0888, weighted_loss: 0.4507, label: 0, bag_size: 1881\n",
      "batch 79, loss: 0.8429, instance_loss: 0.4356, weighted_loss: 0.7207, label: 1, bag_size: 6776\n",
      "batch 99, loss: 0.7378, instance_loss: 0.4409, weighted_loss: 0.6488, label: 0, bag_size: 1884\n",
      "batch 119, loss: 0.9452, instance_loss: 0.4727, weighted_loss: 0.8035, label: 1, bag_size: 10432\n",
      "batch 139, loss: 0.7967, instance_loss: 0.3240, weighted_loss: 0.6549, label: 0, bag_size: 20796\n",
      "batch 159, loss: 0.7204, instance_loss: 0.5074, weighted_loss: 0.6565, label: 1, bag_size: 8012\n",
      "batch 179, loss: 0.8225, instance_loss: 0.7573, weighted_loss: 0.8030, label: 1, bag_size: 3656\n",
      "batch 199, loss: 0.7490, instance_loss: 0.9047, weighted_loss: 0.7957, label: 0, bag_size: 15057\n",
      "batch 219, loss: 0.5900, instance_loss: 1.2806, weighted_loss: 0.7972, label: 0, bag_size: 8981\n",
      "batch 239, loss: 0.6186, instance_loss: 0.1884, weighted_loss: 0.4895, label: 0, bag_size: 11194\n",
      "batch 259, loss: 0.7334, instance_loss: 0.6825, weighted_loss: 0.7181, label: 1, bag_size: 1999\n",
      "batch 279, loss: 0.7603, instance_loss: 0.4982, weighted_loss: 0.6817, label: 1, bag_size: 2936\n",
      "batch 299, loss: 0.6430, instance_loss: 0.1028, weighted_loss: 0.4809, label: 1, bag_size: 10072\n",
      "batch 319, loss: 0.7483, instance_loss: 0.1857, weighted_loss: 0.5795, label: 0, bag_size: 2873\n",
      "batch 339, loss: 0.6380, instance_loss: 0.5392, weighted_loss: 0.6084, label: 0, bag_size: 9060\n",
      "batch 359, loss: 0.6149, instance_loss: 2.7114, weighted_loss: 1.2439, label: 0, bag_size: 2653\n",
      "batch 379, loss: 0.5909, instance_loss: 2.7399, weighted_loss: 1.2356, label: 1, bag_size: 2842\n",
      "batch 399, loss: 0.7166, instance_loss: 0.0751, weighted_loss: 0.5241, label: 1, bag_size: 25695\n",
      "batch 419, loss: 0.7829, instance_loss: 3.5979, weighted_loss: 1.6274, label: 0, bag_size: 2303\n",
      "batch 439, loss: 0.9084, instance_loss: 0.0776, weighted_loss: 0.6592, label: 0, bag_size: 31085\n",
      "batch 459, loss: 0.5909, instance_loss: 0.1234, weighted_loss: 0.4506, label: 1, bag_size: 10112\n",
      "batch 479, loss: 0.5443, instance_loss: 0.1521, weighted_loss: 0.4266, label: 0, bag_size: 10791\n",
      "batch 499, loss: 1.0925, instance_loss: 0.6705, weighted_loss: 0.9659, label: 1, bag_size: 2344\n",
      "batch 519, loss: 0.5831, instance_loss: 0.4901, weighted_loss: 0.5552, label: 0, bag_size: 11146\n",
      "batch 539, loss: 0.9365, instance_loss: 0.8251, weighted_loss: 0.9031, label: 1, bag_size: 4929\n",
      "batch 559, loss: 1.1585, instance_loss: 1.3058, weighted_loss: 1.2027, label: 1, bag_size: 7989\n",
      "batch 579, loss: 0.7721, instance_loss: 0.4724, weighted_loss: 0.6822, label: 1, bag_size: 9330\n",
      "batch 599, loss: 0.6857, instance_loss: 0.1949, weighted_loss: 0.5384, label: 0, bag_size: 21864\n",
      "batch 619, loss: 0.9742, instance_loss: 0.0903, weighted_loss: 0.7090, label: 1, bag_size: 5629\n",
      "batch 639, loss: 0.6068, instance_loss: 0.0119, weighted_loss: 0.4283, label: 0, bag_size: 20150\n",
      "batch 659, loss: 0.5841, instance_loss: 0.3211, weighted_loss: 0.5052, label: 1, bag_size: 7382\n",
      "batch 679, loss: 0.8779, instance_loss: 1.3642, weighted_loss: 1.0238, label: 0, bag_size: 12083\n",
      "batch 699, loss: 0.7150, instance_loss: 0.8844, weighted_loss: 0.7659, label: 0, bag_size: 30751\n",
      "batch 719, loss: 0.6626, instance_loss: 0.5863, weighted_loss: 0.6397, label: 0, bag_size: 2006\n",
      "batch 739, loss: 0.8270, instance_loss: 0.0838, weighted_loss: 0.6041, label: 1, bag_size: 14223\n",
      "batch 759, loss: 0.5609, instance_loss: 0.0877, weighted_loss: 0.4190, label: 1, bag_size: 4821\n",
      "batch 779, loss: 0.7351, instance_loss: 0.0793, weighted_loss: 0.5384, label: 1, bag_size: 6734\n",
      "batch 799, loss: 0.8931, instance_loss: 0.1824, weighted_loss: 0.6799, label: 1, bag_size: 8019\n",
      "batch 819, loss: 0.7977, instance_loss: 2.0172, weighted_loss: 1.1635, label: 1, bag_size: 1875\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9222171532846716: correct 12129/13152\n",
      "class 1 clustering acc 0.5593065693430657: correct 3678/6576\n",
      "Epoch: 1, train_loss: 0.7115, train_clustering_loss:  0.7256, train_error: 0.5231\n",
      "class 0: acc 0.5971896955503513, correct 255/427\n",
      "class 1: acc 0.3468354430379747, correct 137/395\n",
      "\n",
      "Val Set, val_loss: 0.6923, val_error: 0.4220, auc: 0.7117\n",
      "class 0 clustering acc 0.9134174311926605: correct 1593/1744\n",
      "class 1 clustering acc 0.6456422018348624: correct 563/872\n",
      "class 0: acc 0.0, correct 0/46\n",
      "class 1: acc 1.0, correct 63/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4687, instance_loss: 0.1443, weighted_loss: 0.3714, label: 1, bag_size: 9955\n",
      "batch 39, loss: 1.0293, instance_loss: 0.1835, weighted_loss: 0.7756, label: 0, bag_size: 10898\n",
      "batch 59, loss: 0.5482, instance_loss: 0.1328, weighted_loss: 0.4236, label: 0, bag_size: 12083\n",
      "batch 79, loss: 0.4319, instance_loss: 0.0092, weighted_loss: 0.3050, label: 0, bag_size: 10415\n",
      "batch 99, loss: 0.9403, instance_loss: 0.0866, weighted_loss: 0.6842, label: 1, bag_size: 14779\n",
      "batch 119, loss: 0.5118, instance_loss: 0.0089, weighted_loss: 0.3609, label: 0, bag_size: 15747\n",
      "batch 139, loss: 1.0572, instance_loss: 1.0871, weighted_loss: 1.0662, label: 1, bag_size: 6682\n",
      "batch 159, loss: 0.8415, instance_loss: 0.0814, weighted_loss: 0.6135, label: 1, bag_size: 15213\n",
      "batch 179, loss: 0.5060, instance_loss: 0.0911, weighted_loss: 0.3815, label: 0, bag_size: 4465\n",
      "batch 199, loss: 0.4197, instance_loss: 0.0699, weighted_loss: 0.3147, label: 0, bag_size: 20666\n",
      "batch 219, loss: 1.1043, instance_loss: 0.2502, weighted_loss: 0.8481, label: 1, bag_size: 8410\n",
      "batch 239, loss: 0.4828, instance_loss: 1.0507, weighted_loss: 0.6531, label: 0, bag_size: 4523\n",
      "batch 259, loss: 0.4208, instance_loss: 0.4218, weighted_loss: 0.4211, label: 0, bag_size: 22800\n",
      "batch 279, loss: 0.5249, instance_loss: 0.4245, weighted_loss: 0.4948, label: 0, bag_size: 15077\n",
      "batch 299, loss: 0.6777, instance_loss: 0.2503, weighted_loss: 0.5495, label: 1, bag_size: 3437\n",
      "batch 319, loss: 0.6406, instance_loss: 0.1942, weighted_loss: 0.5067, label: 0, bag_size: 24911\n",
      "batch 339, loss: 0.6818, instance_loss: 1.2889, weighted_loss: 0.8639, label: 1, bag_size: 20161\n",
      "batch 359, loss: 0.4046, instance_loss: 0.2541, weighted_loss: 0.3595, label: 1, bag_size: 5221\n",
      "batch 379, loss: 0.3903, instance_loss: 0.0676, weighted_loss: 0.2935, label: 1, bag_size: 14223\n",
      "batch 399, loss: 0.4268, instance_loss: 0.0395, weighted_loss: 0.3106, label: 1, bag_size: 11701\n",
      "batch 419, loss: 0.9029, instance_loss: 3.8582, weighted_loss: 1.7895, label: 0, bag_size: 3897\n",
      "batch 439, loss: 0.5802, instance_loss: 0.3442, weighted_loss: 0.5094, label: 0, bag_size: 2609\n",
      "batch 459, loss: 0.9321, instance_loss: 0.0143, weighted_loss: 0.6568, label: 1, bag_size: 8660\n",
      "batch 479, loss: 0.6711, instance_loss: 0.3116, weighted_loss: 0.5632, label: 0, bag_size: 2322\n",
      "batch 499, loss: 0.6350, instance_loss: 0.1283, weighted_loss: 0.4830, label: 1, bag_size: 4821\n",
      "batch 519, loss: 0.5586, instance_loss: 0.2002, weighted_loss: 0.4511, label: 0, bag_size: 3321\n",
      "batch 539, loss: 0.9339, instance_loss: 0.2099, weighted_loss: 0.7167, label: 1, bag_size: 5894\n",
      "batch 559, loss: 0.6512, instance_loss: 0.0707, weighted_loss: 0.4771, label: 0, bag_size: 9060\n",
      "batch 579, loss: 0.4200, instance_loss: 0.3928, weighted_loss: 0.4118, label: 1, bag_size: 1819\n",
      "batch 599, loss: 0.9292, instance_loss: 0.6014, weighted_loss: 0.8308, label: 0, bag_size: 3321\n",
      "batch 619, loss: 0.7041, instance_loss: 0.5772, weighted_loss: 0.6660, label: 1, bag_size: 7424\n",
      "batch 639, loss: 0.6939, instance_loss: 0.4671, weighted_loss: 0.6258, label: 1, bag_size: 13732\n",
      "batch 659, loss: 0.7198, instance_loss: 0.0000, weighted_loss: 0.5038, label: 0, bag_size: 24911\n",
      "batch 679, loss: 0.4826, instance_loss: 0.0166, weighted_loss: 0.3428, label: 0, bag_size: 3399\n",
      "batch 699, loss: 0.6466, instance_loss: 0.3218, weighted_loss: 0.5492, label: 1, bag_size: 2495\n",
      "batch 719, loss: 0.3179, instance_loss: 0.0046, weighted_loss: 0.2239, label: 1, bag_size: 3576\n",
      "batch 739, loss: 0.4297, instance_loss: 1.1083, weighted_loss: 0.6333, label: 1, bag_size: 10591\n",
      "batch 759, loss: 0.6520, instance_loss: 0.4289, weighted_loss: 0.5850, label: 0, bag_size: 15057\n",
      "batch 779, loss: 0.6181, instance_loss: 2.6041, weighted_loss: 1.2139, label: 0, bag_size: 5105\n",
      "batch 799, loss: 0.5558, instance_loss: 0.4332, weighted_loss: 0.5190, label: 1, bag_size: 1924\n",
      "batch 819, loss: 0.4098, instance_loss: 0.0249, weighted_loss: 0.2943, label: 1, bag_size: 6599\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9414537712895377: correct 12382/13152\n",
      "class 1 clustering acc 0.6733576642335767: correct 4428/6576\n",
      "Epoch: 2, train_loss: 0.6956, train_clustering_loss:  0.5732, train_error: 0.4526\n",
      "class 0: acc 0.6004842615012107, correct 248/413\n",
      "class 1: acc 0.4938875305623472, correct 202/409\n",
      "\n",
      "Val Set, val_loss: 0.6885, val_error: 0.4220, auc: 0.7904\n",
      "class 0 clustering acc 0.9650229357798165: correct 1683/1744\n",
      "class 1 clustering acc 0.8463302752293578: correct 738/872\n",
      "class 0: acc 0.0, correct 0/46\n",
      "class 1: acc 1.0, correct 63/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.9984, instance_loss: 0.2522, weighted_loss: 0.7745, label: 0, bag_size: 1984\n",
      "batch 39, loss: 0.5718, instance_loss: 0.0592, weighted_loss: 0.4180, label: 0, bag_size: 14739\n",
      "batch 59, loss: 1.0674, instance_loss: 0.0039, weighted_loss: 0.7484, label: 1, bag_size: 5629\n",
      "batch 79, loss: 0.4884, instance_loss: 0.0491, weighted_loss: 0.3566, label: 0, bag_size: 17268\n",
      "batch 99, loss: 0.8295, instance_loss: 0.1144, weighted_loss: 0.6149, label: 1, bag_size: 5894\n",
      "batch 119, loss: 1.0116, instance_loss: 3.5731, weighted_loss: 1.7801, label: 1, bag_size: 21252\n",
      "batch 139, loss: 0.5787, instance_loss: 0.6368, weighted_loss: 0.5961, label: 0, bag_size: 11128\n",
      "batch 159, loss: 0.7341, instance_loss: 0.0270, weighted_loss: 0.5219, label: 1, bag_size: 9322\n",
      "batch 179, loss: 0.7203, instance_loss: 2.3310, weighted_loss: 1.2035, label: 1, bag_size: 13362\n",
      "batch 199, loss: 0.6346, instance_loss: 0.0199, weighted_loss: 0.4502, label: 1, bag_size: 8395\n",
      "batch 219, loss: 0.7087, instance_loss: 0.0124, weighted_loss: 0.4998, label: 0, bag_size: 7923\n",
      "batch 239, loss: 0.6360, instance_loss: 0.0259, weighted_loss: 0.4530, label: 0, bag_size: 19659\n",
      "batch 259, loss: 0.7707, instance_loss: 0.3439, weighted_loss: 0.6427, label: 1, bag_size: 5921\n",
      "batch 279, loss: 0.9829, instance_loss: 0.0175, weighted_loss: 0.6933, label: 0, bag_size: 21385\n",
      "batch 299, loss: 0.5209, instance_loss: 0.3186, weighted_loss: 0.4602, label: 1, bag_size: 8438\n",
      "batch 319, loss: 0.6917, instance_loss: 0.0969, weighted_loss: 0.5133, label: 0, bag_size: 10481\n",
      "batch 339, loss: 0.6521, instance_loss: 0.0022, weighted_loss: 0.4571, label: 1, bag_size: 19932\n",
      "batch 359, loss: 0.7268, instance_loss: 0.0934, weighted_loss: 0.5368, label: 0, bag_size: 17083\n",
      "batch 379, loss: 0.6585, instance_loss: 0.1050, weighted_loss: 0.4924, label: 0, bag_size: 10263\n",
      "batch 399, loss: 0.9214, instance_loss: 0.8844, weighted_loss: 0.9103, label: 1, bag_size: 10112\n",
      "batch 419, loss: 0.6694, instance_loss: 0.0053, weighted_loss: 0.4702, label: 1, bag_size: 5605\n",
      "batch 439, loss: 0.9818, instance_loss: 3.5766, weighted_loss: 1.7602, label: 1, bag_size: 1963\n",
      "batch 459, loss: 1.1240, instance_loss: 0.1737, weighted_loss: 0.8389, label: 1, bag_size: 4959\n",
      "batch 479, loss: 0.4106, instance_loss: 0.0458, weighted_loss: 0.3012, label: 0, bag_size: 18215\n",
      "batch 499, loss: 1.1543, instance_loss: 1.1055, weighted_loss: 1.1397, label: 1, bag_size: 11220\n",
      "batch 519, loss: 0.7572, instance_loss: 0.0181, weighted_loss: 0.5355, label: 1, bag_size: 13026\n",
      "batch 539, loss: 0.6548, instance_loss: 0.0807, weighted_loss: 0.4826, label: 1, bag_size: 10396\n",
      "batch 559, loss: 1.0074, instance_loss: 0.1783, weighted_loss: 0.7587, label: 0, bag_size: 890\n",
      "batch 579, loss: 0.5245, instance_loss: 0.0047, weighted_loss: 0.3686, label: 1, bag_size: 6533\n",
      "batch 599, loss: 0.4006, instance_loss: 0.6347, weighted_loss: 0.4708, label: 1, bag_size: 1999\n",
      "batch 619, loss: 1.1977, instance_loss: 0.2492, weighted_loss: 0.9132, label: 0, bag_size: 1891\n",
      "batch 639, loss: 0.8134, instance_loss: 0.0099, weighted_loss: 0.5724, label: 0, bag_size: 10898\n",
      "batch 659, loss: 0.7300, instance_loss: 0.9579, weighted_loss: 0.7984, label: 0, bag_size: 6898\n",
      "batch 679, loss: 0.7988, instance_loss: 1.4391, weighted_loss: 0.9909, label: 1, bag_size: 8438\n",
      "batch 699, loss: 0.6258, instance_loss: 0.0268, weighted_loss: 0.4461, label: 1, bag_size: 6927\n",
      "batch 719, loss: 0.8004, instance_loss: 0.1596, weighted_loss: 0.6082, label: 1, bag_size: 5991\n",
      "batch 739, loss: 0.4103, instance_loss: 1.9201, weighted_loss: 0.8633, label: 0, bag_size: 11390\n",
      "batch 759, loss: 0.4124, instance_loss: 0.1851, weighted_loss: 0.3442, label: 0, bag_size: 10029\n",
      "batch 779, loss: 0.7834, instance_loss: 0.5684, weighted_loss: 0.7189, label: 0, bag_size: 11607\n",
      "batch 799, loss: 0.7314, instance_loss: 0.1635, weighted_loss: 0.5611, label: 0, bag_size: 14893\n",
      "batch 819, loss: 0.5657, instance_loss: 0.3630, weighted_loss: 0.5049, label: 1, bag_size: 8019\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9541514598540146: correct 12549/13152\n",
      "class 1 clustering acc 0.7448296836982968: correct 4898/6576\n",
      "Epoch: 3, train_loss: 0.7033, train_clustering_loss:  0.4304, train_error: 0.4830\n",
      "class 0: acc 0.5690866510538641, correct 243/427\n",
      "class 1: acc 0.4607594936708861, correct 182/395\n",
      "\n",
      "Val Set, val_loss: 0.6841, val_error: 0.4220, auc: 0.8803\n",
      "class 0 clustering acc 0.8755733944954128: correct 1527/1744\n",
      "class 1 clustering acc 0.7144495412844036: correct 623/872\n",
      "class 0: acc 0.0, correct 0/46\n",
      "class 1: acc 1.0, correct 63/63\n",
      "Validation loss decreased (0.685044 --> 0.684085).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6556, instance_loss: 0.1159, weighted_loss: 0.4937, label: 0, bag_size: 2091\n",
      "batch 39, loss: 0.3569, instance_loss: 0.1934, weighted_loss: 0.3079, label: 0, bag_size: 3321\n",
      "batch 59, loss: 0.4564, instance_loss: 1.4583, weighted_loss: 0.7570, label: 0, bag_size: 2815\n",
      "batch 79, loss: 0.5879, instance_loss: 0.4143, weighted_loss: 0.5358, label: 0, bag_size: 13691\n",
      "batch 99, loss: 0.7675, instance_loss: 0.0183, weighted_loss: 0.5427, label: 1, bag_size: 6927\n",
      "batch 119, loss: 0.8535, instance_loss: 0.2298, weighted_loss: 0.6664, label: 1, bag_size: 1316\n",
      "batch 139, loss: 0.7435, instance_loss: 0.0608, weighted_loss: 0.5387, label: 0, bag_size: 14681\n",
      "batch 159, loss: 0.7721, instance_loss: 0.0894, weighted_loss: 0.5673, label: 1, bag_size: 3409\n",
      "batch 179, loss: 0.6832, instance_loss: 0.0441, weighted_loss: 0.4915, label: 0, bag_size: 5999\n",
      "batch 199, loss: 0.6820, instance_loss: 0.6687, weighted_loss: 0.6780, label: 1, bag_size: 14515\n",
      "batch 219, loss: 0.8849, instance_loss: 0.3413, weighted_loss: 0.7218, label: 0, bag_size: 2036\n",
      "batch 239, loss: 0.8495, instance_loss: 1.1486, weighted_loss: 0.9392, label: 0, bag_size: 14264\n",
      "batch 259, loss: 0.5554, instance_loss: 0.0092, weighted_loss: 0.3915, label: 0, bag_size: 21138\n",
      "batch 279, loss: 0.4815, instance_loss: 0.1357, weighted_loss: 0.3777, label: 0, bag_size: 10444\n",
      "batch 299, loss: 0.8565, instance_loss: 0.6213, weighted_loss: 0.7859, label: 1, bag_size: 10848\n",
      "batch 319, loss: 0.5483, instance_loss: 0.1261, weighted_loss: 0.4216, label: 0, bag_size: 15747\n",
      "batch 339, loss: 0.7656, instance_loss: 0.1391, weighted_loss: 0.5776, label: 0, bag_size: 10942\n",
      "batch 359, loss: 0.9584, instance_loss: 0.2671, weighted_loss: 0.7510, label: 0, bag_size: 5120\n",
      "batch 379, loss: 0.8456, instance_loss: 0.2791, weighted_loss: 0.6757, label: 0, bag_size: 2266\n",
      "batch 399, loss: 0.9408, instance_loss: 0.2233, weighted_loss: 0.7256, label: 1, bag_size: 4862\n",
      "batch 419, loss: 0.8852, instance_loss: 0.0962, weighted_loss: 0.6485, label: 1, bag_size: 771\n",
      "batch 439, loss: 0.7481, instance_loss: 0.4807, weighted_loss: 0.6679, label: 0, bag_size: 5409\n",
      "batch 459, loss: 0.7390, instance_loss: 0.4799, weighted_loss: 0.6613, label: 0, bag_size: 10063\n",
      "batch 479, loss: 0.7954, instance_loss: 0.0000, weighted_loss: 0.5568, label: 1, bag_size: 11701\n",
      "batch 499, loss: 0.4941, instance_loss: 0.0032, weighted_loss: 0.3468, label: 1, bag_size: 7468\n",
      "batch 519, loss: 0.5620, instance_loss: 0.0298, weighted_loss: 0.4023, label: 1, bag_size: 5345\n",
      "batch 539, loss: 0.7137, instance_loss: 0.3149, weighted_loss: 0.5940, label: 1, bag_size: 3368\n",
      "batch 559, loss: 0.8444, instance_loss: 0.0073, weighted_loss: 0.5933, label: 1, bag_size: 6599\n",
      "batch 579, loss: 0.9143, instance_loss: 0.8942, weighted_loss: 0.9083, label: 1, bag_size: 2579\n",
      "batch 599, loss: 0.7146, instance_loss: 0.0197, weighted_loss: 0.5061, label: 0, bag_size: 23037\n",
      "batch 619, loss: 0.6965, instance_loss: 0.5181, weighted_loss: 0.6430, label: 0, bag_size: 7823\n",
      "batch 639, loss: 0.6347, instance_loss: 1.7940, weighted_loss: 0.9825, label: 1, bag_size: 4786\n",
      "batch 659, loss: 0.7107, instance_loss: 0.2847, weighted_loss: 0.5829, label: 1, bag_size: 1920\n",
      "batch 679, loss: 0.6189, instance_loss: 0.0066, weighted_loss: 0.4352, label: 0, bag_size: 18738\n",
      "batch 699, loss: 0.8332, instance_loss: 0.0439, weighted_loss: 0.5964, label: 1, bag_size: 1786\n",
      "batch 719, loss: 0.7336, instance_loss: 0.4525, weighted_loss: 0.6492, label: 0, bag_size: 1370\n",
      "batch 739, loss: 0.6691, instance_loss: 0.4918, weighted_loss: 0.6160, label: 0, bag_size: 1498\n",
      "batch 759, loss: 0.6727, instance_loss: 0.5340, weighted_loss: 0.6311, label: 0, bag_size: 2219\n",
      "batch 779, loss: 0.6872, instance_loss: 0.0013, weighted_loss: 0.4814, label: 1, bag_size: 11701\n",
      "batch 799, loss: 0.7908, instance_loss: 0.0620, weighted_loss: 0.5722, label: 0, bag_size: 17633\n",
      "batch 819, loss: 1.0558, instance_loss: 1.8082, weighted_loss: 1.2815, label: 1, bag_size: 2314\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9542274939172749: correct 12550/13152\n",
      "class 1 clustering acc 0.745742092457421: correct 4904/6576\n",
      "Epoch: 4, train_loss: 0.7076, train_clustering_loss:  0.4253, train_error: 0.5097\n",
      "class 0: acc 0.6041666666666666, correct 261/432\n",
      "class 1: acc 0.3641025641025641, correct 142/390\n",
      "\n",
      "Val Set, val_loss: 0.7586, val_error: 0.5780, auc: 0.9106\n",
      "class 0 clustering acc 0.9569954128440367: correct 1669/1744\n",
      "class 1 clustering acc 0.75: correct 654/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.0, correct 0/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6047, instance_loss: 0.0243, weighted_loss: 0.4305, label: 0, bag_size: 3474\n",
      "batch 39, loss: 0.7466, instance_loss: 0.0976, weighted_loss: 0.5519, label: 1, bag_size: 2790\n",
      "batch 59, loss: 0.6637, instance_loss: 0.0622, weighted_loss: 0.4833, label: 0, bag_size: 3474\n",
      "batch 79, loss: 0.5778, instance_loss: 0.0507, weighted_loss: 0.4197, label: 1, bag_size: 5991\n",
      "batch 99, loss: 0.8783, instance_loss: 5.7024, weighted_loss: 2.3255, label: 0, bag_size: 3802\n",
      "batch 119, loss: 0.7634, instance_loss: 0.0515, weighted_loss: 0.5498, label: 0, bag_size: 1909\n",
      "batch 139, loss: 0.7809, instance_loss: 0.0023, weighted_loss: 0.5473, label: 0, bag_size: 21682\n",
      "batch 159, loss: 0.8496, instance_loss: 0.1597, weighted_loss: 0.6426, label: 1, bag_size: 4862\n",
      "batch 179, loss: 0.8020, instance_loss: 0.1212, weighted_loss: 0.5978, label: 0, bag_size: 1458\n",
      "batch 199, loss: 0.8121, instance_loss: 0.6611, weighted_loss: 0.7668, label: 0, bag_size: 2918\n",
      "batch 219, loss: 0.5109, instance_loss: 0.0000, weighted_loss: 0.3576, label: 1, bag_size: 17486\n",
      "batch 239, loss: 0.4832, instance_loss: 0.0009, weighted_loss: 0.3385, label: 1, bag_size: 9955\n",
      "batch 259, loss: 0.8134, instance_loss: 0.0028, weighted_loss: 0.5702, label: 0, bag_size: 13218\n",
      "batch 279, loss: 0.7408, instance_loss: 0.9580, weighted_loss: 0.8060, label: 0, bag_size: 10410\n",
      "batch 299, loss: 0.7393, instance_loss: 0.0244, weighted_loss: 0.5248, label: 1, bag_size: 14202\n",
      "batch 319, loss: 0.7392, instance_loss: 0.0227, weighted_loss: 0.5242, label: 0, bag_size: 24382\n",
      "batch 339, loss: 0.8379, instance_loss: 1.1667, weighted_loss: 0.9365, label: 0, bag_size: 705\n",
      "batch 359, loss: 0.4740, instance_loss: 1.6265, weighted_loss: 0.8198, label: 1, bag_size: 14604\n",
      "batch 379, loss: 0.8383, instance_loss: 0.0139, weighted_loss: 0.5909, label: 0, bag_size: 11735\n",
      "batch 399, loss: 0.7297, instance_loss: 2.4673, weighted_loss: 1.2510, label: 1, bag_size: 2935\n",
      "batch 419, loss: 0.9635, instance_loss: 0.0049, weighted_loss: 0.6759, label: 0, bag_size: 21385\n",
      "batch 439, loss: 0.9056, instance_loss: 0.8222, weighted_loss: 0.8806, label: 0, bag_size: 3474\n",
      "batch 459, loss: 0.8571, instance_loss: 0.0108, weighted_loss: 0.6032, label: 0, bag_size: 4271\n",
      "batch 479, loss: 0.6658, instance_loss: 0.1300, weighted_loss: 0.5051, label: 1, bag_size: 1924\n",
      "batch 499, loss: 0.6283, instance_loss: 0.8919, weighted_loss: 0.7074, label: 1, bag_size: 1683\n",
      "batch 519, loss: 0.4001, instance_loss: 2.0324, weighted_loss: 0.8898, label: 1, bag_size: 2681\n",
      "batch 539, loss: 0.6667, instance_loss: 0.0100, weighted_loss: 0.4697, label: 1, bag_size: 16034\n",
      "batch 559, loss: 0.5939, instance_loss: 0.0243, weighted_loss: 0.4231, label: 1, bag_size: 5155\n",
      "batch 579, loss: 0.6251, instance_loss: 0.0000, weighted_loss: 0.4376, label: 1, bag_size: 15008\n",
      "batch 599, loss: 0.6883, instance_loss: 0.0015, weighted_loss: 0.4822, label: 1, bag_size: 7748\n",
      "batch 619, loss: 1.0266, instance_loss: 1.7882, weighted_loss: 1.2551, label: 0, bag_size: 2458\n",
      "batch 639, loss: 1.0667, instance_loss: 0.1912, weighted_loss: 0.8040, label: 0, bag_size: 30751\n",
      "batch 659, loss: 0.8753, instance_loss: 0.3197, weighted_loss: 0.7086, label: 0, bag_size: 14885\n",
      "batch 679, loss: 0.5816, instance_loss: 0.0034, weighted_loss: 0.4081, label: 1, bag_size: 13051\n",
      "batch 699, loss: 0.6857, instance_loss: 0.0812, weighted_loss: 0.5043, label: 0, bag_size: 16690\n",
      "batch 719, loss: 0.4470, instance_loss: 0.0216, weighted_loss: 0.3194, label: 0, bag_size: 18738\n",
      "batch 739, loss: 0.5663, instance_loss: 3.6119, weighted_loss: 1.4800, label: 0, bag_size: 2160\n",
      "batch 759, loss: 0.5207, instance_loss: 0.0047, weighted_loss: 0.3659, label: 0, bag_size: 18516\n",
      "batch 779, loss: 0.5205, instance_loss: 0.0180, weighted_loss: 0.3698, label: 0, bag_size: 18215\n",
      "batch 799, loss: 0.4637, instance_loss: 0.0111, weighted_loss: 0.3279, label: 0, bag_size: 15077\n",
      "batch 819, loss: 0.6713, instance_loss: 0.0004, weighted_loss: 0.4700, label: 0, bag_size: 21682\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9578771289537713: correct 12598/13152\n",
      "class 1 clustering acc 0.757147201946472: correct 4979/6576\n",
      "Epoch: 5, train_loss: 0.7007, train_clustering_loss:  0.4234, train_error: 0.4903\n",
      "class 0: acc 0.38944723618090454, correct 155/398\n",
      "class 1: acc 0.6226415094339622, correct 264/424\n",
      "\n",
      "Val Set, val_loss: 0.7174, val_error: 0.5780, auc: 0.9210\n",
      "class 0 clustering acc 0.9696100917431193: correct 1691/1744\n",
      "class 1 clustering acc 0.555045871559633: correct 484/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.0, correct 0/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5249, instance_loss: 0.0108, weighted_loss: 0.3707, label: 0, bag_size: 25558\n",
      "batch 39, loss: 0.7510, instance_loss: 1.7677, weighted_loss: 1.0560, label: 1, bag_size: 3980\n",
      "batch 59, loss: 1.0032, instance_loss: 0.7043, weighted_loss: 0.9136, label: 0, bag_size: 17791\n",
      "batch 79, loss: 0.5567, instance_loss: 0.0183, weighted_loss: 0.3952, label: 1, bag_size: 4789\n",
      "batch 99, loss: 0.5112, instance_loss: 0.0014, weighted_loss: 0.3582, label: 1, bag_size: 13368\n",
      "batch 119, loss: 0.6744, instance_loss: 0.2822, weighted_loss: 0.5567, label: 1, bag_size: 4039\n",
      "batch 139, loss: 0.7021, instance_loss: 0.1551, weighted_loss: 0.5380, label: 0, bag_size: 15003\n",
      "batch 159, loss: 0.6471, instance_loss: 0.0338, weighted_loss: 0.4631, label: 0, bag_size: 11113\n",
      "batch 179, loss: 0.6029, instance_loss: 0.7956, weighted_loss: 0.6607, label: 0, bag_size: 1416\n",
      "batch 199, loss: 0.6533, instance_loss: 0.1531, weighted_loss: 0.5032, label: 1, bag_size: 3450\n",
      "batch 219, loss: 0.7492, instance_loss: 0.0826, weighted_loss: 0.5492, label: 1, bag_size: 8522\n",
      "batch 239, loss: 0.5816, instance_loss: 0.1262, weighted_loss: 0.4450, label: 0, bag_size: 1072\n",
      "batch 259, loss: 1.0138, instance_loss: 0.1760, weighted_loss: 0.7625, label: 1, bag_size: 12408\n",
      "batch 279, loss: 0.7834, instance_loss: 0.0420, weighted_loss: 0.5610, label: 0, bag_size: 13225\n",
      "batch 299, loss: 0.7840, instance_loss: 1.4234, weighted_loss: 0.9759, label: 0, bag_size: 24382\n",
      "batch 319, loss: 0.4887, instance_loss: 0.0329, weighted_loss: 0.3520, label: 0, bag_size: 11151\n",
      "batch 339, loss: 0.9006, instance_loss: 0.0009, weighted_loss: 0.6307, label: 1, bag_size: 3557\n",
      "batch 359, loss: 0.7616, instance_loss: 0.0884, weighted_loss: 0.5596, label: 0, bag_size: 13023\n",
      "batch 379, loss: 0.6803, instance_loss: 0.0057, weighted_loss: 0.4779, label: 0, bag_size: 5225\n",
      "batch 399, loss: 0.4539, instance_loss: 0.6030, weighted_loss: 0.4986, label: 0, bag_size: 1437\n",
      "batch 419, loss: 0.6333, instance_loss: 0.0006, weighted_loss: 0.4435, label: 0, bag_size: 32227\n",
      "batch 439, loss: 0.6483, instance_loss: 0.0010, weighted_loss: 0.4541, label: 1, bag_size: 12425\n",
      "batch 459, loss: 0.8512, instance_loss: 2.4730, weighted_loss: 1.3377, label: 0, bag_size: 2732\n",
      "batch 479, loss: 0.7668, instance_loss: 0.0514, weighted_loss: 0.5522, label: 0, bag_size: 12217\n",
      "batch 499, loss: 0.6687, instance_loss: 0.2369, weighted_loss: 0.5391, label: 0, bag_size: 5211\n",
      "batch 519, loss: 0.4498, instance_loss: 0.0102, weighted_loss: 0.3180, label: 0, bag_size: 12217\n",
      "batch 539, loss: 0.4085, instance_loss: 0.0388, weighted_loss: 0.2976, label: 0, bag_size: 25558\n",
      "batch 559, loss: 0.7491, instance_loss: 0.0000, weighted_loss: 0.5244, label: 1, bag_size: 12460\n",
      "batch 579, loss: 0.6195, instance_loss: 0.0017, weighted_loss: 0.4341, label: 1, bag_size: 1360\n",
      "batch 599, loss: 0.6445, instance_loss: 0.0001, weighted_loss: 0.4512, label: 1, bag_size: 3207\n",
      "batch 619, loss: 0.7139, instance_loss: 0.0335, weighted_loss: 0.5098, label: 0, bag_size: 10263\n",
      "batch 639, loss: 0.5508, instance_loss: 0.0006, weighted_loss: 0.3857, label: 0, bag_size: 13218\n",
      "batch 659, loss: 0.9156, instance_loss: 0.0721, weighted_loss: 0.6626, label: 1, bag_size: 2344\n",
      "batch 679, loss: 0.5251, instance_loss: 0.5261, weighted_loss: 0.5254, label: 1, bag_size: 5110\n",
      "batch 699, loss: 0.8405, instance_loss: 0.6741, weighted_loss: 0.7906, label: 0, bag_size: 2732\n",
      "batch 719, loss: 0.5107, instance_loss: 0.0017, weighted_loss: 0.3580, label: 0, bag_size: 14319\n",
      "batch 739, loss: 0.3872, instance_loss: 0.0079, weighted_loss: 0.2734, label: 0, bag_size: 20478\n",
      "batch 759, loss: 0.4290, instance_loss: 0.2392, weighted_loss: 0.3720, label: 0, bag_size: 518\n",
      "batch 779, loss: 0.5820, instance_loss: 0.5982, weighted_loss: 0.5869, label: 0, bag_size: 13332\n",
      "batch 799, loss: 0.5273, instance_loss: 0.1641, weighted_loss: 0.4184, label: 0, bag_size: 2195\n",
      "batch 819, loss: 0.6917, instance_loss: 0.0017, weighted_loss: 0.4847, label: 1, bag_size: 21009\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9647201946472019: correct 12688/13152\n",
      "class 1 clustering acc 0.7606447688564477: correct 5002/6576\n",
      "Epoch: 6, train_loss: 0.7008, train_clustering_loss:  0.3789, train_error: 0.4878\n",
      "class 0: acc 0.6132075471698113, correct 260/424\n",
      "class 1: acc 0.4045226130653266, correct 161/398\n",
      "\n",
      "Val Set, val_loss: 0.6763, val_error: 0.1743, auc: 0.9220\n",
      "class 0 clustering acc 0.974197247706422: correct 1699/1744\n",
      "class 1 clustering acc 0.8451834862385321: correct 737/872\n",
      "class 0: acc 0.8478260869565217, correct 39/46\n",
      "class 1: acc 0.8095238095238095, correct 51/63\n",
      "Validation loss decreased (0.684085 --> 0.676310).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7455, instance_loss: 0.0090, weighted_loss: 0.5246, label: 0, bag_size: 10415\n",
      "batch 39, loss: 0.6127, instance_loss: 0.0114, weighted_loss: 0.4323, label: 0, bag_size: 2534\n",
      "batch 59, loss: 0.6719, instance_loss: 0.1022, weighted_loss: 0.5010, label: 0, bag_size: 11727\n",
      "batch 79, loss: 0.5398, instance_loss: 0.1897, weighted_loss: 0.4348, label: 1, bag_size: 20333\n",
      "batch 99, loss: 0.5322, instance_loss: 0.0206, weighted_loss: 0.3787, label: 0, bag_size: 16521\n",
      "batch 119, loss: 0.7594, instance_loss: 0.1058, weighted_loss: 0.5633, label: 1, bag_size: 7981\n",
      "batch 139, loss: 0.7964, instance_loss: 0.0051, weighted_loss: 0.5590, label: 1, bag_size: 10105\n",
      "batch 159, loss: 0.7683, instance_loss: 0.0071, weighted_loss: 0.5400, label: 1, bag_size: 3450\n",
      "batch 179, loss: 0.9033, instance_loss: 0.0409, weighted_loss: 0.6446, label: 0, bag_size: 11113\n",
      "batch 199, loss: 0.3902, instance_loss: 0.0511, weighted_loss: 0.2885, label: 1, bag_size: 5494\n",
      "batch 219, loss: 0.5156, instance_loss: 1.6710, weighted_loss: 0.8623, label: 1, bag_size: 2842\n",
      "batch 239, loss: 0.5935, instance_loss: 0.1130, weighted_loss: 0.4494, label: 1, bag_size: 10432\n",
      "batch 259, loss: 0.8122, instance_loss: 0.0844, weighted_loss: 0.5939, label: 1, bag_size: 16514\n",
      "batch 279, loss: 0.5717, instance_loss: 0.1440, weighted_loss: 0.4434, label: 0, bag_size: 1797\n",
      "batch 299, loss: 0.5740, instance_loss: 0.7767, weighted_loss: 0.6348, label: 0, bag_size: 17791\n",
      "batch 319, loss: 1.0669, instance_loss: 0.9739, weighted_loss: 1.0390, label: 0, bag_size: 16521\n",
      "batch 339, loss: 0.4855, instance_loss: 0.0000, weighted_loss: 0.3398, label: 1, bag_size: 10112\n",
      "batch 359, loss: 0.7071, instance_loss: 0.0066, weighted_loss: 0.4970, label: 0, bag_size: 10481\n",
      "batch 379, loss: 0.6499, instance_loss: 0.0019, weighted_loss: 0.4555, label: 0, bag_size: 11477\n",
      "batch 399, loss: 0.7048, instance_loss: 0.2963, weighted_loss: 0.5823, label: 0, bag_size: 23368\n",
      "batch 419, loss: 1.1403, instance_loss: 0.4524, weighted_loss: 0.9339, label: 0, bag_size: 2918\n",
      "batch 439, loss: 0.6609, instance_loss: 0.9903, weighted_loss: 0.7597, label: 1, bag_size: 11394\n",
      "batch 459, loss: 0.6951, instance_loss: 0.0761, weighted_loss: 0.5094, label: 0, bag_size: 1826\n",
      "batch 479, loss: 0.7970, instance_loss: 1.6432, weighted_loss: 1.0509, label: 1, bag_size: 13367\n",
      "batch 499, loss: 0.6749, instance_loss: 0.5908, weighted_loss: 0.6497, label: 0, bag_size: 5485\n",
      "batch 519, loss: 0.7934, instance_loss: 0.0433, weighted_loss: 0.5684, label: 1, bag_size: 1483\n",
      "batch 539, loss: 0.5432, instance_loss: 0.1734, weighted_loss: 0.4323, label: 0, bag_size: 2732\n",
      "batch 559, loss: 0.6046, instance_loss: 0.4952, weighted_loss: 0.5718, label: 1, bag_size: 11220\n",
      "batch 579, loss: 0.6477, instance_loss: 0.0021, weighted_loss: 0.4540, label: 0, bag_size: 12593\n",
      "batch 599, loss: 0.8915, instance_loss: 0.5206, weighted_loss: 0.7803, label: 1, bag_size: 1764\n",
      "batch 619, loss: 0.4974, instance_loss: 0.0052, weighted_loss: 0.3497, label: 1, bag_size: 16417\n",
      "batch 639, loss: 0.6231, instance_loss: 0.2061, weighted_loss: 0.4980, label: 1, bag_size: 8660\n",
      "batch 659, loss: 0.5839, instance_loss: 0.0001, weighted_loss: 0.4088, label: 0, bag_size: 11690\n",
      "batch 679, loss: 0.7538, instance_loss: 0.0594, weighted_loss: 0.5455, label: 0, bag_size: 3198\n",
      "batch 699, loss: 0.5377, instance_loss: 0.0067, weighted_loss: 0.3784, label: 0, bag_size: 6851\n",
      "batch 719, loss: 0.7223, instance_loss: 1.8615, weighted_loss: 1.0641, label: 0, bag_size: 2179\n",
      "batch 739, loss: 0.5547, instance_loss: 0.4956, weighted_loss: 0.5370, label: 0, bag_size: 1592\n",
      "batch 759, loss: 0.7406, instance_loss: 0.1021, weighted_loss: 0.5490, label: 1, bag_size: 9519\n",
      "batch 779, loss: 0.7403, instance_loss: 0.0176, weighted_loss: 0.5235, label: 1, bag_size: 5454\n",
      "batch 799, loss: 0.8789, instance_loss: 0.7893, weighted_loss: 0.8521, label: 0, bag_size: 12899\n",
      "batch 819, loss: 0.5679, instance_loss: 0.0544, weighted_loss: 0.4138, label: 0, bag_size: 2424\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9669251824817519: correct 12717/13152\n",
      "class 1 clustering acc 0.8070255474452555: correct 5307/6576\n",
      "Epoch: 7, train_loss: 0.6782, train_clustering_loss:  0.3334, train_error: 0.4246\n",
      "class 0: acc 0.5552825552825553, correct 226/407\n",
      "class 1: acc 0.5951807228915663, correct 247/415\n",
      "\n",
      "Val Set, val_loss: 0.6771, val_error: 0.5780, auc: 0.9265\n",
      "class 0 clustering acc 0.9736238532110092: correct 1698/1744\n",
      "class 1 clustering acc 0.8772935779816514: correct 765/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.0, correct 0/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6251, instance_loss: 0.5159, weighted_loss: 0.5924, label: 1, bag_size: 22286\n",
      "batch 39, loss: 0.8040, instance_loss: 0.1326, weighted_loss: 0.6025, label: 0, bag_size: 23996\n",
      "batch 59, loss: 0.8581, instance_loss: 0.1660, weighted_loss: 0.6504, label: 0, bag_size: 2219\n",
      "batch 79, loss: 0.6561, instance_loss: 0.0007, weighted_loss: 0.4595, label: 1, bag_size: 5690\n",
      "batch 99, loss: 0.5358, instance_loss: 0.0231, weighted_loss: 0.3820, label: 1, bag_size: 10033\n",
      "batch 119, loss: 0.5096, instance_loss: 0.0854, weighted_loss: 0.3824, label: 1, bag_size: 3557\n",
      "batch 139, loss: 0.7649, instance_loss: 0.0665, weighted_loss: 0.5554, label: 0, bag_size: 27158\n",
      "batch 159, loss: 0.4967, instance_loss: 0.0419, weighted_loss: 0.3603, label: 0, bag_size: 8661\n",
      "batch 179, loss: 0.8740, instance_loss: 0.0195, weighted_loss: 0.6176, label: 0, bag_size: 16087\n",
      "batch 199, loss: 0.3369, instance_loss: 0.0129, weighted_loss: 0.2397, label: 1, bag_size: 645\n",
      "batch 219, loss: 0.7472, instance_loss: 0.3251, weighted_loss: 0.6206, label: 0, bag_size: 9421\n",
      "batch 239, loss: 0.5512, instance_loss: 1.4433, weighted_loss: 0.8188, label: 1, bag_size: 21450\n",
      "batch 259, loss: 0.4018, instance_loss: 0.2134, weighted_loss: 0.3453, label: 1, bag_size: 3640\n",
      "batch 279, loss: 0.5504, instance_loss: 0.1691, weighted_loss: 0.4360, label: 0, bag_size: 21874\n",
      "batch 299, loss: 0.5525, instance_loss: 1.9343, weighted_loss: 0.9670, label: 1, bag_size: 1622\n",
      "batch 319, loss: 0.5538, instance_loss: 0.3902, weighted_loss: 0.5047, label: 1, bag_size: 12575\n",
      "batch 339, loss: 0.5604, instance_loss: 1.7793, weighted_loss: 0.9261, label: 0, bag_size: 15967\n",
      "batch 359, loss: 0.5712, instance_loss: 0.0691, weighted_loss: 0.4206, label: 1, bag_size: 12460\n",
      "batch 379, loss: 0.4654, instance_loss: 0.3162, weighted_loss: 0.4206, label: 1, bag_size: 8410\n",
      "batch 399, loss: 0.4118, instance_loss: 0.5905, weighted_loss: 0.4654, label: 0, bag_size: 1891\n",
      "batch 419, loss: 1.1311, instance_loss: 0.9123, weighted_loss: 1.0654, label: 1, bag_size: 1051\n",
      "batch 439, loss: 0.5316, instance_loss: 0.0091, weighted_loss: 0.3749, label: 1, bag_size: 8103\n",
      "batch 459, loss: 0.4749, instance_loss: 0.2012, weighted_loss: 0.3928, label: 1, bag_size: 8522\n",
      "batch 479, loss: 0.6636, instance_loss: 0.3989, weighted_loss: 0.5842, label: 0, bag_size: 2732\n",
      "batch 499, loss: 0.7147, instance_loss: 0.8188, weighted_loss: 0.7459, label: 1, bag_size: 1191\n",
      "batch 519, loss: 0.8607, instance_loss: 0.8343, weighted_loss: 0.8528, label: 0, bag_size: 1614\n",
      "batch 539, loss: 0.6529, instance_loss: 0.1061, weighted_loss: 0.4888, label: 0, bag_size: 10721\n",
      "batch 559, loss: 0.7096, instance_loss: 0.1783, weighted_loss: 0.5502, label: 0, bag_size: 21093\n",
      "batch 579, loss: 0.8175, instance_loss: 0.0002, weighted_loss: 0.5723, label: 1, bag_size: 11884\n",
      "batch 599, loss: 0.5850, instance_loss: 0.8421, weighted_loss: 0.6621, label: 0, bag_size: 6851\n",
      "batch 619, loss: 0.4888, instance_loss: 0.0080, weighted_loss: 0.3445, label: 1, bag_size: 5612\n",
      "batch 639, loss: 0.9258, instance_loss: 0.6169, weighted_loss: 0.8331, label: 1, bag_size: 4128\n",
      "batch 659, loss: 0.5581, instance_loss: 0.2847, weighted_loss: 0.4761, label: 0, bag_size: 3670\n",
      "batch 679, loss: 0.4338, instance_loss: 0.0434, weighted_loss: 0.3166, label: 0, bag_size: 8981\n",
      "batch 699, loss: 1.0279, instance_loss: 2.7741, weighted_loss: 1.5518, label: 1, bag_size: 3121\n",
      "batch 719, loss: 0.8355, instance_loss: 0.0627, weighted_loss: 0.6036, label: 0, bag_size: 14739\n",
      "batch 739, loss: 0.3470, instance_loss: 0.1194, weighted_loss: 0.2788, label: 1, bag_size: 7217\n",
      "batch 759, loss: 0.3301, instance_loss: 0.1707, weighted_loss: 0.2823, label: 1, bag_size: 8754\n",
      "batch 779, loss: 1.2258, instance_loss: 0.5514, weighted_loss: 1.0235, label: 0, bag_size: 1701\n",
      "batch 799, loss: 0.5250, instance_loss: 0.0652, weighted_loss: 0.3871, label: 1, bag_size: 3003\n",
      "batch 819, loss: 0.6765, instance_loss: 0.0024, weighted_loss: 0.4743, label: 1, bag_size: 15665\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9565845498783455: correct 12581/13152\n",
      "class 1 clustering acc 0.7572992700729927: correct 4980/6576\n",
      "Epoch: 8, train_loss: 0.6068, train_clustering_loss:  0.4352, train_error: 0.3029\n",
      "class 0: acc 0.6108247422680413, correct 237/388\n",
      "class 1: acc 0.7741935483870968, correct 336/434\n",
      "\n",
      "Val Set, val_loss: 0.5506, val_error: 0.1651, auc: 0.9251\n",
      "class 0 clustering acc 0.9518348623853211: correct 1660/1744\n",
      "class 1 clustering acc 0.7545871559633027: correct 658/872\n",
      "class 0: acc 0.8913043478260869, correct 41/46\n",
      "class 1: acc 0.7936507936507936, correct 50/63\n",
      "Validation loss decreased (0.676310 --> 0.550584).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5058, instance_loss: 0.0545, weighted_loss: 0.3704, label: 0, bag_size: 19043\n",
      "batch 39, loss: 0.4130, instance_loss: 0.0038, weighted_loss: 0.2903, label: 0, bag_size: 9786\n",
      "batch 59, loss: 0.5212, instance_loss: 0.0094, weighted_loss: 0.3676, label: 0, bag_size: 15967\n",
      "batch 79, loss: 0.7808, instance_loss: 4.9103, weighted_loss: 2.0197, label: 1, bag_size: 7424\n",
      "batch 99, loss: 0.2494, instance_loss: 0.1342, weighted_loss: 0.2148, label: 0, bag_size: 18240\n",
      "batch 119, loss: 0.9313, instance_loss: 0.6839, weighted_loss: 0.8571, label: 1, bag_size: 1172\n",
      "batch 139, loss: 1.0867, instance_loss: 3.2136, weighted_loss: 1.7248, label: 0, bag_size: 2213\n",
      "batch 159, loss: 0.4427, instance_loss: 0.0159, weighted_loss: 0.3147, label: 0, bag_size: 19470\n",
      "batch 179, loss: 0.4271, instance_loss: 0.0267, weighted_loss: 0.3070, label: 0, bag_size: 10898\n",
      "batch 199, loss: 0.6549, instance_loss: 0.1009, weighted_loss: 0.4887, label: 1, bag_size: 11160\n",
      "batch 219, loss: 0.5848, instance_loss: 0.0005, weighted_loss: 0.4095, label: 1, bag_size: 10920\n",
      "batch 239, loss: 0.8487, instance_loss: 0.3720, weighted_loss: 0.7057, label: 0, bag_size: 18738\n",
      "batch 259, loss: 0.1974, instance_loss: 0.0054, weighted_loss: 0.1398, label: 0, bag_size: 11259\n",
      "batch 279, loss: 0.3208, instance_loss: 0.0067, weighted_loss: 0.2266, label: 1, bag_size: 12425\n",
      "batch 299, loss: 0.5437, instance_loss: 0.0284, weighted_loss: 0.3891, label: 0, bag_size: 8866\n",
      "batch 319, loss: 0.5267, instance_loss: 0.0690, weighted_loss: 0.3894, label: 1, bag_size: 14604\n",
      "batch 339, loss: 0.4330, instance_loss: 0.3832, weighted_loss: 0.4181, label: 0, bag_size: 1984\n",
      "batch 359, loss: 0.2848, instance_loss: 0.1939, weighted_loss: 0.2575, label: 0, bag_size: 8812\n",
      "batch 379, loss: 0.1583, instance_loss: 0.0035, weighted_loss: 0.1118, label: 0, bag_size: 11546\n",
      "batch 399, loss: 1.0240, instance_loss: 4.5168, weighted_loss: 2.0718, label: 0, bag_size: 17279\n",
      "batch 419, loss: 0.4581, instance_loss: 0.0524, weighted_loss: 0.3364, label: 0, bag_size: 15077\n",
      "batch 439, loss: 0.3088, instance_loss: 1.8655, weighted_loss: 0.7758, label: 1, bag_size: 9404\n",
      "batch 459, loss: 0.5823, instance_loss: 0.0115, weighted_loss: 0.4110, label: 0, bag_size: 14828\n",
      "batch 479, loss: 0.5647, instance_loss: 1.4820, weighted_loss: 0.8399, label: 1, bag_size: 4929\n",
      "batch 499, loss: 0.2835, instance_loss: 0.5668, weighted_loss: 0.3685, label: 0, bag_size: 10721\n",
      "batch 519, loss: 0.3564, instance_loss: 0.0154, weighted_loss: 0.2541, label: 0, bag_size: 21682\n",
      "batch 539, loss: 1.2299, instance_loss: 0.4670, weighted_loss: 1.0010, label: 1, bag_size: 13089\n",
      "batch 559, loss: 0.2121, instance_loss: 0.0037, weighted_loss: 0.1495, label: 0, bag_size: 21576\n",
      "batch 579, loss: 1.0061, instance_loss: 1.6253, weighted_loss: 1.1919, label: 0, bag_size: 2732\n",
      "batch 599, loss: 0.2077, instance_loss: 0.1484, weighted_loss: 0.1899, label: 0, bag_size: 1438\n",
      "batch 619, loss: 1.2454, instance_loss: 0.2014, weighted_loss: 0.9322, label: 1, bag_size: 7381\n",
      "batch 639, loss: 0.9812, instance_loss: 0.8835, weighted_loss: 0.9519, label: 0, bag_size: 3399\n",
      "batch 659, loss: 0.6922, instance_loss: 0.3477, weighted_loss: 0.5889, label: 1, bag_size: 1038\n",
      "batch 679, loss: 0.6607, instance_loss: 0.4983, weighted_loss: 0.6120, label: 0, bag_size: 1437\n",
      "batch 699, loss: 0.8244, instance_loss: 0.2246, weighted_loss: 0.6445, label: 1, bag_size: 4394\n",
      "batch 719, loss: 0.2542, instance_loss: 0.1724, weighted_loss: 0.2296, label: 0, bag_size: 3459\n",
      "batch 739, loss: 0.6840, instance_loss: 0.6409, weighted_loss: 0.6711, label: 0, bag_size: 8420\n",
      "batch 759, loss: 0.4421, instance_loss: 0.1833, weighted_loss: 0.3645, label: 1, bag_size: 5921\n",
      "batch 779, loss: 0.2636, instance_loss: 0.2989, weighted_loss: 0.2742, label: 1, bag_size: 11266\n",
      "batch 799, loss: 0.1850, instance_loss: 0.1077, weighted_loss: 0.1618, label: 1, bag_size: 5345\n",
      "batch 819, loss: 0.3502, instance_loss: 0.0087, weighted_loss: 0.2478, label: 0, bag_size: 11917\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9613746958637469: correct 12644/13152\n",
      "class 1 clustering acc 0.7311435523114356: correct 4808/6576\n",
      "Epoch: 9, train_loss: 0.5310, train_clustering_loss:  0.4304, train_error: 0.2202\n",
      "class 0: acc 0.7962085308056872, correct 336/422\n",
      "class 1: acc 0.7625, correct 305/400\n",
      "\n",
      "Val Set, val_loss: 0.4561, val_error: 0.1468, auc: 0.9275\n",
      "class 0 clustering acc 0.9592889908256881: correct 1673/1744\n",
      "class 1 clustering acc 0.8497706422018348: correct 741/872\n",
      "class 0: acc 0.9130434782608695, correct 42/46\n",
      "class 1: acc 0.8095238095238095, correct 51/63\n",
      "Validation loss decreased (0.550584 --> 0.456136).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 1.1270, instance_loss: 1.5378, weighted_loss: 1.2502, label: 0, bag_size: 4997\n",
      "batch 39, loss: 0.7021, instance_loss: 0.3756, weighted_loss: 0.6042, label: 0, bag_size: 2148\n",
      "batch 59, loss: 2.4237, instance_loss: 1.8304, weighted_loss: 2.2457, label: 1, bag_size: 2565\n",
      "batch 79, loss: 0.1959, instance_loss: 0.0581, weighted_loss: 0.1546, label: 1, bag_size: 10969\n",
      "batch 99, loss: 0.4142, instance_loss: 0.3014, weighted_loss: 0.3804, label: 0, bag_size: 10029\n",
      "batch 119, loss: 0.3867, instance_loss: 0.1510, weighted_loss: 0.3160, label: 0, bag_size: 2195\n",
      "batch 139, loss: 0.2489, instance_loss: 0.5538, weighted_loss: 0.3404, label: 0, bag_size: 1052\n",
      "batch 159, loss: 0.1267, instance_loss: 0.0001, weighted_loss: 0.0888, label: 0, bag_size: 17791\n",
      "batch 179, loss: 0.5829, instance_loss: 0.0027, weighted_loss: 0.4088, label: 1, bag_size: 6734\n",
      "batch 199, loss: 0.3888, instance_loss: 0.0002, weighted_loss: 0.2722, label: 1, bag_size: 18468\n",
      "batch 219, loss: 0.1020, instance_loss: 0.1334, weighted_loss: 0.1114, label: 0, bag_size: 17437\n",
      "batch 239, loss: 0.7372, instance_loss: 1.1119, weighted_loss: 0.8496, label: 1, bag_size: 1191\n",
      "batch 259, loss: 0.2927, instance_loss: 0.3346, weighted_loss: 0.3053, label: 1, bag_size: 4239\n",
      "batch 279, loss: 0.3807, instance_loss: 0.0836, weighted_loss: 0.2916, label: 0, bag_size: 22681\n",
      "batch 299, loss: 0.9541, instance_loss: 1.6830, weighted_loss: 1.1728, label: 0, bag_size: 6281\n",
      "batch 319, loss: 0.1185, instance_loss: 0.0013, weighted_loss: 0.0834, label: 0, bag_size: 12137\n",
      "batch 339, loss: 0.3111, instance_loss: 0.0218, weighted_loss: 0.2243, label: 1, bag_size: 12795\n",
      "batch 359, loss: 0.2696, instance_loss: 0.7820, weighted_loss: 0.4233, label: 1, bag_size: 7148\n",
      "batch 379, loss: 0.1873, instance_loss: 0.2563, weighted_loss: 0.2080, label: 1, bag_size: 11223\n",
      "batch 399, loss: 0.2037, instance_loss: 0.0015, weighted_loss: 0.1430, label: 1, bag_size: 16417\n",
      "batch 419, loss: 0.2259, instance_loss: 0.0106, weighted_loss: 0.1613, label: 1, bag_size: 7445\n",
      "batch 439, loss: 0.7012, instance_loss: 0.5741, weighted_loss: 0.6631, label: 0, bag_size: 2236\n",
      "batch 459, loss: 0.2055, instance_loss: 0.0921, weighted_loss: 0.1715, label: 0, bag_size: 11146\n",
      "batch 479, loss: 0.1858, instance_loss: 0.0144, weighted_loss: 0.1344, label: 0, bag_size: 13795\n",
      "batch 499, loss: 0.4987, instance_loss: 0.0114, weighted_loss: 0.3525, label: 0, bag_size: 6850\n",
      "batch 519, loss: 0.1925, instance_loss: 0.2888, weighted_loss: 0.2214, label: 1, bag_size: 11701\n",
      "batch 539, loss: 0.7974, instance_loss: 0.3216, weighted_loss: 0.6546, label: 1, bag_size: 1483\n",
      "batch 559, loss: 0.6057, instance_loss: 2.2934, weighted_loss: 1.1120, label: 0, bag_size: 9616\n",
      "batch 579, loss: 0.3660, instance_loss: 0.0036, weighted_loss: 0.2573, label: 1, bag_size: 20161\n",
      "batch 599, loss: 0.5333, instance_loss: 0.0145, weighted_loss: 0.3777, label: 0, bag_size: 14377\n",
      "batch 619, loss: 0.7351, instance_loss: 0.5804, weighted_loss: 0.6887, label: 0, bag_size: 1498\n",
      "batch 639, loss: 2.3695, instance_loss: 8.7149, weighted_loss: 4.2731, label: 0, bag_size: 17279\n",
      "batch 659, loss: 0.0494, instance_loss: 0.2361, weighted_loss: 0.1054, label: 1, bag_size: 7513\n",
      "batch 679, loss: 0.1585, instance_loss: 0.3252, weighted_loss: 0.2085, label: 1, bag_size: 1249\n",
      "batch 699, loss: 0.6532, instance_loss: 1.3560, weighted_loss: 0.8640, label: 1, bag_size: 1038\n",
      "batch 719, loss: 0.5369, instance_loss: 1.1853, weighted_loss: 0.7314, label: 1, bag_size: 12714\n",
      "batch 739, loss: 0.1295, instance_loss: 0.0715, weighted_loss: 0.1121, label: 0, bag_size: 3190\n",
      "batch 759, loss: 0.4024, instance_loss: 0.0627, weighted_loss: 0.3005, label: 0, bag_size: 12131\n",
      "batch 779, loss: 0.0891, instance_loss: 0.0001, weighted_loss: 0.0624, label: 1, bag_size: 5612\n",
      "batch 799, loss: 0.3432, instance_loss: 0.4257, weighted_loss: 0.3680, label: 1, bag_size: 9446\n",
      "batch 819, loss: 0.1669, instance_loss: 0.0000, weighted_loss: 0.1169, label: 1, bag_size: 12349\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9596259124087592: correct 12621/13152\n",
      "class 1 clustering acc 0.7680961070559611: correct 5051/6576\n",
      "Epoch: 10, train_loss: 0.4297, train_clustering_loss:  0.3969, train_error: 0.1557\n",
      "class 0: acc 0.8186528497409327, correct 316/386\n",
      "class 1: acc 0.8669724770642202, correct 378/436\n",
      "\n",
      "Val Set, val_loss: 0.3941, val_error: 0.1468, auc: 0.9282\n",
      "class 0 clustering acc 0.9598623853211009: correct 1674/1744\n",
      "class 1 clustering acc 0.8256880733944955: correct 720/872\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.7936507936507936, correct 50/63\n",
      "Validation loss decreased (0.456136 --> 0.394067).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.8785, instance_loss: 1.4516, weighted_loss: 1.0504, label: 1, bag_size: 1444\n",
      "batch 39, loss: 0.1112, instance_loss: 0.0000, weighted_loss: 0.0778, label: 0, bag_size: 5225\n",
      "batch 59, loss: 0.0802, instance_loss: 0.0000, weighted_loss: 0.0561, label: 1, bag_size: 4877\n",
      "batch 79, loss: 0.0518, instance_loss: 0.0007, weighted_loss: 0.0365, label: 1, bag_size: 11122\n",
      "batch 99, loss: 0.4570, instance_loss: 2.1693, weighted_loss: 0.9707, label: 1, bag_size: 9215\n",
      "batch 119, loss: 0.6730, instance_loss: 0.0769, weighted_loss: 0.4941, label: 0, bag_size: 29270\n",
      "batch 139, loss: 0.2026, instance_loss: 0.0263, weighted_loss: 0.1497, label: 1, bag_size: 12895\n",
      "batch 159, loss: 0.5924, instance_loss: 0.5414, weighted_loss: 0.5771, label: 0, bag_size: 3725\n",
      "batch 179, loss: 0.2140, instance_loss: 0.0000, weighted_loss: 0.1498, label: 1, bag_size: 12408\n",
      "batch 199, loss: 0.6094, instance_loss: 0.0005, weighted_loss: 0.4267, label: 0, bag_size: 19808\n",
      "batch 219, loss: 1.4439, instance_loss: 1.1201, weighted_loss: 1.3468, label: 1, bag_size: 6478\n",
      "batch 239, loss: 0.0714, instance_loss: 0.0024, weighted_loss: 0.0507, label: 0, bag_size: 8252\n",
      "batch 259, loss: 0.6762, instance_loss: 0.0080, weighted_loss: 0.4757, label: 1, bag_size: 10396\n",
      "batch 279, loss: 0.6992, instance_loss: 0.2515, weighted_loss: 0.5649, label: 0, bag_size: 2070\n",
      "batch 299, loss: 0.7947, instance_loss: 0.1190, weighted_loss: 0.5920, label: 1, bag_size: 1459\n",
      "batch 319, loss: 0.1324, instance_loss: 0.0081, weighted_loss: 0.0951, label: 0, bag_size: 11917\n",
      "batch 339, loss: 0.0834, instance_loss: 0.1598, weighted_loss: 0.1063, label: 0, bag_size: 19466\n",
      "batch 359, loss: 0.7721, instance_loss: 1.1703, weighted_loss: 0.8915, label: 0, bag_size: 2653\n",
      "batch 379, loss: 0.2505, instance_loss: 0.0249, weighted_loss: 0.1828, label: 1, bag_size: 21827\n",
      "batch 399, loss: 0.2313, instance_loss: 0.1810, weighted_loss: 0.2162, label: 0, bag_size: 16720\n",
      "batch 419, loss: 0.1742, instance_loss: 0.1420, weighted_loss: 0.1646, label: 0, bag_size: 15464\n",
      "batch 439, loss: 0.4230, instance_loss: 0.2218, weighted_loss: 0.3626, label: 1, bag_size: 5110\n",
      "batch 459, loss: 0.2690, instance_loss: 0.2458, weighted_loss: 0.2620, label: 0, bag_size: 1881\n",
      "batch 479, loss: 1.2693, instance_loss: 0.4712, weighted_loss: 1.0299, label: 1, bag_size: 1316\n",
      "batch 499, loss: 0.0313, instance_loss: 0.0019, weighted_loss: 0.0224, label: 0, bag_size: 13964\n",
      "batch 519, loss: 0.3664, instance_loss: 1.2087, weighted_loss: 0.6191, label: 0, bag_size: 12083\n",
      "batch 539, loss: 0.0377, instance_loss: 0.0800, weighted_loss: 0.0504, label: 1, bag_size: 8466\n",
      "batch 559, loss: 0.0244, instance_loss: 0.0000, weighted_loss: 0.0171, label: 0, bag_size: 8372\n",
      "batch 579, loss: 0.1115, instance_loss: 0.0391, weighted_loss: 0.0898, label: 0, bag_size: 20910\n",
      "batch 599, loss: 0.9197, instance_loss: 0.0290, weighted_loss: 0.6525, label: 1, bag_size: 12946\n",
      "batch 619, loss: 0.0883, instance_loss: 0.0000, weighted_loss: 0.0618, label: 0, bag_size: 31106\n",
      "batch 639, loss: 0.6316, instance_loss: 0.7658, weighted_loss: 0.6718, label: 1, bag_size: 1255\n",
      "batch 659, loss: 0.3997, instance_loss: 0.3197, weighted_loss: 0.3757, label: 1, bag_size: 5310\n",
      "batch 679, loss: 0.1286, instance_loss: 0.0130, weighted_loss: 0.0939, label: 1, bag_size: 15689\n",
      "batch 699, loss: 0.2846, instance_loss: 0.0668, weighted_loss: 0.2193, label: 0, bag_size: 6093\n",
      "batch 719, loss: 0.1072, instance_loss: 0.0016, weighted_loss: 0.0755, label: 1, bag_size: 13026\n",
      "batch 739, loss: 0.2161, instance_loss: 0.0008, weighted_loss: 0.1515, label: 0, bag_size: 9471\n",
      "batch 759, loss: 0.0928, instance_loss: 0.1131, weighted_loss: 0.0989, label: 0, bag_size: 13777\n",
      "batch 779, loss: 0.6871, instance_loss: 0.2331, weighted_loss: 0.5509, label: 1, bag_size: 21252\n",
      "batch 799, loss: 0.0614, instance_loss: 0.0011, weighted_loss: 0.0433, label: 1, bag_size: 4102\n",
      "batch 819, loss: 0.0480, instance_loss: 0.0823, weighted_loss: 0.0583, label: 0, bag_size: 1234\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.965404501216545: correct 12697/13152\n",
      "class 1 clustering acc 0.8044403892944039: correct 5290/6576\n",
      "Epoch: 11, train_loss: 0.3684, train_clustering_loss:  0.3443, train_error: 0.1363\n",
      "class 0: acc 0.8658536585365854, correct 355/410\n",
      "class 1: acc 0.8616504854368932, correct 355/412\n",
      "\n",
      "Val Set, val_loss: 0.5316, val_error: 0.2018, auc: 0.9282\n",
      "class 0 clustering acc 0.8652522935779816: correct 1509/1744\n",
      "class 1 clustering acc 0.6272935779816514: correct 547/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.6507936507936508, correct 41/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6000, instance_loss: 0.6254, weighted_loss: 0.6076, label: 1, bag_size: 1924\n",
      "batch 39, loss: 0.4509, instance_loss: 0.2085, weighted_loss: 0.3782, label: 0, bag_size: 7141\n",
      "batch 59, loss: 0.5943, instance_loss: 0.3147, weighted_loss: 0.5104, label: 0, bag_size: 7141\n",
      "batch 79, loss: 0.4888, instance_loss: 0.0030, weighted_loss: 0.3431, label: 0, bag_size: 20478\n",
      "batch 99, loss: 0.2855, instance_loss: 0.0923, weighted_loss: 0.2275, label: 1, bag_size: 2559\n",
      "batch 119, loss: 0.0537, instance_loss: 0.2524, weighted_loss: 0.1133, label: 0, bag_size: 3787\n",
      "batch 139, loss: 1.0061, instance_loss: 0.5418, weighted_loss: 0.8668, label: 1, bag_size: 13089\n",
      "batch 159, loss: 0.2268, instance_loss: 0.0013, weighted_loss: 0.1591, label: 1, bag_size: 12349\n",
      "batch 179, loss: 0.0853, instance_loss: 0.0013, weighted_loss: 0.0601, label: 0, bag_size: 10898\n",
      "batch 199, loss: 0.2069, instance_loss: 0.0041, weighted_loss: 0.1461, label: 1, bag_size: 12931\n",
      "batch 219, loss: 0.2751, instance_loss: 0.1701, weighted_loss: 0.2436, label: 0, bag_size: 17630\n",
      "batch 239, loss: 0.4402, instance_loss: 0.3644, weighted_loss: 0.4175, label: 1, bag_size: 1875\n",
      "batch 259, loss: 0.1359, instance_loss: 0.0000, weighted_loss: 0.0951, label: 0, bag_size: 21138\n",
      "batch 279, loss: 0.1228, instance_loss: 0.1103, weighted_loss: 0.1190, label: 0, bag_size: 3101\n",
      "batch 299, loss: 0.0341, instance_loss: 0.0042, weighted_loss: 0.0251, label: 0, bag_size: 21082\n",
      "batch 319, loss: 0.7709, instance_loss: 0.7401, weighted_loss: 0.7617, label: 0, bag_size: 2213\n",
      "batch 339, loss: 0.2354, instance_loss: 0.0052, weighted_loss: 0.1664, label: 1, bag_size: 20161\n",
      "batch 359, loss: 0.1205, instance_loss: 0.0088, weighted_loss: 0.0870, label: 0, bag_size: 9885\n",
      "batch 379, loss: 0.8194, instance_loss: 2.7855, weighted_loss: 1.4092, label: 1, bag_size: 2935\n",
      "batch 399, loss: 0.5405, instance_loss: 0.1651, weighted_loss: 0.4278, label: 1, bag_size: 17579\n",
      "batch 419, loss: 0.2626, instance_loss: 0.0806, weighted_loss: 0.2080, label: 1, bag_size: 14202\n",
      "batch 439, loss: 0.3128, instance_loss: 0.3578, weighted_loss: 0.3263, label: 1, bag_size: 6825\n",
      "batch 459, loss: 0.6479, instance_loss: 0.8565, weighted_loss: 0.7105, label: 1, bag_size: 14604\n",
      "batch 479, loss: 0.0391, instance_loss: 0.0297, weighted_loss: 0.0363, label: 1, bag_size: 9877\n",
      "batch 499, loss: 0.1204, instance_loss: 0.0214, weighted_loss: 0.0907, label: 0, bag_size: 8866\n",
      "batch 519, loss: 0.9099, instance_loss: 0.6031, weighted_loss: 0.8179, label: 0, bag_size: 2968\n",
      "batch 539, loss: 0.0679, instance_loss: 0.0006, weighted_loss: 0.0477, label: 0, bag_size: 31106\n",
      "batch 559, loss: 0.9655, instance_loss: 2.6423, weighted_loss: 1.4685, label: 1, bag_size: 15563\n",
      "batch 579, loss: 0.1097, instance_loss: 0.2683, weighted_loss: 0.1573, label: 0, bag_size: 19518\n",
      "batch 599, loss: 0.6929, instance_loss: 0.0729, weighted_loss: 0.5069, label: 1, bag_size: 6682\n",
      "batch 619, loss: 0.2196, instance_loss: 0.0006, weighted_loss: 0.1539, label: 0, bag_size: 13777\n",
      "batch 639, loss: 0.0302, instance_loss: 0.0000, weighted_loss: 0.0211, label: 1, bag_size: 13368\n",
      "batch 659, loss: 0.1970, instance_loss: 0.0002, weighted_loss: 0.1379, label: 1, bag_size: 10725\n",
      "batch 679, loss: 0.0473, instance_loss: 0.0654, weighted_loss: 0.0527, label: 1, bag_size: 6171\n",
      "batch 699, loss: 0.4386, instance_loss: 0.6613, weighted_loss: 0.5054, label: 1, bag_size: 1230\n",
      "batch 719, loss: 0.1503, instance_loss: 0.3381, weighted_loss: 0.2067, label: 1, bag_size: 4250\n",
      "batch 739, loss: 0.0541, instance_loss: 0.0000, weighted_loss: 0.0379, label: 0, bag_size: 16690\n",
      "batch 759, loss: 0.1023, instance_loss: 0.0198, weighted_loss: 0.0775, label: 1, bag_size: 8660\n",
      "batch 779, loss: 0.2291, instance_loss: 0.0205, weighted_loss: 0.1665, label: 1, bag_size: 6781\n",
      "batch 799, loss: 0.1699, instance_loss: 0.2240, weighted_loss: 0.1861, label: 0, bag_size: 763\n",
      "batch 819, loss: 0.0789, instance_loss: 0.0042, weighted_loss: 0.0565, label: 0, bag_size: 17633\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9640358880778589: correct 12679/13152\n",
      "class 1 clustering acc 0.7782846715328468: correct 5118/6576\n",
      "Epoch: 12, train_loss: 0.3938, train_clustering_loss:  0.3502, train_error: 0.1582\n",
      "class 0: acc 0.8278481012658228, correct 327/395\n",
      "class 1: acc 0.8548009367681498, correct 365/427\n",
      "\n",
      "Val Set, val_loss: 0.3561, val_error: 0.1560, auc: 0.9279\n",
      "class 0 clustering acc 0.9747706422018348: correct 1700/1744\n",
      "class 1 clustering acc 0.8027522935779816: correct 700/872\n",
      "class 0: acc 0.9130434782608695, correct 42/46\n",
      "class 1: acc 0.7936507936507936, correct 50/63\n",
      "Validation loss decreased (0.394067 --> 0.356083).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0481, instance_loss: 0.0002, weighted_loss: 0.0338, label: 0, bag_size: 11187\n",
      "batch 39, loss: 0.6910, instance_loss: 0.8318, weighted_loss: 0.7332, label: 1, bag_size: 1794\n",
      "batch 59, loss: 0.0514, instance_loss: 0.0069, weighted_loss: 0.0380, label: 1, bag_size: 15689\n",
      "batch 79, loss: 0.0663, instance_loss: 0.0002, weighted_loss: 0.0465, label: 0, bag_size: 23398\n",
      "batch 99, loss: 0.0323, instance_loss: 0.0000, weighted_loss: 0.0226, label: 1, bag_size: 9955\n",
      "batch 119, loss: 0.2759, instance_loss: 0.0014, weighted_loss: 0.1936, label: 1, bag_size: 7389\n",
      "batch 139, loss: 0.2878, instance_loss: 0.0001, weighted_loss: 0.2015, label: 0, bag_size: 19043\n",
      "batch 159, loss: 0.0469, instance_loss: 0.0032, weighted_loss: 0.0338, label: 0, bag_size: 11527\n",
      "batch 179, loss: 0.0417, instance_loss: 0.0008, weighted_loss: 0.0294, label: 1, bag_size: 11684\n",
      "batch 199, loss: 0.1679, instance_loss: 0.0646, weighted_loss: 0.1369, label: 1, bag_size: 5833\n",
      "batch 219, loss: 0.0794, instance_loss: 0.0006, weighted_loss: 0.0557, label: 0, bag_size: 13777\n",
      "batch 239, loss: 1.5645, instance_loss: 0.9003, weighted_loss: 1.3652, label: 1, bag_size: 1963\n",
      "batch 259, loss: 0.0408, instance_loss: 0.0001, weighted_loss: 0.0286, label: 1, bag_size: 20537\n",
      "batch 279, loss: 0.1231, instance_loss: 0.0964, weighted_loss: 0.1151, label: 1, bag_size: 2308\n",
      "batch 299, loss: 0.1184, instance_loss: 0.4418, weighted_loss: 0.2154, label: 1, bag_size: 14681\n",
      "batch 319, loss: 0.1063, instance_loss: 0.0383, weighted_loss: 0.0859, label: 1, bag_size: 2412\n",
      "batch 339, loss: 0.2287, instance_loss: 0.0067, weighted_loss: 0.1621, label: 1, bag_size: 6842\n",
      "batch 359, loss: 0.1768, instance_loss: 1.3058, weighted_loss: 0.5155, label: 1, bag_size: 7246\n",
      "batch 379, loss: 0.0525, instance_loss: 0.0003, weighted_loss: 0.0369, label: 0, bag_size: 14319\n",
      "batch 399, loss: 0.3117, instance_loss: 0.0849, weighted_loss: 0.2437, label: 1, bag_size: 12178\n",
      "batch 419, loss: 0.0633, instance_loss: 0.0000, weighted_loss: 0.0443, label: 0, bag_size: 14956\n",
      "batch 439, loss: 0.0288, instance_loss: 1.2821, weighted_loss: 0.4048, label: 1, bag_size: 9548\n",
      "batch 459, loss: 0.1468, instance_loss: 0.1262, weighted_loss: 0.1407, label: 0, bag_size: 2036\n",
      "batch 479, loss: 0.0660, instance_loss: 0.0002, weighted_loss: 0.0463, label: 0, bag_size: 31106\n",
      "batch 499, loss: 0.7664, instance_loss: 1.3651, weighted_loss: 0.9460, label: 0, bag_size: 1690\n",
      "batch 519, loss: 1.6804, instance_loss: 3.2617, weighted_loss: 2.1548, label: 1, bag_size: 15192\n",
      "batch 539, loss: 0.0387, instance_loss: 0.8236, weighted_loss: 0.2742, label: 1, bag_size: 11266\n",
      "batch 559, loss: 0.0988, instance_loss: 0.0005, weighted_loss: 0.0693, label: 1, bag_size: 16051\n",
      "batch 579, loss: 0.0261, instance_loss: 0.2857, weighted_loss: 0.1040, label: 1, bag_size: 3224\n",
      "batch 599, loss: 0.1429, instance_loss: 0.0055, weighted_loss: 0.1017, label: 1, bag_size: 4259\n",
      "batch 619, loss: 0.1518, instance_loss: 0.0844, weighted_loss: 0.1316, label: 0, bag_size: 14681\n",
      "batch 639, loss: 0.5022, instance_loss: 0.4472, weighted_loss: 0.4857, label: 0, bag_size: 3321\n",
      "batch 659, loss: 0.0343, instance_loss: 0.0065, weighted_loss: 0.0260, label: 0, bag_size: 5225\n",
      "batch 679, loss: 0.0865, instance_loss: 0.0010, weighted_loss: 0.0608, label: 1, bag_size: 8216\n",
      "batch 699, loss: 0.1230, instance_loss: 0.0000, weighted_loss: 0.0861, label: 1, bag_size: 15093\n",
      "batch 719, loss: 1.3462, instance_loss: 0.9690, weighted_loss: 1.2330, label: 0, bag_size: 1701\n",
      "batch 739, loss: 0.3993, instance_loss: 0.0093, weighted_loss: 0.2823, label: 0, bag_size: 5999\n",
      "batch 759, loss: 0.1156, instance_loss: 0.0673, weighted_loss: 0.1011, label: 1, bag_size: 15008\n",
      "batch 779, loss: 0.3997, instance_loss: 0.2587, weighted_loss: 0.3574, label: 0, bag_size: 8427\n",
      "batch 799, loss: 0.0354, instance_loss: 0.0067, weighted_loss: 0.0268, label: 0, bag_size: 22762\n",
      "batch 819, loss: 0.0268, instance_loss: 0.0027, weighted_loss: 0.0196, label: 0, bag_size: 9060\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9670012165450121: correct 12718/13152\n",
      "class 1 clustering acc 0.8102189781021898: correct 5328/6576\n",
      "Epoch: 13, train_loss: 0.3143, train_clustering_loss:  0.3485, train_error: 0.1302\n",
      "class 0: acc 0.8798076923076923, correct 366/416\n",
      "class 1: acc 0.8596059113300493, correct 349/406\n",
      "\n",
      "Val Set, val_loss: 0.4113, val_error: 0.1743, auc: 0.9282\n",
      "class 0 clustering acc 0.9782110091743119: correct 1706/1744\n",
      "class 1 clustering acc 0.8440366972477065: correct 736/872\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.7301587301587301, correct 46/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0948, instance_loss: 0.1035, weighted_loss: 0.0974, label: 1, bag_size: 6842\n",
      "batch 39, loss: 0.1099, instance_loss: 0.0006, weighted_loss: 0.0771, label: 0, bag_size: 16521\n",
      "batch 59, loss: 0.5236, instance_loss: 0.0118, weighted_loss: 0.3700, label: 1, bag_size: 6533\n",
      "batch 79, loss: 0.3411, instance_loss: 0.0833, weighted_loss: 0.2638, label: 0, bag_size: 8959\n",
      "batch 99, loss: 0.0875, instance_loss: 0.0209, weighted_loss: 0.0675, label: 1, bag_size: 10920\n",
      "batch 119, loss: 0.1564, instance_loss: 0.1474, weighted_loss: 0.1537, label: 0, bag_size: 16211\n",
      "batch 139, loss: 0.3896, instance_loss: 0.2192, weighted_loss: 0.3385, label: 1, bag_size: 2092\n",
      "batch 159, loss: 0.0683, instance_loss: 0.0510, weighted_loss: 0.0631, label: 1, bag_size: 17769\n",
      "batch 179, loss: 0.0468, instance_loss: 0.0087, weighted_loss: 0.0354, label: 1, bag_size: 10028\n",
      "batch 199, loss: 0.0236, instance_loss: 0.0942, weighted_loss: 0.0448, label: 1, bag_size: 7148\n",
      "batch 219, loss: 0.0456, instance_loss: 0.0093, weighted_loss: 0.0347, label: 1, bag_size: 11160\n",
      "batch 239, loss: 0.3487, instance_loss: 0.1177, weighted_loss: 0.2794, label: 1, bag_size: 3674\n",
      "batch 259, loss: 0.0793, instance_loss: 0.0841, weighted_loss: 0.0807, label: 0, bag_size: 9885\n",
      "batch 279, loss: 0.1259, instance_loss: 0.0001, weighted_loss: 0.0882, label: 0, bag_size: 9455\n",
      "batch 299, loss: 0.8288, instance_loss: 1.6898, weighted_loss: 1.0871, label: 1, bag_size: 1794\n",
      "batch 319, loss: 0.2216, instance_loss: 0.0008, weighted_loss: 0.1554, label: 0, bag_size: 8959\n",
      "batch 339, loss: 0.0982, instance_loss: 0.0038, weighted_loss: 0.0699, label: 1, bag_size: 11032\n",
      "batch 359, loss: 0.0408, instance_loss: 0.0002, weighted_loss: 0.0286, label: 1, bag_size: 8660\n",
      "batch 379, loss: 0.6649, instance_loss: 0.3060, weighted_loss: 0.5572, label: 1, bag_size: 16703\n",
      "batch 399, loss: 0.6301, instance_loss: 0.4427, weighted_loss: 0.5739, label: 0, bag_size: 1349\n",
      "batch 419, loss: 0.1087, instance_loss: 0.0308, weighted_loss: 0.0853, label: 1, bag_size: 3003\n",
      "batch 439, loss: 0.1432, instance_loss: 0.0071, weighted_loss: 0.1024, label: 1, bag_size: 4330\n",
      "batch 459, loss: 0.2368, instance_loss: 0.0170, weighted_loss: 0.1709, label: 1, bag_size: 1819\n",
      "batch 479, loss: 0.1390, instance_loss: 0.0158, weighted_loss: 0.1021, label: 0, bag_size: 13691\n",
      "batch 499, loss: 0.0087, instance_loss: 0.0001, weighted_loss: 0.0061, label: 0, bag_size: 23037\n",
      "batch 519, loss: 0.0758, instance_loss: 0.0239, weighted_loss: 0.0602, label: 1, bag_size: 3295\n",
      "batch 539, loss: 0.3719, instance_loss: 0.0415, weighted_loss: 0.2728, label: 0, bag_size: 24382\n",
      "batch 559, loss: 0.0279, instance_loss: 0.0136, weighted_loss: 0.0236, label: 0, bag_size: 1712\n",
      "batch 579, loss: 0.0332, instance_loss: 0.0210, weighted_loss: 0.0296, label: 1, bag_size: 2966\n",
      "batch 599, loss: 4.0180, instance_loss: 0.5678, weighted_loss: 2.9830, label: 1, bag_size: 2565\n",
      "batch 619, loss: 0.6458, instance_loss: 0.2065, weighted_loss: 0.5140, label: 0, bag_size: 1349\n",
      "batch 639, loss: 0.1665, instance_loss: 0.0000, weighted_loss: 0.1166, label: 0, bag_size: 29270\n",
      "batch 659, loss: 0.7891, instance_loss: 1.4085, weighted_loss: 0.9749, label: 0, bag_size: 18516\n",
      "batch 679, loss: 0.6937, instance_loss: 0.0866, weighted_loss: 0.5116, label: 0, bag_size: 8427\n",
      "batch 699, loss: 0.0164, instance_loss: 0.0005, weighted_loss: 0.0116, label: 0, bag_size: 20910\n",
      "batch 719, loss: 0.0019, instance_loss: 0.0010, weighted_loss: 0.0016, label: 0, bag_size: 13225\n",
      "batch 739, loss: 0.1210, instance_loss: 0.0101, weighted_loss: 0.0878, label: 1, bag_size: 8602\n",
      "batch 759, loss: 0.4648, instance_loss: 0.6497, weighted_loss: 0.5203, label: 0, bag_size: 1920\n",
      "batch 779, loss: 0.7922, instance_loss: 1.3271, weighted_loss: 0.9526, label: 1, bag_size: 1038\n",
      "batch 799, loss: 0.1093, instance_loss: 0.0257, weighted_loss: 0.0842, label: 1, bag_size: 7798\n",
      "batch 819, loss: 0.0213, instance_loss: 0.0647, weighted_loss: 0.0343, label: 1, bag_size: 4054\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.972095498783455: correct 12785/13152\n",
      "class 1 clustering acc 0.8275547445255474: correct 5442/6576\n",
      "Epoch: 14, train_loss: 0.3536, train_clustering_loss:  0.3051, train_error: 0.1436\n",
      "class 0: acc 0.8613138686131386, correct 354/411\n",
      "class 1: acc 0.851581508515815, correct 350/411\n",
      "\n",
      "Val Set, val_loss: 0.3361, val_error: 0.1560, auc: 0.9275\n",
      "class 0 clustering acc 0.9696100917431193: correct 1691/1744\n",
      "class 1 clustering acc 0.8394495412844036: correct 732/872\n",
      "class 0: acc 0.8043478260869565, correct 37/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "Validation loss decreased (0.356083 --> 0.336128).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 1.6553, instance_loss: 0.3802, weighted_loss: 1.2728, label: 1, bag_size: 1533\n",
      "batch 39, loss: 0.0075, instance_loss: 0.0258, weighted_loss: 0.0130, label: 1, bag_size: 10033\n",
      "batch 59, loss: 0.1279, instance_loss: 0.3544, weighted_loss: 0.1958, label: 1, bag_size: 7246\n",
      "batch 79, loss: 0.1330, instance_loss: 0.0677, weighted_loss: 0.1134, label: 0, bag_size: 10721\n",
      "batch 99, loss: 0.0197, instance_loss: 0.0098, weighted_loss: 0.0167, label: 0, bag_size: 9851\n",
      "batch 119, loss: 0.0556, instance_loss: 0.0946, weighted_loss: 0.0673, label: 0, bag_size: 3502\n",
      "batch 139, loss: 0.4365, instance_loss: 0.8673, weighted_loss: 0.5657, label: 1, bag_size: 9649\n",
      "batch 159, loss: 0.0946, instance_loss: 0.1362, weighted_loss: 0.1071, label: 1, bag_size: 8191\n",
      "batch 179, loss: 0.0799, instance_loss: 0.0055, weighted_loss: 0.0576, label: 0, bag_size: 2628\n",
      "batch 199, loss: 0.2471, instance_loss: 0.0027, weighted_loss: 0.1738, label: 1, bag_size: 9322\n",
      "batch 219, loss: 0.0176, instance_loss: 0.0113, weighted_loss: 0.0157, label: 1, bag_size: 1022\n",
      "batch 239, loss: 0.0054, instance_loss: 0.0000, weighted_loss: 0.0038, label: 0, bag_size: 18574\n",
      "batch 259, loss: 0.1827, instance_loss: 1.9015, weighted_loss: 0.6983, label: 0, bag_size: 3101\n",
      "batch 279, loss: 0.0261, instance_loss: 0.0243, weighted_loss: 0.0256, label: 1, bag_size: 3207\n",
      "batch 299, loss: 0.6211, instance_loss: 0.4515, weighted_loss: 0.5702, label: 1, bag_size: 3980\n",
      "batch 319, loss: 0.0438, instance_loss: 0.0023, weighted_loss: 0.0314, label: 1, bag_size: 19832\n",
      "batch 339, loss: 0.0569, instance_loss: 0.0000, weighted_loss: 0.0398, label: 0, bag_size: 21385\n",
      "batch 359, loss: 0.0307, instance_loss: 0.0079, weighted_loss: 0.0238, label: 1, bag_size: 4102\n",
      "batch 379, loss: 0.8380, instance_loss: 0.0383, weighted_loss: 0.5981, label: 0, bag_size: 5120\n",
      "batch 399, loss: 0.3956, instance_loss: 0.0699, weighted_loss: 0.2979, label: 1, bag_size: 10591\n",
      "batch 419, loss: 0.2195, instance_loss: 0.1137, weighted_loss: 0.1877, label: 1, bag_size: 2092\n",
      "batch 439, loss: 0.2898, instance_loss: 0.0129, weighted_loss: 0.2067, label: 1, bag_size: 9561\n",
      "batch 459, loss: 0.0275, instance_loss: 0.0026, weighted_loss: 0.0201, label: 0, bag_size: 18954\n",
      "batch 479, loss: 0.0973, instance_loss: 0.0331, weighted_loss: 0.0780, label: 0, bag_size: 11125\n",
      "batch 499, loss: 0.0327, instance_loss: 0.1783, weighted_loss: 0.0764, label: 0, bag_size: 1415\n",
      "batch 519, loss: 0.0646, instance_loss: 0.0062, weighted_loss: 0.0471, label: 0, bag_size: 10898\n",
      "batch 539, loss: 0.3012, instance_loss: 0.1055, weighted_loss: 0.2425, label: 0, bag_size: 15071\n",
      "batch 559, loss: 0.0958, instance_loss: 0.0014, weighted_loss: 0.0674, label: 1, bag_size: 3576\n",
      "batch 579, loss: 0.0058, instance_loss: 0.0000, weighted_loss: 0.0041, label: 0, bag_size: 23037\n",
      "batch 599, loss: 0.2102, instance_loss: 0.0521, weighted_loss: 0.1627, label: 1, bag_size: 4039\n",
      "batch 619, loss: 0.0806, instance_loss: 0.1136, weighted_loss: 0.0905, label: 1, bag_size: 9519\n",
      "batch 639, loss: 0.1333, instance_loss: 0.0007, weighted_loss: 0.0935, label: 1, bag_size: 12931\n",
      "batch 659, loss: 0.2322, instance_loss: 0.0031, weighted_loss: 0.1635, label: 0, bag_size: 3810\n",
      "batch 679, loss: 0.0510, instance_loss: 0.0056, weighted_loss: 0.0374, label: 1, bag_size: 12095\n",
      "batch 699, loss: 0.0885, instance_loss: 0.0079, weighted_loss: 0.0643, label: 1, bag_size: 16890\n",
      "batch 719, loss: 1.5785, instance_loss: 3.0562, weighted_loss: 2.0218, label: 0, bag_size: 2815\n",
      "batch 739, loss: 0.0764, instance_loss: 0.0040, weighted_loss: 0.0547, label: 0, bag_size: 10721\n",
      "batch 759, loss: 0.0289, instance_loss: 0.0002, weighted_loss: 0.0203, label: 0, bag_size: 14828\n",
      "batch 779, loss: 0.0134, instance_loss: 0.0186, weighted_loss: 0.0150, label: 0, bag_size: 9851\n",
      "batch 799, loss: 0.0799, instance_loss: 0.1062, weighted_loss: 0.0878, label: 0, bag_size: 13378\n",
      "batch 819, loss: 0.0645, instance_loss: 0.0005, weighted_loss: 0.0453, label: 1, bag_size: 5494\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9657846715328468: correct 12702/13152\n",
      "class 1 clustering acc 0.7968369829683698: correct 5240/6576\n",
      "Epoch: 15, train_loss: 0.3327, train_clustering_loss:  0.3361, train_error: 0.1277\n",
      "class 0: acc 0.8687350835322196, correct 364/419\n",
      "class 1: acc 0.8759305210918115, correct 353/403\n",
      "\n",
      "Val Set, val_loss: 0.3947, val_error: 0.1560, auc: 0.9268\n",
      "class 0 clustering acc 0.9736238532110092: correct 1698/1744\n",
      "class 1 clustering acc 0.8509174311926605: correct 742/872\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.7777777777777778, correct 49/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0959, instance_loss: 0.0002, weighted_loss: 0.0672, label: 1, bag_size: 29832\n",
      "batch 39, loss: 0.1940, instance_loss: 0.6071, weighted_loss: 0.3179, label: 0, bag_size: 2270\n",
      "batch 59, loss: 0.5244, instance_loss: 0.5980, weighted_loss: 0.5464, label: 0, bag_size: 9597\n",
      "batch 79, loss: 0.0541, instance_loss: 0.0160, weighted_loss: 0.0427, label: 0, bag_size: 6851\n",
      "batch 99, loss: 1.2489, instance_loss: 0.7184, weighted_loss: 1.0897, label: 1, bag_size: 6478\n",
      "batch 119, loss: 0.0720, instance_loss: 0.0040, weighted_loss: 0.0516, label: 0, bag_size: 9866\n",
      "batch 139, loss: 0.1057, instance_loss: 0.0000, weighted_loss: 0.0740, label: 1, bag_size: 11684\n",
      "batch 159, loss: 0.3492, instance_loss: 0.0000, weighted_loss: 0.2444, label: 1, bag_size: 10396\n",
      "batch 179, loss: 0.1620, instance_loss: 0.1043, weighted_loss: 0.1447, label: 0, bag_size: 1826\n",
      "batch 199, loss: 0.0951, instance_loss: 0.0000, weighted_loss: 0.0665, label: 0, bag_size: 2036\n",
      "batch 219, loss: 0.0987, instance_loss: 0.1285, weighted_loss: 0.1076, label: 0, bag_size: 763\n",
      "batch 239, loss: 1.0803, instance_loss: 0.6595, weighted_loss: 0.9541, label: 0, bag_size: 2070\n",
      "batch 259, loss: 0.0330, instance_loss: 0.0000, weighted_loss: 0.0231, label: 0, bag_size: 15841\n",
      "batch 279, loss: 0.1464, instance_loss: 0.0031, weighted_loss: 0.1034, label: 0, bag_size: 18215\n",
      "batch 299, loss: 0.0262, instance_loss: 0.0000, weighted_loss: 0.0183, label: 0, bag_size: 10898\n",
      "batch 319, loss: 0.0339, instance_loss: 0.0114, weighted_loss: 0.0271, label: 1, bag_size: 3968\n",
      "batch 339, loss: 0.0433, instance_loss: 0.0048, weighted_loss: 0.0317, label: 0, bag_size: 8981\n",
      "batch 359, loss: 0.0454, instance_loss: 0.0000, weighted_loss: 0.0318, label: 0, bag_size: 15841\n",
      "batch 379, loss: 0.5168, instance_loss: 0.4225, weighted_loss: 0.4885, label: 0, bag_size: 1437\n",
      "batch 399, loss: 1.8323, instance_loss: 1.0738, weighted_loss: 1.6048, label: 1, bag_size: 1703\n",
      "batch 419, loss: 0.0986, instance_loss: 0.0061, weighted_loss: 0.0709, label: 1, bag_size: 7798\n",
      "batch 439, loss: 1.8933, instance_loss: 4.0138, weighted_loss: 2.5294, label: 1, bag_size: 15185\n",
      "batch 459, loss: 0.0443, instance_loss: 0.0108, weighted_loss: 0.0342, label: 1, bag_size: 2193\n",
      "batch 479, loss: 0.0092, instance_loss: 0.0049, weighted_loss: 0.0079, label: 0, bag_size: 8812\n",
      "batch 499, loss: 0.1190, instance_loss: 0.0370, weighted_loss: 0.0944, label: 1, bag_size: 1924\n",
      "batch 519, loss: 0.1377, instance_loss: 0.1996, weighted_loss: 0.1562, label: 0, bag_size: 1825\n",
      "batch 539, loss: 0.0356, instance_loss: 0.0504, weighted_loss: 0.0400, label: 0, bag_size: 3474\n",
      "batch 559, loss: 1.2783, instance_loss: 0.1364, weighted_loss: 0.9357, label: 0, bag_size: 2242\n",
      "batch 579, loss: 1.7306, instance_loss: 4.7643, weighted_loss: 2.6407, label: 0, bag_size: 17279\n",
      "batch 599, loss: 0.8966, instance_loss: 0.5314, weighted_loss: 0.7871, label: 1, bag_size: 15931\n",
      "batch 619, loss: 0.0042, instance_loss: 0.0002, weighted_loss: 0.0030, label: 0, bag_size: 13964\n",
      "batch 639, loss: 0.0152, instance_loss: 0.0042, weighted_loss: 0.0119, label: 0, bag_size: 8582\n",
      "batch 659, loss: 0.0178, instance_loss: 0.0901, weighted_loss: 0.0395, label: 0, bag_size: 9851\n",
      "batch 679, loss: 0.0455, instance_loss: 0.0081, weighted_loss: 0.0342, label: 0, bag_size: 7235\n",
      "batch 699, loss: 0.0494, instance_loss: 0.0418, weighted_loss: 0.0471, label: 0, bag_size: 8898\n",
      "batch 719, loss: 1.2719, instance_loss: 0.4079, weighted_loss: 1.0127, label: 0, bag_size: 2996\n",
      "batch 739, loss: 0.0256, instance_loss: 0.0536, weighted_loss: 0.0340, label: 1, bag_size: 1022\n",
      "batch 759, loss: 0.1317, instance_loss: 0.0110, weighted_loss: 0.0955, label: 0, bag_size: 2044\n",
      "batch 779, loss: 0.0093, instance_loss: 0.0117, weighted_loss: 0.0100, label: 1, bag_size: 11195\n",
      "batch 799, loss: 0.3923, instance_loss: 0.0788, weighted_loss: 0.2982, label: 1, bag_size: 9561\n",
      "batch 819, loss: 0.1410, instance_loss: 0.0107, weighted_loss: 0.1019, label: 0, bag_size: 5999\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9772658150851582: correct 12853/13152\n",
      "class 1 clustering acc 0.8508211678832117: correct 5595/6576\n",
      "Epoch: 16, train_loss: 0.2891, train_clustering_loss:  0.2497, train_error: 0.1131\n",
      "class 0: acc 0.8883720930232558, correct 382/430\n",
      "class 1: acc 0.8852040816326531, correct 347/392\n",
      "\n",
      "Val Set, val_loss: 0.4291, val_error: 0.1651, auc: 0.9272\n",
      "class 0 clustering acc 0.9673165137614679: correct 1687/1744\n",
      "class 1 clustering acc 0.8761467889908257: correct 764/872\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.7301587301587301, correct 46/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.0623, instance_loss: 0.5800, weighted_loss: 0.9176, label: 0, bag_size: 1732\n",
      "batch 39, loss: 0.0958, instance_loss: 0.0931, weighted_loss: 0.0950, label: 1, bag_size: 4862\n",
      "batch 59, loss: 0.1094, instance_loss: 0.0738, weighted_loss: 0.0987, label: 0, bag_size: 2195\n",
      "batch 79, loss: 0.1260, instance_loss: 0.0014, weighted_loss: 0.0886, label: 0, bag_size: 4271\n",
      "batch 99, loss: 0.0221, instance_loss: 0.0052, weighted_loss: 0.0170, label: 0, bag_size: 19466\n",
      "batch 119, loss: 0.0233, instance_loss: 0.0000, weighted_loss: 0.0163, label: 0, bag_size: 16052\n",
      "batch 139, loss: 0.1872, instance_loss: 0.0169, weighted_loss: 0.1361, label: 0, bag_size: 9387\n",
      "batch 159, loss: 0.1376, instance_loss: 0.0000, weighted_loss: 0.0964, label: 1, bag_size: 7935\n",
      "batch 179, loss: 0.8953, instance_loss: 0.9578, weighted_loss: 0.9140, label: 0, bag_size: 9132\n",
      "batch 199, loss: 0.0484, instance_loss: 0.0032, weighted_loss: 0.0348, label: 1, bag_size: 14230\n",
      "batch 219, loss: 0.0072, instance_loss: 0.0000, weighted_loss: 0.0051, label: 0, bag_size: 7191\n",
      "batch 239, loss: 0.0231, instance_loss: 0.0000, weighted_loss: 0.0162, label: 0, bag_size: 31106\n",
      "batch 259, loss: 0.0634, instance_loss: 0.0002, weighted_loss: 0.0444, label: 0, bag_size: 6851\n",
      "batch 279, loss: 0.1020, instance_loss: 0.4667, weighted_loss: 0.2114, label: 0, bag_size: 1483\n",
      "batch 299, loss: 0.1609, instance_loss: 0.0442, weighted_loss: 0.1259, label: 0, bag_size: 1639\n",
      "batch 319, loss: 0.9320, instance_loss: 0.0887, weighted_loss: 0.6790, label: 1, bag_size: 7981\n",
      "batch 339, loss: 0.4927, instance_loss: 0.3827, weighted_loss: 0.4597, label: 1, bag_size: 1095\n",
      "batch 359, loss: 0.0890, instance_loss: 0.0000, weighted_loss: 0.0623, label: 0, bag_size: 22681\n",
      "batch 379, loss: 0.0057, instance_loss: 0.0883, weighted_loss: 0.0305, label: 0, bag_size: 14305\n",
      "batch 399, loss: 0.5517, instance_loss: 0.2246, weighted_loss: 0.4536, label: 0, bag_size: 14664\n",
      "batch 419, loss: 0.1178, instance_loss: 0.6272, weighted_loss: 0.2706, label: 1, bag_size: 2695\n",
      "batch 439, loss: 0.2062, instance_loss: 0.4875, weighted_loss: 0.2906, label: 1, bag_size: 5110\n",
      "batch 459, loss: 0.0131, instance_loss: 0.0552, weighted_loss: 0.0257, label: 1, bag_size: 2193\n",
      "batch 479, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 21082\n",
      "batch 499, loss: 0.6216, instance_loss: 0.4515, weighted_loss: 0.5706, label: 0, bag_size: 1732\n",
      "batch 519, loss: 0.0900, instance_loss: 0.0000, weighted_loss: 0.0630, label: 1, bag_size: 11256\n",
      "batch 539, loss: 0.0570, instance_loss: 0.0076, weighted_loss: 0.0421, label: 0, bag_size: 3232\n",
      "batch 559, loss: 0.0376, instance_loss: 0.0000, weighted_loss: 0.0263, label: 0, bag_size: 21864\n",
      "batch 579, loss: 0.0561, instance_loss: 0.3214, weighted_loss: 0.1357, label: 0, bag_size: 2534\n",
      "batch 599, loss: 0.0077, instance_loss: 0.0032, weighted_loss: 0.0064, label: 0, bag_size: 10898\n",
      "batch 619, loss: 0.0619, instance_loss: 0.0000, weighted_loss: 0.0433, label: 0, bag_size: 12593\n",
      "batch 639, loss: 0.0108, instance_loss: 0.2103, weighted_loss: 0.0706, label: 0, bag_size: 20666\n",
      "batch 659, loss: 0.0841, instance_loss: 0.0530, weighted_loss: 0.0748, label: 0, bag_size: 12131\n",
      "batch 679, loss: 0.1063, instance_loss: 0.2039, weighted_loss: 0.1356, label: 1, bag_size: 7186\n",
      "batch 699, loss: 0.0031, instance_loss: 0.0507, weighted_loss: 0.0174, label: 0, bag_size: 10535\n",
      "batch 719, loss: 0.1235, instance_loss: 0.2457, weighted_loss: 0.1601, label: 0, bag_size: 1797\n",
      "batch 739, loss: 1.4490, instance_loss: 0.4230, weighted_loss: 1.1412, label: 0, bag_size: 1732\n",
      "batch 759, loss: 0.0030, instance_loss: 0.0002, weighted_loss: 0.0021, label: 0, bag_size: 8372\n",
      "batch 779, loss: 0.3143, instance_loss: 0.0030, weighted_loss: 0.2209, label: 1, bag_size: 9561\n",
      "batch 799, loss: 0.0006, instance_loss: 0.0020, weighted_loss: 0.0010, label: 1, bag_size: 1360\n",
      "batch 819, loss: 0.8020, instance_loss: 0.2277, weighted_loss: 0.6297, label: 0, bag_size: 11306\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9762013381995134: correct 12839/13152\n",
      "class 1 clustering acc 0.8523418491484185: correct 5605/6576\n",
      "Epoch: 17, train_loss: 0.2824, train_clustering_loss:  0.2531, train_error: 0.1119\n",
      "class 0: acc 0.8832116788321168, correct 363/411\n",
      "class 1: acc 0.8929440389294404, correct 367/411\n",
      "\n",
      "Val Set, val_loss: 0.5111, val_error: 0.1835, auc: 0.9293\n",
      "class 0 clustering acc 0.9443807339449541: correct 1647/1744\n",
      "class 1 clustering acc 0.8084862385321101: correct 705/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.6825396825396826, correct 43/63\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 2.7902, instance_loss: 2.0123, weighted_loss: 2.5568, label: 1, bag_size: 3879\n",
      "batch 39, loss: 0.0224, instance_loss: 0.0002, weighted_loss: 0.0157, label: 1, bag_size: 9689\n",
      "batch 59, loss: 0.0117, instance_loss: 0.0000, weighted_loss: 0.0082, label: 1, bag_size: 13051\n",
      "batch 79, loss: 1.5010, instance_loss: 2.9004, weighted_loss: 1.9208, label: 1, bag_size: 13362\n",
      "batch 99, loss: 0.0712, instance_loss: 0.1116, weighted_loss: 0.0833, label: 1, bag_size: 4239\n",
      "batch 119, loss: 0.0076, instance_loss: 0.0289, weighted_loss: 0.0140, label: 1, bag_size: 5612\n",
      "batch 139, loss: 0.2177, instance_loss: 0.4758, weighted_loss: 0.2951, label: 1, bag_size: 5907\n",
      "batch 159, loss: 0.0080, instance_loss: 0.0808, weighted_loss: 0.0298, label: 1, bag_size: 4317\n",
      "batch 179, loss: 0.0770, instance_loss: 0.0843, weighted_loss: 0.0792, label: 0, bag_size: 2063\n",
      "batch 199, loss: 2.3516, instance_loss: 2.8529, weighted_loss: 2.5020, label: 0, bag_size: 2815\n",
      "batch 219, loss: 0.2885, instance_loss: 0.2629, weighted_loss: 0.2808, label: 1, bag_size: 10105\n",
      "batch 239, loss: 0.1554, instance_loss: 0.8456, weighted_loss: 0.3625, label: 0, bag_size: 2079\n",
      "batch 259, loss: 5.8123, instance_loss: 3.5593, weighted_loss: 5.1364, label: 0, bag_size: 3897\n",
      "batch 279, loss: 0.3490, instance_loss: 0.1247, weighted_loss: 0.2817, label: 0, bag_size: 15898\n",
      "batch 299, loss: 0.2367, instance_loss: 0.0623, weighted_loss: 0.1844, label: 0, bag_size: 2918\n",
      "batch 319, loss: 0.0136, instance_loss: 0.0000, weighted_loss: 0.0095, label: 1, bag_size: 4367\n",
      "batch 339, loss: 0.0453, instance_loss: 0.0000, weighted_loss: 0.0317, label: 1, bag_size: 6736\n",
      "batch 359, loss: 0.0929, instance_loss: 0.0874, weighted_loss: 0.0912, label: 0, bag_size: 705\n",
      "batch 379, loss: 0.0422, instance_loss: 0.1134, weighted_loss: 0.0636, label: 1, bag_size: 4054\n",
      "batch 399, loss: 0.0667, instance_loss: 0.0158, weighted_loss: 0.0514, label: 0, bag_size: 24439\n",
      "batch 419, loss: 0.0109, instance_loss: 0.0049, weighted_loss: 0.0091, label: 1, bag_size: 2193\n",
      "batch 439, loss: 0.4757, instance_loss: 0.6180, weighted_loss: 0.5184, label: 0, bag_size: 3399\n",
      "batch 459, loss: 0.0189, instance_loss: 0.0004, weighted_loss: 0.0133, label: 0, bag_size: 15747\n",
      "batch 479, loss: 0.0253, instance_loss: 0.1340, weighted_loss: 0.0579, label: 0, bag_size: 3474\n",
      "batch 499, loss: 0.1449, instance_loss: 0.0768, weighted_loss: 0.1245, label: 1, bag_size: 4330\n",
      "batch 519, loss: 0.0331, instance_loss: 0.0498, weighted_loss: 0.0381, label: 0, bag_size: 8661\n",
      "batch 539, loss: 0.1874, instance_loss: 0.1800, weighted_loss: 0.1852, label: 1, bag_size: 8191\n",
      "batch 559, loss: 0.0023, instance_loss: 0.0101, weighted_loss: 0.0046, label: 0, bag_size: 11546\n",
      "batch 579, loss: 0.0091, instance_loss: 0.0000, weighted_loss: 0.0064, label: 0, bag_size: 13225\n",
      "batch 599, loss: 0.0270, instance_loss: 0.0036, weighted_loss: 0.0200, label: 0, bag_size: 12217\n",
      "batch 619, loss: 1.6797, instance_loss: 0.9031, weighted_loss: 1.4467, label: 1, bag_size: 2842\n",
      "batch 639, loss: 0.4163, instance_loss: 0.4237, weighted_loss: 0.4185, label: 1, bag_size: 2480\n",
      "batch 659, loss: 0.0432, instance_loss: 0.0593, weighted_loss: 0.0480, label: 1, bag_size: 5605\n",
      "batch 679, loss: 0.1000, instance_loss: 0.0013, weighted_loss: 0.0704, label: 0, bag_size: 19043\n",
      "batch 699, loss: 0.0052, instance_loss: 0.0657, weighted_loss: 0.0234, label: 1, bag_size: 2405\n",
      "batch 719, loss: 0.1230, instance_loss: 0.6053, weighted_loss: 0.2677, label: 0, bag_size: 8866\n",
      "batch 739, loss: 0.0539, instance_loss: 0.0988, weighted_loss: 0.0674, label: 0, bag_size: 12561\n",
      "batch 759, loss: 1.8137, instance_loss: 0.3364, weighted_loss: 1.3705, label: 1, bag_size: 1493\n",
      "batch 779, loss: 0.0371, instance_loss: 0.0205, weighted_loss: 0.0321, label: 0, bag_size: 2322\n",
      "batch 799, loss: 0.3081, instance_loss: 0.2902, weighted_loss: 0.3027, label: 0, bag_size: 1826\n",
      "batch 819, loss: 0.3497, instance_loss: 0.0964, weighted_loss: 0.2737, label: 0, bag_size: 11212\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9681417274939172: correct 12733/13152\n",
      "class 1 clustering acc 0.8380474452554745: correct 5511/6576\n",
      "Epoch: 18, train_loss: 0.2871, train_clustering_loss:  0.3132, train_error: 0.1119\n",
      "class 0: acc 0.8814432989690721, correct 342/388\n",
      "class 1: acc 0.8940092165898618, correct 388/434\n",
      "\n",
      "Val Set, val_loss: 0.4070, val_error: 0.1468, auc: 0.9300\n",
      "class 0 clustering acc 0.9627293577981652: correct 1679/1744\n",
      "class 1 clustering acc 0.8222477064220184: correct 717/872\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.7936507936507936, correct 50/63\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0457, instance_loss: 0.0358, weighted_loss: 0.0428, label: 0, bag_size: 1891\n",
      "batch 39, loss: 0.2830, instance_loss: 0.0162, weighted_loss: 0.2029, label: 1, bag_size: 1746\n",
      "batch 59, loss: 0.0027, instance_loss: 0.0000, weighted_loss: 0.0019, label: 1, bag_size: 10392\n",
      "batch 79, loss: 0.0203, instance_loss: 0.0001, weighted_loss: 0.0143, label: 0, bag_size: 22762\n",
      "batch 99, loss: 0.0003, instance_loss: 0.1477, weighted_loss: 0.0445, label: 0, bag_size: 1984\n",
      "batch 119, loss: 0.0111, instance_loss: 0.0995, weighted_loss: 0.0376, label: 0, bag_size: 3474\n",
      "batch 139, loss: 0.1940, instance_loss: 0.5502, weighted_loss: 0.3009, label: 1, bag_size: 8754\n",
      "batch 159, loss: 0.5423, instance_loss: 0.2338, weighted_loss: 0.4498, label: 0, bag_size: 13619\n",
      "batch 179, loss: 0.0673, instance_loss: 0.6819, weighted_loss: 0.2517, label: 0, bag_size: 1483\n",
      "batch 199, loss: 0.0109, instance_loss: 0.0004, weighted_loss: 0.0077, label: 1, bag_size: 5833\n",
      "batch 219, loss: 0.0061, instance_loss: 0.0000, weighted_loss: 0.0042, label: 1, bag_size: 4715\n",
      "batch 239, loss: 0.0058, instance_loss: 0.0071, weighted_loss: 0.0062, label: 1, bag_size: 4442\n",
      "batch 259, loss: 0.0143, instance_loss: 0.0006, weighted_loss: 0.0102, label: 0, bag_size: 11199\n",
      "batch 279, loss: 0.0056, instance_loss: 0.0026, weighted_loss: 0.0047, label: 0, bag_size: 18154\n",
      "batch 299, loss: 0.0234, instance_loss: 0.0417, weighted_loss: 0.0289, label: 0, bag_size: 2748\n",
      "batch 319, loss: 0.0402, instance_loss: 0.1628, weighted_loss: 0.0769, label: 1, bag_size: 7148\n",
      "batch 339, loss: 0.0056, instance_loss: 0.0000, weighted_loss: 0.0039, label: 0, bag_size: 8948\n",
      "batch 359, loss: 0.2828, instance_loss: 0.0226, weighted_loss: 0.2047, label: 1, bag_size: 1875\n",
      "batch 379, loss: 0.0633, instance_loss: 0.4216, weighted_loss: 0.1708, label: 1, bag_size: 5907\n",
      "batch 399, loss: 0.0022, instance_loss: 0.0139, weighted_loss: 0.0057, label: 1, bag_size: 6090\n",
      "batch 419, loss: 0.2122, instance_loss: 0.8329, weighted_loss: 0.3984, label: 0, bag_size: 931\n",
      "batch 439, loss: 0.1284, instance_loss: 0.0042, weighted_loss: 0.0911, label: 0, bag_size: 7989\n",
      "batch 459, loss: 0.3040, instance_loss: 0.0930, weighted_loss: 0.2407, label: 1, bag_size: 4929\n",
      "batch 479, loss: 0.6965, instance_loss: 0.0529, weighted_loss: 0.5035, label: 0, bag_size: 9069\n",
      "batch 499, loss: 0.5974, instance_loss: 0.6540, weighted_loss: 0.6144, label: 0, bag_size: 9421\n",
      "batch 519, loss: 0.0620, instance_loss: 0.0015, weighted_loss: 0.0439, label: 1, bag_size: 5731\n",
      "batch 539, loss: 0.1114, instance_loss: 0.0069, weighted_loss: 0.0800, label: 1, bag_size: 11223\n",
      "batch 559, loss: 0.0087, instance_loss: 0.0000, weighted_loss: 0.0061, label: 1, bag_size: 11642\n",
      "batch 579, loss: 0.0080, instance_loss: 0.0000, weighted_loss: 0.0056, label: 0, bag_size: 25558\n",
      "batch 599, loss: 3.8648, instance_loss: 3.4308, weighted_loss: 3.7346, label: 1, bag_size: 684\n",
      "batch 619, loss: 0.3860, instance_loss: 0.0213, weighted_loss: 0.2766, label: 0, bag_size: 14885\n",
      "batch 639, loss: 0.0083, instance_loss: 0.0000, weighted_loss: 0.0058, label: 0, bag_size: 30751\n",
      "batch 659, loss: 0.1132, instance_loss: 0.0706, weighted_loss: 0.1004, label: 1, bag_size: 10501\n",
      "batch 679, loss: 0.1112, instance_loss: 1.0112, weighted_loss: 0.3812, label: 1, bag_size: 3856\n",
      "batch 699, loss: 2.2901, instance_loss: 0.8489, weighted_loss: 1.8578, label: 1, bag_size: 3121\n",
      "batch 719, loss: 0.0865, instance_loss: 0.0019, weighted_loss: 0.0611, label: 0, bag_size: 3541\n",
      "batch 739, loss: 0.5071, instance_loss: 0.2484, weighted_loss: 0.4295, label: 0, bag_size: 18738\n",
      "batch 759, loss: 0.8060, instance_loss: 0.2352, weighted_loss: 0.6348, label: 1, bag_size: 2455\n",
      "batch 779, loss: 0.0344, instance_loss: 0.0039, weighted_loss: 0.0252, label: 1, bag_size: 5494\n",
      "batch 799, loss: 0.0042, instance_loss: 0.0001, weighted_loss: 0.0030, label: 1, bag_size: 7513\n",
      "batch 819, loss: 0.0509, instance_loss: 0.0037, weighted_loss: 0.0367, label: 0, bag_size: 31780\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9715632603406326: correct 12778/13152\n",
      "class 1 clustering acc 0.8570559610705596: correct 5636/6576\n",
      "Epoch: 19, train_loss: 0.2922, train_clustering_loss:  0.2672, train_error: 0.1083\n",
      "class 0: acc 0.8982188295165394, correct 353/393\n",
      "class 1: acc 0.8857808857808858, correct 380/429\n",
      "\n",
      "Val Set, val_loss: 0.3306, val_error: 0.1376, auc: 0.9306\n",
      "class 0 clustering acc 0.9547018348623854: correct 1665/1744\n",
      "class 1 clustering acc 0.7786697247706422: correct 679/872\n",
      "class 0: acc 0.8695652173913043, correct 40/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "Validation loss decreased (0.336128 --> 0.330552).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0054, instance_loss: 0.0000, weighted_loss: 0.0038, label: 0, bag_size: 11735\n",
      "batch 39, loss: 0.0262, instance_loss: 0.0855, weighted_loss: 0.0440, label: 1, bag_size: 6171\n",
      "batch 59, loss: 0.0107, instance_loss: 0.0011, weighted_loss: 0.0078, label: 1, bag_size: 8522\n",
      "batch 79, loss: 0.0044, instance_loss: 0.2786, weighted_loss: 0.0867, label: 1, bag_size: 1273\n",
      "batch 99, loss: 0.0182, instance_loss: 0.0494, weighted_loss: 0.0275, label: 1, bag_size: 6171\n",
      "batch 119, loss: 0.0307, instance_loss: 0.1296, weighted_loss: 0.0604, label: 0, bag_size: 3265\n",
      "batch 139, loss: 0.0084, instance_loss: 0.0000, weighted_loss: 0.0059, label: 0, bag_size: 32227\n",
      "batch 159, loss: 0.1541, instance_loss: 0.0008, weighted_loss: 0.1081, label: 1, bag_size: 7468\n",
      "batch 179, loss: 0.1469, instance_loss: 0.0267, weighted_loss: 0.1108, label: 0, bag_size: 2063\n",
      "batch 199, loss: 0.0183, instance_loss: 0.1325, weighted_loss: 0.0526, label: 0, bag_size: 2282\n",
      "batch 219, loss: 0.0098, instance_loss: 0.0040, weighted_loss: 0.0081, label: 0, bag_size: 11735\n",
      "batch 239, loss: 0.1013, instance_loss: 0.0105, weighted_loss: 0.0741, label: 1, bag_size: 7468\n",
      "batch 259, loss: 0.0017, instance_loss: 0.0042, weighted_loss: 0.0025, label: 0, bag_size: 23996\n",
      "batch 279, loss: 0.0022, instance_loss: 0.0006, weighted_loss: 0.0017, label: 1, bag_size: 5991\n",
      "batch 299, loss: 0.0153, instance_loss: 0.0062, weighted_loss: 0.0126, label: 1, bag_size: 16267\n",
      "batch 319, loss: 0.0570, instance_loss: 0.0010, weighted_loss: 0.0402, label: 1, bag_size: 7515\n",
      "batch 339, loss: 0.0114, instance_loss: 0.0007, weighted_loss: 0.0082, label: 1, bag_size: 6533\n",
      "batch 359, loss: 0.0042, instance_loss: 0.0041, weighted_loss: 0.0041, label: 1, bag_size: 9877\n",
      "batch 379, loss: 0.0067, instance_loss: 0.0000, weighted_loss: 0.0047, label: 0, bag_size: 31106\n",
      "batch 399, loss: 0.2563, instance_loss: 0.2745, weighted_loss: 0.2618, label: 0, bag_size: 1831\n",
      "batch 419, loss: 0.2579, instance_loss: 0.8382, weighted_loss: 0.4320, label: 1, bag_size: 3856\n",
      "batch 439, loss: 0.0076, instance_loss: 0.0000, weighted_loss: 0.0053, label: 0, bag_size: 12212\n",
      "batch 459, loss: 0.1100, instance_loss: 0.1709, weighted_loss: 0.1283, label: 0, bag_size: 12083\n",
      "batch 479, loss: 0.0064, instance_loss: 0.0000, weighted_loss: 0.0045, label: 0, bag_size: 24911\n",
      "batch 499, loss: 1.8135, instance_loss: 3.3337, weighted_loss: 2.2695, label: 1, bag_size: 2935\n",
      "batch 519, loss: 0.0170, instance_loss: 0.0000, weighted_loss: 0.0119, label: 0, bag_size: 32227\n",
      "batch 539, loss: 0.7270, instance_loss: 1.2352, weighted_loss: 0.8795, label: 1, bag_size: 1339\n",
      "batch 559, loss: 0.0486, instance_loss: 0.0113, weighted_loss: 0.0374, label: 0, bag_size: 11146\n",
      "batch 579, loss: 5.1150, instance_loss: 2.7315, weighted_loss: 4.3999, label: 0, bag_size: 2694\n",
      "batch 599, loss: 0.3109, instance_loss: 0.1736, weighted_loss: 0.2697, label: 0, bag_size: 2548\n",
      "batch 619, loss: 0.2459, instance_loss: 0.5385, weighted_loss: 0.3337, label: 0, bag_size: 9616\n",
      "batch 639, loss: 0.0135, instance_loss: 0.0000, weighted_loss: 0.0094, label: 0, bag_size: 27158\n",
      "batch 659, loss: 0.0680, instance_loss: 0.0000, weighted_loss: 0.0476, label: 0, bag_size: 21093\n",
      "batch 679, loss: 0.0040, instance_loss: 0.0000, weighted_loss: 0.0028, label: 0, bag_size: 18240\n",
      "batch 699, loss: 0.2630, instance_loss: 0.2413, weighted_loss: 0.2565, label: 0, bag_size: 9132\n",
      "batch 719, loss: 0.0109, instance_loss: 0.0000, weighted_loss: 0.0076, label: 0, bag_size: 19390\n",
      "batch 739, loss: 0.3912, instance_loss: 0.0556, weighted_loss: 0.2905, label: 0, bag_size: 7141\n",
      "batch 759, loss: 0.7645, instance_loss: 0.1970, weighted_loss: 0.5943, label: 1, bag_size: 10848\n",
      "batch 779, loss: 0.1105, instance_loss: 0.3821, weighted_loss: 0.1920, label: 0, bag_size: 1826\n",
      "batch 799, loss: 0.0711, instance_loss: 0.0496, weighted_loss: 0.0647, label: 1, bag_size: 4821\n",
      "batch 819, loss: 0.0215, instance_loss: 0.0001, weighted_loss: 0.0151, label: 0, bag_size: 16341\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9699665450121655: correct 12757/13152\n",
      "class 1 clustering acc 0.8371350364963503: correct 5505/6576\n",
      "Epoch: 20, train_loss: 0.2905, train_clustering_loss:  0.2859, train_error: 0.1095\n",
      "class 0: acc 0.8977556109725686, correct 360/401\n",
      "class 1: acc 0.8836104513064132, correct 372/421\n",
      "\n",
      "Val Set, val_loss: 0.3343, val_error: 0.1376, auc: 0.9300\n",
      "class 0 clustering acc 0.974197247706422: correct 1699/1744\n",
      "class 1 clustering acc 0.8818807339449541: correct 769/872\n",
      "class 0: acc 0.8695652173913043, correct 40/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0466, instance_loss: 0.0000, weighted_loss: 0.0326, label: 0, bag_size: 22681\n",
      "batch 39, loss: 0.1627, instance_loss: 0.0076, weighted_loss: 0.1162, label: 1, bag_size: 7468\n",
      "batch 59, loss: 0.0641, instance_loss: 0.1874, weighted_loss: 0.1011, label: 1, bag_size: 2356\n",
      "batch 79, loss: 0.0583, instance_loss: 0.0935, weighted_loss: 0.0689, label: 1, bag_size: 4039\n",
      "batch 99, loss: 0.3892, instance_loss: 0.3760, weighted_loss: 0.3853, label: 0, bag_size: 9421\n",
      "batch 119, loss: 0.2624, instance_loss: 0.7985, weighted_loss: 0.4232, label: 0, bag_size: 9421\n",
      "batch 139, loss: 0.2492, instance_loss: 0.0377, weighted_loss: 0.1857, label: 0, bag_size: 6093\n",
      "batch 159, loss: 0.3628, instance_loss: 0.0369, weighted_loss: 0.2650, label: 0, bag_size: 2351\n",
      "batch 179, loss: 0.0085, instance_loss: 0.0039, weighted_loss: 0.0071, label: 0, bag_size: 11199\n",
      "batch 199, loss: 0.0129, instance_loss: 0.0000, weighted_loss: 0.0090, label: 1, bag_size: 12349\n",
      "batch 219, loss: 0.0351, instance_loss: 0.0076, weighted_loss: 0.0269, label: 0, bag_size: 22681\n",
      "batch 239, loss: 0.0697, instance_loss: 0.0012, weighted_loss: 0.0491, label: 1, bag_size: 11875\n",
      "batch 259, loss: 0.0424, instance_loss: 0.0629, weighted_loss: 0.0486, label: 1, bag_size: 1172\n",
      "batch 279, loss: 0.0110, instance_loss: 0.1018, weighted_loss: 0.0383, label: 0, bag_size: 1061\n",
      "batch 299, loss: 0.3234, instance_loss: 0.2488, weighted_loss: 0.3010, label: 0, bag_size: 11390\n",
      "batch 319, loss: 1.2123, instance_loss: 1.4952, weighted_loss: 1.2972, label: 0, bag_size: 3375\n",
      "batch 339, loss: 0.8648, instance_loss: 0.0977, weighted_loss: 0.6347, label: 0, bag_size: 7239\n",
      "batch 359, loss: 0.1088, instance_loss: 0.1391, weighted_loss: 0.1179, label: 0, bag_size: 5485\n",
      "batch 379, loss: 1.0670, instance_loss: 0.0309, weighted_loss: 0.7562, label: 0, bag_size: 11306\n",
      "batch 399, loss: 0.0511, instance_loss: 0.4444, weighted_loss: 0.1691, label: 0, bag_size: 17083\n",
      "batch 419, loss: 0.0278, instance_loss: 0.0000, weighted_loss: 0.0195, label: 0, bag_size: 5225\n",
      "batch 439, loss: 0.1218, instance_loss: 0.0001, weighted_loss: 0.0853, label: 0, bag_size: 7989\n",
      "batch 459, loss: 0.4405, instance_loss: 0.0003, weighted_loss: 0.3084, label: 1, bag_size: 8592\n",
      "batch 479, loss: 0.0397, instance_loss: 0.0000, weighted_loss: 0.0278, label: 1, bag_size: 28527\n",
      "batch 499, loss: 0.5734, instance_loss: 1.4249, weighted_loss: 0.8289, label: 1, bag_size: 9404\n",
      "batch 519, loss: 0.8237, instance_loss: 1.7171, weighted_loss: 1.0917, label: 1, bag_size: 2731\n",
      "batch 539, loss: 0.1604, instance_loss: 0.0003, weighted_loss: 0.1124, label: 1, bag_size: 34356\n",
      "batch 559, loss: 0.1749, instance_loss: 0.4792, weighted_loss: 0.2662, label: 1, bag_size: 2579\n",
      "batch 579, loss: 0.0266, instance_loss: 0.0825, weighted_loss: 0.0434, label: 1, bag_size: 5561\n",
      "batch 599, loss: 0.1308, instance_loss: 0.0347, weighted_loss: 0.1020, label: 0, bag_size: 9387\n",
      "batch 619, loss: 0.0051, instance_loss: 0.0745, weighted_loss: 0.0260, label: 0, bag_size: 9433\n",
      "batch 639, loss: 0.0650, instance_loss: 0.0007, weighted_loss: 0.0457, label: 0, bag_size: 21385\n",
      "batch 659, loss: 0.6210, instance_loss: 0.5435, weighted_loss: 0.5977, label: 1, bag_size: 2790\n",
      "batch 679, loss: 1.1466, instance_loss: 1.5022, weighted_loss: 1.2533, label: 1, bag_size: 2314\n",
      "batch 699, loss: 0.0032, instance_loss: 0.0000, weighted_loss: 0.0023, label: 0, bag_size: 11546\n",
      "batch 719, loss: 0.0263, instance_loss: 0.0000, weighted_loss: 0.0184, label: 1, bag_size: 20333\n",
      "batch 739, loss: 2.0071, instance_loss: 2.0914, weighted_loss: 2.0324, label: 1, bag_size: 1497\n",
      "batch 759, loss: 0.0266, instance_loss: 0.0026, weighted_loss: 0.0194, label: 0, bag_size: 18954\n",
      "batch 779, loss: 0.0185, instance_loss: 0.0004, weighted_loss: 0.0131, label: 0, bag_size: 11113\n",
      "batch 799, loss: 0.1190, instance_loss: 0.2015, weighted_loss: 0.1438, label: 0, bag_size: 9596\n",
      "batch 819, loss: 0.0350, instance_loss: 0.1103, weighted_loss: 0.0576, label: 0, bag_size: 5999\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9734641119221411: correct 12803/13152\n",
      "class 1 clustering acc 0.8494525547445255: correct 5586/6576\n",
      "Epoch: 21, train_loss: 0.2673, train_clustering_loss:  0.2747, train_error: 0.1010\n",
      "class 0: acc 0.8943488943488943, correct 364/407\n",
      "class 1: acc 0.9036144578313253, correct 375/415\n",
      "\n",
      "Val Set, val_loss: 0.4632, val_error: 0.1560, auc: 0.9310\n",
      "class 0 clustering acc 0.9696100917431193: correct 1691/1744\n",
      "class 1 clustering acc 0.8727064220183486: correct 761/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.7301587301587301, correct 46/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.8828, instance_loss: 2.4707, weighted_loss: 1.3592, label: 1, bag_size: 2935\n",
      "batch 39, loss: 0.0129, instance_loss: 0.0006, weighted_loss: 0.0092, label: 0, bag_size: 9470\n",
      "batch 59, loss: 0.2686, instance_loss: 0.0383, weighted_loss: 0.1995, label: 1, bag_size: 8026\n",
      "batch 79, loss: 0.0280, instance_loss: 0.0983, weighted_loss: 0.0491, label: 1, bag_size: 1172\n",
      "batch 99, loss: 0.0051, instance_loss: 0.0013, weighted_loss: 0.0040, label: 0, bag_size: 9851\n",
      "batch 119, loss: 0.2623, instance_loss: 0.0070, weighted_loss: 0.1857, label: 0, bag_size: 7031\n",
      "batch 139, loss: 0.5013, instance_loss: 0.0123, weighted_loss: 0.3546, label: 1, bag_size: 11220\n",
      "batch 159, loss: 0.2270, instance_loss: 0.0007, weighted_loss: 0.1591, label: 1, bag_size: 8040\n",
      "batch 179, loss: 0.0142, instance_loss: 0.0004, weighted_loss: 0.0101, label: 0, bag_size: 31780\n",
      "batch 199, loss: 0.0232, instance_loss: 0.0047, weighted_loss: 0.0177, label: 1, bag_size: 15609\n",
      "batch 219, loss: 0.0217, instance_loss: 0.0314, weighted_loss: 0.0246, label: 0, bag_size: 4845\n",
      "batch 239, loss: 2.8433, instance_loss: 0.8870, weighted_loss: 2.2564, label: 1, bag_size: 1703\n",
      "batch 259, loss: 0.0839, instance_loss: 0.0021, weighted_loss: 0.0594, label: 0, bag_size: 11194\n",
      "batch 279, loss: 1.0505, instance_loss: 0.4686, weighted_loss: 0.8760, label: 0, bag_size: 4241\n",
      "batch 299, loss: 0.0718, instance_loss: 0.0060, weighted_loss: 0.0520, label: 1, bag_size: 4239\n",
      "batch 319, loss: 0.0029, instance_loss: 0.0086, weighted_loss: 0.0046, label: 0, bag_size: 2179\n",
      "batch 339, loss: 0.0312, instance_loss: 0.0001, weighted_loss: 0.0219, label: 1, bag_size: 20333\n",
      "batch 359, loss: 0.0506, instance_loss: 0.0027, weighted_loss: 0.0362, label: 1, bag_size: 19606\n",
      "batch 379, loss: 0.2015, instance_loss: 0.0580, weighted_loss: 0.1584, label: 1, bag_size: 1867\n",
      "batch 399, loss: 0.0309, instance_loss: 0.0000, weighted_loss: 0.0217, label: 0, bag_size: 9885\n",
      "batch 419, loss: 0.3921, instance_loss: 0.5121, weighted_loss: 0.4281, label: 0, bag_size: 1831\n",
      "batch 439, loss: 0.0134, instance_loss: 0.1592, weighted_loss: 0.0571, label: 0, bag_size: 1712\n",
      "batch 459, loss: 0.0012, instance_loss: 0.0032, weighted_loss: 0.0018, label: 1, bag_size: 8410\n",
      "batch 479, loss: 0.7951, instance_loss: 0.1541, weighted_loss: 0.6028, label: 0, bag_size: 3783\n",
      "batch 499, loss: 0.1950, instance_loss: 0.2530, weighted_loss: 0.2124, label: 0, bag_size: 1142\n",
      "batch 519, loss: 0.0146, instance_loss: 0.0273, weighted_loss: 0.0184, label: 1, bag_size: 4239\n",
      "batch 539, loss: 0.0349, instance_loss: 0.0000, weighted_loss: 0.0244, label: 1, bag_size: 21827\n",
      "batch 559, loss: 0.0496, instance_loss: 0.1110, weighted_loss: 0.0680, label: 1, bag_size: 2848\n",
      "batch 579, loss: 0.0024, instance_loss: 0.0000, weighted_loss: 0.0017, label: 0, bag_size: 11383\n",
      "batch 599, loss: 0.0095, instance_loss: 0.0000, weighted_loss: 0.0066, label: 0, bag_size: 31780\n",
      "batch 619, loss: 0.0494, instance_loss: 0.1082, weighted_loss: 0.0670, label: 0, bag_size: 8898\n",
      "batch 639, loss: 0.4640, instance_loss: 0.0468, weighted_loss: 0.3388, label: 1, bag_size: 5723\n",
      "batch 659, loss: 0.0110, instance_loss: 0.0000, weighted_loss: 0.0077, label: 1, bag_size: 2904\n",
      "batch 679, loss: 0.0334, instance_loss: 0.0343, weighted_loss: 0.0336, label: 0, bag_size: 2624\n",
      "batch 699, loss: 0.0518, instance_loss: 0.0145, weighted_loss: 0.0406, label: 0, bag_size: 19808\n",
      "batch 719, loss: 0.3767, instance_loss: 0.0787, weighted_loss: 0.2873, label: 0, bag_size: 14664\n",
      "batch 739, loss: 0.0196, instance_loss: 0.0095, weighted_loss: 0.0166, label: 1, bag_size: 4250\n",
      "batch 759, loss: 0.0029, instance_loss: 0.0000, weighted_loss: 0.0020, label: 0, bag_size: 21082\n",
      "batch 779, loss: 0.0242, instance_loss: 0.0001, weighted_loss: 0.0170, label: 1, bag_size: 6343\n",
      "batch 799, loss: 0.0680, instance_loss: 0.0002, weighted_loss: 0.0477, label: 1, bag_size: 13026\n",
      "batch 819, loss: 0.1524, instance_loss: 0.4602, weighted_loss: 0.2447, label: 0, bag_size: 6356\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9771897810218978: correct 12852/13152\n",
      "class 1 clustering acc 0.8759124087591241: correct 5760/6576\n",
      "Epoch: 22, train_loss: 0.2581, train_clustering_loss:  0.2288, train_error: 0.1119\n",
      "class 0: acc 0.8808933002481389, correct 355/403\n",
      "class 1: acc 0.8949880668257757, correct 375/419\n",
      "\n",
      "Val Set, val_loss: 0.4205, val_error: 0.1560, auc: 0.9331\n",
      "class 0 clustering acc 0.9747706422018348: correct 1700/1744\n",
      "class 1 clustering acc 0.8899082568807339: correct 776/872\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.7619047619047619, correct 48/63\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0078, instance_loss: 0.0042, weighted_loss: 0.0067, label: 0, bag_size: 9786\n",
      "batch 39, loss: 0.0075, instance_loss: 0.1145, weighted_loss: 0.0396, label: 0, bag_size: 890\n",
      "batch 59, loss: 0.7066, instance_loss: 0.0048, weighted_loss: 0.4960, label: 1, bag_size: 11220\n",
      "batch 79, loss: 0.1663, instance_loss: 1.2388, weighted_loss: 0.4881, label: 1, bag_size: 1999\n",
      "batch 99, loss: 0.0005, instance_loss: 0.6659, weighted_loss: 0.2001, label: 0, bag_size: 1984\n",
      "batch 119, loss: 0.2729, instance_loss: 0.0838, weighted_loss: 0.2162, label: 0, bag_size: 10146\n",
      "batch 139, loss: 0.1805, instance_loss: 0.0304, weighted_loss: 0.1355, label: 1, bag_size: 10072\n",
      "batch 159, loss: 0.0350, instance_loss: 0.0625, weighted_loss: 0.0433, label: 1, bag_size: 3656\n",
      "batch 179, loss: 0.0518, instance_loss: 0.5499, weighted_loss: 0.2012, label: 0, bag_size: 9583\n",
      "batch 199, loss: 0.0323, instance_loss: 0.6886, weighted_loss: 0.2292, label: 0, bag_size: 2367\n",
      "batch 219, loss: 0.0838, instance_loss: 0.3001, weighted_loss: 0.1487, label: 1, bag_size: 2559\n",
      "batch 239, loss: 0.1682, instance_loss: 0.1841, weighted_loss: 0.1730, label: 0, bag_size: 1797\n",
      "batch 259, loss: 0.1119, instance_loss: 0.0341, weighted_loss: 0.0886, label: 0, bag_size: 9069\n",
      "batch 279, loss: 0.6127, instance_loss: 0.7457, weighted_loss: 0.6526, label: 0, bag_size: 11151\n",
      "batch 299, loss: 0.0173, instance_loss: 0.0572, weighted_loss: 0.0293, label: 0, bag_size: 2282\n",
      "batch 319, loss: 0.0579, instance_loss: 0.0583, weighted_loss: 0.0580, label: 1, bag_size: 5921\n",
      "batch 339, loss: 0.0011, instance_loss: 0.0049, weighted_loss: 0.0022, label: 1, bag_size: 1743\n",
      "batch 359, loss: 0.9416, instance_loss: 0.5556, weighted_loss: 0.8258, label: 0, bag_size: 3375\n",
      "batch 379, loss: 0.0590, instance_loss: 0.0586, weighted_loss: 0.0589, label: 1, bag_size: 16379\n",
      "batch 399, loss: 0.0344, instance_loss: 0.0000, weighted_loss: 0.0241, label: 1, bag_size: 5494\n",
      "batch 419, loss: 0.0434, instance_loss: 0.0003, weighted_loss: 0.0305, label: 0, bag_size: 12793\n",
      "batch 439, loss: 0.1977, instance_loss: 0.0001, weighted_loss: 0.1384, label: 0, bag_size: 21874\n",
      "batch 459, loss: 0.4674, instance_loss: 0.6528, weighted_loss: 0.5231, label: 1, bag_size: 12340\n",
      "batch 479, loss: 0.0206, instance_loss: 0.0753, weighted_loss: 0.0370, label: 0, bag_size: 11146\n",
      "batch 499, loss: 3.6024, instance_loss: 2.6229, weighted_loss: 3.3085, label: 1, bag_size: 684\n",
      "batch 519, loss: 0.0185, instance_loss: 0.1175, weighted_loss: 0.0482, label: 1, bag_size: 1249\n",
      "batch 539, loss: 0.5310, instance_loss: 1.4503, weighted_loss: 0.8068, label: 1, bag_size: 15125\n",
      "batch 559, loss: 0.4170, instance_loss: 0.1625, weighted_loss: 0.3406, label: 0, bag_size: 12510\n",
      "batch 579, loss: 0.0003, instance_loss: 0.0073, weighted_loss: 0.0024, label: 1, bag_size: 3004\n",
      "batch 599, loss: 0.0026, instance_loss: 0.0000, weighted_loss: 0.0018, label: 0, bag_size: 13218\n",
      "batch 619, loss: 0.0215, instance_loss: 0.3152, weighted_loss: 0.1096, label: 0, bag_size: 10415\n",
      "batch 639, loss: 0.1627, instance_loss: 0.3034, weighted_loss: 0.2049, label: 1, bag_size: 2495\n",
      "batch 659, loss: 0.0951, instance_loss: 0.7622, weighted_loss: 0.2952, label: 0, bag_size: 3399\n",
      "batch 679, loss: 0.0356, instance_loss: 0.1320, weighted_loss: 0.0645, label: 1, bag_size: 2785\n",
      "batch 699, loss: 0.0244, instance_loss: 0.8914, weighted_loss: 0.2845, label: 0, bag_size: 3101\n",
      "batch 719, loss: 0.3519, instance_loss: 1.4443, weighted_loss: 0.6796, label: 1, bag_size: 15125\n",
      "batch 739, loss: 0.0498, instance_loss: 1.0729, weighted_loss: 0.3567, label: 1, bag_size: 3856\n",
      "batch 759, loss: 0.1267, instance_loss: 0.0412, weighted_loss: 0.1011, label: 1, bag_size: 6734\n",
      "batch 779, loss: 0.5986, instance_loss: 0.6988, weighted_loss: 0.6286, label: 1, bag_size: 4956\n",
      "batch 799, loss: 0.2214, instance_loss: 0.1312, weighted_loss: 0.1943, label: 0, bag_size: 2548\n",
      "batch 819, loss: 0.0229, instance_loss: 0.0024, weighted_loss: 0.0168, label: 1, bag_size: 14202\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9704227493917275: correct 12763/13152\n",
      "class 1 clustering acc 0.8228406326034063: correct 5411/6576\n",
      "Epoch: 23, train_loss: 0.2798, train_clustering_loss:  0.3088, train_error: 0.1095\n",
      "class 0: acc 0.8841607565011821, correct 374/423\n",
      "class 1: acc 0.8972431077694235, correct 358/399\n",
      "\n",
      "Val Set, val_loss: 0.3699, val_error: 0.1284, auc: 0.9341\n",
      "class 0 clustering acc 0.9736238532110092: correct 1698/1744\n",
      "class 1 clustering acc 0.8692660550458715: correct 758/872\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.8253968253968254, correct 52/63\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0119, instance_loss: 0.0111, weighted_loss: 0.0117, label: 1, bag_size: 6171\n",
      "batch 39, loss: 0.0032, instance_loss: 0.0005, weighted_loss: 0.0024, label: 0, bag_size: 9433\n",
      "batch 59, loss: 0.2615, instance_loss: 0.2838, weighted_loss: 0.2682, label: 0, bag_size: 2006\n",
      "batch 79, loss: 0.2095, instance_loss: 0.0203, weighted_loss: 0.1528, label: 1, bag_size: 2092\n",
      "batch 99, loss: 0.0119, instance_loss: 0.0000, weighted_loss: 0.0083, label: 0, bag_size: 15841\n",
      "batch 119, loss: 1.9548, instance_loss: 5.9202, weighted_loss: 3.1444, label: 1, bag_size: 15563\n",
      "batch 139, loss: 0.0434, instance_loss: 0.0318, weighted_loss: 0.0399, label: 1, bag_size: 6533\n",
      "batch 159, loss: 0.0187, instance_loss: 0.0001, weighted_loss: 0.0131, label: 1, bag_size: 6745\n",
      "batch 179, loss: 0.4289, instance_loss: 2.8139, weighted_loss: 1.1444, label: 1, bag_size: 9404\n",
      "batch 199, loss: 0.0932, instance_loss: 0.0015, weighted_loss: 0.0657, label: 1, bag_size: 5110\n",
      "batch 219, loss: 0.0442, instance_loss: 0.0020, weighted_loss: 0.0315, label: 0, bag_size: 16087\n",
      "batch 239, loss: 0.7007, instance_loss: 0.5120, weighted_loss: 0.6441, label: 0, bag_size: 12840\n",
      "batch 259, loss: 0.4869, instance_loss: 0.0150, weighted_loss: 0.3453, label: 1, bag_size: 29832\n",
      "batch 279, loss: 0.0529, instance_loss: 0.2744, weighted_loss: 0.1194, label: 1, bag_size: 2136\n",
      "batch 299, loss: 0.2352, instance_loss: 0.1468, weighted_loss: 0.2087, label: 1, bag_size: 8191\n",
      "batch 319, loss: 0.0572, instance_loss: 0.0156, weighted_loss: 0.0447, label: 0, bag_size: 1797\n",
      "batch 339, loss: 0.0020, instance_loss: 0.0002, weighted_loss: 0.0015, label: 1, bag_size: 4959\n",
      "batch 359, loss: 0.0515, instance_loss: 0.1568, weighted_loss: 0.0831, label: 1, bag_size: 6736\n",
      "batch 379, loss: 0.4487, instance_loss: 0.5560, weighted_loss: 0.4809, label: 0, bag_size: 2996\n",
      "batch 399, loss: 0.0153, instance_loss: 0.0061, weighted_loss: 0.0126, label: 1, bag_size: 11875\n",
      "batch 419, loss: 0.0336, instance_loss: 0.0039, weighted_loss: 0.0247, label: 1, bag_size: 6533\n",
      "batch 439, loss: 0.0069, instance_loss: 0.0006, weighted_loss: 0.0050, label: 0, bag_size: 11690\n",
      "batch 459, loss: 0.0189, instance_loss: 0.0077, weighted_loss: 0.0156, label: 1, bag_size: 9446\n",
      "batch 479, loss: 0.0300, instance_loss: 0.3228, weighted_loss: 0.1178, label: 1, bag_size: 1014\n",
      "batch 499, loss: 0.0028, instance_loss: 0.0003, weighted_loss: 0.0021, label: 0, bag_size: 9851\n",
      "batch 519, loss: 0.0091, instance_loss: 0.0009, weighted_loss: 0.0066, label: 0, bag_size: 11113\n",
      "batch 539, loss: 0.0027, instance_loss: 0.3468, weighted_loss: 0.1059, label: 0, bag_size: 2424\n",
      "batch 559, loss: 0.0904, instance_loss: 0.1738, weighted_loss: 0.1154, label: 0, bag_size: 1909\n",
      "batch 579, loss: 0.1026, instance_loss: 0.0155, weighted_loss: 0.0765, label: 0, bag_size: 6850\n",
      "batch 599, loss: 0.0449, instance_loss: 0.2935, weighted_loss: 0.1195, label: 0, bag_size: 1651\n",
      "batch 619, loss: 0.0139, instance_loss: 0.0815, weighted_loss: 0.0342, label: 0, bag_size: 2282\n",
      "batch 639, loss: 0.0468, instance_loss: 0.0982, weighted_loss: 0.0622, label: 0, bag_size: 12131\n",
      "batch 659, loss: 0.0849, instance_loss: 0.0460, weighted_loss: 0.0733, label: 1, bag_size: 2381\n",
      "batch 679, loss: 0.0131, instance_loss: 0.0000, weighted_loss: 0.0092, label: 0, bag_size: 11113\n",
      "batch 699, loss: 0.0008, instance_loss: 0.0026, weighted_loss: 0.0014, label: 0, bag_size: 14206\n",
      "batch 719, loss: 0.0030, instance_loss: 0.0000, weighted_loss: 0.0021, label: 0, bag_size: 15077\n",
      "batch 739, loss: 0.0947, instance_loss: 0.0029, weighted_loss: 0.0672, label: 1, bag_size: 25695\n",
      "batch 759, loss: 0.0095, instance_loss: 0.0251, weighted_loss: 0.0142, label: 1, bag_size: 7583\n",
      "batch 779, loss: 0.0048, instance_loss: 0.0170, weighted_loss: 0.0085, label: 1, bag_size: 11600\n",
      "batch 799, loss: 0.2318, instance_loss: 0.1011, weighted_loss: 0.1926, label: 0, bag_size: 12510\n",
      "batch 819, loss: 0.0086, instance_loss: 0.0025, weighted_loss: 0.0068, label: 0, bag_size: 10898\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9779501216545012: correct 12862/13152\n",
      "class 1 clustering acc 0.8698296836982968: correct 5720/6576\n",
      "Epoch: 24, train_loss: 0.2240, train_clustering_loss:  0.2285, train_error: 0.0961\n",
      "class 0: acc 0.8987341772151899, correct 355/395\n",
      "class 1: acc 0.9086651053864169, correct 388/427\n",
      "\n",
      "Val Set, val_loss: 0.4422, val_error: 0.1376, auc: 0.9348\n",
      "class 0 clustering acc 0.9730504587155964: correct 1697/1744\n",
      "class 1 clustering acc 0.8761467889908257: correct 764/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.7619047619047619, correct 48/63\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0430, instance_loss: 0.0060, weighted_loss: 0.0319, label: 0, bag_size: 10444\n",
      "batch 39, loss: 0.0115, instance_loss: 0.0000, weighted_loss: 0.0081, label: 0, bag_size: 8582\n",
      "batch 59, loss: 0.7376, instance_loss: 0.1027, weighted_loss: 0.5471, label: 1, bag_size: 3211\n",
      "batch 79, loss: 1.2619, instance_loss: 1.1514, weighted_loss: 1.2288, label: 1, bag_size: 1794\n",
      "batch 99, loss: 0.0383, instance_loss: 0.0003, weighted_loss: 0.0269, label: 1, bag_size: 7798\n",
      "batch 119, loss: 0.0570, instance_loss: 0.2310, weighted_loss: 0.1092, label: 0, bag_size: 5485\n",
      "batch 139, loss: 0.0011, instance_loss: 0.0001, weighted_loss: 0.0008, label: 1, bag_size: 2936\n",
      "batch 159, loss: 1.1960, instance_loss: 0.8875, weighted_loss: 1.1035, label: 0, bag_size: 1142\n",
      "batch 179, loss: 0.1880, instance_loss: 0.0014, weighted_loss: 0.1320, label: 0, bag_size: 2351\n",
      "batch 199, loss: 0.0024, instance_loss: 0.0101, weighted_loss: 0.0047, label: 0, bag_size: 17791\n",
      "batch 219, loss: 0.0227, instance_loss: 0.0237, weighted_loss: 0.0230, label: 1, bag_size: 15689\n",
      "batch 239, loss: 0.0140, instance_loss: 0.0350, weighted_loss: 0.0203, label: 0, bag_size: 10263\n",
      "batch 259, loss: 0.0044, instance_loss: 0.0000, weighted_loss: 0.0031, label: 0, bag_size: 14828\n",
      "batch 279, loss: 0.0058, instance_loss: 0.0009, weighted_loss: 0.0044, label: 0, bag_size: 11199\n",
      "batch 299, loss: 0.5865, instance_loss: 0.0313, weighted_loss: 0.4199, label: 1, bag_size: 12714\n",
      "batch 319, loss: 0.1173, instance_loss: 0.0008, weighted_loss: 0.0823, label: 1, bag_size: 7389\n",
      "batch 339, loss: 0.0055, instance_loss: 0.0000, weighted_loss: 0.0038, label: 1, bag_size: 19932\n",
      "batch 359, loss: 0.1145, instance_loss: 0.0000, weighted_loss: 0.0802, label: 1, bag_size: 7445\n",
      "batch 379, loss: 0.1006, instance_loss: 0.0209, weighted_loss: 0.0767, label: 0, bag_size: 3708\n",
      "batch 399, loss: 0.0216, instance_loss: 0.0013, weighted_loss: 0.0155, label: 1, bag_size: 2344\n",
      "batch 419, loss: 0.2727, instance_loss: 0.0850, weighted_loss: 0.2164, label: 0, bag_size: 11212\n",
      "batch 439, loss: 0.0437, instance_loss: 0.0599, weighted_loss: 0.0485, label: 0, bag_size: 2236\n",
      "batch 459, loss: 0.1666, instance_loss: 0.0070, weighted_loss: 0.1187, label: 1, bag_size: 13692\n",
      "batch 479, loss: 0.0093, instance_loss: 0.0004, weighted_loss: 0.0066, label: 1, bag_size: 1412\n",
      "batch 499, loss: 0.0112, instance_loss: 0.0000, weighted_loss: 0.0079, label: 0, bag_size: 19880\n",
      "batch 519, loss: 0.0436, instance_loss: 0.4719, weighted_loss: 0.1721, label: 1, bag_size: 20767\n",
      "batch 539, loss: 0.0238, instance_loss: 0.0044, weighted_loss: 0.0180, label: 0, bag_size: 19067\n",
      "batch 559, loss: 0.0139, instance_loss: 0.0011, weighted_loss: 0.0100, label: 0, bag_size: 15636\n",
      "batch 579, loss: 0.0480, instance_loss: 0.4731, weighted_loss: 0.1755, label: 0, bag_size: 763\n",
      "batch 599, loss: 0.2306, instance_loss: 0.2572, weighted_loss: 0.2386, label: 1, bag_size: 1572\n",
      "batch 619, loss: 0.0409, instance_loss: 0.0000, weighted_loss: 0.0286, label: 0, bag_size: 27012\n",
      "batch 639, loss: 1.3454, instance_loss: 0.0870, weighted_loss: 0.9679, label: 1, bag_size: 2455\n",
      "batch 659, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 7513\n",
      "batch 679, loss: 0.0142, instance_loss: 0.0000, weighted_loss: 0.0100, label: 1, bag_size: 21827\n",
      "batch 699, loss: 0.2687, instance_loss: 0.1685, weighted_loss: 0.2387, label: 0, bag_size: 4959\n",
      "batch 719, loss: 0.2192, instance_loss: 1.1455, weighted_loss: 0.4971, label: 1, bag_size: 10072\n",
      "batch 739, loss: 0.0562, instance_loss: 0.0000, weighted_loss: 0.0393, label: 1, bag_size: 6928\n",
      "batch 759, loss: 0.0162, instance_loss: 0.0000, weighted_loss: 0.0113, label: 0, bag_size: 31106\n",
      "batch 779, loss: 0.3347, instance_loss: 0.0000, weighted_loss: 0.2343, label: 0, bag_size: 20555\n",
      "batch 799, loss: 0.0442, instance_loss: 0.0000, weighted_loss: 0.0310, label: 1, bag_size: 16051\n",
      "batch 819, loss: 0.0078, instance_loss: 0.0000, weighted_loss: 0.0055, label: 0, bag_size: 16607\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9771137469586375: correct 12851/13152\n",
      "class 1 clustering acc 0.8713503649635036: correct 5730/6576\n",
      "Epoch: 25, train_loss: 0.2432, train_clustering_loss:  0.2245, train_error: 0.0998\n",
      "class 0: acc 0.8927680798004988, correct 358/401\n",
      "class 1: acc 0.9073634204275535, correct 382/421\n",
      "\n",
      "Val Set, val_loss: 0.3879, val_error: 0.1376, auc: 0.9365\n",
      "class 0 clustering acc 0.9587155963302753: correct 1672/1744\n",
      "class 1 clustering acc 0.8394495412844036: correct 732/872\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.7936507936507936, correct 50/63\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1513, instance_loss: 0.0330, weighted_loss: 0.1158, label: 1, bag_size: 2480\n",
      "batch 39, loss: 0.0282, instance_loss: 0.1246, weighted_loss: 0.0571, label: 1, bag_size: 7148\n",
      "batch 59, loss: 0.0335, instance_loss: 0.0000, weighted_loss: 0.0235, label: 0, bag_size: 8898\n",
      "batch 79, loss: 0.0061, instance_loss: 0.0000, weighted_loss: 0.0043, label: 0, bag_size: 11727\n",
      "batch 99, loss: 0.1809, instance_loss: 0.0102, weighted_loss: 0.1297, label: 1, bag_size: 7445\n",
      "batch 119, loss: 0.1954, instance_loss: 0.2231, weighted_loss: 0.2037, label: 0, bag_size: 1142\n",
      "batch 139, loss: 0.0868, instance_loss: 0.0103, weighted_loss: 0.0638, label: 1, bag_size: 5256\n",
      "batch 159, loss: 0.0071, instance_loss: 0.0000, weighted_loss: 0.0049, label: 1, bag_size: 4862\n",
      "batch 179, loss: 0.0003, instance_loss: 0.0835, weighted_loss: 0.0252, label: 0, bag_size: 3787\n",
      "batch 199, loss: 0.0031, instance_loss: 0.0001, weighted_loss: 0.0022, label: 1, bag_size: 5833\n",
      "batch 219, loss: 0.0597, instance_loss: 0.5625, weighted_loss: 0.2105, label: 0, bag_size: 1549\n",
      "batch 239, loss: 0.0031, instance_loss: 0.0000, weighted_loss: 0.0021, label: 1, bag_size: 19832\n",
      "batch 259, loss: 0.0179, instance_loss: 0.0126, weighted_loss: 0.0163, label: 0, bag_size: 11125\n",
      "batch 279, loss: 0.0219, instance_loss: 0.3079, weighted_loss: 0.1077, label: 1, bag_size: 9548\n",
      "batch 299, loss: 0.0290, instance_loss: 0.0001, weighted_loss: 0.0204, label: 1, bag_size: 3576\n",
      "batch 319, loss: 1.0359, instance_loss: 0.8804, weighted_loss: 0.9892, label: 0, bag_size: 17279\n",
      "batch 339, loss: 0.0278, instance_loss: 0.3846, weighted_loss: 0.1348, label: 0, bag_size: 1772\n",
      "batch 359, loss: 0.0116, instance_loss: 1.4751, weighted_loss: 0.4506, label: 0, bag_size: 1760\n",
      "batch 379, loss: 0.0152, instance_loss: 0.0000, weighted_loss: 0.0106, label: 0, bag_size: 19808\n",
      "batch 399, loss: 1.4329, instance_loss: 2.0517, weighted_loss: 1.6185, label: 1, bag_size: 983\n",
      "batch 419, loss: 0.0594, instance_loss: 0.1109, weighted_loss: 0.0748, label: 0, bag_size: 2104\n",
      "batch 439, loss: 0.1352, instance_loss: 0.0921, weighted_loss: 0.1223, label: 0, bag_size: 9616\n",
      "batch 459, loss: 0.0176, instance_loss: 0.0002, weighted_loss: 0.0123, label: 1, bag_size: 14230\n",
      "batch 479, loss: 0.0123, instance_loss: 0.0001, weighted_loss: 0.0087, label: 1, bag_size: 4330\n",
      "batch 499, loss: 0.0427, instance_loss: 0.0128, weighted_loss: 0.0337, label: 1, bag_size: 4039\n",
      "batch 519, loss: 0.2399, instance_loss: 0.0103, weighted_loss: 0.1710, label: 1, bag_size: 6842\n",
      "batch 539, loss: 0.0275, instance_loss: 0.0000, weighted_loss: 0.0193, label: 1, bag_size: 11032\n",
      "batch 559, loss: 0.0026, instance_loss: 0.0000, weighted_loss: 0.0018, label: 1, bag_size: 16417\n",
      "batch 579, loss: 0.0257, instance_loss: 0.2767, weighted_loss: 0.1010, label: 0, bag_size: 5485\n",
      "batch 599, loss: 0.0273, instance_loss: 0.0975, weighted_loss: 0.0484, label: 1, bag_size: 4239\n",
      "batch 619, loss: 0.2301, instance_loss: 0.3540, weighted_loss: 0.2672, label: 0, bag_size: 5485\n",
      "batch 639, loss: 0.0453, instance_loss: 0.0036, weighted_loss: 0.0328, label: 0, bag_size: 17155\n",
      "batch 659, loss: 0.0004, instance_loss: 0.0007, weighted_loss: 0.0005, label: 0, bag_size: 23037\n",
      "batch 679, loss: 0.0532, instance_loss: 0.0024, weighted_loss: 0.0380, label: 1, bag_size: 4789\n",
      "batch 699, loss: 0.0043, instance_loss: 0.0000, weighted_loss: 0.0030, label: 0, bag_size: 25558\n",
      "batch 719, loss: 0.1267, instance_loss: 0.0119, weighted_loss: 0.0923, label: 1, bag_size: 8395\n",
      "batch 739, loss: 0.0943, instance_loss: 0.0040, weighted_loss: 0.0672, label: 0, bag_size: 3541\n",
      "batch 759, loss: 0.7856, instance_loss: 0.0284, weighted_loss: 0.5584, label: 1, bag_size: 12714\n",
      "batch 779, loss: 0.0051, instance_loss: 0.0000, weighted_loss: 0.0036, label: 1, bag_size: 11389\n",
      "batch 799, loss: 0.1304, instance_loss: 0.9141, weighted_loss: 0.3655, label: 1, bag_size: 2814\n",
      "batch 819, loss: 0.0005, instance_loss: 0.0163, weighted_loss: 0.0052, label: 0, bag_size: 14266\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9758211678832117: correct 12834/13152\n",
      "class 1 clustering acc 0.853558394160584: correct 5613/6576\n",
      "Epoch: 26, train_loss: 0.2297, train_clustering_loss:  0.2602, train_error: 0.0839\n",
      "class 0: acc 0.9211136890951276, correct 397/431\n",
      "class 1: acc 0.9104859335038363, correct 356/391\n",
      "\n",
      "Val Set, val_loss: 0.4421, val_error: 0.1284, auc: 0.9369\n",
      "class 0 clustering acc 0.9529816513761468: correct 1662/1744\n",
      "class 1 clustering acc 0.8944954128440367: correct 780/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.7777777777777778, correct 49/63\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2526, instance_loss: 0.0000, weighted_loss: 0.1768, label: 1, bag_size: 11256\n",
      "batch 39, loss: 0.0211, instance_loss: 0.0000, weighted_loss: 0.0148, label: 1, bag_size: 1838\n",
      "batch 59, loss: 1.4517, instance_loss: 2.3298, weighted_loss: 1.7151, label: 1, bag_size: 13367\n",
      "batch 79, loss: 0.0080, instance_loss: 0.1336, weighted_loss: 0.0457, label: 1, bag_size: 699\n",
      "batch 99, loss: 0.0061, instance_loss: 0.0000, weighted_loss: 0.0043, label: 0, bag_size: 11383\n",
      "batch 119, loss: 0.3778, instance_loss: 0.1975, weighted_loss: 0.3237, label: 0, bag_size: 5161\n",
      "batch 139, loss: 0.0529, instance_loss: 0.0024, weighted_loss: 0.0378, label: 0, bag_size: 10146\n",
      "batch 159, loss: 1.0293, instance_loss: 0.0121, weighted_loss: 0.7242, label: 0, bag_size: 5211\n",
      "batch 179, loss: 0.0344, instance_loss: 0.1023, weighted_loss: 0.0548, label: 0, bag_size: 2351\n",
      "batch 199, loss: 0.0339, instance_loss: 0.1525, weighted_loss: 0.0694, label: 1, bag_size: 12178\n",
      "batch 219, loss: 1.6345, instance_loss: 2.9804, weighted_loss: 2.0382, label: 1, bag_size: 12712\n",
      "batch 239, loss: 0.0112, instance_loss: 0.0005, weighted_loss: 0.0080, label: 1, bag_size: 10482\n",
      "batch 259, loss: 0.3714, instance_loss: 0.2983, weighted_loss: 0.3495, label: 0, bag_size: 5409\n",
      "batch 279, loss: 0.0274, instance_loss: 0.0024, weighted_loss: 0.0199, label: 1, bag_size: 10396\n",
      "batch 299, loss: 0.4235, instance_loss: 0.0000, weighted_loss: 0.2964, label: 1, bag_size: 10591\n",
      "batch 319, loss: 0.0031, instance_loss: 0.0890, weighted_loss: 0.0288, label: 0, bag_size: 21076\n",
      "batch 339, loss: 1.1464, instance_loss: 4.1486, weighted_loss: 2.0470, label: 0, bag_size: 11128\n",
      "batch 359, loss: 0.0009, instance_loss: 0.0262, weighted_loss: 0.0085, label: 0, bag_size: 13225\n",
      "batch 379, loss: 0.0078, instance_loss: 0.0000, weighted_loss: 0.0055, label: 1, bag_size: 6317\n",
      "batch 399, loss: 0.0097, instance_loss: 0.0099, weighted_loss: 0.0097, label: 0, bag_size: 24911\n",
      "batch 419, loss: 0.2861, instance_loss: 0.1892, weighted_loss: 0.2570, label: 0, bag_size: 1684\n",
      "batch 439, loss: 0.2099, instance_loss: 0.1176, weighted_loss: 0.1822, label: 1, bag_size: 2480\n",
      "batch 459, loss: 0.0030, instance_loss: 0.1176, weighted_loss: 0.0374, label: 0, bag_size: 21576\n",
      "batch 479, loss: 0.0481, instance_loss: 0.1448, weighted_loss: 0.0771, label: 0, bag_size: 2322\n",
      "batch 499, loss: 0.8605, instance_loss: 0.1111, weighted_loss: 0.6357, label: 1, bag_size: 1294\n",
      "batch 519, loss: 0.0990, instance_loss: 0.0000, weighted_loss: 0.0693, label: 1, bag_size: 16890\n",
      "batch 539, loss: 0.0868, instance_loss: 0.0035, weighted_loss: 0.0618, label: 1, bag_size: 10432\n",
      "batch 559, loss: 0.0477, instance_loss: 0.0214, weighted_loss: 0.0398, label: 1, bag_size: 2695\n",
      "batch 579, loss: 0.1774, instance_loss: 0.2999, weighted_loss: 0.2141, label: 0, bag_size: 13332\n",
      "batch 599, loss: 0.0070, instance_loss: 0.0012, weighted_loss: 0.0053, label: 1, bag_size: 9322\n",
      "batch 619, loss: 0.0429, instance_loss: 0.0755, weighted_loss: 0.0527, label: 1, bag_size: 8040\n",
      "batch 639, loss: 0.3646, instance_loss: 0.0487, weighted_loss: 0.2698, label: 0, bag_size: 18738\n",
      "batch 659, loss: 1.9368, instance_loss: 1.0312, weighted_loss: 1.6651, label: 0, bag_size: 7239\n",
      "batch 679, loss: 0.0063, instance_loss: 0.0000, weighted_loss: 0.0044, label: 1, bag_size: 9571\n",
      "batch 699, loss: 0.1029, instance_loss: 0.8320, weighted_loss: 0.3216, label: 1, bag_size: 14604\n",
      "batch 719, loss: 0.0129, instance_loss: 0.0000, weighted_loss: 0.0091, label: 1, bag_size: 3576\n",
      "batch 739, loss: 0.0067, instance_loss: 0.0075, weighted_loss: 0.0070, label: 0, bag_size: 8981\n",
      "batch 759, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 7078\n",
      "batch 779, loss: 0.0035, instance_loss: 0.0000, weighted_loss: 0.0025, label: 1, bag_size: 7078\n",
      "batch 799, loss: 0.0022, instance_loss: 0.0018, weighted_loss: 0.0021, label: 1, bag_size: 3437\n",
      "batch 819, loss: 0.0046, instance_loss: 0.0084, weighted_loss: 0.0057, label: 0, bag_size: 18154\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9729318734793188: correct 12796/13152\n",
      "class 1 clustering acc 0.8251216545012166: correct 5426/6576\n",
      "Epoch: 27, train_loss: 0.2369, train_clustering_loss:  0.2903, train_error: 0.0900\n",
      "class 0: acc 0.8967391304347826, correct 330/368\n",
      "class 1: acc 0.920704845814978, correct 418/454\n",
      "\n",
      "Val Set, val_loss: 0.3123, val_error: 0.1101, auc: 0.9386\n",
      "class 0 clustering acc 0.9696100917431193: correct 1691/1744\n",
      "class 1 clustering acc 0.7396788990825688: correct 645/872\n",
      "class 0: acc 0.9130434782608695, correct 42/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "Validation loss decreased (0.330552 --> 0.312258).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1440, instance_loss: 0.0488, weighted_loss: 0.1154, label: 0, bag_size: 2351\n",
      "batch 39, loss: 0.0163, instance_loss: 0.0304, weighted_loss: 0.0205, label: 0, bag_size: 9866\n",
      "batch 59, loss: 0.1459, instance_loss: 0.0986, weighted_loss: 0.1317, label: 0, bag_size: 3810\n",
      "batch 79, loss: 0.0018, instance_loss: 0.0557, weighted_loss: 0.0180, label: 0, bag_size: 8981\n",
      "batch 99, loss: 0.5114, instance_loss: 0.0030, weighted_loss: 0.3589, label: 0, bag_size: 15672\n",
      "batch 119, loss: 0.0101, instance_loss: 0.4022, weighted_loss: 0.1277, label: 0, bag_size: 16341\n",
      "batch 139, loss: 0.0121, instance_loss: 0.4110, weighted_loss: 0.1318, label: 0, bag_size: 1772\n",
      "batch 159, loss: 0.0483, instance_loss: 0.9613, weighted_loss: 0.3222, label: 0, bag_size: 2609\n",
      "batch 179, loss: 0.0545, instance_loss: 0.5338, weighted_loss: 0.1983, label: 0, bag_size: 14681\n",
      "batch 199, loss: 0.0128, instance_loss: 0.0245, weighted_loss: 0.0163, label: 0, bag_size: 12149\n",
      "batch 219, loss: 0.0064, instance_loss: 0.0071, weighted_loss: 0.0066, label: 0, bag_size: 22828\n",
      "batch 239, loss: 0.0400, instance_loss: 0.0011, weighted_loss: 0.0283, label: 1, bag_size: 9230\n",
      "batch 259, loss: 1.5988, instance_loss: 3.5710, weighted_loss: 2.1904, label: 0, bag_size: 17279\n",
      "batch 279, loss: 0.0028, instance_loss: 0.0612, weighted_loss: 0.0203, label: 0, bag_size: 18045\n",
      "batch 299, loss: 0.1175, instance_loss: 0.0020, weighted_loss: 0.0829, label: 1, bag_size: 20767\n",
      "batch 319, loss: 0.0023, instance_loss: 0.0687, weighted_loss: 0.0222, label: 0, bag_size: 10535\n",
      "batch 339, loss: 0.1218, instance_loss: 0.0450, weighted_loss: 0.0988, label: 0, bag_size: 12732\n",
      "batch 359, loss: 0.0334, instance_loss: 0.0082, weighted_loss: 0.0259, label: 1, bag_size: 8438\n",
      "batch 379, loss: 0.0079, instance_loss: 0.0607, weighted_loss: 0.0238, label: 0, bag_size: 9471\n",
      "batch 399, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 13218\n",
      "batch 419, loss: 0.0275, instance_loss: 0.0018, weighted_loss: 0.0198, label: 1, bag_size: 9955\n",
      "batch 439, loss: 0.0026, instance_loss: 0.0000, weighted_loss: 0.0018, label: 1, bag_size: 15213\n",
      "batch 459, loss: 0.0315, instance_loss: 0.1033, weighted_loss: 0.0530, label: 1, bag_size: 20333\n",
      "batch 479, loss: 0.0646, instance_loss: 0.0273, weighted_loss: 0.0534, label: 1, bag_size: 4821\n",
      "batch 499, loss: 0.0864, instance_loss: 0.1571, weighted_loss: 0.1076, label: 0, bag_size: 1760\n",
      "batch 519, loss: 0.5299, instance_loss: 1.6272, weighted_loss: 0.8591, label: 1, bag_size: 2395\n",
      "batch 539, loss: 0.0225, instance_loss: 0.0017, weighted_loss: 0.0162, label: 0, bag_size: 20555\n",
      "batch 559, loss: 0.0223, instance_loss: 0.0000, weighted_loss: 0.0156, label: 1, bag_size: 4102\n",
      "batch 579, loss: 0.0175, instance_loss: 0.0000, weighted_loss: 0.0123, label: 0, bag_size: 18954\n",
      "batch 599, loss: 0.0293, instance_loss: 0.0000, weighted_loss: 0.0205, label: 0, bag_size: 5551\n",
      "batch 619, loss: 0.0004, instance_loss: 0.0600, weighted_loss: 0.0183, label: 0, bag_size: 2424\n",
      "batch 639, loss: 0.0033, instance_loss: 0.3243, weighted_loss: 0.0996, label: 0, bag_size: 10415\n",
      "batch 659, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 5221\n",
      "batch 679, loss: 0.1906, instance_loss: 0.0320, weighted_loss: 0.1431, label: 0, bag_size: 16521\n",
      "batch 699, loss: 0.6114, instance_loss: 0.2998, weighted_loss: 0.5179, label: 0, bag_size: 1800\n",
      "batch 719, loss: 0.0034, instance_loss: 0.0000, weighted_loss: 0.0024, label: 0, bag_size: 6851\n",
      "batch 739, loss: 0.0123, instance_loss: 0.0002, weighted_loss: 0.0086, label: 1, bag_size: 11387\n",
      "batch 759, loss: 0.3428, instance_loss: 0.1709, weighted_loss: 0.2913, label: 1, bag_size: 1609\n",
      "batch 779, loss: 1.0095, instance_loss: 0.7517, weighted_loss: 0.9322, label: 0, bag_size: 7835\n",
      "batch 799, loss: 1.5488, instance_loss: 1.0651, weighted_loss: 1.4037, label: 1, bag_size: 13362\n",
      "batch 819, loss: 0.0024, instance_loss: 0.0001, weighted_loss: 0.0017, label: 1, bag_size: 5731\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9708029197080292: correct 12768/13152\n",
      "class 1 clustering acc 0.8362226277372263: correct 5499/6576\n",
      "Epoch: 28, train_loss: 0.2213, train_clustering_loss:  0.2875, train_error: 0.0766\n",
      "class 0: acc 0.9210526315789473, correct 385/418\n",
      "class 1: acc 0.9257425742574258, correct 374/404\n",
      "\n",
      "Val Set, val_loss: 0.3420, val_error: 0.1101, auc: 0.9389\n",
      "class 0 clustering acc 0.9862385321100917: correct 1720/1744\n",
      "class 1 clustering acc 0.8692660550458715: correct 758/872\n",
      "class 0: acc 0.9130434782608695, correct 42/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0969, instance_loss: 0.5453, weighted_loss: 0.2314, label: 0, bag_size: 2004\n",
      "batch 39, loss: 0.7128, instance_loss: 0.6856, weighted_loss: 0.7046, label: 1, bag_size: 15192\n",
      "batch 59, loss: 0.0493, instance_loss: 0.0209, weighted_loss: 0.0408, label: 0, bag_size: 12732\n",
      "batch 79, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 27158\n",
      "batch 99, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 23037\n",
      "batch 119, loss: 0.0520, instance_loss: 0.0091, weighted_loss: 0.0392, label: 1, bag_size: 1786\n",
      "batch 139, loss: 0.5536, instance_loss: 0.2849, weighted_loss: 0.4730, label: 1, bag_size: 7748\n",
      "batch 159, loss: 0.0773, instance_loss: 0.0540, weighted_loss: 0.0703, label: 0, bag_size: 12083\n",
      "batch 179, loss: 0.0114, instance_loss: 0.0001, weighted_loss: 0.0080, label: 1, bag_size: 7371\n",
      "batch 199, loss: 0.6323, instance_loss: 0.8432, weighted_loss: 0.6956, label: 1, bag_size: 1497\n",
      "batch 219, loss: 0.4075, instance_loss: 0.1116, weighted_loss: 0.3188, label: 1, bag_size: 7989\n",
      "batch 239, loss: 0.0470, instance_loss: 0.0000, weighted_loss: 0.0329, label: 1, bag_size: 5494\n",
      "batch 259, loss: 0.0243, instance_loss: 0.0000, weighted_loss: 0.0170, label: 1, bag_size: 6781\n",
      "batch 279, loss: 0.0054, instance_loss: 0.0000, weighted_loss: 0.0038, label: 1, bag_size: 13015\n",
      "batch 299, loss: 0.3468, instance_loss: 0.2469, weighted_loss: 0.3168, label: 0, bag_size: 4598\n",
      "batch 319, loss: 0.2374, instance_loss: 0.0063, weighted_loss: 0.1681, label: 0, bag_size: 11390\n",
      "batch 339, loss: 0.0136, instance_loss: 0.0010, weighted_loss: 0.0098, label: 1, bag_size: 7217\n",
      "batch 359, loss: 0.0310, instance_loss: 0.0334, weighted_loss: 0.0317, label: 1, bag_size: 2308\n",
      "batch 379, loss: 0.0362, instance_loss: 0.0010, weighted_loss: 0.0256, label: 1, bag_size: 2678\n",
      "batch 399, loss: 0.0939, instance_loss: 0.0030, weighted_loss: 0.0666, label: 0, bag_size: 9387\n",
      "batch 419, loss: 0.0217, instance_loss: 0.0001, weighted_loss: 0.0152, label: 1, bag_size: 4821\n",
      "batch 439, loss: 0.2018, instance_loss: 0.0000, weighted_loss: 0.1413, label: 0, bag_size: 25420\n",
      "batch 459, loss: 0.2364, instance_loss: 0.0541, weighted_loss: 0.1817, label: 1, bag_size: 5723\n",
      "batch 479, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 11546\n",
      "batch 499, loss: 0.0254, instance_loss: 0.0000, weighted_loss: 0.0178, label: 0, bag_size: 18777\n",
      "batch 519, loss: 0.0220, instance_loss: 0.0637, weighted_loss: 0.0345, label: 1, bag_size: 6776\n",
      "batch 539, loss: 0.1852, instance_loss: 0.0036, weighted_loss: 0.1307, label: 1, bag_size: 2480\n",
      "batch 559, loss: 0.0057, instance_loss: 0.0002, weighted_loss: 0.0040, label: 0, bag_size: 4497\n",
      "batch 579, loss: 0.1397, instance_loss: 0.0238, weighted_loss: 0.1049, label: 0, bag_size: 18738\n",
      "batch 599, loss: 0.0998, instance_loss: 0.0504, weighted_loss: 0.0850, label: 1, bag_size: 2480\n",
      "batch 619, loss: 0.0024, instance_loss: 0.0678, weighted_loss: 0.0220, label: 1, bag_size: 1316\n",
      "batch 639, loss: 0.0041, instance_loss: 0.0000, weighted_loss: 0.0029, label: 0, bag_size: 15967\n",
      "batch 659, loss: 0.1195, instance_loss: 0.0320, weighted_loss: 0.0932, label: 0, bag_size: 7557\n",
      "batch 679, loss: 0.0039, instance_loss: 0.0000, weighted_loss: 0.0028, label: 1, bag_size: 5317\n",
      "batch 699, loss: 0.0713, instance_loss: 0.0001, weighted_loss: 0.0499, label: 1, bag_size: 19972\n",
      "batch 719, loss: 0.0233, instance_loss: 0.0185, weighted_loss: 0.0219, label: 1, bag_size: 6599\n",
      "batch 739, loss: 0.0107, instance_loss: 0.0002, weighted_loss: 0.0075, label: 1, bag_size: 8466\n",
      "batch 759, loss: 0.0485, instance_loss: 0.0163, weighted_loss: 0.0389, label: 1, bag_size: 2308\n",
      "batch 779, loss: 0.0065, instance_loss: 0.1826, weighted_loss: 0.0593, label: 1, bag_size: 6950\n",
      "batch 799, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 21218\n",
      "batch 819, loss: 0.0102, instance_loss: 0.0000, weighted_loss: 0.0071, label: 1, bag_size: 3409\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9838807785888077: correct 12940/13152\n",
      "class 1 clustering acc 0.8868613138686131: correct 5832/6576\n",
      "Epoch: 29, train_loss: 0.2075, train_clustering_loss:  0.1814, train_error: 0.0815\n",
      "class 0: acc 0.9138755980861244, correct 382/418\n",
      "class 1: acc 0.9232673267326733, correct 373/404\n",
      "\n",
      "Val Set, val_loss: 0.3558, val_error: 0.1376, auc: 0.9386\n",
      "class 0 clustering acc 0.9805045871559633: correct 1710/1744\n",
      "class 1 clustering acc 0.9036697247706422: correct 788/872\n",
      "class 0: acc 0.9130434782608695, correct 42/46\n",
      "class 1: acc 0.8253968253968254, correct 52/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0176, instance_loss: 0.0000, weighted_loss: 0.0123, label: 1, bag_size: 19832\n",
      "batch 39, loss: 0.0037, instance_loss: 0.0008, weighted_loss: 0.0029, label: 0, bag_size: 14305\n",
      "batch 59, loss: 0.0143, instance_loss: 0.0914, weighted_loss: 0.0374, label: 0, bag_size: 1202\n",
      "batch 79, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 19435\n",
      "batch 99, loss: 0.0363, instance_loss: 0.0000, weighted_loss: 0.0254, label: 1, bag_size: 13786\n",
      "batch 119, loss: 0.0353, instance_loss: 0.0024, weighted_loss: 0.0254, label: 1, bag_size: 2344\n",
      "batch 139, loss: 0.0010, instance_loss: 0.0001, weighted_loss: 0.0008, label: 0, bag_size: 15967\n",
      "batch 159, loss: 0.0368, instance_loss: 0.0004, weighted_loss: 0.0259, label: 1, bag_size: 13786\n",
      "batch 179, loss: 0.0260, instance_loss: 0.0484, weighted_loss: 0.0327, label: 1, bag_size: 4786\n",
      "batch 199, loss: 0.2764, instance_loss: 1.4572, weighted_loss: 0.6307, label: 0, bag_size: 4345\n",
      "batch 219, loss: 4.7733, instance_loss: 1.7307, weighted_loss: 3.8605, label: 0, bag_size: 3468\n",
      "batch 239, loss: 0.0080, instance_loss: 0.0000, weighted_loss: 0.0056, label: 1, bag_size: 3207\n",
      "batch 259, loss: 4.7690, instance_loss: 0.3840, weighted_loss: 3.4535, label: 0, bag_size: 3802\n",
      "batch 279, loss: 0.3563, instance_loss: 0.0764, weighted_loss: 0.2723, label: 0, bag_size: 15898\n",
      "batch 299, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 7191\n",
      "batch 319, loss: 0.5256, instance_loss: 0.0001, weighted_loss: 0.3680, label: 0, bag_size: 3876\n",
      "batch 339, loss: 0.0840, instance_loss: 0.2810, weighted_loss: 0.1431, label: 1, bag_size: 7148\n",
      "batch 359, loss: 0.0893, instance_loss: 0.0010, weighted_loss: 0.0628, label: 0, bag_size: 10029\n",
      "batch 379, loss: 0.0208, instance_loss: 0.0006, weighted_loss: 0.0147, label: 1, bag_size: 8466\n",
      "batch 399, loss: 0.0851, instance_loss: 0.0120, weighted_loss: 0.0632, label: 0, bag_size: 3725\n",
      "batch 419, loss: 0.0046, instance_loss: 0.0005, weighted_loss: 0.0034, label: 1, bag_size: 10033\n",
      "batch 439, loss: 0.1477, instance_loss: 0.0074, weighted_loss: 0.1056, label: 1, bag_size: 16034\n",
      "batch 459, loss: 0.0518, instance_loss: 0.2147, weighted_loss: 0.1006, label: 0, bag_size: 7989\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0251, weighted_loss: 0.0076, label: 0, bag_size: 1984\n",
      "batch 499, loss: 0.0555, instance_loss: 0.0063, weighted_loss: 0.0408, label: 1, bag_size: 7148\n",
      "batch 519, loss: 1.4043, instance_loss: 1.7238, weighted_loss: 1.5002, label: 1, bag_size: 2935\n",
      "batch 539, loss: 1.1468, instance_loss: 0.7296, weighted_loss: 1.0216, label: 0, bag_size: 24382\n",
      "batch 559, loss: 0.2697, instance_loss: 0.1329, weighted_loss: 0.2287, label: 1, bag_size: 9561\n",
      "batch 579, loss: 0.0114, instance_loss: 0.0188, weighted_loss: 0.0136, label: 1, bag_size: 8216\n",
      "batch 599, loss: 0.0487, instance_loss: 0.0008, weighted_loss: 0.0343, label: 0, bag_size: 12793\n",
      "batch 619, loss: 0.0122, instance_loss: 0.0000, weighted_loss: 0.0085, label: 1, bag_size: 12408\n",
      "batch 639, loss: 0.0257, instance_loss: 0.0027, weighted_loss: 0.0188, label: 0, bag_size: 13795\n",
      "batch 659, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 13218\n",
      "batch 679, loss: 0.0050, instance_loss: 0.0000, weighted_loss: 0.0035, label: 0, bag_size: 24911\n",
      "batch 699, loss: 0.4785, instance_loss: 0.9113, weighted_loss: 0.6083, label: 0, bag_size: 7917\n",
      "batch 719, loss: 0.6839, instance_loss: 0.2364, weighted_loss: 0.5496, label: 1, bag_size: 1339\n",
      "batch 739, loss: 0.0173, instance_loss: 0.0000, weighted_loss: 0.0121, label: 1, bag_size: 3224\n",
      "batch 759, loss: 0.1195, instance_loss: 0.0022, weighted_loss: 0.0843, label: 1, bag_size: 2678\n",
      "batch 779, loss: 0.2426, instance_loss: 0.0367, weighted_loss: 0.1809, label: 0, bag_size: 7557\n",
      "batch 799, loss: 0.0202, instance_loss: 0.3245, weighted_loss: 0.1115, label: 0, bag_size: 931\n",
      "batch 819, loss: 0.0042, instance_loss: 0.0000, weighted_loss: 0.0029, label: 0, bag_size: 11113\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9797749391727494: correct 12886/13152\n",
      "class 1 clustering acc 0.8822992700729927: correct 5802/6576\n",
      "Epoch: 30, train_loss: 0.2449, train_clustering_loss:  0.2055, train_error: 0.0961\n",
      "class 0: acc 0.8953771289537713, correct 368/411\n",
      "class 1: acc 0.9124087591240876, correct 375/411\n",
      "\n",
      "Val Set, val_loss: 0.4311, val_error: 0.1284, auc: 0.9417\n",
      "class 0 clustering acc 0.9713302752293578: correct 1694/1744\n",
      "class 1 clustering acc 0.8910550458715596: correct 777/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.7777777777777778, correct 49/63\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0225, instance_loss: 0.0000, weighted_loss: 0.0157, label: 1, bag_size: 10498\n",
      "batch 39, loss: 0.0034, instance_loss: 0.0000, weighted_loss: 0.0024, label: 0, bag_size: 22828\n",
      "batch 59, loss: 1.4107, instance_loss: 0.8217, weighted_loss: 1.2340, label: 1, bag_size: 1284\n",
      "batch 79, loss: 0.0656, instance_loss: 0.0007, weighted_loss: 0.0462, label: 1, bag_size: 8592\n",
      "batch 99, loss: 1.0926, instance_loss: 1.3060, weighted_loss: 1.1567, label: 0, bag_size: 1592\n",
      "batch 119, loss: 0.0874, instance_loss: 0.0000, weighted_loss: 0.0612, label: 1, bag_size: 17579\n",
      "batch 139, loss: 0.0038, instance_loss: 0.0000, weighted_loss: 0.0026, label: 1, bag_size: 5317\n",
      "batch 159, loss: 2.1650, instance_loss: 0.1527, weighted_loss: 1.5613, label: 0, bag_size: 17279\n",
      "batch 179, loss: 0.0760, instance_loss: 0.0010, weighted_loss: 0.0535, label: 1, bag_size: 5605\n",
      "batch 199, loss: 0.0410, instance_loss: 0.0000, weighted_loss: 0.0287, label: 1, bag_size: 28527\n",
      "batch 219, loss: 0.0039, instance_loss: 0.0000, weighted_loss: 0.0027, label: 1, bag_size: 19932\n",
      "batch 239, loss: 0.1533, instance_loss: 0.1949, weighted_loss: 0.1658, label: 1, bag_size: 3368\n",
      "batch 259, loss: 0.0212, instance_loss: 0.0044, weighted_loss: 0.0161, label: 1, bag_size: 699\n",
      "batch 279, loss: 0.1343, instance_loss: 0.0627, weighted_loss: 0.1128, label: 1, bag_size: 16154\n",
      "batch 299, loss: 0.0086, instance_loss: 0.0000, weighted_loss: 0.0060, label: 1, bag_size: 20537\n",
      "batch 319, loss: 0.1861, instance_loss: 0.0420, weighted_loss: 0.1429, label: 0, bag_size: 11922\n",
      "batch 339, loss: 0.0032, instance_loss: 0.0000, weighted_loss: 0.0022, label: 0, bag_size: 16052\n",
      "batch 359, loss: 0.0300, instance_loss: 0.0327, weighted_loss: 0.0308, label: 0, bag_size: 1826\n",
      "batch 379, loss: 1.9633, instance_loss: 0.0290, weighted_loss: 1.3830, label: 0, bag_size: 5211\n",
      "batch 399, loss: 0.2199, instance_loss: 2.4198, weighted_loss: 0.8799, label: 0, bag_size: 1690\n",
      "batch 419, loss: 0.0024, instance_loss: 0.0002, weighted_loss: 0.0018, label: 1, bag_size: 13051\n",
      "batch 439, loss: 0.0007, instance_loss: 0.0046, weighted_loss: 0.0019, label: 1, bag_size: 617\n",
      "batch 459, loss: 0.0063, instance_loss: 0.0000, weighted_loss: 0.0044, label: 0, bag_size: 26271\n",
      "batch 479, loss: 0.0017, instance_loss: 0.0005, weighted_loss: 0.0013, label: 0, bag_size: 11512\n",
      "batch 499, loss: 0.0410, instance_loss: 0.0023, weighted_loss: 0.0294, label: 0, bag_size: 2036\n",
      "batch 519, loss: 0.0391, instance_loss: 0.0491, weighted_loss: 0.0421, label: 0, bag_size: 11259\n",
      "batch 539, loss: 0.7117, instance_loss: 1.3771, weighted_loss: 0.9113, label: 0, bag_size: 2270\n",
      "batch 559, loss: 0.0837, instance_loss: 0.0006, weighted_loss: 0.0588, label: 0, bag_size: 25814\n",
      "batch 579, loss: 0.0552, instance_loss: 0.8908, weighted_loss: 0.3059, label: 1, bag_size: 9519\n",
      "batch 599, loss: 0.0018, instance_loss: 0.0000, weighted_loss: 0.0013, label: 1, bag_size: 14223\n",
      "batch 619, loss: 0.0051, instance_loss: 0.0000, weighted_loss: 0.0036, label: 1, bag_size: 17486\n",
      "batch 639, loss: 0.0599, instance_loss: 0.0000, weighted_loss: 0.0419, label: 1, bag_size: 18603\n",
      "batch 659, loss: 0.6230, instance_loss: 0.0058, weighted_loss: 0.4379, label: 1, bag_size: 7748\n",
      "batch 679, loss: 0.0307, instance_loss: 0.9085, weighted_loss: 0.2940, label: 1, bag_size: 3619\n",
      "batch 699, loss: 0.0006, instance_loss: 0.0036, weighted_loss: 0.0015, label: 0, bag_size: 13964\n",
      "batch 719, loss: 0.0215, instance_loss: 0.1145, weighted_loss: 0.0494, label: 0, bag_size: 1458\n",
      "batch 739, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 14319\n",
      "batch 759, loss: 0.0139, instance_loss: 0.0000, weighted_loss: 0.0097, label: 1, bag_size: 13015\n",
      "batch 779, loss: 0.0645, instance_loss: 0.0869, weighted_loss: 0.0712, label: 1, bag_size: 2814\n",
      "batch 799, loss: 0.0031, instance_loss: 0.0035, weighted_loss: 0.0032, label: 0, bag_size: 6851\n",
      "batch 819, loss: 0.0026, instance_loss: 0.0000, weighted_loss: 0.0018, label: 0, bag_size: 31106\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9813716545012166: correct 12907/13152\n",
      "class 1 clustering acc 0.8877737226277372: correct 5838/6576\n",
      "Epoch: 31, train_loss: 0.1816, train_clustering_loss:  0.1975, train_error: 0.0730\n",
      "class 0: acc 0.9154228855721394, correct 368/402\n",
      "class 1: acc 0.9380952380952381, correct 394/420\n",
      "\n",
      "Val Set, val_loss: 0.3438, val_error: 0.1009, auc: 0.9431\n",
      "class 0 clustering acc 0.9678899082568807: correct 1688/1744\n",
      "class 1 clustering acc 0.8623853211009175: correct 752/872\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0145, instance_loss: 0.2080, weighted_loss: 0.0725, label: 1, bag_size: 1022\n",
      "batch 39, loss: 0.3638, instance_loss: 0.0327, weighted_loss: 0.2645, label: 0, bag_size: 10029\n",
      "batch 59, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 5049\n",
      "batch 79, loss: 0.0192, instance_loss: 0.0008, weighted_loss: 0.0137, label: 1, bag_size: 8040\n",
      "batch 99, loss: 0.0029, instance_loss: 0.0001, weighted_loss: 0.0021, label: 1, bag_size: 2936\n",
      "batch 119, loss: 0.0029, instance_loss: 0.0000, weighted_loss: 0.0020, label: 1, bag_size: 6769\n",
      "batch 139, loss: 0.0559, instance_loss: 0.0104, weighted_loss: 0.0423, label: 0, bag_size: 2760\n",
      "batch 159, loss: 0.0028, instance_loss: 0.0001, weighted_loss: 0.0020, label: 1, bag_size: 4102\n",
      "batch 179, loss: 0.0186, instance_loss: 0.3340, weighted_loss: 0.1132, label: 0, bag_size: 1772\n",
      "batch 199, loss: 0.0060, instance_loss: 0.3617, weighted_loss: 0.1127, label: 0, bag_size: 2091\n",
      "batch 219, loss: 0.3322, instance_loss: 0.2885, weighted_loss: 0.3191, label: 1, bag_size: 3211\n",
      "batch 239, loss: 0.0057, instance_loss: 0.0006, weighted_loss: 0.0042, label: 0, bag_size: 4497\n",
      "batch 259, loss: 1.5830, instance_loss: 0.8723, weighted_loss: 1.3698, label: 1, bag_size: 2842\n",
      "batch 279, loss: 0.0176, instance_loss: 0.0005, weighted_loss: 0.0125, label: 0, bag_size: 19808\n",
      "batch 299, loss: 0.1595, instance_loss: 0.0870, weighted_loss: 0.1377, label: 1, bag_size: 2455\n",
      "batch 319, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 19472\n",
      "batch 339, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 15313\n",
      "batch 359, loss: 0.0079, instance_loss: 0.0003, weighted_loss: 0.0056, label: 1, bag_size: 16051\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 4442\n",
      "batch 399, loss: 0.0917, instance_loss: 0.0012, weighted_loss: 0.0646, label: 0, bag_size: 17083\n",
      "batch 419, loss: 0.5816, instance_loss: 0.6113, weighted_loss: 0.5905, label: 0, bag_size: 11212\n",
      "batch 439, loss: 0.2660, instance_loss: 0.0021, weighted_loss: 0.1868, label: 1, bag_size: 10432\n",
      "batch 459, loss: 0.0054, instance_loss: 0.0000, weighted_loss: 0.0038, label: 1, bag_size: 7583\n",
      "batch 479, loss: 0.2718, instance_loss: 0.4849, weighted_loss: 0.3357, label: 0, bag_size: 1690\n",
      "batch 499, loss: 0.0232, instance_loss: 0.0324, weighted_loss: 0.0260, label: 0, bag_size: 2351\n",
      "batch 519, loss: 0.3140, instance_loss: 0.1378, weighted_loss: 0.2611, label: 0, bag_size: 6898\n",
      "batch 539, loss: 0.0303, instance_loss: 0.0000, weighted_loss: 0.0212, label: 0, bag_size: 30828\n",
      "batch 559, loss: 0.0228, instance_loss: 0.0000, weighted_loss: 0.0159, label: 1, bag_size: 4821\n",
      "batch 579, loss: 0.0195, instance_loss: 0.0000, weighted_loss: 0.0137, label: 1, bag_size: 15609\n",
      "batch 599, loss: 0.0135, instance_loss: 0.0000, weighted_loss: 0.0094, label: 1, bag_size: 10969\n",
      "batch 619, loss: 1.9035, instance_loss: 1.9831, weighted_loss: 1.9274, label: 0, bag_size: 4692\n",
      "batch 639, loss: 0.0041, instance_loss: 0.0002, weighted_loss: 0.0029, label: 0, bag_size: 19518\n",
      "batch 659, loss: 0.0025, instance_loss: 0.0000, weighted_loss: 0.0017, label: 1, bag_size: 2686\n",
      "batch 679, loss: 0.0184, instance_loss: 0.0000, weighted_loss: 0.0129, label: 1, bag_size: 18603\n",
      "batch 699, loss: 0.0762, instance_loss: 0.0313, weighted_loss: 0.0627, label: 1, bag_size: 7981\n",
      "batch 719, loss: 0.0130, instance_loss: 0.4517, weighted_loss: 0.1446, label: 1, bag_size: 1255\n",
      "batch 739, loss: 2.6042, instance_loss: 1.9054, weighted_loss: 2.3945, label: 0, bag_size: 14264\n",
      "batch 759, loss: 0.0251, instance_loss: 0.4331, weighted_loss: 0.1475, label: 0, bag_size: 2195\n",
      "batch 779, loss: 0.0967, instance_loss: 0.2452, weighted_loss: 0.1412, label: 0, bag_size: 9069\n",
      "batch 799, loss: 0.1936, instance_loss: 0.0238, weighted_loss: 0.1427, label: 0, bag_size: 15071\n",
      "batch 819, loss: 0.0251, instance_loss: 0.0000, weighted_loss: 0.0176, label: 1, bag_size: 5494\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9835006082725061: correct 12935/13152\n",
      "class 1 clustering acc 0.8956812652068127: correct 5890/6576\n",
      "Epoch: 32, train_loss: 0.2015, train_clustering_loss:  0.1797, train_error: 0.0779\n",
      "class 0: acc 0.9095607235142119, correct 352/387\n",
      "class 1: acc 0.9333333333333333, correct 406/435\n",
      "\n",
      "Val Set, val_loss: 0.4687, val_error: 0.1376, auc: 0.9438\n",
      "class 0 clustering acc 0.9592889908256881: correct 1673/1744\n",
      "class 1 clustering acc 0.856651376146789: correct 747/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.7619047619047619, correct 48/63\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.0344, weighted_loss: 0.0104, label: 1, bag_size: 1360\n",
      "batch 39, loss: 0.0067, instance_loss: 0.2147, weighted_loss: 0.0691, label: 0, bag_size: 3101\n",
      "batch 59, loss: 0.0018, instance_loss: 0.0932, weighted_loss: 0.0292, label: 0, bag_size: 9885\n",
      "batch 79, loss: 0.2878, instance_loss: 0.0041, weighted_loss: 0.2027, label: 1, bag_size: 2559\n",
      "batch 99, loss: 0.0103, instance_loss: 0.0000, weighted_loss: 0.0072, label: 0, bag_size: 14828\n",
      "batch 119, loss: 0.0445, instance_loss: 0.0106, weighted_loss: 0.0344, label: 1, bag_size: 10432\n",
      "batch 139, loss: 0.0184, instance_loss: 0.0025, weighted_loss: 0.0137, label: 1, bag_size: 8685\n",
      "batch 159, loss: 0.0019, instance_loss: 0.2572, weighted_loss: 0.0785, label: 0, bag_size: 803\n",
      "batch 179, loss: 0.0007, instance_loss: 0.1619, weighted_loss: 0.0490, label: 1, bag_size: 3634\n",
      "batch 199, loss: 1.5126, instance_loss: 2.0443, weighted_loss: 1.6721, label: 1, bag_size: 9215\n",
      "batch 219, loss: 0.0043, instance_loss: 0.0095, weighted_loss: 0.0059, label: 0, bag_size: 18045\n",
      "batch 239, loss: 0.0002, instance_loss: 0.0782, weighted_loss: 0.0236, label: 0, bag_size: 3787\n",
      "batch 259, loss: 0.0465, instance_loss: 0.8353, weighted_loss: 0.2831, label: 1, bag_size: 2579\n",
      "batch 279, loss: 0.0521, instance_loss: 0.0094, weighted_loss: 0.0393, label: 1, bag_size: 14202\n",
      "batch 299, loss: 0.0438, instance_loss: 0.0664, weighted_loss: 0.0506, label: 1, bag_size: 2579\n",
      "batch 319, loss: 0.0345, instance_loss: 0.0000, weighted_loss: 0.0241, label: 1, bag_size: 15609\n",
      "batch 339, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 19390\n",
      "batch 359, loss: 0.0025, instance_loss: 0.0244, weighted_loss: 0.0091, label: 0, bag_size: 17368\n",
      "batch 379, loss: 0.1380, instance_loss: 0.0176, weighted_loss: 0.1019, label: 1, bag_size: 8026\n",
      "batch 399, loss: 0.0048, instance_loss: 0.0213, weighted_loss: 0.0097, label: 0, bag_size: 14377\n",
      "batch 419, loss: 0.0401, instance_loss: 0.0058, weighted_loss: 0.0298, label: 1, bag_size: 4054\n",
      "batch 439, loss: 0.0010, instance_loss: 0.2518, weighted_loss: 0.0763, label: 1, bag_size: 2405\n",
      "batch 459, loss: 0.0344, instance_loss: 0.0011, weighted_loss: 0.0244, label: 1, bag_size: 6171\n",
      "batch 479, loss: 0.0050, instance_loss: 0.0000, weighted_loss: 0.0035, label: 0, bag_size: 12201\n",
      "batch 499, loss: 0.0353, instance_loss: 0.2921, weighted_loss: 0.1123, label: 1, bag_size: 1014\n",
      "batch 519, loss: 0.0287, instance_loss: 0.1773, weighted_loss: 0.0732, label: 1, bag_size: 4330\n",
      "batch 539, loss: 0.0217, instance_loss: 0.3234, weighted_loss: 0.1122, label: 0, bag_size: 2036\n",
      "batch 559, loss: 0.0707, instance_loss: 0.0011, weighted_loss: 0.0498, label: 1, bag_size: 21701\n",
      "batch 579, loss: 0.0024, instance_loss: 0.0001, weighted_loss: 0.0017, label: 1, bag_size: 3207\n",
      "batch 599, loss: 0.0008, instance_loss: 0.0640, weighted_loss: 0.0197, label: 0, bag_size: 3190\n",
      "batch 619, loss: 0.0184, instance_loss: 0.0000, weighted_loss: 0.0129, label: 0, bag_size: 15003\n",
      "batch 639, loss: 0.0197, instance_loss: 0.0093, weighted_loss: 0.0166, label: 1, bag_size: 3968\n",
      "batch 659, loss: 0.0185, instance_loss: 0.0000, weighted_loss: 0.0129, label: 0, bag_size: 23791\n",
      "batch 679, loss: 0.0438, instance_loss: 0.0058, weighted_loss: 0.0324, label: 1, bag_size: 8935\n",
      "batch 699, loss: 0.0613, instance_loss: 0.0090, weighted_loss: 0.0456, label: 0, bag_size: 2518\n",
      "batch 719, loss: 0.5603, instance_loss: 0.0090, weighted_loss: 0.3949, label: 1, bag_size: 4939\n",
      "batch 739, loss: 0.2810, instance_loss: 0.0197, weighted_loss: 0.2026, label: 1, bag_size: 4929\n",
      "batch 759, loss: 0.0364, instance_loss: 0.0191, weighted_loss: 0.0312, label: 0, bag_size: 13205\n",
      "batch 779, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0014, label: 0, bag_size: 19659\n",
      "batch 799, loss: 1.4880, instance_loss: 0.1825, weighted_loss: 1.0964, label: 1, bag_size: 12494\n",
      "batch 819, loss: 1.3869, instance_loss: 0.0008, weighted_loss: 0.9711, label: 0, bag_size: 5211\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9767335766423357: correct 12846/13152\n",
      "class 1 clustering acc 0.8911192214111923: correct 5860/6576\n",
      "Epoch: 33, train_loss: 0.2143, train_clustering_loss:  0.2214, train_error: 0.0864\n",
      "class 0: acc 0.9086651053864169, correct 388/427\n",
      "class 1: acc 0.9189873417721519, correct 363/395\n",
      "\n",
      "Val Set, val_loss: 0.4122, val_error: 0.1193, auc: 0.9451\n",
      "class 0 clustering acc 0.9655963302752294: correct 1684/1744\n",
      "class 1 clustering acc 0.7694954128440367: correct 671/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.7936507936507936, correct 50/63\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0047, instance_loss: 0.0138, weighted_loss: 0.0074, label: 1, bag_size: 645\n",
      "batch 39, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 15850\n",
      "batch 59, loss: 0.0478, instance_loss: 0.0287, weighted_loss: 0.0421, label: 1, bag_size: 11223\n",
      "batch 79, loss: 0.5091, instance_loss: 1.0205, weighted_loss: 0.6625, label: 0, bag_size: 3783\n",
      "batch 99, loss: 0.1335, instance_loss: 0.0498, weighted_loss: 0.1084, label: 1, bag_size: 2904\n",
      "batch 119, loss: 0.0032, instance_loss: 0.0002, weighted_loss: 0.0023, label: 1, bag_size: 5561\n",
      "batch 139, loss: 0.4331, instance_loss: 0.2099, weighted_loss: 0.3661, label: 1, bag_size: 16514\n",
      "batch 159, loss: 0.3449, instance_loss: 0.0233, weighted_loss: 0.2484, label: 0, bag_size: 14885\n",
      "batch 179, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 11735\n",
      "batch 199, loss: 0.1767, instance_loss: 0.0096, weighted_loss: 0.1266, label: 0, bag_size: 9421\n",
      "batch 219, loss: 0.0320, instance_loss: 0.0165, weighted_loss: 0.0274, label: 0, bag_size: 12131\n",
      "batch 239, loss: 0.1441, instance_loss: 0.0007, weighted_loss: 0.1011, label: 0, bag_size: 3444\n",
      "batch 259, loss: 0.0018, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 11512\n",
      "batch 279, loss: 0.0030, instance_loss: 0.0001, weighted_loss: 0.0021, label: 0, bag_size: 3232\n",
      "batch 299, loss: 0.0139, instance_loss: 0.0009, weighted_loss: 0.0100, label: 0, bag_size: 2044\n",
      "batch 319, loss: 0.4264, instance_loss: 1.6558, weighted_loss: 0.7952, label: 1, bag_size: 1230\n",
      "batch 339, loss: 0.4894, instance_loss: 0.0880, weighted_loss: 0.3690, label: 1, bag_size: 12714\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0207, weighted_loss: 0.0063, label: 0, bag_size: 14206\n",
      "batch 379, loss: 0.0953, instance_loss: 0.2994, weighted_loss: 0.1565, label: 0, bag_size: 2242\n",
      "batch 399, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 23996\n",
      "batch 419, loss: 0.0210, instance_loss: 0.0089, weighted_loss: 0.0174, label: 1, bag_size: 4308\n",
      "batch 439, loss: 0.0273, instance_loss: 0.0000, weighted_loss: 0.0191, label: 1, bag_size: 8602\n",
      "batch 459, loss: 1.9928, instance_loss: 0.3613, weighted_loss: 1.5034, label: 1, bag_size: 3879\n",
      "batch 479, loss: 0.0158, instance_loss: 0.0000, weighted_loss: 0.0111, label: 1, bag_size: 1823\n",
      "batch 499, loss: 0.0047, instance_loss: 0.0836, weighted_loss: 0.0284, label: 0, bag_size: 2179\n",
      "batch 519, loss: 0.0027, instance_loss: 0.0002, weighted_loss: 0.0019, label: 0, bag_size: 9949\n",
      "batch 539, loss: 0.0391, instance_loss: 0.0059, weighted_loss: 0.0291, label: 1, bag_size: 2356\n",
      "batch 559, loss: 1.6772, instance_loss: 1.4873, weighted_loss: 1.6203, label: 0, bag_size: 2959\n",
      "batch 579, loss: 0.0106, instance_loss: 0.0472, weighted_loss: 0.0216, label: 0, bag_size: 3198\n",
      "batch 599, loss: 0.0836, instance_loss: 0.0101, weighted_loss: 0.0616, label: 0, bag_size: 10415\n",
      "batch 619, loss: 0.3158, instance_loss: 0.2031, weighted_loss: 0.2819, label: 1, bag_size: 771\n",
      "batch 639, loss: 0.0017, instance_loss: 0.0002, weighted_loss: 0.0013, label: 0, bag_size: 11527\n",
      "batch 659, loss: 0.0540, instance_loss: 0.0000, weighted_loss: 0.0378, label: 0, bag_size: 2351\n",
      "batch 679, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 12687\n",
      "batch 699, loss: 0.0081, instance_loss: 0.0000, weighted_loss: 0.0057, label: 1, bag_size: 19832\n",
      "batch 719, loss: 0.4879, instance_loss: 1.4193, weighted_loss: 0.7673, label: 1, bag_size: 983\n",
      "batch 739, loss: 0.2462, instance_loss: 0.0014, weighted_loss: 0.1727, label: 1, bag_size: 10591\n",
      "batch 759, loss: 0.0031, instance_loss: 0.0000, weighted_loss: 0.0021, label: 1, bag_size: 14433\n",
      "batch 779, loss: 0.0070, instance_loss: 0.1134, weighted_loss: 0.0389, label: 0, bag_size: 3474\n",
      "batch 799, loss: 0.6213, instance_loss: 0.0026, weighted_loss: 0.4357, label: 1, bag_size: 15192\n",
      "batch 819, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 8522\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9827402676399026: correct 12925/13152\n",
      "class 1 clustering acc 0.8838199513381995: correct 5812/6576\n",
      "Epoch: 34, train_loss: 0.1905, train_clustering_loss:  0.1928, train_error: 0.0681\n",
      "class 0: acc 0.9226804123711341, correct 358/388\n",
      "class 1: acc 0.9400921658986175, correct 408/434\n",
      "\n",
      "Val Set, val_loss: 0.2915, val_error: 0.1193, auc: 0.9465\n",
      "class 0 clustering acc 0.9386467889908257: correct 1637/1744\n",
      "class 1 clustering acc 0.8268348623853211: correct 721/872\n",
      "class 0: acc 0.8695652173913043, correct 40/46\n",
      "class 1: acc 0.8888888888888888, correct 56/63\n",
      "Validation loss decreased (0.312258 --> 0.291532).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1561, instance_loss: 0.5217, weighted_loss: 0.2657, label: 0, bag_size: 931\n",
      "batch 39, loss: 0.0068, instance_loss: 0.0058, weighted_loss: 0.0065, label: 1, bag_size: 22264\n",
      "batch 59, loss: 0.0409, instance_loss: 0.6898, weighted_loss: 0.2356, label: 0, bag_size: 3265\n",
      "batch 79, loss: 0.0047, instance_loss: 0.0008, weighted_loss: 0.0035, label: 1, bag_size: 2848\n",
      "batch 99, loss: 0.0124, instance_loss: 0.0000, weighted_loss: 0.0087, label: 0, bag_size: 18777\n",
      "batch 119, loss: 2.9202, instance_loss: 0.2468, weighted_loss: 2.1182, label: 1, bag_size: 1819\n",
      "batch 139, loss: 0.2309, instance_loss: 0.0537, weighted_loss: 0.1778, label: 0, bag_size: 7917\n",
      "batch 159, loss: 0.0012, instance_loss: 0.0019, weighted_loss: 0.0014, label: 0, bag_size: 8252\n",
      "batch 179, loss: 0.0046, instance_loss: 0.0000, weighted_loss: 0.0032, label: 1, bag_size: 4102\n",
      "batch 199, loss: 0.0216, instance_loss: 0.0000, weighted_loss: 0.0151, label: 1, bag_size: 7583\n",
      "batch 219, loss: 0.0392, instance_loss: 0.0000, weighted_loss: 0.0274, label: 1, bag_size: 10028\n",
      "batch 239, loss: 0.0961, instance_loss: 0.0053, weighted_loss: 0.0688, label: 0, bag_size: 1831\n",
      "batch 259, loss: 0.0098, instance_loss: 0.0011, weighted_loss: 0.0072, label: 1, bag_size: 1172\n",
      "batch 279, loss: 0.0012, instance_loss: 0.0024, weighted_loss: 0.0016, label: 1, bag_size: 6769\n",
      "batch 299, loss: 0.0233, instance_loss: 0.0534, weighted_loss: 0.0323, label: 1, bag_size: 2559\n",
      "batch 319, loss: 0.0334, instance_loss: 0.0000, weighted_loss: 0.0234, label: 0, bag_size: 14828\n",
      "batch 339, loss: 0.1425, instance_loss: 0.2978, weighted_loss: 0.1891, label: 1, bag_size: 2395\n",
      "batch 359, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 13255\n",
      "batch 379, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 19472\n",
      "batch 399, loss: 0.0204, instance_loss: 0.0209, weighted_loss: 0.0206, label: 0, bag_size: 1614\n",
      "batch 419, loss: 0.0309, instance_loss: 0.1348, weighted_loss: 0.0621, label: 0, bag_size: 10721\n",
      "batch 439, loss: 0.4990, instance_loss: 0.0217, weighted_loss: 0.3558, label: 0, bag_size: 12840\n",
      "batch 459, loss: 0.0550, instance_loss: 0.0143, weighted_loss: 0.0428, label: 0, bag_size: 2006\n",
      "batch 479, loss: 0.4000, instance_loss: 0.4515, weighted_loss: 0.4155, label: 0, bag_size: 2270\n",
      "batch 499, loss: 0.5661, instance_loss: 0.0115, weighted_loss: 0.3997, label: 0, bag_size: 5211\n",
      "batch 519, loss: 0.0877, instance_loss: 0.0001, weighted_loss: 0.0614, label: 1, bag_size: 4821\n",
      "batch 539, loss: 0.0214, instance_loss: 0.0009, weighted_loss: 0.0152, label: 1, bag_size: 8685\n",
      "batch 559, loss: 1.1359, instance_loss: 0.2637, weighted_loss: 0.8742, label: 0, bag_size: 14664\n",
      "batch 579, loss: 0.2393, instance_loss: 0.2933, weighted_loss: 0.2555, label: 1, bag_size: 1875\n",
      "batch 599, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 17437\n",
      "batch 619, loss: 0.0217, instance_loss: 0.1997, weighted_loss: 0.0751, label: 0, bag_size: 1772\n",
      "batch 639, loss: 0.0720, instance_loss: 0.2289, weighted_loss: 0.1191, label: 0, bag_size: 6727\n",
      "batch 659, loss: 0.0823, instance_loss: 0.0522, weighted_loss: 0.0732, label: 1, bag_size: 11394\n",
      "batch 679, loss: 0.0002, instance_loss: 0.0006, weighted_loss: 0.0003, label: 1, bag_size: 2485\n",
      "batch 699, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 20150\n",
      "batch 719, loss: 0.0080, instance_loss: 0.0165, weighted_loss: 0.0106, label: 1, bag_size: 4330\n",
      "batch 739, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 15213\n",
      "batch 759, loss: 0.0081, instance_loss: 0.2903, weighted_loss: 0.0927, label: 1, bag_size: 22264\n",
      "batch 779, loss: 1.1558, instance_loss: 0.0849, weighted_loss: 0.8345, label: 0, bag_size: 5120\n",
      "batch 799, loss: 0.1736, instance_loss: 0.0423, weighted_loss: 0.1342, label: 0, bag_size: 14625\n",
      "batch 819, loss: 0.0613, instance_loss: 1.5160, weighted_loss: 0.4978, label: 1, bag_size: 6825\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9797749391727494: correct 12886/13152\n",
      "class 1 clustering acc 0.8953771289537713: correct 5888/6576\n",
      "Epoch: 35, train_loss: 0.2051, train_clustering_loss:  0.2029, train_error: 0.0815\n",
      "class 0: acc 0.9152542372881356, correct 378/413\n",
      "class 1: acc 0.921760391198044, correct 377/409\n",
      "\n",
      "Val Set, val_loss: 0.3305, val_error: 0.0917, auc: 0.9458\n",
      "class 0 clustering acc 0.9587155963302753: correct 1672/1744\n",
      "class 1 clustering acc 0.7958715596330275: correct 694/872\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0846, instance_loss: 0.0004, weighted_loss: 0.0594, label: 1, bag_size: 18603\n",
      "batch 39, loss: 0.0280, instance_loss: 1.5224, weighted_loss: 0.4764, label: 1, bag_size: 1622\n",
      "batch 59, loss: 0.0333, instance_loss: 0.1990, weighted_loss: 0.0830, label: 0, bag_size: 1651\n",
      "batch 79, loss: 0.0655, instance_loss: 0.0070, weighted_loss: 0.0479, label: 1, bag_size: 2480\n",
      "batch 99, loss: 0.4088, instance_loss: 0.6499, weighted_loss: 0.4811, label: 1, bag_size: 1437\n",
      "batch 119, loss: 0.5640, instance_loss: 0.0303, weighted_loss: 0.4039, label: 1, bag_size: 1483\n",
      "batch 139, loss: 0.0271, instance_loss: 0.0053, weighted_loss: 0.0206, label: 0, bag_size: 2534\n",
      "batch 159, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 5225\n",
      "batch 179, loss: 0.0031, instance_loss: 0.0023, weighted_loss: 0.0029, label: 0, bag_size: 12731\n",
      "batch 199, loss: 0.0036, instance_loss: 0.0000, weighted_loss: 0.0025, label: 1, bag_size: 10394\n",
      "batch 219, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 10394\n",
      "batch 239, loss: 0.5419, instance_loss: 0.9753, weighted_loss: 0.6720, label: 1, bag_size: 4939\n",
      "batch 259, loss: 0.0353, instance_loss: 0.0000, weighted_loss: 0.0247, label: 1, bag_size: 29832\n",
      "batch 279, loss: 0.0892, instance_loss: 0.0230, weighted_loss: 0.0694, label: 1, bag_size: 9913\n",
      "batch 299, loss: 0.7381, instance_loss: 0.1551, weighted_loss: 0.5632, label: 0, bag_size: 2070\n",
      "batch 319, loss: 0.0618, instance_loss: 0.0005, weighted_loss: 0.0434, label: 1, bag_size: 2678\n",
      "batch 339, loss: 0.0017, instance_loss: 0.0000, weighted_loss: 0.0012, label: 1, bag_size: 6950\n",
      "batch 359, loss: 0.0164, instance_loss: 0.0002, weighted_loss: 0.0116, label: 1, bag_size: 5345\n",
      "batch 379, loss: 0.1454, instance_loss: 0.0157, weighted_loss: 0.1065, label: 0, bag_size: 6281\n",
      "batch 399, loss: 0.0044, instance_loss: 0.0000, weighted_loss: 0.0031, label: 0, bag_size: 14625\n",
      "batch 419, loss: 0.0005, instance_loss: 0.0004, weighted_loss: 0.0005, label: 1, bag_size: 3634\n",
      "batch 439, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 4102\n",
      "batch 459, loss: 0.0151, instance_loss: 0.0000, weighted_loss: 0.0105, label: 0, bag_size: 21385\n",
      "batch 479, loss: 0.0694, instance_loss: 0.0018, weighted_loss: 0.0491, label: 1, bag_size: 11160\n",
      "batch 499, loss: 0.0423, instance_loss: 0.1349, weighted_loss: 0.0701, label: 0, bag_size: 2148\n",
      "batch 519, loss: 0.0292, instance_loss: 0.0000, weighted_loss: 0.0204, label: 0, bag_size: 9866\n",
      "batch 539, loss: 0.0418, instance_loss: 0.1492, weighted_loss: 0.0740, label: 0, bag_size: 1745\n",
      "batch 559, loss: 0.0316, instance_loss: 0.0268, weighted_loss: 0.0302, label: 0, bag_size: 2006\n",
      "batch 579, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 18468\n",
      "batch 599, loss: 1.2125, instance_loss: 0.4427, weighted_loss: 0.9815, label: 1, bag_size: 7989\n",
      "batch 619, loss: 0.0122, instance_loss: 0.0678, weighted_loss: 0.0289, label: 0, bag_size: 763\n",
      "batch 639, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 14319\n",
      "batch 659, loss: 0.0150, instance_loss: 0.0012, weighted_loss: 0.0109, label: 1, bag_size: 6599\n",
      "batch 679, loss: 0.0086, instance_loss: 0.0000, weighted_loss: 0.0060, label: 1, bag_size: 6734\n",
      "batch 699, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 3437\n",
      "batch 719, loss: 0.1628, instance_loss: 0.1668, weighted_loss: 0.1640, label: 0, bag_size: 7557\n",
      "batch 739, loss: 0.1525, instance_loss: 0.0011, weighted_loss: 0.1071, label: 1, bag_size: 10622\n",
      "batch 759, loss: 0.0257, instance_loss: 0.2364, weighted_loss: 0.0889, label: 0, bag_size: 1614\n",
      "batch 779, loss: 1.2494, instance_loss: 0.0917, weighted_loss: 0.9021, label: 0, bag_size: 2959\n",
      "batch 799, loss: 0.0059, instance_loss: 0.0000, weighted_loss: 0.0041, label: 0, bag_size: 2195\n",
      "batch 819, loss: 0.0504, instance_loss: 0.5938, weighted_loss: 0.2134, label: 0, bag_size: 4523\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9844890510948905: correct 12948/13152\n",
      "class 1 clustering acc 0.9247262773722628: correct 6081/6576\n",
      "Epoch: 36, train_loss: 0.1712, train_clustering_loss:  0.1432, train_error: 0.0633\n",
      "class 0: acc 0.9285714285714286, correct 377/406\n",
      "class 1: acc 0.9447115384615384, correct 393/416\n",
      "\n",
      "Val Set, val_loss: 0.3292, val_error: 0.1009, auc: 0.9500\n",
      "class 0 clustering acc 0.9512614678899083: correct 1659/1744\n",
      "class 1 clustering acc 0.893348623853211: correct 779/872\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0450, instance_loss: 0.0001, weighted_loss: 0.0315, label: 0, bag_size: 4271\n",
      "batch 39, loss: 0.3336, instance_loss: 0.2008, weighted_loss: 0.2937, label: 0, bag_size: 2270\n",
      "batch 59, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 21874\n",
      "batch 79, loss: 0.0083, instance_loss: 0.1631, weighted_loss: 0.0547, label: 0, bag_size: 2091\n",
      "batch 99, loss: 1.7837, instance_loss: 3.2634, weighted_loss: 2.2276, label: 1, bag_size: 12712\n",
      "batch 119, loss: 0.0101, instance_loss: 0.0026, weighted_loss: 0.0079, label: 0, bag_size: 11194\n",
      "batch 139, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 1, bag_size: 11122\n",
      "batch 159, loss: 0.0034, instance_loss: 0.0000, weighted_loss: 0.0024, label: 1, bag_size: 18649\n",
      "batch 179, loss: 0.0002, instance_loss: 0.0012, weighted_loss: 0.0005, label: 1, bag_size: 3634\n",
      "batch 199, loss: 0.0164, instance_loss: 0.0000, weighted_loss: 0.0115, label: 1, bag_size: 6731\n",
      "batch 219, loss: 0.8094, instance_loss: 1.9729, weighted_loss: 1.1585, label: 1, bag_size: 2681\n",
      "batch 239, loss: 0.0038, instance_loss: 0.1646, weighted_loss: 0.0520, label: 0, bag_size: 16720\n",
      "batch 259, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 10867\n",
      "batch 279, loss: 0.1372, instance_loss: 1.3875, weighted_loss: 0.5123, label: 1, bag_size: 16514\n",
      "batch 299, loss: 0.0023, instance_loss: 0.0188, weighted_loss: 0.0073, label: 0, bag_size: 8981\n",
      "batch 319, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 12137\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 20796\n",
      "batch 359, loss: 0.0016, instance_loss: 0.0082, weighted_loss: 0.0036, label: 0, bag_size: 22762\n",
      "batch 379, loss: 0.4982, instance_loss: 0.0215, weighted_loss: 0.3552, label: 0, bag_size: 18516\n",
      "batch 399, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 6317\n",
      "batch 419, loss: 0.0400, instance_loss: 0.0000, weighted_loss: 0.0280, label: 0, bag_size: 9455\n",
      "batch 439, loss: 0.1020, instance_loss: 0.0000, weighted_loss: 0.0714, label: 1, bag_size: 15931\n",
      "batch 459, loss: 0.0400, instance_loss: 0.0030, weighted_loss: 0.0289, label: 1, bag_size: 9062\n",
      "batch 479, loss: 0.0197, instance_loss: 0.4105, weighted_loss: 0.1369, label: 1, bag_size: 3980\n",
      "batch 499, loss: 0.0378, instance_loss: 0.0479, weighted_loss: 0.0408, label: 0, bag_size: 1458\n",
      "batch 519, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 11865\n",
      "batch 539, loss: 0.5973, instance_loss: 0.0221, weighted_loss: 0.4247, label: 0, bag_size: 6093\n",
      "batch 559, loss: 0.0029, instance_loss: 0.0000, weighted_loss: 0.0020, label: 1, bag_size: 9408\n",
      "batch 579, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 21385\n",
      "batch 599, loss: 0.0410, instance_loss: 0.0004, weighted_loss: 0.0288, label: 1, bag_size: 14230\n",
      "batch 619, loss: 0.0155, instance_loss: 0.0001, weighted_loss: 0.0109, label: 1, bag_size: 3450\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 18574\n",
      "batch 659, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 11477\n",
      "batch 679, loss: 0.0161, instance_loss: 0.2089, weighted_loss: 0.0739, label: 0, bag_size: 2490\n",
      "batch 699, loss: 0.0002, instance_loss: 0.0019, weighted_loss: 0.0007, label: 0, bag_size: 18240\n",
      "batch 719, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 12137\n",
      "batch 739, loss: 0.6722, instance_loss: 0.9798, weighted_loss: 0.7645, label: 0, bag_size: 14264\n",
      "batch 759, loss: 0.0156, instance_loss: 0.0084, weighted_loss: 0.0134, label: 0, bag_size: 4497\n",
      "batch 779, loss: 0.0265, instance_loss: 0.0000, weighted_loss: 0.0185, label: 1, bag_size: 16267\n",
      "batch 799, loss: 0.0041, instance_loss: 0.0002, weighted_loss: 0.0029, label: 0, bag_size: 8582\n",
      "batch 819, loss: 0.0088, instance_loss: 0.0419, weighted_loss: 0.0187, label: 0, bag_size: 9866\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9772658150851582: correct 12853/13152\n",
      "class 1 clustering acc 0.8873175182481752: correct 5835/6576\n",
      "Epoch: 37, train_loss: 0.1930, train_clustering_loss:  0.2137, train_error: 0.0791\n",
      "class 0: acc 0.913151364764268, correct 368/403\n",
      "class 1: acc 0.9284009546539379, correct 389/419\n",
      "\n",
      "Val Set, val_loss: 0.3420, val_error: 0.0917, auc: 0.9489\n",
      "class 0 clustering acc 0.9627293577981652: correct 1679/1744\n",
      "class 1 clustering acc 0.8635321100917431: correct 753/872\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 19039\n",
      "batch 39, loss: 0.0073, instance_loss: 0.0000, weighted_loss: 0.0051, label: 1, bag_size: 16034\n",
      "batch 59, loss: 0.0575, instance_loss: 0.0228, weighted_loss: 0.0470, label: 1, bag_size: 1786\n",
      "batch 79, loss: 0.0994, instance_loss: 0.1700, weighted_loss: 0.1206, label: 0, bag_size: 763\n",
      "batch 99, loss: 0.0091, instance_loss: 0.1403, weighted_loss: 0.0485, label: 0, bag_size: 13023\n",
      "batch 119, loss: 0.1045, instance_loss: 0.2458, weighted_loss: 0.1469, label: 0, bag_size: 9132\n",
      "batch 139, loss: 0.0459, instance_loss: 0.0002, weighted_loss: 0.0322, label: 0, bag_size: 18777\n",
      "batch 159, loss: 0.0845, instance_loss: 0.3200, weighted_loss: 0.1552, label: 0, bag_size: 2490\n",
      "batch 179, loss: 0.4872, instance_loss: 0.0090, weighted_loss: 0.3437, label: 0, bag_size: 7835\n",
      "batch 199, loss: 0.0098, instance_loss: 0.0000, weighted_loss: 0.0069, label: 1, bag_size: 11032\n",
      "batch 219, loss: 0.2921, instance_loss: 3.7017, weighted_loss: 1.3150, label: 1, bag_size: 1867\n",
      "batch 239, loss: 0.0007, instance_loss: 0.0010, weighted_loss: 0.0008, label: 0, bag_size: 11512\n",
      "batch 259, loss: 1.0558, instance_loss: 0.6156, weighted_loss: 0.9237, label: 1, bag_size: 3121\n",
      "batch 279, loss: 0.1655, instance_loss: 0.0551, weighted_loss: 0.1324, label: 1, bag_size: 9519\n",
      "batch 299, loss: 0.7080, instance_loss: 0.0094, weighted_loss: 0.4985, label: 0, bag_size: 5120\n",
      "batch 319, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 10867\n",
      "batch 339, loss: 0.0017, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 9060\n",
      "batch 359, loss: 0.0146, instance_loss: 0.0115, weighted_loss: 0.0137, label: 0, bag_size: 13880\n",
      "batch 379, loss: 0.0360, instance_loss: 0.0031, weighted_loss: 0.0261, label: 1, bag_size: 2559\n",
      "batch 399, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 10920\n",
      "batch 419, loss: 0.0040, instance_loss: 0.0002, weighted_loss: 0.0029, label: 1, bag_size: 6016\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 4442\n",
      "batch 459, loss: 0.0484, instance_loss: 0.1143, weighted_loss: 0.0681, label: 0, bag_size: 12899\n",
      "batch 479, loss: 0.0634, instance_loss: 0.0062, weighted_loss: 0.0463, label: 1, bag_size: 13026\n",
      "batch 499, loss: 0.0123, instance_loss: 0.5324, weighted_loss: 0.1683, label: 1, bag_size: 1064\n",
      "batch 519, loss: 0.0028, instance_loss: 0.0005, weighted_loss: 0.0021, label: 1, bag_size: 13255\n",
      "batch 539, loss: 0.0208, instance_loss: 0.0177, weighted_loss: 0.0199, label: 1, bag_size: 9470\n",
      "batch 559, loss: 0.0004, instance_loss: 0.0028, weighted_loss: 0.0011, label: 1, bag_size: 10392\n",
      "batch 579, loss: 0.3841, instance_loss: 0.0947, weighted_loss: 0.2973, label: 0, bag_size: 13619\n",
      "batch 599, loss: 0.0140, instance_loss: 0.0000, weighted_loss: 0.0098, label: 0, bag_size: 18132\n",
      "batch 619, loss: 0.0796, instance_loss: 0.2664, weighted_loss: 0.1357, label: 0, bag_size: 6727\n",
      "batch 639, loss: 0.0157, instance_loss: 0.0016, weighted_loss: 0.0115, label: 0, bag_size: 9069\n",
      "batch 659, loss: 0.0078, instance_loss: 0.0000, weighted_loss: 0.0054, label: 1, bag_size: 11032\n",
      "batch 679, loss: 0.3817, instance_loss: 0.7846, weighted_loss: 0.5026, label: 1, bag_size: 1683\n",
      "batch 699, loss: 0.2121, instance_loss: 0.1984, weighted_loss: 0.2080, label: 1, bag_size: 2842\n",
      "batch 719, loss: 1.4165, instance_loss: 0.1087, weighted_loss: 1.0242, label: 0, bag_size: 1953\n",
      "batch 739, loss: 0.0409, instance_loss: 0.0052, weighted_loss: 0.0302, label: 1, bag_size: 8935\n",
      "batch 759, loss: 0.0387, instance_loss: 0.0017, weighted_loss: 0.0276, label: 1, bag_size: 7148\n",
      "batch 779, loss: 0.0412, instance_loss: 0.0020, weighted_loss: 0.0294, label: 0, bag_size: 8330\n",
      "batch 799, loss: 0.0009, instance_loss: 0.1538, weighted_loss: 0.0467, label: 0, bag_size: 9930\n",
      "batch 819, loss: 0.0013, instance_loss: 0.0562, weighted_loss: 0.0178, label: 0, bag_size: 2091\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9796989051094891: correct 12885/13152\n",
      "class 1 clustering acc 0.8918795620437956: correct 5865/6576\n",
      "Epoch: 38, train_loss: 0.1911, train_clustering_loss:  0.1938, train_error: 0.0706\n",
      "class 0: acc 0.9264705882352942, correct 378/408\n",
      "class 1: acc 0.9323671497584541, correct 386/414\n",
      "\n",
      "Val Set, val_loss: 0.4362, val_error: 0.1193, auc: 0.9517\n",
      "class 0 clustering acc 0.9420871559633027: correct 1643/1744\n",
      "class 1 clustering acc 0.805045871559633: correct 702/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.7936507936507936, correct 50/63\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0192, instance_loss: 0.0009, weighted_loss: 0.0137, label: 1, bag_size: 8216\n",
      "batch 39, loss: 0.2015, instance_loss: 0.0021, weighted_loss: 0.1417, label: 1, bag_size: 12425\n",
      "batch 59, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 8812\n",
      "batch 79, loss: 0.9298, instance_loss: 0.4636, weighted_loss: 0.7899, label: 0, bag_size: 2179\n",
      "batch 99, loss: 0.0098, instance_loss: 0.0046, weighted_loss: 0.0082, label: 0, bag_size: 2036\n",
      "batch 119, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 5833\n",
      "batch 139, loss: 0.0257, instance_loss: 0.0083, weighted_loss: 0.0205, label: 1, bag_size: 10028\n",
      "batch 159, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 1, bag_size: 10725\n",
      "batch 179, loss: 0.0415, instance_loss: 0.1275, weighted_loss: 0.0673, label: 1, bag_size: 2681\n",
      "batch 199, loss: 0.8786, instance_loss: 0.4961, weighted_loss: 0.7638, label: 1, bag_size: 771\n",
      "batch 219, loss: 0.0226, instance_loss: 0.0000, weighted_loss: 0.0158, label: 1, bag_size: 17579\n",
      "batch 239, loss: 0.0263, instance_loss: 0.0032, weighted_loss: 0.0193, label: 0, bag_size: 2760\n",
      "batch 259, loss: 0.0020, instance_loss: 0.0000, weighted_loss: 0.0014, label: 1, bag_size: 19039\n",
      "batch 279, loss: 0.0671, instance_loss: 0.0000, weighted_loss: 0.0470, label: 1, bag_size: 13732\n",
      "batch 299, loss: 0.0173, instance_loss: 0.0454, weighted_loss: 0.0258, label: 1, bag_size: 2790\n",
      "batch 319, loss: 0.0176, instance_loss: 0.0000, weighted_loss: 0.0123, label: 0, bag_size: 14956\n",
      "batch 339, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 13368\n",
      "batch 359, loss: 0.0253, instance_loss: 0.1859, weighted_loss: 0.0735, label: 1, bag_size: 8040\n",
      "batch 379, loss: 0.0058, instance_loss: 0.0004, weighted_loss: 0.0042, label: 1, bag_size: 20537\n",
      "batch 399, loss: 0.0022, instance_loss: 0.0384, weighted_loss: 0.0130, label: 0, bag_size: 3265\n",
      "batch 419, loss: 0.0476, instance_loss: 0.3653, weighted_loss: 0.1429, label: 1, bag_size: 6825\n",
      "batch 439, loss: 0.0034, instance_loss: 0.0000, weighted_loss: 0.0024, label: 1, bag_size: 17769\n",
      "batch 459, loss: 0.0003, instance_loss: 0.0200, weighted_loss: 0.0062, label: 1, bag_size: 1412\n",
      "batch 479, loss: 0.0033, instance_loss: 0.0900, weighted_loss: 0.0293, label: 0, bag_size: 7823\n",
      "batch 499, loss: 0.0002, instance_loss: 0.0993, weighted_loss: 0.0299, label: 0, bag_size: 15077\n",
      "batch 519, loss: 0.0424, instance_loss: 0.0001, weighted_loss: 0.0297, label: 1, bag_size: 1437\n",
      "batch 539, loss: 0.0088, instance_loss: 0.0939, weighted_loss: 0.0343, label: 0, bag_size: 16521\n",
      "batch 559, loss: 0.0209, instance_loss: 0.0000, weighted_loss: 0.0146, label: 1, bag_size: 28527\n",
      "batch 579, loss: 0.4587, instance_loss: 0.0451, weighted_loss: 0.3347, label: 0, bag_size: 11922\n",
      "batch 599, loss: 0.0042, instance_loss: 0.0005, weighted_loss: 0.0031, label: 1, bag_size: 7371\n",
      "batch 619, loss: 0.0807, instance_loss: 0.1852, weighted_loss: 0.1121, label: 0, bag_size: 13619\n",
      "batch 639, loss: 0.0009, instance_loss: 0.0291, weighted_loss: 0.0094, label: 0, bag_size: 3265\n",
      "batch 659, loss: 0.4711, instance_loss: 0.8041, weighted_loss: 0.5710, label: 1, bag_size: 1683\n",
      "batch 679, loss: 0.1520, instance_loss: 0.0272, weighted_loss: 0.1146, label: 1, bag_size: 10591\n",
      "batch 699, loss: 0.0028, instance_loss: 0.4770, weighted_loss: 0.1450, label: 1, bag_size: 14433\n",
      "batch 719, loss: 0.0478, instance_loss: 0.0505, weighted_loss: 0.0486, label: 1, bag_size: 2140\n",
      "batch 739, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 14515\n",
      "batch 759, loss: 0.2081, instance_loss: 0.9190, weighted_loss: 0.4214, label: 0, bag_size: 2104\n",
      "batch 779, loss: 0.0082, instance_loss: 0.9263, weighted_loss: 0.2836, label: 1, bag_size: 3980\n",
      "batch 799, loss: 0.0055, instance_loss: 0.0012, weighted_loss: 0.0042, label: 1, bag_size: 2412\n",
      "batch 819, loss: 0.0017, instance_loss: 0.0023, weighted_loss: 0.0019, label: 0, bag_size: 10942\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9799270072992701: correct 12888/13152\n",
      "class 1 clustering acc 0.8982664233576643: correct 5907/6576\n",
      "Epoch: 39, train_loss: 0.2031, train_clustering_loss:  0.1920, train_error: 0.0657\n",
      "class 0: acc 0.9175257731958762, correct 356/388\n",
      "class 1: acc 0.9493087557603687, correct 412/434\n",
      "\n",
      "Val Set, val_loss: 0.5723, val_error: 0.1651, auc: 0.9517\n",
      "class 0 clustering acc 0.9283256880733946: correct 1619/1744\n",
      "class 1 clustering acc 0.7305045871559633: correct 637/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.7142857142857143, correct 45/63\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 19043\n",
      "batch 39, loss: 0.0621, instance_loss: 0.0003, weighted_loss: 0.0436, label: 1, bag_size: 5605\n",
      "batch 59, loss: 0.0012, instance_loss: 0.0001, weighted_loss: 0.0009, label: 1, bag_size: 10867\n",
      "batch 79, loss: 0.0753, instance_loss: 0.5777, weighted_loss: 0.2260, label: 0, bag_size: 931\n",
      "batch 99, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 11884\n",
      "batch 119, loss: 0.1046, instance_loss: 0.1819, weighted_loss: 0.1278, label: 1, bag_size: 5155\n",
      "batch 139, loss: 0.1031, instance_loss: 0.0324, weighted_loss: 0.0819, label: 1, bag_size: 2140\n",
      "batch 159, loss: 0.0088, instance_loss: 0.0012, weighted_loss: 0.0065, label: 0, bag_size: 13777\n",
      "batch 179, loss: 0.2826, instance_loss: 2.7786, weighted_loss: 1.0314, label: 1, bag_size: 13089\n",
      "batch 199, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 15213\n",
      "batch 219, loss: 0.0131, instance_loss: 0.9244, weighted_loss: 0.2865, label: 0, bag_size: 2091\n",
      "batch 239, loss: 0.3175, instance_loss: 0.2180, weighted_loss: 0.2876, label: 0, bag_size: 2996\n",
      "batch 259, loss: 2.9643, instance_loss: 0.1923, weighted_loss: 2.1327, label: 1, bag_size: 2565\n",
      "batch 279, loss: 0.0272, instance_loss: 0.0344, weighted_loss: 0.0294, label: 0, bag_size: 1884\n",
      "batch 299, loss: 0.0172, instance_loss: 0.0000, weighted_loss: 0.0121, label: 1, bag_size: 9062\n",
      "batch 319, loss: 0.0172, instance_loss: 0.0879, weighted_loss: 0.0384, label: 0, bag_size: 2820\n",
      "batch 339, loss: 0.0113, instance_loss: 0.0008, weighted_loss: 0.0082, label: 0, bag_size: 2548\n",
      "batch 359, loss: 0.3099, instance_loss: 0.0158, weighted_loss: 0.2217, label: 0, bag_size: 6093\n",
      "batch 379, loss: 0.0137, instance_loss: 0.0380, weighted_loss: 0.0210, label: 1, bag_size: 5605\n",
      "batch 399, loss: 0.0354, instance_loss: 0.1743, weighted_loss: 0.0771, label: 0, bag_size: 12083\n",
      "batch 419, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 14223\n",
      "batch 439, loss: 1.6546, instance_loss: 0.4759, weighted_loss: 1.3010, label: 0, bag_size: 2653\n",
      "batch 459, loss: 0.0105, instance_loss: 0.3891, weighted_loss: 0.1241, label: 1, bag_size: 6927\n",
      "batch 479, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 11642\n",
      "batch 499, loss: 0.2462, instance_loss: 0.3311, weighted_loss: 0.2716, label: 0, bag_size: 11212\n",
      "batch 519, loss: 0.1635, instance_loss: 0.0312, weighted_loss: 0.1238, label: 0, bag_size: 26208\n",
      "batch 539, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 19932\n",
      "batch 559, loss: 0.0598, instance_loss: 0.5538, weighted_loss: 0.2080, label: 1, bag_size: 1999\n",
      "batch 579, loss: 0.0091, instance_loss: 0.1520, weighted_loss: 0.0520, label: 0, bag_size: 12131\n",
      "batch 599, loss: 0.0214, instance_loss: 0.0111, weighted_loss: 0.0183, label: 0, bag_size: 22681\n",
      "batch 619, loss: 0.4839, instance_loss: 1.2866, weighted_loss: 0.7248, label: 1, bag_size: 1191\n",
      "batch 639, loss: 0.0305, instance_loss: 0.0000, weighted_loss: 0.0214, label: 1, bag_size: 7873\n",
      "batch 659, loss: 0.0058, instance_loss: 0.0000, weighted_loss: 0.0041, label: 1, bag_size: 16267\n",
      "batch 679, loss: 0.1519, instance_loss: 0.3078, weighted_loss: 0.1987, label: 1, bag_size: 2681\n",
      "batch 699, loss: 0.0013, instance_loss: 0.2908, weighted_loss: 0.0882, label: 0, bag_size: 14319\n",
      "batch 719, loss: 0.7972, instance_loss: 0.1729, weighted_loss: 0.6099, label: 0, bag_size: 6093\n",
      "batch 739, loss: 0.6809, instance_loss: 1.5399, weighted_loss: 0.9386, label: 1, bag_size: 1095\n",
      "batch 759, loss: 0.0022, instance_loss: 0.0135, weighted_loss: 0.0056, label: 0, bag_size: 9949\n",
      "batch 779, loss: 0.0064, instance_loss: 0.0069, weighted_loss: 0.0066, label: 0, bag_size: 10898\n",
      "batch 799, loss: 0.2142, instance_loss: 0.1100, weighted_loss: 0.1830, label: 1, bag_size: 7351\n",
      "batch 819, loss: 0.0074, instance_loss: 0.0701, weighted_loss: 0.0262, label: 0, bag_size: 1072\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9776459854014599: correct 12858/13152\n",
      "class 1 clustering acc 0.8614659367396593: correct 5665/6576\n",
      "Epoch: 40, train_loss: 0.1954, train_clustering_loss:  0.2292, train_error: 0.0706\n",
      "class 0: acc 0.9285714285714286, correct 390/420\n",
      "class 1: acc 0.9303482587064676, correct 374/402\n",
      "\n",
      "Val Set, val_loss: 0.3789, val_error: 0.0826, auc: 0.9513\n",
      "class 0 clustering acc 0.9529816513761468: correct 1662/1744\n",
      "class 1 clustering acc 0.7522935779816514: correct 656/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0352, weighted_loss: 0.0107, label: 0, bag_size: 17791\n",
      "batch 39, loss: 0.2676, instance_loss: 0.2399, weighted_loss: 0.2593, label: 0, bag_size: 3238\n",
      "batch 59, loss: 0.0141, instance_loss: 0.0000, weighted_loss: 0.0099, label: 1, bag_size: 20537\n",
      "batch 79, loss: 0.2565, instance_loss: 0.0243, weighted_loss: 0.1868, label: 1, bag_size: 20161\n",
      "batch 99, loss: 0.0133, instance_loss: 0.1810, weighted_loss: 0.0636, label: 0, bag_size: 1651\n",
      "batch 119, loss: 0.1437, instance_loss: 0.0003, weighted_loss: 0.1007, label: 1, bag_size: 11223\n",
      "batch 139, loss: 0.0020, instance_loss: 0.0003, weighted_loss: 0.0015, label: 1, bag_size: 9065\n",
      "batch 159, loss: 0.0058, instance_loss: 0.4309, weighted_loss: 0.1334, label: 0, bag_size: 2367\n",
      "batch 179, loss: 0.0048, instance_loss: 0.0219, weighted_loss: 0.0099, label: 0, bag_size: 12593\n",
      "batch 199, loss: 0.0046, instance_loss: 0.0020, weighted_loss: 0.0038, label: 1, bag_size: 8003\n",
      "batch 219, loss: 0.0026, instance_loss: 0.0013, weighted_loss: 0.0022, label: 1, bag_size: 4480\n",
      "batch 239, loss: 0.0295, instance_loss: 0.0016, weighted_loss: 0.0212, label: 1, bag_size: 8448\n",
      "batch 259, loss: 0.0285, instance_loss: 0.2145, weighted_loss: 0.0843, label: 1, bag_size: 2412\n",
      "batch 279, loss: 0.0051, instance_loss: 0.0003, weighted_loss: 0.0036, label: 0, bag_size: 21032\n",
      "batch 299, loss: 0.9334, instance_loss: 0.8948, weighted_loss: 0.9218, label: 1, bag_size: 1191\n",
      "batch 319, loss: 0.0643, instance_loss: 0.0015, weighted_loss: 0.0455, label: 1, bag_size: 13015\n",
      "batch 339, loss: 0.0051, instance_loss: 0.0426, weighted_loss: 0.0163, label: 0, bag_size: 9171\n",
      "batch 359, loss: 0.1177, instance_loss: 0.0399, weighted_loss: 0.0943, label: 0, bag_size: 9387\n",
      "batch 379, loss: 0.0083, instance_loss: 0.0720, weighted_loss: 0.0274, label: 0, bag_size: 11125\n",
      "batch 399, loss: 0.5766, instance_loss: 0.3339, weighted_loss: 0.5038, label: 0, bag_size: 14264\n",
      "batch 419, loss: 0.8970, instance_loss: 0.0217, weighted_loss: 0.6344, label: 1, bag_size: 21450\n",
      "batch 439, loss: 0.0002, instance_loss: 0.0911, weighted_loss: 0.0275, label: 0, bag_size: 18225\n",
      "batch 459, loss: 0.0179, instance_loss: 0.0000, weighted_loss: 0.0125, label: 1, bag_size: 11875\n",
      "batch 479, loss: 0.0114, instance_loss: 0.4706, weighted_loss: 0.1492, label: 0, bag_size: 10146\n",
      "batch 499, loss: 0.0026, instance_loss: 0.0179, weighted_loss: 0.0072, label: 1, bag_size: 10112\n",
      "batch 519, loss: 0.0011, instance_loss: 0.0035, weighted_loss: 0.0018, label: 1, bag_size: 10112\n",
      "batch 539, loss: 0.0405, instance_loss: 0.0000, weighted_loss: 0.0283, label: 1, bag_size: 18699\n",
      "batch 559, loss: 0.0125, instance_loss: 0.0000, weighted_loss: 0.0087, label: 1, bag_size: 2695\n",
      "batch 579, loss: 0.0004, instance_loss: 0.2125, weighted_loss: 0.0640, label: 0, bag_size: 11865\n",
      "batch 599, loss: 0.0072, instance_loss: 0.0028, weighted_loss: 0.0059, label: 0, bag_size: 25558\n",
      "batch 619, loss: 0.0065, instance_loss: 0.0000, weighted_loss: 0.0045, label: 1, bag_size: 9971\n",
      "batch 639, loss: 0.6280, instance_loss: 1.1989, weighted_loss: 0.7993, label: 1, bag_size: 1236\n",
      "batch 659, loss: 0.0002, instance_loss: 0.0627, weighted_loss: 0.0190, label: 1, bag_size: 1316\n",
      "batch 679, loss: 0.1092, instance_loss: 0.2252, weighted_loss: 0.1440, label: 1, bag_size: 2682\n",
      "batch 699, loss: 0.1031, instance_loss: 0.0318, weighted_loss: 0.0817, label: 0, bag_size: 3238\n",
      "batch 719, loss: 0.4464, instance_loss: 0.6385, weighted_loss: 0.5040, label: 0, bag_size: 4997\n",
      "batch 739, loss: 0.0111, instance_loss: 0.1104, weighted_loss: 0.0409, label: 0, bag_size: 10490\n",
      "batch 759, loss: 0.0444, instance_loss: 0.0000, weighted_loss: 0.0311, label: 1, bag_size: 20767\n",
      "batch 779, loss: 0.0168, instance_loss: 0.0000, weighted_loss: 0.0118, label: 1, bag_size: 3409\n",
      "batch 799, loss: 0.0720, instance_loss: 0.3243, weighted_loss: 0.1477, label: 0, bag_size: 12131\n",
      "batch 819, loss: 0.0037, instance_loss: 0.0022, weighted_loss: 0.0032, label: 1, bag_size: 7381\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9743004866180048: correct 12814/13152\n",
      "class 1 clustering acc 0.8643552311435523: correct 5684/6576\n",
      "Epoch: 41, train_loss: 0.2223, train_clustering_loss:  0.2459, train_error: 0.0803\n",
      "class 0: acc 0.908433734939759, correct 377/415\n",
      "class 1: acc 0.9312039312039312, correct 379/407\n",
      "\n",
      "Val Set, val_loss: 0.3019, val_error: 0.0826, auc: 0.9545\n",
      "class 0 clustering acc 0.9827981651376146: correct 1714/1744\n",
      "class 1 clustering acc 0.7672018348623854: correct 669/872\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.8888888888888888, correct 56/63\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0609, weighted_loss: 0.0184, label: 0, bag_size: 10995\n",
      "batch 39, loss: 0.2210, instance_loss: 0.8111, weighted_loss: 0.3980, label: 1, bag_size: 11386\n",
      "batch 59, loss: 0.0165, instance_loss: 0.1726, weighted_loss: 0.0633, label: 0, bag_size: 2367\n",
      "batch 79, loss: 0.0096, instance_loss: 0.0000, weighted_loss: 0.0067, label: 1, bag_size: 15093\n",
      "batch 99, loss: 0.0017, instance_loss: 0.0399, weighted_loss: 0.0132, label: 0, bag_size: 21032\n",
      "batch 119, loss: 0.0183, instance_loss: 0.2390, weighted_loss: 0.0845, label: 0, bag_size: 1614\n",
      "batch 139, loss: 0.0153, instance_loss: 0.1185, weighted_loss: 0.0463, label: 0, bag_size: 2282\n",
      "batch 159, loss: 0.0517, instance_loss: 0.3017, weighted_loss: 0.1267, label: 1, bag_size: 1572\n",
      "batch 179, loss: 0.0673, instance_loss: 0.1599, weighted_loss: 0.0951, label: 1, bag_size: 3656\n",
      "batch 199, loss: 0.0322, instance_loss: 0.0760, weighted_loss: 0.0453, label: 1, bag_size: 4054\n",
      "batch 219, loss: 0.8998, instance_loss: 0.7733, weighted_loss: 0.8619, label: 0, bag_size: 15898\n",
      "batch 239, loss: 0.0149, instance_loss: 0.0270, weighted_loss: 0.0185, label: 0, bag_size: 1772\n",
      "batch 259, loss: 0.2175, instance_loss: 0.9532, weighted_loss: 0.4382, label: 1, bag_size: 9649\n",
      "batch 279, loss: 0.0463, instance_loss: 0.0157, weighted_loss: 0.0371, label: 1, bag_size: 2682\n",
      "batch 299, loss: 0.0469, instance_loss: 0.0185, weighted_loss: 0.0384, label: 0, bag_size: 763\n",
      "batch 319, loss: 0.0001, instance_loss: 0.1052, weighted_loss: 0.0316, label: 1, bag_size: 629\n",
      "batch 339, loss: 0.0005, instance_loss: 0.0002, weighted_loss: 0.0004, label: 1, bag_size: 5612\n",
      "batch 359, loss: 0.3125, instance_loss: 0.7443, weighted_loss: 0.4421, label: 1, bag_size: 9330\n",
      "batch 379, loss: 0.0162, instance_loss: 0.0000, weighted_loss: 0.0113, label: 1, bag_size: 8466\n",
      "batch 399, loss: 0.6054, instance_loss: 0.0005, weighted_loss: 0.4239, label: 1, bag_size: 8395\n",
      "batch 419, loss: 0.0092, instance_loss: 0.1173, weighted_loss: 0.0416, label: 0, bag_size: 10898\n",
      "batch 439, loss: 0.3245, instance_loss: 0.0000, weighted_loss: 0.2272, label: 1, bag_size: 12714\n",
      "batch 459, loss: 0.0074, instance_loss: 0.0061, weighted_loss: 0.0070, label: 0, bag_size: 11113\n",
      "batch 479, loss: 0.0098, instance_loss: 0.0360, weighted_loss: 0.0176, label: 0, bag_size: 7381\n",
      "batch 499, loss: 1.3232, instance_loss: 0.0475, weighted_loss: 0.9405, label: 0, bag_size: 1437\n",
      "batch 519, loss: 0.0006, instance_loss: 0.0322, weighted_loss: 0.0101, label: 0, bag_size: 13691\n",
      "batch 539, loss: 0.0160, instance_loss: 0.0000, weighted_loss: 0.0112, label: 1, bag_size: 7371\n",
      "batch 559, loss: 0.0198, instance_loss: 0.2879, weighted_loss: 0.1002, label: 1, bag_size: 1999\n",
      "batch 579, loss: 0.1751, instance_loss: 0.1700, weighted_loss: 0.1735, label: 0, bag_size: 3399\n",
      "batch 599, loss: 0.0021, instance_loss: 0.0520, weighted_loss: 0.0171, label: 0, bag_size: 15636\n",
      "batch 619, loss: 0.3746, instance_loss: 0.0000, weighted_loss: 0.2622, label: 1, bag_size: 11256\n",
      "batch 639, loss: 0.2419, instance_loss: 0.0176, weighted_loss: 0.1746, label: 1, bag_size: 2092\n",
      "batch 659, loss: 0.2347, instance_loss: 0.0053, weighted_loss: 0.1659, label: 0, bag_size: 11151\n",
      "batch 679, loss: 0.0016, instance_loss: 0.6675, weighted_loss: 0.2014, label: 0, bag_size: 2820\n",
      "batch 699, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 1, bag_size: 15008\n",
      "batch 719, loss: 0.0014, instance_loss: 0.0002, weighted_loss: 0.0010, label: 1, bag_size: 4128\n",
      "batch 739, loss: 0.2719, instance_loss: 0.0944, weighted_loss: 0.2187, label: 1, bag_size: 2395\n",
      "batch 759, loss: 0.1597, instance_loss: 0.0000, weighted_loss: 0.1118, label: 1, bag_size: 8395\n",
      "batch 779, loss: 0.0104, instance_loss: 0.0057, weighted_loss: 0.0090, label: 0, bag_size: 13378\n",
      "batch 799, loss: 1.3423, instance_loss: 1.8818, weighted_loss: 1.5042, label: 1, bag_size: 9404\n",
      "batch 819, loss: 0.7457, instance_loss: 0.3933, weighted_loss: 0.6400, label: 0, bag_size: 2996\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.979242700729927: correct 12879/13152\n",
      "class 1 clustering acc 0.8976581508515815: correct 5903/6576\n",
      "Epoch: 42, train_loss: 0.1750, train_clustering_loss:  0.1911, train_error: 0.0681\n",
      "class 0: acc 0.9197080291970803, correct 378/411\n",
      "class 1: acc 0.9440389294403893, correct 388/411\n",
      "\n",
      "Val Set, val_loss: 0.2879, val_error: 0.1009, auc: 0.9545\n",
      "class 0 clustering acc 0.9701834862385321: correct 1692/1744\n",
      "class 1 clustering acc 0.7568807339449541: correct 660/872\n",
      "class 0: acc 0.9130434782608695, correct 42/46\n",
      "class 1: acc 0.8888888888888888, correct 56/63\n",
      "Validation loss decreased (0.291532 --> 0.287913).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0115, instance_loss: 0.0033, weighted_loss: 0.0090, label: 1, bag_size: 2412\n",
      "batch 39, loss: 0.0436, instance_loss: 0.0002, weighted_loss: 0.0305, label: 1, bag_size: 1838\n",
      "batch 59, loss: 0.0006, instance_loss: 0.0007, weighted_loss: 0.0006, label: 1, bag_size: 1273\n",
      "batch 79, loss: 0.0590, instance_loss: 0.0347, weighted_loss: 0.0517, label: 0, bag_size: 5999\n",
      "batch 99, loss: 0.0056, instance_loss: 0.0000, weighted_loss: 0.0039, label: 0, bag_size: 11113\n",
      "batch 119, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 1, bag_size: 6769\n",
      "batch 139, loss: 0.0778, instance_loss: 0.0002, weighted_loss: 0.0545, label: 1, bag_size: 13026\n",
      "batch 159, loss: 0.0181, instance_loss: 0.0022, weighted_loss: 0.0133, label: 1, bag_size: 2695\n",
      "batch 179, loss: 0.2181, instance_loss: 0.5948, weighted_loss: 0.3311, label: 0, bag_size: 8427\n",
      "batch 199, loss: 0.0009, instance_loss: 0.0021, weighted_loss: 0.0013, label: 1, bag_size: 4250\n",
      "batch 219, loss: 0.2227, instance_loss: 0.0257, weighted_loss: 0.1636, label: 0, bag_size: 2732\n",
      "batch 239, loss: 0.0006, instance_loss: 0.0013, weighted_loss: 0.0008, label: 0, bag_size: 27158\n",
      "batch 259, loss: 0.0724, instance_loss: 0.7235, weighted_loss: 0.2678, label: 1, bag_size: 7389\n",
      "batch 279, loss: 0.0014, instance_loss: 0.6488, weighted_loss: 0.1956, label: 1, bag_size: 4128\n",
      "batch 299, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 10995\n",
      "batch 319, loss: 0.0747, instance_loss: 0.1585, weighted_loss: 0.0998, label: 1, bag_size: 1242\n",
      "batch 339, loss: 0.0294, instance_loss: 0.0434, weighted_loss: 0.0336, label: 1, bag_size: 2814\n",
      "batch 359, loss: 2.1365, instance_loss: 0.0055, weighted_loss: 1.4972, label: 0, bag_size: 2918\n",
      "batch 379, loss: 0.3035, instance_loss: 0.0318, weighted_loss: 0.2220, label: 1, bag_size: 12603\n",
      "batch 399, loss: 0.0252, instance_loss: 0.2269, weighted_loss: 0.0857, label: 0, bag_size: 12732\n",
      "batch 419, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 19832\n",
      "batch 439, loss: 0.0020, instance_loss: 0.0003, weighted_loss: 0.0015, label: 0, bag_size: 19466\n",
      "batch 459, loss: 0.3939, instance_loss: 0.0804, weighted_loss: 0.2998, label: 1, bag_size: 1230\n",
      "batch 479, loss: 0.0180, instance_loss: 0.0657, weighted_loss: 0.0323, label: 0, bag_size: 7989\n",
      "batch 499, loss: 0.0049, instance_loss: 0.0000, weighted_loss: 0.0034, label: 0, bag_size: 18045\n",
      "batch 519, loss: 0.0061, instance_loss: 0.0000, weighted_loss: 0.0043, label: 1, bag_size: 14681\n",
      "batch 539, loss: 0.0111, instance_loss: 0.1487, weighted_loss: 0.0524, label: 0, bag_size: 4959\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 18154\n",
      "batch 579, loss: 0.3037, instance_loss: 1.6143, weighted_loss: 0.6969, label: 1, bag_size: 1236\n",
      "batch 599, loss: 0.0379, instance_loss: 0.0008, weighted_loss: 0.0268, label: 1, bag_size: 16890\n",
      "batch 619, loss: 0.0618, instance_loss: 0.0070, weighted_loss: 0.0454, label: 1, bag_size: 13026\n",
      "batch 639, loss: 0.0172, instance_loss: 0.0036, weighted_loss: 0.0131, label: 1, bag_size: 8466\n",
      "batch 659, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 14223\n",
      "batch 679, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 32227\n",
      "batch 699, loss: 0.0064, instance_loss: 0.1559, weighted_loss: 0.0512, label: 0, bag_size: 931\n",
      "batch 719, loss: 0.0030, instance_loss: 0.0000, weighted_loss: 0.0021, label: 0, bag_size: 14828\n",
      "batch 739, loss: 0.0457, instance_loss: 0.2033, weighted_loss: 0.0930, label: 1, bag_size: 3368\n",
      "batch 759, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 3228\n",
      "batch 779, loss: 0.6207, instance_loss: 0.0011, weighted_loss: 0.4348, label: 1, bag_size: 21701\n",
      "batch 799, loss: 0.0273, instance_loss: 0.3403, weighted_loss: 0.1212, label: 1, bag_size: 14604\n",
      "batch 819, loss: 0.0989, instance_loss: 0.3592, weighted_loss: 0.1770, label: 0, bag_size: 4241\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9791666666666666: correct 12878/13152\n",
      "class 1 clustering acc 0.8815389294403893: correct 5797/6576\n",
      "Epoch: 43, train_loss: 0.1583, train_clustering_loss:  0.2390, train_error: 0.0547\n",
      "class 0: acc 0.947242206235012, correct 395/417\n",
      "class 1: acc 0.9432098765432099, correct 382/405\n",
      "\n",
      "Val Set, val_loss: 0.5533, val_error: 0.1835, auc: 0.9538\n",
      "class 0 clustering acc 0.9793577981651376: correct 1708/1744\n",
      "class 1 clustering acc 0.9346330275229358: correct 815/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.6825396825396826, correct 43/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0407, instance_loss: 0.1251, weighted_loss: 0.0660, label: 0, bag_size: 1920\n",
      "batch 39, loss: 0.0045, instance_loss: 0.0008, weighted_loss: 0.0034, label: 1, bag_size: 6950\n",
      "batch 59, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 21576\n",
      "batch 79, loss: 2.9101, instance_loss: 9.0282, weighted_loss: 4.7455, label: 1, bag_size: 15563\n",
      "batch 99, loss: 1.7870, instance_loss: 0.3318, weighted_loss: 1.3504, label: 0, bag_size: 1437\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0164, weighted_loss: 0.0050, label: 0, bag_size: 13225\n",
      "batch 139, loss: 0.0165, instance_loss: 0.0000, weighted_loss: 0.0115, label: 0, bag_size: 7381\n",
      "batch 159, loss: 0.0052, instance_loss: 0.0540, weighted_loss: 0.0198, label: 1, bag_size: 5864\n",
      "batch 179, loss: 0.0429, instance_loss: 0.1385, weighted_loss: 0.0716, label: 0, bag_size: 1142\n",
      "batch 199, loss: 0.4517, instance_loss: 0.6373, weighted_loss: 0.5074, label: 1, bag_size: 1794\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 7191\n",
      "batch 239, loss: 0.1162, instance_loss: 0.1163, weighted_loss: 0.1162, label: 1, bag_size: 6752\n",
      "batch 259, loss: 0.1640, instance_loss: 0.1897, weighted_loss: 0.1717, label: 0, bag_size: 1772\n",
      "batch 279, loss: 0.0381, instance_loss: 0.0209, weighted_loss: 0.0329, label: 1, bag_size: 8680\n",
      "batch 299, loss: 0.0304, instance_loss: 0.0269, weighted_loss: 0.0293, label: 0, bag_size: 9387\n",
      "batch 319, loss: 0.0061, instance_loss: 0.0072, weighted_loss: 0.0065, label: 0, bag_size: 2652\n",
      "batch 339, loss: 0.0030, instance_loss: 0.0001, weighted_loss: 0.0021, label: 0, bag_size: 14625\n",
      "batch 359, loss: 0.1659, instance_loss: 0.0001, weighted_loss: 0.1161, label: 1, bag_size: 12425\n",
      "batch 379, loss: 0.2673, instance_loss: 1.7937, weighted_loss: 0.7252, label: 1, bag_size: 6682\n",
      "batch 399, loss: 0.0075, instance_loss: 0.0000, weighted_loss: 0.0053, label: 1, bag_size: 10112\n",
      "batch 419, loss: 0.0055, instance_loss: 0.1006, weighted_loss: 0.0340, label: 0, bag_size: 1349\n",
      "batch 439, loss: 0.0140, instance_loss: 0.0030, weighted_loss: 0.0107, label: 1, bag_size: 3224\n",
      "batch 459, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 15841\n",
      "batch 479, loss: 0.0005, instance_loss: 0.2717, weighted_loss: 0.0819, label: 1, bag_size: 12349\n",
      "batch 499, loss: 0.0823, instance_loss: 0.0067, weighted_loss: 0.0596, label: 1, bag_size: 7798\n",
      "batch 519, loss: 0.0352, instance_loss: 0.0074, weighted_loss: 0.0269, label: 0, bag_size: 20478\n",
      "batch 539, loss: 0.1244, instance_loss: 0.0648, weighted_loss: 0.1065, label: 0, bag_size: 1498\n",
      "batch 559, loss: 0.0091, instance_loss: 0.0166, weighted_loss: 0.0113, label: 0, bag_size: 8755\n",
      "batch 579, loss: 0.4581, instance_loss: 2.2011, weighted_loss: 0.9810, label: 0, bag_size: 1714\n",
      "batch 599, loss: 0.0110, instance_loss: 0.0052, weighted_loss: 0.0093, label: 1, bag_size: 4959\n",
      "batch 619, loss: 0.0446, instance_loss: 0.0028, weighted_loss: 0.0320, label: 1, bag_size: 1823\n",
      "batch 639, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 18154\n",
      "batch 659, loss: 0.0688, instance_loss: 0.0355, weighted_loss: 0.0588, label: 0, bag_size: 2266\n",
      "batch 679, loss: 0.0068, instance_loss: 0.0023, weighted_loss: 0.0054, label: 0, bag_size: 5999\n",
      "batch 699, loss: 0.0270, instance_loss: 0.0156, weighted_loss: 0.0236, label: 0, bag_size: 2652\n",
      "batch 719, loss: 0.0074, instance_loss: 0.0002, weighted_loss: 0.0053, label: 1, bag_size: 6090\n",
      "batch 739, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 4715\n",
      "batch 759, loss: 0.0018, instance_loss: 0.0000, weighted_loss: 0.0013, label: 1, bag_size: 22286\n",
      "batch 779, loss: 0.0000, instance_loss: 0.6572, weighted_loss: 0.1972, label: 0, bag_size: 1984\n",
      "batch 799, loss: 0.0261, instance_loss: 0.0001, weighted_loss: 0.0183, label: 0, bag_size: 3444\n",
      "batch 819, loss: 0.0009, instance_loss: 0.0071, weighted_loss: 0.0027, label: 0, bag_size: 11383\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.980155109489051: correct 12891/13152\n",
      "class 1 clustering acc 0.8946167883211679: correct 5883/6576\n",
      "Epoch: 44, train_loss: 0.1793, train_clustering_loss:  0.2018, train_error: 0.0681\n",
      "class 0: acc 0.9278846153846154, correct 386/416\n",
      "class 1: acc 0.9359605911330049, correct 380/406\n",
      "\n",
      "Val Set, val_loss: 0.2957, val_error: 0.0826, auc: 0.9586\n",
      "class 0 clustering acc 0.9713302752293578: correct 1694/1744\n",
      "class 1 clustering acc 0.8451834862385321: correct 737/872\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.8888888888888888, correct 56/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0017, instance_loss: 0.0000, weighted_loss: 0.0012, label: 1, bag_size: 3651\n",
      "batch 39, loss: 0.0463, instance_loss: 0.0731, weighted_loss: 0.0544, label: 1, bag_size: 7424\n",
      "batch 59, loss: 0.0004, instance_loss: 0.0855, weighted_loss: 0.0259, label: 1, bag_size: 3207\n",
      "batch 79, loss: 0.0164, instance_loss: 0.0022, weighted_loss: 0.0122, label: 1, bag_size: 6205\n",
      "batch 99, loss: 0.0023, instance_loss: 0.0007, weighted_loss: 0.0018, label: 1, bag_size: 7371\n",
      "batch 119, loss: 0.0064, instance_loss: 0.0248, weighted_loss: 0.0120, label: 0, bag_size: 1831\n",
      "batch 139, loss: 0.3456, instance_loss: 0.1236, weighted_loss: 0.2790, label: 0, bag_size: 1127\n",
      "batch 159, loss: 0.3125, instance_loss: 0.3903, weighted_loss: 0.3358, label: 0, bag_size: 13619\n",
      "batch 179, loss: 0.0641, instance_loss: 0.0000, weighted_loss: 0.0449, label: 1, bag_size: 7217\n",
      "batch 199, loss: 0.0096, instance_loss: 0.0035, weighted_loss: 0.0078, label: 1, bag_size: 3968\n",
      "batch 219, loss: 0.0026, instance_loss: 0.0029, weighted_loss: 0.0027, label: 1, bag_size: 2193\n",
      "batch 239, loss: 0.0075, instance_loss: 0.0072, weighted_loss: 0.0074, label: 0, bag_size: 3265\n",
      "batch 259, loss: 0.0188, instance_loss: 0.0536, weighted_loss: 0.0293, label: 0, bag_size: 9866\n",
      "batch 279, loss: 0.0216, instance_loss: 0.4111, weighted_loss: 0.1384, label: 1, bag_size: 5763\n",
      "batch 299, loss: 0.1401, instance_loss: 0.0393, weighted_loss: 0.1098, label: 1, bag_size: 1822\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 6851\n",
      "batch 339, loss: 0.0005, instance_loss: 0.0120, weighted_loss: 0.0040, label: 0, bag_size: 18240\n",
      "batch 359, loss: 0.0233, instance_loss: 0.1067, weighted_loss: 0.0483, label: 0, bag_size: 6898\n",
      "batch 379, loss: 2.0046, instance_loss: 0.4611, weighted_loss: 1.5416, label: 0, bag_size: 2179\n",
      "batch 399, loss: 0.0051, instance_loss: 0.1909, weighted_loss: 0.0608, label: 0, bag_size: 12793\n",
      "batch 419, loss: 0.0023, instance_loss: 0.0714, weighted_loss: 0.0231, label: 0, bag_size: 10444\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0050, weighted_loss: 0.0016, label: 0, bag_size: 8252\n",
      "batch 459, loss: 0.0403, instance_loss: 0.0055, weighted_loss: 0.0299, label: 0, bag_size: 22498\n",
      "batch 479, loss: 0.1065, instance_loss: 0.0515, weighted_loss: 0.0900, label: 1, bag_size: 12946\n",
      "batch 499, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 11884\n",
      "batch 519, loss: 0.0086, instance_loss: 0.5264, weighted_loss: 0.1640, label: 1, bag_size: 3651\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0047, weighted_loss: 0.0014, label: 0, bag_size: 14266\n",
      "batch 559, loss: 0.0222, instance_loss: 0.0000, weighted_loss: 0.0155, label: 1, bag_size: 22286\n",
      "batch 579, loss: 0.0136, instance_loss: 0.0026, weighted_loss: 0.0103, label: 1, bag_size: 4880\n",
      "batch 599, loss: 0.3340, instance_loss: 0.0097, weighted_loss: 0.2367, label: 0, bag_size: 2070\n",
      "batch 619, loss: 1.1767, instance_loss: 2.8383, weighted_loss: 1.6752, label: 1, bag_size: 13367\n",
      "batch 639, loss: 0.0116, instance_loss: 0.0003, weighted_loss: 0.0082, label: 1, bag_size: 12095\n",
      "batch 659, loss: 0.0001, instance_loss: 0.0034, weighted_loss: 0.0011, label: 1, bag_size: 5049\n",
      "batch 679, loss: 0.0014, instance_loss: 0.0006, weighted_loss: 0.0011, label: 1, bag_size: 16034\n",
      "batch 699, loss: 0.0041, instance_loss: 0.0096, weighted_loss: 0.0058, label: 0, bag_size: 2873\n",
      "batch 719, loss: 0.0004, instance_loss: 0.0832, weighted_loss: 0.0252, label: 0, bag_size: 3232\n",
      "batch 739, loss: 0.0535, instance_loss: 0.0033, weighted_loss: 0.0384, label: 1, bag_size: 5345\n",
      "batch 759, loss: 0.0020, instance_loss: 0.0001, weighted_loss: 0.0014, label: 1, bag_size: 4250\n",
      "batch 779, loss: 0.0512, instance_loss: 0.2814, weighted_loss: 0.1202, label: 0, bag_size: 2043\n",
      "batch 799, loss: 0.6640, instance_loss: 0.0536, weighted_loss: 0.4809, label: 0, bag_size: 1437\n",
      "batch 819, loss: 0.0070, instance_loss: 0.0000, weighted_loss: 0.0049, label: 0, bag_size: 18215\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9804592457420924: correct 12895/13152\n",
      "class 1 clustering acc 0.9151459854014599: correct 6018/6576\n",
      "Epoch: 45, train_loss: 0.1566, train_clustering_loss:  0.1723, train_error: 0.0608\n",
      "class 0: acc 0.929471032745592, correct 369/397\n",
      "class 1: acc 0.9482352941176471, correct 403/425\n",
      "\n",
      "Val Set, val_loss: 0.3165, val_error: 0.0917, auc: 0.9576\n",
      "class 0 clustering acc 0.9638761467889908: correct 1681/1744\n",
      "class 1 clustering acc 0.7717889908256881: correct 673/872\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3827, instance_loss: 0.1725, weighted_loss: 0.3197, label: 1, bag_size: 15689\n",
      "batch 39, loss: 0.9224, instance_loss: 0.3529, weighted_loss: 0.7515, label: 0, bag_size: 9132\n",
      "batch 59, loss: 0.0004, instance_loss: 0.0015, weighted_loss: 0.0007, label: 0, bag_size: 11512\n",
      "batch 79, loss: 0.0510, instance_loss: 0.0950, weighted_loss: 0.0642, label: 1, bag_size: 5921\n",
      "batch 99, loss: 0.0010, instance_loss: 0.0005, weighted_loss: 0.0008, label: 1, bag_size: 8410\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0168, weighted_loss: 0.0050, label: 0, bag_size: 21218\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0049, weighted_loss: 0.0015, label: 0, bag_size: 8948\n",
      "batch 159, loss: 0.0012, instance_loss: 0.4068, weighted_loss: 0.1229, label: 1, bag_size: 2136\n",
      "batch 179, loss: 0.0705, instance_loss: 0.0009, weighted_loss: 0.0496, label: 0, bag_size: 8549\n",
      "batch 199, loss: 0.0066, instance_loss: 0.0246, weighted_loss: 0.0120, label: 0, bag_size: 13795\n",
      "batch 219, loss: 0.0004, instance_loss: 0.2387, weighted_loss: 0.0719, label: 1, bag_size: 2405\n",
      "batch 239, loss: 0.0989, instance_loss: 0.1198, weighted_loss: 0.1051, label: 1, bag_size: 9689\n",
      "batch 259, loss: 0.0032, instance_loss: 0.0000, weighted_loss: 0.0022, label: 1, bag_size: 10392\n",
      "batch 279, loss: 0.0528, instance_loss: 0.0025, weighted_loss: 0.0377, label: 1, bag_size: 16890\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0206, weighted_loss: 0.0062, label: 1, bag_size: 1316\n",
      "batch 319, loss: 0.0009, instance_loss: 0.0002, weighted_loss: 0.0007, label: 1, bag_size: 18468\n",
      "batch 339, loss: 0.0222, instance_loss: 0.0587, weighted_loss: 0.0332, label: 1, bag_size: 1493\n",
      "batch 359, loss: 0.0004, instance_loss: 0.0244, weighted_loss: 0.0076, label: 1, bag_size: 4259\n",
      "batch 379, loss: 0.0026, instance_loss: 0.0163, weighted_loss: 0.0067, label: 1, bag_size: 14604\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 16782\n",
      "batch 419, loss: 0.0099, instance_loss: 0.0082, weighted_loss: 0.0094, label: 1, bag_size: 5629\n",
      "batch 439, loss: 0.0028, instance_loss: 0.0001, weighted_loss: 0.0020, label: 1, bag_size: 5256\n",
      "batch 459, loss: 0.1194, instance_loss: 0.4135, weighted_loss: 0.2076, label: 0, bag_size: 2266\n",
      "batch 479, loss: 0.0010, instance_loss: 0.0086, weighted_loss: 0.0033, label: 0, bag_size: 4465\n",
      "batch 499, loss: 0.0000, instance_loss: 0.1325, weighted_loss: 0.0398, label: 1, bag_size: 3295\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0084, weighted_loss: 0.0025, label: 0, bag_size: 14266\n",
      "batch 539, loss: 0.1100, instance_loss: 0.1154, weighted_loss: 0.1116, label: 1, bag_size: 6928\n",
      "batch 559, loss: 0.0811, instance_loss: 0.0065, weighted_loss: 0.0587, label: 0, bag_size: 11151\n",
      "batch 579, loss: 1.1795, instance_loss: 0.0000, weighted_loss: 0.8256, label: 1, bag_size: 7748\n",
      "batch 599, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 5317\n",
      "batch 619, loss: 0.0005, instance_loss: 0.1813, weighted_loss: 0.0547, label: 0, bag_size: 2179\n",
      "batch 639, loss: 0.0190, instance_loss: 0.0003, weighted_loss: 0.0134, label: 1, bag_size: 4239\n",
      "batch 659, loss: 0.2230, instance_loss: 0.4190, weighted_loss: 0.2818, label: 0, bag_size: 7031\n",
      "batch 679, loss: 0.0451, instance_loss: 0.0364, weighted_loss: 0.0425, label: 0, bag_size: 4523\n",
      "batch 699, loss: 0.0391, instance_loss: 0.0000, weighted_loss: 0.0273, label: 1, bag_size: 16267\n",
      "batch 719, loss: 0.0001, instance_loss: 0.0032, weighted_loss: 0.0010, label: 0, bag_size: 6851\n",
      "batch 739, loss: 0.0369, instance_loss: 0.0237, weighted_loss: 0.0329, label: 0, bag_size: 3670\n",
      "batch 759, loss: 0.1688, instance_loss: 0.0000, weighted_loss: 0.1181, label: 1, bag_size: 19606\n",
      "batch 779, loss: 0.0019, instance_loss: 0.0928, weighted_loss: 0.0292, label: 1, bag_size: 16512\n",
      "batch 799, loss: 0.0064, instance_loss: 0.0155, weighted_loss: 0.0091, label: 0, bag_size: 6898\n",
      "batch 819, loss: 0.0046, instance_loss: 0.0001, weighted_loss: 0.0033, label: 1, bag_size: 7382\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9814476885644768: correct 12908/13152\n",
      "class 1 clustering acc 0.9078467153284672: correct 5970/6576\n",
      "Epoch: 46, train_loss: 0.1723, train_clustering_loss:  0.1736, train_error: 0.0645\n",
      "class 0: acc 0.9257425742574258, correct 374/404\n",
      "class 1: acc 0.9449760765550239, correct 395/418\n",
      "\n",
      "Val Set, val_loss: 0.3476, val_error: 0.0826, auc: 0.9565\n",
      "class 0 clustering acc 0.9248853211009175: correct 1613/1744\n",
      "class 1 clustering acc 0.8073394495412844: correct 704/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0005, weighted_loss: 0.0003, label: 0, bag_size: 19659\n",
      "batch 39, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 18045\n",
      "batch 59, loss: 0.0664, instance_loss: 0.0038, weighted_loss: 0.0476, label: 1, bag_size: 3224\n",
      "batch 79, loss: 0.0076, instance_loss: 0.0000, weighted_loss: 0.0053, label: 0, bag_size: 25027\n",
      "batch 99, loss: 0.0513, instance_loss: 0.0003, weighted_loss: 0.0360, label: 1, bag_size: 19606\n",
      "batch 119, loss: 0.8718, instance_loss: 0.2913, weighted_loss: 0.6976, label: 0, bag_size: 2959\n",
      "batch 139, loss: 0.0007, instance_loss: 0.0010, weighted_loss: 0.0008, label: 0, bag_size: 8898\n",
      "batch 159, loss: 0.0951, instance_loss: 0.2645, weighted_loss: 0.1459, label: 0, bag_size: 4598\n",
      "batch 179, loss: 0.0030, instance_loss: 0.0000, weighted_loss: 0.0021, label: 1, bag_size: 2695\n",
      "batch 199, loss: 0.0060, instance_loss: 0.0085, weighted_loss: 0.0068, label: 1, bag_size: 7371\n",
      "batch 219, loss: 1.3398, instance_loss: 0.7213, weighted_loss: 1.1542, label: 1, bag_size: 8103\n",
      "batch 239, loss: 0.0277, instance_loss: 0.0399, weighted_loss: 0.0313, label: 1, bag_size: 1924\n",
      "batch 259, loss: 0.0306, instance_loss: 0.0362, weighted_loss: 0.0323, label: 1, bag_size: 1924\n",
      "batch 279, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 17486\n",
      "batch 299, loss: 0.0006, instance_loss: 0.0848, weighted_loss: 0.0258, label: 0, bag_size: 10535\n",
      "batch 319, loss: 0.0008, instance_loss: 0.8311, weighted_loss: 0.2499, label: 1, bag_size: 18649\n",
      "batch 339, loss: 0.0003, instance_loss: 0.0627, weighted_loss: 0.0190, label: 0, bag_size: 21385\n",
      "batch 359, loss: 0.3445, instance_loss: 0.0054, weighted_loss: 0.2428, label: 1, bag_size: 7445\n",
      "batch 379, loss: 0.0107, instance_loss: 0.0000, weighted_loss: 0.0075, label: 1, bag_size: 22286\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0728, weighted_loss: 0.0219, label: 0, bag_size: 21218\n",
      "batch 419, loss: 0.9817, instance_loss: 0.7928, weighted_loss: 0.9250, label: 0, bag_size: 1437\n",
      "batch 439, loss: 1.0931, instance_loss: 0.0720, weighted_loss: 0.7868, label: 0, bag_size: 11212\n",
      "batch 459, loss: 0.1771, instance_loss: 0.0016, weighted_loss: 0.1245, label: 0, bag_size: 15672\n",
      "batch 479, loss: 0.0150, instance_loss: 0.2592, weighted_loss: 0.0882, label: 1, bag_size: 2759\n",
      "batch 499, loss: 0.0816, instance_loss: 0.0009, weighted_loss: 0.0574, label: 1, bag_size: 3674\n",
      "batch 519, loss: 0.0084, instance_loss: 0.0036, weighted_loss: 0.0070, label: 1, bag_size: 8216\n",
      "batch 539, loss: 0.0167, instance_loss: 0.0165, weighted_loss: 0.0166, label: 0, bag_size: 3557\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 17486\n",
      "batch 579, loss: 0.0007, instance_loss: 0.0478, weighted_loss: 0.0148, label: 0, bag_size: 14305\n",
      "batch 599, loss: 0.0014, instance_loss: 0.0326, weighted_loss: 0.0107, label: 1, bag_size: 3207\n",
      "batch 619, loss: 2.9826, instance_loss: 2.1970, weighted_loss: 2.7469, label: 0, bag_size: 4692\n",
      "batch 639, loss: 2.7413, instance_loss: 0.2421, weighted_loss: 1.9915, label: 0, bag_size: 23618\n",
      "batch 659, loss: 0.0332, instance_loss: 0.0229, weighted_loss: 0.0301, label: 0, bag_size: 3321\n",
      "batch 679, loss: 0.0002, instance_loss: 0.0020, weighted_loss: 0.0008, label: 0, bag_size: 10791\n",
      "batch 699, loss: 0.0074, instance_loss: 0.0015, weighted_loss: 0.0056, label: 1, bag_size: 5629\n",
      "batch 719, loss: 0.0025, instance_loss: 0.0383, weighted_loss: 0.0132, label: 1, bag_size: 1172\n",
      "batch 739, loss: 0.3426, instance_loss: 0.2827, weighted_loss: 0.3246, label: 0, bag_size: 2959\n",
      "batch 759, loss: 0.0777, instance_loss: 0.0119, weighted_loss: 0.0580, label: 1, bag_size: 11684\n",
      "batch 779, loss: 0.0235, instance_loss: 0.0004, weighted_loss: 0.0166, label: 1, bag_size: 13015\n",
      "batch 799, loss: 0.0233, instance_loss: 0.0084, weighted_loss: 0.0188, label: 0, bag_size: 14885\n",
      "batch 819, loss: 0.1159, instance_loss: 0.0006, weighted_loss: 0.0813, label: 0, bag_size: 10029\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9798509732360098: correct 12887/13152\n",
      "class 1 clustering acc 0.9017639902676399: correct 5930/6576\n",
      "Epoch: 47, train_loss: 0.1621, train_clustering_loss:  0.1978, train_error: 0.0633\n",
      "class 0: acc 0.9363207547169812, correct 397/424\n",
      "class 1: acc 0.9371859296482412, correct 373/398\n",
      "\n",
      "Val Set, val_loss: 0.3412, val_error: 0.0917, auc: 0.9555\n",
      "class 0 clustering acc 0.955848623853211: correct 1667/1744\n",
      "class 1 clustering acc 0.7809633027522935: correct 681/872\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0808, instance_loss: 0.0333, weighted_loss: 0.0665, label: 0, bag_size: 3725\n",
      "batch 39, loss: 0.1813, instance_loss: 0.0035, weighted_loss: 0.1279, label: 1, bag_size: 15192\n",
      "batch 59, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 11727\n",
      "batch 79, loss: 0.0089, instance_loss: 0.1037, weighted_loss: 0.0373, label: 0, bag_size: 3474\n",
      "batch 99, loss: 0.1140, instance_loss: 0.0000, weighted_loss: 0.0798, label: 0, bag_size: 47866\n",
      "batch 119, loss: 1.1475, instance_loss: 0.0113, weighted_loss: 0.8066, label: 1, bag_size: 9215\n",
      "batch 139, loss: 0.0256, instance_loss: 0.0000, weighted_loss: 0.0179, label: 0, bag_size: 29270\n",
      "batch 159, loss: 0.0002, instance_loss: 0.0001, weighted_loss: 0.0002, label: 1, bag_size: 5561\n",
      "batch 179, loss: 0.0312, instance_loss: 0.0000, weighted_loss: 0.0218, label: 0, bag_size: 10113\n",
      "batch 199, loss: 0.0156, instance_loss: 0.0000, weighted_loss: 0.0109, label: 0, bag_size: 25814\n",
      "batch 219, loss: 0.0069, instance_loss: 0.0001, weighted_loss: 0.0049, label: 1, bag_size: 9548\n",
      "batch 239, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 16052\n",
      "batch 259, loss: 0.0301, instance_loss: 0.0000, weighted_loss: 0.0210, label: 1, bag_size: 2140\n",
      "batch 279, loss: 0.0088, instance_loss: 0.0000, weighted_loss: 0.0062, label: 1, bag_size: 11387\n",
      "batch 299, loss: 0.0020, instance_loss: 0.0000, weighted_loss: 0.0014, label: 0, bag_size: 19880\n",
      "batch 319, loss: 0.0024, instance_loss: 0.0000, weighted_loss: 0.0017, label: 1, bag_size: 1022\n",
      "batch 339, loss: 0.0420, instance_loss: 0.0044, weighted_loss: 0.0307, label: 0, bag_size: 9471\n",
      "batch 359, loss: 0.0261, instance_loss: 0.1454, weighted_loss: 0.0619, label: 0, bag_size: 1891\n",
      "batch 379, loss: 0.0034, instance_loss: 0.2378, weighted_loss: 0.0737, label: 0, bag_size: 1234\n",
      "batch 399, loss: 0.0717, instance_loss: 0.0024, weighted_loss: 0.0509, label: 1, bag_size: 2848\n",
      "batch 419, loss: 1.2205, instance_loss: 0.0383, weighted_loss: 0.8659, label: 1, bag_size: 1819\n",
      "batch 439, loss: 0.1505, instance_loss: 0.0707, weighted_loss: 0.1265, label: 0, bag_size: 2242\n",
      "batch 459, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 10995\n",
      "batch 479, loss: 0.0252, instance_loss: 0.0041, weighted_loss: 0.0189, label: 0, bag_size: 5999\n",
      "batch 499, loss: 1.4060, instance_loss: 0.3562, weighted_loss: 1.0911, label: 1, bag_size: 1038\n",
      "batch 519, loss: 0.0002, instance_loss: 0.0044, weighted_loss: 0.0015, label: 0, bag_size: 8252\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 22800\n",
      "batch 559, loss: 0.0004, instance_loss: 0.0342, weighted_loss: 0.0106, label: 0, bag_size: 2548\n",
      "batch 579, loss: 0.0036, instance_loss: 0.2194, weighted_loss: 0.0683, label: 1, bag_size: 14433\n",
      "batch 599, loss: 0.3252, instance_loss: 0.2267, weighted_loss: 0.2957, label: 0, bag_size: 4241\n",
      "batch 619, loss: 0.0106, instance_loss: 0.0004, weighted_loss: 0.0075, label: 1, bag_size: 9913\n",
      "batch 639, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 22828\n",
      "batch 659, loss: 0.0271, instance_loss: 0.1222, weighted_loss: 0.0556, label: 0, bag_size: 11194\n",
      "batch 679, loss: 0.0016, instance_loss: 0.0478, weighted_loss: 0.0154, label: 0, bag_size: 8372\n",
      "batch 699, loss: 0.0281, instance_loss: 0.0533, weighted_loss: 0.0356, label: 0, bag_size: 1438\n",
      "batch 719, loss: 0.0137, instance_loss: 0.0769, weighted_loss: 0.0327, label: 0, bag_size: 2518\n",
      "batch 739, loss: 0.1536, instance_loss: 0.0436, weighted_loss: 0.1206, label: 0, bag_size: 6093\n",
      "batch 759, loss: 0.0352, instance_loss: 0.0393, weighted_loss: 0.0364, label: 0, bag_size: 7923\n",
      "batch 779, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 27158\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14266\n",
      "batch 819, loss: 0.0619, instance_loss: 0.0073, weighted_loss: 0.0455, label: 1, bag_size: 2140\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9833485401459854: correct 12933/13152\n",
      "class 1 clustering acc 0.9105839416058394: correct 5988/6576\n",
      "Epoch: 48, train_loss: 0.1619, train_clustering_loss:  0.1688, train_error: 0.0669\n",
      "class 0: acc 0.9305555555555556, correct 402/432\n",
      "class 1: acc 0.9358974358974359, correct 365/390\n",
      "\n",
      "Val Set, val_loss: 0.3390, val_error: 0.0826, auc: 0.9596\n",
      "class 0 clustering acc 0.9587155963302753: correct 1672/1744\n",
      "class 1 clustering acc 0.8761467889908257: correct 764/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0942, instance_loss: 0.4076, weighted_loss: 0.1882, label: 1, bag_size: 8754\n",
      "batch 39, loss: 0.0127, instance_loss: 0.0019, weighted_loss: 0.0095, label: 1, bag_size: 10392\n",
      "batch 59, loss: 0.0006, instance_loss: 0.0345, weighted_loss: 0.0108, label: 0, bag_size: 3265\n",
      "batch 79, loss: 2.4442, instance_loss: 0.5241, weighted_loss: 1.8682, label: 1, bag_size: 12494\n",
      "batch 99, loss: 0.2523, instance_loss: 0.0019, weighted_loss: 0.1772, label: 1, bag_size: 13362\n",
      "batch 119, loss: 0.0024, instance_loss: 0.0255, weighted_loss: 0.0093, label: 1, bag_size: 2455\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14266\n",
      "batch 159, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 9759\n",
      "batch 179, loss: 0.0011, instance_loss: 0.0300, weighted_loss: 0.0098, label: 0, bag_size: 22681\n",
      "batch 199, loss: 0.0183, instance_loss: 0.0109, weighted_loss: 0.0161, label: 1, bag_size: 2814\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0048, weighted_loss: 0.0015, label: 0, bag_size: 11690\n",
      "batch 239, loss: 0.1766, instance_loss: 0.0149, weighted_loss: 0.1281, label: 1, bag_size: 13692\n",
      "batch 259, loss: 0.0051, instance_loss: 0.0174, weighted_loss: 0.0088, label: 1, bag_size: 5256\n",
      "batch 279, loss: 0.0025, instance_loss: 0.0000, weighted_loss: 0.0017, label: 1, bag_size: 8602\n",
      "batch 299, loss: 0.0013, instance_loss: 0.0034, weighted_loss: 0.0020, label: 0, bag_size: 12593\n",
      "batch 319, loss: 0.0204, instance_loss: 0.0639, weighted_loss: 0.0335, label: 1, bag_size: 5454\n",
      "batch 339, loss: 0.0146, instance_loss: 0.0126, weighted_loss: 0.0140, label: 0, bag_size: 1349\n",
      "batch 359, loss: 0.0083, instance_loss: 0.0216, weighted_loss: 0.0123, label: 0, bag_size: 8755\n",
      "batch 379, loss: 0.2168, instance_loss: 0.5225, weighted_loss: 0.3085, label: 0, bag_size: 2098\n",
      "batch 399, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 19039\n",
      "batch 419, loss: 0.0015, instance_loss: 0.0031, weighted_loss: 0.0020, label: 0, bag_size: 8898\n",
      "batch 439, loss: 0.0744, instance_loss: 0.2190, weighted_loss: 0.1178, label: 1, bag_size: 1123\n",
      "batch 459, loss: 0.0004, instance_loss: 0.0433, weighted_loss: 0.0132, label: 1, bag_size: 3576\n",
      "batch 479, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 11477\n",
      "batch 499, loss: 0.0001, instance_loss: 0.4645, weighted_loss: 0.1394, label: 0, bag_size: 10481\n",
      "batch 519, loss: 0.0002, instance_loss: 0.0063, weighted_loss: 0.0020, label: 0, bag_size: 1061\n",
      "batch 539, loss: 0.0040, instance_loss: 0.0031, weighted_loss: 0.0037, label: 0, bag_size: 5409\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 17486\n",
      "batch 579, loss: 0.1130, instance_loss: 0.0374, weighted_loss: 0.0903, label: 0, bag_size: 3502\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0035, weighted_loss: 0.0011, label: 1, bag_size: 1360\n",
      "batch 619, loss: 0.0006, instance_loss: 0.1066, weighted_loss: 0.0324, label: 0, bag_size: 2424\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0103, weighted_loss: 0.0032, label: 0, bag_size: 8981\n",
      "batch 659, loss: 0.0145, instance_loss: 0.1754, weighted_loss: 0.0627, label: 1, bag_size: 549\n",
      "batch 679, loss: 0.4171, instance_loss: 0.0603, weighted_loss: 0.3100, label: 1, bag_size: 8026\n",
      "batch 699, loss: 1.7240, instance_loss: 0.3329, weighted_loss: 1.3066, label: 0, bag_size: 5105\n",
      "batch 719, loss: 0.0031, instance_loss: 0.0029, weighted_loss: 0.0031, label: 1, bag_size: 9446\n",
      "batch 739, loss: 1.6299, instance_loss: 0.0584, weighted_loss: 1.1584, label: 0, bag_size: 2918\n",
      "batch 759, loss: 0.3827, instance_loss: 0.4389, weighted_loss: 0.3996, label: 0, bag_size: 1592\n",
      "batch 779, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9759\n",
      "batch 799, loss: 0.0033, instance_loss: 0.0482, weighted_loss: 0.0168, label: 1, bag_size: 1255\n",
      "batch 819, loss: 0.0002, instance_loss: 0.0812, weighted_loss: 0.0245, label: 0, bag_size: 21385\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9834245742092458: correct 12934/13152\n",
      "class 1 clustering acc 0.9087591240875912: correct 5976/6576\n",
      "Epoch: 49, train_loss: 0.1536, train_clustering_loss:  0.1642, train_error: 0.0645\n",
      "class 0: acc 0.9321608040201005, correct 371/398\n",
      "class 1: acc 0.9386792452830188, correct 398/424\n",
      "\n",
      "Val Set, val_loss: 0.3248, val_error: 0.1009, auc: 0.9569\n",
      "class 0 clustering acc 0.9724770642201835: correct 1696/1744\n",
      "class 1 clustering acc 0.7809633027522935: correct 681/872\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0030, instance_loss: 0.0533, weighted_loss: 0.0181, label: 0, bag_size: 2628\n",
      "batch 39, loss: 0.0029, instance_loss: 0.0025, weighted_loss: 0.0028, label: 1, bag_size: 2278\n",
      "batch 59, loss: 0.0010, instance_loss: 0.0594, weighted_loss: 0.0185, label: 1, bag_size: 4317\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0111, weighted_loss: 0.0033, label: 0, bag_size: 13225\n",
      "batch 99, loss: 0.0010, instance_loss: 0.0017, weighted_loss: 0.0012, label: 0, bag_size: 12731\n",
      "batch 119, loss: 0.0577, instance_loss: 0.0294, weighted_loss: 0.0492, label: 1, bag_size: 16890\n",
      "batch 139, loss: 0.0918, instance_loss: 0.1306, weighted_loss: 0.1034, label: 0, bag_size: 3444\n",
      "batch 159, loss: 0.1691, instance_loss: 0.0302, weighted_loss: 0.1274, label: 1, bag_size: 10622\n",
      "batch 179, loss: 0.0017, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 2036\n",
      "batch 199, loss: 0.0467, instance_loss: 0.0036, weighted_loss: 0.0338, label: 0, bag_size: 6093\n",
      "batch 219, loss: 0.0073, instance_loss: 0.0254, weighted_loss: 0.0127, label: 0, bag_size: 14739\n",
      "batch 239, loss: 0.0061, instance_loss: 0.0000, weighted_loss: 0.0042, label: 0, bag_size: 14625\n",
      "batch 259, loss: 0.0046, instance_loss: 0.0004, weighted_loss: 0.0034, label: 1, bag_size: 10969\n",
      "batch 279, loss: 0.0011, instance_loss: 0.0699, weighted_loss: 0.0217, label: 0, bag_size: 2303\n",
      "batch 299, loss: 0.4148, instance_loss: 0.0003, weighted_loss: 0.2904, label: 0, bag_size: 5120\n",
      "batch 319, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 12687\n",
      "batch 339, loss: 0.0219, instance_loss: 0.4298, weighted_loss: 0.1443, label: 0, bag_size: 1800\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 16341\n",
      "batch 379, loss: 1.5927, instance_loss: 5.9463, weighted_loss: 2.8988, label: 1, bag_size: 3879\n",
      "batch 399, loss: 0.0001, instance_loss: 0.0290, weighted_loss: 0.0087, label: 1, bag_size: 1360\n",
      "batch 419, loss: 0.0658, instance_loss: 0.0000, weighted_loss: 0.0461, label: 1, bag_size: 5345\n",
      "batch 439, loss: 0.1998, instance_loss: 1.1096, weighted_loss: 0.4727, label: 1, bag_size: 1095\n",
      "batch 459, loss: 0.2801, instance_loss: 0.0898, weighted_loss: 0.2230, label: 0, bag_size: 7835\n",
      "batch 479, loss: 0.0010, instance_loss: 0.0033, weighted_loss: 0.0017, label: 0, bag_size: 21864\n",
      "batch 499, loss: 0.2351, instance_loss: 0.5743, weighted_loss: 0.3369, label: 1, bag_size: 10848\n",
      "batch 519, loss: 0.5997, instance_loss: 0.1434, weighted_loss: 0.4628, label: 0, bag_size: 1953\n",
      "batch 539, loss: 0.0044, instance_loss: 0.0999, weighted_loss: 0.0331, label: 1, bag_size: 5894\n",
      "batch 559, loss: 0.0786, instance_loss: 0.1244, weighted_loss: 0.0924, label: 0, bag_size: 1684\n",
      "batch 579, loss: 0.4773, instance_loss: 0.0000, weighted_loss: 0.3341, label: 0, bag_size: 21361\n",
      "batch 599, loss: 2.4047, instance_loss: 1.1859, weighted_loss: 2.0391, label: 1, bag_size: 1497\n",
      "batch 619, loss: 0.2761, instance_loss: 0.1219, weighted_loss: 0.2299, label: 0, bag_size: 8427\n",
      "batch 639, loss: 0.0111, instance_loss: 0.0795, weighted_loss: 0.0316, label: 1, bag_size: 11266\n",
      "batch 659, loss: 0.0507, instance_loss: 0.1135, weighted_loss: 0.0695, label: 0, bag_size: 2760\n",
      "batch 679, loss: 0.0239, instance_loss: 0.0841, weighted_loss: 0.0420, label: 0, bag_size: 931\n",
      "batch 699, loss: 0.0647, instance_loss: 2.5077, weighted_loss: 0.7976, label: 0, bag_size: 2996\n",
      "batch 719, loss: 0.0005, instance_loss: 0.0196, weighted_loss: 0.0062, label: 0, bag_size: 23368\n",
      "batch 739, loss: 0.0230, instance_loss: 0.0148, weighted_loss: 0.0205, label: 0, bag_size: 18954\n",
      "batch 759, loss: 0.0029, instance_loss: 0.3229, weighted_loss: 0.0989, label: 0, bag_size: 2534\n",
      "batch 779, loss: 0.0775, instance_loss: 0.0000, weighted_loss: 0.0542, label: 0, bag_size: 3238\n",
      "batch 799, loss: 0.0074, instance_loss: 0.0004, weighted_loss: 0.0053, label: 1, bag_size: 28527\n",
      "batch 819, loss: 0.0002, instance_loss: 0.1406, weighted_loss: 0.0423, label: 1, bag_size: 19932\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9755930656934306: correct 12831/13152\n",
      "class 1 clustering acc 0.8725669099756691: correct 5738/6576\n",
      "Epoch: 50, train_loss: 0.1338, train_clustering_loss:  0.2430, train_error: 0.0511\n",
      "class 0: acc 0.9382422802850356, correct 395/421\n",
      "class 1: acc 0.9600997506234414, correct 385/401\n",
      "\n",
      "Val Set, val_loss: 0.2857, val_error: 0.1193, auc: 0.9589\n",
      "class 0 clustering acc 0.9409403669724771: correct 1641/1744\n",
      "class 1 clustering acc 0.7637614678899083: correct 666/872\n",
      "class 0: acc 0.8695652173913043, correct 40/46\n",
      "class 1: acc 0.8888888888888888, correct 56/63\n",
      "Validation loss decreased (0.287913 --> 0.285709).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0265, instance_loss: 0.2852, weighted_loss: 0.1041, label: 1, bag_size: 1064\n",
      "batch 39, loss: 0.0149, instance_loss: 0.0241, weighted_loss: 0.0176, label: 0, bag_size: 19880\n",
      "batch 59, loss: 0.0105, instance_loss: 0.0001, weighted_loss: 0.0074, label: 0, bag_size: 11917\n",
      "batch 79, loss: 0.0307, instance_loss: 0.0527, weighted_loss: 0.0373, label: 1, bag_size: 18699\n",
      "batch 99, loss: 0.0479, instance_loss: 0.3274, weighted_loss: 0.1318, label: 1, bag_size: 9519\n",
      "batch 119, loss: 0.0110, instance_loss: 0.0516, weighted_loss: 0.0232, label: 0, bag_size: 9866\n",
      "batch 139, loss: 0.0291, instance_loss: 0.1743, weighted_loss: 0.0727, label: 1, bag_size: 3674\n",
      "batch 159, loss: 0.0003, instance_loss: 0.0186, weighted_loss: 0.0058, label: 0, bag_size: 3190\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15850\n",
      "batch 199, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 21093\n",
      "batch 219, loss: 0.0096, instance_loss: 0.1477, weighted_loss: 0.0511, label: 0, bag_size: 3557\n",
      "batch 239, loss: 0.0041, instance_loss: 0.0237, weighted_loss: 0.0100, label: 1, bag_size: 12697\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 10481\n",
      "batch 279, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 27012\n",
      "batch 299, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 19039\n",
      "batch 319, loss: 0.0120, instance_loss: 0.0000, weighted_loss: 0.0084, label: 0, bag_size: 11151\n",
      "batch 339, loss: 0.2243, instance_loss: 0.0694, weighted_loss: 0.1778, label: 1, bag_size: 2140\n",
      "batch 359, loss: 4.1691, instance_loss: 2.3626, weighted_loss: 3.6272, label: 1, bag_size: 684\n",
      "batch 379, loss: 0.0003, instance_loss: 0.0085, weighted_loss: 0.0028, label: 0, bag_size: 11125\n",
      "batch 399, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 17437\n",
      "batch 419, loss: 0.0664, instance_loss: 0.0199, weighted_loss: 0.0525, label: 1, bag_size: 16514\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0640, weighted_loss: 0.0193, label: 0, bag_size: 11735\n",
      "batch 459, loss: 0.0014, instance_loss: 0.0003, weighted_loss: 0.0011, label: 1, bag_size: 9065\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0008, weighted_loss: 0.0003, label: 0, bag_size: 16052\n",
      "batch 499, loss: 0.0030, instance_loss: 0.2216, weighted_loss: 0.0686, label: 0, bag_size: 1483\n",
      "batch 519, loss: 0.1397, instance_loss: 0.0471, weighted_loss: 0.1119, label: 1, bag_size: 3656\n",
      "batch 539, loss: 0.4823, instance_loss: 0.4224, weighted_loss: 0.4643, label: 1, bag_size: 1230\n",
      "batch 559, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 16087\n",
      "batch 579, loss: 0.0020, instance_loss: 0.0000, weighted_loss: 0.0014, label: 0, bag_size: 31085\n",
      "batch 599, loss: 0.1365, instance_loss: 0.0384, weighted_loss: 0.1071, label: 0, bag_size: 13619\n",
      "batch 619, loss: 0.0005, instance_loss: 0.2450, weighted_loss: 0.0738, label: 0, bag_size: 19808\n",
      "batch 639, loss: 0.2193, instance_loss: 0.0000, weighted_loss: 0.1535, label: 1, bag_size: 12714\n",
      "batch 659, loss: 0.0402, instance_loss: 0.0000, weighted_loss: 0.0281, label: 1, bag_size: 21701\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 20150\n",
      "batch 699, loss: 0.0538, instance_loss: 0.0850, weighted_loss: 0.0632, label: 1, bag_size: 8754\n",
      "batch 719, loss: 0.0519, instance_loss: 0.3231, weighted_loss: 0.1332, label: 0, bag_size: 1142\n",
      "batch 739, loss: 0.0026, instance_loss: 0.1415, weighted_loss: 0.0443, label: 0, bag_size: 1483\n",
      "batch 759, loss: 0.0079, instance_loss: 0.0041, weighted_loss: 0.0067, label: 1, bag_size: 4039\n",
      "batch 779, loss: 0.0002, instance_loss: 0.0103, weighted_loss: 0.0032, label: 1, bag_size: 699\n",
      "batch 799, loss: 0.0286, instance_loss: 0.2228, weighted_loss: 0.0869, label: 0, bag_size: 16690\n",
      "batch 819, loss: 1.2576, instance_loss: 0.1195, weighted_loss: 0.9162, label: 0, bag_size: 2653\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.979470802919708: correct 12882/13152\n",
      "class 1 clustering acc 0.8756082725060828: correct 5758/6576\n",
      "Epoch: 51, train_loss: 0.1892, train_clustering_loss:  0.2093, train_error: 0.0779\n",
      "class 0: acc 0.9216589861751152, correct 400/434\n",
      "class 1: acc 0.9226804123711341, correct 358/388\n",
      "\n",
      "Val Set, val_loss: 0.3340, val_error: 0.0826, auc: 0.9586\n",
      "class 0 clustering acc 0.9644495412844036: correct 1682/1744\n",
      "class 1 clustering acc 0.8474770642201835: correct 739/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0059, instance_loss: 0.0099, weighted_loss: 0.0071, label: 0, bag_size: 12731\n",
      "batch 39, loss: 0.0012, instance_loss: 0.0051, weighted_loss: 0.0023, label: 0, bag_size: 12731\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0024, weighted_loss: 0.0007, label: 0, bag_size: 3787\n",
      "batch 79, loss: 0.1091, instance_loss: 0.0008, weighted_loss: 0.0766, label: 1, bag_size: 16565\n",
      "batch 99, loss: 0.0002, instance_loss: 0.0020, weighted_loss: 0.0007, label: 1, bag_size: 4102\n",
      "batch 119, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 15093\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0590, weighted_loss: 0.0177, label: 1, bag_size: 629\n",
      "batch 159, loss: 0.2222, instance_loss: 0.0127, weighted_loss: 0.1593, label: 1, bag_size: 1483\n",
      "batch 179, loss: 0.0728, instance_loss: 0.0058, weighted_loss: 0.0527, label: 0, bag_size: 3876\n",
      "batch 199, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0014, label: 1, bag_size: 20161\n",
      "batch 219, loss: 0.2675, instance_loss: 0.0435, weighted_loss: 0.2003, label: 0, bag_size: 3321\n",
      "batch 239, loss: 1.9822, instance_loss: 0.0131, weighted_loss: 1.3914, label: 0, bag_size: 3897\n",
      "batch 259, loss: 0.6137, instance_loss: 0.0229, weighted_loss: 0.4364, label: 1, bag_size: 2681\n",
      "batch 279, loss: 0.0255, instance_loss: 0.0212, weighted_loss: 0.0242, label: 1, bag_size: 7148\n",
      "batch 299, loss: 0.0355, instance_loss: 0.0008, weighted_loss: 0.0251, label: 1, bag_size: 6781\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0006, label: 0, bag_size: 12217\n",
      "batch 339, loss: 0.3869, instance_loss: 0.3994, weighted_loss: 0.3906, label: 1, bag_size: 2935\n",
      "batch 359, loss: 0.0007, instance_loss: 0.0856, weighted_loss: 0.0262, label: 1, bag_size: 5991\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14266\n",
      "batch 399, loss: 0.0106, instance_loss: 0.0203, weighted_loss: 0.0135, label: 0, bag_size: 3725\n",
      "batch 419, loss: 0.0012, instance_loss: 0.0006, weighted_loss: 0.0011, label: 0, bag_size: 14305\n",
      "batch 439, loss: 0.3573, instance_loss: 0.1206, weighted_loss: 0.2863, label: 0, bag_size: 2160\n",
      "batch 459, loss: 0.0050, instance_loss: 0.0000, weighted_loss: 0.0035, label: 0, bag_size: 21032\n",
      "batch 479, loss: 0.1103, instance_loss: 0.1092, weighted_loss: 0.1100, label: 0, bag_size: 17482\n",
      "batch 499, loss: 0.0025, instance_loss: 0.0011, weighted_loss: 0.0021, label: 0, bag_size: 13691\n",
      "batch 519, loss: 0.0298, instance_loss: 0.3415, weighted_loss: 0.1233, label: 1, bag_size: 8935\n",
      "batch 539, loss: 0.0002, instance_loss: 0.0001, weighted_loss: 0.0002, label: 0, bag_size: 19435\n",
      "batch 559, loss: 0.1882, instance_loss: 0.9807, weighted_loss: 0.4259, label: 1, bag_size: 21252\n",
      "batch 579, loss: 0.2412, instance_loss: 0.7078, weighted_loss: 0.3812, label: 1, bag_size: 9330\n",
      "batch 599, loss: 0.1372, instance_loss: 0.1683, weighted_loss: 0.1465, label: 0, bag_size: 3399\n",
      "batch 619, loss: 0.2676, instance_loss: 0.8832, weighted_loss: 0.4523, label: 0, bag_size: 2458\n",
      "batch 639, loss: 0.2672, instance_loss: 1.8460, weighted_loss: 0.7408, label: 1, bag_size: 21252\n",
      "batch 659, loss: 0.0385, instance_loss: 0.2111, weighted_loss: 0.0902, label: 0, bag_size: 2104\n",
      "batch 679, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 1, bag_size: 16512\n",
      "batch 699, loss: 0.0015, instance_loss: 0.0116, weighted_loss: 0.0045, label: 1, bag_size: 621\n",
      "batch 719, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 15665\n",
      "batch 739, loss: 0.1460, instance_loss: 0.2233, weighted_loss: 0.1692, label: 1, bag_size: 1242\n",
      "batch 759, loss: 0.0007, instance_loss: 0.0162, weighted_loss: 0.0053, label: 1, bag_size: 3549\n",
      "batch 779, loss: 3.0029, instance_loss: 2.0498, weighted_loss: 2.7170, label: 1, bag_size: 1497\n",
      "batch 799, loss: 0.9976, instance_loss: 0.3207, weighted_loss: 0.7945, label: 0, bag_size: 1953\n",
      "batch 819, loss: 1.1351, instance_loss: 0.4359, weighted_loss: 0.9253, label: 0, bag_size: 2918\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9784063260340633: correct 12868/13152\n",
      "class 1 clustering acc 0.8853406326034063: correct 5822/6576\n",
      "Epoch: 52, train_loss: 0.1604, train_clustering_loss:  0.2133, train_error: 0.0718\n",
      "class 0: acc 0.9228915662650602, correct 383/415\n",
      "class 1: acc 0.9336609336609336, correct 380/407\n",
      "\n",
      "Val Set, val_loss: 0.3543, val_error: 0.0826, auc: 0.9596\n",
      "class 0 clustering acc 0.9701834862385321: correct 1692/1744\n",
      "class 1 clustering acc 0.8038990825688074: correct 701/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0406, instance_loss: 0.0173, weighted_loss: 0.0336, label: 1, bag_size: 8191\n",
      "batch 39, loss: 0.4947, instance_loss: 0.0202, weighted_loss: 0.3523, label: 0, bag_size: 5120\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0366, weighted_loss: 0.0110, label: 0, bag_size: 11690\n",
      "batch 79, loss: 0.1979, instance_loss: 0.0000, weighted_loss: 0.1385, label: 0, bag_size: 26208\n",
      "batch 99, loss: 3.3569, instance_loss: 3.6365, weighted_loss: 3.4408, label: 1, bag_size: 15563\n",
      "batch 119, loss: 0.0008, instance_loss: 0.0044, weighted_loss: 0.0019, label: 1, bag_size: 4959\n",
      "batch 139, loss: 0.0276, instance_loss: 0.0006, weighted_loss: 0.0195, label: 1, bag_size: 10028\n",
      "batch 159, loss: 0.0020, instance_loss: 0.0000, weighted_loss: 0.0014, label: 1, bag_size: 15213\n",
      "batch 179, loss: 0.0955, instance_loss: 0.2709, weighted_loss: 0.1481, label: 1, bag_size: 1525\n",
      "batch 199, loss: 0.4354, instance_loss: 0.3381, weighted_loss: 0.4062, label: 0, bag_size: 2996\n",
      "batch 219, loss: 0.4459, instance_loss: 0.0058, weighted_loss: 0.3139, label: 1, bag_size: 11256\n",
      "batch 239, loss: 0.0007, instance_loss: 0.0533, weighted_loss: 0.0165, label: 1, bag_size: 20333\n",
      "batch 259, loss: 0.0307, instance_loss: 0.0027, weighted_loss: 0.0223, label: 0, bag_size: 14739\n",
      "batch 279, loss: 0.0350, instance_loss: 0.0604, weighted_loss: 0.0426, label: 1, bag_size: 3368\n",
      "batch 299, loss: 0.0060, instance_loss: 0.0006, weighted_loss: 0.0043, label: 1, bag_size: 4959\n",
      "batch 319, loss: 0.0242, instance_loss: 0.0674, weighted_loss: 0.0371, label: 0, bag_size: 2518\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11546\n",
      "batch 359, loss: 0.1393, instance_loss: 0.0626, weighted_loss: 0.1162, label: 1, bag_size: 7613\n",
      "batch 379, loss: 0.0065, instance_loss: 0.5282, weighted_loss: 0.1630, label: 1, bag_size: 3003\n",
      "batch 399, loss: 0.0001, instance_loss: 0.0007, weighted_loss: 0.0003, label: 0, bag_size: 17437\n",
      "batch 419, loss: 0.0023, instance_loss: 0.0284, weighted_loss: 0.0101, label: 1, bag_size: 3453\n",
      "batch 439, loss: 0.0055, instance_loss: 0.1772, weighted_loss: 0.0570, label: 0, bag_size: 1213\n",
      "batch 459, loss: 0.0833, instance_loss: 0.0464, weighted_loss: 0.0722, label: 0, bag_size: 1684\n",
      "batch 479, loss: 0.0075, instance_loss: 0.0558, weighted_loss: 0.0220, label: 1, bag_size: 1459\n",
      "batch 499, loss: 0.0003, instance_loss: 0.0005, weighted_loss: 0.0004, label: 0, bag_size: 27012\n",
      "batch 519, loss: 0.1742, instance_loss: 0.1657, weighted_loss: 0.1716, label: 0, bag_size: 8420\n",
      "batch 539, loss: 0.0002, instance_loss: 0.0104, weighted_loss: 0.0032, label: 0, bag_size: 1984\n",
      "batch 559, loss: 0.0256, instance_loss: 0.0000, weighted_loss: 0.0179, label: 0, bag_size: 14828\n",
      "batch 579, loss: 0.1085, instance_loss: 0.0236, weighted_loss: 0.0831, label: 1, bag_size: 4939\n",
      "batch 599, loss: 0.0511, instance_loss: 0.0099, weighted_loss: 0.0387, label: 1, bag_size: 13692\n",
      "batch 619, loss: 0.0638, instance_loss: 0.0000, weighted_loss: 0.0447, label: 0, bag_size: 2732\n",
      "batch 639, loss: 1.2800, instance_loss: 0.1534, weighted_loss: 0.9420, label: 1, bag_size: 2935\n",
      "batch 659, loss: 0.0757, instance_loss: 0.0103, weighted_loss: 0.0561, label: 1, bag_size: 19972\n",
      "batch 679, loss: 0.0096, instance_loss: 0.0948, weighted_loss: 0.0351, label: 1, bag_size: 3224\n",
      "batch 699, loss: 0.0050, instance_loss: 0.0166, weighted_loss: 0.0085, label: 1, bag_size: 645\n",
      "batch 719, loss: 0.0097, instance_loss: 0.0025, weighted_loss: 0.0075, label: 0, bag_size: 11122\n",
      "batch 739, loss: 0.0062, instance_loss: 0.3244, weighted_loss: 0.1016, label: 0, bag_size: 890\n",
      "batch 759, loss: 0.2639, instance_loss: 2.5320, weighted_loss: 0.9443, label: 0, bag_size: 1760\n",
      "batch 779, loss: 0.0026, instance_loss: 0.0015, weighted_loss: 0.0023, label: 1, bag_size: 7382\n",
      "batch 799, loss: 0.0025, instance_loss: 0.0042, weighted_loss: 0.0030, label: 0, bag_size: 16211\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 20666\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9793187347931873: correct 12880/13152\n",
      "class 1 clustering acc 0.8991788321167883: correct 5913/6576\n",
      "Epoch: 53, train_loss: 0.1693, train_clustering_loss:  0.1977, train_error: 0.0596\n",
      "class 0: acc 0.9323671497584541, correct 386/414\n",
      "class 1: acc 0.9485294117647058, correct 387/408\n",
      "\n",
      "Val Set, val_loss: 0.3972, val_error: 0.0917, auc: 0.9596\n",
      "class 0 clustering acc 0.9564220183486238: correct 1668/1744\n",
      "class 1 clustering acc 0.8348623853211009: correct 728/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.8412698412698413, correct 53/63\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0013, instance_loss: 0.0002, weighted_loss: 0.0010, label: 0, bag_size: 24911\n",
      "batch 39, loss: 0.0056, instance_loss: 0.0000, weighted_loss: 0.0039, label: 1, bag_size: 11032\n",
      "batch 59, loss: 0.1798, instance_loss: 0.0834, weighted_loss: 0.1509, label: 1, bag_size: 1963\n",
      "batch 79, loss: 0.0226, instance_loss: 0.0023, weighted_loss: 0.0165, label: 1, bag_size: 6171\n",
      "batch 99, loss: 3.2755, instance_loss: 1.7866, weighted_loss: 2.8288, label: 0, bag_size: 2694\n",
      "batch 119, loss: 0.0116, instance_loss: 0.8068, weighted_loss: 0.2502, label: 1, bag_size: 14604\n",
      "batch 139, loss: 0.5524, instance_loss: 0.5586, weighted_loss: 0.5542, label: 1, bag_size: 2937\n",
      "batch 159, loss: 0.0133, instance_loss: 0.6252, weighted_loss: 0.1969, label: 0, bag_size: 1416\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11477\n",
      "batch 199, loss: 0.0940, instance_loss: 0.0228, weighted_loss: 0.0727, label: 0, bag_size: 4997\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0092, weighted_loss: 0.0028, label: 1, bag_size: 1412\n",
      "batch 239, loss: 2.8046, instance_loss: 3.3800, weighted_loss: 2.9772, label: 1, bag_size: 15185\n",
      "batch 259, loss: 0.0096, instance_loss: 0.0377, weighted_loss: 0.0180, label: 0, bag_size: 12201\n",
      "batch 279, loss: 0.0009, instance_loss: 0.0058, weighted_loss: 0.0023, label: 0, bag_size: 9485\n",
      "batch 299, loss: 0.0047, instance_loss: 0.0166, weighted_loss: 0.0083, label: 0, bag_size: 9866\n",
      "batch 319, loss: 0.0441, instance_loss: 0.3677, weighted_loss: 0.1412, label: 0, bag_size: 26208\n",
      "batch 339, loss: 0.0031, instance_loss: 0.0086, weighted_loss: 0.0048, label: 1, bag_size: 699\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14266\n",
      "batch 379, loss: 0.0349, instance_loss: 0.0000, weighted_loss: 0.0244, label: 0, bag_size: 10113\n",
      "batch 399, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 22800\n",
      "batch 419, loss: 0.0237, instance_loss: 0.0920, weighted_loss: 0.0442, label: 0, bag_size: 1234\n",
      "batch 439, loss: 1.5416, instance_loss: 0.0380, weighted_loss: 1.0905, label: 1, bag_size: 1819\n",
      "batch 459, loss: 0.1518, instance_loss: 0.0035, weighted_loss: 0.1073, label: 0, bag_size: 2732\n",
      "batch 479, loss: 0.0869, instance_loss: 1.2013, weighted_loss: 0.4213, label: 1, bag_size: 13089\n",
      "batch 499, loss: 0.0106, instance_loss: 0.0000, weighted_loss: 0.0074, label: 1, bag_size: 9877\n",
      "batch 519, loss: 0.0062, instance_loss: 0.5925, weighted_loss: 0.1821, label: 0, bag_size: 890\n",
      "batch 539, loss: 0.0005, instance_loss: 0.0088, weighted_loss: 0.0030, label: 1, bag_size: 2966\n",
      "batch 559, loss: 0.0005, instance_loss: 0.0001, weighted_loss: 0.0003, label: 1, bag_size: 9408\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23996\n",
      "batch 599, loss: 0.0063, instance_loss: 0.0000, weighted_loss: 0.0044, label: 0, bag_size: 9866\n",
      "batch 619, loss: 0.0244, instance_loss: 0.0000, weighted_loss: 0.0171, label: 0, bag_size: 19067\n",
      "batch 639, loss: 0.0397, instance_loss: 0.0106, weighted_loss: 0.0310, label: 1, bag_size: 16514\n",
      "batch 659, loss: 0.1098, instance_loss: 0.0085, weighted_loss: 0.0794, label: 0, bag_size: 7141\n",
      "batch 679, loss: 0.0223, instance_loss: 0.0330, weighted_loss: 0.0255, label: 1, bag_size: 8660\n",
      "batch 699, loss: 0.0003, instance_loss: 0.2508, weighted_loss: 0.0755, label: 1, bag_size: 2405\n",
      "batch 719, loss: 0.0080, instance_loss: 0.0089, weighted_loss: 0.0083, label: 1, bag_size: 5454\n",
      "batch 739, loss: 0.0058, instance_loss: 0.3377, weighted_loss: 0.1054, label: 0, bag_size: 2534\n",
      "batch 759, loss: 0.0730, instance_loss: 0.0916, weighted_loss: 0.0786, label: 1, bag_size: 1444\n",
      "batch 779, loss: 0.0052, instance_loss: 0.0072, weighted_loss: 0.0058, label: 0, bag_size: 2043\n",
      "batch 799, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0015, label: 1, bag_size: 11884\n",
      "batch 819, loss: 0.0330, instance_loss: 0.0030, weighted_loss: 0.0240, label: 0, bag_size: 10415\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9826642335766423: correct 12924/13152\n",
      "class 1 clustering acc 0.9114963503649635: correct 5994/6576\n",
      "Epoch: 54, train_loss: 0.1447, train_clustering_loss:  0.1666, train_error: 0.0547\n",
      "class 0: acc 0.9429280397022333, correct 380/403\n",
      "class 1: acc 0.9474940334128878, correct 397/419\n",
      "\n",
      "Val Set, val_loss: 0.3107, val_error: 0.0826, auc: 0.9651\n",
      "class 0 clustering acc 0.9690366972477065: correct 1690/1744\n",
      "class 1 clustering acc 0.8761467889908257: correct 764/872\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0120, instance_loss: 0.0000, weighted_loss: 0.0084, label: 1, bag_size: 13786\n",
      "batch 39, loss: 1.4241, instance_loss: 0.7838, weighted_loss: 1.2320, label: 0, bag_size: 3375\n",
      "batch 59, loss: 0.0074, instance_loss: 0.0000, weighted_loss: 0.0052, label: 1, bag_size: 12895\n",
      "batch 79, loss: 0.0003, instance_loss: 0.0140, weighted_loss: 0.0044, label: 1, bag_size: 2904\n",
      "batch 99, loss: 0.0153, instance_loss: 0.0082, weighted_loss: 0.0132, label: 0, bag_size: 3502\n",
      "batch 119, loss: 0.0097, instance_loss: 0.0000, weighted_loss: 0.0068, label: 0, bag_size: 24439\n",
      "batch 139, loss: 0.0130, instance_loss: 0.0010, weighted_loss: 0.0094, label: 0, bag_size: 1684\n",
      "batch 159, loss: 0.0003, instance_loss: 0.0030, weighted_loss: 0.0011, label: 1, bag_size: 5731\n",
      "batch 179, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 21864\n",
      "batch 199, loss: 0.0533, instance_loss: 0.0000, weighted_loss: 0.0373, label: 0, bag_size: 5999\n",
      "batch 219, loss: 0.0049, instance_loss: 0.2331, weighted_loss: 0.0734, label: 1, bag_size: 3619\n",
      "batch 239, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 14625\n",
      "batch 259, loss: 0.0118, instance_loss: 0.0000, weighted_loss: 0.0083, label: 0, bag_size: 3876\n",
      "batch 279, loss: 0.0121, instance_loss: 0.0000, weighted_loss: 0.0085, label: 1, bag_size: 8019\n",
      "batch 299, loss: 0.0573, instance_loss: 0.0046, weighted_loss: 0.0415, label: 0, bag_size: 3502\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 7078\n",
      "batch 339, loss: 0.0103, instance_loss: 0.4547, weighted_loss: 0.1436, label: 0, bag_size: 1891\n",
      "batch 359, loss: 0.0018, instance_loss: 0.0000, weighted_loss: 0.0013, label: 0, bag_size: 13691\n",
      "batch 379, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 8522\n",
      "batch 399, loss: 0.2742, instance_loss: 0.1725, weighted_loss: 0.2437, label: 1, bag_size: 3211\n",
      "batch 419, loss: 0.0076, instance_loss: 0.0019, weighted_loss: 0.0059, label: 0, bag_size: 7031\n",
      "batch 439, loss: 0.0340, instance_loss: 0.0074, weighted_loss: 0.0260, label: 0, bag_size: 2148\n",
      "batch 459, loss: 0.0675, instance_loss: 0.0012, weighted_loss: 0.0476, label: 1, bag_size: 5292\n",
      "batch 479, loss: 0.0311, instance_loss: 0.0252, weighted_loss: 0.0294, label: 1, bag_size: 13440\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11195\n",
      "batch 519, loss: 0.0072, instance_loss: 0.0000, weighted_loss: 0.0050, label: 0, bag_size: 25027\n",
      "batch 539, loss: 0.0155, instance_loss: 0.0000, weighted_loss: 0.0108, label: 0, bag_size: 7637\n",
      "batch 559, loss: 0.0527, instance_loss: 0.0036, weighted_loss: 0.0380, label: 1, bag_size: 2356\n",
      "batch 579, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 22828\n",
      "batch 599, loss: 0.0095, instance_loss: 0.0000, weighted_loss: 0.0067, label: 0, bag_size: 19880\n",
      "batch 619, loss: 0.0045, instance_loss: 0.0006, weighted_loss: 0.0033, label: 1, bag_size: 1249\n",
      "batch 639, loss: 0.0626, instance_loss: 0.0000, weighted_loss: 0.0438, label: 1, bag_size: 4330\n",
      "batch 659, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 3228\n",
      "batch 679, loss: 0.0008, instance_loss: 0.0006, weighted_loss: 0.0007, label: 0, bag_size: 16052\n",
      "batch 699, loss: 0.0471, instance_loss: 0.0003, weighted_loss: 0.0330, label: 0, bag_size: 7923\n",
      "batch 719, loss: 0.0228, instance_loss: 0.5517, weighted_loss: 0.1815, label: 1, bag_size: 20767\n",
      "batch 739, loss: 0.0051, instance_loss: 0.0000, weighted_loss: 0.0036, label: 1, bag_size: 16267\n",
      "batch 759, loss: 0.0034, instance_loss: 0.0000, weighted_loss: 0.0024, label: 0, bag_size: 13964\n",
      "batch 779, loss: 0.1417, instance_loss: 4.4766, weighted_loss: 1.4422, label: 0, bag_size: 2213\n",
      "batch 799, loss: 0.0013, instance_loss: 0.0240, weighted_loss: 0.0081, label: 1, bag_size: 10105\n",
      "batch 819, loss: 0.0483, instance_loss: 0.0179, weighted_loss: 0.0392, label: 1, bag_size: 4880\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9867700729927007: correct 12978/13152\n",
      "class 1 clustering acc 0.9368917274939172: correct 6161/6576\n",
      "Epoch: 55, train_loss: 0.1444, train_clustering_loss:  0.1266, train_error: 0.0620\n",
      "class 0: acc 0.9318734793187348, correct 383/411\n",
      "class 1: acc 0.9440389294403893, correct 388/411\n",
      "\n",
      "Val Set, val_loss: 0.5611, val_error: 0.1468, auc: 0.9631\n",
      "class 0 clustering acc 0.9317660550458715: correct 1625/1744\n",
      "class 1 clustering acc 0.7912844036697247: correct 690/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.746031746031746, correct 47/63\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0011, instance_loss: 0.0010, weighted_loss: 0.0010, label: 0, bag_size: 24439\n",
      "batch 39, loss: 0.0065, instance_loss: 0.0962, weighted_loss: 0.0334, label: 1, bag_size: 1339\n",
      "batch 59, loss: 0.2028, instance_loss: 0.0226, weighted_loss: 0.1487, label: 1, bag_size: 16514\n",
      "batch 79, loss: 0.0007, instance_loss: 0.1428, weighted_loss: 0.0433, label: 0, bag_size: 3101\n",
      "batch 99, loss: 2.4133, instance_loss: 0.0890, weighted_loss: 1.7160, label: 1, bag_size: 9215\n",
      "batch 119, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 11727\n",
      "batch 139, loss: 0.0172, instance_loss: 0.0000, weighted_loss: 0.0120, label: 0, bag_size: 10113\n",
      "batch 159, loss: 0.0035, instance_loss: 0.0000, weighted_loss: 0.0024, label: 1, bag_size: 10501\n",
      "batch 179, loss: 0.6944, instance_loss: 0.2915, weighted_loss: 0.5735, label: 0, bag_size: 9421\n",
      "batch 199, loss: 0.1664, instance_loss: 0.0384, weighted_loss: 0.1280, label: 1, bag_size: 7932\n",
      "batch 219, loss: 0.0259, instance_loss: 0.0629, weighted_loss: 0.0370, label: 0, bag_size: 1797\n",
      "batch 239, loss: 0.0048, instance_loss: 0.0013, weighted_loss: 0.0037, label: 1, bag_size: 12946\n",
      "batch 259, loss: 0.0070, instance_loss: 0.0000, weighted_loss: 0.0049, label: 1, bag_size: 14887\n",
      "batch 279, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 7235\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11642\n",
      "batch 319, loss: 0.1068, instance_loss: 0.0002, weighted_loss: 0.0748, label: 0, bag_size: 18738\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 7191\n",
      "batch 359, loss: 0.0234, instance_loss: 0.0390, weighted_loss: 0.0280, label: 0, bag_size: 18954\n",
      "batch 379, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 14681\n",
      "batch 399, loss: 0.0014, instance_loss: 0.1345, weighted_loss: 0.0413, label: 0, bag_size: 1909\n",
      "batch 419, loss: 0.3209, instance_loss: 0.0162, weighted_loss: 0.2295, label: 1, bag_size: 7066\n",
      "batch 439, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 4959\n",
      "batch 459, loss: 0.0006, instance_loss: 0.0018, weighted_loss: 0.0010, label: 1, bag_size: 20333\n",
      "batch 479, loss: 0.0026, instance_loss: 0.0000, weighted_loss: 0.0018, label: 1, bag_size: 9408\n",
      "batch 499, loss: 0.0271, instance_loss: 0.0578, weighted_loss: 0.0363, label: 0, bag_size: 2242\n",
      "batch 519, loss: 0.0611, instance_loss: 0.0063, weighted_loss: 0.0446, label: 1, bag_size: 9230\n",
      "batch 539, loss: 0.0188, instance_loss: 0.0000, weighted_loss: 0.0132, label: 0, bag_size: 3710\n",
      "batch 559, loss: 0.2618, instance_loss: 0.0191, weighted_loss: 0.1890, label: 0, bag_size: 15057\n",
      "batch 579, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 3003\n",
      "batch 599, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 14956\n",
      "batch 619, loss: 0.0543, instance_loss: 0.0027, weighted_loss: 0.0388, label: 1, bag_size: 6682\n",
      "batch 639, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 3541\n",
      "batch 659, loss: 0.0110, instance_loss: 0.0010, weighted_loss: 0.0080, label: 0, bag_size: 9471\n",
      "batch 679, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 15008\n",
      "batch 699, loss: 0.0210, instance_loss: 0.0029, weighted_loss: 0.0156, label: 1, bag_size: 13015\n",
      "batch 719, loss: 0.0358, instance_loss: 0.0000, weighted_loss: 0.0250, label: 0, bag_size: 11122\n",
      "batch 739, loss: 0.0166, instance_loss: 0.0182, weighted_loss: 0.0171, label: 1, bag_size: 3211\n",
      "batch 759, loss: 0.0019, instance_loss: 0.0000, weighted_loss: 0.0013, label: 0, bag_size: 25027\n",
      "batch 779, loss: 0.1278, instance_loss: 0.1756, weighted_loss: 0.1421, label: 1, bag_size: 4956\n",
      "batch 799, loss: 0.0377, instance_loss: 0.0000, weighted_loss: 0.0264, label: 0, bag_size: 14885\n",
      "batch 819, loss: 0.2313, instance_loss: 0.2571, weighted_loss: 0.2390, label: 1, bag_size: 11220\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9809154501216545: correct 12901/13152\n",
      "class 1 clustering acc 0.9037408759124088: correct 5943/6576\n",
      "Epoch: 56, train_loss: 0.1650, train_clustering_loss:  0.1815, train_error: 0.0693\n",
      "class 0: acc 0.9130434782608695, correct 357/391\n",
      "class 1: acc 0.9466357308584686, correct 408/431\n",
      "\n",
      "Val Set, val_loss: 0.3094, val_error: 0.0917, auc: 0.9634\n",
      "class 0 clustering acc 0.963302752293578: correct 1680/1744\n",
      "class 1 clustering acc 0.7270642201834863: correct 634/872\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0048, instance_loss: 0.0002, weighted_loss: 0.0035, label: 1, bag_size: 21009\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 4715\n",
      "batch 59, loss: 0.0025, instance_loss: 0.0076, weighted_loss: 0.0040, label: 1, bag_size: 10725\n",
      "batch 79, loss: 0.4795, instance_loss: 0.0111, weighted_loss: 0.3390, label: 1, bag_size: 10432\n",
      "batch 99, loss: 0.0025, instance_loss: 0.0507, weighted_loss: 0.0169, label: 1, bag_size: 2381\n",
      "batch 119, loss: 0.0007, instance_loss: 0.0142, weighted_loss: 0.0047, label: 1, bag_size: 3437\n",
      "batch 139, loss: 0.0002, instance_loss: 0.0002, weighted_loss: 0.0002, label: 0, bag_size: 16720\n",
      "batch 159, loss: 0.0965, instance_loss: 0.1021, weighted_loss: 0.0982, label: 0, bag_size: 9616\n",
      "batch 179, loss: 0.0096, instance_loss: 1.4867, weighted_loss: 0.4527, label: 1, bag_size: 10072\n",
      "batch 199, loss: 0.1440, instance_loss: 0.0683, weighted_loss: 0.1213, label: 1, bag_size: 1786\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19390\n",
      "batch 239, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 12593\n",
      "batch 259, loss: 0.0241, instance_loss: 0.0134, weighted_loss: 0.0209, label: 0, bag_size: 1651\n",
      "batch 279, loss: 0.0020, instance_loss: 0.0022, weighted_loss: 0.0021, label: 0, bag_size: 6898\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 0, bag_size: 10898\n",
      "batch 319, loss: 0.0072, instance_loss: 0.0880, weighted_loss: 0.0314, label: 0, bag_size: 1797\n",
      "batch 339, loss: 0.0809, instance_loss: 0.0127, weighted_loss: 0.0604, label: 0, bag_size: 11922\n",
      "batch 359, loss: 0.0051, instance_loss: 0.0000, weighted_loss: 0.0035, label: 1, bag_size: 12697\n",
      "batch 379, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 25027\n",
      "batch 399, loss: 0.0016, instance_loss: 0.0004, weighted_loss: 0.0013, label: 1, bag_size: 4317\n",
      "batch 419, loss: 0.0028, instance_loss: 0.0000, weighted_loss: 0.0020, label: 1, bag_size: 9877\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 6752\n",
      "batch 459, loss: 0.0464, instance_loss: 0.1259, weighted_loss: 0.0702, label: 0, bag_size: 2518\n",
      "batch 479, loss: 0.1002, instance_loss: 0.0445, weighted_loss: 0.0835, label: 1, bag_size: 16379\n",
      "batch 499, loss: 0.0109, instance_loss: 1.6992, weighted_loss: 0.5174, label: 1, bag_size: 20767\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0012, weighted_loss: 0.0005, label: 1, bag_size: 5561\n",
      "batch 539, loss: 0.0319, instance_loss: 0.0000, weighted_loss: 0.0223, label: 1, bag_size: 6731\n",
      "batch 559, loss: 0.0961, instance_loss: 0.0080, weighted_loss: 0.0697, label: 1, bag_size: 7424\n",
      "batch 579, loss: 0.2812, instance_loss: 0.0056, weighted_loss: 0.1985, label: 0, bag_size: 7141\n",
      "batch 599, loss: 0.0024, instance_loss: 0.1878, weighted_loss: 0.0580, label: 0, bag_size: 3198\n",
      "batch 619, loss: 0.0029, instance_loss: 0.1144, weighted_loss: 0.0364, label: 0, bag_size: 10415\n",
      "batch 639, loss: 0.0058, instance_loss: 0.0000, weighted_loss: 0.0041, label: 1, bag_size: 14230\n",
      "batch 659, loss: 0.0006, instance_loss: 0.0652, weighted_loss: 0.0200, label: 0, bag_size: 11113\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 14206\n",
      "batch 699, loss: 0.0187, instance_loss: 0.4317, weighted_loss: 0.1426, label: 1, bag_size: 2785\n",
      "batch 719, loss: 0.0158, instance_loss: 0.1411, weighted_loss: 0.0534, label: 0, bag_size: 10490\n",
      "batch 739, loss: 0.0048, instance_loss: 0.0666, weighted_loss: 0.0234, label: 1, bag_size: 5894\n",
      "batch 759, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 10105\n",
      "batch 779, loss: 0.0029, instance_loss: 0.0155, weighted_loss: 0.0067, label: 1, bag_size: 12758\n",
      "batch 799, loss: 0.0016, instance_loss: 0.7269, weighted_loss: 0.2192, label: 0, bag_size: 2820\n",
      "batch 819, loss: 0.0051, instance_loss: 0.0000, weighted_loss: 0.0035, label: 1, bag_size: 13786\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9815237226277372: correct 12909/13152\n",
      "class 1 clustering acc 0.8973540145985401: correct 5901/6576\n",
      "Epoch: 57, train_loss: 0.1015, train_clustering_loss:  0.1848, train_error: 0.0353\n",
      "class 0: acc 0.9454545454545454, correct 364/385\n",
      "class 1: acc 0.9816933638443935, correct 429/437\n",
      "\n",
      "Val Set, val_loss: 0.2830, val_error: 0.1009, auc: 0.9641\n",
      "class 0 clustering acc 0.970756880733945: correct 1693/1744\n",
      "class 1 clustering acc 0.8337155963302753: correct 727/872\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "Validation loss decreased (0.285709 --> 0.283005).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6210, instance_loss: 0.7498, weighted_loss: 0.6596, label: 0, bag_size: 2959\n",
      "batch 39, loss: 0.0131, instance_loss: 0.5016, weighted_loss: 0.1597, label: 1, bag_size: 8019\n",
      "batch 59, loss: 0.0002, instance_loss: 0.0269, weighted_loss: 0.0082, label: 1, bag_size: 617\n",
      "batch 79, loss: 0.5192, instance_loss: 0.0070, weighted_loss: 0.3656, label: 0, bag_size: 15898\n",
      "batch 99, loss: 0.1561, instance_loss: 0.0213, weighted_loss: 0.1156, label: 1, bag_size: 1888\n",
      "batch 119, loss: 0.0007, instance_loss: 0.0039, weighted_loss: 0.0017, label: 0, bag_size: 16341\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11865\n",
      "batch 159, loss: 0.0254, instance_loss: 0.0000, weighted_loss: 0.0177, label: 1, bag_size: 7583\n",
      "batch 179, loss: 0.1271, instance_loss: 0.6012, weighted_loss: 0.2693, label: 0, bag_size: 8420\n",
      "batch 199, loss: 0.0044, instance_loss: 0.0000, weighted_loss: 0.0031, label: 1, bag_size: 9877\n",
      "batch 219, loss: 1.3222, instance_loss: 2.2393, weighted_loss: 1.5973, label: 0, bag_size: 11128\n",
      "batch 239, loss: 0.9058, instance_loss: 0.0834, weighted_loss: 0.6591, label: 0, bag_size: 23618\n",
      "batch 259, loss: 0.0217, instance_loss: 0.5317, weighted_loss: 0.1747, label: 0, bag_size: 1772\n",
      "batch 279, loss: 0.0003, instance_loss: 0.0612, weighted_loss: 0.0186, label: 0, bag_size: 18045\n",
      "batch 299, loss: 0.0007, instance_loss: 0.0006, weighted_loss: 0.0007, label: 0, bag_size: 14625\n",
      "batch 319, loss: 0.0023, instance_loss: 0.0929, weighted_loss: 0.0295, label: 0, bag_size: 2548\n",
      "batch 339, loss: 0.0040, instance_loss: 0.0000, weighted_loss: 0.0028, label: 1, bag_size: 6090\n",
      "batch 359, loss: 0.0125, instance_loss: 0.1724, weighted_loss: 0.0605, label: 1, bag_size: 7246\n",
      "batch 379, loss: 0.0199, instance_loss: 0.0000, weighted_loss: 0.0139, label: 1, bag_size: 13732\n",
      "batch 399, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 2936\n",
      "batch 419, loss: 0.0288, instance_loss: 0.0000, weighted_loss: 0.0202, label: 1, bag_size: 5345\n",
      "batch 439, loss: 0.0883, instance_loss: 0.0000, weighted_loss: 0.0618, label: 1, bag_size: 12575\n",
      "batch 459, loss: 0.0138, instance_loss: 0.0747, weighted_loss: 0.0321, label: 0, bag_size: 2609\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 4465\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19472\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 12349\n",
      "batch 539, loss: 0.0137, instance_loss: 0.1618, weighted_loss: 0.0581, label: 0, bag_size: 1370\n",
      "batch 559, loss: 1.9935, instance_loss: 3.7340, weighted_loss: 2.5157, label: 0, bag_size: 2732\n",
      "batch 579, loss: 0.0260, instance_loss: 0.2318, weighted_loss: 0.0877, label: 1, bag_size: 4054\n",
      "batch 599, loss: 0.0129, instance_loss: 0.0000, weighted_loss: 0.0090, label: 1, bag_size: 14030\n",
      "batch 619, loss: 0.0003, instance_loss: 0.0010, weighted_loss: 0.0005, label: 0, bag_size: 2179\n",
      "batch 639, loss: 0.3622, instance_loss: 0.1559, weighted_loss: 0.3003, label: 1, bag_size: 1683\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0005, label: 1, bag_size: 4102\n",
      "batch 679, loss: 0.0096, instance_loss: 0.3062, weighted_loss: 0.0986, label: 0, bag_size: 1052\n",
      "batch 699, loss: 0.8080, instance_loss: 0.0099, weighted_loss: 0.5686, label: 0, bag_size: 2219\n",
      "batch 719, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 2044\n",
      "batch 739, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 4394\n",
      "batch 759, loss: 0.3033, instance_loss: 0.0000, weighted_loss: 0.2123, label: 1, bag_size: 9561\n",
      "batch 779, loss: 0.0076, instance_loss: 0.8301, weighted_loss: 0.2544, label: 1, bag_size: 10396\n",
      "batch 799, loss: 0.0106, instance_loss: 0.0237, weighted_loss: 0.0145, label: 1, bag_size: 2344\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0325, weighted_loss: 0.0098, label: 1, bag_size: 9759\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9815997566909975: correct 12910/13152\n",
      "class 1 clustering acc 0.8949209245742092: correct 5885/6576\n",
      "Epoch: 58, train_loss: 0.1562, train_clustering_loss:  0.1817, train_error: 0.0596\n",
      "class 0: acc 0.9323671497584541, correct 386/414\n",
      "class 1: acc 0.9485294117647058, correct 387/408\n",
      "\n",
      "Val Set, val_loss: 0.2743, val_error: 0.1101, auc: 0.9658\n",
      "class 0 clustering acc 0.9512614678899083: correct 1659/1744\n",
      "class 1 clustering acc 0.8038990825688074: correct 701/872\n",
      "class 0: acc 0.9130434782608695, correct 42/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "Validation loss decreased (0.283005 --> 0.274336).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0568, instance_loss: 0.0000, weighted_loss: 0.0398, label: 1, bag_size: 13732\n",
      "batch 39, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 8661\n",
      "batch 59, loss: 0.0017, instance_loss: 0.0009, weighted_loss: 0.0014, label: 0, bag_size: 6898\n",
      "batch 79, loss: 0.0046, instance_loss: 0.0000, weighted_loss: 0.0032, label: 0, bag_size: 9485\n",
      "batch 99, loss: 0.1539, instance_loss: 0.2791, weighted_loss: 0.1915, label: 0, bag_size: 2266\n",
      "batch 119, loss: 0.0061, instance_loss: 0.2133, weighted_loss: 0.0683, label: 0, bag_size: 763\n",
      "batch 139, loss: 0.0096, instance_loss: 0.0002, weighted_loss: 0.0068, label: 0, bag_size: 12510\n",
      "batch 159, loss: 0.2199, instance_loss: 0.5633, weighted_loss: 0.3229, label: 1, bag_size: 2314\n",
      "batch 179, loss: 0.1148, instance_loss: 0.0408, weighted_loss: 0.0926, label: 1, bag_size: 8026\n",
      "batch 199, loss: 0.0327, instance_loss: 0.1276, weighted_loss: 0.0611, label: 1, bag_size: 11394\n",
      "batch 219, loss: 0.0432, instance_loss: 0.0454, weighted_loss: 0.0439, label: 1, bag_size: 7873\n",
      "batch 239, loss: 0.0312, instance_loss: 0.0228, weighted_loss: 0.0287, label: 1, bag_size: 2356\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21082\n",
      "batch 279, loss: 0.8760, instance_loss: 0.1936, weighted_loss: 0.6712, label: 1, bag_size: 1703\n",
      "batch 299, loss: 0.2874, instance_loss: 0.0964, weighted_loss: 0.2301, label: 1, bag_size: 1038\n",
      "batch 319, loss: 0.0017, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 24911\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0005, weighted_loss: 0.0002, label: 0, bag_size: 12217\n",
      "batch 359, loss: 0.0060, instance_loss: 0.1714, weighted_loss: 0.0556, label: 0, bag_size: 890\n",
      "batch 379, loss: 0.0007, instance_loss: 0.0049, weighted_loss: 0.0020, label: 1, bag_size: 4880\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11122\n",
      "batch 419, loss: 0.0094, instance_loss: 0.4720, weighted_loss: 0.1482, label: 1, bag_size: 9330\n",
      "batch 439, loss: 0.4501, instance_loss: 0.0397, weighted_loss: 0.3270, label: 1, bag_size: 1284\n",
      "batch 459, loss: 0.0024, instance_loss: 0.1507, weighted_loss: 0.0469, label: 1, bag_size: 6343\n",
      "batch 479, loss: 0.0205, instance_loss: 0.0203, weighted_loss: 0.0204, label: 1, bag_size: 5110\n",
      "batch 499, loss: 0.3710, instance_loss: 0.0055, weighted_loss: 0.2614, label: 1, bag_size: 8395\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 20666\n",
      "batch 539, loss: 0.0206, instance_loss: 0.0726, weighted_loss: 0.0362, label: 0, bag_size: 1415\n",
      "batch 559, loss: 0.0111, instance_loss: 0.0042, weighted_loss: 0.0090, label: 0, bag_size: 2063\n",
      "batch 579, loss: 0.0193, instance_loss: 0.4823, weighted_loss: 0.1582, label: 1, bag_size: 12946\n",
      "batch 599, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 15332\n",
      "batch 619, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0015, label: 0, bag_size: 23791\n",
      "batch 639, loss: 0.0015, instance_loss: 1.0378, weighted_loss: 0.3124, label: 0, bag_size: 1825\n",
      "batch 659, loss: 0.0125, instance_loss: 0.5377, weighted_loss: 0.1700, label: 1, bag_size: 1999\n",
      "batch 679, loss: 0.0468, instance_loss: 0.1456, weighted_loss: 0.0764, label: 1, bag_size: 10848\n",
      "batch 699, loss: 0.4981, instance_loss: 0.0546, weighted_loss: 0.3650, label: 1, bag_size: 12712\n",
      "batch 719, loss: 0.0144, instance_loss: 0.0342, weighted_loss: 0.0203, label: 1, bag_size: 8438\n",
      "batch 739, loss: 0.0184, instance_loss: 0.0001, weighted_loss: 0.0129, label: 1, bag_size: 5605\n",
      "batch 759, loss: 1.1298, instance_loss: 0.0656, weighted_loss: 0.8106, label: 1, bag_size: 2565\n",
      "batch 779, loss: 0.0166, instance_loss: 0.0039, weighted_loss: 0.0128, label: 0, bag_size: 3725\n",
      "batch 799, loss: 0.0061, instance_loss: 0.0000, weighted_loss: 0.0043, label: 1, bag_size: 16051\n",
      "batch 819, loss: 0.0001, instance_loss: 0.0799, weighted_loss: 0.0240, label: 1, bag_size: 3295\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9835006082725061: correct 12935/13152\n",
      "class 1 clustering acc 0.9125608272506083: correct 6001/6576\n",
      "Epoch: 59, train_loss: 0.1337, train_clustering_loss:  0.1597, train_error: 0.0535\n",
      "class 0: acc 0.94, correct 376/400\n",
      "class 1: acc 0.95260663507109, correct 402/422\n",
      "\n",
      "Val Set, val_loss: 0.5043, val_error: 0.1284, auc: 0.9658\n",
      "class 0 clustering acc 0.9346330275229358: correct 1630/1744\n",
      "class 1 clustering acc 0.801605504587156: correct 699/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.7777777777777778, correct 49/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1743, instance_loss: 0.2611, weighted_loss: 0.2003, label: 1, bag_size: 2937\n",
      "batch 39, loss: 0.0225, instance_loss: 0.0438, weighted_loss: 0.0289, label: 1, bag_size: 1525\n",
      "batch 59, loss: 0.0326, instance_loss: 0.0889, weighted_loss: 0.0495, label: 1, bag_size: 5256\n",
      "batch 79, loss: 0.0073, instance_loss: 0.0002, weighted_loss: 0.0052, label: 1, bag_size: 7382\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23398\n",
      "batch 119, loss: 0.0088, instance_loss: 0.0000, weighted_loss: 0.0062, label: 0, bag_size: 18215\n",
      "batch 139, loss: 0.0500, instance_loss: 0.0062, weighted_loss: 0.0368, label: 1, bag_size: 6927\n",
      "batch 159, loss: 0.0008, instance_loss: 0.3482, weighted_loss: 0.1050, label: 0, bag_size: 1825\n",
      "batch 179, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 5551\n",
      "batch 199, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 10146\n",
      "batch 219, loss: 0.0145, instance_loss: 0.0022, weighted_loss: 0.0108, label: 0, bag_size: 12083\n",
      "batch 239, loss: 0.0179, instance_loss: 0.0000, weighted_loss: 0.0125, label: 1, bag_size: 8680\n",
      "batch 259, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 23714\n",
      "batch 279, loss: 0.4981, instance_loss: 0.0134, weighted_loss: 0.3527, label: 1, bag_size: 12712\n",
      "batch 299, loss: 0.1158, instance_loss: 0.3053, weighted_loss: 0.1726, label: 1, bag_size: 7468\n",
      "batch 319, loss: 0.0000, instance_loss: 0.4349, weighted_loss: 0.1305, label: 0, bag_size: 518\n",
      "batch 339, loss: 0.0041, instance_loss: 0.1900, weighted_loss: 0.0598, label: 0, bag_size: 3557\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 31780\n",
      "batch 379, loss: 0.0519, instance_loss: 0.0010, weighted_loss: 0.0366, label: 0, bag_size: 3710\n",
      "batch 399, loss: 1.2593, instance_loss: 0.3311, weighted_loss: 0.9809, label: 0, bag_size: 2918\n",
      "batch 419, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 7235\n",
      "batch 439, loss: 0.0030, instance_loss: 0.0245, weighted_loss: 0.0094, label: 0, bag_size: 20555\n",
      "batch 459, loss: 0.0005, instance_loss: 0.0286, weighted_loss: 0.0089, label: 0, bag_size: 2091\n",
      "batch 479, loss: 0.0532, instance_loss: 0.8828, weighted_loss: 0.3021, label: 1, bag_size: 5921\n",
      "batch 499, loss: 0.0004, instance_loss: 0.0004, weighted_loss: 0.0004, label: 0, bag_size: 2920\n",
      "batch 519, loss: 0.0125, instance_loss: 0.0769, weighted_loss: 0.0318, label: 1, bag_size: 11032\n",
      "batch 539, loss: 0.0009, instance_loss: 0.0958, weighted_loss: 0.0294, label: 0, bag_size: 19808\n",
      "batch 559, loss: 0.0022, instance_loss: 0.0073, weighted_loss: 0.0037, label: 1, bag_size: 7515\n",
      "batch 579, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 9470\n",
      "batch 599, loss: 0.0302, instance_loss: 0.1417, weighted_loss: 0.0637, label: 0, bag_size: 9596\n",
      "batch 619, loss: 0.0029, instance_loss: 0.0121, weighted_loss: 0.0057, label: 0, bag_size: 2322\n",
      "batch 639, loss: 0.0177, instance_loss: 0.0002, weighted_loss: 0.0125, label: 0, bag_size: 7989\n",
      "batch 659, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 2036\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8948\n",
      "batch 699, loss: 0.0021, instance_loss: 0.0002, weighted_loss: 0.0015, label: 1, bag_size: 4880\n",
      "batch 719, loss: 0.0015, instance_loss: 0.0006, weighted_loss: 0.0013, label: 0, bag_size: 2652\n",
      "batch 739, loss: 0.1796, instance_loss: 0.3010, weighted_loss: 0.2160, label: 1, bag_size: 1191\n",
      "batch 759, loss: 0.0344, instance_loss: 0.1898, weighted_loss: 0.0810, label: 1, bag_size: 7424\n",
      "batch 779, loss: 0.0074, instance_loss: 0.0181, weighted_loss: 0.0106, label: 1, bag_size: 12095\n",
      "batch 799, loss: 0.0607, instance_loss: 0.6864, weighted_loss: 0.2484, label: 1, bag_size: 7981\n",
      "batch 819, loss: 0.0001, instance_loss: 0.0342, weighted_loss: 0.0103, label: 1, bag_size: 3634\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9812956204379562: correct 12906/13152\n",
      "class 1 clustering acc 0.8943126520681265: correct 5881/6576\n",
      "Epoch: 60, train_loss: 0.1417, train_clustering_loss:  0.2010, train_error: 0.0596\n",
      "class 0: acc 0.9427207637231504, correct 395/419\n",
      "class 1: acc 0.9379652605459057, correct 378/403\n",
      "\n",
      "Val Set, val_loss: 0.2696, val_error: 0.1009, auc: 0.9672\n",
      "class 0 clustering acc 0.9673165137614679: correct 1687/1744\n",
      "class 1 clustering acc 0.8486238532110092: correct 740/872\n",
      "class 0: acc 0.9130434782608695, correct 42/46\n",
      "class 1: acc 0.8888888888888888, correct 56/63\n",
      "Validation loss decreased (0.274336 --> 0.269592).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 10444\n",
      "batch 39, loss: 0.0006, instance_loss: 0.0008, weighted_loss: 0.0007, label: 0, bag_size: 8582\n",
      "batch 59, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 30675\n",
      "batch 79, loss: 0.0333, instance_loss: 0.0024, weighted_loss: 0.0241, label: 1, bag_size: 12178\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19390\n",
      "batch 119, loss: 0.0032, instance_loss: 0.2434, weighted_loss: 0.0753, label: 0, bag_size: 2457\n",
      "batch 139, loss: 0.0714, instance_loss: 0.0116, weighted_loss: 0.0535, label: 1, bag_size: 7066\n",
      "batch 159, loss: 0.0027, instance_loss: 0.1313, weighted_loss: 0.0413, label: 0, bag_size: 14377\n",
      "batch 179, loss: 0.0005, instance_loss: 0.7399, weighted_loss: 0.2223, label: 1, bag_size: 3651\n",
      "batch 199, loss: 0.0000, instance_loss: 0.1148, weighted_loss: 0.0344, label: 0, bag_size: 1984\n",
      "batch 219, loss: 0.0023, instance_loss: 0.0625, weighted_loss: 0.0204, label: 1, bag_size: 2559\n",
      "batch 239, loss: 0.0001, instance_loss: 0.0021, weighted_loss: 0.0007, label: 0, bag_size: 17791\n",
      "batch 259, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 21076\n",
      "batch 279, loss: 0.0058, instance_loss: 0.0099, weighted_loss: 0.0070, label: 1, bag_size: 4880\n",
      "batch 299, loss: 0.0882, instance_loss: 0.0317, weighted_loss: 0.0713, label: 1, bag_size: 8438\n",
      "batch 319, loss: 0.2609, instance_loss: 0.0897, weighted_loss: 0.2095, label: 1, bag_size: 2935\n",
      "batch 339, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 2873\n",
      "batch 359, loss: 0.2748, instance_loss: 0.0309, weighted_loss: 0.2016, label: 0, bag_size: 11151\n",
      "batch 379, loss: 0.4399, instance_loss: 0.0017, weighted_loss: 0.3084, label: 1, bag_size: 10591\n",
      "batch 399, loss: 0.0053, instance_loss: 0.1096, weighted_loss: 0.0366, label: 0, bag_size: 10721\n",
      "batch 419, loss: 0.3245, instance_loss: 0.1486, weighted_loss: 0.2717, label: 0, bag_size: 13619\n",
      "batch 439, loss: 0.0042, instance_loss: 0.0410, weighted_loss: 0.0152, label: 0, bag_size: 2063\n",
      "batch 459, loss: 0.0197, instance_loss: 0.1710, weighted_loss: 0.0651, label: 1, bag_size: 2559\n",
      "batch 479, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 11125\n",
      "batch 499, loss: 0.0219, instance_loss: 0.0000, weighted_loss: 0.0153, label: 0, bag_size: 18738\n",
      "batch 519, loss: 0.0011, instance_loss: 0.0027, weighted_loss: 0.0016, label: 0, bag_size: 13691\n",
      "batch 539, loss: 0.0266, instance_loss: 0.1716, weighted_loss: 0.0701, label: 1, bag_size: 8191\n",
      "batch 559, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 9949\n",
      "batch 579, loss: 2.7368, instance_loss: 3.6285, weighted_loss: 3.0043, label: 1, bag_size: 13367\n",
      "batch 599, loss: 0.0003, instance_loss: 0.0007, weighted_loss: 0.0004, label: 1, bag_size: 12931\n",
      "batch 619, loss: 0.0018, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 14681\n",
      "batch 639, loss: 0.0134, instance_loss: 0.0289, weighted_loss: 0.0180, label: 0, bag_size: 8549\n",
      "batch 659, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 13051\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19472\n",
      "batch 699, loss: 0.0043, instance_loss: 0.0032, weighted_loss: 0.0039, label: 1, bag_size: 5894\n",
      "batch 719, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 21076\n",
      "batch 739, loss: 0.6633, instance_loss: 0.6126, weighted_loss: 0.6481, label: 1, bag_size: 3121\n",
      "batch 759, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 14956\n",
      "batch 779, loss: 0.5745, instance_loss: 0.1080, weighted_loss: 0.4346, label: 1, bag_size: 1764\n",
      "batch 799, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 12687\n",
      "batch 819, loss: 0.0659, instance_loss: 0.0360, weighted_loss: 0.0570, label: 0, bag_size: 12510\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9824361313868614: correct 12921/13152\n",
      "class 1 clustering acc 0.895529197080292: correct 5889/6576\n",
      "Epoch: 61, train_loss: 0.1265, train_clustering_loss:  0.1803, train_error: 0.0462\n",
      "class 0: acc 0.9437652811735942, correct 386/409\n",
      "class 1: acc 0.9636803874092009, correct 398/413\n",
      "\n",
      "Val Set, val_loss: 0.3121, val_error: 0.0826, auc: 0.9655\n",
      "class 0 clustering acc 0.9627293577981652: correct 1679/1744\n",
      "class 1 clustering acc 0.8772935779816514: correct 765/872\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 4271\n",
      "batch 39, loss: 0.0991, instance_loss: 0.4787, weighted_loss: 0.2130, label: 0, bag_size: 3670\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 16782\n",
      "batch 79, loss: 0.0340, instance_loss: 0.0012, weighted_loss: 0.0242, label: 1, bag_size: 4239\n",
      "batch 99, loss: 0.0086, instance_loss: 0.1423, weighted_loss: 0.0487, label: 0, bag_size: 2628\n",
      "batch 119, loss: 0.0063, instance_loss: 0.1143, weighted_loss: 0.0387, label: 1, bag_size: 5864\n",
      "batch 139, loss: 0.0002, instance_loss: 0.2248, weighted_loss: 0.0676, label: 0, bag_size: 2548\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0104, weighted_loss: 0.0031, label: 0, bag_size: 16087\n",
      "batch 179, loss: 0.0274, instance_loss: 1.0137, weighted_loss: 0.3233, label: 1, bag_size: 1999\n",
      "batch 199, loss: 0.0010, instance_loss: 0.1434, weighted_loss: 0.0437, label: 0, bag_size: 705\n",
      "batch 219, loss: 0.0127, instance_loss: 1.0205, weighted_loss: 0.3150, label: 1, bag_size: 3450\n",
      "batch 239, loss: 0.1452, instance_loss: 0.1875, weighted_loss: 0.1579, label: 0, bag_size: 24382\n",
      "batch 259, loss: 0.0022, instance_loss: 0.0834, weighted_loss: 0.0265, label: 1, bag_size: 7935\n",
      "batch 279, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 9971\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 6752\n",
      "batch 319, loss: 0.0000, instance_loss: 0.1458, weighted_loss: 0.0438, label: 1, bag_size: 10112\n",
      "batch 339, loss: 0.0027, instance_loss: 0.0000, weighted_loss: 0.0019, label: 0, bag_size: 7235\n",
      "batch 359, loss: 0.0544, instance_loss: 0.1479, weighted_loss: 0.0825, label: 1, bag_size: 7186\n",
      "batch 379, loss: 0.0576, instance_loss: 0.0000, weighted_loss: 0.0403, label: 0, bag_size: 10415\n",
      "batch 399, loss: 2.0971, instance_loss: 3.8598, weighted_loss: 2.6259, label: 1, bag_size: 15185\n",
      "batch 419, loss: 0.0005, instance_loss: 0.0031, weighted_loss: 0.0013, label: 1, bag_size: 10392\n",
      "batch 439, loss: 0.1048, instance_loss: 0.4478, weighted_loss: 0.2077, label: 1, bag_size: 12460\n",
      "batch 459, loss: 0.0023, instance_loss: 0.0001, weighted_loss: 0.0017, label: 1, bag_size: 8602\n",
      "batch 479, loss: 0.0008, instance_loss: 0.0189, weighted_loss: 0.0063, label: 1, bag_size: 3453\n",
      "batch 499, loss: 0.0004, instance_loss: 0.0118, weighted_loss: 0.0038, label: 1, bag_size: 14433\n",
      "batch 519, loss: 0.0107, instance_loss: 0.0324, weighted_loss: 0.0172, label: 1, bag_size: 20767\n",
      "batch 539, loss: 0.0015, instance_loss: 0.1719, weighted_loss: 0.0526, label: 1, bag_size: 6928\n",
      "batch 559, loss: 0.0355, instance_loss: 0.1067, weighted_loss: 0.0569, label: 0, bag_size: 8549\n",
      "batch 579, loss: 0.0051, instance_loss: 0.0000, weighted_loss: 0.0036, label: 1, bag_size: 16267\n",
      "batch 599, loss: 0.0977, instance_loss: 0.2567, weighted_loss: 0.1454, label: 1, bag_size: 8191\n",
      "batch 619, loss: 0.0164, instance_loss: 0.0814, weighted_loss: 0.0359, label: 1, bag_size: 5256\n",
      "batch 639, loss: 0.0025, instance_loss: 0.7487, weighted_loss: 0.2264, label: 0, bag_size: 2079\n",
      "batch 659, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 22681\n",
      "batch 679, loss: 0.3618, instance_loss: 0.0111, weighted_loss: 0.2566, label: 1, bag_size: 2356\n",
      "batch 699, loss: 0.0160, instance_loss: 0.2436, weighted_loss: 0.0843, label: 0, bag_size: 16690\n",
      "batch 719, loss: 0.2371, instance_loss: 0.0505, weighted_loss: 0.1812, label: 0, bag_size: 2160\n",
      "batch 739, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8948\n",
      "batch 759, loss: 0.0038, instance_loss: 0.0021, weighted_loss: 0.0033, label: 1, bag_size: 18603\n",
      "batch 779, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19472\n",
      "batch 799, loss: 0.0124, instance_loss: 0.1347, weighted_loss: 0.0491, label: 1, bag_size: 3003\n",
      "batch 819, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 12593\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9787104622871047: correct 12872/13152\n",
      "class 1 clustering acc 0.882147201946472: correct 5801/6576\n",
      "Epoch: 62, train_loss: 0.1184, train_clustering_loss:  0.2060, train_error: 0.0450\n",
      "class 0: acc 0.95, correct 399/420\n",
      "class 1: acc 0.9601990049751243, correct 386/402\n",
      "\n",
      "Val Set, val_loss: 0.3769, val_error: 0.0826, auc: 0.9655\n",
      "class 0 clustering acc 0.9231651376146789: correct 1610/1744\n",
      "class 1 clustering acc 0.8027522935779816: correct 700/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0121, instance_loss: 0.0487, weighted_loss: 0.0230, label: 1, bag_size: 1064\n",
      "batch 39, loss: 0.0136, instance_loss: 0.0000, weighted_loss: 0.0095, label: 1, bag_size: 11684\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 12149\n",
      "batch 79, loss: 0.9280, instance_loss: 0.3614, weighted_loss: 0.7580, label: 0, bag_size: 2098\n",
      "batch 99, loss: 0.0309, instance_loss: 0.0213, weighted_loss: 0.0280, label: 1, bag_size: 6478\n",
      "batch 119, loss: 0.0100, instance_loss: 0.0113, weighted_loss: 0.0104, label: 1, bag_size: 8660\n",
      "batch 139, loss: 0.5320, instance_loss: 0.1942, weighted_loss: 0.4307, label: 1, bag_size: 2395\n",
      "batch 159, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 11735\n",
      "batch 179, loss: 0.0440, instance_loss: 0.0000, weighted_loss: 0.0308, label: 1, bag_size: 15609\n",
      "batch 199, loss: 0.0327, instance_loss: 0.0423, weighted_loss: 0.0356, label: 0, bag_size: 7031\n",
      "batch 219, loss: 0.0244, instance_loss: 1.0844, weighted_loss: 0.3424, label: 1, bag_size: 3211\n",
      "batch 239, loss: 0.0132, instance_loss: 0.0000, weighted_loss: 0.0092, label: 1, bag_size: 7583\n",
      "batch 259, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 25558\n",
      "batch 279, loss: 0.0283, instance_loss: 0.2522, weighted_loss: 0.0955, label: 0, bag_size: 2104\n",
      "batch 299, loss: 0.0086, instance_loss: 0.3750, weighted_loss: 0.1185, label: 1, bag_size: 1249\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0094, weighted_loss: 0.0028, label: 0, bag_size: 11527\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0258, weighted_loss: 0.0077, label: 0, bag_size: 9433\n",
      "batch 359, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 14202\n",
      "batch 379, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 14433\n",
      "batch 399, loss: 0.0661, instance_loss: 0.0696, weighted_loss: 0.0672, label: 0, bag_size: 6884\n",
      "batch 419, loss: 0.7715, instance_loss: 0.0508, weighted_loss: 0.5553, label: 1, bag_size: 10622\n",
      "batch 439, loss: 0.0109, instance_loss: 0.0000, weighted_loss: 0.0076, label: 1, bag_size: 12460\n",
      "batch 459, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 26271\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15850\n",
      "batch 499, loss: 3.6917, instance_loss: 3.0368, weighted_loss: 3.4952, label: 0, bag_size: 2732\n",
      "batch 519, loss: 0.0211, instance_loss: 0.1631, weighted_loss: 0.0637, label: 0, bag_size: 4598\n",
      "batch 539, loss: 0.0024, instance_loss: 0.0099, weighted_loss: 0.0046, label: 1, bag_size: 621\n",
      "batch 559, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 15332\n",
      "batch 579, loss: 0.1566, instance_loss: 0.0000, weighted_loss: 0.1096, label: 1, bag_size: 13089\n",
      "batch 599, loss: 0.0006, instance_loss: 0.0004, weighted_loss: 0.0005, label: 0, bag_size: 8661\n",
      "batch 619, loss: 0.0103, instance_loss: 0.0075, weighted_loss: 0.0095, label: 1, bag_size: 645\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 15332\n",
      "batch 659, loss: 0.0005, instance_loss: 0.5747, weighted_loss: 0.1728, label: 0, bag_size: 7381\n",
      "batch 679, loss: 0.0566, instance_loss: 0.2746, weighted_loss: 0.1220, label: 1, bag_size: 6825\n",
      "batch 699, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 6752\n",
      "batch 719, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 18649\n",
      "batch 739, loss: 0.0217, instance_loss: 0.0000, weighted_loss: 0.0152, label: 1, bag_size: 6090\n",
      "batch 759, loss: 0.0037, instance_loss: 0.3743, weighted_loss: 0.1148, label: 1, bag_size: 8040\n",
      "batch 779, loss: 0.1933, instance_loss: 0.0383, weighted_loss: 0.1468, label: 0, bag_size: 4418\n",
      "batch 799, loss: 0.0032, instance_loss: 0.1409, weighted_loss: 0.0445, label: 1, bag_size: 20537\n",
      "batch 819, loss: 0.0013, instance_loss: 0.0540, weighted_loss: 0.0171, label: 0, bag_size: 2760\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.979242700729927: correct 12879/13152\n",
      "class 1 clustering acc 0.9014598540145985: correct 5928/6576\n",
      "Epoch: 63, train_loss: 0.1503, train_clustering_loss:  0.1881, train_error: 0.0620\n",
      "class 0: acc 0.921832884097035, correct 342/371\n",
      "class 1: acc 0.9512195121951219, correct 429/451\n",
      "\n",
      "Val Set, val_loss: 0.3272, val_error: 0.0734, auc: 0.9648\n",
      "class 0 clustering acc 0.9547018348623854: correct 1665/1744\n",
      "class 1 clustering acc 0.6961009174311926: correct 607/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2174, instance_loss: 0.6717, weighted_loss: 0.3537, label: 0, bag_size: 2098\n",
      "batch 39, loss: 0.0141, instance_loss: 0.1761, weighted_loss: 0.0627, label: 1, bag_size: 621\n",
      "batch 59, loss: 0.0053, instance_loss: 0.0000, weighted_loss: 0.0037, label: 1, bag_size: 28527\n",
      "batch 79, loss: 0.0159, instance_loss: 0.3598, weighted_loss: 0.1191, label: 1, bag_size: 16051\n",
      "batch 99, loss: 0.0006, instance_loss: 0.1111, weighted_loss: 0.0337, label: 0, bag_size: 10444\n",
      "batch 119, loss: 0.0158, instance_loss: 0.0019, weighted_loss: 0.0117, label: 0, bag_size: 7557\n",
      "batch 139, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 8981\n",
      "batch 159, loss: 0.0290, instance_loss: 0.0000, weighted_loss: 0.0203, label: 0, bag_size: 9866\n",
      "batch 179, loss: 0.0098, instance_loss: 0.0126, weighted_loss: 0.0106, label: 1, bag_size: 2140\n",
      "batch 199, loss: 0.0207, instance_loss: 0.0985, weighted_loss: 0.0441, label: 0, bag_size: 3557\n",
      "batch 219, loss: 0.0408, instance_loss: 0.0114, weighted_loss: 0.0320, label: 0, bag_size: 17630\n",
      "batch 239, loss: 0.0052, instance_loss: 0.0091, weighted_loss: 0.0063, label: 1, bag_size: 8754\n",
      "batch 259, loss: 0.0963, instance_loss: 0.0182, weighted_loss: 0.0729, label: 1, bag_size: 15192\n",
      "batch 279, loss: 0.0295, instance_loss: 0.0000, weighted_loss: 0.0206, label: 0, bag_size: 20230\n",
      "batch 299, loss: 0.0084, instance_loss: 0.3056, weighted_loss: 0.0976, label: 1, bag_size: 1459\n",
      "batch 319, loss: 0.0271, instance_loss: 0.0605, weighted_loss: 0.0371, label: 1, bag_size: 2682\n",
      "batch 339, loss: 0.0007, instance_loss: 0.0094, weighted_loss: 0.0033, label: 1, bag_size: 1172\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 22828\n",
      "batch 379, loss: 0.3067, instance_loss: 0.0192, weighted_loss: 0.2204, label: 1, bag_size: 7217\n",
      "batch 399, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 2748\n",
      "batch 419, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 21864\n",
      "batch 439, loss: 0.4230, instance_loss: 0.4543, weighted_loss: 0.4324, label: 1, bag_size: 1123\n",
      "batch 459, loss: 0.0004, instance_loss: 0.0437, weighted_loss: 0.0134, label: 0, bag_size: 2195\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0023, weighted_loss: 0.0007, label: 0, bag_size: 518\n",
      "batch 499, loss: 0.0036, instance_loss: 0.0150, weighted_loss: 0.0070, label: 0, bag_size: 1920\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0074, weighted_loss: 0.0022, label: 0, bag_size: 3459\n",
      "batch 539, loss: 0.0028, instance_loss: 0.1364, weighted_loss: 0.0429, label: 1, bag_size: 9747\n",
      "batch 559, loss: 0.0070, instance_loss: 0.0573, weighted_loss: 0.0221, label: 0, bag_size: 1651\n",
      "batch 579, loss: 0.0297, instance_loss: 0.0061, weighted_loss: 0.0226, label: 0, bag_size: 1772\n",
      "batch 599, loss: 0.0129, instance_loss: 0.2197, weighted_loss: 0.0750, label: 0, bag_size: 1614\n",
      "batch 619, loss: 0.0122, instance_loss: 0.0000, weighted_loss: 0.0085, label: 1, bag_size: 21009\n",
      "batch 639, loss: 0.1609, instance_loss: 0.0010, weighted_loss: 0.1129, label: 0, bag_size: 7141\n",
      "batch 659, loss: 0.0003, instance_loss: 0.0032, weighted_loss: 0.0012, label: 0, bag_size: 9470\n",
      "batch 679, loss: 0.0024, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 9455\n",
      "batch 699, loss: 0.0007, instance_loss: 0.0121, weighted_loss: 0.0041, label: 1, bag_size: 5991\n",
      "batch 719, loss: 0.0052, instance_loss: 0.0008, weighted_loss: 0.0039, label: 1, bag_size: 2638\n",
      "batch 739, loss: 0.0016, instance_loss: 0.0140, weighted_loss: 0.0053, label: 1, bag_size: 2278\n",
      "batch 759, loss: 0.0005, instance_loss: 0.0690, weighted_loss: 0.0211, label: 1, bag_size: 2966\n",
      "batch 779, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 27158\n",
      "batch 799, loss: 0.0148, instance_loss: 1.4691, weighted_loss: 0.4511, label: 0, bag_size: 11607\n",
      "batch 819, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 15665\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9807633819951338: correct 12899/13152\n",
      "class 1 clustering acc 0.9008515815085159: correct 5924/6576\n",
      "Epoch: 64, train_loss: 0.1405, train_clustering_loss:  0.1798, train_error: 0.0633\n",
      "class 0: acc 0.9331742243436754, correct 391/419\n",
      "class 1: acc 0.9404466501240695, correct 379/403\n",
      "\n",
      "Val Set, val_loss: 0.2538, val_error: 0.1009, auc: 0.9696\n",
      "class 0 clustering acc 0.9564220183486238: correct 1668/1744\n",
      "class 1 clustering acc 0.7912844036697247: correct 690/872\n",
      "class 0: acc 0.9130434782608695, correct 42/46\n",
      "class 1: acc 0.8888888888888888, correct 56/63\n",
      "Validation loss decreased (0.269592 --> 0.253762).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0150, instance_loss: 0.0000, weighted_loss: 0.0105, label: 1, bag_size: 10033\n",
      "batch 39, loss: 0.0036, instance_loss: 0.0110, weighted_loss: 0.0058, label: 1, bag_size: 8935\n",
      "batch 59, loss: 0.0056, instance_loss: 0.0461, weighted_loss: 0.0177, label: 1, bag_size: 7246\n",
      "batch 79, loss: 0.0029, instance_loss: 0.0631, weighted_loss: 0.0209, label: 0, bag_size: 2457\n",
      "batch 99, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 1, bag_size: 20333\n",
      "batch 119, loss: 0.0287, instance_loss: 0.1114, weighted_loss: 0.0535, label: 0, bag_size: 4418\n",
      "batch 139, loss: 0.1881, instance_loss: 0.0020, weighted_loss: 0.1323, label: 1, bag_size: 5292\n",
      "batch 159, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 10791\n",
      "batch 179, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 11512\n",
      "batch 199, loss: 0.7732, instance_loss: 0.1018, weighted_loss: 0.5718, label: 1, bag_size: 1703\n",
      "batch 219, loss: 0.3460, instance_loss: 0.2104, weighted_loss: 0.3053, label: 1, bag_size: 2842\n",
      "batch 239, loss: 2.1869, instance_loss: 0.6195, weighted_loss: 1.7167, label: 0, bag_size: 11212\n",
      "batch 259, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 16211\n",
      "batch 279, loss: 0.0420, instance_loss: 0.1235, weighted_loss: 0.0664, label: 1, bag_size: 5921\n",
      "batch 299, loss: 0.0051, instance_loss: 0.0000, weighted_loss: 0.0036, label: 0, bag_size: 9471\n",
      "batch 319, loss: 0.0876, instance_loss: 0.0281, weighted_loss: 0.0698, label: 1, bag_size: 6171\n",
      "batch 339, loss: 0.5238, instance_loss: 0.2719, weighted_loss: 0.4482, label: 0, bag_size: 14893\n",
      "batch 359, loss: 0.0040, instance_loss: 0.0000, weighted_loss: 0.0028, label: 1, bag_size: 11600\n",
      "batch 379, loss: 0.0761, instance_loss: 0.0079, weighted_loss: 0.0556, label: 1, bag_size: 12340\n",
      "batch 399, loss: 0.0473, instance_loss: 0.0000, weighted_loss: 0.0331, label: 1, bag_size: 12575\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0739, weighted_loss: 0.0222, label: 0, bag_size: 9930\n",
      "batch 439, loss: 0.6692, instance_loss: 0.2122, weighted_loss: 0.5321, label: 1, bag_size: 3879\n",
      "batch 459, loss: 0.0075, instance_loss: 0.0729, weighted_loss: 0.0271, label: 1, bag_size: 4786\n",
      "batch 479, loss: 0.0610, instance_loss: 0.0772, weighted_loss: 0.0658, label: 1, bag_size: 25695\n",
      "batch 499, loss: 0.1042, instance_loss: 0.0000, weighted_loss: 0.0730, label: 1, bag_size: 29832\n",
      "batch 519, loss: 0.0005, instance_loss: 0.0007, weighted_loss: 0.0006, label: 0, bag_size: 8959\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0002, label: 1, bag_size: 5561\n",
      "batch 559, loss: 0.0030, instance_loss: 0.0223, weighted_loss: 0.0088, label: 0, bag_size: 2036\n",
      "batch 579, loss: 0.0086, instance_loss: 0.0234, weighted_loss: 0.0131, label: 0, bag_size: 21093\n",
      "batch 599, loss: 0.0319, instance_loss: 0.0000, weighted_loss: 0.0223, label: 0, bag_size: 16521\n",
      "batch 619, loss: 0.0205, instance_loss: 0.0637, weighted_loss: 0.0335, label: 1, bag_size: 8026\n",
      "batch 639, loss: 0.2930, instance_loss: 0.0014, weighted_loss: 0.2056, label: 1, bag_size: 9215\n",
      "batch 659, loss: 0.0031, instance_loss: 0.0089, weighted_loss: 0.0048, label: 1, bag_size: 2146\n",
      "batch 679, loss: 0.0012, instance_loss: 0.1294, weighted_loss: 0.0397, label: 1, bag_size: 2785\n",
      "batch 699, loss: 0.0053, instance_loss: 0.2957, weighted_loss: 0.0924, label: 1, bag_size: 2681\n",
      "batch 719, loss: 0.0197, instance_loss: 0.0227, weighted_loss: 0.0206, label: 1, bag_size: 5110\n",
      "batch 739, loss: 3.7300, instance_loss: 4.5984, weighted_loss: 3.9905, label: 1, bag_size: 13367\n",
      "batch 759, loss: 0.2535, instance_loss: 0.7209, weighted_loss: 0.3937, label: 0, bag_size: 1814\n",
      "batch 779, loss: 0.0040, instance_loss: 0.0000, weighted_loss: 0.0028, label: 1, bag_size: 14230\n",
      "batch 799, loss: 0.1141, instance_loss: 1.1990, weighted_loss: 0.4396, label: 0, bag_size: 5161\n",
      "batch 819, loss: 0.1972, instance_loss: 0.0150, weighted_loss: 0.1425, label: 1, bag_size: 21701\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9825121654501217: correct 12922/13152\n",
      "class 1 clustering acc 0.9035888077858881: correct 5942/6576\n",
      "Epoch: 65, train_loss: 0.1421, train_clustering_loss:  0.1663, train_error: 0.0438\n",
      "class 0: acc 0.9552238805970149, correct 384/402\n",
      "class 1: acc 0.9571428571428572, correct 402/420\n",
      "\n",
      "Val Set, val_loss: 0.3930, val_error: 0.0917, auc: 0.9696\n",
      "class 0 clustering acc 0.9346330275229358: correct 1630/1744\n",
      "class 1 clustering acc 0.823394495412844: correct 718/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.8412698412698413, correct 53/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 5833\n",
      "batch 39, loss: 2.1839, instance_loss: 3.0709, weighted_loss: 2.4500, label: 1, bag_size: 2731\n",
      "batch 59, loss: 0.3232, instance_loss: 0.0120, weighted_loss: 0.2298, label: 0, bag_size: 7141\n",
      "batch 79, loss: 0.0003, instance_loss: 0.0022, weighted_loss: 0.0009, label: 0, bag_size: 2036\n",
      "batch 99, loss: 0.0002, instance_loss: 0.0940, weighted_loss: 0.0284, label: 1, bag_size: 14604\n",
      "batch 119, loss: 0.1589, instance_loss: 0.2347, weighted_loss: 0.1816, label: 0, bag_size: 3783\n",
      "batch 139, loss: 0.0030, instance_loss: 0.0419, weighted_loss: 0.0147, label: 1, bag_size: 4959\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15967\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 19466\n",
      "batch 199, loss: 0.0163, instance_loss: 0.1613, weighted_loss: 0.0598, label: 0, bag_size: 11151\n",
      "batch 219, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 11187\n",
      "batch 239, loss: 0.0065, instance_loss: 0.0578, weighted_loss: 0.0219, label: 1, bag_size: 2681\n",
      "batch 259, loss: 0.1338, instance_loss: 0.0575, weighted_loss: 0.1109, label: 0, bag_size: 10029\n",
      "batch 279, loss: 0.0160, instance_loss: 0.0000, weighted_loss: 0.0112, label: 1, bag_size: 16267\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 21864\n",
      "batch 319, loss: 0.0029, instance_loss: 0.0497, weighted_loss: 0.0170, label: 1, bag_size: 2480\n",
      "batch 339, loss: 0.5723, instance_loss: 0.1828, weighted_loss: 0.4554, label: 1, bag_size: 1963\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 7191\n",
      "batch 379, loss: 0.2612, instance_loss: 0.1478, weighted_loss: 0.2272, label: 0, bag_size: 14264\n",
      "batch 399, loss: 0.0009, instance_loss: 0.0776, weighted_loss: 0.0239, label: 0, bag_size: 9786\n",
      "batch 419, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 5612\n",
      "batch 439, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 1, bag_size: 14604\n",
      "batch 459, loss: 0.0121, instance_loss: 0.2093, weighted_loss: 0.0712, label: 1, bag_size: 6928\n",
      "batch 479, loss: 0.0037, instance_loss: 0.0048, weighted_loss: 0.0041, label: 1, bag_size: 645\n",
      "batch 499, loss: 0.1093, instance_loss: 0.2917, weighted_loss: 0.1640, label: 0, bag_size: 1498\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 21082\n",
      "batch 539, loss: 0.0013, instance_loss: 0.0001, weighted_loss: 0.0010, label: 0, bag_size: 19043\n",
      "batch 559, loss: 0.0010, instance_loss: 0.0001, weighted_loss: 0.0007, label: 0, bag_size: 8959\n",
      "batch 579, loss: 0.0952, instance_loss: 0.1466, weighted_loss: 0.1106, label: 1, bag_size: 8026\n",
      "batch 599, loss: 0.0116, instance_loss: 0.0000, weighted_loss: 0.0081, label: 1, bag_size: 9571\n",
      "batch 619, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 18777\n",
      "batch 639, loss: 0.3452, instance_loss: 0.2057, weighted_loss: 0.3033, label: 1, bag_size: 8395\n",
      "batch 659, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 2920\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21385\n",
      "batch 699, loss: 0.8867, instance_loss: 0.0024, weighted_loss: 0.6214, label: 1, bag_size: 21450\n",
      "batch 719, loss: 0.0066, instance_loss: 0.0595, weighted_loss: 0.0225, label: 1, bag_size: 3656\n",
      "batch 739, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 4394\n",
      "batch 759, loss: 0.0140, instance_loss: 0.5929, weighted_loss: 0.1876, label: 0, bag_size: 14739\n",
      "batch 779, loss: 0.0056, instance_loss: 0.0000, weighted_loss: 0.0039, label: 0, bag_size: 21032\n",
      "batch 799, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 22828\n",
      "batch 819, loss: 0.0020, instance_loss: 0.0000, weighted_loss: 0.0014, label: 0, bag_size: 11199\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9860097323600974: correct 12968/13152\n",
      "class 1 clustering acc 0.9194038929440389: correct 6046/6576\n",
      "Epoch: 66, train_loss: 0.1250, train_clustering_loss:  0.1461, train_error: 0.0523\n",
      "class 0: acc 0.9381188118811881, correct 379/404\n",
      "class 1: acc 0.9569377990430622, correct 400/418\n",
      "\n",
      "Val Set, val_loss: 0.2828, val_error: 0.0917, auc: 0.9683\n",
      "class 0 clustering acc 0.9512614678899083: correct 1659/1744\n",
      "class 1 clustering acc 0.8302752293577982: correct 724/872\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0011, instance_loss: 0.2078, weighted_loss: 0.0631, label: 0, bag_size: 2760\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 18468\n",
      "batch 59, loss: 0.0027, instance_loss: 0.0000, weighted_loss: 0.0019, label: 0, bag_size: 2351\n",
      "batch 79, loss: 0.1971, instance_loss: 0.1240, weighted_loss: 0.1752, label: 1, bag_size: 1875\n",
      "batch 99, loss: 0.0055, instance_loss: 0.0963, weighted_loss: 0.0327, label: 0, bag_size: 2624\n",
      "batch 119, loss: 0.0803, instance_loss: 0.7365, weighted_loss: 0.2772, label: 0, bag_size: 2104\n",
      "batch 139, loss: 0.0046, instance_loss: 0.0358, weighted_loss: 0.0140, label: 0, bag_size: 1826\n",
      "batch 159, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 15736\n",
      "batch 179, loss: 0.0102, instance_loss: 0.0000, weighted_loss: 0.0072, label: 0, bag_size: 24439\n",
      "batch 199, loss: 0.3180, instance_loss: 0.3397, weighted_loss: 0.3245, label: 0, bag_size: 1127\n",
      "batch 219, loss: 0.0117, instance_loss: 0.0000, weighted_loss: 0.0082, label: 1, bag_size: 10028\n",
      "batch 239, loss: 0.0020, instance_loss: 0.0072, weighted_loss: 0.0036, label: 1, bag_size: 3640\n",
      "batch 259, loss: 0.0010, instance_loss: 0.1069, weighted_loss: 0.0328, label: 1, bag_size: 3453\n",
      "batch 279, loss: 0.4368, instance_loss: 0.0002, weighted_loss: 0.3058, label: 1, bag_size: 11256\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0038, weighted_loss: 0.0012, label: 1, bag_size: 2686\n",
      "batch 319, loss: 0.0249, instance_loss: 0.0000, weighted_loss: 0.0174, label: 1, bag_size: 19972\n",
      "batch 339, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 12931\n",
      "batch 359, loss: 0.0984, instance_loss: 0.0878, weighted_loss: 0.0952, label: 0, bag_size: 3654\n",
      "batch 379, loss: 0.0005, instance_loss: 0.0221, weighted_loss: 0.0070, label: 1, bag_size: 2966\n",
      "batch 399, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 18045\n",
      "batch 419, loss: 0.0010, instance_loss: 0.0007, weighted_loss: 0.0009, label: 1, bag_size: 4330\n",
      "batch 439, loss: 0.5313, instance_loss: 0.4822, weighted_loss: 0.5165, label: 1, bag_size: 2314\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11727\n",
      "batch 479, loss: 0.0006, instance_loss: 0.0096, weighted_loss: 0.0033, label: 0, bag_size: 2091\n",
      "batch 499, loss: 0.2559, instance_loss: 0.8369, weighted_loss: 0.4302, label: 1, bag_size: 1920\n",
      "batch 519, loss: 0.0009, instance_loss: 0.0006, weighted_loss: 0.0008, label: 1, bag_size: 7515\n",
      "batch 539, loss: 0.8822, instance_loss: 0.0453, weighted_loss: 0.6312, label: 0, bag_size: 2070\n",
      "batch 559, loss: 0.7302, instance_loss: 0.0661, weighted_loss: 0.5310, label: 1, bag_size: 2842\n",
      "batch 579, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 9470\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9433\n",
      "batch 619, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 11512\n",
      "batch 639, loss: 0.0080, instance_loss: 0.2197, weighted_loss: 0.0715, label: 0, bag_size: 2148\n",
      "batch 659, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 18649\n",
      "batch 679, loss: 0.0701, instance_loss: 0.0000, weighted_loss: 0.0491, label: 1, bag_size: 2356\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 22800\n",
      "batch 719, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 7371\n",
      "batch 739, loss: 0.0002, instance_loss: 0.0226, weighted_loss: 0.0069, label: 1, bag_size: 4877\n",
      "batch 759, loss: 0.0660, instance_loss: 0.0000, weighted_loss: 0.0462, label: 1, bag_size: 8012\n",
      "batch 779, loss: 0.2739, instance_loss: 0.0086, weighted_loss: 0.1943, label: 0, bag_size: 13619\n",
      "batch 799, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 30675\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8372\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9876824817518248: correct 12990/13152\n",
      "class 1 clustering acc 0.9315693430656934: correct 6126/6576\n",
      "Epoch: 67, train_loss: 0.1113, train_clustering_loss:  0.1174, train_error: 0.0401\n",
      "class 0: acc 0.9568181818181818, correct 421/440\n",
      "class 1: acc 0.9633507853403142, correct 368/382\n",
      "\n",
      "Val Set, val_loss: 0.5498, val_error: 0.1468, auc: 0.9662\n",
      "class 0 clustering acc 0.955848623853211: correct 1667/1744\n",
      "class 1 clustering acc 0.8704128440366973: correct 759/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.746031746031746, correct 47/63\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0015, label: 0, bag_size: 5999\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11865\n",
      "batch 59, loss: 0.0042, instance_loss: 0.0000, weighted_loss: 0.0029, label: 0, bag_size: 24439\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 22800\n",
      "batch 99, loss: 0.0038, instance_loss: 0.2644, weighted_loss: 0.0820, label: 0, bag_size: 2195\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0942, weighted_loss: 0.0283, label: 1, bag_size: 6317\n",
      "batch 139, loss: 0.8443, instance_loss: 0.0000, weighted_loss: 0.5910, label: 1, bag_size: 7066\n",
      "batch 159, loss: 0.0008, instance_loss: 0.0001, weighted_loss: 0.0006, label: 1, bag_size: 5340\n",
      "batch 179, loss: 0.8960, instance_loss: 1.4048, weighted_loss: 1.0486, label: 0, bag_size: 2918\n",
      "batch 199, loss: 0.0124, instance_loss: 0.2350, weighted_loss: 0.0792, label: 1, bag_size: 16514\n",
      "batch 219, loss: 0.0028, instance_loss: 0.3874, weighted_loss: 0.1182, label: 0, bag_size: 1416\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 7078\n",
      "batch 259, loss: 0.2806, instance_loss: 0.0140, weighted_loss: 0.2006, label: 0, bag_size: 24382\n",
      "batch 279, loss: 0.0019, instance_loss: 0.0000, weighted_loss: 0.0014, label: 0, bag_size: 14739\n",
      "batch 299, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 11259\n",
      "batch 319, loss: 0.1503, instance_loss: 0.0073, weighted_loss: 0.1074, label: 1, bag_size: 12714\n",
      "batch 339, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 16341\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21385\n",
      "batch 379, loss: 0.1562, instance_loss: 0.0937, weighted_loss: 0.1374, label: 1, bag_size: 5292\n",
      "batch 399, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 21076\n",
      "batch 419, loss: 0.0403, instance_loss: 0.0580, weighted_loss: 0.0456, label: 1, bag_size: 4939\n",
      "batch 439, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 2044\n",
      "batch 459, loss: 0.0231, instance_loss: 0.5400, weighted_loss: 0.1782, label: 1, bag_size: 1242\n",
      "batch 479, loss: 0.1531, instance_loss: 0.0108, weighted_loss: 0.1104, label: 0, bag_size: 9616\n",
      "batch 499, loss: 0.0032, instance_loss: 0.0007, weighted_loss: 0.0024, label: 1, bag_size: 7217\n",
      "batch 519, loss: 0.0307, instance_loss: 0.0000, weighted_loss: 0.0215, label: 0, bag_size: 4845\n",
      "batch 539, loss: 0.0957, instance_loss: 0.0048, weighted_loss: 0.0684, label: 1, bag_size: 4789\n",
      "batch 559, loss: 0.0024, instance_loss: 0.0048, weighted_loss: 0.0031, label: 1, bag_size: 8935\n",
      "batch 579, loss: 0.0113, instance_loss: 0.0508, weighted_loss: 0.0232, label: 1, bag_size: 12340\n",
      "batch 599, loss: 0.0239, instance_loss: 0.0000, weighted_loss: 0.0167, label: 1, bag_size: 25695\n",
      "batch 619, loss: 0.0093, instance_loss: 0.0000, weighted_loss: 0.0065, label: 1, bag_size: 9877\n",
      "batch 639, loss: 0.0250, instance_loss: 0.5663, weighted_loss: 0.1874, label: 0, bag_size: 2004\n",
      "batch 659, loss: 0.0003, instance_loss: 0.0046, weighted_loss: 0.0016, label: 1, bag_size: 14618\n",
      "batch 679, loss: 0.0002, instance_loss: 0.0608, weighted_loss: 0.0184, label: 1, bag_size: 5991\n",
      "batch 699, loss: 2.6975, instance_loss: 0.4500, weighted_loss: 2.0233, label: 1, bag_size: 684\n",
      "batch 719, loss: 0.0118, instance_loss: 0.0003, weighted_loss: 0.0083, label: 0, bag_size: 12083\n",
      "batch 739, loss: 0.3003, instance_loss: 0.0022, weighted_loss: 0.2109, label: 1, bag_size: 12714\n",
      "batch 759, loss: 0.0001, instance_loss: 0.0487, weighted_loss: 0.0147, label: 0, bag_size: 8252\n",
      "batch 779, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 3232\n",
      "batch 799, loss: 0.2230, instance_loss: 0.0017, weighted_loss: 0.1566, label: 1, bag_size: 15931\n",
      "batch 819, loss: 0.0026, instance_loss: 0.0000, weighted_loss: 0.0018, label: 1, bag_size: 12178\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9846411192214112: correct 12950/13152\n",
      "class 1 clustering acc 0.9151459854014599: correct 6018/6576\n",
      "Epoch: 68, train_loss: 0.1146, train_clustering_loss:  0.1657, train_error: 0.0426\n",
      "class 0: acc 0.9552238805970149, correct 384/402\n",
      "class 1: acc 0.9595238095238096, correct 403/420\n",
      "\n",
      "Val Set, val_loss: 0.3099, val_error: 0.0734, auc: 0.9700\n",
      "class 0 clustering acc 0.963302752293578: correct 1680/1744\n",
      "class 1 clustering acc 0.8669724770642202: correct 756/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 3.1188, instance_loss: 1.4169, weighted_loss: 2.6082, label: 1, bag_size: 13367\n",
      "batch 39, loss: 0.0052, instance_loss: 0.0000, weighted_loss: 0.0037, label: 0, bag_size: 18777\n",
      "batch 59, loss: 0.1578, instance_loss: 1.1294, weighted_loss: 0.4493, label: 0, bag_size: 2996\n",
      "batch 79, loss: 2.3670, instance_loss: 0.1567, weighted_loss: 1.7039, label: 0, bag_size: 3897\n",
      "batch 99, loss: 0.0484, instance_loss: 0.0874, weighted_loss: 0.0601, label: 1, bag_size: 8191\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 19470\n",
      "batch 139, loss: 0.0023, instance_loss: 0.0016, weighted_loss: 0.0021, label: 1, bag_size: 8040\n",
      "batch 159, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 12201\n",
      "batch 179, loss: 0.4124, instance_loss: 0.0102, weighted_loss: 0.2918, label: 0, bag_size: 1732\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 21385\n",
      "batch 219, loss: 0.0381, instance_loss: 0.0215, weighted_loss: 0.0331, label: 0, bag_size: 13023\n",
      "batch 239, loss: 0.1148, instance_loss: 0.4829, weighted_loss: 0.2252, label: 1, bag_size: 2314\n",
      "batch 259, loss: 0.0256, instance_loss: 0.0137, weighted_loss: 0.0220, label: 1, bag_size: 9689\n",
      "batch 279, loss: 0.0007, instance_loss: 0.0002, weighted_loss: 0.0006, label: 0, bag_size: 1891\n",
      "batch 299, loss: 1.0867, instance_loss: 0.3864, weighted_loss: 0.8766, label: 0, bag_size: 3468\n",
      "batch 319, loss: 0.2727, instance_loss: 0.0077, weighted_loss: 0.1932, label: 1, bag_size: 4789\n",
      "batch 339, loss: 0.0039, instance_loss: 0.0000, weighted_loss: 0.0027, label: 0, bag_size: 14625\n",
      "batch 359, loss: 3.5606, instance_loss: 1.8116, weighted_loss: 3.0359, label: 1, bag_size: 15563\n",
      "batch 379, loss: 0.0003, instance_loss: 0.0023, weighted_loss: 0.0009, label: 1, bag_size: 7515\n",
      "batch 399, loss: 0.0012, instance_loss: 0.0005, weighted_loss: 0.0010, label: 1, bag_size: 8935\n",
      "batch 419, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 9971\n",
      "batch 439, loss: 0.0003, instance_loss: 0.1109, weighted_loss: 0.0335, label: 1, bag_size: 4877\n",
      "batch 459, loss: 0.0620, instance_loss: 0.1613, weighted_loss: 0.0918, label: 0, bag_size: 9583\n",
      "batch 479, loss: 0.0007, instance_loss: 0.0054, weighted_loss: 0.0021, label: 1, bag_size: 3634\n",
      "batch 499, loss: 0.0019, instance_loss: 0.0008, weighted_loss: 0.0016, label: 1, bag_size: 5731\n",
      "batch 519, loss: 0.0201, instance_loss: 0.0887, weighted_loss: 0.0406, label: 1, bag_size: 7513\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0031, weighted_loss: 0.0010, label: 0, bag_size: 9851\n",
      "batch 559, loss: 0.0152, instance_loss: 0.0000, weighted_loss: 0.0106, label: 0, bag_size: 18738\n",
      "batch 579, loss: 0.0112, instance_loss: 0.0000, weighted_loss: 0.0079, label: 1, bag_size: 10396\n",
      "batch 599, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 8522\n",
      "batch 619, loss: 0.0191, instance_loss: 1.1327, weighted_loss: 0.3532, label: 0, bag_size: 2815\n",
      "batch 639, loss: 0.0018, instance_loss: 0.0047, weighted_loss: 0.0027, label: 1, bag_size: 3003\n",
      "batch 659, loss: 0.0021, instance_loss: 0.0642, weighted_loss: 0.0207, label: 0, bag_size: 803\n",
      "batch 679, loss: 0.1178, instance_loss: 0.0150, weighted_loss: 0.0869, label: 1, bag_size: 2681\n",
      "batch 699, loss: 0.0261, instance_loss: 0.5827, weighted_loss: 0.1931, label: 0, bag_size: 6884\n",
      "batch 719, loss: 0.0127, instance_loss: 0.4006, weighted_loss: 0.1291, label: 0, bag_size: 10721\n",
      "batch 739, loss: 0.0001, instance_loss: 0.0220, weighted_loss: 0.0067, label: 1, bag_size: 10394\n",
      "batch 759, loss: 0.0027, instance_loss: 0.0000, weighted_loss: 0.0019, label: 0, bag_size: 1891\n",
      "batch 779, loss: 0.0150, instance_loss: 0.0000, weighted_loss: 0.0105, label: 0, bag_size: 13205\n",
      "batch 799, loss: 0.0001, instance_loss: 0.0021, weighted_loss: 0.0007, label: 1, bag_size: 9759\n",
      "batch 819, loss: 0.0001, instance_loss: 0.0206, weighted_loss: 0.0062, label: 1, bag_size: 14779\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9857816301703163: correct 12965/13152\n",
      "class 1 clustering acc 0.9181873479318735: correct 6038/6576\n",
      "Epoch: 69, train_loss: 0.1436, train_clustering_loss:  0.1512, train_error: 0.0523\n",
      "class 0: acc 0.9351620947630923, correct 375/401\n",
      "class 1: acc 0.9596199524940617, correct 404/421\n",
      "\n",
      "Val Set, val_loss: 0.3122, val_error: 0.0826, auc: 0.9683\n",
      "class 0 clustering acc 0.9610091743119266: correct 1676/1744\n",
      "class 1 clustering acc 0.8577981651376146: correct 748/872\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.1275, weighted_loss: 0.0383, label: 1, bag_size: 2405\n",
      "batch 39, loss: 0.0009, instance_loss: 0.0162, weighted_loss: 0.0055, label: 1, bag_size: 7381\n",
      "batch 59, loss: 0.0003, instance_loss: 0.0907, weighted_loss: 0.0274, label: 1, bag_size: 1622\n",
      "batch 79, loss: 0.0060, instance_loss: 0.0176, weighted_loss: 0.0094, label: 1, bag_size: 2814\n",
      "batch 99, loss: 0.1838, instance_loss: 0.0867, weighted_loss: 0.1547, label: 1, bag_size: 4929\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11389\n",
      "batch 139, loss: 0.0796, instance_loss: 0.0529, weighted_loss: 0.0716, label: 1, bag_size: 4929\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11477\n",
      "batch 179, loss: 0.1587, instance_loss: 1.2159, weighted_loss: 0.4759, label: 1, bag_size: 9404\n",
      "batch 199, loss: 0.0644, instance_loss: 4.2463, weighted_loss: 1.3190, label: 0, bag_size: 18738\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0742, weighted_loss: 0.0223, label: 1, bag_size: 3437\n",
      "batch 239, loss: 0.1147, instance_loss: 0.1887, weighted_loss: 0.1369, label: 1, bag_size: 5292\n",
      "batch 259, loss: 0.0040, instance_loss: 0.0055, weighted_loss: 0.0045, label: 0, bag_size: 931\n",
      "batch 279, loss: 0.0589, instance_loss: 0.0029, weighted_loss: 0.0421, label: 1, bag_size: 12460\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 7381\n",
      "batch 319, loss: 0.0008, instance_loss: 0.3015, weighted_loss: 0.0910, label: 1, bag_size: 7381\n",
      "batch 339, loss: 0.0320, instance_loss: 0.7823, weighted_loss: 0.2571, label: 0, bag_size: 2458\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0099, weighted_loss: 0.0030, label: 1, bag_size: 9759\n",
      "batch 379, loss: 0.1221, instance_loss: 0.1820, weighted_loss: 0.1401, label: 0, bag_size: 5161\n",
      "batch 399, loss: 0.0711, instance_loss: 0.0001, weighted_loss: 0.0498, label: 0, bag_size: 17482\n",
      "batch 419, loss: 0.0187, instance_loss: 0.4039, weighted_loss: 0.1343, label: 1, bag_size: 9330\n",
      "batch 439, loss: 0.0022, instance_loss: 0.0422, weighted_loss: 0.0142, label: 1, bag_size: 3003\n",
      "batch 459, loss: 0.0016, instance_loss: 0.0941, weighted_loss: 0.0294, label: 1, bag_size: 4317\n",
      "batch 479, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 9471\n",
      "batch 499, loss: 0.0000, instance_loss: 0.2065, weighted_loss: 0.0620, label: 0, bag_size: 19472\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 20666\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 22828\n",
      "batch 559, loss: 0.0293, instance_loss: 0.0000, weighted_loss: 0.0205, label: 0, bag_size: 11122\n",
      "batch 579, loss: 0.0046, instance_loss: 0.2756, weighted_loss: 0.0859, label: 1, bag_size: 7981\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21576\n",
      "batch 619, loss: 0.0013, instance_loss: 0.1975, weighted_loss: 0.0601, label: 0, bag_size: 7381\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19472\n",
      "batch 659, loss: 0.0392, instance_loss: 0.3208, weighted_loss: 0.1237, label: 0, bag_size: 2996\n",
      "batch 679, loss: 0.0008, instance_loss: 0.0571, weighted_loss: 0.0177, label: 0, bag_size: 2236\n",
      "batch 699, loss: 0.0225, instance_loss: 0.0039, weighted_loss: 0.0169, label: 1, bag_size: 6842\n",
      "batch 719, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 11600\n",
      "batch 739, loss: 0.0013, instance_loss: 0.4639, weighted_loss: 0.1401, label: 0, bag_size: 3101\n",
      "batch 759, loss: 0.0012, instance_loss: 0.0002, weighted_loss: 0.0009, label: 1, bag_size: 6734\n",
      "batch 779, loss: 0.0015, instance_loss: 0.0005, weighted_loss: 0.0012, label: 1, bag_size: 2638\n",
      "batch 799, loss: 0.0596, instance_loss: 0.2333, weighted_loss: 0.1117, label: 0, bag_size: 10415\n",
      "batch 819, loss: 0.0747, instance_loss: 0.0948, weighted_loss: 0.0807, label: 1, bag_size: 16154\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9805352798053528: correct 12896/13152\n",
      "class 1 clustering acc 0.9110401459854015: correct 5991/6576\n",
      "Epoch: 70, train_loss: 0.1166, train_clustering_loss:  0.1805, train_error: 0.0450\n",
      "class 0: acc 0.9523809523809523, correct 380/399\n",
      "class 1: acc 0.9574468085106383, correct 405/423\n",
      "\n",
      "Val Set, val_loss: 0.2780, val_error: 0.1009, auc: 0.9686\n",
      "class 0 clustering acc 0.9627293577981652: correct 1679/1744\n",
      "class 1 clustering acc 0.8555045871559633: correct 746/872\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0461, instance_loss: 0.0003, weighted_loss: 0.0323, label: 1, bag_size: 2936\n",
      "batch 39, loss: 0.6975, instance_loss: 0.0437, weighted_loss: 0.5014, label: 1, bag_size: 6736\n",
      "batch 59, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 11032\n",
      "batch 79, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 2548\n",
      "batch 99, loss: 0.0318, instance_loss: 1.3383, weighted_loss: 0.4238, label: 1, bag_size: 1294\n",
      "batch 119, loss: 0.0451, instance_loss: 0.0470, weighted_loss: 0.0457, label: 1, bag_size: 2814\n",
      "batch 139, loss: 0.0330, instance_loss: 0.0155, weighted_loss: 0.0278, label: 1, bag_size: 1483\n",
      "batch 159, loss: 0.0070, instance_loss: 0.4265, weighted_loss: 0.1329, label: 1, bag_size: 3651\n",
      "batch 179, loss: 0.0007, instance_loss: 0.1300, weighted_loss: 0.0395, label: 1, bag_size: 5894\n",
      "batch 199, loss: 0.0057, instance_loss: 0.0000, weighted_loss: 0.0040, label: 0, bag_size: 25027\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0015, weighted_loss: 0.0005, label: 1, bag_size: 19832\n",
      "batch 239, loss: 0.0048, instance_loss: 0.0000, weighted_loss: 0.0034, label: 0, bag_size: 10942\n",
      "batch 259, loss: 0.0541, instance_loss: 0.0032, weighted_loss: 0.0388, label: 1, bag_size: 34356\n",
      "batch 279, loss: 0.0529, instance_loss: 0.1139, weighted_loss: 0.0712, label: 0, bag_size: 1789\n",
      "batch 299, loss: 0.2933, instance_loss: 0.2875, weighted_loss: 0.2916, label: 0, bag_size: 6367\n",
      "batch 319, loss: 0.0056, instance_loss: 0.0116, weighted_loss: 0.0074, label: 1, bag_size: 10498\n",
      "batch 339, loss: 0.0105, instance_loss: 0.0252, weighted_loss: 0.0149, label: 0, bag_size: 2360\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9433\n",
      "batch 379, loss: 0.6402, instance_loss: 0.1429, weighted_loss: 0.4910, label: 0, bag_size: 1732\n",
      "batch 399, loss: 0.0011, instance_loss: 0.0053, weighted_loss: 0.0024, label: 0, bag_size: 3725\n",
      "batch 419, loss: 0.0009, instance_loss: 0.0085, weighted_loss: 0.0032, label: 1, bag_size: 9446\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 27012\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 20150\n",
      "batch 479, loss: 0.0019, instance_loss: 0.0040, weighted_loss: 0.0025, label: 0, bag_size: 2367\n",
      "batch 499, loss: 0.0016, instance_loss: 0.1755, weighted_loss: 0.0538, label: 0, bag_size: 1508\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 6752\n",
      "batch 539, loss: 0.0060, instance_loss: 0.0000, weighted_loss: 0.0042, label: 0, bag_size: 10415\n",
      "batch 559, loss: 0.0060, instance_loss: 0.0000, weighted_loss: 0.0042, label: 1, bag_size: 19606\n",
      "batch 579, loss: 0.0170, instance_loss: 0.8054, weighted_loss: 0.2535, label: 0, bag_size: 1690\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 20150\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21874\n",
      "batch 639, loss: 0.0103, instance_loss: 0.1886, weighted_loss: 0.0638, label: 1, bag_size: 7371\n",
      "batch 659, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 3541\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 12212\n",
      "batch 699, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 15213\n",
      "batch 719, loss: 0.3540, instance_loss: 0.6136, weighted_loss: 0.4319, label: 1, bag_size: 1963\n",
      "batch 739, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 2968\n",
      "batch 759, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8981\n",
      "batch 779, loss: 0.0183, instance_loss: 0.0022, weighted_loss: 0.0135, label: 0, bag_size: 16690\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 15716\n",
      "batch 819, loss: 0.0158, instance_loss: 0.0523, weighted_loss: 0.0268, label: 1, bag_size: 19606\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9847171532846716: correct 12951/13152\n",
      "class 1 clustering acc 0.9139294403892944: correct 6010/6576\n",
      "Epoch: 71, train_loss: 0.1049, train_clustering_loss:  0.1574, train_error: 0.0426\n",
      "class 0: acc 0.9559164733178654, correct 412/431\n",
      "class 1: acc 0.959079283887468, correct 375/391\n",
      "\n",
      "Val Set, val_loss: 0.3843, val_error: 0.1009, auc: 0.9710\n",
      "class 0 clustering acc 0.9621559633027523: correct 1678/1744\n",
      "class 1 clustering acc 0.8084862385321101: correct 705/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.8253968253968254, correct 52/63\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0155, instance_loss: 0.0000, weighted_loss: 0.0108, label: 0, bag_size: 14885\n",
      "batch 39, loss: 0.0028, instance_loss: 0.0051, weighted_loss: 0.0035, label: 1, bag_size: 12946\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0048, weighted_loss: 0.0015, label: 1, bag_size: 4317\n",
      "batch 79, loss: 0.0040, instance_loss: 0.0002, weighted_loss: 0.0029, label: 0, bag_size: 12083\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23791\n",
      "batch 119, loss: 0.0003, instance_loss: 0.0640, weighted_loss: 0.0194, label: 1, bag_size: 6533\n",
      "batch 139, loss: 0.9513, instance_loss: 0.2015, weighted_loss: 0.7264, label: 1, bag_size: 8103\n",
      "batch 159, loss: 0.0172, instance_loss: 0.4097, weighted_loss: 0.1349, label: 0, bag_size: 4523\n",
      "batch 179, loss: 0.0019, instance_loss: 0.0000, weighted_loss: 0.0014, label: 0, bag_size: 16607\n",
      "batch 199, loss: 0.5449, instance_loss: 3.1477, weighted_loss: 1.3257, label: 0, bag_size: 1714\n",
      "batch 219, loss: 0.0008, instance_loss: 0.9217, weighted_loss: 0.2771, label: 1, bag_size: 3450\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 18574\n",
      "batch 259, loss: 0.6928, instance_loss: 1.5750, weighted_loss: 0.9575, label: 0, bag_size: 11306\n",
      "batch 279, loss: 0.0006, instance_loss: 0.0004, weighted_loss: 0.0005, label: 0, bag_size: 9949\n",
      "batch 299, loss: 0.3816, instance_loss: 0.0618, weighted_loss: 0.2856, label: 0, bag_size: 9132\n",
      "batch 319, loss: 0.0263, instance_loss: 0.6208, weighted_loss: 0.2046, label: 0, bag_size: 2815\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0832, weighted_loss: 0.0250, label: 1, bag_size: 10112\n",
      "batch 359, loss: 2.1491, instance_loss: 0.8021, weighted_loss: 1.7450, label: 1, bag_size: 2731\n",
      "batch 379, loss: 0.0092, instance_loss: 0.0148, weighted_loss: 0.0109, label: 1, bag_size: 19606\n",
      "batch 399, loss: 0.0002, instance_loss: 0.0471, weighted_loss: 0.0143, label: 1, bag_size: 16512\n",
      "batch 419, loss: 0.0280, instance_loss: 0.0504, weighted_loss: 0.0348, label: 1, bag_size: 7186\n",
      "batch 439, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 9485\n",
      "batch 459, loss: 0.0171, instance_loss: 0.5154, weighted_loss: 0.1666, label: 0, bag_size: 1416\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 14377\n",
      "batch 499, loss: 0.0008, instance_loss: 0.0002, weighted_loss: 0.0006, label: 0, bag_size: 2044\n",
      "batch 519, loss: 0.1079, instance_loss: 0.1694, weighted_loss: 0.1263, label: 1, bag_size: 2314\n",
      "batch 539, loss: 0.0090, instance_loss: 0.0521, weighted_loss: 0.0219, label: 1, bag_size: 8216\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 7191\n",
      "batch 579, loss: 0.0436, instance_loss: 0.0258, weighted_loss: 0.0382, label: 1, bag_size: 6842\n",
      "batch 599, loss: 0.0362, instance_loss: 0.2474, weighted_loss: 0.0996, label: 0, bag_size: 3708\n",
      "batch 619, loss: 0.0032, instance_loss: 0.0333, weighted_loss: 0.0123, label: 1, bag_size: 2278\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8812\n",
      "batch 659, loss: 3.4476, instance_loss: 2.9549, weighted_loss: 3.2998, label: 0, bag_size: 4692\n",
      "batch 679, loss: 0.1549, instance_loss: 2.1817, weighted_loss: 0.7629, label: 0, bag_size: 1592\n",
      "batch 699, loss: 0.0010, instance_loss: 0.1210, weighted_loss: 0.0370, label: 1, bag_size: 14030\n",
      "batch 719, loss: 0.0396, instance_loss: 0.0117, weighted_loss: 0.0313, label: 1, bag_size: 4789\n",
      "batch 739, loss: 0.0225, instance_loss: 0.0224, weighted_loss: 0.0225, label: 1, bag_size: 1230\n",
      "batch 759, loss: 0.0151, instance_loss: 0.1118, weighted_loss: 0.0441, label: 1, bag_size: 11386\n",
      "batch 779, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 30751\n",
      "batch 799, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 8582\n",
      "batch 819, loss: 0.0035, instance_loss: 0.1826, weighted_loss: 0.0572, label: 1, bag_size: 2904\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9808394160583942: correct 12900/13152\n",
      "class 1 clustering acc 0.9116484184914841: correct 5995/6576\n",
      "Epoch: 72, train_loss: 0.1177, train_clustering_loss:  0.1871, train_error: 0.0499\n",
      "class 0: acc 0.9514563106796117, correct 392/412\n",
      "class 1: acc 0.948780487804878, correct 389/410\n",
      "\n",
      "Val Set, val_loss: 0.2448, val_error: 0.1193, auc: 0.9724\n",
      "class 0 clustering acc 0.9650229357798165: correct 1683/1744\n",
      "class 1 clustering acc 0.8956422018348624: correct 781/872\n",
      "class 0: acc 0.8478260869565217, correct 39/46\n",
      "class 1: acc 0.9047619047619048, correct 57/63\n",
      "Validation loss decreased (0.253762 --> 0.244836).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0666, instance_loss: 0.0000, weighted_loss: 0.0466, label: 0, bag_size: 3810\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0060, weighted_loss: 0.0018, label: 1, bag_size: 15665\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0454, weighted_loss: 0.0137, label: 1, bag_size: 18649\n",
      "batch 79, loss: 0.0300, instance_loss: 0.0040, weighted_loss: 0.0222, label: 0, bag_size: 3502\n",
      "batch 99, loss: 0.1838, instance_loss: 0.5470, weighted_loss: 0.2928, label: 0, bag_size: 3089\n",
      "batch 119, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 25027\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 20796\n",
      "batch 159, loss: 0.0011, instance_loss: 0.0290, weighted_loss: 0.0095, label: 1, bag_size: 2495\n",
      "batch 179, loss: 0.0071, instance_loss: 0.0296, weighted_loss: 0.0138, label: 1, bag_size: 9747\n",
      "batch 199, loss: 0.2549, instance_loss: 0.0063, weighted_loss: 0.1803, label: 0, bag_size: 25420\n",
      "batch 219, loss: 0.0007, instance_loss: 0.3893, weighted_loss: 0.1173, label: 0, bag_size: 1438\n",
      "batch 239, loss: 0.0058, instance_loss: 0.0008, weighted_loss: 0.0043, label: 0, bag_size: 4997\n",
      "batch 259, loss: 0.0081, instance_loss: 0.0013, weighted_loss: 0.0060, label: 0, bag_size: 13691\n",
      "batch 279, loss: 0.0173, instance_loss: 0.0007, weighted_loss: 0.0124, label: 0, bag_size: 13378\n",
      "batch 299, loss: 0.0129, instance_loss: 0.0023, weighted_loss: 0.0098, label: 0, bag_size: 3502\n",
      "batch 319, loss: 0.0023, instance_loss: 0.0161, weighted_loss: 0.0065, label: 1, bag_size: 4308\n",
      "batch 339, loss: 0.0004, instance_loss: 0.0108, weighted_loss: 0.0035, label: 0, bag_size: 6281\n",
      "batch 359, loss: 0.0187, instance_loss: 0.0012, weighted_loss: 0.0134, label: 0, bag_size: 3810\n",
      "batch 379, loss: 0.0144, instance_loss: 0.8860, weighted_loss: 0.2759, label: 1, bag_size: 21252\n",
      "batch 399, loss: 0.0917, instance_loss: 0.0201, weighted_loss: 0.0702, label: 1, bag_size: 1683\n",
      "batch 419, loss: 0.7145, instance_loss: 0.4810, weighted_loss: 0.6444, label: 0, bag_size: 12840\n",
      "batch 439, loss: 0.0013, instance_loss: 0.0017, weighted_loss: 0.0014, label: 0, bag_size: 2063\n",
      "batch 459, loss: 0.0007, instance_loss: 0.1076, weighted_loss: 0.0328, label: 1, bag_size: 2966\n",
      "batch 479, loss: 0.0348, instance_loss: 0.0927, weighted_loss: 0.0521, label: 1, bag_size: 5723\n",
      "batch 499, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 8866\n",
      "batch 519, loss: 0.0093, instance_loss: 0.0000, weighted_loss: 0.0065, label: 1, bag_size: 1838\n",
      "batch 539, loss: 0.3037, instance_loss: 0.3523, weighted_loss: 0.3183, label: 1, bag_size: 7148\n",
      "batch 559, loss: 0.0038, instance_loss: 0.0019, weighted_loss: 0.0032, label: 1, bag_size: 10501\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0008, weighted_loss: 0.0003, label: 1, bag_size: 2904\n",
      "batch 599, loss: 0.0780, instance_loss: 0.0008, weighted_loss: 0.0548, label: 1, bag_size: 16379\n",
      "batch 619, loss: 0.1177, instance_loss: 0.0527, weighted_loss: 0.0982, label: 0, bag_size: 9616\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 9171\n",
      "batch 659, loss: 0.0070, instance_loss: 0.0204, weighted_loss: 0.0110, label: 1, bag_size: 6599\n",
      "batch 679, loss: 0.9088, instance_loss: 0.0038, weighted_loss: 0.6373, label: 0, bag_size: 13619\n",
      "batch 699, loss: 0.0026, instance_loss: 0.2070, weighted_loss: 0.0639, label: 1, bag_size: 6928\n",
      "batch 719, loss: 0.0041, instance_loss: 0.0371, weighted_loss: 0.0140, label: 1, bag_size: 3656\n",
      "batch 739, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 9786\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 1061\n",
      "batch 779, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14956\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 6851\n",
      "batch 819, loss: 0.0025, instance_loss: 0.1868, weighted_loss: 0.0578, label: 0, bag_size: 2006\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9835766423357665: correct 12936/13152\n",
      "class 1 clustering acc 0.916058394160584: correct 6024/6576\n",
      "Epoch: 73, train_loss: 0.1320, train_clustering_loss:  0.1491, train_error: 0.0523\n",
      "class 0: acc 0.9498806682577565, correct 398/419\n",
      "class 1: acc 0.9454094292803971, correct 381/403\n",
      "\n",
      "Val Set, val_loss: 0.2748, val_error: 0.0917, auc: 0.9727\n",
      "class 0 clustering acc 0.970756880733945: correct 1693/1744\n",
      "class 1 clustering acc 0.8784403669724771: correct 766/872\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5155, instance_loss: 0.4111, weighted_loss: 0.4842, label: 1, bag_size: 1764\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23996\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 518\n",
      "batch 79, loss: 0.0164, instance_loss: 0.0007, weighted_loss: 0.0117, label: 0, bag_size: 22498\n",
      "batch 99, loss: 0.1984, instance_loss: 0.0020, weighted_loss: 0.1395, label: 1, bag_size: 11220\n",
      "batch 119, loss: 0.7275, instance_loss: 0.0599, weighted_loss: 0.5273, label: 0, bag_size: 7239\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8330\n",
      "batch 159, loss: 0.9478, instance_loss: 0.0000, weighted_loss: 0.6635, label: 0, bag_size: 5211\n",
      "batch 179, loss: 0.0073, instance_loss: 0.0963, weighted_loss: 0.0340, label: 1, bag_size: 1064\n",
      "batch 199, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 18215\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0104, weighted_loss: 0.0033, label: 1, bag_size: 6016\n",
      "batch 239, loss: 0.2305, instance_loss: 2.0881, weighted_loss: 0.7878, label: 0, bag_size: 1760\n",
      "batch 259, loss: 0.0002, instance_loss: 0.0052, weighted_loss: 0.0017, label: 1, bag_size: 7381\n",
      "batch 279, loss: 0.0013, instance_loss: 0.0006, weighted_loss: 0.0011, label: 1, bag_size: 4480\n",
      "batch 299, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 21864\n",
      "batch 319, loss: 0.2387, instance_loss: 0.0037, weighted_loss: 0.1682, label: 1, bag_size: 10432\n",
      "batch 339, loss: 0.2426, instance_loss: 0.0027, weighted_loss: 0.1707, label: 1, bag_size: 1483\n",
      "batch 359, loss: 0.2181, instance_loss: 0.5375, weighted_loss: 0.3139, label: 0, bag_size: 5161\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11546\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19472\n",
      "batch 419, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 8981\n",
      "batch 439, loss: 0.0384, instance_loss: 0.0001, weighted_loss: 0.0269, label: 1, bag_size: 9322\n",
      "batch 459, loss: 0.0162, instance_loss: 0.0000, weighted_loss: 0.0113, label: 0, bag_size: 22498\n",
      "batch 479, loss: 0.0003, instance_loss: 0.0014, weighted_loss: 0.0006, label: 1, bag_size: 5894\n",
      "batch 499, loss: 0.3889, instance_loss: 1.2102, weighted_loss: 0.6353, label: 1, bag_size: 13367\n",
      "batch 519, loss: 0.0038, instance_loss: 0.0713, weighted_loss: 0.0240, label: 1, bag_size: 4423\n",
      "batch 539, loss: 0.0030, instance_loss: 0.0000, weighted_loss: 0.0021, label: 1, bag_size: 12895\n",
      "batch 559, loss: 0.0063, instance_loss: 0.2379, weighted_loss: 0.0758, label: 0, bag_size: 1234\n",
      "batch 579, loss: 0.0002, instance_loss: 0.0016, weighted_loss: 0.0006, label: 0, bag_size: 2652\n",
      "batch 599, loss: 0.0431, instance_loss: 0.1682, weighted_loss: 0.0806, label: 0, bag_size: 2098\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 10444\n",
      "batch 639, loss: 0.0000, instance_loss: 0.2026, weighted_loss: 0.0608, label: 1, bag_size: 7381\n",
      "batch 659, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 19808\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0103, weighted_loss: 0.0031, label: 0, bag_size: 2424\n",
      "batch 699, loss: 0.0041, instance_loss: 0.0000, weighted_loss: 0.0029, label: 1, bag_size: 11884\n",
      "batch 719, loss: 0.0993, instance_loss: 0.0631, weighted_loss: 0.0884, label: 1, bag_size: 4929\n",
      "batch 739, loss: 0.5570, instance_loss: 1.1361, weighted_loss: 0.7307, label: 0, bag_size: 24382\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 31106\n",
      "batch 779, loss: 0.1481, instance_loss: 0.3866, weighted_loss: 0.2196, label: 0, bag_size: 2179\n",
      "batch 799, loss: 0.0938, instance_loss: 0.0000, weighted_loss: 0.0657, label: 1, bag_size: 15125\n",
      "batch 819, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 12201\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.985933698296837: correct 12967/13152\n",
      "class 1 clustering acc 0.9222931873479319: correct 6065/6576\n",
      "Epoch: 74, train_loss: 0.1013, train_clustering_loss:  0.1341, train_error: 0.0414\n",
      "class 0: acc 0.953810623556582, correct 413/433\n",
      "class 1: acc 0.9640102827763496, correct 375/389\n",
      "\n",
      "Val Set, val_loss: 0.3197, val_error: 0.0826, auc: 0.9724\n",
      "class 0 clustering acc 0.970756880733945: correct 1693/1744\n",
      "class 1 clustering acc 0.8876146788990825: correct 774/872\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0123, instance_loss: 0.1383, weighted_loss: 0.0501, label: 0, bag_size: 1881\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 4862\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 20666\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 27158\n",
      "batch 99, loss: 0.2268, instance_loss: 0.3384, weighted_loss: 0.2603, label: 1, bag_size: 13362\n",
      "batch 119, loss: 0.2863, instance_loss: 0.6198, weighted_loss: 0.3864, label: 0, bag_size: 3321\n",
      "batch 139, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 11259\n",
      "batch 159, loss: 0.0019, instance_loss: 0.0000, weighted_loss: 0.0013, label: 1, bag_size: 30675\n",
      "batch 179, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 9949\n",
      "batch 199, loss: 0.0170, instance_loss: 0.0000, weighted_loss: 0.0119, label: 0, bag_size: 10490\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 19470\n",
      "batch 239, loss: 2.0257, instance_loss: 0.3773, weighted_loss: 1.5312, label: 1, bag_size: 13367\n",
      "batch 259, loss: 0.0101, instance_loss: 0.0002, weighted_loss: 0.0071, label: 1, bag_size: 6745\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11512\n",
      "batch 299, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 31085\n",
      "batch 319, loss: 0.9709, instance_loss: 0.1727, weighted_loss: 0.7315, label: 1, bag_size: 1038\n",
      "batch 339, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 8981\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19435\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 5833\n",
      "batch 399, loss: 0.4794, instance_loss: 0.0160, weighted_loss: 0.3404, label: 0, bag_size: 12840\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0305, weighted_loss: 0.0092, label: 1, bag_size: 14779\n",
      "batch 439, loss: 0.0255, instance_loss: 0.0013, weighted_loss: 0.0182, label: 0, bag_size: 47866\n",
      "batch 459, loss: 0.0257, instance_loss: 0.0000, weighted_loss: 0.0180, label: 0, bag_size: 10029\n",
      "batch 479, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 19808\n",
      "batch 499, loss: 0.1342, instance_loss: 0.0000, weighted_loss: 0.0939, label: 1, bag_size: 15689\n",
      "batch 519, loss: 0.0038, instance_loss: 0.0152, weighted_loss: 0.0072, label: 1, bag_size: 2412\n",
      "batch 539, loss: 0.2116, instance_loss: 0.0209, weighted_loss: 0.1544, label: 1, bag_size: 1819\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 16052\n",
      "batch 579, loss: 0.0032, instance_loss: 0.0000, weighted_loss: 0.0023, label: 0, bag_size: 13880\n",
      "batch 599, loss: 0.0006, instance_loss: 0.2341, weighted_loss: 0.0707, label: 1, bag_size: 10105\n",
      "batch 619, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 7011\n",
      "batch 639, loss: 0.0029, instance_loss: 0.0089, weighted_loss: 0.0047, label: 0, bag_size: 4523\n",
      "batch 659, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0015, label: 0, bag_size: 21032\n",
      "batch 679, loss: 0.0050, instance_loss: 0.6359, weighted_loss: 0.1943, label: 0, bag_size: 803\n",
      "batch 699, loss: 2.4527, instance_loss: 4.3176, weighted_loss: 3.0121, label: 1, bag_size: 15563\n",
      "batch 719, loss: 0.0019, instance_loss: 0.0002, weighted_loss: 0.0014, label: 1, bag_size: 29832\n",
      "batch 739, loss: 0.0069, instance_loss: 0.4946, weighted_loss: 0.1532, label: 0, bag_size: 6727\n",
      "batch 759, loss: 1.1767, instance_loss: 0.2139, weighted_loss: 0.8879, label: 1, bag_size: 2731\n",
      "batch 779, loss: 0.0058, instance_loss: 0.2772, weighted_loss: 0.0872, label: 0, bag_size: 7823\n",
      "batch 799, loss: 0.0028, instance_loss: 0.0166, weighted_loss: 0.0069, label: 1, bag_size: 4039\n",
      "batch 819, loss: 0.0011, instance_loss: 0.0067, weighted_loss: 0.0028, label: 0, bag_size: 16720\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9858576642335767: correct 12966/13152\n",
      "class 1 clustering acc 0.920316301703163: correct 6052/6576\n",
      "Epoch: 75, train_loss: 0.1112, train_clustering_loss:  0.1404, train_error: 0.0438\n",
      "class 0: acc 0.9576059850374065, correct 384/401\n",
      "class 1: acc 0.9548693586698337, correct 402/421\n",
      "\n",
      "Val Set, val_loss: 0.2500, val_error: 0.1101, auc: 0.9724\n",
      "class 0 clustering acc 0.9598623853211009: correct 1674/1744\n",
      "class 1 clustering acc 0.8589449541284404: correct 749/872\n",
      "class 0: acc 0.8478260869565217, correct 39/46\n",
      "class 1: acc 0.9206349206349206, correct 58/63\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 10791\n",
      "batch 39, loss: 0.0005, instance_loss: 0.0848, weighted_loss: 0.0258, label: 1, bag_size: 1255\n",
      "batch 59, loss: 0.0002, instance_loss: 0.0001, weighted_loss: 0.0002, label: 0, bag_size: 2652\n",
      "batch 79, loss: 0.0102, instance_loss: 0.0143, weighted_loss: 0.0114, label: 1, bag_size: 7513\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0072, weighted_loss: 0.0022, label: 1, bag_size: 2686\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 12793\n",
      "batch 139, loss: 0.1526, instance_loss: 0.4096, weighted_loss: 0.2297, label: 0, bag_size: 7917\n",
      "batch 159, loss: 0.0461, instance_loss: 0.0434, weighted_loss: 0.0453, label: 1, bag_size: 12603\n",
      "batch 179, loss: 0.0016, instance_loss: 0.3999, weighted_loss: 0.1211, label: 1, bag_size: 645\n",
      "batch 199, loss: 0.0026, instance_loss: 0.0542, weighted_loss: 0.0181, label: 1, bag_size: 1294\n",
      "batch 219, loss: 0.1359, instance_loss: 0.0189, weighted_loss: 0.1008, label: 1, bag_size: 9519\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0341, weighted_loss: 0.0102, label: 1, bag_size: 14515\n",
      "batch 259, loss: 0.1820, instance_loss: 0.1634, weighted_loss: 0.1764, label: 0, bag_size: 10063\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 1, bag_size: 4102\n",
      "batch 299, loss: 0.1153, instance_loss: 0.0278, weighted_loss: 0.0891, label: 0, bag_size: 13619\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 2424\n",
      "batch 339, loss: 0.0008, instance_loss: 0.6127, weighted_loss: 0.1844, label: 1, bag_size: 14604\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0062, weighted_loss: 0.0019, label: 1, bag_size: 699\n",
      "batch 379, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 20333\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15850\n",
      "batch 419, loss: 0.0002, instance_loss: 0.2024, weighted_loss: 0.0608, label: 0, bag_size: 1213\n",
      "batch 439, loss: 0.0044, instance_loss: 0.1390, weighted_loss: 0.0448, label: 1, bag_size: 6928\n",
      "batch 459, loss: 0.0534, instance_loss: 0.0000, weighted_loss: 0.0374, label: 1, bag_size: 10591\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 7191\n",
      "batch 499, loss: 0.0351, instance_loss: 0.0000, weighted_loss: 0.0245, label: 0, bag_size: 20478\n",
      "batch 519, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 9065\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 31106\n",
      "batch 559, loss: 0.0028, instance_loss: 0.0000, weighted_loss: 0.0019, label: 0, bag_size: 14739\n",
      "batch 579, loss: 0.0068, instance_loss: 0.6628, weighted_loss: 0.2036, label: 1, bag_size: 8003\n",
      "batch 599, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0015, label: 1, bag_size: 2936\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 699\n",
      "batch 639, loss: 0.0136, instance_loss: 0.0014, weighted_loss: 0.0099, label: 1, bag_size: 1888\n",
      "batch 659, loss: 0.0129, instance_loss: 0.0001, weighted_loss: 0.0090, label: 1, bag_size: 6927\n",
      "batch 679, loss: 0.0061, instance_loss: 0.0003, weighted_loss: 0.0044, label: 0, bag_size: 1826\n",
      "batch 699, loss: 0.3676, instance_loss: 0.0983, weighted_loss: 0.2868, label: 0, bag_size: 4241\n",
      "batch 719, loss: 0.2469, instance_loss: 0.0000, weighted_loss: 0.1728, label: 1, bag_size: 13026\n",
      "batch 739, loss: 0.0008, instance_loss: 0.0005, weighted_loss: 0.0007, label: 1, bag_size: 19832\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 12137\n",
      "batch 779, loss: 0.0005, instance_loss: 0.0064, weighted_loss: 0.0023, label: 0, bag_size: 4497\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 4271\n",
      "batch 819, loss: 0.0209, instance_loss: 0.0000, weighted_loss: 0.0147, label: 1, bag_size: 8466\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9865419708029197: correct 12975/13152\n",
      "class 1 clustering acc 0.9236618004866181: correct 6074/6576\n",
      "Epoch: 76, train_loss: 0.0928, train_clustering_loss:  0.1416, train_error: 0.0401\n",
      "class 0: acc 0.9532019704433498, correct 387/406\n",
      "class 1: acc 0.9663461538461539, correct 402/416\n",
      "\n",
      "Val Set, val_loss: 0.3628, val_error: 0.0826, auc: 0.9724\n",
      "class 0 clustering acc 0.955848623853211: correct 1667/1744\n",
      "class 1 clustering acc 0.8784403669724771: correct 766/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0318, instance_loss: 0.0375, weighted_loss: 0.0335, label: 0, bag_size: 3198\n",
      "batch 39, loss: 0.1535, instance_loss: 0.0104, weighted_loss: 0.1106, label: 0, bag_size: 21361\n",
      "batch 59, loss: 0.0237, instance_loss: 0.0018, weighted_loss: 0.0171, label: 0, bag_size: 7637\n",
      "batch 79, loss: 0.0079, instance_loss: 1.0545, weighted_loss: 0.3219, label: 0, bag_size: 11607\n",
      "batch 99, loss: 0.0004, instance_loss: 0.2556, weighted_loss: 0.0769, label: 1, bag_size: 1249\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0851, weighted_loss: 0.0256, label: 1, bag_size: 5561\n",
      "batch 139, loss: 0.0004, instance_loss: 0.1272, weighted_loss: 0.0384, label: 1, bag_size: 10394\n",
      "batch 159, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 19067\n",
      "batch 179, loss: 0.0001, instance_loss: 0.1134, weighted_loss: 0.0341, label: 0, bag_size: 2548\n",
      "batch 199, loss: 0.0009, instance_loss: 0.0067, weighted_loss: 0.0026, label: 1, bag_size: 621\n",
      "batch 219, loss: 0.2740, instance_loss: 1.6466, weighted_loss: 0.6858, label: 0, bag_size: 17279\n",
      "batch 239, loss: 0.0144, instance_loss: 0.0008, weighted_loss: 0.0103, label: 1, bag_size: 4821\n",
      "batch 259, loss: 0.5413, instance_loss: 2.3234, weighted_loss: 1.0759, label: 1, bag_size: 2937\n",
      "batch 279, loss: 0.0045, instance_loss: 0.0031, weighted_loss: 0.0041, label: 0, bag_size: 2968\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 22800\n",
      "batch 319, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 13691\n",
      "batch 339, loss: 0.0001, instance_loss: 0.1033, weighted_loss: 0.0311, label: 0, bag_size: 10263\n",
      "batch 359, loss: 0.0008, instance_loss: 0.0639, weighted_loss: 0.0197, label: 0, bag_size: 19518\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0997, weighted_loss: 0.0299, label: 1, bag_size: 617\n",
      "batch 399, loss: 0.0065, instance_loss: 0.0000, weighted_loss: 0.0046, label: 1, bag_size: 9955\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 8410\n",
      "batch 439, loss: 0.0008, instance_loss: 0.0011, weighted_loss: 0.0009, label: 1, bag_size: 1123\n",
      "batch 459, loss: 0.0048, instance_loss: 0.0000, weighted_loss: 0.0034, label: 1, bag_size: 11032\n",
      "batch 479, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 30828\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 3004\n",
      "batch 519, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 19808\n",
      "batch 539, loss: 0.0010, instance_loss: 0.7514, weighted_loss: 0.2261, label: 1, bag_size: 3619\n",
      "batch 559, loss: 0.0048, instance_loss: 0.0000, weighted_loss: 0.0034, label: 0, bag_size: 17083\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0303, weighted_loss: 0.0091, label: 0, bag_size: 3265\n",
      "batch 599, loss: 0.0134, instance_loss: 0.0156, weighted_loss: 0.0141, label: 0, bag_size: 2160\n",
      "batch 619, loss: 0.1085, instance_loss: 0.0000, weighted_loss: 0.0759, label: 1, bag_size: 11160\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9759\n",
      "batch 659, loss: 0.0967, instance_loss: 0.0050, weighted_loss: 0.0692, label: 0, bag_size: 26208\n",
      "batch 679, loss: 0.0022, instance_loss: 0.0065, weighted_loss: 0.0035, label: 0, bag_size: 1202\n",
      "batch 699, loss: 0.0009, instance_loss: 0.0245, weighted_loss: 0.0080, label: 0, bag_size: 2624\n",
      "batch 719, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 22264\n",
      "batch 739, loss: 0.0005, instance_loss: 0.7317, weighted_loss: 0.2198, label: 1, bag_size: 1755\n",
      "batch 759, loss: 0.0004, instance_loss: 0.0912, weighted_loss: 0.0277, label: 0, bag_size: 2195\n",
      "batch 779, loss: 0.0062, instance_loss: 0.1469, weighted_loss: 0.0484, label: 1, bag_size: 1242\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0188, weighted_loss: 0.0057, label: 1, bag_size: 617\n",
      "batch 819, loss: 0.0918, instance_loss: 0.5982, weighted_loss: 0.2437, label: 1, bag_size: 1703\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9854014598540146: correct 12960/13152\n",
      "class 1 clustering acc 0.9152980535279805: correct 6019/6576\n",
      "Epoch: 77, train_loss: 0.1096, train_clustering_loss:  0.1551, train_error: 0.0450\n",
      "class 0: acc 0.9539951573849879, correct 394/413\n",
      "class 1: acc 0.9559902200488998, correct 391/409\n",
      "\n",
      "Val Set, val_loss: 0.2554, val_error: 0.1101, auc: 0.9738\n",
      "class 0 clustering acc 0.9524082568807339: correct 1661/1744\n",
      "class 1 clustering acc 0.6376146788990825: correct 556/872\n",
      "class 0: acc 0.8478260869565217, correct 39/46\n",
      "class 1: acc 0.9206349206349206, correct 58/63\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.1839, weighted_loss: 0.0552, label: 1, bag_size: 4442\n",
      "batch 39, loss: 0.0046, instance_loss: 0.2101, weighted_loss: 0.0662, label: 1, bag_size: 7119\n",
      "batch 59, loss: 0.0015, instance_loss: 0.0033, weighted_loss: 0.0020, label: 1, bag_size: 14202\n",
      "batch 79, loss: 0.0698, instance_loss: 0.7036, weighted_loss: 0.2599, label: 0, bag_size: 14264\n",
      "batch 99, loss: 0.0004, instance_loss: 0.1682, weighted_loss: 0.0508, label: 0, bag_size: 1814\n",
      "batch 119, loss: 0.0001, instance_loss: 0.2208, weighted_loss: 0.0663, label: 0, bag_size: 1825\n",
      "batch 139, loss: 0.0008, instance_loss: 0.0209, weighted_loss: 0.0068, label: 0, bag_size: 11146\n",
      "batch 159, loss: 0.9520, instance_loss: 0.0648, weighted_loss: 0.6859, label: 0, bag_size: 4241\n",
      "batch 179, loss: 0.0005, instance_loss: 0.0345, weighted_loss: 0.0107, label: 1, bag_size: 7935\n",
      "batch 199, loss: 0.0025, instance_loss: 0.0025, weighted_loss: 0.0025, label: 1, bag_size: 5110\n",
      "batch 219, loss: 0.0046, instance_loss: 0.0165, weighted_loss: 0.0082, label: 0, bag_size: 10029\n",
      "batch 239, loss: 0.0362, instance_loss: 0.0014, weighted_loss: 0.0258, label: 0, bag_size: 12899\n",
      "batch 259, loss: 0.0822, instance_loss: 0.0002, weighted_loss: 0.0576, label: 1, bag_size: 6736\n",
      "batch 279, loss: 0.0132, instance_loss: 0.3430, weighted_loss: 0.1122, label: 1, bag_size: 7932\n",
      "batch 299, loss: 0.0048, instance_loss: 0.0065, weighted_loss: 0.0053, label: 0, bag_size: 2360\n",
      "batch 319, loss: 0.0000, instance_loss: 0.1493, weighted_loss: 0.0448, label: 0, bag_size: 21082\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0047, weighted_loss: 0.0014, label: 0, bag_size: 9885\n",
      "batch 359, loss: 0.0133, instance_loss: 0.0000, weighted_loss: 0.0093, label: 0, bag_size: 19880\n",
      "batch 379, loss: 0.0797, instance_loss: 0.0100, weighted_loss: 0.0588, label: 0, bag_size: 11390\n",
      "batch 399, loss: 0.1829, instance_loss: 0.5895, weighted_loss: 0.3048, label: 1, bag_size: 1683\n",
      "batch 419, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 15313\n",
      "batch 439, loss: 0.0017, instance_loss: 0.1483, weighted_loss: 0.0457, label: 1, bag_size: 3576\n",
      "batch 459, loss: 0.0560, instance_loss: 0.3301, weighted_loss: 0.1382, label: 1, bag_size: 4308\n",
      "batch 479, loss: 0.0009, instance_loss: 0.0667, weighted_loss: 0.0206, label: 1, bag_size: 5894\n",
      "batch 499, loss: 0.1359, instance_loss: 0.2008, weighted_loss: 0.1553, label: 0, bag_size: 2179\n",
      "batch 519, loss: 0.0086, instance_loss: 0.0008, weighted_loss: 0.0063, label: 1, bag_size: 7217\n",
      "batch 539, loss: 0.0168, instance_loss: 0.5203, weighted_loss: 0.1679, label: 0, bag_size: 3101\n",
      "batch 559, loss: 0.0030, instance_loss: 0.0805, weighted_loss: 0.0262, label: 1, bag_size: 5441\n",
      "batch 579, loss: 0.0064, instance_loss: 0.1264, weighted_loss: 0.0424, label: 1, bag_size: 1924\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11266\n",
      "batch 619, loss: 0.0062, instance_loss: 0.1105, weighted_loss: 0.0375, label: 1, bag_size: 8754\n",
      "batch 639, loss: 0.1171, instance_loss: 0.0152, weighted_loss: 0.0865, label: 0, bag_size: 7835\n",
      "batch 659, loss: 0.0130, instance_loss: 0.0000, weighted_loss: 0.0091, label: 1, bag_size: 7445\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0530, weighted_loss: 0.0159, label: 1, bag_size: 3937\n",
      "batch 699, loss: 0.0056, instance_loss: 0.0000, weighted_loss: 0.0039, label: 1, bag_size: 7583\n",
      "batch 719, loss: 0.0010, instance_loss: 0.0565, weighted_loss: 0.0177, label: 1, bag_size: 2695\n",
      "batch 739, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11113\n",
      "batch 759, loss: 0.0002, instance_loss: 0.0014, weighted_loss: 0.0006, label: 1, bag_size: 2193\n",
      "batch 779, loss: 0.0057, instance_loss: 0.0244, weighted_loss: 0.0113, label: 0, bag_size: 13205\n",
      "batch 799, loss: 0.3902, instance_loss: 1.1897, weighted_loss: 0.6301, label: 1, bag_size: 21252\n",
      "batch 819, loss: 0.0375, instance_loss: 0.4928, weighted_loss: 0.1741, label: 1, bag_size: 1191\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9815237226277372: correct 12909/13152\n",
      "class 1 clustering acc 0.892183698296837: correct 5867/6576\n",
      "Epoch: 78, train_loss: 0.0964, train_clustering_loss:  0.1843, train_error: 0.0353\n",
      "class 0: acc 0.9543147208121827, correct 376/394\n",
      "class 1: acc 0.9742990654205608, correct 417/428\n",
      "\n",
      "Val Set, val_loss: 0.2918, val_error: 0.1009, auc: 0.9727\n",
      "class 0 clustering acc 0.9581422018348624: correct 1671/1744\n",
      "class 1 clustering acc 0.8302752293577982: correct 724/872\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0907, instance_loss: 0.0911, weighted_loss: 0.0908, label: 1, bag_size: 1786\n",
      "batch 39, loss: 0.0011, instance_loss: 0.3357, weighted_loss: 0.1015, label: 0, bag_size: 2624\n",
      "batch 59, loss: 0.0014, instance_loss: 0.3154, weighted_loss: 0.0956, label: 1, bag_size: 1437\n",
      "batch 79, loss: 0.0004, instance_loss: 0.0644, weighted_loss: 0.0196, label: 0, bag_size: 9949\n",
      "batch 99, loss: 0.0030, instance_loss: 0.0846, weighted_loss: 0.0275, label: 1, bag_size: 8680\n",
      "batch 119, loss: 0.1119, instance_loss: 0.0014, weighted_loss: 0.0787, label: 1, bag_size: 5903\n",
      "batch 139, loss: 0.0405, instance_loss: 0.1130, weighted_loss: 0.0623, label: 0, bag_size: 3783\n",
      "batch 159, loss: 0.0318, instance_loss: 0.3571, weighted_loss: 0.1294, label: 1, bag_size: 4929\n",
      "batch 179, loss: 2.0209, instance_loss: 0.6193, weighted_loss: 1.6004, label: 0, bag_size: 9421\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 3541\n",
      "batch 219, loss: 0.1275, instance_loss: 0.0523, weighted_loss: 0.1050, label: 1, bag_size: 4054\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8948\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19435\n",
      "batch 279, loss: 0.0000, instance_loss: 0.9899, weighted_loss: 0.2970, label: 1, bag_size: 2405\n",
      "batch 299, loss: 0.0782, instance_loss: 0.0467, weighted_loss: 0.0687, label: 0, bag_size: 7557\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0382, weighted_loss: 0.0115, label: 0, bag_size: 11727\n",
      "batch 339, loss: 0.4202, instance_loss: 0.1180, weighted_loss: 0.3296, label: 1, bag_size: 2565\n",
      "batch 359, loss: 0.0059, instance_loss: 0.0047, weighted_loss: 0.0055, label: 1, bag_size: 8438\n",
      "batch 379, loss: 0.0011, instance_loss: 0.0051, weighted_loss: 0.0023, label: 1, bag_size: 6731\n",
      "batch 399, loss: 0.0005, instance_loss: 0.0005, weighted_loss: 0.0005, label: 1, bag_size: 12697\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 10146\n",
      "batch 439, loss: 0.2271, instance_loss: 1.5972, weighted_loss: 0.6381, label: 1, bag_size: 15192\n",
      "batch 459, loss: 0.0788, instance_loss: 0.3340, weighted_loss: 0.1554, label: 1, bag_size: 13362\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0172, weighted_loss: 0.0052, label: 0, bag_size: 16992\n",
      "batch 499, loss: 0.0244, instance_loss: 0.3798, weighted_loss: 0.1310, label: 1, bag_size: 4786\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21576\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 6752\n",
      "batch 559, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 10146\n",
      "batch 579, loss: 0.8617, instance_loss: 0.0140, weighted_loss: 0.6074, label: 0, bag_size: 11212\n",
      "batch 599, loss: 0.0980, instance_loss: 0.3425, weighted_loss: 0.1714, label: 1, bag_size: 10848\n",
      "batch 619, loss: 0.0034, instance_loss: 0.0004, weighted_loss: 0.0025, label: 1, bag_size: 4880\n",
      "batch 639, loss: 0.0006, instance_loss: 0.0854, weighted_loss: 0.0260, label: 0, bag_size: 2457\n",
      "batch 659, loss: 0.1480, instance_loss: 2.0434, weighted_loss: 0.7166, label: 0, bag_size: 2458\n",
      "batch 679, loss: 0.0001, instance_loss: 0.2818, weighted_loss: 0.0846, label: 1, bag_size: 5991\n",
      "batch 699, loss: 0.0025, instance_loss: 0.0000, weighted_loss: 0.0018, label: 1, bag_size: 19606\n",
      "batch 719, loss: 0.0022, instance_loss: 0.0111, weighted_loss: 0.0049, label: 1, bag_size: 4330\n",
      "batch 739, loss: 1.3516, instance_loss: 0.6878, weighted_loss: 1.1525, label: 0, bag_size: 1127\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0457, weighted_loss: 0.0137, label: 0, bag_size: 25558\n",
      "batch 779, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 12931\n",
      "batch 799, loss: 0.0239, instance_loss: 1.3851, weighted_loss: 0.4322, label: 1, bag_size: 16514\n",
      "batch 819, loss: 0.1392, instance_loss: 1.8599, weighted_loss: 0.6554, label: 1, bag_size: 3121\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9768096107055961: correct 12847/13152\n",
      "class 1 clustering acc 0.8759124087591241: correct 5760/6576\n",
      "Epoch: 79, train_loss: 0.1281, train_clustering_loss:  0.2090, train_error: 0.0584\n",
      "class 0: acc 0.9394673123486683, correct 388/413\n",
      "class 1: acc 0.9437652811735942, correct 386/409\n",
      "\n",
      "Val Set, val_loss: 0.3213, val_error: 0.0826, auc: 0.9741\n",
      "class 0 clustering acc 0.9524082568807339: correct 1661/1744\n",
      "class 1 clustering acc 0.8073394495412844: correct 704/872\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0377, weighted_loss: 0.0113, label: 0, bag_size: 19472\n",
      "batch 39, loss: 0.0115, instance_loss: 0.0416, weighted_loss: 0.0205, label: 1, bag_size: 4786\n",
      "batch 59, loss: 0.0327, instance_loss: 0.0996, weighted_loss: 0.0528, label: 0, bag_size: 4959\n",
      "batch 79, loss: 0.0022, instance_loss: 0.0528, weighted_loss: 0.0174, label: 0, bag_size: 1826\n",
      "batch 99, loss: 0.0006, instance_loss: 0.0006, weighted_loss: 0.0006, label: 1, bag_size: 8685\n",
      "batch 119, loss: 0.0010, instance_loss: 0.0003, weighted_loss: 0.0008, label: 0, bag_size: 2748\n",
      "batch 139, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 12593\n",
      "batch 159, loss: 0.0020, instance_loss: 0.0399, weighted_loss: 0.0134, label: 0, bag_size: 1831\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 16341\n",
      "batch 199, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 7935\n",
      "batch 219, loss: 0.0021, instance_loss: 0.0776, weighted_loss: 0.0248, label: 0, bag_size: 1483\n",
      "batch 239, loss: 0.0014, instance_loss: 0.0003, weighted_loss: 0.0010, label: 1, bag_size: 3640\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 22264\n",
      "batch 279, loss: 0.0163, instance_loss: 0.2162, weighted_loss: 0.0763, label: 0, bag_size: 10721\n",
      "batch 299, loss: 0.0003, instance_loss: 0.4214, weighted_loss: 0.1266, label: 0, bag_size: 12201\n",
      "batch 319, loss: 0.0561, instance_loss: 0.2475, weighted_loss: 0.1135, label: 1, bag_size: 16514\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0870, weighted_loss: 0.0262, label: 0, bag_size: 14377\n",
      "batch 359, loss: 0.0188, instance_loss: 0.8441, weighted_loss: 0.2664, label: 0, bag_size: 6093\n",
      "batch 379, loss: 0.0091, instance_loss: 0.1290, weighted_loss: 0.0450, label: 1, bag_size: 4821\n",
      "batch 399, loss: 0.0302, instance_loss: 0.0000, weighted_loss: 0.0211, label: 0, bag_size: 3238\n",
      "batch 419, loss: 0.1104, instance_loss: 0.0481, weighted_loss: 0.0917, label: 1, bag_size: 9330\n",
      "batch 439, loss: 0.0027, instance_loss: 0.0704, weighted_loss: 0.0230, label: 0, bag_size: 1909\n",
      "batch 459, loss: 0.0004, instance_loss: 0.0003, weighted_loss: 0.0004, label: 1, bag_size: 14223\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0637, weighted_loss: 0.0192, label: 0, bag_size: 11146\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 10995\n",
      "batch 519, loss: 0.0008, instance_loss: 0.0027, weighted_loss: 0.0014, label: 0, bag_size: 7235\n",
      "batch 539, loss: 0.0016, instance_loss: 0.4030, weighted_loss: 0.1220, label: 0, bag_size: 6281\n",
      "batch 559, loss: 0.0012, instance_loss: 0.0156, weighted_loss: 0.0056, label: 1, bag_size: 2308\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19472\n",
      "batch 599, loss: 0.0003, instance_loss: 0.0001, weighted_loss: 0.0002, label: 0, bag_size: 21874\n",
      "batch 619, loss: 0.0211, instance_loss: 0.3281, weighted_loss: 0.1132, label: 0, bag_size: 22498\n",
      "batch 639, loss: 0.4775, instance_loss: 1.0622, weighted_loss: 0.6529, label: 0, bag_size: 7239\n",
      "batch 659, loss: 0.0160, instance_loss: 0.0463, weighted_loss: 0.0251, label: 0, bag_size: 25420\n",
      "batch 679, loss: 0.1306, instance_loss: 0.1342, weighted_loss: 0.1317, label: 0, bag_size: 2179\n",
      "batch 699, loss: 0.0023, instance_loss: 0.0735, weighted_loss: 0.0236, label: 1, bag_size: 2559\n",
      "batch 719, loss: 0.3027, instance_loss: 0.0870, weighted_loss: 0.2380, label: 0, bag_size: 14264\n",
      "batch 739, loss: 0.0003, instance_loss: 0.3897, weighted_loss: 0.1171, label: 1, bag_size: 3651\n",
      "batch 759, loss: 0.0040, instance_loss: 0.0000, weighted_loss: 0.0028, label: 0, bag_size: 11259\n",
      "batch 779, loss: 0.0020, instance_loss: 0.0000, weighted_loss: 0.0014, label: 0, bag_size: 11194\n",
      "batch 799, loss: 0.5944, instance_loss: 0.7373, weighted_loss: 0.6373, label: 0, bag_size: 14664\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21082\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9841088807785888: correct 12943/13152\n",
      "class 1 clustering acc 0.9137773722627737: correct 6009/6576\n",
      "Epoch: 80, train_loss: 0.1067, train_clustering_loss:  0.1571, train_error: 0.0438\n",
      "class 0: acc 0.9484029484029484, correct 386/407\n",
      "class 1: acc 0.963855421686747, correct 400/415\n",
      "\n",
      "Val Set, val_loss: 0.3251, val_error: 0.0917, auc: 0.9741\n",
      "class 0 clustering acc 0.9627293577981652: correct 1679/1744\n",
      "class 1 clustering acc 0.8646788990825688: correct 754/872\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0216, instance_loss: 0.0010, weighted_loss: 0.0155, label: 0, bag_size: 1920\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 12149\n",
      "batch 59, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 8866\n",
      "batch 79, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 13205\n",
      "batch 99, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 21864\n",
      "batch 119, loss: 0.0042, instance_loss: 0.0000, weighted_loss: 0.0030, label: 0, bag_size: 12148\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0510, weighted_loss: 0.0154, label: 0, bag_size: 2748\n",
      "batch 159, loss: 0.0173, instance_loss: 0.6765, weighted_loss: 0.2151, label: 1, bag_size: 3450\n",
      "batch 179, loss: 0.0020, instance_loss: 0.0322, weighted_loss: 0.0111, label: 0, bag_size: 1814\n",
      "batch 199, loss: 0.0018, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 13205\n",
      "batch 219, loss: 0.0003, instance_loss: 0.0102, weighted_loss: 0.0033, label: 0, bag_size: 7823\n",
      "batch 239, loss: 0.0127, instance_loss: 0.0269, weighted_loss: 0.0170, label: 1, bag_size: 1123\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 3232\n",
      "batch 279, loss: 0.0031, instance_loss: 0.0412, weighted_loss: 0.0146, label: 0, bag_size: 6093\n",
      "batch 299, loss: 0.0517, instance_loss: 0.0160, weighted_loss: 0.0410, label: 1, bag_size: 6171\n",
      "batch 319, loss: 0.0020, instance_loss: 0.2176, weighted_loss: 0.0667, label: 1, bag_size: 10396\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0012, weighted_loss: 0.0004, label: 1, bag_size: 4877\n",
      "batch 359, loss: 0.4775, instance_loss: 0.0140, weighted_loss: 0.3384, label: 0, bag_size: 3468\n",
      "batch 379, loss: 0.0012, instance_loss: 0.3048, weighted_loss: 0.0923, label: 1, bag_size: 8040\n",
      "batch 399, loss: 0.0114, instance_loss: 0.0000, weighted_loss: 0.0080, label: 0, bag_size: 20230\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 16341\n",
      "batch 439, loss: 0.0661, instance_loss: 0.2820, weighted_loss: 0.1309, label: 0, bag_size: 9596\n",
      "batch 459, loss: 0.0023, instance_loss: 0.2586, weighted_loss: 0.0792, label: 0, bag_size: 1416\n",
      "batch 479, loss: 0.0305, instance_loss: 0.0350, weighted_loss: 0.0318, label: 0, bag_size: 24382\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0041, weighted_loss: 0.0013, label: 0, bag_size: 26271\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 17633\n",
      "batch 539, loss: 0.0189, instance_loss: 0.0188, weighted_loss: 0.0189, label: 1, bag_size: 1888\n",
      "batch 559, loss: 0.0492, instance_loss: 0.6583, weighted_loss: 0.2319, label: 1, bag_size: 16703\n",
      "batch 579, loss: 0.0148, instance_loss: 0.1024, weighted_loss: 0.0410, label: 0, bag_size: 2282\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0039, weighted_loss: 0.0012, label: 0, bag_size: 2534\n",
      "batch 619, loss: 0.0007, instance_loss: 0.0054, weighted_loss: 0.0021, label: 1, bag_size: 12795\n",
      "batch 639, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 10392\n",
      "batch 659, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 14618\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 8959\n",
      "batch 699, loss: 0.0164, instance_loss: 0.1079, weighted_loss: 0.0439, label: 0, bag_size: 6093\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 18574\n",
      "batch 739, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 4497\n",
      "batch 759, loss: 0.0001, instance_loss: 0.0006, weighted_loss: 0.0003, label: 0, bag_size: 16052\n",
      "batch 779, loss: 0.0383, instance_loss: 0.0070, weighted_loss: 0.0289, label: 1, bag_size: 4054\n",
      "batch 799, loss: 0.0018, instance_loss: 0.3160, weighted_loss: 0.0961, label: 0, bag_size: 1234\n",
      "batch 819, loss: 0.2107, instance_loss: 0.0401, weighted_loss: 0.1595, label: 1, bag_size: 9322\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9892031630170316: correct 13010/13152\n",
      "class 1 clustering acc 0.9362834549878345: correct 6157/6576\n",
      "Epoch: 81, train_loss: 0.0874, train_clustering_loss:  0.1135, train_error: 0.0231\n",
      "class 0: acc 0.9741784037558685, correct 415/426\n",
      "class 1: acc 0.9797979797979798, correct 388/396\n",
      "\n",
      "Val Set, val_loss: 0.4668, val_error: 0.1009, auc: 0.9741\n",
      "class 0 clustering acc 0.9644495412844036: correct 1682/1744\n",
      "class 1 clustering acc 0.8727064220183486: correct 761/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.8253968253968254, correct 52/63\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0017, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 18954\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23714\n",
      "batch 59, loss: 0.0001, instance_loss: 1.8423, weighted_loss: 0.5528, label: 1, bag_size: 10105\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 4959\n",
      "batch 99, loss: 1.1276, instance_loss: 0.0000, weighted_loss: 0.7893, label: 0, bag_size: 20478\n",
      "batch 119, loss: 0.0093, instance_loss: 0.0000, weighted_loss: 0.0065, label: 1, bag_size: 21009\n",
      "batch 139, loss: 0.0041, instance_loss: 1.5832, weighted_loss: 0.4778, label: 1, bag_size: 3450\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 16087\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 32227\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0042, weighted_loss: 0.0013, label: 1, bag_size: 2966\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 2322\n",
      "batch 239, loss: 0.0005, instance_loss: 0.0150, weighted_loss: 0.0048, label: 1, bag_size: 2790\n",
      "batch 259, loss: 0.0608, instance_loss: 2.1604, weighted_loss: 0.6907, label: 0, bag_size: 47866\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0427, weighted_loss: 0.0129, label: 0, bag_size: 13691\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 25558\n",
      "batch 319, loss: 0.0000, instance_loss: 0.1628, weighted_loss: 0.0489, label: 1, bag_size: 1255\n",
      "batch 339, loss: 0.1275, instance_loss: 0.7393, weighted_loss: 0.3111, label: 1, bag_size: 1051\n",
      "batch 359, loss: 0.0138, instance_loss: 0.0293, weighted_loss: 0.0185, label: 1, bag_size: 1015\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 5612\n",
      "batch 399, loss: 0.5295, instance_loss: 0.0099, weighted_loss: 0.3736, label: 0, bag_size: 7428\n",
      "batch 419, loss: 0.0148, instance_loss: 0.0000, weighted_loss: 0.0104, label: 0, bag_size: 25814\n",
      "batch 439, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 2873\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 16782\n",
      "batch 479, loss: 0.0056, instance_loss: 0.0000, weighted_loss: 0.0039, label: 0, bag_size: 2282\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 26271\n",
      "batch 519, loss: 0.0078, instance_loss: 0.0229, weighted_loss: 0.0123, label: 1, bag_size: 1822\n",
      "batch 539, loss: 0.0421, instance_loss: 0.2419, weighted_loss: 0.1021, label: 1, bag_size: 2480\n",
      "batch 559, loss: 0.0096, instance_loss: 1.0201, weighted_loss: 0.3127, label: 1, bag_size: 25695\n",
      "batch 579, loss: 0.0125, instance_loss: 0.0291, weighted_loss: 0.0175, label: 0, bag_size: 4845\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0304, weighted_loss: 0.0092, label: 1, bag_size: 5731\n",
      "batch 619, loss: 0.0001, instance_loss: 0.1095, weighted_loss: 0.0329, label: 0, bag_size: 3265\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0480, weighted_loss: 0.0144, label: 1, bag_size: 1743\n",
      "batch 659, loss: 0.3646, instance_loss: 0.1383, weighted_loss: 0.2967, label: 0, bag_size: 14264\n",
      "batch 679, loss: 0.0015, instance_loss: 0.0060, weighted_loss: 0.0029, label: 1, bag_size: 2695\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 5612\n",
      "batch 719, loss: 0.0193, instance_loss: 0.2465, weighted_loss: 0.0875, label: 0, bag_size: 4418\n",
      "batch 739, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 2548\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 17633\n",
      "batch 779, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 12212\n",
      "batch 799, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 11884\n",
      "batch 819, loss: 0.0096, instance_loss: 0.0173, weighted_loss: 0.0119, label: 1, bag_size: 20767\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9863138686131386: correct 12972/13152\n",
      "class 1 clustering acc 0.9227493917274939: correct 6068/6576\n",
      "Epoch: 82, train_loss: 0.0757, train_clustering_loss:  0.1341, train_error: 0.0280\n",
      "class 0: acc 0.9635036496350365, correct 396/411\n",
      "class 1: acc 0.9805352798053528, correct 403/411\n",
      "\n",
      "Val Set, val_loss: 0.3608, val_error: 0.0826, auc: 0.9755\n",
      "class 0 clustering acc 0.9512614678899083: correct 1659/1744\n",
      "class 1 clustering acc 0.8715596330275229: correct 760/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1373, instance_loss: 0.7152, weighted_loss: 0.3106, label: 1, bag_size: 11223\n",
      "batch 39, loss: 0.3470, instance_loss: 0.0000, weighted_loss: 0.2429, label: 1, bag_size: 10622\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0503, weighted_loss: 0.0151, label: 1, bag_size: 3453\n",
      "batch 79, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 18954\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0041, weighted_loss: 0.0012, label: 1, bag_size: 5833\n",
      "batch 119, loss: 0.0077, instance_loss: 0.0000, weighted_loss: 0.0054, label: 0, bag_size: 9596\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9471\n",
      "batch 159, loss: 0.0004, instance_loss: 0.2713, weighted_loss: 0.0816, label: 1, bag_size: 12095\n",
      "batch 179, loss: 0.0042, instance_loss: 0.0388, weighted_loss: 0.0146, label: 0, bag_size: 12731\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0054, weighted_loss: 0.0016, label: 1, bag_size: 1412\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 31780\n",
      "batch 239, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 2179\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11917\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19390\n",
      "batch 299, loss: 0.6375, instance_loss: 0.1377, weighted_loss: 0.4876, label: 0, bag_size: 1127\n",
      "batch 319, loss: 0.0090, instance_loss: 0.2706, weighted_loss: 0.0875, label: 1, bag_size: 2785\n",
      "batch 339, loss: 0.0598, instance_loss: 0.1359, weighted_loss: 0.0826, label: 1, bag_size: 8026\n",
      "batch 359, loss: 0.3648, instance_loss: 0.2397, weighted_loss: 0.3273, label: 0, bag_size: 1437\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 6769\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 3004\n",
      "batch 419, loss: 0.0049, instance_loss: 0.0000, weighted_loss: 0.0034, label: 0, bag_size: 5999\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0026, weighted_loss: 0.0008, label: 1, bag_size: 8935\n",
      "batch 459, loss: 0.0003, instance_loss: 0.0375, weighted_loss: 0.0114, label: 1, bag_size: 5441\n",
      "batch 479, loss: 0.2117, instance_loss: 0.0702, weighted_loss: 0.1693, label: 1, bag_size: 12575\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0732, weighted_loss: 0.0220, label: 1, bag_size: 3207\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 10263\n",
      "batch 539, loss: 0.0140, instance_loss: 0.1382, weighted_loss: 0.0513, label: 1, bag_size: 7468\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0013, weighted_loss: 0.0004, label: 1, bag_size: 13368\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0008, weighted_loss: 0.0003, label: 1, bag_size: 4959\n",
      "batch 599, loss: 0.0029, instance_loss: 0.0014, weighted_loss: 0.0024, label: 1, bag_size: 8191\n",
      "batch 619, loss: 1.4481, instance_loss: 2.4379, weighted_loss: 1.7450, label: 1, bag_size: 2731\n",
      "batch 639, loss: 0.0500, instance_loss: 0.0000, weighted_loss: 0.0350, label: 0, bag_size: 15672\n",
      "batch 659, loss: 0.0002, instance_loss: 0.0137, weighted_loss: 0.0043, label: 0, bag_size: 12687\n",
      "batch 679, loss: 0.0058, instance_loss: 0.0990, weighted_loss: 0.0337, label: 0, bag_size: 12731\n",
      "batch 699, loss: 0.0291, instance_loss: 0.0130, weighted_loss: 0.0243, label: 1, bag_size: 9519\n",
      "batch 719, loss: 0.0001, instance_loss: 0.5715, weighted_loss: 0.1716, label: 0, bag_size: 2820\n",
      "batch 739, loss: 0.3015, instance_loss: 0.0345, weighted_loss: 0.2214, label: 1, bag_size: 1230\n",
      "batch 759, loss: 0.0131, instance_loss: 0.1671, weighted_loss: 0.0593, label: 1, bag_size: 7424\n",
      "batch 779, loss: 0.2224, instance_loss: 0.0013, weighted_loss: 0.1560, label: 1, bag_size: 13362\n",
      "batch 799, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 6731\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 13225\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9878345498783455: correct 12992/13152\n",
      "class 1 clustering acc 0.9338503649635036: correct 6141/6576\n",
      "Epoch: 83, train_loss: 0.0705, train_clustering_loss:  0.1137, train_error: 0.0292\n",
      "class 0: acc 0.9770642201834863, correct 426/436\n",
      "class 1: acc 0.9637305699481865, correct 372/386\n",
      "\n",
      "Val Set, val_loss: 0.2924, val_error: 0.1009, auc: 0.9748\n",
      "class 0 clustering acc 0.9518348623853211: correct 1660/1744\n",
      "class 1 clustering acc 0.8543577981651376: correct 745/872\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23398\n",
      "batch 39, loss: 0.0083, instance_loss: 0.0637, weighted_loss: 0.0249, label: 1, bag_size: 2480\n",
      "batch 59, loss: 0.0521, instance_loss: 0.1796, weighted_loss: 0.0904, label: 1, bag_size: 1339\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11477\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23037\n",
      "batch 119, loss: 0.0404, instance_loss: 0.0310, weighted_loss: 0.0376, label: 0, bag_size: 9597\n",
      "batch 139, loss: 0.0097, instance_loss: 0.0256, weighted_loss: 0.0145, label: 1, bag_size: 3211\n",
      "batch 159, loss: 0.4856, instance_loss: 0.0067, weighted_loss: 0.3419, label: 1, bag_size: 9215\n",
      "batch 179, loss: 0.0065, instance_loss: 0.0000, weighted_loss: 0.0046, label: 0, bag_size: 13378\n",
      "batch 199, loss: 0.0293, instance_loss: 0.0007, weighted_loss: 0.0207, label: 1, bag_size: 7873\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 2036\n",
      "batch 239, loss: 0.0042, instance_loss: 0.0000, weighted_loss: 0.0030, label: 0, bag_size: 3710\n",
      "batch 259, loss: 0.0095, instance_loss: 0.5819, weighted_loss: 0.1813, label: 0, bag_size: 803\n",
      "batch 279, loss: 3.6727, instance_loss: 0.4871, weighted_loss: 2.7170, label: 0, bag_size: 2815\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0013, weighted_loss: 0.0005, label: 1, bag_size: 6734\n",
      "batch 319, loss: 0.0030, instance_loss: 0.0083, weighted_loss: 0.0046, label: 1, bag_size: 9230\n",
      "batch 339, loss: 0.0013, instance_loss: 0.1615, weighted_loss: 0.0494, label: 1, bag_size: 1014\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0410, weighted_loss: 0.0123, label: 1, bag_size: 17486\n",
      "batch 379, loss: 0.0113, instance_loss: 0.1541, weighted_loss: 0.0541, label: 1, bag_size: 1572\n",
      "batch 399, loss: 0.0065, instance_loss: 0.3775, weighted_loss: 0.1178, label: 0, bag_size: 19880\n",
      "batch 419, loss: 0.0005, instance_loss: 0.0009, weighted_loss: 0.0006, label: 0, bag_size: 15003\n",
      "batch 439, loss: 0.0041, instance_loss: 0.2494, weighted_loss: 0.0777, label: 1, bag_size: 25695\n",
      "batch 459, loss: 0.0000, instance_loss: 0.2178, weighted_loss: 0.0654, label: 1, bag_size: 3980\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0847, weighted_loss: 0.0254, label: 0, bag_size: 11527\n",
      "batch 499, loss: 0.0370, instance_loss: 0.0123, weighted_loss: 0.0296, label: 0, bag_size: 11390\n",
      "batch 519, loss: 0.0009, instance_loss: 0.0995, weighted_loss: 0.0305, label: 0, bag_size: 4523\n",
      "batch 539, loss: 0.0636, instance_loss: 0.0946, weighted_loss: 0.0729, label: 1, bag_size: 10622\n",
      "batch 559, loss: 0.0023, instance_loss: 0.6770, weighted_loss: 0.2047, label: 0, bag_size: 6884\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 518\n",
      "batch 599, loss: 0.0016, instance_loss: 0.0084, weighted_loss: 0.0036, label: 1, bag_size: 2638\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14266\n",
      "batch 639, loss: 0.0005, instance_loss: 0.3471, weighted_loss: 0.1045, label: 0, bag_size: 1438\n",
      "batch 659, loss: 0.0004, instance_loss: 0.0007, weighted_loss: 0.0005, label: 1, bag_size: 7246\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23714\n",
      "batch 699, loss: 0.0132, instance_loss: 0.0463, weighted_loss: 0.0231, label: 1, bag_size: 7513\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 22800\n",
      "batch 739, loss: 0.0683, instance_loss: 1.1981, weighted_loss: 0.4073, label: 0, bag_size: 10410\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0187, weighted_loss: 0.0056, label: 1, bag_size: 3557\n",
      "batch 779, loss: 0.0000, instance_loss: 0.1440, weighted_loss: 0.0432, label: 1, bag_size: 14515\n",
      "batch 799, loss: 0.9113, instance_loss: 0.3863, weighted_loss: 0.7538, label: 0, bag_size: 3802\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0193, weighted_loss: 0.0058, label: 0, bag_size: 2748\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.976581508515815: correct 12844/13152\n",
      "class 1 clustering acc 0.8698296836982968: correct 5720/6576\n",
      "Epoch: 84, train_loss: 0.1200, train_clustering_loss:  0.2363, train_error: 0.0438\n",
      "class 0: acc 0.9498806682577565, correct 398/419\n",
      "class 1: acc 0.9627791563275434, correct 388/403\n",
      "\n",
      "Val Set, val_loss: 0.3812, val_error: 0.0917, auc: 0.9752\n",
      "class 0 clustering acc 0.9753440366972477: correct 1701/1744\n",
      "class 1 clustering acc 0.8520642201834863: correct 743/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.8412698412698413, correct 53/63\n",
      "EarlyStopping counter: 12 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0024, instance_loss: 0.0125, weighted_loss: 0.0054, label: 1, bag_size: 5921\n",
      "batch 39, loss: 0.0016, instance_loss: 0.0310, weighted_loss: 0.0104, label: 1, bag_size: 8754\n",
      "batch 59, loss: 0.0016, instance_loss: 0.1066, weighted_loss: 0.0331, label: 1, bag_size: 1437\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9433\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8372\n",
      "batch 119, loss: 0.2245, instance_loss: 0.3115, weighted_loss: 0.2506, label: 1, bag_size: 15931\n",
      "batch 139, loss: 0.0020, instance_loss: 0.0000, weighted_loss: 0.0014, label: 0, bag_size: 16607\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15747\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0341, weighted_loss: 0.0102, label: 1, bag_size: 3295\n",
      "batch 199, loss: 0.0028, instance_loss: 0.1870, weighted_loss: 0.0580, label: 1, bag_size: 15332\n",
      "batch 219, loss: 0.0076, instance_loss: 0.0025, weighted_loss: 0.0061, label: 1, bag_size: 645\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0003, label: 1, bag_size: 9408\n",
      "batch 259, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 11194\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 13368\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 10920\n",
      "batch 319, loss: 0.0126, instance_loss: 0.0000, weighted_loss: 0.0088, label: 0, bag_size: 24439\n",
      "batch 339, loss: 1.1342, instance_loss: 0.0000, weighted_loss: 0.7939, label: 0, bag_size: 13619\n",
      "batch 359, loss: 0.0027, instance_loss: 0.0767, weighted_loss: 0.0249, label: 0, bag_size: 2367\n",
      "batch 379, loss: 0.2747, instance_loss: 0.2681, weighted_loss: 0.2727, label: 1, bag_size: 1794\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 24911\n",
      "batch 419, loss: 0.0881, instance_loss: 0.0114, weighted_loss: 0.0651, label: 1, bag_size: 2344\n",
      "batch 439, loss: 2.9846, instance_loss: 0.0926, weighted_loss: 2.1170, label: 0, bag_size: 2815\n",
      "batch 459, loss: 0.0067, instance_loss: 0.0000, weighted_loss: 0.0047, label: 1, bag_size: 11884\n",
      "batch 479, loss: 0.0047, instance_loss: 0.1723, weighted_loss: 0.0550, label: 1, bag_size: 1920\n",
      "batch 499, loss: 0.0013, instance_loss: 0.0081, weighted_loss: 0.0034, label: 1, bag_size: 645\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23037\n",
      "batch 539, loss: 0.0030, instance_loss: 0.0049, weighted_loss: 0.0036, label: 0, bag_size: 3474\n",
      "batch 559, loss: 0.0052, instance_loss: 0.0153, weighted_loss: 0.0082, label: 0, bag_size: 3474\n",
      "batch 579, loss: 0.0054, instance_loss: 0.0002, weighted_loss: 0.0039, label: 1, bag_size: 5494\n",
      "batch 599, loss: 0.0111, instance_loss: 0.0000, weighted_loss: 0.0078, label: 1, bag_size: 16379\n",
      "batch 619, loss: 0.2292, instance_loss: 0.2696, weighted_loss: 0.2413, label: 0, bag_size: 8420\n",
      "batch 639, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 10942\n",
      "batch 659, loss: 0.0030, instance_loss: 0.0000, weighted_loss: 0.0021, label: 1, bag_size: 9548\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 1, bag_size: 699\n",
      "batch 699, loss: 0.0022, instance_loss: 0.0000, weighted_loss: 0.0015, label: 0, bag_size: 13880\n",
      "batch 719, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 8582\n",
      "batch 739, loss: 0.0218, instance_loss: 0.0024, weighted_loss: 0.0160, label: 0, bag_size: 16690\n",
      "batch 759, loss: 1.5411, instance_loss: 1.0587, weighted_loss: 1.3964, label: 0, bag_size: 11306\n",
      "batch 779, loss: 0.0000, instance_loss: 0.0098, weighted_loss: 0.0029, label: 1, bag_size: 1360\n",
      "batch 799, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 24439\n",
      "batch 819, loss: 0.0057, instance_loss: 0.0000, weighted_loss: 0.0040, label: 0, bag_size: 12561\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9904957420924574: correct 13027/13152\n",
      "class 1 clustering acc 0.9365875912408759: correct 6159/6576\n",
      "Epoch: 85, train_loss: 0.0870, train_clustering_loss:  0.1002, train_error: 0.0328\n",
      "class 0: acc 0.9618320610687023, correct 378/393\n",
      "class 1: acc 0.972027972027972, correct 417/429\n",
      "\n",
      "Val Set, val_loss: 0.3103, val_error: 0.0826, auc: 0.9748\n",
      "class 0 clustering acc 0.9587155963302753: correct 1672/1744\n",
      "class 1 clustering acc 0.8853211009174312: correct 772/872\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 13 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0025, instance_loss: 0.0001, weighted_loss: 0.0018, label: 0, bag_size: 1508\n",
      "batch 39, loss: 0.0013, instance_loss: 0.0007, weighted_loss: 0.0011, label: 0, bag_size: 1891\n",
      "batch 59, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 10725\n",
      "batch 79, loss: 0.0402, instance_loss: 0.0380, weighted_loss: 0.0395, label: 1, bag_size: 1823\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 22800\n",
      "batch 119, loss: 0.0155, instance_loss: 0.0004, weighted_loss: 0.0110, label: 0, bag_size: 4845\n",
      "batch 139, loss: 0.0262, instance_loss: 0.0020, weighted_loss: 0.0190, label: 1, bag_size: 12714\n",
      "batch 159, loss: 0.0471, instance_loss: 0.1698, weighted_loss: 0.0839, label: 0, bag_size: 2624\n",
      "batch 179, loss: 0.0336, instance_loss: 0.0054, weighted_loss: 0.0252, label: 0, bag_size: 931\n",
      "batch 199, loss: 0.0007, instance_loss: 0.0901, weighted_loss: 0.0275, label: 1, bag_size: 8935\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8330\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11546\n",
      "batch 259, loss: 0.0000, instance_loss: 0.1204, weighted_loss: 0.0361, label: 0, bag_size: 705\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11865\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0101, weighted_loss: 0.0030, label: 1, bag_size: 5731\n",
      "batch 319, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 13015\n",
      "batch 339, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 18215\n",
      "batch 359, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 1826\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0064, weighted_loss: 0.0019, label: 0, bag_size: 18574\n",
      "batch 399, loss: 0.4948, instance_loss: 0.6170, weighted_loss: 0.5315, label: 1, bag_size: 1703\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 1, bag_size: 699\n",
      "batch 439, loss: 0.0000, instance_loss: 0.2523, weighted_loss: 0.0757, label: 1, bag_size: 2136\n",
      "batch 459, loss: 0.1239, instance_loss: 0.0000, weighted_loss: 0.0867, label: 1, bag_size: 34356\n",
      "batch 479, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 1826\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 2652\n",
      "batch 519, loss: 0.0922, instance_loss: 0.0000, weighted_loss: 0.0646, label: 0, bag_size: 2219\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 20555\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11383\n",
      "batch 579, loss: 0.0310, instance_loss: 2.3850, weighted_loss: 0.7372, label: 0, bag_size: 2959\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0164, weighted_loss: 0.0049, label: 1, bag_size: 6533\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0969, weighted_loss: 0.0291, label: 0, bag_size: 19390\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23996\n",
      "batch 659, loss: 0.0001, instance_loss: 0.0015, weighted_loss: 0.0005, label: 0, bag_size: 2534\n",
      "batch 679, loss: 0.0572, instance_loss: 0.5210, weighted_loss: 0.1963, label: 0, bag_size: 3502\n",
      "batch 699, loss: 0.0130, instance_loss: 0.3105, weighted_loss: 0.1022, label: 0, bag_size: 1614\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 12212\n",
      "batch 739, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21082\n",
      "batch 759, loss: 0.0189, instance_loss: 0.0064, weighted_loss: 0.0152, label: 1, bag_size: 5629\n",
      "batch 779, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 8582\n",
      "batch 799, loss: 0.0001, instance_loss: 0.0061, weighted_loss: 0.0019, label: 1, bag_size: 9446\n",
      "batch 819, loss: 0.0001, instance_loss: 0.0007, weighted_loss: 0.0003, label: 0, bag_size: 2091\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9878345498783455: correct 12992/13152\n",
      "class 1 clustering acc 0.933242092457421: correct 6137/6576\n",
      "Epoch: 86, train_loss: 0.0951, train_clustering_loss:  0.1618, train_error: 0.0316\n",
      "class 0: acc 0.9603960396039604, correct 388/404\n",
      "class 1: acc 0.9760765550239234, correct 408/418\n",
      "\n",
      "Val Set, val_loss: 0.4175, val_error: 0.1009, auc: 0.9752\n",
      "class 0 clustering acc 0.9598623853211009: correct 1674/1744\n",
      "class 1 clustering acc 0.8337155963302753: correct 727/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.8253968253968254, correct 52/63\n",
      "EarlyStopping counter: 14 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 6851\n",
      "batch 39, loss: 0.0228, instance_loss: 0.3931, weighted_loss: 0.1339, label: 1, bag_size: 1794\n",
      "batch 59, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 6850\n",
      "batch 79, loss: 0.0033, instance_loss: 0.2367, weighted_loss: 0.0733, label: 0, bag_size: 1370\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0680, weighted_loss: 0.0205, label: 1, bag_size: 1622\n",
      "batch 119, loss: 0.0087, instance_loss: 1.0095, weighted_loss: 0.3090, label: 0, bag_size: 6093\n",
      "batch 139, loss: 0.6209, instance_loss: 0.8523, weighted_loss: 0.6903, label: 0, bag_size: 3399\n",
      "batch 159, loss: 0.0062, instance_loss: 0.2350, weighted_loss: 0.0748, label: 0, bag_size: 5485\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15077\n",
      "batch 199, loss: 0.0001, instance_loss: 0.2934, weighted_loss: 0.0881, label: 1, bag_size: 2455\n",
      "batch 219, loss: 0.0000, instance_loss: 0.1125, weighted_loss: 0.0338, label: 1, bag_size: 5991\n",
      "batch 239, loss: 0.0002, instance_loss: 1.1977, weighted_loss: 0.3594, label: 0, bag_size: 14377\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0233, weighted_loss: 0.0070, label: 1, bag_size: 629\n",
      "batch 279, loss: 0.0013, instance_loss: 0.0022, weighted_loss: 0.0015, label: 0, bag_size: 9866\n",
      "batch 299, loss: 0.0762, instance_loss: 0.5568, weighted_loss: 0.2204, label: 0, bag_size: 2815\n",
      "batch 319, loss: 0.0028, instance_loss: 0.0000, weighted_loss: 0.0020, label: 0, bag_size: 18132\n",
      "batch 339, loss: 0.0170, instance_loss: 0.0000, weighted_loss: 0.0119, label: 0, bag_size: 3710\n",
      "batch 359, loss: 0.5817, instance_loss: 0.0680, weighted_loss: 0.4276, label: 1, bag_size: 9215\n",
      "batch 379, loss: 0.0136, instance_loss: 0.0000, weighted_loss: 0.0095, label: 0, bag_size: 47866\n",
      "batch 399, loss: 0.0054, instance_loss: 0.5006, weighted_loss: 0.1540, label: 0, bag_size: 1800\n",
      "batch 419, loss: 0.1072, instance_loss: 0.1784, weighted_loss: 0.1285, label: 0, bag_size: 5105\n",
      "batch 439, loss: 0.0009, instance_loss: 0.0136, weighted_loss: 0.0047, label: 1, bag_size: 8003\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 17268\n",
      "batch 479, loss: 0.0022, instance_loss: 0.1754, weighted_loss: 0.0542, label: 1, bag_size: 7513\n",
      "batch 499, loss: 0.0004, instance_loss: 0.0044, weighted_loss: 0.0016, label: 1, bag_size: 2695\n",
      "batch 519, loss: 0.0377, instance_loss: 0.0200, weighted_loss: 0.0324, label: 1, bag_size: 10432\n",
      "batch 539, loss: 0.0000, instance_loss: 0.1048, weighted_loss: 0.0315, label: 1, bag_size: 14779\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 12201\n",
      "batch 579, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 14625\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 6533\n",
      "batch 619, loss: 0.0280, instance_loss: 0.0005, weighted_loss: 0.0198, label: 0, bag_size: 3876\n",
      "batch 639, loss: 0.0003, instance_loss: 0.0010, weighted_loss: 0.0005, label: 1, bag_size: 8522\n",
      "batch 659, loss: 0.0231, instance_loss: 0.4161, weighted_loss: 0.1410, label: 0, bag_size: 1234\n",
      "batch 679, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 6731\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8582\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9885\n",
      "batch 739, loss: 0.0010, instance_loss: 0.2073, weighted_loss: 0.0629, label: 1, bag_size: 1525\n",
      "batch 759, loss: 0.0035, instance_loss: 0.3387, weighted_loss: 0.1040, label: 1, bag_size: 2480\n",
      "batch 779, loss: 0.5186, instance_loss: 0.4539, weighted_loss: 0.4992, label: 1, bag_size: 1038\n",
      "batch 799, loss: 0.0004, instance_loss: 0.0005, weighted_loss: 0.0004, label: 0, bag_size: 9471\n",
      "batch 819, loss: 0.0315, instance_loss: 0.1479, weighted_loss: 0.0664, label: 0, bag_size: 3557\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9868461070559611: correct 12979/13152\n",
      "class 1 clustering acc 0.9206204379562044: correct 6054/6576\n",
      "Epoch: 87, train_loss: 0.1050, train_clustering_loss:  0.1469, train_error: 0.0401\n",
      "class 0: acc 0.9665871121718377, correct 405/419\n",
      "class 1: acc 0.9528535980148883, correct 384/403\n",
      "\n",
      "Val Set, val_loss: 0.2764, val_error: 0.1009, auc: 0.9752\n",
      "class 0 clustering acc 0.9650229357798165: correct 1683/1744\n",
      "class 1 clustering acc 0.8681192660550459: correct 757/872\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 15 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0024, instance_loss: 0.0084, weighted_loss: 0.0042, label: 0, bag_size: 1920\n",
      "batch 39, loss: 0.2317, instance_loss: 0.0027, weighted_loss: 0.1630, label: 1, bag_size: 7798\n",
      "batch 59, loss: 0.0090, instance_loss: 0.2455, weighted_loss: 0.0799, label: 0, bag_size: 2815\n",
      "batch 79, loss: 0.0000, instance_loss: 0.1303, weighted_loss: 0.0391, label: 1, bag_size: 4128\n",
      "batch 99, loss: 0.0125, instance_loss: 0.0491, weighted_loss: 0.0235, label: 1, bag_size: 4929\n",
      "batch 119, loss: 0.1550, instance_loss: 0.1195, weighted_loss: 0.1443, label: 1, bag_size: 5605\n",
      "batch 139, loss: 0.0001, instance_loss: 0.1038, weighted_loss: 0.0312, label: 1, bag_size: 5340\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 16782\n",
      "batch 179, loss: 0.0175, instance_loss: 0.4321, weighted_loss: 0.1418, label: 0, bag_size: 1416\n",
      "batch 199, loss: 0.0040, instance_loss: 0.0793, weighted_loss: 0.0266, label: 0, bag_size: 3670\n",
      "batch 219, loss: 0.2414, instance_loss: 0.0486, weighted_loss: 0.1836, label: 1, bag_size: 13089\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9885\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9433\n",
      "batch 279, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 6682\n",
      "batch 299, loss: 0.0012, instance_loss: 0.0568, weighted_loss: 0.0179, label: 0, bag_size: 2820\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0033, weighted_loss: 0.0010, label: 1, bag_size: 7381\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 27158\n",
      "batch 359, loss: 0.0948, instance_loss: 0.1417, weighted_loss: 0.1088, label: 1, bag_size: 2395\n",
      "batch 379, loss: 0.0003, instance_loss: 0.0110, weighted_loss: 0.0035, label: 1, bag_size: 8935\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 10920\n",
      "batch 419, loss: 2.9454, instance_loss: 0.0055, weighted_loss: 2.0634, label: 0, bag_size: 18516\n",
      "batch 439, loss: 0.0042, instance_loss: 0.3142, weighted_loss: 0.0972, label: 0, bag_size: 2628\n",
      "batch 459, loss: 0.0315, instance_loss: 0.0000, weighted_loss: 0.0221, label: 0, bag_size: 20230\n",
      "batch 479, loss: 0.0011, instance_loss: 0.3824, weighted_loss: 0.1155, label: 1, bag_size: 2759\n",
      "batch 499, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 17268\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 13880\n",
      "batch 539, loss: 0.0010, instance_loss: 0.0013, weighted_loss: 0.0011, label: 1, bag_size: 5690\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0032, weighted_loss: 0.0010, label: 1, bag_size: 10392\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 18045\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0558, weighted_loss: 0.0168, label: 0, bag_size: 705\n",
      "batch 619, loss: 0.0028, instance_loss: 0.0019, weighted_loss: 0.0025, label: 1, bag_size: 7110\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0341, weighted_loss: 0.0103, label: 1, bag_size: 1172\n",
      "batch 659, loss: 0.0129, instance_loss: 0.0000, weighted_loss: 0.0091, label: 0, bag_size: 29270\n",
      "batch 679, loss: 0.6076, instance_loss: 0.0539, weighted_loss: 0.4415, label: 0, bag_size: 3375\n",
      "batch 699, loss: 0.3393, instance_loss: 3.2660, weighted_loss: 1.2173, label: 0, bag_size: 17279\n",
      "batch 719, loss: 0.0090, instance_loss: 0.0001, weighted_loss: 0.0063, label: 1, bag_size: 7382\n",
      "batch 739, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 14206\n",
      "batch 759, loss: 0.3006, instance_loss: 0.0000, weighted_loss: 0.2104, label: 0, bag_size: 20478\n",
      "batch 779, loss: 0.0008, instance_loss: 0.0013, weighted_loss: 0.0009, label: 0, bag_size: 2351\n",
      "batch 799, loss: 0.0161, instance_loss: 0.4967, weighted_loss: 0.1603, label: 1, bag_size: 1525\n",
      "batch 819, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 15464\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9880626520681265: correct 12995/13152\n",
      "class 1 clustering acc 0.9382603406326034: correct 6170/6576\n",
      "Epoch: 88, train_loss: 0.1069, train_clustering_loss:  0.1134, train_error: 0.0401\n",
      "class 0: acc 0.964824120603015, correct 384/398\n",
      "class 1: acc 0.9551886792452831, correct 405/424\n",
      "\n",
      "Val Set, val_loss: 0.2832, val_error: 0.1009, auc: 0.9758\n",
      "class 0 clustering acc 0.9701834862385321: correct 1692/1744\n",
      "class 1 clustering acc 0.9002293577981652: correct 785/872\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 16 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0274, instance_loss: 0.0420, weighted_loss: 0.0317, label: 1, bag_size: 2146\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9433\n",
      "batch 59, loss: 0.0040, instance_loss: 0.0053, weighted_loss: 0.0044, label: 1, bag_size: 12946\n",
      "batch 79, loss: 0.2960, instance_loss: 0.0000, weighted_loss: 0.2072, label: 0, bag_size: 23618\n",
      "batch 99, loss: 0.7432, instance_loss: 0.6491, weighted_loss: 0.7150, label: 0, bag_size: 2653\n",
      "batch 119, loss: 0.0916, instance_loss: 0.1981, weighted_loss: 0.1236, label: 1, bag_size: 7389\n",
      "batch 139, loss: 0.6024, instance_loss: 0.0045, weighted_loss: 0.4230, label: 0, bag_size: 4241\n",
      "batch 159, loss: 0.0052, instance_loss: 0.3700, weighted_loss: 0.1147, label: 0, bag_size: 803\n",
      "batch 179, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 10392\n",
      "batch 199, loss: 0.0007, instance_loss: 0.0050, weighted_loss: 0.0020, label: 1, bag_size: 7371\n",
      "batch 219, loss: 0.0129, instance_loss: 0.0000, weighted_loss: 0.0090, label: 0, bag_size: 3710\n",
      "batch 239, loss: 0.0007, instance_loss: 0.1670, weighted_loss: 0.0506, label: 1, bag_size: 6343\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14206\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0008, weighted_loss: 0.0003, label: 0, bag_size: 5409\n",
      "batch 299, loss: 0.0144, instance_loss: 0.0181, weighted_loss: 0.0155, label: 1, bag_size: 6171\n",
      "batch 319, loss: 0.2637, instance_loss: 0.3235, weighted_loss: 0.2816, label: 0, bag_size: 3375\n",
      "batch 339, loss: 0.0303, instance_loss: 0.0966, weighted_loss: 0.0502, label: 1, bag_size: 7389\n",
      "batch 359, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 6281\n",
      "batch 379, loss: 0.3532, instance_loss: 0.1624, weighted_loss: 0.2960, label: 0, bag_size: 7428\n",
      "batch 399, loss: 0.3612, instance_loss: 0.0889, weighted_loss: 0.2795, label: 1, bag_size: 15931\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 15213\n",
      "batch 439, loss: 0.0004, instance_loss: 0.0532, weighted_loss: 0.0162, label: 1, bag_size: 9649\n",
      "batch 459, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 20555\n",
      "batch 479, loss: 0.0104, instance_loss: 0.0967, weighted_loss: 0.0363, label: 1, bag_size: 6825\n",
      "batch 499, loss: 0.1531, instance_loss: 0.0003, weighted_loss: 0.1073, label: 1, bag_size: 12425\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 8981\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0001, weighted_loss: 0.0002, label: 0, bag_size: 12687\n",
      "batch 559, loss: 0.0159, instance_loss: 0.0671, weighted_loss: 0.0312, label: 1, bag_size: 7468\n",
      "batch 579, loss: 0.0081, instance_loss: 0.0000, weighted_loss: 0.0057, label: 0, bag_size: 12561\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0034, weighted_loss: 0.0010, label: 1, bag_size: 1360\n",
      "batch 619, loss: 0.0292, instance_loss: 0.0014, weighted_loss: 0.0209, label: 0, bag_size: 17083\n",
      "batch 639, loss: 0.0009, instance_loss: 0.0004, weighted_loss: 0.0007, label: 1, bag_size: 19606\n",
      "batch 659, loss: 0.0212, instance_loss: 0.5182, weighted_loss: 0.1703, label: 1, bag_size: 1483\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 5612\n",
      "batch 699, loss: 0.0237, instance_loss: 0.0000, weighted_loss: 0.0166, label: 0, bag_size: 8549\n",
      "batch 719, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 14681\n",
      "batch 739, loss: 0.0087, instance_loss: 0.0000, weighted_loss: 0.0061, label: 0, bag_size: 21032\n",
      "batch 759, loss: 0.0275, instance_loss: 0.0015, weighted_loss: 0.0197, label: 1, bag_size: 6736\n",
      "batch 779, loss: 0.0012, instance_loss: 0.0003, weighted_loss: 0.0009, label: 0, bag_size: 2360\n",
      "batch 799, loss: 0.3069, instance_loss: 1.3064, weighted_loss: 0.6068, label: 0, bag_size: 10063\n",
      "batch 819, loss: 0.1248, instance_loss: 0.0000, weighted_loss: 0.0873, label: 0, bag_size: 25814\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9884428223844283: correct 13000/13152\n",
      "class 1 clustering acc 0.9472323600973236: correct 6229/6576\n",
      "Epoch: 89, train_loss: 0.0910, train_clustering_loss:  0.1056, train_error: 0.0292\n",
      "class 0: acc 0.9719387755102041, correct 381/392\n",
      "class 1: acc 0.9697674418604652, correct 417/430\n",
      "\n",
      "Val Set, val_loss: 0.2529, val_error: 0.1193, auc: 0.9769\n",
      "class 0 clustering acc 0.9592889908256881: correct 1673/1744\n",
      "class 1 clustering acc 0.8646788990825688: correct 754/872\n",
      "class 0: acc 0.8478260869565217, correct 39/46\n",
      "class 1: acc 0.9047619047619048, correct 57/63\n",
      "EarlyStopping counter: 17 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0287, instance_loss: 0.1697, weighted_loss: 0.0710, label: 1, bag_size: 3121\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 16521\n",
      "batch 59, loss: 0.7515, instance_loss: 1.8138, weighted_loss: 1.0702, label: 1, bag_size: 15185\n",
      "batch 79, loss: 0.0020, instance_loss: 0.0000, weighted_loss: 0.0014, label: 0, bag_size: 10113\n",
      "batch 99, loss: 0.0295, instance_loss: 0.6046, weighted_loss: 0.2020, label: 1, bag_size: 3968\n",
      "batch 119, loss: 0.0003, instance_loss: 0.0004, weighted_loss: 0.0003, label: 1, bag_size: 19500\n",
      "batch 139, loss: 0.0707, instance_loss: 0.2530, weighted_loss: 0.1254, label: 1, bag_size: 7351\n",
      "batch 159, loss: 0.0268, instance_loss: 0.0129, weighted_loss: 0.0226, label: 1, bag_size: 11394\n",
      "batch 179, loss: 0.0056, instance_loss: 0.0418, weighted_loss: 0.0165, label: 1, bag_size: 3368\n",
      "batch 199, loss: 0.0227, instance_loss: 0.0000, weighted_loss: 0.0159, label: 0, bag_size: 10029\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 3459\n",
      "batch 239, loss: 0.0222, instance_loss: 0.0007, weighted_loss: 0.0157, label: 1, bag_size: 21827\n",
      "batch 259, loss: 0.0272, instance_loss: 0.0000, weighted_loss: 0.0190, label: 0, bag_size: 9597\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0148, weighted_loss: 0.0045, label: 1, bag_size: 15332\n",
      "batch 299, loss: 1.1163, instance_loss: 0.0000, weighted_loss: 0.7814, label: 0, bag_size: 23618\n",
      "batch 319, loss: 0.0501, instance_loss: 0.0002, weighted_loss: 0.0351, label: 0, bag_size: 2282\n",
      "batch 339, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 16087\n",
      "batch 359, loss: 0.0030, instance_loss: 0.0000, weighted_loss: 0.0021, label: 0, bag_size: 13378\n",
      "batch 379, loss: 0.0451, instance_loss: 0.0008, weighted_loss: 0.0318, label: 1, bag_size: 21701\n",
      "batch 399, loss: 0.0006, instance_loss: 0.0042, weighted_loss: 0.0017, label: 0, bag_size: 9583\n",
      "batch 419, loss: 0.3837, instance_loss: 0.0974, weighted_loss: 0.2978, label: 1, bag_size: 7424\n",
      "batch 439, loss: 0.3383, instance_loss: 0.1391, weighted_loss: 0.2786, label: 1, bag_size: 9215\n",
      "batch 459, loss: 0.0483, instance_loss: 0.0028, weighted_loss: 0.0346, label: 1, bag_size: 13440\n",
      "batch 479, loss: 0.0248, instance_loss: 0.1927, weighted_loss: 0.0751, label: 1, bag_size: 12340\n",
      "batch 499, loss: 0.0174, instance_loss: 0.0392, weighted_loss: 0.0239, label: 1, bag_size: 6825\n",
      "batch 519, loss: 0.0124, instance_loss: 0.0246, weighted_loss: 0.0161, label: 0, bag_size: 24382\n",
      "batch 539, loss: 0.0203, instance_loss: 0.0498, weighted_loss: 0.0291, label: 0, bag_size: 9616\n",
      "batch 559, loss: 0.0015, instance_loss: 0.0252, weighted_loss: 0.0086, label: 1, bag_size: 4039\n",
      "batch 579, loss: 0.0075, instance_loss: 0.0032, weighted_loss: 0.0062, label: 1, bag_size: 1823\n",
      "batch 599, loss: 0.1103, instance_loss: 0.0402, weighted_loss: 0.0892, label: 0, bag_size: 2653\n",
      "batch 619, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 9387\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 15213\n",
      "batch 659, loss: 0.0015, instance_loss: 0.0023, weighted_loss: 0.0017, label: 1, bag_size: 6731\n",
      "batch 679, loss: 0.0003, instance_loss: 0.0041, weighted_loss: 0.0014, label: 0, bag_size: 12732\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19043\n",
      "batch 719, loss: 0.0266, instance_loss: 0.0939, weighted_loss: 0.0468, label: 1, bag_size: 9519\n",
      "batch 739, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 29270\n",
      "batch 759, loss: 0.0008, instance_loss: 0.3313, weighted_loss: 0.0999, label: 1, bag_size: 1064\n",
      "batch 779, loss: 0.0049, instance_loss: 0.0267, weighted_loss: 0.0114, label: 1, bag_size: 11386\n",
      "batch 799, loss: 0.0969, instance_loss: 0.0215, weighted_loss: 0.0743, label: 1, bag_size: 7445\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0425, weighted_loss: 0.0128, label: 1, bag_size: 629\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9853254257907542: correct 12959/13152\n",
      "class 1 clustering acc 0.9409975669099757: correct 6188/6576\n",
      "Epoch: 90, train_loss: 0.0663, train_clustering_loss:  0.1351, train_error: 0.0231\n",
      "class 0: acc 0.9705882352941176, correct 396/408\n",
      "class 1: acc 0.9830917874396136, correct 407/414\n",
      "\n",
      "Val Set, val_loss: 0.2779, val_error: 0.1101, auc: 0.9752\n",
      "class 0 clustering acc 0.9655963302752294: correct 1684/1744\n",
      "class 1 clustering acc 0.8600917431192661: correct 750/872\n",
      "class 0: acc 0.9130434782608695, correct 42/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 18 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 12201\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 11122\n",
      "batch 59, loss: 0.0061, instance_loss: 0.0001, weighted_loss: 0.0043, label: 0, bag_size: 8427\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0034, weighted_loss: 0.0010, label: 1, bag_size: 1316\n",
      "batch 99, loss: 0.0070, instance_loss: 0.1096, weighted_loss: 0.0378, label: 1, bag_size: 3656\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 13691\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 1072\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11146\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11759\n",
      "batch 199, loss: 0.0004, instance_loss: 0.0001, weighted_loss: 0.0003, label: 1, bag_size: 5894\n",
      "batch 219, loss: 0.0005, instance_loss: 0.0011, weighted_loss: 0.0007, label: 0, bag_size: 1651\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 6851\n",
      "batch 259, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 16607\n",
      "batch 279, loss: 0.0134, instance_loss: 0.0007, weighted_loss: 0.0096, label: 1, bag_size: 8592\n",
      "batch 299, loss: 0.6254, instance_loss: 1.5473, weighted_loss: 0.9020, label: 0, bag_size: 2959\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15967\n",
      "batch 339, loss: 0.0002, instance_loss: 0.0026, weighted_loss: 0.0009, label: 1, bag_size: 2848\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0264, weighted_loss: 0.0080, label: 1, bag_size: 1249\n",
      "batch 379, loss: 0.0724, instance_loss: 0.0000, weighted_loss: 0.0507, label: 0, bag_size: 11922\n",
      "batch 399, loss: 0.0084, instance_loss: 0.0000, weighted_loss: 0.0059, label: 0, bag_size: 26208\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14319\n",
      "batch 439, loss: 0.0022, instance_loss: 0.0020, weighted_loss: 0.0021, label: 1, bag_size: 22286\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0269, weighted_loss: 0.0082, label: 1, bag_size: 2455\n",
      "batch 479, loss: 0.1732, instance_loss: 0.0010, weighted_loss: 0.1216, label: 0, bag_size: 2815\n",
      "batch 499, loss: 0.0227, instance_loss: 0.2761, weighted_loss: 0.0987, label: 0, bag_size: 2266\n",
      "batch 519, loss: 0.2353, instance_loss: 0.0906, weighted_loss: 0.1919, label: 1, bag_size: 5292\n",
      "batch 539, loss: 0.0094, instance_loss: 0.0164, weighted_loss: 0.0115, label: 1, bag_size: 8466\n",
      "batch 559, loss: 0.0492, instance_loss: 0.0000, weighted_loss: 0.0345, label: 1, bag_size: 9689\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 12593\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0098, weighted_loss: 0.0030, label: 1, bag_size: 617\n",
      "batch 619, loss: 1.9740, instance_loss: 0.1225, weighted_loss: 1.4185, label: 1, bag_size: 1038\n",
      "batch 639, loss: 0.0029, instance_loss: 0.0001, weighted_loss: 0.0020, label: 1, bag_size: 12603\n",
      "batch 659, loss: 0.0033, instance_loss: 0.0044, weighted_loss: 0.0036, label: 1, bag_size: 8466\n",
      "batch 679, loss: 0.2976, instance_loss: 2.6102, weighted_loss: 0.9914, label: 0, bag_size: 1592\n",
      "batch 699, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 2006\n",
      "batch 719, loss: 0.0887, instance_loss: 0.0060, weighted_loss: 0.0639, label: 1, bag_size: 19972\n",
      "batch 739, loss: 0.0008, instance_loss: 0.0002, weighted_loss: 0.0006, label: 1, bag_size: 12895\n",
      "batch 759, loss: 0.0009, instance_loss: 0.0193, weighted_loss: 0.0064, label: 0, bag_size: 2242\n",
      "batch 779, loss: 0.0000, instance_loss: 0.5595, weighted_loss: 0.1679, label: 0, bag_size: 1891\n",
      "batch 799, loss: 0.0001, instance_loss: 0.0032, weighted_loss: 0.0011, label: 0, bag_size: 1909\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15841\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9916362530413625: correct 13042/13152\n",
      "class 1 clustering acc 0.9635036496350365: correct 6336/6576\n",
      "Epoch: 91, train_loss: 0.0678, train_clustering_loss:  0.0728, train_error: 0.0207\n",
      "class 0: acc 0.9779951100244498, correct 400/409\n",
      "class 1: acc 0.9806295399515739, correct 405/413\n",
      "\n",
      "Val Set, val_loss: 0.2492, val_error: 0.1193, auc: 0.9765\n",
      "class 0 clustering acc 0.9604357798165137: correct 1675/1744\n",
      "class 1 clustering acc 0.8864678899082569: correct 773/872\n",
      "class 0: acc 0.8478260869565217, correct 39/46\n",
      "class 1: acc 0.9047619047619048, correct 57/63\n",
      "EarlyStopping counter: 19 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0007, instance_loss: 0.0815, weighted_loss: 0.0249, label: 1, bag_size: 12460\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21874\n",
      "batch 59, loss: 0.0488, instance_loss: 0.0000, weighted_loss: 0.0341, label: 0, bag_size: 19880\n",
      "batch 79, loss: 0.0015, instance_loss: 0.3419, weighted_loss: 0.1036, label: 0, bag_size: 1508\n",
      "batch 99, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 11125\n",
      "batch 119, loss: 0.0002, instance_loss: 0.0005, weighted_loss: 0.0003, label: 1, bag_size: 2412\n",
      "batch 139, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 16512\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8372\n",
      "batch 179, loss: 0.0007, instance_loss: 0.0963, weighted_loss: 0.0294, label: 1, bag_size: 3450\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9786\n",
      "batch 219, loss: 0.1055, instance_loss: 0.0000, weighted_loss: 0.0739, label: 0, bag_size: 15672\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 17368\n",
      "batch 259, loss: 0.0007, instance_loss: 0.0388, weighted_loss: 0.0121, label: 0, bag_size: 2367\n",
      "batch 279, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 12201\n",
      "batch 299, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 16521\n",
      "batch 319, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 5999\n",
      "batch 339, loss: 0.0067, instance_loss: 0.3797, weighted_loss: 0.1186, label: 0, bag_size: 3507\n",
      "batch 359, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 3101\n",
      "batch 379, loss: 0.0030, instance_loss: 0.0039, weighted_loss: 0.0032, label: 0, bag_size: 3474\n",
      "batch 399, loss: 0.0008, instance_loss: 0.0493, weighted_loss: 0.0154, label: 0, bag_size: 4523\n",
      "batch 419, loss: 0.2229, instance_loss: 0.0022, weighted_loss: 0.1567, label: 1, bag_size: 12494\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19470\n",
      "batch 459, loss: 0.0849, instance_loss: 0.0091, weighted_loss: 0.0621, label: 1, bag_size: 2356\n",
      "batch 479, loss: 0.0289, instance_loss: 0.9865, weighted_loss: 0.3162, label: 1, bag_size: 16703\n",
      "batch 499, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 6731\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11146\n",
      "batch 539, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 10415\n",
      "batch 559, loss: 0.0030, instance_loss: 0.0000, weighted_loss: 0.0021, label: 1, bag_size: 21009\n",
      "batch 579, loss: 0.0008, instance_loss: 0.0680, weighted_loss: 0.0210, label: 0, bag_size: 2242\n",
      "batch 599, loss: 0.0021, instance_loss: 0.0310, weighted_loss: 0.0108, label: 1, bag_size: 3619\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 2322\n",
      "batch 639, loss: 0.0611, instance_loss: 0.0000, weighted_loss: 0.0428, label: 0, bag_size: 15898\n",
      "batch 659, loss: 0.0086, instance_loss: 0.0000, weighted_loss: 0.0060, label: 1, bag_size: 4308\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0057, weighted_loss: 0.0018, label: 1, bag_size: 5110\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0005, label: 1, bag_size: 1360\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 8935\n",
      "batch 739, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14681\n",
      "batch 759, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 12201\n",
      "batch 779, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 11146\n",
      "batch 799, loss: 0.1058, instance_loss: 0.2292, weighted_loss: 0.1428, label: 1, bag_size: 5292\n",
      "batch 819, loss: 0.0071, instance_loss: 0.1201, weighted_loss: 0.0410, label: 0, bag_size: 1052\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9917883211678832: correct 13044/13152\n",
      "class 1 clustering acc 0.9587895377128953: correct 6305/6576\n",
      "Epoch: 92, train_loss: 0.0858, train_clustering_loss:  0.0773, train_error: 0.0377\n",
      "class 0: acc 0.9652777777777778, correct 417/432\n",
      "class 1: acc 0.958974358974359, correct 374/390\n",
      "\n",
      "Val Set, val_loss: 0.2588, val_error: 0.1009, auc: 0.9783\n",
      "class 0 clustering acc 0.9684633027522935: correct 1689/1744\n",
      "class 1 clustering acc 0.9059633027522935: correct 790/872\n",
      "class 0: acc 0.8913043478260869, correct 41/46\n",
      "class 1: acc 0.9047619047619048, correct 57/63\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Val error: 0.1193, ROC AUC: 0.9724\n",
      "Test error: 0.0625, ROC AUC: 0.9978\n",
      "class 0: acc 0.9019607843137255, correct 46/51\n",
      "class 1: acc 0.9777777777777777, correct 44/45\n",
      "\n",
      "Training Fold 2!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 822 samples\n",
      "Validating on 92 samples\n",
      "Testing on 113 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Setting tau to 1.0\n",
      "Done!\n",
      "MCBAT_SB(\n",
      "  (instance_loss_fn): SmoothTop1SVM()\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (transformer_low): TransformerEncoder_FLASH(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FLASH(\n",
      "            (attn_fn): LaplacianAttnFn()\n",
      "            (rel_pos_bias): T5RelativePositionBias(\n",
      "              (relative_attention_bias): Embedding(32, 1)\n",
      "            )\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (to_hidden): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (to_qk): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (qk_offset_scale): OffsetScale()\n",
      "            (to_out): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer_high): TransformerEncoder_FLASH(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FLASH(\n",
      "            (attn_fn): LaplacianAttnFn()\n",
      "            (rel_pos_bias): T5RelativePositionBias(\n",
      "              (relative_attention_bias): Embedding(32, 1)\n",
      "            )\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (to_hidden): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (to_qk): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (qk_offset_scale): OffsetScale()\n",
      "            (to_out): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attention): Attn_Net_Gated(\n",
      "    (attention_a): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_b): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (fusion_encoder): FusionEncoder()\n",
      "  (attention_V2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (attention_U2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (attention_weights2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Total number of parameters: 17086091\n",
      "Total number of trainable parameters: 17086091\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... 2\n",
      "Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6726, instance_loss: 0.7555, weighted_loss: 0.6975, label: 0, bag_size: 10304\n",
      "batch 39, loss: 0.4310, instance_loss: 4.7842, weighted_loss: 1.7369, label: 1, bag_size: 34356\n",
      "batch 59, loss: 0.6679, instance_loss: 1.3768, weighted_loss: 0.8806, label: 1, bag_size: 6171\n",
      "batch 79, loss: 0.7071, instance_loss: 0.7793, weighted_loss: 0.7288, label: 0, bag_size: 4241\n",
      "batch 99, loss: 0.8016, instance_loss: 1.0432, weighted_loss: 0.8741, label: 1, bag_size: 4929\n",
      "batch 119, loss: 0.5755, instance_loss: 0.9103, weighted_loss: 0.6759, label: 1, bag_size: 2682\n",
      "batch 139, loss: 0.8209, instance_loss: 3.4817, weighted_loss: 1.6192, label: 0, bag_size: 2322\n",
      "batch 159, loss: 0.9578, instance_loss: 0.6230, weighted_loss: 0.8574, label: 0, bag_size: 16087\n",
      "batch 179, loss: 0.5646, instance_loss: 1.9379, weighted_loss: 0.9766, label: 1, bag_size: 20870\n",
      "batch 199, loss: 0.8275, instance_loss: 2.1646, weighted_loss: 1.2287, label: 0, bag_size: 12561\n",
      "batch 219, loss: 0.6033, instance_loss: 2.1231, weighted_loss: 1.0593, label: 1, bag_size: 1572\n",
      "batch 239, loss: 0.5283, instance_loss: 1.1656, weighted_loss: 0.7195, label: 1, bag_size: 1764\n",
      "batch 259, loss: 0.5059, instance_loss: 1.5069, weighted_loss: 0.8062, label: 1, bag_size: 3121\n",
      "batch 279, loss: 0.6754, instance_loss: 0.7136, weighted_loss: 0.6869, label: 1, bag_size: 10028\n",
      "batch 299, loss: 0.6018, instance_loss: 1.7198, weighted_loss: 0.9372, label: 1, bag_size: 6171\n",
      "batch 319, loss: 0.6529, instance_loss: 0.8635, weighted_loss: 0.7161, label: 0, bag_size: 22681\n",
      "batch 339, loss: 0.4056, instance_loss: 0.8455, weighted_loss: 0.5376, label: 0, bag_size: 5211\n",
      "batch 359, loss: 1.0088, instance_loss: 1.3869, weighted_loss: 1.1222, label: 1, bag_size: 1755\n",
      "batch 379, loss: 0.4609, instance_loss: 0.2984, weighted_loss: 0.4122, label: 0, bag_size: 12687\n",
      "batch 399, loss: 0.8958, instance_loss: 3.4956, weighted_loss: 1.6757, label: 1, bag_size: 12697\n",
      "batch 419, loss: 0.7477, instance_loss: 0.1785, weighted_loss: 0.5769, label: 1, bag_size: 5612\n",
      "batch 439, loss: 0.7784, instance_loss: 0.8138, weighted_loss: 0.7890, label: 1, bag_size: 2937\n",
      "batch 459, loss: 0.8950, instance_loss: 2.0077, weighted_loss: 1.2288, label: 0, bag_size: 7917\n",
      "batch 479, loss: 0.7365, instance_loss: 1.1197, weighted_loss: 0.8514, label: 1, bag_size: 2485\n",
      "batch 499, loss: 0.6141, instance_loss: 0.9693, weighted_loss: 0.7206, label: 0, bag_size: 3708\n",
      "batch 519, loss: 0.7481, instance_loss: 0.4042, weighted_loss: 0.6449, label: 1, bag_size: 7767\n",
      "batch 539, loss: 0.5097, instance_loss: 1.0748, weighted_loss: 0.6792, label: 1, bag_size: 1339\n",
      "batch 559, loss: 0.6405, instance_loss: 0.4781, weighted_loss: 0.5918, label: 1, bag_size: 3409\n",
      "batch 579, loss: 0.6926, instance_loss: 0.2798, weighted_loss: 0.5687, label: 1, bag_size: 1483\n",
      "batch 599, loss: 0.5250, instance_loss: 0.0455, weighted_loss: 0.3811, label: 1, bag_size: 7217\n",
      "batch 619, loss: 0.4624, instance_loss: 0.4630, weighted_loss: 0.4626, label: 1, bag_size: 7217\n",
      "batch 639, loss: 0.5731, instance_loss: 0.8717, weighted_loss: 0.6627, label: 1, bag_size: 1095\n",
      "batch 659, loss: 0.8336, instance_loss: 1.3129, weighted_loss: 0.9774, label: 0, bag_size: 6727\n",
      "batch 679, loss: 0.7296, instance_loss: 1.4734, weighted_loss: 0.9527, label: 1, bag_size: 12795\n",
      "batch 699, loss: 0.6206, instance_loss: 1.6427, weighted_loss: 0.9272, label: 0, bag_size: 11690\n",
      "batch 719, loss: 0.7809, instance_loss: 0.3325, weighted_loss: 0.6464, label: 0, bag_size: 3654\n",
      "batch 739, loss: 0.4645, instance_loss: 0.0034, weighted_loss: 0.3261, label: 0, bag_size: 23791\n",
      "batch 759, loss: 0.3938, instance_loss: 1.4393, weighted_loss: 0.7074, label: 0, bag_size: 12131\n",
      "batch 779, loss: 0.9286, instance_loss: 0.9000, weighted_loss: 0.9200, label: 1, bag_size: 8438\n",
      "batch 799, loss: 0.8308, instance_loss: 0.3951, weighted_loss: 0.7001, label: 0, bag_size: 19880\n",
      "batch 819, loss: 0.9111, instance_loss: 0.7425, weighted_loss: 0.8605, label: 0, bag_size: 10365\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.8924878345498783: correct 11738/13152\n",
      "class 1 clustering acc 0.3117396593673966: correct 2050/6576\n",
      "Epoch: 0, train_loss: 0.7069, train_clustering_loss:  1.1333, train_error: 0.4915\n",
      "class 0: acc 0.3844221105527638, correct 153/398\n",
      "class 1: acc 0.625, correct 265/424\n",
      "\n",
      "Val Set, val_loss: 0.6819, val_error: 0.4130, auc: 0.7727\n",
      "class 0 clustering acc 0.9558423913043478: correct 1407/1472\n",
      "class 1 clustering acc 0.6086956521739131: correct 448/736\n",
      "class 0: acc 0.0, correct 0/38\n",
      "class 1: acc 1.0, correct 54/54\n",
      "Validation loss decreased (inf --> 0.681883).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7005, instance_loss: 0.1876, weighted_loss: 0.5466, label: 1, bag_size: 5731\n",
      "batch 39, loss: 0.6267, instance_loss: 0.6464, weighted_loss: 0.6326, label: 0, bag_size: 19390\n",
      "batch 59, loss: 0.5334, instance_loss: 0.0455, weighted_loss: 0.3870, label: 0, bag_size: 3908\n",
      "batch 79, loss: 0.8533, instance_loss: 0.8643, weighted_loss: 0.8566, label: 1, bag_size: 10498\n",
      "batch 99, loss: 0.7593, instance_loss: 0.6164, weighted_loss: 0.7164, label: 0, bag_size: 5225\n",
      "batch 119, loss: 0.9929, instance_loss: 0.2553, weighted_loss: 0.7716, label: 1, bag_size: 7613\n",
      "batch 139, loss: 0.8412, instance_loss: 0.6172, weighted_loss: 0.7740, label: 0, bag_size: 16341\n",
      "batch 159, loss: 0.7842, instance_loss: 1.6753, weighted_loss: 1.0515, label: 1, bag_size: 7217\n",
      "batch 179, loss: 0.7768, instance_loss: 1.0266, weighted_loss: 0.8517, label: 1, bag_size: 2904\n",
      "batch 199, loss: 0.7112, instance_loss: 0.3719, weighted_loss: 0.6094, label: 0, bag_size: 18132\n",
      "batch 219, loss: 0.5584, instance_loss: 0.3375, weighted_loss: 0.4921, label: 0, bag_size: 9930\n",
      "batch 239, loss: 0.6152, instance_loss: 0.2679, weighted_loss: 0.5110, label: 0, bag_size: 11146\n",
      "batch 259, loss: 0.6663, instance_loss: 1.6771, weighted_loss: 0.9696, label: 1, bag_size: 1845\n",
      "batch 279, loss: 0.7007, instance_loss: 0.3978, weighted_loss: 0.6099, label: 1, bag_size: 3453\n",
      "batch 299, loss: 0.6566, instance_loss: 0.4585, weighted_loss: 0.5972, label: 1, bag_size: 1123\n",
      "batch 319, loss: 0.7076, instance_loss: 0.3491, weighted_loss: 0.6000, label: 0, bag_size: 5211\n",
      "batch 339, loss: 0.6734, instance_loss: 0.4441, weighted_loss: 0.6046, label: 0, bag_size: 11727\n",
      "batch 359, loss: 0.6368, instance_loss: 1.6046, weighted_loss: 0.9271, label: 0, bag_size: 1712\n",
      "batch 379, loss: 0.5807, instance_loss: 1.2943, weighted_loss: 0.7948, label: 1, bag_size: 2814\n",
      "batch 399, loss: 0.7003, instance_loss: 0.1536, weighted_loss: 0.5363, label: 1, bag_size: 3652\n",
      "batch 419, loss: 0.6268, instance_loss: 7.7364, weighted_loss: 2.7597, label: 0, bag_size: 2242\n",
      "batch 439, loss: 1.0724, instance_loss: 0.0499, weighted_loss: 0.7656, label: 0, bag_size: 31085\n",
      "batch 459, loss: 0.5130, instance_loss: 0.7148, weighted_loss: 0.5736, label: 1, bag_size: 3437\n",
      "batch 479, loss: 0.6060, instance_loss: 1.4685, weighted_loss: 0.8647, label: 0, bag_size: 11477\n",
      "batch 499, loss: 0.9967, instance_loss: 0.6159, weighted_loss: 0.8824, label: 1, bag_size: 13365\n",
      "batch 519, loss: 0.5792, instance_loss: 0.2479, weighted_loss: 0.4798, label: 0, bag_size: 11690\n",
      "batch 539, loss: 1.0636, instance_loss: 0.5198, weighted_loss: 0.9004, label: 1, bag_size: 3674\n",
      "batch 559, loss: 0.8803, instance_loss: 3.3155, weighted_loss: 1.6109, label: 1, bag_size: 3121\n",
      "batch 579, loss: 0.9701, instance_loss: 1.1929, weighted_loss: 1.0369, label: 1, bag_size: 13362\n",
      "batch 599, loss: 0.7072, instance_loss: 0.8725, weighted_loss: 0.7568, label: 0, bag_size: 18954\n",
      "batch 619, loss: 0.9457, instance_loss: 0.8502, weighted_loss: 0.9171, label: 1, bag_size: 4821\n",
      "batch 639, loss: 0.5507, instance_loss: 0.0282, weighted_loss: 0.3940, label: 0, bag_size: 19659\n",
      "batch 659, loss: 0.6286, instance_loss: 1.0529, weighted_loss: 0.7559, label: 1, bag_size: 1242\n",
      "batch 679, loss: 0.7796, instance_loss: 0.0151, weighted_loss: 0.5503, label: 0, bag_size: 32227\n",
      "batch 699, loss: 0.6816, instance_loss: 0.1280, weighted_loss: 0.5155, label: 0, bag_size: 16211\n",
      "batch 719, loss: 0.8017, instance_loss: 1.3134, weighted_loss: 0.9552, label: 0, bag_size: 1452\n",
      "batch 739, loss: 0.8648, instance_loss: 0.3522, weighted_loss: 0.7110, label: 1, bag_size: 12460\n",
      "batch 759, loss: 0.6877, instance_loss: 0.0427, weighted_loss: 0.4942, label: 1, bag_size: 5345\n",
      "batch 779, loss: 0.7925, instance_loss: 0.9176, weighted_loss: 0.8300, label: 1, bag_size: 3856\n",
      "batch 799, loss: 0.7854, instance_loss: 0.0836, weighted_loss: 0.5748, label: 1, bag_size: 22286\n",
      "batch 819, loss: 0.7925, instance_loss: 0.1540, weighted_loss: 0.6010, label: 1, bag_size: 1249\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9251064476885644: correct 12167/13152\n",
      "class 1 clustering acc 0.5022810218978102: correct 3303/6576\n",
      "Epoch: 1, train_loss: 0.7100, train_clustering_loss:  0.7639, train_error: 0.5158\n",
      "class 0: acc 0.6042154566744731, correct 258/427\n",
      "class 1: acc 0.35443037974683544, correct 140/395\n",
      "\n",
      "Val Set, val_loss: 0.6927, val_error: 0.4130, auc: 0.8765\n",
      "class 0 clustering acc 0.9130434782608695: correct 1344/1472\n",
      "class 1 clustering acc 0.6114130434782609: correct 450/736\n",
      "class 0: acc 0.0, correct 0/38\n",
      "class 1: acc 1.0, correct 54/54\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5153, instance_loss: 2.1976, weighted_loss: 1.0200, label: 1, bag_size: 10966\n",
      "batch 39, loss: 1.0514, instance_loss: 0.2937, weighted_loss: 0.8241, label: 0, bag_size: 10535\n",
      "batch 59, loss: 0.5429, instance_loss: 0.1670, weighted_loss: 0.4301, label: 0, bag_size: 32227\n",
      "batch 79, loss: 0.4105, instance_loss: 0.3745, weighted_loss: 0.3997, label: 0, bag_size: 10942\n",
      "batch 99, loss: 0.9083, instance_loss: 0.1404, weighted_loss: 0.6780, label: 1, bag_size: 7148\n",
      "batch 119, loss: 0.5671, instance_loss: 0.0074, weighted_loss: 0.3992, label: 0, bag_size: 19470\n",
      "batch 139, loss: 1.0678, instance_loss: 1.1058, weighted_loss: 1.0792, label: 1, bag_size: 2790\n",
      "batch 159, loss: 1.0003, instance_loss: 0.3245, weighted_loss: 0.7975, label: 1, bag_size: 6453\n",
      "batch 179, loss: 0.5481, instance_loss: 0.5618, weighted_loss: 0.5522, label: 0, bag_size: 2303\n",
      "batch 199, loss: 0.4166, instance_loss: 0.0116, weighted_loss: 0.2951, label: 0, bag_size: 18944\n",
      "batch 219, loss: 1.2604, instance_loss: 0.0220, weighted_loss: 0.8889, label: 1, bag_size: 13051\n",
      "batch 239, loss: 0.5095, instance_loss: 0.3611, weighted_loss: 0.4650, label: 0, bag_size: 4345\n",
      "batch 259, loss: 0.5213, instance_loss: 0.0297, weighted_loss: 0.3738, label: 0, bag_size: 23996\n",
      "batch 279, loss: 0.5107, instance_loss: 0.2177, weighted_loss: 0.4228, label: 0, bag_size: 18574\n",
      "batch 299, loss: 0.7416, instance_loss: 0.6326, weighted_loss: 0.7089, label: 1, bag_size: 2136\n",
      "batch 319, loss: 0.6878, instance_loss: 1.6159, weighted_loss: 0.9663, label: 0, bag_size: 2918\n",
      "batch 339, loss: 0.7307, instance_loss: 0.4669, weighted_loss: 0.6515, label: 1, bag_size: 2405\n",
      "batch 359, loss: 0.4169, instance_loss: 0.0287, weighted_loss: 0.3004, label: 1, bag_size: 11875\n",
      "batch 379, loss: 0.3705, instance_loss: 0.0420, weighted_loss: 0.2720, label: 1, bag_size: 10482\n",
      "batch 399, loss: 0.5665, instance_loss: 0.9593, weighted_loss: 0.6843, label: 1, bag_size: 898\n",
      "batch 419, loss: 0.7770, instance_loss: 1.6551, weighted_loss: 1.0404, label: 0, bag_size: 3802\n",
      "batch 439, loss: 0.7144, instance_loss: 1.2243, weighted_loss: 0.8674, label: 0, bag_size: 2213\n",
      "batch 459, loss: 0.9250, instance_loss: 0.0539, weighted_loss: 0.6637, label: 1, bag_size: 10867\n",
      "batch 479, loss: 0.5940, instance_loss: 0.2759, weighted_loss: 0.4986, label: 0, bag_size: 1072\n",
      "batch 499, loss: 0.7882, instance_loss: 0.2691, weighted_loss: 0.6324, label: 1, bag_size: 3968\n",
      "batch 519, loss: 0.5541, instance_loss: 0.4985, weighted_loss: 0.5374, label: 0, bag_size: 2654\n",
      "batch 539, loss: 0.9701, instance_loss: 0.4955, weighted_loss: 0.8277, label: 1, bag_size: 4102\n",
      "batch 559, loss: 0.6706, instance_loss: 0.8896, weighted_loss: 0.7363, label: 0, bag_size: 11194\n",
      "batch 579, loss: 0.4780, instance_loss: 0.3551, weighted_loss: 0.4412, label: 1, bag_size: 1764\n",
      "batch 599, loss: 0.9885, instance_loss: 0.2732, weighted_loss: 0.7739, label: 0, bag_size: 2654\n",
      "batch 619, loss: 0.6122, instance_loss: 0.8689, weighted_loss: 0.6892, label: 1, bag_size: 6928\n",
      "batch 639, loss: 0.7042, instance_loss: 0.0543, weighted_loss: 0.5092, label: 1, bag_size: 15008\n",
      "batch 659, loss: 0.7229, instance_loss: 0.6050, weighted_loss: 0.6876, label: 0, bag_size: 1953\n",
      "batch 679, loss: 0.6636, instance_loss: 0.0893, weighted_loss: 0.4913, label: 0, bag_size: 3399\n",
      "batch 699, loss: 0.5914, instance_loss: 2.2901, weighted_loss: 1.1010, label: 1, bag_size: 2842\n",
      "batch 719, loss: 0.4209, instance_loss: 0.1028, weighted_loss: 0.3255, label: 1, bag_size: 3576\n",
      "batch 739, loss: 0.3717, instance_loss: 0.1302, weighted_loss: 0.2993, label: 1, bag_size: 13026\n",
      "batch 759, loss: 0.6719, instance_loss: 0.2035, weighted_loss: 0.5314, label: 0, bag_size: 13691\n",
      "batch 779, loss: 0.7665, instance_loss: 0.6838, weighted_loss: 0.7417, label: 0, bag_size: 2219\n",
      "batch 799, loss: 0.5349, instance_loss: 0.7979, weighted_loss: 0.6138, label: 1, bag_size: 1755\n",
      "batch 819, loss: 0.4715, instance_loss: 0.0713, weighted_loss: 0.3514, label: 1, bag_size: 16565\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9405413625304136: correct 12370/13152\n",
      "class 1 clustering acc 0.5883515815085159: correct 3869/6576\n",
      "Epoch: 2, train_loss: 0.6964, train_clustering_loss:  0.6194, train_error: 0.4586\n",
      "class 0: acc 0.6029055690072639, correct 249/413\n",
      "class 1: acc 0.4792176039119804, correct 196/409\n",
      "\n",
      "Val Set, val_loss: 0.6828, val_error: 0.4130, auc: 0.9518\n",
      "class 0 clustering acc 0.9802989130434783: correct 1443/1472\n",
      "class 1 clustering acc 0.5475543478260869: correct 403/736\n",
      "class 0: acc 0.0, correct 0/38\n",
      "class 1: acc 1.0, correct 54/54\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.9019, instance_loss: 1.0313, weighted_loss: 0.9407, label: 0, bag_size: 2457\n",
      "batch 39, loss: 0.6049, instance_loss: 0.0113, weighted_loss: 0.4268, label: 0, bag_size: 23368\n",
      "batch 59, loss: 0.9354, instance_loss: 0.1212, weighted_loss: 0.6911, label: 1, bag_size: 4821\n",
      "batch 79, loss: 0.4711, instance_loss: 0.0493, weighted_loss: 0.3446, label: 0, bag_size: 23796\n",
      "batch 99, loss: 0.9204, instance_loss: 0.3090, weighted_loss: 0.7369, label: 1, bag_size: 4102\n",
      "batch 119, loss: 0.9617, instance_loss: 2.8816, weighted_loss: 1.5377, label: 1, bag_size: 12494\n",
      "batch 139, loss: 0.4747, instance_loss: 0.8498, weighted_loss: 0.5872, label: 0, bag_size: 1824\n",
      "batch 159, loss: 0.7107, instance_loss: 1.3521, weighted_loss: 0.9031, label: 1, bag_size: 14604\n",
      "batch 179, loss: 0.6575, instance_loss: 0.1695, weighted_loss: 0.5111, label: 1, bag_size: 4239\n",
      "batch 199, loss: 0.5319, instance_loss: 0.1018, weighted_loss: 0.4029, label: 1, bag_size: 6752\n",
      "batch 219, loss: 0.7348, instance_loss: 0.0911, weighted_loss: 0.5417, label: 0, bag_size: 5297\n",
      "batch 239, loss: 0.6501, instance_loss: 0.1313, weighted_loss: 0.4944, label: 0, bag_size: 19667\n",
      "batch 259, loss: 0.7599, instance_loss: 0.2485, weighted_loss: 0.6065, label: 1, bag_size: 2678\n",
      "batch 279, loss: 1.0739, instance_loss: 0.0616, weighted_loss: 0.7702, label: 0, bag_size: 21385\n",
      "batch 299, loss: 0.4250, instance_loss: 1.1383, weighted_loss: 0.6390, label: 1, bag_size: 10460\n",
      "batch 319, loss: 0.6248, instance_loss: 0.0016, weighted_loss: 0.4379, label: 0, bag_size: 10304\n",
      "batch 339, loss: 0.7328, instance_loss: 1.0752, weighted_loss: 0.8355, label: 1, bag_size: 1064\n",
      "batch 359, loss: 0.6425, instance_loss: 0.3125, weighted_loss: 0.5435, label: 0, bag_size: 1508\n",
      "batch 379, loss: 0.5986, instance_loss: 0.4147, weighted_loss: 0.5434, label: 0, bag_size: 16992\n",
      "batch 399, loss: 0.8029, instance_loss: 0.2202, weighted_loss: 0.6281, label: 1, bag_size: 3437\n",
      "batch 419, loss: 0.7657, instance_loss: 0.2208, weighted_loss: 0.6022, label: 1, bag_size: 7669\n",
      "batch 439, loss: 1.0086, instance_loss: 0.3500, weighted_loss: 0.8110, label: 1, bag_size: 8602\n",
      "batch 459, loss: 1.0547, instance_loss: 0.0070, weighted_loss: 0.7404, label: 1, bag_size: 6164\n",
      "batch 479, loss: 0.4216, instance_loss: 0.0114, weighted_loss: 0.2985, label: 0, bag_size: 23037\n",
      "batch 499, loss: 1.1009, instance_loss: 0.1076, weighted_loss: 0.8029, label: 1, bag_size: 10432\n",
      "batch 519, loss: 0.7272, instance_loss: 0.0312, weighted_loss: 0.5184, label: 1, bag_size: 16379\n",
      "batch 539, loss: 0.6376, instance_loss: 1.3181, weighted_loss: 0.8417, label: 1, bag_size: 12712\n",
      "batch 559, loss: 0.8262, instance_loss: 0.3469, weighted_loss: 0.6824, label: 0, bag_size: 1920\n",
      "batch 579, loss: 0.5232, instance_loss: 0.0200, weighted_loss: 0.3722, label: 1, bag_size: 10501\n",
      "batch 599, loss: 0.3921, instance_loss: 0.7673, weighted_loss: 0.5047, label: 1, bag_size: 1845\n",
      "batch 619, loss: 1.3011, instance_loss: 0.3682, weighted_loss: 1.0212, label: 0, bag_size: 1891\n",
      "batch 639, loss: 0.9573, instance_loss: 0.0090, weighted_loss: 0.6728, label: 0, bag_size: 21404\n",
      "batch 659, loss: 0.7037, instance_loss: 0.6196, weighted_loss: 0.6785, label: 0, bag_size: 1213\n",
      "batch 679, loss: 0.6689, instance_loss: 0.0275, weighted_loss: 0.4765, label: 1, bag_size: 10460\n",
      "batch 699, loss: 0.6712, instance_loss: 0.3837, weighted_loss: 0.5849, label: 1, bag_size: 2193\n",
      "batch 719, loss: 0.8519, instance_loss: 1.8194, weighted_loss: 1.1422, label: 1, bag_size: 7381\n",
      "batch 739, loss: 0.3877, instance_loss: 0.6772, weighted_loss: 0.4746, label: 0, bag_size: 11390\n",
      "batch 759, loss: 0.4487, instance_loss: 0.0466, weighted_loss: 0.3281, label: 0, bag_size: 10814\n",
      "batch 779, loss: 0.6006, instance_loss: 0.0957, weighted_loss: 0.4491, label: 0, bag_size: 12796\n",
      "batch 799, loss: 0.7370, instance_loss: 0.0176, weighted_loss: 0.5212, label: 0, bag_size: 18132\n",
      "batch 819, loss: 0.6898, instance_loss: 0.0039, weighted_loss: 0.4840, label: 1, bag_size: 15093\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9449513381995134: correct 12428/13152\n",
      "class 1 clustering acc 0.6180048661800487: correct 4064/6576\n",
      "Epoch: 3, train_loss: 0.7030, train_clustering_loss:  0.5806, train_error: 0.4915\n",
      "class 0: acc 0.5714285714285714, correct 244/427\n",
      "class 1: acc 0.44050632911392407, correct 174/395\n",
      "\n",
      "Val Set, val_loss: 0.6794, val_error: 0.4130, auc: 0.9591\n",
      "class 0 clustering acc 0.953125: correct 1403/1472\n",
      "class 1 clustering acc 0.6168478260869565: correct 454/736\n",
      "class 0: acc 0.0, correct 0/38\n",
      "class 1: acc 1.0, correct 54/54\n",
      "Validation loss decreased (0.681883 --> 0.679400).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6423, instance_loss: 0.0850, weighted_loss: 0.4751, label: 0, bag_size: 20478\n",
      "batch 39, loss: 0.4110, instance_loss: 0.0207, weighted_loss: 0.2939, label: 0, bag_size: 2998\n",
      "batch 59, loss: 0.4597, instance_loss: 2.0745, weighted_loss: 0.9441, label: 0, bag_size: 2815\n",
      "batch 79, loss: 0.4997, instance_loss: 1.0451, weighted_loss: 0.6633, label: 0, bag_size: 21361\n",
      "batch 99, loss: 0.9107, instance_loss: 0.2543, weighted_loss: 0.7138, label: 1, bag_size: 2193\n",
      "batch 119, loss: 0.7317, instance_loss: 0.0854, weighted_loss: 0.5378, label: 1, bag_size: 9955\n",
      "batch 139, loss: 0.6292, instance_loss: 0.0051, weighted_loss: 0.4420, label: 0, bag_size: 15841\n",
      "batch 159, loss: 0.7268, instance_loss: 0.0274, weighted_loss: 0.5170, label: 1, bag_size: 18699\n",
      "batch 179, loss: 0.5467, instance_loss: 0.2648, weighted_loss: 0.4622, label: 0, bag_size: 5999\n",
      "batch 199, loss: 0.5736, instance_loss: 0.1056, weighted_loss: 0.4332, label: 1, bag_size: 6781\n",
      "batch 219, loss: 0.8247, instance_loss: 1.7252, weighted_loss: 1.0948, label: 0, bag_size: 4523\n",
      "batch 239, loss: 0.9113, instance_loss: 0.3585, weighted_loss: 0.7455, label: 0, bag_size: 2467\n",
      "batch 259, loss: 0.5107, instance_loss: 0.2247, weighted_loss: 0.4249, label: 0, bag_size: 21138\n",
      "batch 279, loss: 0.4587, instance_loss: 0.0201, weighted_loss: 0.3271, label: 0, bag_size: 9252\n",
      "batch 299, loss: 0.7846, instance_loss: 2.2198, weighted_loss: 1.2152, label: 1, bag_size: 14604\n",
      "batch 319, loss: 0.5357, instance_loss: 0.3166, weighted_loss: 0.4700, label: 0, bag_size: 19435\n",
      "batch 339, loss: 0.7219, instance_loss: 0.2600, weighted_loss: 0.5833, label: 0, bag_size: 11199\n",
      "batch 359, loss: 1.0778, instance_loss: 0.7633, weighted_loss: 0.9834, label: 0, bag_size: 2959\n",
      "batch 379, loss: 0.7605, instance_loss: 0.8349, weighted_loss: 0.7828, label: 0, bag_size: 18777\n",
      "batch 399, loss: 0.8354, instance_loss: 0.0200, weighted_loss: 0.5908, label: 1, bag_size: 5864\n",
      "batch 419, loss: 0.7745, instance_loss: 0.0018, weighted_loss: 0.5427, label: 1, bag_size: 7351\n",
      "batch 439, loss: 0.8533, instance_loss: 2.2692, weighted_loss: 1.2780, label: 0, bag_size: 5409\n",
      "batch 459, loss: 0.5853, instance_loss: 0.5065, weighted_loss: 0.5617, label: 0, bag_size: 11607\n",
      "batch 479, loss: 0.7483, instance_loss: 1.8644, weighted_loss: 1.0831, label: 1, bag_size: 898\n",
      "batch 499, loss: 0.4650, instance_loss: 0.0612, weighted_loss: 0.3439, label: 1, bag_size: 1759\n",
      "batch 519, loss: 0.6066, instance_loss: 0.2570, weighted_loss: 0.5017, label: 1, bag_size: 3968\n",
      "batch 539, loss: 0.6151, instance_loss: 0.2510, weighted_loss: 0.5059, label: 1, bag_size: 2137\n",
      "batch 559, loss: 0.8871, instance_loss: 1.1112, weighted_loss: 0.9543, label: 1, bag_size: 16565\n",
      "batch 579, loss: 1.2032, instance_loss: 1.4358, weighted_loss: 1.2730, label: 1, bag_size: 1437\n",
      "batch 599, loss: 0.6408, instance_loss: 0.1157, weighted_loss: 0.4832, label: 0, bag_size: 7605\n",
      "batch 619, loss: 0.7219, instance_loss: 2.1821, weighted_loss: 1.1599, label: 0, bag_size: 9132\n",
      "batch 639, loss: 0.6113, instance_loss: 0.0377, weighted_loss: 0.4392, label: 1, bag_size: 19832\n",
      "batch 659, loss: 0.6398, instance_loss: 0.5075, weighted_loss: 0.6001, label: 1, bag_size: 2179\n",
      "batch 679, loss: 0.6434, instance_loss: 0.0672, weighted_loss: 0.4706, label: 0, bag_size: 9069\n",
      "batch 699, loss: 0.6417, instance_loss: 0.1154, weighted_loss: 0.4838, label: 1, bag_size: 1819\n",
      "batch 719, loss: 0.5975, instance_loss: 0.1980, weighted_loss: 0.4777, label: 0, bag_size: 890\n",
      "batch 739, loss: 0.6013, instance_loss: 0.2044, weighted_loss: 0.4822, label: 0, bag_size: 5551\n",
      "batch 759, loss: 0.6673, instance_loss: 0.9868, weighted_loss: 0.7631, label: 0, bag_size: 4997\n",
      "batch 779, loss: 0.7979, instance_loss: 2.0734, weighted_loss: 1.1806, label: 1, bag_size: 898\n",
      "batch 799, loss: 0.7203, instance_loss: 0.0573, weighted_loss: 0.5214, label: 0, bag_size: 19472\n",
      "batch 819, loss: 1.0235, instance_loss: 2.9715, weighted_loss: 1.6079, label: 1, bag_size: 20870\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9520985401459854: correct 12522/13152\n",
      "class 1 clustering acc 0.6881082725060828: correct 4525/6576\n",
      "Epoch: 4, train_loss: 0.6964, train_clustering_loss:  0.4855, train_error: 0.4708\n",
      "class 0: acc 0.6527777777777778, correct 282/432\n",
      "class 1: acc 0.3923076923076923, correct 153/390\n",
      "\n",
      "Val Set, val_loss: 0.7501, val_error: 0.5870, auc: 0.9552\n",
      "class 0 clustering acc 0.9850543478260869: correct 1450/1472\n",
      "class 1 clustering acc 0.7717391304347826: correct 568/736\n",
      "class 0: acc 1.0, correct 38/38\n",
      "class 1: acc 0.0, correct 0/54\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6338, instance_loss: 0.1243, weighted_loss: 0.4809, label: 0, bag_size: 11654\n",
      "batch 39, loss: 0.7688, instance_loss: 0.0110, weighted_loss: 0.5415, label: 1, bag_size: 2092\n",
      "batch 59, loss: 0.6434, instance_loss: 0.0086, weighted_loss: 0.4530, label: 0, bag_size: 11654\n",
      "batch 79, loss: 0.6379, instance_loss: 0.4107, weighted_loss: 0.5697, label: 1, bag_size: 7381\n",
      "batch 99, loss: 1.0056, instance_loss: 4.6369, weighted_loss: 2.0950, label: 0, bag_size: 3802\n",
      "batch 119, loss: 0.8749, instance_loss: 0.9631, weighted_loss: 0.9014, label: 0, bag_size: 11922\n",
      "batch 139, loss: 0.6713, instance_loss: 0.0883, weighted_loss: 0.4964, label: 0, bag_size: 7917\n",
      "batch 159, loss: 0.8064, instance_loss: 0.4006, weighted_loss: 0.6847, label: 1, bag_size: 5864\n",
      "batch 179, loss: 0.8310, instance_loss: 0.7314, weighted_loss: 0.8011, label: 0, bag_size: 25814\n",
      "batch 199, loss: 0.7521, instance_loss: 0.3695, weighted_loss: 0.6374, label: 0, bag_size: 2873\n",
      "batch 219, loss: 0.5138, instance_loss: 1.1727, weighted_loss: 0.7115, label: 1, bag_size: 11729\n",
      "batch 239, loss: 0.5217, instance_loss: 0.1282, weighted_loss: 0.4036, label: 1, bag_size: 10966\n",
      "batch 259, loss: 0.7438, instance_loss: 0.1082, weighted_loss: 0.5531, label: 0, bag_size: 18076\n",
      "batch 279, loss: 0.7582, instance_loss: 0.3841, weighted_loss: 0.6460, label: 0, bag_size: 5161\n",
      "batch 299, loss: 0.5765, instance_loss: 0.0082, weighted_loss: 0.4060, label: 1, bag_size: 6599\n",
      "batch 319, loss: 0.5954, instance_loss: 0.7663, weighted_loss: 0.6467, label: 0, bag_size: 12083\n",
      "batch 339, loss: 0.7494, instance_loss: 0.4649, weighted_loss: 0.6641, label: 0, bag_size: 705\n",
      "batch 359, loss: 0.4722, instance_loss: 0.0058, weighted_loss: 0.3323, label: 1, bag_size: 14230\n",
      "batch 379, loss: 0.7677, instance_loss: 0.0339, weighted_loss: 0.5475, label: 0, bag_size: 15898\n",
      "batch 399, loss: 0.8941, instance_loss: 3.2745, weighted_loss: 1.6082, label: 1, bag_size: 25831\n",
      "batch 419, loss: 0.8254, instance_loss: 0.6412, weighted_loss: 0.7701, label: 0, bag_size: 25027\n",
      "batch 439, loss: 0.7691, instance_loss: 0.0332, weighted_loss: 0.5483, label: 0, bag_size: 11654\n",
      "batch 459, loss: 0.7348, instance_loss: 0.1442, weighted_loss: 0.5576, label: 0, bag_size: 1120\n",
      "batch 479, loss: 0.7515, instance_loss: 2.5796, weighted_loss: 1.3000, label: 1, bag_size: 1703\n",
      "batch 499, loss: 0.5224, instance_loss: 0.1229, weighted_loss: 0.4026, label: 1, bag_size: 1819\n",
      "batch 519, loss: 0.4268, instance_loss: 0.0498, weighted_loss: 0.3137, label: 1, bag_size: 19832\n",
      "batch 539, loss: 0.6757, instance_loss: 0.5641, weighted_loss: 0.6422, label: 1, bag_size: 4308\n",
      "batch 559, loss: 0.5474, instance_loss: 0.2994, weighted_loss: 0.4730, label: 1, bag_size: 2495\n",
      "batch 579, loss: 0.5859, instance_loss: 0.0073, weighted_loss: 0.4123, label: 1, bag_size: 16051\n",
      "batch 599, loss: 0.6291, instance_loss: 0.4123, weighted_loss: 0.5640, label: 1, bag_size: 7066\n",
      "batch 619, loss: 1.0412, instance_loss: 3.1869, weighted_loss: 1.6849, label: 0, bag_size: 2732\n",
      "batch 639, loss: 0.9032, instance_loss: 0.2486, weighted_loss: 0.7068, label: 0, bag_size: 16211\n",
      "batch 659, loss: 0.8301, instance_loss: 0.0493, weighted_loss: 0.5958, label: 0, bag_size: 13023\n",
      "batch 679, loss: 0.6074, instance_loss: 0.1240, weighted_loss: 0.4624, label: 1, bag_size: 629\n",
      "batch 699, loss: 0.5816, instance_loss: 0.0359, weighted_loss: 0.4179, label: 0, bag_size: 13795\n",
      "batch 719, loss: 0.5107, instance_loss: 0.1139, weighted_loss: 0.3916, label: 0, bag_size: 18738\n",
      "batch 739, loss: 0.5281, instance_loss: 1.2780, weighted_loss: 0.7531, label: 0, bag_size: 3160\n",
      "batch 759, loss: 0.5087, instance_loss: 0.0702, weighted_loss: 0.3772, label: 0, bag_size: 18516\n",
      "batch 779, loss: 0.4048, instance_loss: 0.0017, weighted_loss: 0.2839, label: 0, bag_size: 23037\n",
      "batch 799, loss: 0.3263, instance_loss: 0.1562, weighted_loss: 0.2753, label: 0, bag_size: 18574\n",
      "batch 819, loss: 0.5666, instance_loss: 0.0423, weighted_loss: 0.4093, label: 0, bag_size: 7141\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9593978102189781: correct 12618/13152\n",
      "class 1 clustering acc 0.7405717761557178: correct 4870/6576\n",
      "Epoch: 5, train_loss: 0.6660, train_clustering_loss:  0.4426, train_error: 0.4100\n",
      "class 0: acc 0.5100502512562815, correct 203/398\n",
      "class 1: acc 0.6650943396226415, correct 282/424\n",
      "\n",
      "Val Set, val_loss: 0.6339, val_error: 0.3043, auc: 0.9669\n",
      "class 0 clustering acc 0.9449728260869565: correct 1391/1472\n",
      "class 1 clustering acc 0.751358695652174: correct 553/736\n",
      "class 0: acc 1.0, correct 38/38\n",
      "class 1: acc 0.48148148148148145, correct 26/54\n",
      "Validation loss decreased (0.679400 --> 0.633942).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4676, instance_loss: 0.1733, weighted_loss: 0.3793, label: 0, bag_size: 3502\n",
      "batch 39, loss: 0.7127, instance_loss: 0.7060, weighted_loss: 0.7107, label: 1, bag_size: 4786\n",
      "batch 59, loss: 1.0056, instance_loss: 0.3483, weighted_loss: 0.8084, label: 0, bag_size: 22800\n",
      "batch 79, loss: 0.5285, instance_loss: 0.1239, weighted_loss: 0.4072, label: 1, bag_size: 3683\n",
      "batch 99, loss: 0.4405, instance_loss: 0.0072, weighted_loss: 0.3105, label: 1, bag_size: 8216\n",
      "batch 119, loss: 0.4365, instance_loss: 0.0242, weighted_loss: 0.3128, label: 1, bag_size: 12425\n",
      "batch 139, loss: 0.6596, instance_loss: 0.2773, weighted_loss: 0.5449, label: 0, bag_size: 12899\n",
      "batch 159, loss: 0.5245, instance_loss: 0.0402, weighted_loss: 0.3792, label: 0, bag_size: 21576\n",
      "batch 179, loss: 0.4637, instance_loss: 0.3423, weighted_loss: 0.4273, label: 0, bag_size: 3787\n",
      "batch 199, loss: 0.5436, instance_loss: 0.0332, weighted_loss: 0.3905, label: 1, bag_size: 7468\n",
      "batch 219, loss: 0.4944, instance_loss: 0.0036, weighted_loss: 0.3471, label: 1, bag_size: 14223\n",
      "batch 239, loss: 0.4528, instance_loss: 0.2416, weighted_loss: 0.3895, label: 0, bag_size: 2006\n",
      "batch 259, loss: 0.6881, instance_loss: 0.0205, weighted_loss: 0.4878, label: 1, bag_size: 9878\n",
      "batch 279, loss: 0.8736, instance_loss: 0.6818, weighted_loss: 0.8161, label: 0, bag_size: 9616\n",
      "batch 299, loss: 0.5218, instance_loss: 0.1856, weighted_loss: 0.4210, label: 0, bag_size: 12083\n",
      "batch 319, loss: 0.3060, instance_loss: 0.0140, weighted_loss: 0.2184, label: 0, bag_size: 10068\n",
      "batch 339, loss: 0.8703, instance_loss: 0.0121, weighted_loss: 0.6128, label: 1, bag_size: 14779\n",
      "batch 359, loss: 0.6319, instance_loss: 0.2756, weighted_loss: 0.5250, label: 0, bag_size: 24382\n",
      "batch 379, loss: 0.5534, instance_loss: 0.0052, weighted_loss: 0.3889, label: 0, bag_size: 21319\n",
      "batch 399, loss: 0.3060, instance_loss: 0.1494, weighted_loss: 0.2590, label: 0, bag_size: 14885\n",
      "batch 419, loss: 0.5699, instance_loss: 0.2773, weighted_loss: 0.4821, label: 0, bag_size: 1953\n",
      "batch 439, loss: 0.4520, instance_loss: 0.1383, weighted_loss: 0.3578, label: 1, bag_size: 617\n",
      "batch 459, loss: 1.0628, instance_loss: 3.1177, weighted_loss: 1.6793, label: 0, bag_size: 2732\n",
      "batch 479, loss: 0.5116, instance_loss: 0.0240, weighted_loss: 0.3653, label: 0, bag_size: 13892\n",
      "batch 499, loss: 0.4164, instance_loss: 0.0898, weighted_loss: 0.3184, label: 0, bag_size: 3657\n",
      "batch 519, loss: 0.2195, instance_loss: 0.0004, weighted_loss: 0.1538, label: 0, bag_size: 13892\n",
      "batch 539, loss: 0.4065, instance_loss: 0.1407, weighted_loss: 0.3268, label: 0, bag_size: 3502\n",
      "batch 559, loss: 0.6203, instance_loss: 0.0123, weighted_loss: 0.4379, label: 1, bag_size: 10396\n",
      "batch 579, loss: 0.4257, instance_loss: 0.0009, weighted_loss: 0.2983, label: 1, bag_size: 6731\n",
      "batch 599, loss: 0.5264, instance_loss: 0.0009, weighted_loss: 0.3688, label: 1, bag_size: 6606\n",
      "batch 619, loss: 0.4292, instance_loss: 0.0463, weighted_loss: 0.3143, label: 0, bag_size: 16992\n",
      "batch 639, loss: 0.3541, instance_loss: 0.0170, weighted_loss: 0.2530, label: 0, bag_size: 18076\n",
      "batch 659, loss: 0.7977, instance_loss: 0.0326, weighted_loss: 0.5682, label: 1, bag_size: 16162\n",
      "batch 679, loss: 0.3209, instance_loss: 0.5432, weighted_loss: 0.3876, label: 1, bag_size: 6533\n",
      "batch 699, loss: 0.9408, instance_loss: 0.6812, weighted_loss: 0.8629, label: 0, bag_size: 2732\n",
      "batch 719, loss: 0.2970, instance_loss: 0.1550, weighted_loss: 0.2544, label: 0, bag_size: 9597\n",
      "batch 739, loss: 0.2966, instance_loss: 0.2196, weighted_loss: 0.2735, label: 0, bag_size: 1639\n",
      "batch 759, loss: 0.3872, instance_loss: 0.6211, weighted_loss: 0.4574, label: 0, bag_size: 6884\n",
      "batch 779, loss: 0.4854, instance_loss: 0.0578, weighted_loss: 0.3571, label: 0, bag_size: 11259\n",
      "batch 799, loss: 0.5191, instance_loss: 0.1982, weighted_loss: 0.4229, label: 0, bag_size: 2351\n",
      "batch 819, loss: 0.5627, instance_loss: 0.0965, weighted_loss: 0.4228, label: 1, bag_size: 699\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9566605839416058: correct 12582/13152\n",
      "class 1 clustering acc 0.7527372262773723: correct 4950/6576\n",
      "Epoch: 6, train_loss: 0.5984, train_clustering_loss:  0.4328, train_error: 0.2786\n",
      "class 0: acc 0.7523584905660378, correct 319/424\n",
      "class 1: acc 0.6884422110552764, correct 274/398\n",
      "\n",
      "Val Set, val_loss: 0.4948, val_error: 0.1196, auc: 0.9659\n",
      "class 0 clustering acc 0.9252717391304348: correct 1362/1472\n",
      "class 1 clustering acc 0.6453804347826086: correct 475/736\n",
      "class 0: acc 0.868421052631579, correct 33/38\n",
      "class 1: acc 0.8888888888888888, correct 48/54\n",
      "Validation loss decreased (0.633942 --> 0.494815).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5785, instance_loss: 0.3982, weighted_loss: 0.5244, label: 0, bag_size: 11122\n",
      "batch 39, loss: 0.4584, instance_loss: 0.0571, weighted_loss: 0.3380, label: 0, bag_size: 2534\n",
      "batch 59, loss: 0.4380, instance_loss: 0.3346, weighted_loss: 0.4070, label: 0, bag_size: 10581\n",
      "batch 79, loss: 0.2596, instance_loss: 0.0023, weighted_loss: 0.1824, label: 1, bag_size: 7583\n",
      "batch 99, loss: 0.3541, instance_loss: 0.0360, weighted_loss: 0.2586, label: 0, bag_size: 17368\n",
      "batch 119, loss: 1.3254, instance_loss: 1.4547, weighted_loss: 1.3642, label: 1, bag_size: 9942\n",
      "batch 139, loss: 0.3506, instance_loss: 0.1482, weighted_loss: 0.2899, label: 1, bag_size: 14223\n",
      "batch 159, loss: 0.8965, instance_loss: 1.4598, weighted_loss: 1.0655, label: 1, bag_size: 4939\n",
      "batch 179, loss: 0.5647, instance_loss: 0.4538, weighted_loss: 0.5314, label: 0, bag_size: 21576\n",
      "batch 199, loss: 0.1706, instance_loss: 0.2850, weighted_loss: 0.2049, label: 1, bag_size: 4367\n",
      "batch 219, loss: 0.4185, instance_loss: 0.2090, weighted_loss: 0.3556, label: 1, bag_size: 2814\n",
      "batch 239, loss: 0.3460, instance_loss: 0.5445, weighted_loss: 0.4055, label: 1, bag_size: 7613\n",
      "batch 259, loss: 0.2115, instance_loss: 0.0249, weighted_loss: 0.1555, label: 1, bag_size: 11600\n",
      "batch 279, loss: 0.3482, instance_loss: 0.2144, weighted_loss: 0.3081, label: 0, bag_size: 3265\n",
      "batch 299, loss: 0.3536, instance_loss: 0.0228, weighted_loss: 0.2544, label: 0, bag_size: 22800\n",
      "batch 319, loss: 0.9050, instance_loss: 0.4891, weighted_loss: 0.7802, label: 0, bag_size: 17368\n",
      "batch 339, loss: 0.1941, instance_loss: 0.0120, weighted_loss: 0.1395, label: 1, bag_size: 5612\n",
      "batch 359, loss: 0.2355, instance_loss: 0.1231, weighted_loss: 0.2018, label: 0, bag_size: 10304\n",
      "batch 379, loss: 0.2130, instance_loss: 0.0002, weighted_loss: 0.1492, label: 0, bag_size: 7709\n",
      "batch 399, loss: 0.4434, instance_loss: 0.1949, weighted_loss: 0.3688, label: 0, bag_size: 21093\n",
      "batch 419, loss: 1.2171, instance_loss: 0.5192, weighted_loss: 1.0077, label: 0, bag_size: 5120\n",
      "batch 439, loss: 0.3010, instance_loss: 0.0597, weighted_loss: 0.2286, label: 1, bag_size: 2638\n",
      "batch 459, loss: 0.7591, instance_loss: 1.7753, weighted_loss: 1.0640, label: 0, bag_size: 2996\n",
      "batch 479, loss: 1.5209, instance_loss: 2.1958, weighted_loss: 1.7234, label: 1, bag_size: 12180\n",
      "batch 499, loss: 0.7376, instance_loss: 1.6621, weighted_loss: 1.0150, label: 0, bag_size: 3399\n",
      "batch 519, loss: 1.2560, instance_loss: 2.0970, weighted_loss: 1.5083, label: 1, bag_size: 1649\n",
      "batch 539, loss: 0.4594, instance_loss: 0.6931, weighted_loss: 0.5295, label: 0, bag_size: 1797\n",
      "batch 559, loss: 0.3256, instance_loss: 0.0431, weighted_loss: 0.2408, label: 1, bag_size: 7613\n",
      "batch 579, loss: 0.1991, instance_loss: 0.0031, weighted_loss: 0.1403, label: 0, bag_size: 12201\n",
      "batch 599, loss: 0.7596, instance_loss: 0.3562, weighted_loss: 0.6386, label: 1, bag_size: 4939\n",
      "batch 619, loss: 0.5206, instance_loss: 0.0043, weighted_loss: 0.3657, label: 1, bag_size: 11316\n",
      "batch 639, loss: 0.2685, instance_loss: 0.2346, weighted_loss: 0.2583, label: 1, bag_size: 10867\n",
      "batch 659, loss: 0.2297, instance_loss: 0.1600, weighted_loss: 0.2088, label: 0, bag_size: 10128\n",
      "batch 679, loss: 0.2927, instance_loss: 0.0187, weighted_loss: 0.2105, label: 0, bag_size: 18415\n",
      "batch 699, loss: 0.2639, instance_loss: 0.0128, weighted_loss: 0.1886, label: 0, bag_size: 11759\n",
      "batch 719, loss: 0.4709, instance_loss: 0.5043, weighted_loss: 0.4809, label: 0, bag_size: 1458\n",
      "batch 739, loss: 0.3861, instance_loss: 0.0783, weighted_loss: 0.2938, label: 0, bag_size: 2044\n",
      "batch 759, loss: 0.8260, instance_loss: 0.0287, weighted_loss: 0.5868, label: 1, bag_size: 4039\n",
      "batch 779, loss: 0.6192, instance_loss: 1.3229, weighted_loss: 0.8303, label: 1, bag_size: 1123\n",
      "batch 799, loss: 0.4460, instance_loss: 0.0901, weighted_loss: 0.3392, label: 0, bag_size: 19518\n",
      "batch 819, loss: 0.2176, instance_loss: 0.0257, weighted_loss: 0.1600, label: 0, bag_size: 12732\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.954455596107056: correct 12553/13152\n",
      "class 1 clustering acc 0.7300790754257908: correct 4801/6576\n",
      "Epoch: 7, train_loss: 0.5108, train_clustering_loss:  0.4429, train_error: 0.2032\n",
      "class 0: acc 0.7813267813267813, correct 318/407\n",
      "class 1: acc 0.8120481927710843, correct 337/415\n",
      "\n",
      "Val Set, val_loss: 0.4726, val_error: 0.2609, auc: 0.9625\n",
      "class 0 clustering acc 0.9510869565217391: correct 1400/1472\n",
      "class 1 clustering acc 0.8165760869565217: correct 601/736\n",
      "class 0: acc 1.0, correct 38/38\n",
      "class 1: acc 0.5555555555555556, correct 30/54\n",
      "Validation loss decreased (0.494815 --> 0.472608).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3579, instance_loss: 0.0055, weighted_loss: 0.2522, label: 1, bag_size: 8026\n",
      "batch 39, loss: 0.4228, instance_loss: 0.0074, weighted_loss: 0.2982, label: 0, bag_size: 27158\n",
      "batch 59, loss: 0.9725, instance_loss: 0.5226, weighted_loss: 0.8375, label: 0, bag_size: 4997\n",
      "batch 79, loss: 0.8973, instance_loss: 0.0103, weighted_loss: 0.6312, label: 1, bag_size: 16703\n",
      "batch 99, loss: 0.1948, instance_loss: 0.0050, weighted_loss: 0.1378, label: 1, bag_size: 11387\n",
      "batch 119, loss: 0.3213, instance_loss: 0.0083, weighted_loss: 0.2274, label: 1, bag_size: 14779\n",
      "batch 139, loss: 0.3779, instance_loss: 0.1192, weighted_loss: 0.3003, label: 0, bag_size: 4902\n",
      "batch 159, loss: 0.1760, instance_loss: 0.0815, weighted_loss: 0.1476, label: 0, bag_size: 13205\n",
      "batch 179, loss: 0.3122, instance_loss: 0.1241, weighted_loss: 0.2558, label: 0, bag_size: 13795\n",
      "batch 199, loss: 0.2921, instance_loss: 0.5658, weighted_loss: 0.3742, label: 1, bag_size: 15125\n",
      "batch 219, loss: 0.1984, instance_loss: 0.2072, weighted_loss: 0.2010, label: 0, bag_size: 9234\n",
      "batch 239, loss: 0.4869, instance_loss: 0.5029, weighted_loss: 0.4917, label: 1, bag_size: 2904\n",
      "batch 259, loss: 0.1641, instance_loss: 0.1772, weighted_loss: 0.1680, label: 1, bag_size: 3640\n",
      "batch 279, loss: 0.1206, instance_loss: 0.0402, weighted_loss: 0.0964, label: 0, bag_size: 21874\n",
      "batch 299, loss: 0.1324, instance_loss: 0.0632, weighted_loss: 0.1116, label: 1, bag_size: 2848\n",
      "batch 319, loss: 0.3400, instance_loss: 0.1324, weighted_loss: 0.2777, label: 1, bag_size: 5731\n",
      "batch 339, loss: 0.2406, instance_loss: 0.0155, weighted_loss: 0.1731, label: 0, bag_size: 17155\n",
      "batch 359, loss: 0.4724, instance_loss: 0.1412, weighted_loss: 0.3731, label: 1, bag_size: 10396\n",
      "batch 379, loss: 0.0930, instance_loss: 0.0008, weighted_loss: 0.0653, label: 1, bag_size: 13051\n",
      "batch 399, loss: 0.1437, instance_loss: 0.0733, weighted_loss: 0.1226, label: 0, bag_size: 1891\n",
      "batch 419, loss: 0.3081, instance_loss: 0.0132, weighted_loss: 0.2196, label: 1, bag_size: 1838\n",
      "batch 439, loss: 0.2119, instance_loss: 0.0145, weighted_loss: 0.1527, label: 1, bag_size: 1638\n",
      "batch 459, loss: 0.3943, instance_loss: 0.0715, weighted_loss: 0.2975, label: 1, bag_size: 10105\n",
      "batch 479, loss: 0.4677, instance_loss: 0.2412, weighted_loss: 0.3997, label: 0, bag_size: 1797\n",
      "batch 499, loss: 0.3164, instance_loss: 0.0817, weighted_loss: 0.2460, label: 1, bag_size: 5903\n",
      "batch 519, loss: 0.8590, instance_loss: 0.7581, weighted_loss: 0.8287, label: 0, bag_size: 1142\n",
      "batch 539, loss: 0.1631, instance_loss: 0.0217, weighted_loss: 0.1207, label: 0, bag_size: 18240\n",
      "batch 559, loss: 0.3474, instance_loss: 0.0277, weighted_loss: 0.2515, label: 0, bag_size: 13591\n",
      "batch 579, loss: 0.4260, instance_loss: 0.0635, weighted_loss: 0.3173, label: 1, bag_size: 8522\n",
      "batch 599, loss: 0.2461, instance_loss: 0.0270, weighted_loss: 0.1804, label: 0, bag_size: 11759\n",
      "batch 619, loss: 0.8687, instance_loss: 0.6611, weighted_loss: 0.8064, label: 1, bag_size: 1746\n",
      "batch 639, loss: 0.2592, instance_loss: 3.2241, weighted_loss: 1.1487, label: 1, bag_size: 19500\n",
      "batch 659, loss: 0.2337, instance_loss: 0.2805, weighted_loss: 0.2477, label: 0, bag_size: 11146\n",
      "batch 679, loss: 0.1925, instance_loss: 0.8403, weighted_loss: 0.3868, label: 0, bag_size: 9930\n",
      "batch 699, loss: 0.0816, instance_loss: 0.0050, weighted_loss: 0.0586, label: 1, bag_size: 9322\n",
      "batch 719, loss: 0.2388, instance_loss: 0.0018, weighted_loss: 0.1677, label: 0, bag_size: 23368\n",
      "batch 739, loss: 0.3467, instance_loss: 0.1212, weighted_loss: 0.2791, label: 1, bag_size: 8103\n",
      "batch 759, loss: 0.1810, instance_loss: 0.1036, weighted_loss: 0.1577, label: 1, bag_size: 4976\n",
      "batch 779, loss: 0.8181, instance_loss: 0.5847, weighted_loss: 0.7481, label: 0, bag_size: 3725\n",
      "batch 799, loss: 1.0936, instance_loss: 1.2179, weighted_loss: 1.1309, label: 1, bag_size: 1831\n",
      "batch 819, loss: 1.0750, instance_loss: 1.1692, weighted_loss: 1.1033, label: 1, bag_size: 10671\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9621350364963503: correct 12654/13152\n",
      "class 1 clustering acc 0.7627737226277372: correct 5016/6576\n",
      "Epoch: 8, train_loss: 0.4227, train_clustering_loss:  0.3835, train_error: 0.1715\n",
      "class 0: acc 0.8144329896907216, correct 316/388\n",
      "class 1: acc 0.8410138248847926, correct 365/434\n",
      "\n",
      "Val Set, val_loss: 0.3610, val_error: 0.1413, auc: 0.9605\n",
      "class 0 clustering acc 0.9592391304347826: correct 1412/1472\n",
      "class 1 clustering acc 0.626358695652174: correct 461/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.7962962962962963, correct 43/54\n",
      "Validation loss decreased (0.472608 --> 0.360970).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1770, instance_loss: 0.0921, weighted_loss: 0.1516, label: 0, bag_size: 19470\n",
      "batch 39, loss: 0.1472, instance_loss: 0.0370, weighted_loss: 0.1142, label: 0, bag_size: 11735\n",
      "batch 59, loss: 0.3616, instance_loss: 0.0998, weighted_loss: 0.2830, label: 0, bag_size: 17155\n",
      "batch 79, loss: 0.4728, instance_loss: 1.4674, weighted_loss: 0.7712, label: 1, bag_size: 6928\n",
      "batch 99, loss: 0.0783, instance_loss: 0.0150, weighted_loss: 0.0593, label: 0, bag_size: 17630\n",
      "batch 119, loss: 0.5449, instance_loss: 0.0234, weighted_loss: 0.3885, label: 1, bag_size: 11642\n",
      "batch 139, loss: 1.0033, instance_loss: 0.7927, weighted_loss: 0.9401, label: 0, bag_size: 2195\n",
      "batch 159, loss: 0.0841, instance_loss: 0.0294, weighted_loss: 0.0677, label: 0, bag_size: 25558\n",
      "batch 179, loss: 0.1298, instance_loss: 0.0525, weighted_loss: 0.1066, label: 0, bag_size: 22426\n",
      "batch 199, loss: 0.2331, instance_loss: 0.0181, weighted_loss: 0.1686, label: 1, bag_size: 5991\n",
      "batch 219, loss: 0.2802, instance_loss: 0.0004, weighted_loss: 0.1963, label: 1, bag_size: 21827\n",
      "batch 239, loss: 0.9043, instance_loss: 0.0383, weighted_loss: 0.6445, label: 0, bag_size: 18738\n",
      "batch 259, loss: 0.0159, instance_loss: 0.1281, weighted_loss: 0.0496, label: 0, bag_size: 8372\n",
      "batch 279, loss: 0.0975, instance_loss: 0.0215, weighted_loss: 0.0747, label: 1, bag_size: 617\n",
      "batch 299, loss: 0.1141, instance_loss: 0.2081, weighted_loss: 0.1423, label: 0, bag_size: 11917\n",
      "batch 319, loss: 0.2295, instance_loss: 0.0631, weighted_loss: 0.1796, label: 1, bag_size: 14230\n",
      "batch 339, loss: 1.8799, instance_loss: 2.2625, weighted_loss: 1.9947, label: 0, bag_size: 2815\n",
      "batch 359, loss: 0.0310, instance_loss: 0.0091, weighted_loss: 0.0244, label: 0, bag_size: 8948\n",
      "batch 379, loss: 0.0708, instance_loss: 0.0113, weighted_loss: 0.0529, label: 0, bag_size: 21682\n",
      "batch 399, loss: 0.5097, instance_loss: 0.4038, weighted_loss: 0.4779, label: 0, bag_size: 2467\n",
      "batch 419, loss: 0.0515, instance_loss: 0.0749, weighted_loss: 0.0585, label: 0, bag_size: 18574\n",
      "batch 439, loss: 0.0606, instance_loss: 0.0874, weighted_loss: 0.0686, label: 1, bag_size: 6171\n",
      "batch 459, loss: 0.9206, instance_loss: 0.1420, weighted_loss: 0.6870, label: 0, bag_size: 2920\n",
      "batch 479, loss: 0.3842, instance_loss: 0.2492, weighted_loss: 0.3437, label: 1, bag_size: 3674\n",
      "batch 499, loss: 0.0542, instance_loss: 0.0442, weighted_loss: 0.0512, label: 0, bag_size: 18240\n",
      "batch 519, loss: 0.6233, instance_loss: 1.9670, weighted_loss: 1.0264, label: 0, bag_size: 7917\n",
      "batch 539, loss: 2.0699, instance_loss: 0.9138, weighted_loss: 1.7231, label: 1, bag_size: 19470\n",
      "batch 559, loss: 0.1186, instance_loss: 0.0175, weighted_loss: 0.0883, label: 0, bag_size: 17268\n",
      "batch 579, loss: 1.5284, instance_loss: 1.7330, weighted_loss: 1.5898, label: 0, bag_size: 2732\n",
      "batch 599, loss: 0.1138, instance_loss: 0.4501, weighted_loss: 0.2147, label: 0, bag_size: 1438\n",
      "batch 619, loss: 1.4780, instance_loss: 0.2343, weighted_loss: 1.1049, label: 1, bag_size: 15192\n",
      "batch 639, loss: 0.9390, instance_loss: 0.4646, weighted_loss: 0.7967, label: 0, bag_size: 3399\n",
      "batch 659, loss: 0.0561, instance_loss: 0.0048, weighted_loss: 0.0407, label: 1, bag_size: 11701\n",
      "batch 679, loss: 0.2247, instance_loss: 0.0143, weighted_loss: 0.1616, label: 0, bag_size: 17482\n",
      "batch 699, loss: 0.4226, instance_loss: 0.0289, weighted_loss: 0.3045, label: 1, bag_size: 6726\n",
      "batch 719, loss: 0.0502, instance_loss: 0.0279, weighted_loss: 0.0435, label: 0, bag_size: 11146\n",
      "batch 739, loss: 1.0268, instance_loss: 1.2257, weighted_loss: 1.0865, label: 0, bag_size: 7428\n",
      "batch 759, loss: 0.8370, instance_loss: 0.0648, weighted_loss: 0.6053, label: 1, bag_size: 2678\n",
      "batch 779, loss: 0.2285, instance_loss: 0.0998, weighted_loss: 0.1899, label: 1, bag_size: 9561\n",
      "batch 799, loss: 0.1241, instance_loss: 0.0000, weighted_loss: 0.0869, label: 1, bag_size: 3968\n",
      "batch 819, loss: 0.0805, instance_loss: 0.0414, weighted_loss: 0.0688, label: 0, bag_size: 13205\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9617548661800487: correct 12649/13152\n",
      "class 1 clustering acc 0.7718978102189781: correct 5076/6576\n",
      "Epoch: 9, train_loss: 0.4125, train_clustering_loss:  0.3969, train_error: 0.1703\n",
      "class 0: acc 0.8436018957345972, correct 356/422\n",
      "class 1: acc 0.815, correct 326/400\n",
      "\n",
      "Val Set, val_loss: 0.3086, val_error: 0.1196, auc: 0.9571\n",
      "class 0 clustering acc 0.9796195652173914: correct 1442/1472\n",
      "class 1 clustering acc 0.8342391304347826: correct 614/736\n",
      "class 0: acc 0.9210526315789473, correct 35/38\n",
      "class 1: acc 0.8518518518518519, correct 46/54\n",
      "Validation loss decreased (0.360970 --> 0.308552).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 1.0445, instance_loss: 0.1662, weighted_loss: 0.7810, label: 0, bag_size: 2070\n",
      "batch 39, loss: 0.4836, instance_loss: 0.2065, weighted_loss: 0.4005, label: 0, bag_size: 2148\n",
      "batch 59, loss: 0.3120, instance_loss: 0.1437, weighted_loss: 0.2615, label: 1, bag_size: 1172\n",
      "batch 79, loss: 0.1405, instance_loss: 0.0285, weighted_loss: 0.1069, label: 1, bag_size: 20161\n",
      "batch 99, loss: 0.1533, instance_loss: 0.0416, weighted_loss: 0.1198, label: 0, bag_size: 10814\n",
      "batch 119, loss: 0.6824, instance_loss: 0.0583, weighted_loss: 0.4952, label: 0, bag_size: 2351\n",
      "batch 139, loss: 0.1542, instance_loss: 0.0403, weighted_loss: 0.1200, label: 0, bag_size: 1639\n",
      "batch 159, loss: 0.0893, instance_loss: 0.0002, weighted_loss: 0.0626, label: 0, bag_size: 22800\n",
      "batch 179, loss: 0.1911, instance_loss: 0.4648, weighted_loss: 0.2732, label: 1, bag_size: 3856\n",
      "batch 199, loss: 0.0628, instance_loss: 0.0089, weighted_loss: 0.0466, label: 1, bag_size: 5221\n",
      "batch 219, loss: 0.0164, instance_loss: 0.0000, weighted_loss: 0.0115, label: 0, bag_size: 17437\n",
      "batch 239, loss: 0.2749, instance_loss: 0.0503, weighted_loss: 0.2075, label: 1, bag_size: 5903\n",
      "batch 259, loss: 0.1591, instance_loss: 0.2800, weighted_loss: 0.1954, label: 1, bag_size: 8012\n",
      "batch 279, loss: 0.1717, instance_loss: 0.0112, weighted_loss: 0.1235, label: 0, bag_size: 12524\n",
      "batch 299, loss: 0.8740, instance_loss: 0.3293, weighted_loss: 0.7106, label: 0, bag_size: 5409\n",
      "batch 319, loss: 0.0885, instance_loss: 0.0531, weighted_loss: 0.0779, label: 0, bag_size: 13777\n",
      "batch 339, loss: 0.0189, instance_loss: 0.0045, weighted_loss: 0.0146, label: 1, bag_size: 4877\n",
      "batch 359, loss: 0.2237, instance_loss: 0.0036, weighted_loss: 0.1577, label: 1, bag_size: 16565\n",
      "batch 379, loss: 0.4015, instance_loss: 0.4998, weighted_loss: 0.4310, label: 1, bag_size: 1497\n",
      "batch 399, loss: 0.6742, instance_loss: 1.3832, weighted_loss: 0.8869, label: 1, bag_size: 11316\n",
      "batch 419, loss: 0.0666, instance_loss: 0.0345, weighted_loss: 0.0570, label: 1, bag_size: 8395\n",
      "batch 439, loss: 0.3579, instance_loss: 0.3427, weighted_loss: 0.3533, label: 0, bag_size: 2236\n",
      "batch 459, loss: 0.0306, instance_loss: 0.0345, weighted_loss: 0.0318, label: 0, bag_size: 11690\n",
      "batch 479, loss: 0.1500, instance_loss: 0.0001, weighted_loss: 0.1050, label: 0, bag_size: 19435\n",
      "batch 499, loss: 0.2965, instance_loss: 0.0114, weighted_loss: 0.2110, label: 0, bag_size: 5999\n",
      "batch 519, loss: 2.2249, instance_loss: 1.9081, weighted_loss: 2.1299, label: 1, bag_size: 898\n",
      "batch 539, loss: 0.4303, instance_loss: 0.0514, weighted_loss: 0.3167, label: 1, bag_size: 2278\n",
      "batch 559, loss: 0.1584, instance_loss: 0.2394, weighted_loss: 0.1827, label: 0, bag_size: 11281\n",
      "batch 579, loss: 0.0502, instance_loss: 0.0611, weighted_loss: 0.0534, label: 1, bag_size: 2405\n",
      "batch 599, loss: 0.0516, instance_loss: 0.0431, weighted_loss: 0.0491, label: 0, bag_size: 23796\n",
      "batch 619, loss: 0.2747, instance_loss: 0.2588, weighted_loss: 0.2699, label: 0, bag_size: 5225\n",
      "batch 639, loss: 1.6113, instance_loss: 0.9527, weighted_loss: 1.4137, label: 0, bag_size: 2467\n",
      "batch 659, loss: 0.0592, instance_loss: 0.0127, weighted_loss: 0.0453, label: 1, bag_size: 22286\n",
      "batch 679, loss: 0.0580, instance_loss: 0.0076, weighted_loss: 0.0429, label: 1, bag_size: 1786\n",
      "batch 699, loss: 0.1140, instance_loss: 0.0048, weighted_loss: 0.0812, label: 1, bag_size: 8680\n",
      "batch 719, loss: 0.1139, instance_loss: 0.3375, weighted_loss: 0.1810, label: 1, bag_size: 16154\n",
      "batch 739, loss: 0.1668, instance_loss: 0.0953, weighted_loss: 0.1454, label: 0, bag_size: 1416\n",
      "batch 759, loss: 0.0288, instance_loss: 0.1891, weighted_loss: 0.0769, label: 0, bag_size: 19466\n",
      "batch 779, loss: 0.2665, instance_loss: 0.3019, weighted_loss: 0.2771, label: 1, bag_size: 2136\n",
      "batch 799, loss: 0.1608, instance_loss: 0.0098, weighted_loss: 0.1155, label: 1, bag_size: 11387\n",
      "batch 819, loss: 0.0537, instance_loss: 0.1689, weighted_loss: 0.0883, label: 1, bag_size: 16051\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.965860705596107: correct 12703/13152\n",
      "class 1 clustering acc 0.7933394160583942: correct 5217/6576\n",
      "Epoch: 10, train_loss: 0.3891, train_clustering_loss:  0.3588, train_error: 0.1569\n",
      "class 0: acc 0.8341968911917098, correct 322/386\n",
      "class 1: acc 0.8509174311926605, correct 371/436\n",
      "\n",
      "Val Set, val_loss: 0.2854, val_error: 0.1196, auc: 0.9625\n",
      "class 0 clustering acc 0.9653532608695652: correct 1421/1472\n",
      "class 1 clustering acc 0.8233695652173914: correct 606/736\n",
      "class 0: acc 0.9210526315789473, correct 35/38\n",
      "class 1: acc 0.8518518518518519, correct 46/54\n",
      "Validation loss decreased (0.308552 --> 0.285389).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 1.5474, instance_loss: 0.4451, weighted_loss: 1.2167, label: 1, bag_size: 2455\n",
      "batch 39, loss: 0.1172, instance_loss: 0.0008, weighted_loss: 0.0823, label: 0, bag_size: 21319\n",
      "batch 59, loss: 0.1431, instance_loss: 0.4299, weighted_loss: 0.2292, label: 1, bag_size: 1412\n",
      "batch 79, loss: 0.0506, instance_loss: 0.0543, weighted_loss: 0.0517, label: 1, bag_size: 2485\n",
      "batch 99, loss: 0.0406, instance_loss: 0.2031, weighted_loss: 0.0894, label: 1, bag_size: 6171\n",
      "batch 119, loss: 0.1191, instance_loss: 0.6796, weighted_loss: 0.2873, label: 0, bag_size: 12561\n",
      "batch 139, loss: 0.3308, instance_loss: 0.3177, weighted_loss: 0.3269, label: 1, bag_size: 10346\n",
      "batch 159, loss: 1.7393, instance_loss: 1.2390, weighted_loss: 1.5893, label: 0, bag_size: 7239\n",
      "batch 179, loss: 0.0720, instance_loss: 0.0000, weighted_loss: 0.0504, label: 1, bag_size: 9878\n",
      "batch 199, loss: 0.0274, instance_loss: 0.1756, weighted_loss: 0.0718, label: 0, bag_size: 19659\n",
      "batch 219, loss: 0.0297, instance_loss: 0.0780, weighted_loss: 0.0442, label: 1, bag_size: 4880\n",
      "batch 239, loss: 0.0310, instance_loss: 0.0091, weighted_loss: 0.0245, label: 0, bag_size: 17791\n",
      "batch 259, loss: 1.9075, instance_loss: 3.2159, weighted_loss: 2.3001, label: 1, bag_size: 16514\n",
      "batch 279, loss: 1.0019, instance_loss: 0.7684, weighted_loss: 0.9318, label: 0, bag_size: 2070\n",
      "batch 299, loss: 1.4074, instance_loss: 1.1835, weighted_loss: 1.3402, label: 1, bag_size: 2681\n",
      "batch 319, loss: 0.0414, instance_loss: 0.0258, weighted_loss: 0.0367, label: 0, bag_size: 13205\n",
      "batch 339, loss: 0.1196, instance_loss: 0.6861, weighted_loss: 0.2895, label: 0, bag_size: 22681\n",
      "batch 359, loss: 0.1514, instance_loss: 0.2414, weighted_loss: 0.1784, label: 0, bag_size: 1712\n",
      "batch 379, loss: 0.0310, instance_loss: 0.0036, weighted_loss: 0.0228, label: 1, bag_size: 9408\n",
      "batch 399, loss: 0.4018, instance_loss: 0.4860, weighted_loss: 0.4271, label: 0, bag_size: 14664\n",
      "batch 419, loss: 0.0202, instance_loss: 0.4269, weighted_loss: 0.1422, label: 0, bag_size: 30751\n",
      "batch 439, loss: 0.0982, instance_loss: 0.3392, weighted_loss: 0.1705, label: 1, bag_size: 6533\n",
      "batch 459, loss: 0.2320, instance_loss: 0.1357, weighted_loss: 0.2031, label: 0, bag_size: 3908\n",
      "batch 479, loss: 0.0557, instance_loss: 0.1804, weighted_loss: 0.0931, label: 1, bag_size: 9955\n",
      "batch 499, loss: 0.1080, instance_loss: 0.0951, weighted_loss: 0.1041, label: 0, bag_size: 4465\n",
      "batch 519, loss: 0.0808, instance_loss: 0.0028, weighted_loss: 0.0574, label: 0, bag_size: 24911\n",
      "batch 539, loss: 0.1438, instance_loss: 0.0049, weighted_loss: 0.1021, label: 1, bag_size: 15093\n",
      "batch 559, loss: 0.0589, instance_loss: 0.0117, weighted_loss: 0.0447, label: 0, bag_size: 8549\n",
      "batch 579, loss: 0.0640, instance_loss: 0.0005, weighted_loss: 0.0449, label: 0, bag_size: 22870\n",
      "batch 599, loss: 1.1170, instance_loss: 2.5570, weighted_loss: 1.5490, label: 1, bag_size: 15185\n",
      "batch 619, loss: 0.0343, instance_loss: 0.0171, weighted_loss: 0.0291, label: 0, bag_size: 31780\n",
      "batch 639, loss: 0.8708, instance_loss: 1.4720, weighted_loss: 1.0512, label: 1, bag_size: 1236\n",
      "batch 659, loss: 0.3337, instance_loss: 0.2166, weighted_loss: 0.2985, label: 1, bag_size: 5110\n",
      "batch 679, loss: 0.0338, instance_loss: 0.0328, weighted_loss: 0.0335, label: 1, bag_size: 1101\n",
      "batch 699, loss: 0.0876, instance_loss: 0.8757, weighted_loss: 0.3241, label: 0, bag_size: 1052\n",
      "batch 719, loss: 0.0483, instance_loss: 0.3926, weighted_loss: 0.1516, label: 1, bag_size: 16379\n",
      "batch 739, loss: 0.1385, instance_loss: 0.0258, weighted_loss: 0.1047, label: 0, bag_size: 8981\n",
      "batch 759, loss: 0.3347, instance_loss: 0.3529, weighted_loss: 0.3402, label: 0, bag_size: 10063\n",
      "batch 779, loss: 0.6216, instance_loss: 0.4061, weighted_loss: 0.5570, label: 1, bag_size: 15185\n",
      "batch 799, loss: 0.0409, instance_loss: 0.0555, weighted_loss: 0.0453, label: 1, bag_size: 12795\n",
      "batch 819, loss: 0.1903, instance_loss: 0.2988, weighted_loss: 0.2229, label: 0, bag_size: 1614\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9543035279805353: correct 12551/13152\n",
      "class 1 clustering acc 0.7545620437956204: correct 4962/6576\n",
      "Epoch: 11, train_loss: 0.3509, train_clustering_loss:  0.4283, train_error: 0.1521\n",
      "class 0: acc 0.848780487804878, correct 348/410\n",
      "class 1: acc 0.8470873786407767, correct 349/412\n",
      "\n",
      "Val Set, val_loss: 0.3109, val_error: 0.1196, auc: 0.9644\n",
      "class 0 clustering acc 0.8743206521739131: correct 1287/1472\n",
      "class 1 clustering acc 0.6168478260869565: correct 454/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.8333333333333334, correct 45/54\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 2.1896, instance_loss: 0.3182, weighted_loss: 1.6282, label: 1, bag_size: 1755\n",
      "batch 39, loss: 0.1001, instance_loss: 0.1005, weighted_loss: 0.1002, label: 0, bag_size: 9542\n",
      "batch 59, loss: 0.1606, instance_loss: 0.0156, weighted_loss: 0.1171, label: 0, bag_size: 9234\n",
      "batch 79, loss: 0.1322, instance_loss: 0.1411, weighted_loss: 0.1349, label: 0, bag_size: 1639\n",
      "batch 99, loss: 0.1978, instance_loss: 0.1103, weighted_loss: 0.1715, label: 1, bag_size: 2559\n",
      "batch 119, loss: 0.2347, instance_loss: 0.4981, weighted_loss: 0.3137, label: 0, bag_size: 3101\n",
      "batch 139, loss: 2.1113, instance_loss: 2.8435, weighted_loss: 2.3310, label: 1, bag_size: 20870\n",
      "batch 159, loss: 0.0905, instance_loss: 0.1071, weighted_loss: 0.0955, label: 1, bag_size: 16051\n",
      "batch 179, loss: 0.0107, instance_loss: 0.0107, weighted_loss: 0.0107, label: 0, bag_size: 10535\n",
      "batch 199, loss: 0.0679, instance_loss: 0.0293, weighted_loss: 0.0563, label: 1, bag_size: 6752\n",
      "batch 219, loss: 0.0553, instance_loss: 0.1574, weighted_loss: 0.0859, label: 0, bag_size: 11778\n",
      "batch 239, loss: 0.2869, instance_loss: 0.4219, weighted_loss: 0.3274, label: 1, bag_size: 1572\n",
      "batch 259, loss: 0.0433, instance_loss: 0.3822, weighted_loss: 0.1450, label: 0, bag_size: 22828\n",
      "batch 279, loss: 0.0033, instance_loss: 0.2676, weighted_loss: 0.0826, label: 0, bag_size: 1984\n",
      "batch 299, loss: 0.0147, instance_loss: 0.0004, weighted_loss: 0.0104, label: 0, bag_size: 21082\n",
      "batch 319, loss: 0.1230, instance_loss: 0.0035, weighted_loss: 0.0871, label: 0, bag_size: 2195\n",
      "batch 339, loss: 0.0182, instance_loss: 0.0338, weighted_loss: 0.0229, label: 1, bag_size: 5049\n",
      "batch 359, loss: 0.1264, instance_loss: 0.0025, weighted_loss: 0.0892, label: 0, bag_size: 11727\n",
      "batch 379, loss: 2.1615, instance_loss: 6.1849, weighted_loss: 3.3685, label: 1, bag_size: 25831\n",
      "batch 399, loss: 2.0274, instance_loss: 0.8732, weighted_loss: 1.6812, label: 1, bag_size: 7981\n",
      "batch 419, loss: 0.0391, instance_loss: 0.0668, weighted_loss: 0.0474, label: 1, bag_size: 6599\n",
      "batch 439, loss: 0.1702, instance_loss: 0.0747, weighted_loss: 0.1416, label: 1, bag_size: 9230\n",
      "batch 459, loss: 0.2285, instance_loss: 0.0761, weighted_loss: 0.1828, label: 1, bag_size: 14230\n",
      "batch 479, loss: 0.0059, instance_loss: 0.0400, weighted_loss: 0.0161, label: 1, bag_size: 11195\n",
      "batch 499, loss: 0.0413, instance_loss: 0.0040, weighted_loss: 0.0301, label: 0, bag_size: 11917\n",
      "batch 519, loss: 0.2267, instance_loss: 1.3753, weighted_loss: 0.5713, label: 0, bag_size: 2091\n",
      "batch 539, loss: 0.0458, instance_loss: 0.0035, weighted_loss: 0.0331, label: 0, bag_size: 31780\n",
      "batch 559, loss: 2.0093, instance_loss: 0.1103, weighted_loss: 1.4396, label: 1, bag_size: 12180\n",
      "batch 579, loss: 0.1292, instance_loss: 0.0074, weighted_loss: 0.0927, label: 0, bag_size: 23791\n",
      "batch 599, loss: 0.2839, instance_loss: 0.0190, weighted_loss: 0.2045, label: 1, bag_size: 2092\n",
      "batch 619, loss: 0.0235, instance_loss: 0.0021, weighted_loss: 0.0171, label: 0, bag_size: 16936\n",
      "batch 639, loss: 0.0633, instance_loss: 0.6286, weighted_loss: 0.2329, label: 1, bag_size: 10028\n",
      "batch 659, loss: 0.4278, instance_loss: 0.1085, weighted_loss: 0.3320, label: 1, bag_size: 9983\n",
      "batch 679, loss: 0.0497, instance_loss: 0.0184, weighted_loss: 0.0403, label: 1, bag_size: 21701\n",
      "batch 699, loss: 0.0765, instance_loss: 0.4604, weighted_loss: 0.1917, label: 1, bag_size: 1051\n",
      "batch 719, loss: 0.0615, instance_loss: 0.3246, weighted_loss: 0.1404, label: 1, bag_size: 10112\n",
      "batch 739, loss: 0.0084, instance_loss: 0.0088, weighted_loss: 0.0086, label: 0, bag_size: 13795\n",
      "batch 759, loss: 0.1549, instance_loss: 0.0312, weighted_loss: 0.1178, label: 1, bag_size: 18468\n",
      "batch 779, loss: 0.0824, instance_loss: 0.0197, weighted_loss: 0.0636, label: 1, bag_size: 11363\n",
      "batch 799, loss: 0.2562, instance_loss: 0.0489, weighted_loss: 0.1940, label: 0, bag_size: 15057\n",
      "batch 819, loss: 0.1086, instance_loss: 0.3246, weighted_loss: 0.1734, label: 0, bag_size: 19472\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9610705596107056: correct 12640/13152\n",
      "class 1 clustering acc 0.7802615571776156: correct 5131/6576\n",
      "Epoch: 12, train_loss: 0.3848, train_clustering_loss:  0.3830, train_error: 0.1545\n",
      "class 0: acc 0.8455696202531645, correct 334/395\n",
      "class 1: acc 0.8454332552693209, correct 361/427\n",
      "\n",
      "Val Set, val_loss: 0.2522, val_error: 0.1087, auc: 0.9659\n",
      "class 0 clustering acc 0.9836956521739131: correct 1448/1472\n",
      "class 1 clustering acc 0.7119565217391305: correct 524/736\n",
      "class 0: acc 0.8947368421052632, correct 34/38\n",
      "class 1: acc 0.8888888888888888, correct 48/54\n",
      "Validation loss decreased (0.285389 --> 0.252241).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0230, instance_loss: 0.0287, weighted_loss: 0.0247, label: 0, bag_size: 9949\n",
      "batch 39, loss: 0.6156, instance_loss: 0.3479, weighted_loss: 0.5353, label: 1, bag_size: 1230\n",
      "batch 59, loss: 0.0325, instance_loss: 0.2009, weighted_loss: 0.0831, label: 1, bag_size: 617\n",
      "batch 79, loss: 0.4341, instance_loss: 0.0299, weighted_loss: 0.3128, label: 0, bag_size: 7612\n",
      "batch 99, loss: 0.4414, instance_loss: 0.0564, weighted_loss: 0.3259, label: 1, bag_size: 10966\n",
      "batch 119, loss: 0.0223, instance_loss: 0.1056, weighted_loss: 0.0473, label: 1, bag_size: 6776\n",
      "batch 139, loss: 0.1423, instance_loss: 0.0112, weighted_loss: 0.1030, label: 0, bag_size: 19470\n",
      "batch 159, loss: 0.0624, instance_loss: 0.0004, weighted_loss: 0.0438, label: 0, bag_size: 23714\n",
      "batch 179, loss: 0.3789, instance_loss: 0.2890, weighted_loss: 0.3519, label: 1, bag_size: 7768\n",
      "batch 199, loss: 0.2053, instance_loss: 0.0017, weighted_loss: 0.1442, label: 1, bag_size: 9533\n",
      "batch 219, loss: 0.0082, instance_loss: 0.0000, weighted_loss: 0.0058, label: 0, bag_size: 16936\n",
      "batch 239, loss: 0.1596, instance_loss: 0.0009, weighted_loss: 0.1120, label: 1, bag_size: 8602\n",
      "batch 259, loss: 0.0837, instance_loss: 0.6147, weighted_loss: 0.2430, label: 1, bag_size: 22264\n",
      "batch 279, loss: 0.1444, instance_loss: 0.0219, weighted_loss: 0.1077, label: 1, bag_size: 3980\n",
      "batch 299, loss: 0.0386, instance_loss: 0.0752, weighted_loss: 0.0496, label: 1, bag_size: 7650\n",
      "batch 319, loss: 0.1819, instance_loss: 0.0990, weighted_loss: 0.1570, label: 1, bag_size: 9062\n",
      "batch 339, loss: 0.1319, instance_loss: 0.0032, weighted_loss: 0.0933, label: 1, bag_size: 5318\n",
      "batch 359, loss: 0.0168, instance_loss: 0.0015, weighted_loss: 0.0122, label: 1, bag_size: 9756\n",
      "batch 379, loss: 0.2691, instance_loss: 0.0526, weighted_loss: 0.2041, label: 0, bag_size: 7557\n",
      "batch 399, loss: 0.0369, instance_loss: 0.0015, weighted_loss: 0.0263, label: 1, bag_size: 4423\n",
      "batch 419, loss: 0.0483, instance_loss: 0.0013, weighted_loss: 0.0342, label: 0, bag_size: 8898\n",
      "batch 439, loss: 0.0638, instance_loss: 0.0070, weighted_loss: 0.0467, label: 1, bag_size: 15689\n",
      "batch 459, loss: 0.4617, instance_loss: 0.2418, weighted_loss: 0.3957, label: 0, bag_size: 4523\n",
      "batch 479, loss: 0.0244, instance_loss: 0.0000, weighted_loss: 0.0171, label: 0, bag_size: 31780\n",
      "batch 499, loss: 0.1353, instance_loss: 0.2107, weighted_loss: 0.1579, label: 0, bag_size: 2296\n",
      "batch 519, loss: 1.4517, instance_loss: 3.1848, weighted_loss: 1.9716, label: 1, bag_size: 11386\n",
      "batch 539, loss: 0.3924, instance_loss: 0.0232, weighted_loss: 0.2816, label: 1, bag_size: 9561\n",
      "batch 559, loss: 0.1424, instance_loss: 0.0030, weighted_loss: 0.1006, label: 1, bag_size: 12626\n",
      "batch 579, loss: 0.3738, instance_loss: 0.5638, weighted_loss: 0.4308, label: 1, bag_size: 771\n",
      "batch 599, loss: 0.2200, instance_loss: 0.5581, weighted_loss: 0.3214, label: 1, bag_size: 3856\n",
      "batch 619, loss: 0.0593, instance_loss: 0.0858, weighted_loss: 0.0673, label: 0, bag_size: 15841\n",
      "batch 639, loss: 0.5317, instance_loss: 0.1205, weighted_loss: 0.4084, label: 0, bag_size: 2654\n",
      "batch 659, loss: 0.0693, instance_loss: 0.0014, weighted_loss: 0.0489, label: 0, bag_size: 21319\n",
      "batch 679, loss: 0.0147, instance_loss: 0.0002, weighted_loss: 0.0104, label: 1, bag_size: 6090\n",
      "batch 699, loss: 0.1044, instance_loss: 0.4045, weighted_loss: 0.1944, label: 1, bag_size: 8191\n",
      "batch 719, loss: 0.7072, instance_loss: 0.0393, weighted_loss: 0.5068, label: 0, bag_size: 3725\n",
      "batch 739, loss: 0.3762, instance_loss: 0.0047, weighted_loss: 0.2647, label: 0, bag_size: 5999\n",
      "batch 759, loss: 0.0318, instance_loss: 0.0249, weighted_loss: 0.0298, label: 1, bag_size: 16512\n",
      "batch 779, loss: 0.3392, instance_loss: 0.0268, weighted_loss: 0.2455, label: 0, bag_size: 1684\n",
      "batch 799, loss: 0.5381, instance_loss: 0.2449, weighted_loss: 0.4501, label: 0, bag_size: 4959\n",
      "batch 819, loss: 0.0747, instance_loss: 0.0498, weighted_loss: 0.0672, label: 0, bag_size: 11194\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9739203163017032: correct 12809/13152\n",
      "class 1 clustering acc 0.8400243309002433: correct 5524/6576\n",
      "Epoch: 13, train_loss: 0.3108, train_clustering_loss:  0.2752, train_error: 0.1302\n",
      "class 0: acc 0.875, correct 364/416\n",
      "class 1: acc 0.8645320197044335, correct 351/406\n",
      "\n",
      "Val Set, val_loss: 0.2725, val_error: 0.1196, auc: 0.9688\n",
      "class 0 clustering acc 0.9877717391304348: correct 1454/1472\n",
      "class 1 clustering acc 0.8777173913043478: correct 646/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.8333333333333334, correct 45/54\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0667, instance_loss: 0.0059, weighted_loss: 0.0485, label: 1, bag_size: 5318\n",
      "batch 39, loss: 0.0584, instance_loss: 0.1706, weighted_loss: 0.0921, label: 0, bag_size: 16521\n",
      "batch 59, loss: 0.1385, instance_loss: 0.1964, weighted_loss: 0.1559, label: 1, bag_size: 10501\n",
      "batch 79, loss: 0.0604, instance_loss: 0.3422, weighted_loss: 0.1449, label: 0, bag_size: 10751\n",
      "batch 99, loss: 0.1366, instance_loss: 0.0000, weighted_loss: 0.0956, label: 1, bag_size: 21827\n",
      "batch 119, loss: 0.4220, instance_loss: 0.0060, weighted_loss: 0.2972, label: 0, bag_size: 1962\n",
      "batch 139, loss: 0.0515, instance_loss: 0.0341, weighted_loss: 0.0463, label: 1, bag_size: 2381\n",
      "batch 159, loss: 0.0677, instance_loss: 0.0062, weighted_loss: 0.0492, label: 1, bag_size: 6205\n",
      "batch 179, loss: 0.0153, instance_loss: 0.1370, weighted_loss: 0.0518, label: 1, bag_size: 629\n",
      "batch 199, loss: 0.0316, instance_loss: 0.0049, weighted_loss: 0.0236, label: 1, bag_size: 12697\n",
      "batch 219, loss: 0.0154, instance_loss: 0.0002, weighted_loss: 0.0108, label: 1, bag_size: 5991\n",
      "batch 239, loss: 0.1323, instance_loss: 0.1584, weighted_loss: 0.1401, label: 1, bag_size: 4821\n",
      "batch 259, loss: 0.0374, instance_loss: 0.0761, weighted_loss: 0.0490, label: 0, bag_size: 11727\n",
      "batch 279, loss: 0.0319, instance_loss: 0.0097, weighted_loss: 0.0252, label: 0, bag_size: 9851\n",
      "batch 299, loss: 1.0553, instance_loss: 1.0569, weighted_loss: 1.0558, label: 1, bag_size: 1230\n",
      "batch 319, loss: 0.0821, instance_loss: 0.0031, weighted_loss: 0.0584, label: 0, bag_size: 9415\n",
      "batch 339, loss: 0.1204, instance_loss: 0.0015, weighted_loss: 0.0847, label: 1, bag_size: 14515\n",
      "batch 359, loss: 0.0627, instance_loss: 0.0001, weighted_loss: 0.0439, label: 1, bag_size: 10867\n",
      "batch 379, loss: 1.0825, instance_loss: 1.2160, weighted_loss: 1.1225, label: 1, bag_size: 13367\n",
      "batch 399, loss: 0.0357, instance_loss: 0.0098, weighted_loss: 0.0279, label: 0, bag_size: 2179\n",
      "batch 419, loss: 0.9985, instance_loss: 1.5236, weighted_loss: 1.1561, label: 1, bag_size: 1831\n",
      "batch 439, loss: 0.2706, instance_loss: 0.0122, weighted_loss: 0.1931, label: 1, bag_size: 6016\n",
      "batch 459, loss: 0.1526, instance_loss: 0.0170, weighted_loss: 0.1120, label: 1, bag_size: 1764\n",
      "batch 479, loss: 1.0058, instance_loss: 0.3385, weighted_loss: 0.8056, label: 0, bag_size: 21361\n",
      "batch 499, loss: 0.0497, instance_loss: 0.0272, weighted_loss: 0.0429, label: 0, bag_size: 7605\n",
      "batch 519, loss: 0.0511, instance_loss: 0.0000, weighted_loss: 0.0358, label: 1, bag_size: 28527\n",
      "batch 539, loss: 0.0478, instance_loss: 0.0364, weighted_loss: 0.0444, label: 0, bag_size: 12083\n",
      "batch 559, loss: 0.1703, instance_loss: 0.8031, weighted_loss: 0.3602, label: 0, bag_size: 5009\n",
      "batch 579, loss: 0.0808, instance_loss: 0.0005, weighted_loss: 0.0567, label: 1, bag_size: 7110\n",
      "batch 599, loss: 0.0622, instance_loss: 0.0509, weighted_loss: 0.0588, label: 1, bag_size: 1172\n",
      "batch 619, loss: 0.0074, instance_loss: 0.0002, weighted_loss: 0.0053, label: 0, bag_size: 2179\n",
      "batch 639, loss: 0.2202, instance_loss: 0.2353, weighted_loss: 0.2247, label: 0, bag_size: 29270\n",
      "batch 659, loss: 0.4765, instance_loss: 0.1050, weighted_loss: 0.3651, label: 0, bag_size: 18516\n",
      "batch 679, loss: 0.2588, instance_loss: 0.0336, weighted_loss: 0.1913, label: 0, bag_size: 1684\n",
      "batch 699, loss: 0.0112, instance_loss: 0.0000, weighted_loss: 0.0079, label: 0, bag_size: 22870\n",
      "batch 719, loss: 0.0326, instance_loss: 0.2419, weighted_loss: 0.0954, label: 0, bag_size: 9616\n",
      "batch 739, loss: 0.0318, instance_loss: 0.0005, weighted_loss: 0.0224, label: 1, bag_size: 10920\n",
      "batch 759, loss: 0.2969, instance_loss: 0.2616, weighted_loss: 0.2863, label: 0, bag_size: 1349\n",
      "batch 779, loss: 0.1609, instance_loss: 0.0021, weighted_loss: 0.1133, label: 1, bag_size: 8680\n",
      "batch 799, loss: 0.1482, instance_loss: 0.0000, weighted_loss: 0.1037, label: 1, bag_size: 2662\n",
      "batch 819, loss: 0.0212, instance_loss: 0.2634, weighted_loss: 0.0939, label: 1, bag_size: 2522\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9695103406326034: correct 12751/13152\n",
      "class 1 clustering acc 0.836830900243309: correct 5503/6576\n",
      "Epoch: 14, train_loss: 0.3262, train_clustering_loss:  0.3031, train_error: 0.1265\n",
      "class 0: acc 0.878345498783455, correct 361/411\n",
      "class 1: acc 0.8686131386861314, correct 357/411\n",
      "\n",
      "Val Set, val_loss: 0.2319, val_error: 0.0870, auc: 0.9712\n",
      "class 0 clustering acc 0.9966032608695652: correct 1467/1472\n",
      "class 1 clustering acc 0.8491847826086957: correct 625/736\n",
      "class 0: acc 0.8947368421052632, correct 34/38\n",
      "class 1: acc 0.9259259259259259, correct 50/54\n",
      "Validation loss decreased (0.252241 --> 0.231932).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0072, instance_loss: 0.0015, weighted_loss: 0.0055, label: 1, bag_size: 9322\n",
      "batch 39, loss: 0.0385, instance_loss: 0.0001, weighted_loss: 0.0270, label: 1, bag_size: 11387\n",
      "batch 59, loss: 0.0531, instance_loss: 0.1434, weighted_loss: 0.0802, label: 1, bag_size: 2485\n",
      "batch 79, loss: 0.0304, instance_loss: 2.1020, weighted_loss: 0.6518, label: 0, bag_size: 18240\n",
      "batch 99, loss: 0.0187, instance_loss: 0.1717, weighted_loss: 0.0646, label: 0, bag_size: 21082\n",
      "batch 119, loss: 0.0114, instance_loss: 0.0181, weighted_loss: 0.0134, label: 0, bag_size: 11900\n",
      "batch 139, loss: 0.7343, instance_loss: 3.5395, weighted_loss: 1.5758, label: 1, bag_size: 9913\n",
      "batch 159, loss: 0.0221, instance_loss: 0.0117, weighted_loss: 0.0190, label: 1, bag_size: 4128\n",
      "batch 179, loss: 0.0694, instance_loss: 0.0548, weighted_loss: 0.0650, label: 0, bag_size: 3198\n",
      "batch 199, loss: 0.2854, instance_loss: 0.2616, weighted_loss: 0.2782, label: 1, bag_size: 14604\n",
      "batch 219, loss: 0.0049, instance_loss: 0.0073, weighted_loss: 0.0056, label: 1, bag_size: 3634\n",
      "batch 239, loss: 0.0803, instance_loss: 0.0005, weighted_loss: 0.0564, label: 0, bag_size: 17919\n",
      "batch 259, loss: 0.0030, instance_loss: 0.0000, weighted_loss: 0.0021, label: 0, bag_size: 1984\n",
      "batch 279, loss: 0.0427, instance_loss: 0.0373, weighted_loss: 0.0410, label: 1, bag_size: 6966\n",
      "batch 299, loss: 0.5700, instance_loss: 0.0661, weighted_loss: 0.4188, label: 1, bag_size: 4786\n",
      "batch 319, loss: 0.0361, instance_loss: 0.0000, weighted_loss: 0.0253, label: 1, bag_size: 19932\n",
      "batch 339, loss: 0.0616, instance_loss: 0.0004, weighted_loss: 0.0432, label: 0, bag_size: 21385\n",
      "batch 359, loss: 0.0118, instance_loss: 0.0002, weighted_loss: 0.0083, label: 1, bag_size: 11981\n",
      "batch 379, loss: 0.3560, instance_loss: 0.4663, weighted_loss: 0.3891, label: 0, bag_size: 3654\n",
      "batch 399, loss: 0.0601, instance_loss: 0.0125, weighted_loss: 0.0458, label: 1, bag_size: 13026\n",
      "batch 419, loss: 0.0275, instance_loss: 0.0005, weighted_loss: 0.0194, label: 1, bag_size: 2381\n",
      "batch 439, loss: 0.1353, instance_loss: 0.0016, weighted_loss: 0.0952, label: 1, bag_size: 5023\n",
      "batch 459, loss: 0.1934, instance_loss: 0.0001, weighted_loss: 0.1354, label: 0, bag_size: 2844\n",
      "batch 479, loss: 0.1272, instance_loss: 0.0109, weighted_loss: 0.0924, label: 0, bag_size: 12793\n",
      "batch 499, loss: 0.3485, instance_loss: 0.1712, weighted_loss: 0.2953, label: 0, bag_size: 14893\n",
      "batch 519, loss: 0.0248, instance_loss: 0.0319, weighted_loss: 0.0269, label: 0, bag_size: 9786\n",
      "batch 539, loss: 0.0450, instance_loss: 0.0026, weighted_loss: 0.0323, label: 0, bag_size: 15841\n",
      "batch 559, loss: 0.1031, instance_loss: 0.0030, weighted_loss: 0.0731, label: 1, bag_size: 3576\n",
      "batch 579, loss: 0.0904, instance_loss: 0.0206, weighted_loss: 0.0695, label: 0, bag_size: 7605\n",
      "batch 599, loss: 0.0257, instance_loss: 0.0004, weighted_loss: 0.0181, label: 1, bag_size: 21009\n",
      "batch 619, loss: 0.0218, instance_loss: 0.0050, weighted_loss: 0.0167, label: 1, bag_size: 9548\n",
      "batch 639, loss: 0.0792, instance_loss: 0.0000, weighted_loss: 0.0555, label: 1, bag_size: 6752\n",
      "batch 659, loss: 0.0972, instance_loss: 0.0515, weighted_loss: 0.0835, label: 0, bag_size: 1630\n",
      "batch 679, loss: 0.0174, instance_loss: 0.0053, weighted_loss: 0.0138, label: 1, bag_size: 9747\n",
      "batch 699, loss: 0.0721, instance_loss: 0.1626, weighted_loss: 0.0992, label: 1, bag_size: 9470\n",
      "batch 719, loss: 1.5802, instance_loss: 2.8862, weighted_loss: 1.9720, label: 0, bag_size: 2815\n",
      "batch 739, loss: 0.0141, instance_loss: 0.0101, weighted_loss: 0.0129, label: 0, bag_size: 18240\n",
      "batch 759, loss: 0.1219, instance_loss: 0.0054, weighted_loss: 0.0869, label: 0, bag_size: 2920\n",
      "batch 779, loss: 0.0109, instance_loss: 0.0002, weighted_loss: 0.0077, label: 0, bag_size: 21082\n",
      "batch 799, loss: 0.1757, instance_loss: 0.0032, weighted_loss: 0.1240, label: 0, bag_size: 2036\n",
      "batch 819, loss: 0.0264, instance_loss: 0.0000, weighted_loss: 0.0185, label: 1, bag_size: 4367\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9699665450121655: correct 12757/13152\n",
      "class 1 clustering acc 0.8347019464720195: correct 5489/6576\n",
      "Epoch: 15, train_loss: 0.3359, train_clustering_loss:  0.3019, train_error: 0.1229\n",
      "class 0: acc 0.8997613365155132, correct 377/419\n",
      "class 1: acc 0.8535980148883374, correct 344/403\n",
      "\n",
      "Val Set, val_loss: 0.2227, val_error: 0.0761, auc: 0.9722\n",
      "class 0 clustering acc 0.970108695652174: correct 1428/1472\n",
      "class 1 clustering acc 0.9198369565217391: correct 677/736\n",
      "class 0: acc 0.8947368421052632, correct 34/38\n",
      "class 1: acc 0.9444444444444444, correct 51/54\n",
      "Validation loss decreased (0.231932 --> 0.222658).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0839, instance_loss: 0.0563, weighted_loss: 0.0756, label: 1, bag_size: 3652\n",
      "batch 39, loss: 0.0818, instance_loss: 0.1342, weighted_loss: 0.0975, label: 0, bag_size: 2814\n",
      "batch 59, loss: 0.0570, instance_loss: 0.1125, weighted_loss: 0.0736, label: 0, bag_size: 6624\n",
      "batch 79, loss: 0.0424, instance_loss: 0.0005, weighted_loss: 0.0298, label: 0, bag_size: 11759\n",
      "batch 99, loss: 0.0071, instance_loss: 0.0051, weighted_loss: 0.0065, label: 1, bag_size: 4880\n",
      "batch 119, loss: 0.0190, instance_loss: 0.0484, weighted_loss: 0.0278, label: 0, bag_size: 8661\n",
      "batch 139, loss: 0.1110, instance_loss: 0.0024, weighted_loss: 0.0784, label: 1, bag_size: 11223\n",
      "batch 159, loss: 0.8423, instance_loss: 0.8182, weighted_loss: 0.8351, label: 1, bag_size: 12712\n",
      "batch 179, loss: 1.7836, instance_loss: 1.4121, weighted_loss: 1.6722, label: 0, bag_size: 2996\n",
      "batch 199, loss: 0.3218, instance_loss: 0.0095, weighted_loss: 0.2281, label: 0, bag_size: 7381\n",
      "batch 219, loss: 0.7437, instance_loss: 2.0693, weighted_loss: 1.1414, label: 0, bag_size: 15057\n",
      "batch 239, loss: 1.3618, instance_loss: 0.2981, weighted_loss: 1.0427, label: 0, bag_size: 1701\n",
      "batch 259, loss: 0.0493, instance_loss: 0.0021, weighted_loss: 0.0351, label: 0, bag_size: 16211\n",
      "batch 279, loss: 0.0093, instance_loss: 0.0000, weighted_loss: 0.0065, label: 0, bag_size: 23037\n",
      "batch 299, loss: 0.0055, instance_loss: 0.0000, weighted_loss: 0.0038, label: 0, bag_size: 10535\n",
      "batch 319, loss: 0.0308, instance_loss: 0.0526, weighted_loss: 0.0373, label: 1, bag_size: 1249\n",
      "batch 339, loss: 0.1407, instance_loss: 0.1429, weighted_loss: 0.1413, label: 0, bag_size: 9930\n",
      "batch 359, loss: 0.0410, instance_loss: 0.0100, weighted_loss: 0.0317, label: 0, bag_size: 16211\n",
      "batch 379, loss: 0.1117, instance_loss: 0.1069, weighted_loss: 0.1102, label: 0, bag_size: 14885\n",
      "batch 399, loss: 0.1674, instance_loss: 0.0349, weighted_loss: 0.1277, label: 1, bag_size: 25695\n",
      "batch 419, loss: 0.2383, instance_loss: 0.0272, weighted_loss: 0.1750, label: 1, bag_size: 2662\n",
      "batch 439, loss: 1.1050, instance_loss: 0.5316, weighted_loss: 0.9330, label: 1, bag_size: 12340\n",
      "batch 459, loss: 0.0104, instance_loss: 0.0103, weighted_loss: 0.0104, label: 1, bag_size: 11981\n",
      "batch 479, loss: 0.0394, instance_loss: 0.0199, weighted_loss: 0.0335, label: 0, bag_size: 10415\n",
      "batch 499, loss: 1.6819, instance_loss: 0.1387, weighted_loss: 1.2190, label: 1, bag_size: 1703\n",
      "batch 519, loss: 0.4999, instance_loss: 0.7150, weighted_loss: 0.5644, label: 0, bag_size: 65728\n",
      "batch 539, loss: 0.0142, instance_loss: 0.0000, weighted_loss: 0.0099, label: 0, bag_size: 11654\n",
      "batch 559, loss: 0.1040, instance_loss: 0.1932, weighted_loss: 0.1308, label: 0, bag_size: 18777\n",
      "batch 579, loss: 1.0657, instance_loss: 0.2465, weighted_loss: 0.8200, label: 0, bag_size: 2467\n",
      "batch 599, loss: 0.8652, instance_loss: 1.8962, weighted_loss: 1.1745, label: 1, bag_size: 11386\n",
      "batch 619, loss: 0.0418, instance_loss: 0.0007, weighted_loss: 0.0294, label: 0, bag_size: 4465\n",
      "batch 639, loss: 0.0068, instance_loss: 0.2257, weighted_loss: 0.0724, label: 0, bag_size: 8812\n",
      "batch 659, loss: 0.0316, instance_loss: 0.0783, weighted_loss: 0.0456, label: 0, bag_size: 21082\n",
      "batch 679, loss: 0.0526, instance_loss: 0.0000, weighted_loss: 0.0368, label: 0, bag_size: 7235\n",
      "batch 699, loss: 0.0883, instance_loss: 0.0097, weighted_loss: 0.0647, label: 0, bag_size: 9888\n",
      "batch 719, loss: 0.0860, instance_loss: 0.0194, weighted_loss: 0.0661, label: 0, bag_size: 1884\n",
      "batch 739, loss: 0.0376, instance_loss: 0.0473, weighted_loss: 0.0405, label: 1, bag_size: 11684\n",
      "batch 759, loss: 0.1079, instance_loss: 0.0067, weighted_loss: 0.0776, label: 0, bag_size: 3908\n",
      "batch 779, loss: 0.0498, instance_loss: 0.0043, weighted_loss: 0.0361, label: 1, bag_size: 5340\n",
      "batch 799, loss: 0.0745, instance_loss: 0.0192, weighted_loss: 0.0579, label: 1, bag_size: 5023\n",
      "batch 819, loss: 0.2191, instance_loss: 0.0888, weighted_loss: 0.1800, label: 0, bag_size: 5999\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9717913625304136: correct 12781/13152\n",
      "class 1 clustering acc 0.8316605839416058: correct 5469/6576\n",
      "Epoch: 16, train_loss: 0.3336, train_clustering_loss:  0.2887, train_error: 0.1375\n",
      "class 0: acc 0.8744186046511628, correct 376/430\n",
      "class 1: acc 0.8494897959183674, correct 333/392\n",
      "\n",
      "Val Set, val_loss: 0.2429, val_error: 0.1196, auc: 0.9761\n",
      "class 0 clustering acc 0.9585597826086957: correct 1411/1472\n",
      "class 1 clustering acc 0.7934782608695652: correct 584/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.8333333333333334, correct 45/54\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1244, instance_loss: 0.4016, weighted_loss: 0.2075, label: 0, bag_size: 17482\n",
      "batch 39, loss: 0.0128, instance_loss: 0.0441, weighted_loss: 0.0222, label: 1, bag_size: 5864\n",
      "batch 59, loss: 0.0167, instance_loss: 0.1205, weighted_loss: 0.0479, label: 0, bag_size: 3190\n",
      "batch 79, loss: 0.0599, instance_loss: 0.4725, weighted_loss: 0.1837, label: 0, bag_size: 1120\n",
      "batch 99, loss: 0.0802, instance_loss: 0.0005, weighted_loss: 0.0563, label: 0, bag_size: 22681\n",
      "batch 119, loss: 0.0659, instance_loss: 0.0655, weighted_loss: 0.0657, label: 0, bag_size: 13880\n",
      "batch 139, loss: 0.1527, instance_loss: 0.0607, weighted_loss: 0.1251, label: 0, bag_size: 13602\n",
      "batch 159, loss: 0.5685, instance_loss: 1.0905, weighted_loss: 0.7251, label: 1, bag_size: 9215\n",
      "batch 179, loss: 0.6651, instance_loss: 0.5460, weighted_loss: 0.6294, label: 0, bag_size: 5161\n",
      "batch 199, loss: 0.0538, instance_loss: 0.0128, weighted_loss: 0.0415, label: 1, bag_size: 10912\n",
      "batch 219, loss: 0.0497, instance_loss: 0.2164, weighted_loss: 0.0997, label: 0, bag_size: 8549\n",
      "batch 239, loss: 0.0329, instance_loss: 0.0006, weighted_loss: 0.0232, label: 0, bag_size: 31106\n",
      "batch 259, loss: 0.2727, instance_loss: 0.2996, weighted_loss: 0.2808, label: 0, bag_size: 11194\n",
      "batch 279, loss: 0.1691, instance_loss: 0.1542, weighted_loss: 0.1646, label: 0, bag_size: 13378\n",
      "batch 299, loss: 0.5815, instance_loss: 0.0000, weighted_loss: 0.4071, label: 0, bag_size: 10113\n",
      "batch 319, loss: 1.0647, instance_loss: 0.0017, weighted_loss: 0.7458, label: 1, bag_size: 13477\n",
      "batch 339, loss: 0.2141, instance_loss: 0.7689, weighted_loss: 0.3806, label: 1, bag_size: 16548\n",
      "batch 359, loss: 0.0226, instance_loss: 0.0000, weighted_loss: 0.0158, label: 0, bag_size: 12524\n",
      "batch 379, loss: 0.3236, instance_loss: 0.5053, weighted_loss: 0.3781, label: 0, bag_size: 9421\n",
      "batch 399, loss: 0.2901, instance_loss: 0.8332, weighted_loss: 0.4531, label: 0, bag_size: 10381\n",
      "batch 419, loss: 0.0391, instance_loss: 0.0020, weighted_loss: 0.0280, label: 1, bag_size: 13732\n",
      "batch 439, loss: 0.0675, instance_loss: 0.0108, weighted_loss: 0.0505, label: 1, bag_size: 6533\n",
      "batch 459, loss: 0.0123, instance_loss: 0.0061, weighted_loss: 0.0104, label: 1, bag_size: 10112\n",
      "batch 479, loss: 0.0112, instance_loss: 0.0000, weighted_loss: 0.0078, label: 0, bag_size: 21076\n",
      "batch 499, loss: 0.0608, instance_loss: 0.0001, weighted_loss: 0.0426, label: 0, bag_size: 17482\n",
      "batch 519, loss: 0.0337, instance_loss: 0.0206, weighted_loss: 0.0298, label: 1, bag_size: 5907\n",
      "batch 539, loss: 0.0725, instance_loss: 0.4530, weighted_loss: 0.1866, label: 0, bag_size: 1483\n",
      "batch 559, loss: 0.0153, instance_loss: 0.0020, weighted_loss: 0.0113, label: 0, bag_size: 24911\n",
      "batch 579, loss: 0.0648, instance_loss: 0.0046, weighted_loss: 0.0468, label: 0, bag_size: 2534\n",
      "batch 599, loss: 0.0155, instance_loss: 0.0000, weighted_loss: 0.0108, label: 0, bag_size: 22426\n",
      "batch 619, loss: 0.0389, instance_loss: 0.0119, weighted_loss: 0.0308, label: 0, bag_size: 8582\n",
      "batch 639, loss: 0.0770, instance_loss: 0.0107, weighted_loss: 0.0571, label: 0, bag_size: 18944\n",
      "batch 659, loss: 0.0137, instance_loss: 0.0019, weighted_loss: 0.0102, label: 0, bag_size: 19466\n",
      "batch 679, loss: 0.0898, instance_loss: 0.0164, weighted_loss: 0.0677, label: 1, bag_size: 8935\n",
      "batch 699, loss: 0.0121, instance_loss: 0.0350, weighted_loss: 0.0190, label: 0, bag_size: 13225\n",
      "batch 719, loss: 0.0247, instance_loss: 0.1934, weighted_loss: 0.0753, label: 0, bag_size: 3265\n",
      "batch 739, loss: 0.1510, instance_loss: 0.2441, weighted_loss: 0.1789, label: 0, bag_size: 17482\n",
      "batch 759, loss: 0.0579, instance_loss: 0.2319, weighted_loss: 0.1101, label: 0, bag_size: 8549\n",
      "batch 779, loss: 0.0554, instance_loss: 0.0541, weighted_loss: 0.0550, label: 1, bag_size: 5023\n",
      "batch 799, loss: 0.0351, instance_loss: 0.0090, weighted_loss: 0.0273, label: 1, bag_size: 6731\n",
      "batch 819, loss: 0.6077, instance_loss: 0.9647, weighted_loss: 0.7148, label: 0, bag_size: 8420\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9699665450121655: correct 12757/13152\n",
      "class 1 clustering acc 0.8523418491484185: correct 5605/6576\n",
      "Epoch: 17, train_loss: 0.2723, train_clustering_loss:  0.2751, train_error: 0.1010\n",
      "class 0: acc 0.9124087591240876, correct 375/411\n",
      "class 1: acc 0.8856447688564477, correct 364/411\n",
      "\n",
      "Val Set, val_loss: 0.2425, val_error: 0.1196, auc: 0.9815\n",
      "class 0 clustering acc 0.9775815217391305: correct 1439/1472\n",
      "class 1 clustering acc 0.9211956521739131: correct 678/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.8333333333333334, correct 45/54\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.7673, instance_loss: 2.1285, weighted_loss: 1.8756, label: 1, bag_size: 1703\n",
      "batch 39, loss: 0.0079, instance_loss: 0.0244, weighted_loss: 0.0128, label: 1, bag_size: 9971\n",
      "batch 59, loss: 0.0041, instance_loss: 0.1022, weighted_loss: 0.0335, label: 1, bag_size: 629\n",
      "batch 79, loss: 0.0129, instance_loss: 0.0336, weighted_loss: 0.0191, label: 1, bag_size: 4239\n",
      "batch 99, loss: 0.0610, instance_loss: 0.0180, weighted_loss: 0.0481, label: 1, bag_size: 8012\n",
      "batch 119, loss: 0.2865, instance_loss: 0.1670, weighted_loss: 0.2506, label: 1, bag_size: 1746\n",
      "batch 139, loss: 0.0378, instance_loss: 0.0040, weighted_loss: 0.0277, label: 1, bag_size: 3082\n",
      "batch 159, loss: 0.0262, instance_loss: 0.0100, weighted_loss: 0.0213, label: 1, bag_size: 3409\n",
      "batch 179, loss: 0.1228, instance_loss: 0.1696, weighted_loss: 0.1368, label: 0, bag_size: 3321\n",
      "batch 199, loss: 1.8282, instance_loss: 1.1228, weighted_loss: 1.6166, label: 0, bag_size: 2815\n",
      "batch 219, loss: 0.0133, instance_loss: 0.0065, weighted_loss: 0.0113, label: 1, bag_size: 14223\n",
      "batch 239, loss: 0.0265, instance_loss: 0.0000, weighted_loss: 0.0186, label: 0, bag_size: 9470\n",
      "batch 259, loss: 6.5527, instance_loss: 2.8745, weighted_loss: 5.4492, label: 0, bag_size: 3897\n",
      "batch 279, loss: 0.1658, instance_loss: 0.8303, weighted_loss: 0.3651, label: 0, bag_size: 8744\n",
      "batch 299, loss: 0.6937, instance_loss: 0.1081, weighted_loss: 0.5180, label: 0, bag_size: 5120\n",
      "batch 319, loss: 0.4563, instance_loss: 0.1588, weighted_loss: 0.3671, label: 1, bag_size: 6478\n",
      "batch 339, loss: 0.0139, instance_loss: 0.0074, weighted_loss: 0.0120, label: 1, bag_size: 14202\n",
      "batch 359, loss: 0.6753, instance_loss: 1.0565, weighted_loss: 0.7897, label: 0, bag_size: 1438\n",
      "batch 379, loss: 0.0518, instance_loss: 0.0004, weighted_loss: 0.0364, label: 1, bag_size: 2522\n",
      "batch 399, loss: 0.1214, instance_loss: 0.0026, weighted_loss: 0.0858, label: 0, bag_size: 14739\n",
      "batch 419, loss: 0.0069, instance_loss: 0.0056, weighted_loss: 0.0065, label: 1, bag_size: 11981\n",
      "batch 439, loss: 0.8706, instance_loss: 1.0783, weighted_loss: 0.9329, label: 0, bag_size: 3399\n",
      "batch 459, loss: 0.0666, instance_loss: 0.0000, weighted_loss: 0.0466, label: 0, bag_size: 19470\n",
      "batch 479, loss: 0.0116, instance_loss: 0.0000, weighted_loss: 0.0081, label: 0, bag_size: 11654\n",
      "batch 499, loss: 0.5445, instance_loss: 0.0027, weighted_loss: 0.3820, label: 1, bag_size: 6016\n",
      "batch 519, loss: 0.0483, instance_loss: 0.1730, weighted_loss: 0.0858, label: 0, bag_size: 13205\n",
      "batch 539, loss: 0.0213, instance_loss: 0.0568, weighted_loss: 0.0319, label: 1, bag_size: 4128\n",
      "batch 559, loss: 0.0102, instance_loss: 0.0095, weighted_loss: 0.0100, label: 0, bag_size: 14305\n",
      "batch 579, loss: 0.5235, instance_loss: 1.8416, weighted_loss: 0.9189, label: 0, bag_size: 9616\n",
      "batch 599, loss: 0.0382, instance_loss: 0.0142, weighted_loss: 0.0310, label: 0, bag_size: 13892\n",
      "batch 619, loss: 0.1602, instance_loss: 0.1035, weighted_loss: 0.1432, label: 1, bag_size: 2814\n",
      "batch 639, loss: 6.4586, instance_loss: 1.2934, weighted_loss: 4.9090, label: 1, bag_size: 2565\n",
      "batch 659, loss: 0.1678, instance_loss: 0.0399, weighted_loss: 0.1294, label: 1, bag_size: 10622\n",
      "batch 679, loss: 0.7856, instance_loss: 0.4821, weighted_loss: 0.6946, label: 0, bag_size: 15672\n",
      "batch 699, loss: 0.0407, instance_loss: 0.0216, weighted_loss: 0.0350, label: 1, bag_size: 5256\n",
      "batch 719, loss: 0.1676, instance_loss: 0.0315, weighted_loss: 0.1268, label: 0, bag_size: 11917\n",
      "batch 739, loss: 0.0662, instance_loss: 0.0992, weighted_loss: 0.0761, label: 0, bag_size: 12561\n",
      "batch 759, loss: 0.5028, instance_loss: 0.5288, weighted_loss: 0.5106, label: 1, bag_size: 1969\n",
      "batch 779, loss: 0.0987, instance_loss: 0.1286, weighted_loss: 0.1077, label: 0, bag_size: 1072\n",
      "batch 799, loss: 2.7894, instance_loss: 0.7961, weighted_loss: 2.1914, label: 0, bag_size: 2996\n",
      "batch 819, loss: 0.2729, instance_loss: 0.0852, weighted_loss: 0.2166, label: 0, bag_size: 11212\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9708029197080292: correct 12768/13152\n",
      "class 1 clustering acc 0.8325729927007299: correct 5475/6576\n",
      "Epoch: 18, train_loss: 0.3160, train_clustering_loss:  0.2909, train_error: 0.1156\n",
      "class 0: acc 0.8891752577319587, correct 345/388\n",
      "class 1: acc 0.880184331797235, correct 382/434\n",
      "\n",
      "Val Set, val_loss: 0.2235, val_error: 0.1196, auc: 0.9825\n",
      "class 0 clustering acc 0.9796195652173914: correct 1442/1472\n",
      "class 1 clustering acc 0.8070652173913043: correct 594/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.8333333333333334, correct 45/54\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0744, instance_loss: 0.1819, weighted_loss: 0.1067, label: 0, bag_size: 1814\n",
      "batch 39, loss: 0.0033, instance_loss: 0.0250, weighted_loss: 0.0098, label: 1, bag_size: 1273\n",
      "batch 59, loss: 0.0064, instance_loss: 0.0395, weighted_loss: 0.0163, label: 1, bag_size: 6950\n",
      "batch 79, loss: 0.4435, instance_loss: 0.6906, weighted_loss: 0.5177, label: 0, bag_size: 6367\n",
      "batch 99, loss: 0.1118, instance_loss: 0.0015, weighted_loss: 0.0787, label: 0, bag_size: 2457\n",
      "batch 119, loss: 0.0064, instance_loss: 0.0005, weighted_loss: 0.0046, label: 0, bag_size: 11654\n",
      "batch 139, loss: 0.1370, instance_loss: 0.0109, weighted_loss: 0.0992, label: 1, bag_size: 4976\n",
      "batch 159, loss: 0.0449, instance_loss: 0.0000, weighted_loss: 0.0315, label: 0, bag_size: 14333\n",
      "batch 179, loss: 0.2356, instance_loss: 0.2460, weighted_loss: 0.2387, label: 0, bag_size: 13378\n",
      "batch 199, loss: 0.0278, instance_loss: 0.0217, weighted_loss: 0.0259, label: 1, bag_size: 9533\n",
      "batch 219, loss: 0.0393, instance_loss: 0.1597, weighted_loss: 0.0754, label: 1, bag_size: 2140\n",
      "batch 239, loss: 0.0105, instance_loss: 0.0113, weighted_loss: 0.0107, label: 1, bag_size: 1781\n",
      "batch 259, loss: 0.0080, instance_loss: 0.0024, weighted_loss: 0.0063, label: 0, bag_size: 13225\n",
      "batch 279, loss: 0.0072, instance_loss: 0.0000, weighted_loss: 0.0050, label: 0, bag_size: 20666\n",
      "batch 299, loss: 0.2726, instance_loss: 0.2665, weighted_loss: 0.2708, label: 0, bag_size: 1630\n",
      "batch 319, loss: 0.0714, instance_loss: 0.0869, weighted_loss: 0.0760, label: 1, bag_size: 16565\n",
      "batch 339, loss: 0.1314, instance_loss: 0.0748, weighted_loss: 0.1144, label: 0, bag_size: 11122\n",
      "batch 359, loss: 0.0087, instance_loss: 0.0737, weighted_loss: 0.0282, label: 1, bag_size: 1249\n",
      "batch 379, loss: 0.0421, instance_loss: 0.0432, weighted_loss: 0.0424, label: 1, bag_size: 3082\n",
      "batch 399, loss: 0.0020, instance_loss: 1.9802, weighted_loss: 0.5954, label: 1, bag_size: 3634\n",
      "batch 419, loss: 0.2061, instance_loss: 0.4743, weighted_loss: 0.2866, label: 0, bag_size: 1614\n",
      "batch 439, loss: 0.2560, instance_loss: 0.0401, weighted_loss: 0.1912, label: 0, bag_size: 7989\n",
      "batch 459, loss: 0.0461, instance_loss: 0.0000, weighted_loss: 0.0323, label: 1, bag_size: 5629\n",
      "batch 479, loss: 0.3530, instance_loss: 0.0210, weighted_loss: 0.2534, label: 0, bag_size: 9069\n",
      "batch 499, loss: 0.0141, instance_loss: 0.0001, weighted_loss: 0.0099, label: 0, bag_size: 9234\n",
      "batch 519, loss: 0.0365, instance_loss: 0.0601, weighted_loss: 0.0436, label: 1, bag_size: 7078\n",
      "batch 539, loss: 0.5434, instance_loss: 0.9213, weighted_loss: 0.6568, label: 1, bag_size: 1497\n",
      "batch 559, loss: 0.0259, instance_loss: 0.1021, weighted_loss: 0.0487, label: 1, bag_size: 12575\n",
      "batch 579, loss: 0.0625, instance_loss: 0.0595, weighted_loss: 0.0616, label: 0, bag_size: 3502\n",
      "batch 599, loss: 2.8000, instance_loss: 3.7313, weighted_loss: 3.0794, label: 1, bag_size: 684\n",
      "batch 619, loss: 0.3300, instance_loss: 0.1679, weighted_loss: 0.2814, label: 0, bag_size: 9583\n",
      "batch 639, loss: 0.0213, instance_loss: 0.0117, weighted_loss: 0.0185, label: 0, bag_size: 16211\n",
      "batch 659, loss: 0.0681, instance_loss: 0.0548, weighted_loss: 0.0641, label: 1, bag_size: 20537\n",
      "batch 679, loss: 0.0564, instance_loss: 0.0463, weighted_loss: 0.0534, label: 1, bag_size: 16162\n",
      "batch 699, loss: 0.0115, instance_loss: 0.0002, weighted_loss: 0.0081, label: 1, bag_size: 9322\n",
      "batch 719, loss: 0.3869, instance_loss: 0.7406, weighted_loss: 0.4930, label: 0, bag_size: 2269\n",
      "batch 739, loss: 0.4133, instance_loss: 0.5677, weighted_loss: 0.4596, label: 0, bag_size: 18738\n",
      "batch 759, loss: 0.0947, instance_loss: 0.0003, weighted_loss: 0.0664, label: 1, bag_size: 2559\n",
      "batch 779, loss: 0.0300, instance_loss: 0.0035, weighted_loss: 0.0221, label: 1, bag_size: 4367\n",
      "batch 799, loss: 0.0206, instance_loss: 0.0002, weighted_loss: 0.0145, label: 1, bag_size: 22286\n",
      "batch 819, loss: 0.0281, instance_loss: 0.3088, weighted_loss: 0.1123, label: 0, bag_size: 17633\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9661648418491484: correct 12707/13152\n",
      "class 1 clustering acc 0.82588199513382: correct 5431/6576\n",
      "Epoch: 19, train_loss: 0.3243, train_clustering_loss:  0.3194, train_error: 0.1229\n",
      "class 0: acc 0.8880407124681934, correct 349/393\n",
      "class 1: acc 0.8671328671328671, correct 372/429\n",
      "\n",
      "Val Set, val_loss: 0.1999, val_error: 0.0870, auc: 0.9825\n",
      "class 0 clustering acc 0.8620923913043478: correct 1269/1472\n",
      "class 1 clustering acc 0.5720108695652174: correct 421/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.8888888888888888, correct 48/54\n",
      "Validation loss decreased (0.222658 --> 0.199925).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1925, instance_loss: 1.5264, weighted_loss: 0.5927, label: 0, bag_size: 13619\n",
      "batch 39, loss: 0.0776, instance_loss: 0.1681, weighted_loss: 0.1047, label: 1, bag_size: 15689\n",
      "batch 59, loss: 0.0194, instance_loss: 0.2489, weighted_loss: 0.0882, label: 1, bag_size: 14223\n",
      "batch 79, loss: 0.0716, instance_loss: 0.0109, weighted_loss: 0.0534, label: 1, bag_size: 12895\n",
      "batch 99, loss: 0.0430, instance_loss: 0.0828, weighted_loss: 0.0549, label: 1, bag_size: 15689\n",
      "batch 119, loss: 0.1899, instance_loss: 0.5534, weighted_loss: 0.2989, label: 0, bag_size: 9596\n",
      "batch 139, loss: 0.1804, instance_loss: 0.2286, weighted_loss: 0.1948, label: 0, bag_size: 1953\n",
      "batch 159, loss: 0.1669, instance_loss: 0.0328, weighted_loss: 0.1267, label: 1, bag_size: 1759\n",
      "batch 179, loss: 0.4723, instance_loss: 0.1858, weighted_loss: 0.3863, label: 0, bag_size: 3321\n",
      "batch 199, loss: 0.0539, instance_loss: 0.1064, weighted_loss: 0.0697, label: 0, bag_size: 3474\n",
      "batch 219, loss: 0.5112, instance_loss: 0.3660, weighted_loss: 0.4676, label: 0, bag_size: 13619\n",
      "batch 239, loss: 0.1672, instance_loss: 0.8709, weighted_loss: 0.3783, label: 1, bag_size: 865\n",
      "batch 259, loss: 0.0103, instance_loss: 0.0947, weighted_loss: 0.0356, label: 0, bag_size: 27158\n",
      "batch 279, loss: 0.1698, instance_loss: 0.0046, weighted_loss: 0.1203, label: 1, bag_size: 7381\n",
      "batch 299, loss: 0.0332, instance_loss: 0.0192, weighted_loss: 0.0290, label: 1, bag_size: 11220\n",
      "batch 319, loss: 0.0306, instance_loss: 0.0008, weighted_loss: 0.0217, label: 1, bag_size: 14515\n",
      "batch 339, loss: 0.0048, instance_loss: 0.0034, weighted_loss: 0.0044, label: 1, bag_size: 10501\n",
      "batch 359, loss: 0.0025, instance_loss: 0.0145, weighted_loss: 0.0061, label: 1, bag_size: 11195\n",
      "batch 379, loss: 0.0061, instance_loss: 0.0032, weighted_loss: 0.0052, label: 0, bag_size: 31106\n",
      "batch 399, loss: 0.0991, instance_loss: 0.5662, weighted_loss: 0.2392, label: 0, bag_size: 15003\n",
      "batch 419, loss: 0.0424, instance_loss: 0.0122, weighted_loss: 0.0333, label: 1, bag_size: 16162\n",
      "batch 439, loss: 0.0683, instance_loss: 0.0020, weighted_loss: 0.0484, label: 0, bag_size: 2652\n",
      "batch 459, loss: 0.0142, instance_loss: 0.0001, weighted_loss: 0.0100, label: 0, bag_size: 24911\n",
      "batch 479, loss: 0.3160, instance_loss: 0.0518, weighted_loss: 0.2367, label: 0, bag_size: 1953\n",
      "batch 499, loss: 3.3067, instance_loss: 3.6101, weighted_loss: 3.3977, label: 1, bag_size: 25831\n",
      "batch 519, loss: 0.0248, instance_loss: 0.0054, weighted_loss: 0.0190, label: 0, bag_size: 16607\n",
      "batch 539, loss: 0.0031, instance_loss: 0.0600, weighted_loss: 0.0202, label: 1, bag_size: 1743\n",
      "batch 559, loss: 0.0076, instance_loss: 0.0242, weighted_loss: 0.0126, label: 0, bag_size: 11690\n",
      "batch 579, loss: 6.0122, instance_loss: 0.4075, weighted_loss: 4.3308, label: 0, bag_size: 3897\n",
      "batch 599, loss: 1.2870, instance_loss: 0.0027, weighted_loss: 0.9017, label: 0, bag_size: 5211\n",
      "batch 619, loss: 0.1214, instance_loss: 0.3937, weighted_loss: 0.2031, label: 0, bag_size: 11281\n",
      "batch 639, loss: 0.0755, instance_loss: 0.0073, weighted_loss: 0.0551, label: 0, bag_size: 4902\n",
      "batch 659, loss: 0.0250, instance_loss: 0.0010, weighted_loss: 0.0178, label: 0, bag_size: 13591\n",
      "batch 679, loss: 0.0287, instance_loss: 0.0033, weighted_loss: 0.0211, label: 0, bag_size: 17630\n",
      "batch 699, loss: 0.6998, instance_loss: 0.1027, weighted_loss: 0.5207, label: 0, bag_size: 8427\n",
      "batch 719, loss: 0.0639, instance_loss: 0.0039, weighted_loss: 0.0459, label: 0, bag_size: 19880\n",
      "batch 739, loss: 0.0324, instance_loss: 0.0040, weighted_loss: 0.0239, label: 0, bag_size: 9234\n",
      "batch 759, loss: 0.1170, instance_loss: 0.0122, weighted_loss: 0.0856, label: 1, bag_size: 18649\n",
      "batch 779, loss: 1.0353, instance_loss: 0.3601, weighted_loss: 0.8328, label: 0, bag_size: 2996\n",
      "batch 799, loss: 0.2697, instance_loss: 0.0008, weighted_loss: 0.1890, label: 1, bag_size: 5345\n",
      "batch 819, loss: 0.0390, instance_loss: 0.0124, weighted_loss: 0.0310, label: 0, bag_size: 20230\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9708029197080292: correct 12768/13152\n",
      "class 1 clustering acc 0.8512773722627737: correct 5598/6576\n",
      "Epoch: 20, train_loss: 0.2954, train_clustering_loss:  0.2789, train_error: 0.1241\n",
      "class 0: acc 0.885286783042394, correct 355/401\n",
      "class 1: acc 0.8669833729216152, correct 365/421\n",
      "\n",
      "Val Set, val_loss: 0.1741, val_error: 0.0652, auc: 0.9844\n",
      "class 0 clustering acc 0.9735054347826086: correct 1433/1472\n",
      "class 1 clustering acc 0.9130434782608695: correct 672/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.9259259259259259, correct 50/54\n",
      "Validation loss decreased (0.199925 --> 0.174107).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0258, instance_loss: 0.0001, weighted_loss: 0.0181, label: 0, bag_size: 12524\n",
      "batch 39, loss: 0.1251, instance_loss: 0.0331, weighted_loss: 0.0975, label: 1, bag_size: 1759\n",
      "batch 59, loss: 0.1248, instance_loss: 0.0896, weighted_loss: 0.1143, label: 1, bag_size: 2356\n",
      "batch 79, loss: 0.0498, instance_loss: 0.0000, weighted_loss: 0.0348, label: 1, bag_size: 12425\n",
      "batch 99, loss: 0.0569, instance_loss: 0.0001, weighted_loss: 0.0399, label: 0, bag_size: 9234\n",
      "batch 119, loss: 0.0240, instance_loss: 0.0076, weighted_loss: 0.0191, label: 0, bag_size: 9234\n",
      "batch 139, loss: 0.1368, instance_loss: 0.4918, weighted_loss: 0.2433, label: 0, bag_size: 1052\n",
      "batch 159, loss: 0.0157, instance_loss: 0.1734, weighted_loss: 0.0630, label: 0, bag_size: 3190\n",
      "batch 179, loss: 0.0399, instance_loss: 0.0768, weighted_loss: 0.0510, label: 0, bag_size: 16720\n",
      "batch 199, loss: 0.0283, instance_loss: 0.0000, weighted_loss: 0.0198, label: 1, bag_size: 16051\n",
      "batch 219, loss: 0.0163, instance_loss: 0.0000, weighted_loss: 0.0114, label: 0, bag_size: 12524\n",
      "batch 239, loss: 0.0020, instance_loss: 0.2449, weighted_loss: 0.0749, label: 1, bag_size: 9759\n",
      "batch 259, loss: 0.0054, instance_loss: 0.0000, weighted_loss: 0.0038, label: 1, bag_size: 11642\n",
      "batch 279, loss: 0.1117, instance_loss: 0.4536, weighted_loss: 0.2143, label: 0, bag_size: 7823\n",
      "batch 299, loss: 0.2484, instance_loss: 0.1154, weighted_loss: 0.2085, label: 0, bag_size: 11390\n",
      "batch 319, loss: 0.1631, instance_loss: 0.0671, weighted_loss: 0.1343, label: 0, bag_size: 2382\n",
      "batch 339, loss: 1.4805, instance_loss: 1.8350, weighted_loss: 1.5869, label: 0, bag_size: 7239\n",
      "batch 359, loss: 0.1966, instance_loss: 0.0579, weighted_loss: 0.1550, label: 0, bag_size: 5485\n",
      "batch 379, loss: 0.8855, instance_loss: 0.4900, weighted_loss: 0.7668, label: 0, bag_size: 11306\n",
      "batch 399, loss: 0.1969, instance_loss: 0.0247, weighted_loss: 0.1453, label: 0, bag_size: 1508\n",
      "batch 419, loss: 0.1389, instance_loss: 0.2334, weighted_loss: 0.1673, label: 0, bag_size: 21319\n",
      "batch 439, loss: 0.4670, instance_loss: 0.4775, weighted_loss: 0.4702, label: 0, bag_size: 7989\n",
      "batch 459, loss: 0.1268, instance_loss: 0.9826, weighted_loss: 0.3835, label: 1, bag_size: 689\n",
      "batch 479, loss: 1.3662, instance_loss: 0.8014, weighted_loss: 1.1968, label: 1, bag_size: 1095\n",
      "batch 499, loss: 0.0212, instance_loss: 0.0137, weighted_loss: 0.0190, label: 1, bag_size: 6171\n",
      "batch 519, loss: 0.8731, instance_loss: 2.7410, weighted_loss: 1.4335, label: 1, bag_size: 2937\n",
      "batch 539, loss: 0.1139, instance_loss: 0.0004, weighted_loss: 0.0798, label: 1, bag_size: 29832\n",
      "batch 559, loss: 0.6729, instance_loss: 0.1448, weighted_loss: 0.5144, label: 1, bag_size: 1437\n",
      "batch 579, loss: 0.1178, instance_loss: 0.0005, weighted_loss: 0.0826, label: 1, bag_size: 8040\n",
      "batch 599, loss: 0.1437, instance_loss: 0.0459, weighted_loss: 0.1144, label: 0, bag_size: 13602\n",
      "batch 619, loss: 0.0468, instance_loss: 0.1479, weighted_loss: 0.0771, label: 0, bag_size: 10365\n",
      "batch 639, loss: 0.2016, instance_loss: 0.0166, weighted_loss: 0.1461, label: 0, bag_size: 25027\n",
      "batch 659, loss: 0.0220, instance_loss: 0.0296, weighted_loss: 0.0243, label: 1, bag_size: 4330\n",
      "batch 679, loss: 2.9201, instance_loss: 1.8589, weighted_loss: 2.6017, label: 1, bag_size: 20870\n",
      "batch 699, loss: 0.0202, instance_loss: 0.0068, weighted_loss: 0.0162, label: 0, bag_size: 14305\n",
      "batch 719, loss: 0.0283, instance_loss: 0.0000, weighted_loss: 0.0198, label: 1, bag_size: 7583\n",
      "batch 739, loss: 0.0986, instance_loss: 0.0016, weighted_loss: 0.0695, label: 1, bag_size: 5155\n",
      "batch 759, loss: 0.0805, instance_loss: 0.0002, weighted_loss: 0.0564, label: 0, bag_size: 2844\n",
      "batch 779, loss: 0.0044, instance_loss: 0.0003, weighted_loss: 0.0031, label: 0, bag_size: 21576\n",
      "batch 799, loss: 0.4566, instance_loss: 0.1334, weighted_loss: 0.3596, label: 0, bag_size: 9132\n",
      "batch 819, loss: 2.7361, instance_loss: 1.5375, weighted_loss: 2.3765, label: 0, bag_size: 2694\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9717153284671532: correct 12780/13152\n",
      "class 1 clustering acc 0.8470194647201946: correct 5570/6576\n",
      "Epoch: 21, train_loss: 0.2861, train_clustering_loss:  0.2805, train_error: 0.1095\n",
      "class 0: acc 0.8968058968058968, correct 365/407\n",
      "class 1: acc 0.8843373493975903, correct 367/415\n",
      "\n",
      "Val Set, val_loss: 0.3702, val_error: 0.1630, auc: 0.9854\n",
      "class 0 clustering acc 0.9476902173913043: correct 1395/1472\n",
      "class 1 clustering acc 0.813858695652174: correct 599/736\n",
      "class 0: acc 1.0, correct 38/38\n",
      "class 1: acc 0.7222222222222222, correct 39/54\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 3.1688, instance_loss: 0.0047, weighted_loss: 2.2195, label: 1, bag_size: 25831\n",
      "batch 39, loss: 0.1536, instance_loss: 0.0351, weighted_loss: 0.1181, label: 0, bag_size: 1772\n",
      "batch 59, loss: 0.0487, instance_loss: 0.7525, weighted_loss: 0.2598, label: 1, bag_size: 3295\n",
      "batch 79, loss: 0.0157, instance_loss: 0.4977, weighted_loss: 0.1603, label: 1, bag_size: 10920\n",
      "batch 99, loss: 0.0073, instance_loss: 0.0175, weighted_loss: 0.0103, label: 0, bag_size: 21082\n",
      "batch 119, loss: 0.1483, instance_loss: 0.0006, weighted_loss: 0.1040, label: 0, bag_size: 5965\n",
      "batch 139, loss: 0.0863, instance_loss: 0.0000, weighted_loss: 0.0604, label: 1, bag_size: 10432\n",
      "batch 159, loss: 0.6626, instance_loss: 0.0000, weighted_loss: 0.4639, label: 1, bag_size: 15931\n",
      "batch 179, loss: 0.0287, instance_loss: 0.0000, weighted_loss: 0.0201, label: 0, bag_size: 31780\n",
      "batch 199, loss: 0.0680, instance_loss: 0.0608, weighted_loss: 0.0658, label: 1, bag_size: 13786\n",
      "batch 219, loss: 0.0423, instance_loss: 0.0709, weighted_loss: 0.0509, label: 0, bag_size: 4845\n",
      "batch 239, loss: 0.1141, instance_loss: 0.2208, weighted_loss: 0.1462, label: 1, bag_size: 25695\n",
      "batch 259, loss: 0.0107, instance_loss: 0.1752, weighted_loss: 0.0600, label: 0, bag_size: 11146\n",
      "batch 279, loss: 0.6109, instance_loss: 0.3349, weighted_loss: 0.5281, label: 0, bag_size: 1498\n",
      "batch 299, loss: 0.0146, instance_loss: 0.0030, weighted_loss: 0.0111, label: 1, bag_size: 7217\n",
      "batch 319, loss: 0.0610, instance_loss: 0.0472, weighted_loss: 0.0568, label: 0, bag_size: 2732\n",
      "batch 339, loss: 0.0271, instance_loss: 0.0088, weighted_loss: 0.0216, label: 1, bag_size: 7583\n",
      "batch 359, loss: 0.0146, instance_loss: 0.0821, weighted_loss: 0.0348, label: 1, bag_size: 2412\n",
      "batch 379, loss: 0.0242, instance_loss: 0.0032, weighted_loss: 0.0179, label: 1, bag_size: 5690\n",
      "batch 399, loss: 0.0570, instance_loss: 0.0209, weighted_loss: 0.0462, label: 0, bag_size: 11727\n",
      "batch 419, loss: 0.1388, instance_loss: 0.1401, weighted_loss: 0.1392, label: 0, bag_size: 19808\n",
      "batch 439, loss: 0.6330, instance_loss: 0.3906, weighted_loss: 0.5602, label: 0, bag_size: 5009\n",
      "batch 459, loss: 0.0188, instance_loss: 0.0005, weighted_loss: 0.0133, label: 1, bag_size: 10028\n",
      "batch 479, loss: 0.4953, instance_loss: 0.3844, weighted_loss: 0.4620, label: 0, bag_size: 3783\n",
      "batch 499, loss: 0.4616, instance_loss: 1.2666, weighted_loss: 0.7031, label: 0, bag_size: 5639\n",
      "batch 519, loss: 0.0042, instance_loss: 0.2961, weighted_loss: 0.0918, label: 1, bag_size: 7217\n",
      "batch 539, loss: 0.0285, instance_loss: 0.2007, weighted_loss: 0.0802, label: 1, bag_size: 9408\n",
      "batch 559, loss: 0.0130, instance_loss: 0.1463, weighted_loss: 0.0530, label: 1, bag_size: 3004\n",
      "batch 579, loss: 0.0045, instance_loss: 0.0021, weighted_loss: 0.0038, label: 0, bag_size: 11383\n",
      "batch 599, loss: 0.0042, instance_loss: 0.0194, weighted_loss: 0.0088, label: 0, bag_size: 17633\n",
      "batch 619, loss: 0.0876, instance_loss: 0.0217, weighted_loss: 0.0678, label: 0, bag_size: 9888\n",
      "batch 639, loss: 0.4998, instance_loss: 4.0535, weighted_loss: 1.5659, label: 1, bag_size: 2314\n",
      "batch 659, loss: 0.0425, instance_loss: 0.0003, weighted_loss: 0.0298, label: 1, bag_size: 18699\n",
      "batch 679, loss: 0.1566, instance_loss: 0.0250, weighted_loss: 0.1171, label: 0, bag_size: 7637\n",
      "batch 699, loss: 0.0828, instance_loss: 0.1283, weighted_loss: 0.0964, label: 0, bag_size: 10721\n",
      "batch 719, loss: 0.2512, instance_loss: 0.0816, weighted_loss: 0.2003, label: 0, bag_size: 10381\n",
      "batch 739, loss: 0.0042, instance_loss: 0.4055, weighted_loss: 0.1246, label: 1, bag_size: 10112\n",
      "batch 759, loss: 0.0109, instance_loss: 0.0954, weighted_loss: 0.0362, label: 0, bag_size: 21076\n",
      "batch 779, loss: 0.1089, instance_loss: 0.3925, weighted_loss: 0.1940, label: 1, bag_size: 5160\n",
      "batch 799, loss: 0.0338, instance_loss: 0.0649, weighted_loss: 0.0431, label: 1, bag_size: 16379\n",
      "batch 819, loss: 0.1613, instance_loss: 0.0674, weighted_loss: 0.1331, label: 0, bag_size: 6850\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9627433090024331: correct 12662/13152\n",
      "class 1 clustering acc 0.8277068126520681: correct 5443/6576\n",
      "Epoch: 22, train_loss: 0.2596, train_clustering_loss:  0.4076, train_error: 0.1046\n",
      "class 0: acc 0.9081885856079405, correct 366/403\n",
      "class 1: acc 0.883054892601432, correct 370/419\n",
      "\n",
      "Val Set, val_loss: 0.2039, val_error: 0.1087, auc: 0.9868\n",
      "class 0 clustering acc 0.985733695652174: correct 1451/1472\n",
      "class 1 clustering acc 0.8260869565217391: correct 608/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.8518518518518519, correct 46/54\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0045, instance_loss: 0.0039, weighted_loss: 0.0043, label: 0, bag_size: 11735\n",
      "batch 39, loss: 0.1570, instance_loss: 0.1050, weighted_loss: 0.1414, label: 0, bag_size: 1920\n",
      "batch 59, loss: 0.0496, instance_loss: 0.0311, weighted_loss: 0.0441, label: 1, bag_size: 10432\n",
      "batch 79, loss: 1.5900, instance_loss: 1.4466, weighted_loss: 1.5470, label: 1, bag_size: 1845\n",
      "batch 99, loss: 0.7872, instance_loss: 1.5457, weighted_loss: 1.0148, label: 0, bag_size: 2815\n",
      "batch 119, loss: 0.1216, instance_loss: 0.0755, weighted_loss: 0.1078, label: 0, bag_size: 1213\n",
      "batch 139, loss: 0.2832, instance_loss: 0.1349, weighted_loss: 0.2387, label: 1, bag_size: 1483\n",
      "batch 159, loss: 0.0285, instance_loss: 0.0108, weighted_loss: 0.0232, label: 1, bag_size: 2904\n",
      "batch 179, loss: 0.2989, instance_loss: 0.5450, weighted_loss: 0.3727, label: 0, bag_size: 23618\n",
      "batch 199, loss: 0.1347, instance_loss: 0.5537, weighted_loss: 0.2604, label: 0, bag_size: 2367\n",
      "batch 219, loss: 0.4553, instance_loss: 0.2339, weighted_loss: 0.3889, label: 1, bag_size: 3937\n",
      "batch 239, loss: 0.4088, instance_loss: 0.2275, weighted_loss: 0.3544, label: 0, bag_size: 6884\n",
      "batch 259, loss: 0.3355, instance_loss: 0.0556, weighted_loss: 0.2516, label: 0, bag_size: 9069\n",
      "batch 279, loss: 0.0300, instance_loss: 0.0160, weighted_loss: 0.0258, label: 0, bag_size: 8788\n",
      "batch 299, loss: 0.0567, instance_loss: 0.2342, weighted_loss: 0.1100, label: 0, bag_size: 3474\n",
      "batch 319, loss: 0.0372, instance_loss: 0.1076, weighted_loss: 0.0583, label: 1, bag_size: 2678\n",
      "batch 339, loss: 0.0587, instance_loss: 0.1278, weighted_loss: 0.0794, label: 1, bag_size: 19972\n",
      "batch 359, loss: 0.0574, instance_loss: 0.0952, weighted_loss: 0.0688, label: 0, bag_size: 2382\n",
      "batch 379, loss: 0.0042, instance_loss: 0.0000, weighted_loss: 0.0029, label: 1, bag_size: 10392\n",
      "batch 399, loss: 0.0194, instance_loss: 0.0012, weighted_loss: 0.0139, label: 1, bag_size: 4367\n",
      "batch 419, loss: 0.2990, instance_loss: 0.2879, weighted_loss: 0.2957, label: 0, bag_size: 7557\n",
      "batch 439, loss: 0.2434, instance_loss: 0.0401, weighted_loss: 0.1824, label: 0, bag_size: 21874\n",
      "batch 459, loss: 2.9881, instance_loss: 0.9384, weighted_loss: 2.3732, label: 1, bag_size: 1533\n",
      "batch 479, loss: 0.0048, instance_loss: 0.0344, weighted_loss: 0.0137, label: 0, bag_size: 11690\n",
      "batch 499, loss: 0.1366, instance_loss: 0.2351, weighted_loss: 0.1661, label: 1, bag_size: 6825\n",
      "batch 519, loss: 0.4047, instance_loss: 0.2726, weighted_loss: 0.3651, label: 1, bag_size: 1572\n",
      "batch 539, loss: 0.0126, instance_loss: 0.3197, weighted_loss: 0.1047, label: 1, bag_size: 2405\n",
      "batch 559, loss: 0.2848, instance_loss: 0.3214, weighted_loss: 0.2958, label: 0, bag_size: 15255\n",
      "batch 579, loss: 0.0010, instance_loss: 0.0084, weighted_loss: 0.0032, label: 1, bag_size: 4715\n",
      "batch 599, loss: 0.0368, instance_loss: 0.0004, weighted_loss: 0.0259, label: 0, bag_size: 18076\n",
      "batch 619, loss: 0.0520, instance_loss: 0.0410, weighted_loss: 0.0487, label: 0, bag_size: 10942\n",
      "batch 639, loss: 2.0120, instance_loss: 0.0668, weighted_loss: 1.4285, label: 1, bag_size: 2842\n",
      "batch 659, loss: 0.7412, instance_loss: 0.3146, weighted_loss: 0.6132, label: 0, bag_size: 3399\n",
      "batch 679, loss: 0.0169, instance_loss: 0.2945, weighted_loss: 0.1001, label: 1, bag_size: 18095\n",
      "batch 699, loss: 1.1821, instance_loss: 2.0233, weighted_loss: 1.4345, label: 0, bag_size: 2815\n",
      "batch 719, loss: 0.0116, instance_loss: 0.6182, weighted_loss: 0.1936, label: 1, bag_size: 2405\n",
      "batch 739, loss: 0.0851, instance_loss: 0.2926, weighted_loss: 0.1473, label: 1, bag_size: 16162\n",
      "batch 759, loss: 0.0300, instance_loss: 0.2512, weighted_loss: 0.0963, label: 1, bag_size: 2344\n",
      "batch 779, loss: 0.2223, instance_loss: 1.0634, weighted_loss: 0.4746, label: 1, bag_size: 6928\n",
      "batch 799, loss: 0.8573, instance_loss: 0.0409, weighted_loss: 0.6124, label: 0, bag_size: 5211\n",
      "batch 819, loss: 0.1128, instance_loss: 0.0812, weighted_loss: 0.1033, label: 1, bag_size: 6599\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9698144768856448: correct 12755/13152\n",
      "class 1 clustering acc 0.7930352798053528: correct 5215/6576\n",
      "Epoch: 23, train_loss: 0.2968, train_clustering_loss:  0.3435, train_error: 0.1156\n",
      "class 0: acc 0.8959810874704491, correct 379/423\n",
      "class 1: acc 0.8721804511278195, correct 348/399\n",
      "\n",
      "Val Set, val_loss: 0.1799, val_error: 0.0870, auc: 0.9878\n",
      "class 0 clustering acc 0.9789402173913043: correct 1441/1472\n",
      "class 1 clustering acc 0.875: correct 644/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.8888888888888888, correct 48/54\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0596, instance_loss: 0.0033, weighted_loss: 0.0428, label: 1, bag_size: 21701\n",
      "batch 39, loss: 0.0386, instance_loss: 0.0000, weighted_loss: 0.0270, label: 0, bag_size: 10365\n",
      "batch 59, loss: 0.0372, instance_loss: 0.0150, weighted_loss: 0.0306, label: 0, bag_size: 1452\n",
      "batch 79, loss: 0.0188, instance_loss: 0.0104, weighted_loss: 0.0163, label: 1, bag_size: 4330\n",
      "batch 99, loss: 0.0246, instance_loss: 0.0029, weighted_loss: 0.0181, label: 0, bag_size: 16211\n",
      "batch 119, loss: 0.8428, instance_loss: 1.8076, weighted_loss: 1.1323, label: 1, bag_size: 12180\n",
      "batch 139, loss: 0.0083, instance_loss: 0.0949, weighted_loss: 0.0343, label: 1, bag_size: 10501\n",
      "batch 159, loss: 0.0638, instance_loss: 0.0714, weighted_loss: 0.0661, label: 1, bag_size: 1244\n",
      "batch 179, loss: 0.0224, instance_loss: 0.2068, weighted_loss: 0.0777, label: 1, bag_size: 9548\n",
      "batch 199, loss: 0.0194, instance_loss: 0.0023, weighted_loss: 0.0143, label: 1, bag_size: 6533\n",
      "batch 219, loss: 0.0172, instance_loss: 0.0000, weighted_loss: 0.0121, label: 0, bag_size: 15747\n",
      "batch 239, loss: 0.4270, instance_loss: 0.0176, weighted_loss: 0.3042, label: 0, bag_size: 12840\n",
      "batch 259, loss: 0.4818, instance_loss: 0.0045, weighted_loss: 0.3386, label: 1, bag_size: 3652\n",
      "batch 279, loss: 0.0506, instance_loss: 0.0532, weighted_loss: 0.0514, label: 1, bag_size: 1316\n",
      "batch 299, loss: 0.0365, instance_loss: 0.0023, weighted_loss: 0.0262, label: 1, bag_size: 4128\n",
      "batch 319, loss: 0.0075, instance_loss: 0.1180, weighted_loss: 0.0407, label: 0, bag_size: 3265\n",
      "batch 339, loss: 0.0023, instance_loss: 0.0037, weighted_loss: 0.0027, label: 1, bag_size: 6164\n",
      "batch 359, loss: 0.0177, instance_loss: 0.0000, weighted_loss: 0.0124, label: 1, bag_size: 14202\n",
      "batch 379, loss: 0.0308, instance_loss: 0.0024, weighted_loss: 0.0223, label: 0, bag_size: 2044\n",
      "batch 399, loss: 0.1022, instance_loss: 0.0000, weighted_loss: 0.0715, label: 1, bag_size: 9561\n",
      "batch 419, loss: 0.0151, instance_loss: 0.0000, weighted_loss: 0.0106, label: 1, bag_size: 10501\n",
      "batch 439, loss: 0.0573, instance_loss: 0.0746, weighted_loss: 0.0625, label: 0, bag_size: 10128\n",
      "batch 459, loss: 0.0112, instance_loss: 0.0000, weighted_loss: 0.0078, label: 1, bag_size: 11387\n",
      "batch 479, loss: 0.0070, instance_loss: 0.0000, weighted_loss: 0.0049, label: 1, bag_size: 7382\n",
      "batch 499, loss: 0.0084, instance_loss: 0.0082, weighted_loss: 0.0083, label: 0, bag_size: 21082\n",
      "batch 519, loss: 0.0022, instance_loss: 0.0005, weighted_loss: 0.0017, label: 0, bag_size: 21576\n",
      "batch 539, loss: 0.1333, instance_loss: 0.5069, weighted_loss: 0.2454, label: 0, bag_size: 12732\n",
      "batch 559, loss: 0.8442, instance_loss: 0.7046, weighted_loss: 0.8024, label: 0, bag_size: 11922\n",
      "batch 579, loss: 0.0653, instance_loss: 0.0162, weighted_loss: 0.0506, label: 0, bag_size: 6850\n",
      "batch 599, loss: 0.0727, instance_loss: 0.0044, weighted_loss: 0.0522, label: 0, bag_size: 3238\n",
      "batch 619, loss: 0.0807, instance_loss: 0.1185, weighted_loss: 0.0921, label: 0, bag_size: 3198\n",
      "batch 639, loss: 0.0078, instance_loss: 0.0017, weighted_loss: 0.0060, label: 0, bag_size: 19466\n",
      "batch 659, loss: 0.4783, instance_loss: 1.1181, weighted_loss: 0.6702, label: 1, bag_size: 7748\n",
      "batch 679, loss: 0.0013, instance_loss: 0.0005, weighted_loss: 0.0011, label: 0, bag_size: 21576\n",
      "batch 699, loss: 0.0153, instance_loss: 0.0155, weighted_loss: 0.0154, label: 0, bag_size: 14305\n",
      "batch 719, loss: 0.0056, instance_loss: 0.0000, weighted_loss: 0.0039, label: 0, bag_size: 18574\n",
      "batch 739, loss: 0.0976, instance_loss: 0.0205, weighted_loss: 0.0745, label: 1, bag_size: 6665\n",
      "batch 759, loss: 0.0220, instance_loss: 0.0353, weighted_loss: 0.0260, label: 1, bag_size: 12697\n",
      "batch 779, loss: 0.0311, instance_loss: 0.2500, weighted_loss: 0.0968, label: 1, bag_size: 15609\n",
      "batch 799, loss: 0.1842, instance_loss: 0.0109, weighted_loss: 0.1322, label: 0, bag_size: 13602\n",
      "batch 819, loss: 0.0181, instance_loss: 0.0001, weighted_loss: 0.0127, label: 0, bag_size: 9786\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9729318734793188: correct 12796/13152\n",
      "class 1 clustering acc 0.8511253041362531: correct 5597/6576\n",
      "Epoch: 24, train_loss: 0.2288, train_clustering_loss:  0.2834, train_error: 0.0876\n",
      "class 0: acc 0.9139240506329114, correct 361/395\n",
      "class 1: acc 0.9110070257611241, correct 389/427\n",
      "\n",
      "Val Set, val_loss: 0.1693, val_error: 0.0870, auc: 0.9868\n",
      "class 0 clustering acc 0.9612771739130435: correct 1415/1472\n",
      "class 1 clustering acc 0.873641304347826: correct 643/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.8888888888888888, correct 48/54\n",
      "Validation loss decreased (0.174107 --> 0.169266).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0107, instance_loss: 0.0023, weighted_loss: 0.0082, label: 0, bag_size: 9252\n",
      "batch 39, loss: 0.0217, instance_loss: 0.0049, weighted_loss: 0.0166, label: 0, bag_size: 10814\n",
      "batch 59, loss: 0.0107, instance_loss: 0.0464, weighted_loss: 0.0214, label: 1, bag_size: 7515\n",
      "batch 79, loss: 0.0266, instance_loss: 0.4850, weighted_loss: 0.1641, label: 1, bag_size: 2278\n",
      "batch 99, loss: 0.0186, instance_loss: 0.2612, weighted_loss: 0.0914, label: 1, bag_size: 2662\n",
      "batch 119, loss: 0.3244, instance_loss: 0.8533, weighted_loss: 0.4831, label: 0, bag_size: 5485\n",
      "batch 139, loss: 0.0008, instance_loss: 0.0278, weighted_loss: 0.0089, label: 1, bag_size: 3453\n",
      "batch 159, loss: 0.4974, instance_loss: 1.8348, weighted_loss: 0.8986, label: 0, bag_size: 5639\n",
      "batch 179, loss: 0.0047, instance_loss: 0.0005, weighted_loss: 0.0035, label: 0, bag_size: 3190\n",
      "batch 199, loss: 0.0221, instance_loss: 0.1321, weighted_loss: 0.0551, label: 0, bag_size: 22800\n",
      "batch 219, loss: 0.0092, instance_loss: 0.4888, weighted_loss: 0.1531, label: 1, bag_size: 617\n",
      "batch 239, loss: 0.0185, instance_loss: 0.0016, weighted_loss: 0.0135, label: 0, bag_size: 16992\n",
      "batch 259, loss: 0.0442, instance_loss: 0.1222, weighted_loss: 0.0676, label: 0, bag_size: 2920\n",
      "batch 279, loss: 0.0221, instance_loss: 0.0000, weighted_loss: 0.0155, label: 0, bag_size: 13225\n",
      "batch 299, loss: 0.1007, instance_loss: 0.1016, weighted_loss: 0.1009, label: 1, bag_size: 16154\n",
      "batch 319, loss: 0.0059, instance_loss: 0.1117, weighted_loss: 0.0377, label: 1, bag_size: 6776\n",
      "batch 339, loss: 0.2027, instance_loss: 0.1409, weighted_loss: 0.1842, label: 1, bag_size: 1064\n",
      "batch 359, loss: 0.0564, instance_loss: 0.0431, weighted_loss: 0.0524, label: 1, bag_size: 8395\n",
      "batch 379, loss: 0.5950, instance_loss: 0.0021, weighted_loss: 0.4171, label: 0, bag_size: 5297\n",
      "batch 399, loss: 0.0160, instance_loss: 0.0098, weighted_loss: 0.0141, label: 1, bag_size: 16162\n",
      "batch 419, loss: 0.1949, instance_loss: 0.1807, weighted_loss: 0.1906, label: 0, bag_size: 11212\n",
      "batch 439, loss: 0.0649, instance_loss: 0.6389, weighted_loss: 0.2371, label: 0, bag_size: 2236\n",
      "batch 459, loss: 0.0589, instance_loss: 0.0002, weighted_loss: 0.0413, label: 1, bag_size: 5454\n",
      "batch 479, loss: 0.0060, instance_loss: 0.3616, weighted_loss: 0.1127, label: 1, bag_size: 1316\n",
      "batch 499, loss: 0.2475, instance_loss: 0.0534, weighted_loss: 0.1892, label: 0, bag_size: 16690\n",
      "batch 519, loss: 0.0214, instance_loss: 0.0008, weighted_loss: 0.0152, label: 1, bag_size: 7371\n",
      "batch 539, loss: 0.0792, instance_loss: 0.0606, weighted_loss: 0.0737, label: 0, bag_size: 6652\n",
      "batch 559, loss: 0.0763, instance_loss: 0.0005, weighted_loss: 0.0535, label: 0, bag_size: 19067\n",
      "batch 579, loss: 0.9718, instance_loss: 1.7094, weighted_loss: 1.1931, label: 0, bag_size: 15057\n",
      "batch 599, loss: 0.0888, instance_loss: 0.3924, weighted_loss: 0.1798, label: 1, bag_size: 1786\n",
      "batch 619, loss: 0.0393, instance_loss: 0.0000, weighted_loss: 0.0275, label: 0, bag_size: 27012\n",
      "batch 639, loss: 0.1503, instance_loss: 0.3123, weighted_loss: 0.1989, label: 1, bag_size: 2559\n",
      "batch 659, loss: 0.0230, instance_loss: 0.0016, weighted_loss: 0.0166, label: 1, bag_size: 22286\n",
      "batch 679, loss: 0.0042, instance_loss: 0.0000, weighted_loss: 0.0029, label: 1, bag_size: 5731\n",
      "batch 699, loss: 0.0505, instance_loss: 0.1285, weighted_loss: 0.0739, label: 0, bag_size: 3557\n",
      "batch 719, loss: 0.3406, instance_loss: 0.5476, weighted_loss: 0.4027, label: 1, bag_size: 1483\n",
      "batch 739, loss: 0.0113, instance_loss: 0.0000, weighted_loss: 0.0079, label: 1, bag_size: 11160\n",
      "batch 759, loss: 0.0248, instance_loss: 0.0000, weighted_loss: 0.0173, label: 0, bag_size: 31780\n",
      "batch 779, loss: 0.0082, instance_loss: 0.0022, weighted_loss: 0.0064, label: 0, bag_size: 15747\n",
      "batch 799, loss: 0.1902, instance_loss: 0.7940, weighted_loss: 0.3713, label: 1, bag_size: 12626\n",
      "batch 819, loss: 0.1531, instance_loss: 0.3358, weighted_loss: 0.2079, label: 0, bag_size: 2548\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9651003649635036: correct 12693/13152\n",
      "class 1 clustering acc 0.8149330900243309: correct 5359/6576\n",
      "Epoch: 25, train_loss: 0.2501, train_clustering_loss:  0.3435, train_error: 0.0925\n",
      "class 0: acc 0.9102244389027432, correct 365/401\n",
      "class 1: acc 0.9049881235154394, correct 381/421\n",
      "\n",
      "Val Set, val_loss: 0.1715, val_error: 0.0870, auc: 0.9873\n",
      "class 0 clustering acc 0.970108695652174: correct 1428/1472\n",
      "class 1 clustering acc 0.720108695652174: correct 530/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.8888888888888888, correct 48/54\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 3.4119, instance_loss: 0.6928, weighted_loss: 2.5962, label: 1, bag_size: 2565\n",
      "batch 39, loss: 0.0234, instance_loss: 0.4998, weighted_loss: 0.1663, label: 1, bag_size: 16565\n",
      "batch 59, loss: 0.0344, instance_loss: 0.3227, weighted_loss: 0.1209, label: 0, bag_size: 9888\n",
      "batch 79, loss: 0.0021, instance_loss: 0.0071, weighted_loss: 0.0036, label: 0, bag_size: 10581\n",
      "batch 99, loss: 0.1881, instance_loss: 0.0542, weighted_loss: 0.1479, label: 1, bag_size: 8395\n",
      "batch 119, loss: 0.1102, instance_loss: 1.3394, weighted_loss: 0.4789, label: 0, bag_size: 5639\n",
      "batch 139, loss: 0.8356, instance_loss: 1.5761, weighted_loss: 1.0578, label: 1, bag_size: 2935\n",
      "batch 159, loss: 0.0025, instance_loss: 0.3824, weighted_loss: 0.1165, label: 1, bag_size: 5864\n",
      "batch 179, loss: 0.0004, instance_loss: 0.0182, weighted_loss: 0.0057, label: 0, bag_size: 1984\n",
      "batch 199, loss: 0.0123, instance_loss: 0.0096, weighted_loss: 0.0115, label: 1, bag_size: 6317\n",
      "batch 219, loss: 0.0859, instance_loss: 0.0018, weighted_loss: 0.0607, label: 0, bag_size: 2511\n",
      "batch 239, loss: 0.0102, instance_loss: 0.0412, weighted_loss: 0.0195, label: 1, bag_size: 19932\n",
      "batch 259, loss: 0.0155, instance_loss: 0.0147, weighted_loss: 0.0153, label: 0, bag_size: 12793\n",
      "batch 279, loss: 0.0863, instance_loss: 0.0504, weighted_loss: 0.0755, label: 1, bag_size: 15689\n",
      "batch 299, loss: 0.0170, instance_loss: 0.1372, weighted_loss: 0.0530, label: 1, bag_size: 3576\n",
      "batch 319, loss: 1.6637, instance_loss: 0.6755, weighted_loss: 1.3672, label: 0, bag_size: 3760\n",
      "batch 339, loss: 0.0399, instance_loss: 0.1773, weighted_loss: 0.0811, label: 0, bag_size: 1370\n",
      "batch 359, loss: 0.0238, instance_loss: 0.0043, weighted_loss: 0.0179, label: 0, bag_size: 14956\n",
      "batch 379, loss: 0.0149, instance_loss: 0.0220, weighted_loss: 0.0170, label: 0, bag_size: 10721\n",
      "batch 399, loss: 0.2043, instance_loss: 0.5418, weighted_loss: 0.3055, label: 1, bag_size: 2140\n",
      "batch 419, loss: 0.1871, instance_loss: 0.2100, weighted_loss: 0.1940, label: 0, bag_size: 4241\n",
      "batch 439, loss: 0.0297, instance_loss: 0.0041, weighted_loss: 0.0220, label: 0, bag_size: 11281\n",
      "batch 459, loss: 0.0313, instance_loss: 0.0632, weighted_loss: 0.0409, label: 1, bag_size: 10912\n",
      "batch 479, loss: 0.1628, instance_loss: 0.0126, weighted_loss: 0.1177, label: 1, bag_size: 6016\n",
      "batch 499, loss: 0.0378, instance_loss: 0.0008, weighted_loss: 0.0267, label: 1, bag_size: 21009\n",
      "batch 519, loss: 0.0475, instance_loss: 0.0073, weighted_loss: 0.0355, label: 1, bag_size: 5318\n",
      "batch 539, loss: 0.0111, instance_loss: 0.0829, weighted_loss: 0.0327, label: 1, bag_size: 12349\n",
      "batch 559, loss: 0.5206, instance_loss: 0.3405, weighted_loss: 0.4666, label: 1, bag_size: 11316\n",
      "batch 579, loss: 0.1138, instance_loss: 0.1364, weighted_loss: 0.1205, label: 0, bag_size: 5485\n",
      "batch 599, loss: 0.0981, instance_loss: 0.0151, weighted_loss: 0.0732, label: 1, bag_size: 8012\n",
      "batch 619, loss: 0.3527, instance_loss: 0.4137, weighted_loss: 0.3710, label: 0, bag_size: 5485\n",
      "batch 639, loss: 0.0021, instance_loss: 0.0178, weighted_loss: 0.0068, label: 0, bag_size: 21218\n",
      "batch 659, loss: 0.0788, instance_loss: 0.0613, weighted_loss: 0.0736, label: 0, bag_size: 7605\n",
      "batch 679, loss: 0.0114, instance_loss: 0.0012, weighted_loss: 0.0083, label: 1, bag_size: 3683\n",
      "batch 699, loss: 0.0512, instance_loss: 0.0046, weighted_loss: 0.0372, label: 0, bag_size: 3502\n",
      "batch 719, loss: 0.1326, instance_loss: 0.0884, weighted_loss: 0.1193, label: 1, bag_size: 9649\n",
      "batch 739, loss: 0.3332, instance_loss: 0.0345, weighted_loss: 0.2436, label: 0, bag_size: 2269\n",
      "batch 759, loss: 0.2114, instance_loss: 0.0582, weighted_loss: 0.1655, label: 1, bag_size: 16154\n",
      "batch 779, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 6769\n",
      "batch 799, loss: 0.0787, instance_loss: 0.1318, weighted_loss: 0.0947, label: 1, bag_size: 2814\n",
      "batch 819, loss: 0.0177, instance_loss: 0.0179, weighted_loss: 0.0178, label: 0, bag_size: 21032\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9712591240875912: correct 12774/13152\n",
      "class 1 clustering acc 0.8410888077858881: correct 5531/6576\n",
      "Epoch: 26, train_loss: 0.2326, train_clustering_loss:  0.2675, train_error: 0.0925\n",
      "class 0: acc 0.91415313225058, correct 394/431\n",
      "class 1: acc 0.9002557544757033, correct 352/391\n",
      "\n",
      "Val Set, val_loss: 0.1424, val_error: 0.0543, auc: 0.9898\n",
      "class 0 clustering acc 0.984375: correct 1449/1472\n",
      "class 1 clustering acc 0.8301630434782609: correct 611/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.9444444444444444, correct 51/54\n",
      "Validation loss decreased (0.169266 --> 0.142411).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0785, instance_loss: 0.0047, weighted_loss: 0.0563, label: 1, bag_size: 11223\n",
      "batch 39, loss: 0.3374, instance_loss: 0.0923, weighted_loss: 0.2638, label: 1, bag_size: 4929\n",
      "batch 59, loss: 0.3269, instance_loss: 0.0142, weighted_loss: 0.2331, label: 1, bag_size: 10492\n",
      "batch 79, loss: 0.2891, instance_loss: 0.2069, weighted_loss: 0.2645, label: 1, bag_size: 1038\n",
      "batch 99, loss: 0.3081, instance_loss: 0.0061, weighted_loss: 0.2175, label: 0, bag_size: 10490\n",
      "batch 119, loss: 0.0262, instance_loss: 0.2331, weighted_loss: 0.0883, label: 0, bag_size: 1651\n",
      "batch 139, loss: 0.0278, instance_loss: 0.0525, weighted_loss: 0.0352, label: 0, bag_size: 1213\n",
      "batch 159, loss: 0.0326, instance_loss: 0.0151, weighted_loss: 0.0273, label: 0, bag_size: 3657\n",
      "batch 179, loss: 0.0043, instance_loss: 0.0240, weighted_loss: 0.0102, label: 0, bag_size: 3190\n",
      "batch 199, loss: 0.0062, instance_loss: 0.1290, weighted_loss: 0.0431, label: 1, bag_size: 4423\n",
      "batch 219, loss: 0.9093, instance_loss: 0.5118, weighted_loss: 0.7901, label: 1, bag_size: 16514\n",
      "batch 239, loss: 0.0213, instance_loss: 0.0730, weighted_loss: 0.0368, label: 1, bag_size: 10396\n",
      "batch 259, loss: 0.3277, instance_loss: 0.1638, weighted_loss: 0.2786, label: 0, bag_size: 5409\n",
      "batch 279, loss: 0.5422, instance_loss: 1.9727, weighted_loss: 0.9713, label: 1, bag_size: 12712\n",
      "batch 299, loss: 0.1297, instance_loss: 0.0349, weighted_loss: 0.1013, label: 1, bag_size: 13026\n",
      "batch 319, loss: 0.0081, instance_loss: 0.1868, weighted_loss: 0.0617, label: 0, bag_size: 15636\n",
      "batch 339, loss: 0.0707, instance_loss: 0.3346, weighted_loss: 0.1499, label: 0, bag_size: 1824\n",
      "batch 359, loss: 1.1703, instance_loss: 0.7571, weighted_loss: 1.0464, label: 0, bag_size: 9616\n",
      "batch 379, loss: 0.0163, instance_loss: 0.0059, weighted_loss: 0.0132, label: 1, bag_size: 9689\n",
      "batch 399, loss: 0.5392, instance_loss: 0.3250, weighted_loss: 0.4749, label: 0, bag_size: 1953\n",
      "batch 419, loss: 0.2443, instance_loss: 0.0003, weighted_loss: 0.1711, label: 0, bag_size: 3238\n",
      "batch 439, loss: 4.0356, instance_loss: 0.3765, weighted_loss: 2.9379, label: 1, bag_size: 2565\n",
      "batch 459, loss: 0.0492, instance_loss: 0.0200, weighted_loss: 0.0404, label: 0, bag_size: 13777\n",
      "batch 479, loss: 0.1248, instance_loss: 0.0221, weighted_loss: 0.0940, label: 0, bag_size: 1072\n",
      "batch 499, loss: 1.5244, instance_loss: 1.0336, weighted_loss: 1.3772, label: 1, bag_size: 1444\n",
      "batch 519, loss: 0.1409, instance_loss: 0.0357, weighted_loss: 0.1094, label: 1, bag_size: 9470\n",
      "batch 539, loss: 0.2725, instance_loss: 0.0570, weighted_loss: 0.2078, label: 1, bag_size: 7613\n",
      "batch 559, loss: 0.0509, instance_loss: 0.0771, weighted_loss: 0.0588, label: 1, bag_size: 13732\n",
      "batch 579, loss: 0.3896, instance_loss: 0.2043, weighted_loss: 0.3340, label: 0, bag_size: 12722\n",
      "batch 599, loss: 0.0237, instance_loss: 0.0580, weighted_loss: 0.0340, label: 1, bag_size: 14604\n",
      "batch 619, loss: 1.1778, instance_loss: 0.0595, weighted_loss: 0.8423, label: 1, bag_size: 15931\n",
      "batch 639, loss: 0.2241, instance_loss: 0.0075, weighted_loss: 0.1591, label: 0, bag_size: 18738\n",
      "batch 659, loss: 1.0727, instance_loss: 0.1885, weighted_loss: 0.8075, label: 0, bag_size: 11306\n",
      "batch 679, loss: 1.6328, instance_loss: 2.3071, weighted_loss: 1.8351, label: 1, bag_size: 9162\n",
      "batch 699, loss: 0.0236, instance_loss: 0.0069, weighted_loss: 0.0186, label: 1, bag_size: 12575\n",
      "batch 719, loss: 0.0163, instance_loss: 0.0052, weighted_loss: 0.0130, label: 1, bag_size: 3576\n",
      "batch 739, loss: 0.0102, instance_loss: 0.0016, weighted_loss: 0.0076, label: 0, bag_size: 8981\n",
      "batch 759, loss: 0.0046, instance_loss: 0.0000, weighted_loss: 0.0032, label: 1, bag_size: 6317\n",
      "batch 779, loss: 0.0055, instance_loss: 0.0000, weighted_loss: 0.0038, label: 1, bag_size: 6317\n",
      "batch 799, loss: 0.0435, instance_loss: 0.2823, weighted_loss: 0.1152, label: 1, bag_size: 2136\n",
      "batch 819, loss: 0.0035, instance_loss: 0.0000, weighted_loss: 0.0025, label: 0, bag_size: 18574\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.973007907542579: correct 12797/13152\n",
      "class 1 clustering acc 0.8553832116788321: correct 5625/6576\n",
      "Epoch: 27, train_loss: 0.2773, train_clustering_loss:  0.2506, train_error: 0.1022\n",
      "class 0: acc 0.8804347826086957, correct 324/368\n",
      "class 1: acc 0.9118942731277533, correct 414/454\n",
      "\n",
      "Val Set, val_loss: 0.1514, val_error: 0.0435, auc: 0.9878\n",
      "class 0 clustering acc 0.9646739130434783: correct 1420/1472\n",
      "class 1 clustering acc 0.904891304347826: correct 666/736\n",
      "class 0: acc 0.8947368421052632, correct 34/38\n",
      "class 1: acc 1.0, correct 54/54\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0047, instance_loss: 0.0109, weighted_loss: 0.0065, label: 0, bag_size: 3190\n",
      "batch 39, loss: 0.0046, instance_loss: 0.0000, weighted_loss: 0.0032, label: 0, bag_size: 12593\n",
      "batch 59, loss: 0.9459, instance_loss: 0.0823, weighted_loss: 0.6868, label: 0, bag_size: 3760\n",
      "batch 79, loss: 0.0109, instance_loss: 0.0636, weighted_loss: 0.0267, label: 0, bag_size: 9930\n",
      "batch 99, loss: 0.0083, instance_loss: 0.0000, weighted_loss: 0.0058, label: 0, bag_size: 15736\n",
      "batch 119, loss: 0.0074, instance_loss: 0.0080, weighted_loss: 0.0076, label: 0, bag_size: 19390\n",
      "batch 139, loss: 0.0850, instance_loss: 0.2626, weighted_loss: 0.1383, label: 0, bag_size: 1370\n",
      "batch 159, loss: 0.0599, instance_loss: 0.1884, weighted_loss: 0.0984, label: 0, bag_size: 2195\n",
      "batch 179, loss: 0.0450, instance_loss: 0.0000, weighted_loss: 0.0315, label: 0, bag_size: 15841\n",
      "batch 199, loss: 0.0106, instance_loss: 0.0000, weighted_loss: 0.0074, label: 0, bag_size: 14319\n",
      "batch 219, loss: 0.0071, instance_loss: 0.0000, weighted_loss: 0.0050, label: 0, bag_size: 26271\n",
      "batch 239, loss: 0.9541, instance_loss: 1.1411, weighted_loss: 1.0102, label: 1, bag_size: 1867\n",
      "batch 259, loss: 1.3458, instance_loss: 0.2623, weighted_loss: 1.0207, label: 0, bag_size: 3760\n",
      "batch 279, loss: 0.0108, instance_loss: 0.0052, weighted_loss: 0.0091, label: 0, bag_size: 3228\n",
      "batch 299, loss: 0.0055, instance_loss: 0.0004, weighted_loss: 0.0040, label: 1, bag_size: 7371\n",
      "batch 319, loss: 0.0044, instance_loss: 0.0000, weighted_loss: 0.0031, label: 0, bag_size: 13225\n",
      "batch 339, loss: 0.1225, instance_loss: 0.0000, weighted_loss: 0.0858, label: 0, bag_size: 8959\n",
      "batch 359, loss: 0.1283, instance_loss: 0.0080, weighted_loss: 0.0922, label: 1, bag_size: 10460\n",
      "batch 379, loss: 0.0146, instance_loss: 0.0005, weighted_loss: 0.0104, label: 0, bag_size: 8981\n",
      "batch 399, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 18076\n",
      "batch 419, loss: 0.5262, instance_loss: 0.0247, weighted_loss: 0.3758, label: 1, bag_size: 10966\n",
      "batch 439, loss: 0.0025, instance_loss: 0.0135, weighted_loss: 0.0058, label: 1, bag_size: 6875\n",
      "batch 459, loss: 0.1707, instance_loss: 0.5814, weighted_loss: 0.2939, label: 1, bag_size: 7148\n",
      "batch 479, loss: 0.2036, instance_loss: 0.0826, weighted_loss: 0.1673, label: 1, bag_size: 3968\n",
      "batch 499, loss: 0.0129, instance_loss: 0.0009, weighted_loss: 0.0093, label: 0, bag_size: 14956\n",
      "batch 519, loss: 0.5217, instance_loss: 1.6388, weighted_loss: 0.8568, label: 1, bag_size: 5366\n",
      "batch 539, loss: 0.0151, instance_loss: 0.0000, weighted_loss: 0.0106, label: 0, bag_size: 19043\n",
      "batch 559, loss: 0.0083, instance_loss: 0.0110, weighted_loss: 0.0091, label: 1, bag_size: 12795\n",
      "batch 579, loss: 0.0185, instance_loss: 0.0004, weighted_loss: 0.0131, label: 0, bag_size: 2844\n",
      "batch 599, loss: 0.0606, instance_loss: 0.0024, weighted_loss: 0.0432, label: 0, bag_size: 12910\n",
      "batch 619, loss: 0.0075, instance_loss: 0.0000, weighted_loss: 0.0053, label: 0, bag_size: 12732\n",
      "batch 639, loss: 0.0057, instance_loss: 0.0000, weighted_loss: 0.0040, label: 0, bag_size: 10942\n",
      "batch 659, loss: 0.0205, instance_loss: 0.0000, weighted_loss: 0.0144, label: 1, bag_size: 11875\n",
      "batch 679, loss: 0.1357, instance_loss: 0.0001, weighted_loss: 0.0950, label: 0, bag_size: 16521\n",
      "batch 699, loss: 0.4071, instance_loss: 0.4805, weighted_loss: 0.4291, label: 0, bag_size: 2242\n",
      "batch 719, loss: 0.0047, instance_loss: 0.0135, weighted_loss: 0.0073, label: 0, bag_size: 11759\n",
      "batch 739, loss: 0.0243, instance_loss: 0.1651, weighted_loss: 0.0665, label: 1, bag_size: 5340\n",
      "batch 759, loss: 0.0220, instance_loss: 0.0126, weighted_loss: 0.0191, label: 1, bag_size: 7186\n",
      "batch 779, loss: 0.7607, instance_loss: 0.9244, weighted_loss: 0.8098, label: 0, bag_size: 6356\n",
      "batch 799, loss: 0.0509, instance_loss: 0.2731, weighted_loss: 0.1176, label: 1, bag_size: 5763\n",
      "batch 819, loss: 0.0183, instance_loss: 0.0089, weighted_loss: 0.0155, label: 1, bag_size: 9408\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9766575425790754: correct 12845/13152\n",
      "class 1 clustering acc 0.8780413625304136: correct 5774/6576\n",
      "Epoch: 28, train_loss: 0.2208, train_clustering_loss:  0.2195, train_error: 0.0888\n",
      "class 0: acc 0.9066985645933014, correct 379/418\n",
      "class 1: acc 0.9158415841584159, correct 370/404\n",
      "\n",
      "Val Set, val_loss: 0.1576, val_error: 0.0870, auc: 0.9922\n",
      "class 0 clustering acc 0.9789402173913043: correct 1441/1472\n",
      "class 1 clustering acc 0.8953804347826086: correct 659/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.8888888888888888, correct 48/54\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0168, instance_loss: 0.0142, weighted_loss: 0.0160, label: 0, bag_size: 2534\n",
      "batch 39, loss: 0.4824, instance_loss: 2.6217, weighted_loss: 1.1242, label: 1, bag_size: 11386\n",
      "batch 59, loss: 0.0544, instance_loss: 0.0011, weighted_loss: 0.0384, label: 0, bag_size: 8959\n",
      "batch 79, loss: 0.0027, instance_loss: 0.0592, weighted_loss: 0.0196, label: 0, bag_size: 4902\n",
      "batch 99, loss: 0.0506, instance_loss: 0.0134, weighted_loss: 0.0394, label: 0, bag_size: 7605\n",
      "batch 119, loss: 1.3234, instance_loss: 0.6461, weighted_loss: 1.1202, label: 1, bag_size: 1683\n",
      "batch 139, loss: 0.0500, instance_loss: 0.0097, weighted_loss: 0.0379, label: 1, bag_size: 7445\n",
      "batch 159, loss: 0.0055, instance_loss: 0.0002, weighted_loss: 0.0039, label: 0, bag_size: 24911\n",
      "batch 179, loss: 0.0238, instance_loss: 0.0074, weighted_loss: 0.0189, label: 1, bag_size: 7119\n",
      "batch 199, loss: 0.0440, instance_loss: 0.0126, weighted_loss: 0.0346, label: 1, bag_size: 5155\n",
      "batch 219, loss: 1.2295, instance_loss: 0.1405, weighted_loss: 0.9028, label: 1, bag_size: 3121\n",
      "batch 239, loss: 0.0046, instance_loss: 0.0067, weighted_loss: 0.0052, label: 1, bag_size: 4367\n",
      "batch 259, loss: 0.0258, instance_loss: 0.0141, weighted_loss: 0.0223, label: 1, bag_size: 11363\n",
      "batch 279, loss: 0.0057, instance_loss: 0.0094, weighted_loss: 0.0068, label: 1, bag_size: 10558\n",
      "batch 299, loss: 0.0486, instance_loss: 0.2371, weighted_loss: 0.1051, label: 0, bag_size: 2609\n",
      "batch 319, loss: 0.0719, instance_loss: 0.0000, weighted_loss: 0.0503, label: 0, bag_size: 11390\n",
      "batch 339, loss: 0.1861, instance_loss: 0.0579, weighted_loss: 0.1476, label: 1, bag_size: 8103\n",
      "batch 359, loss: 0.0175, instance_loss: 0.6424, weighted_loss: 0.2050, label: 1, bag_size: 3980\n",
      "batch 379, loss: 0.0126, instance_loss: 0.0060, weighted_loss: 0.0106, label: 1, bag_size: 3683\n",
      "batch 399, loss: 0.7569, instance_loss: 0.2731, weighted_loss: 0.6118, label: 0, bag_size: 12722\n",
      "batch 419, loss: 0.0298, instance_loss: 0.0101, weighted_loss: 0.0239, label: 1, bag_size: 5345\n",
      "batch 439, loss: 0.0074, instance_loss: 0.0675, weighted_loss: 0.0254, label: 0, bag_size: 1824\n",
      "batch 459, loss: 0.3495, instance_loss: 0.5714, weighted_loss: 0.4161, label: 1, bag_size: 2314\n",
      "batch 479, loss: 0.0060, instance_loss: 0.0000, weighted_loss: 0.0042, label: 0, bag_size: 21682\n",
      "batch 499, loss: 0.0124, instance_loss: 0.0417, weighted_loss: 0.0212, label: 0, bag_size: 12131\n",
      "batch 519, loss: 0.0764, instance_loss: 0.0252, weighted_loss: 0.0610, label: 1, bag_size: 11160\n",
      "batch 539, loss: 4.0407, instance_loss: 0.2865, weighted_loss: 2.9144, label: 1, bag_size: 2565\n",
      "batch 559, loss: 0.0210, instance_loss: 0.1800, weighted_loss: 0.0687, label: 0, bag_size: 12687\n",
      "batch 579, loss: 0.1963, instance_loss: 0.0161, weighted_loss: 0.1422, label: 0, bag_size: 18738\n",
      "batch 599, loss: 0.3467, instance_loss: 0.9875, weighted_loss: 0.5390, label: 1, bag_size: 1963\n",
      "batch 619, loss: 0.0055, instance_loss: 0.0198, weighted_loss: 0.0098, label: 1, bag_size: 9955\n",
      "batch 639, loss: 0.0899, instance_loss: 0.0341, weighted_loss: 0.0732, label: 0, bag_size: 17155\n",
      "batch 659, loss: 0.0344, instance_loss: 0.0005, weighted_loss: 0.0242, label: 0, bag_size: 8788\n",
      "batch 679, loss: 0.0539, instance_loss: 0.0056, weighted_loss: 0.0394, label: 1, bag_size: 11875\n",
      "batch 699, loss: 0.0448, instance_loss: 0.0160, weighted_loss: 0.0361, label: 1, bag_size: 9636\n",
      "batch 719, loss: 0.1073, instance_loss: 0.0000, weighted_loss: 0.0751, label: 1, bag_size: 16565\n",
      "batch 739, loss: 0.0075, instance_loss: 0.0081, weighted_loss: 0.0077, label: 1, bag_size: 14202\n",
      "batch 759, loss: 0.0746, instance_loss: 0.0357, weighted_loss: 0.0629, label: 1, bag_size: 4786\n",
      "batch 779, loss: 0.0043, instance_loss: 0.0027, weighted_loss: 0.0038, label: 1, bag_size: 14030\n",
      "batch 799, loss: 0.0838, instance_loss: 0.0000, weighted_loss: 0.0586, label: 0, bag_size: 8025\n",
      "batch 819, loss: 0.1583, instance_loss: 1.2390, weighted_loss: 0.4825, label: 1, bag_size: 11394\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9738442822384428: correct 12808/13152\n",
      "class 1 clustering acc 0.8527980535279805: correct 5608/6576\n",
      "Epoch: 29, train_loss: 0.2619, train_clustering_loss:  0.2588, train_error: 0.0876\n",
      "class 0: acc 0.9138755980861244, correct 382/418\n",
      "class 1: acc 0.9108910891089109, correct 368/404\n",
      "\n",
      "Val Set, val_loss: 0.1680, val_error: 0.0870, auc: 0.9932\n",
      "class 0 clustering acc 0.9707880434782609: correct 1429/1472\n",
      "class 1 clustering acc 0.8546195652173914: correct 629/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.8888888888888888, correct 48/54\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0141, instance_loss: 0.1571, weighted_loss: 0.0570, label: 1, bag_size: 13194\n",
      "batch 39, loss: 0.1209, instance_loss: 0.0373, weighted_loss: 0.0958, label: 0, bag_size: 7141\n",
      "batch 59, loss: 0.6935, instance_loss: 0.1212, weighted_loss: 0.5218, label: 0, bag_size: 4418\n",
      "batch 79, loss: 0.0469, instance_loss: 0.0284, weighted_loss: 0.0413, label: 0, bag_size: 4959\n",
      "batch 99, loss: 0.0791, instance_loss: 0.2785, weighted_loss: 0.1389, label: 1, bag_size: 16267\n",
      "batch 119, loss: 0.0182, instance_loss: 0.0000, weighted_loss: 0.0128, label: 1, bag_size: 13365\n",
      "batch 139, loss: 0.0302, instance_loss: 0.1498, weighted_loss: 0.0661, label: 0, bag_size: 17155\n",
      "batch 159, loss: 0.0254, instance_loss: 0.0046, weighted_loss: 0.0191, label: 1, bag_size: 14887\n",
      "batch 179, loss: 0.0121, instance_loss: 0.0009, weighted_loss: 0.0087, label: 1, bag_size: 19832\n",
      "batch 199, loss: 2.4485, instance_loss: 0.9308, weighted_loss: 1.9932, label: 0, bag_size: 2179\n",
      "batch 219, loss: 3.5780, instance_loss: 2.9785, weighted_loss: 3.3982, label: 0, bag_size: 5105\n",
      "batch 239, loss: 0.0029, instance_loss: 0.0000, weighted_loss: 0.0020, label: 1, bag_size: 6966\n",
      "batch 259, loss: 4.4717, instance_loss: 0.3423, weighted_loss: 3.2329, label: 0, bag_size: 3468\n",
      "batch 279, loss: 0.1257, instance_loss: 0.9080, weighted_loss: 0.3604, label: 0, bag_size: 8744\n",
      "batch 299, loss: 0.0636, instance_loss: 0.0045, weighted_loss: 0.0458, label: 0, bag_size: 10415\n",
      "batch 319, loss: 0.0580, instance_loss: 0.0006, weighted_loss: 0.0408, label: 0, bag_size: 3710\n",
      "batch 339, loss: 0.1047, instance_loss: 0.0000, weighted_loss: 0.0733, label: 1, bag_size: 16565\n",
      "batch 359, loss: 0.2340, instance_loss: 1.1951, weighted_loss: 0.5223, label: 0, bag_size: 15255\n",
      "batch 379, loss: 0.0152, instance_loss: 0.0000, weighted_loss: 0.0107, label: 1, bag_size: 15093\n",
      "batch 399, loss: 0.0923, instance_loss: 0.1081, weighted_loss: 0.0971, label: 0, bag_size: 3725\n",
      "batch 419, loss: 0.0053, instance_loss: 0.0002, weighted_loss: 0.0038, label: 1, bag_size: 18468\n",
      "batch 439, loss: 0.0204, instance_loss: 0.0289, weighted_loss: 0.0229, label: 1, bag_size: 4308\n",
      "batch 459, loss: 0.1966, instance_loss: 0.0667, weighted_loss: 0.1576, label: 0, bag_size: 7989\n",
      "batch 479, loss: 0.0588, instance_loss: 0.0769, weighted_loss: 0.0643, label: 0, bag_size: 2457\n",
      "batch 499, loss: 0.1695, instance_loss: 0.0000, weighted_loss: 0.1187, label: 1, bag_size: 16565\n",
      "batch 519, loss: 4.6162, instance_loss: 3.7052, weighted_loss: 4.3429, label: 1, bag_size: 25831\n",
      "batch 539, loss: 0.5761, instance_loss: 0.9407, weighted_loss: 0.6855, label: 0, bag_size: 12083\n",
      "batch 559, loss: 0.0152, instance_loss: 0.0050, weighted_loss: 0.0121, label: 1, bag_size: 5023\n",
      "batch 579, loss: 0.0166, instance_loss: 0.0091, weighted_loss: 0.0143, label: 1, bag_size: 6090\n",
      "batch 599, loss: 0.5276, instance_loss: 0.2044, weighted_loss: 0.4307, label: 0, bag_size: 7557\n",
      "batch 619, loss: 0.0074, instance_loss: 0.0683, weighted_loss: 0.0257, label: 1, bag_size: 9878\n",
      "batch 639, loss: 0.0085, instance_loss: 0.0171, weighted_loss: 0.0111, label: 0, bag_size: 22762\n",
      "batch 659, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 18076\n",
      "batch 679, loss: 0.7443, instance_loss: 0.1721, weighted_loss: 0.5726, label: 0, bag_size: 2918\n",
      "batch 699, loss: 0.0341, instance_loss: 0.0000, weighted_loss: 0.0238, label: 0, bag_size: 9542\n",
      "batch 719, loss: 0.0004, instance_loss: 0.1652, weighted_loss: 0.0498, label: 1, bag_size: 1743\n",
      "batch 739, loss: 0.1801, instance_loss: 0.0533, weighted_loss: 0.1420, label: 1, bag_size: 771\n",
      "batch 759, loss: 0.0111, instance_loss: 0.0049, weighted_loss: 0.0092, label: 1, bag_size: 3683\n",
      "batch 779, loss: 0.0164, instance_loss: 0.0000, weighted_loss: 0.0115, label: 0, bag_size: 8788\n",
      "batch 799, loss: 0.0157, instance_loss: 1.3020, weighted_loss: 0.4016, label: 0, bag_size: 1614\n",
      "batch 819, loss: 0.0022, instance_loss: 0.0005, weighted_loss: 0.0017, label: 0, bag_size: 21576\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9774939172749392: correct 12856/13152\n",
      "class 1 clustering acc 0.8851885644768857: correct 5821/6576\n",
      "Epoch: 30, train_loss: 0.2275, train_clustering_loss:  0.2221, train_error: 0.0839\n",
      "class 0: acc 0.9051094890510949, correct 372/411\n",
      "class 1: acc 0.927007299270073, correct 381/411\n",
      "\n",
      "Val Set, val_loss: 0.1392, val_error: 0.0652, auc: 0.9932\n",
      "class 0 clustering acc 0.9741847826086957: correct 1434/1472\n",
      "class 1 clustering acc 0.8967391304347826: correct 660/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.9259259259259259, correct 50/54\n",
      "Validation loss decreased (0.142411 --> 0.139210).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0413, instance_loss: 0.0257, weighted_loss: 0.0366, label: 1, bag_size: 2682\n",
      "batch 39, loss: 0.0047, instance_loss: 0.0000, weighted_loss: 0.0033, label: 0, bag_size: 26271\n",
      "batch 59, loss: 0.0271, instance_loss: 0.0000, weighted_loss: 0.0190, label: 1, bag_size: 3980\n",
      "batch 79, loss: 0.0890, instance_loss: 0.0047, weighted_loss: 0.0637, label: 1, bag_size: 1638\n",
      "batch 99, loss: 0.0409, instance_loss: 0.0002, weighted_loss: 0.0287, label: 0, bag_size: 2044\n",
      "batch 119, loss: 0.2356, instance_loss: 0.0039, weighted_loss: 0.1661, label: 1, bag_size: 7981\n",
      "batch 139, loss: 0.0108, instance_loss: 0.0290, weighted_loss: 0.0163, label: 1, bag_size: 11266\n",
      "batch 159, loss: 0.9933, instance_loss: 0.0275, weighted_loss: 0.7036, label: 0, bag_size: 3760\n",
      "batch 179, loss: 0.2793, instance_loss: 0.1845, weighted_loss: 0.2509, label: 1, bag_size: 7669\n",
      "batch 199, loss: 1.0168, instance_loss: 0.6645, weighted_loss: 0.9111, label: 1, bag_size: 1095\n",
      "batch 219, loss: 0.0445, instance_loss: 0.2306, weighted_loss: 0.1003, label: 1, bag_size: 1064\n",
      "batch 239, loss: 0.3057, instance_loss: 0.2546, weighted_loss: 0.2904, label: 1, bag_size: 2137\n",
      "batch 259, loss: 0.1824, instance_loss: 2.1620, weighted_loss: 0.7763, label: 1, bag_size: 8982\n",
      "batch 279, loss: 0.1502, instance_loss: 0.0310, weighted_loss: 0.1145, label: 1, bag_size: 21450\n",
      "batch 299, loss: 0.0025, instance_loss: 0.2935, weighted_loss: 0.0898, label: 1, bag_size: 22264\n",
      "batch 319, loss: 0.9804, instance_loss: 2.2419, weighted_loss: 1.3588, label: 0, bag_size: 1714\n",
      "batch 339, loss: 0.0092, instance_loss: 0.0005, weighted_loss: 0.0066, label: 0, bag_size: 13880\n",
      "batch 359, loss: 0.2683, instance_loss: 0.0365, weighted_loss: 0.1988, label: 0, bag_size: 2996\n",
      "batch 379, loss: 1.2134, instance_loss: 0.1632, weighted_loss: 0.8984, label: 0, bag_size: 2959\n",
      "batch 399, loss: 0.1236, instance_loss: 0.3060, weighted_loss: 0.1784, label: 0, bag_size: 2296\n",
      "batch 419, loss: 0.0003, instance_loss: 0.0271, weighted_loss: 0.0084, label: 1, bag_size: 629\n",
      "batch 439, loss: 0.0771, instance_loss: 0.2351, weighted_loss: 0.1245, label: 1, bag_size: 1038\n",
      "batch 459, loss: 0.0135, instance_loss: 0.0000, weighted_loss: 0.0095, label: 0, bag_size: 26271\n",
      "batch 479, loss: 0.0048, instance_loss: 0.0016, weighted_loss: 0.0038, label: 0, bag_size: 11512\n",
      "batch 499, loss: 0.2370, instance_loss: 0.3494, weighted_loss: 0.2707, label: 0, bag_size: 4523\n",
      "batch 519, loss: 0.0017, instance_loss: 0.0001, weighted_loss: 0.0012, label: 0, bag_size: 8948\n",
      "batch 539, loss: 0.0269, instance_loss: 0.0070, weighted_loss: 0.0209, label: 0, bag_size: 2998\n",
      "batch 559, loss: 0.1337, instance_loss: 0.0039, weighted_loss: 0.0948, label: 0, bag_size: 18738\n",
      "batch 579, loss: 0.0073, instance_loss: 0.0013, weighted_loss: 0.0055, label: 1, bag_size: 4039\n",
      "batch 599, loss: 0.0250, instance_loss: 0.0035, weighted_loss: 0.0185, label: 1, bag_size: 10482\n",
      "batch 619, loss: 1.0296, instance_loss: 0.0000, weighted_loss: 0.7207, label: 1, bag_size: 11729\n",
      "batch 639, loss: 0.0914, instance_loss: 0.0136, weighted_loss: 0.0681, label: 1, bag_size: 10072\n",
      "batch 659, loss: 0.2122, instance_loss: 0.0111, weighted_loss: 0.1519, label: 1, bag_size: 7066\n",
      "batch 679, loss: 0.0042, instance_loss: 0.0000, weighted_loss: 0.0030, label: 1, bag_size: 20333\n",
      "batch 699, loss: 0.0060, instance_loss: 0.0019, weighted_loss: 0.0048, label: 0, bag_size: 4465\n",
      "batch 719, loss: 0.1590, instance_loss: 0.2057, weighted_loss: 0.1730, label: 0, bag_size: 3893\n",
      "batch 739, loss: 0.7805, instance_loss: 0.1936, weighted_loss: 0.6044, label: 0, bag_size: 9597\n",
      "batch 759, loss: 0.0037, instance_loss: 0.0000, weighted_loss: 0.0026, label: 1, bag_size: 10558\n",
      "batch 779, loss: 0.0809, instance_loss: 0.3398, weighted_loss: 0.1585, label: 1, bag_size: 1294\n",
      "batch 799, loss: 0.0034, instance_loss: 0.0006, weighted_loss: 0.0026, label: 0, bag_size: 11759\n",
      "batch 819, loss: 0.0073, instance_loss: 0.0000, weighted_loss: 0.0051, label: 0, bag_size: 31106\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9763534063260341: correct 12841/13152\n",
      "class 1 clustering acc 0.8791058394160584: correct 5781/6576\n",
      "Epoch: 31, train_loss: 0.2189, train_clustering_loss:  0.2268, train_error: 0.0791\n",
      "class 0: acc 0.9253731343283582, correct 372/402\n",
      "class 1: acc 0.9166666666666666, correct 385/420\n",
      "\n",
      "Val Set, val_loss: 0.1106, val_error: 0.0435, auc: 0.9951\n",
      "class 0 clustering acc 0.9911684782608695: correct 1459/1472\n",
      "class 1 clustering acc 0.845108695652174: correct 622/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "Validation loss decreased (0.139210 --> 0.110581).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0046, instance_loss: 0.1097, weighted_loss: 0.0362, label: 1, bag_size: 3634\n",
      "batch 39, loss: 0.0363, instance_loss: 0.0576, weighted_loss: 0.0427, label: 0, bag_size: 10814\n",
      "batch 59, loss: 0.3933, instance_loss: 0.1472, weighted_loss: 0.3195, label: 1, bag_size: 6360\n",
      "batch 79, loss: 0.2266, instance_loss: 0.0116, weighted_loss: 0.1621, label: 1, bag_size: 15931\n",
      "batch 99, loss: 0.0030, instance_loss: 0.0049, weighted_loss: 0.0035, label: 1, bag_size: 3453\n",
      "batch 119, loss: 0.0954, instance_loss: 0.0845, weighted_loss: 0.0921, label: 1, bag_size: 5516\n",
      "batch 139, loss: 0.1111, instance_loss: 0.4332, weighted_loss: 0.2077, label: 0, bag_size: 2104\n",
      "batch 159, loss: 0.0033, instance_loss: 0.0877, weighted_loss: 0.0286, label: 1, bag_size: 12795\n",
      "batch 179, loss: 0.0337, instance_loss: 0.2707, weighted_loss: 0.1048, label: 0, bag_size: 1370\n",
      "batch 199, loss: 0.1386, instance_loss: 0.1430, weighted_loss: 0.1399, label: 0, bag_size: 17083\n",
      "batch 219, loss: 0.0174, instance_loss: 0.0451, weighted_loss: 0.0257, label: 1, bag_size: 11032\n",
      "batch 239, loss: 0.0343, instance_loss: 0.6049, weighted_loss: 0.2055, label: 0, bag_size: 12687\n",
      "batch 259, loss: 0.1403, instance_loss: 0.0968, weighted_loss: 0.1272, label: 1, bag_size: 2814\n",
      "batch 279, loss: 0.0518, instance_loss: 0.0893, weighted_loss: 0.0631, label: 0, bag_size: 10721\n",
      "batch 299, loss: 0.0286, instance_loss: 0.0394, weighted_loss: 0.0318, label: 1, bag_size: 2559\n",
      "batch 319, loss: 0.0037, instance_loss: 0.0000, weighted_loss: 0.0026, label: 0, bag_size: 19472\n",
      "batch 339, loss: 0.0012, instance_loss: 0.0004, weighted_loss: 0.0010, label: 0, bag_size: 16782\n",
      "batch 359, loss: 0.0838, instance_loss: 0.0918, weighted_loss: 0.0862, label: 1, bag_size: 12626\n",
      "batch 379, loss: 0.0322, instance_loss: 0.0402, weighted_loss: 0.0346, label: 1, bag_size: 1622\n",
      "batch 399, loss: 0.0178, instance_loss: 0.0000, weighted_loss: 0.0125, label: 0, bag_size: 2036\n",
      "batch 419, loss: 0.5922, instance_loss: 0.0693, weighted_loss: 0.4353, label: 0, bag_size: 11212\n",
      "batch 439, loss: 0.1877, instance_loss: 0.0002, weighted_loss: 0.1314, label: 1, bag_size: 10591\n",
      "batch 459, loss: 0.0348, instance_loss: 0.0009, weighted_loss: 0.0246, label: 1, bag_size: 21701\n",
      "batch 479, loss: 0.2314, instance_loss: 0.3312, weighted_loss: 0.2613, label: 0, bag_size: 2296\n",
      "batch 499, loss: 0.0034, instance_loss: 0.0155, weighted_loss: 0.0070, label: 0, bag_size: 3190\n",
      "batch 519, loss: 0.3002, instance_loss: 0.0561, weighted_loss: 0.2270, label: 0, bag_size: 1213\n",
      "batch 539, loss: 0.0471, instance_loss: 0.0000, weighted_loss: 0.0329, label: 0, bag_size: 30828\n",
      "batch 559, loss: 0.0510, instance_loss: 0.1081, weighted_loss: 0.0681, label: 1, bag_size: 3968\n",
      "batch 579, loss: 0.0129, instance_loss: 0.0043, weighted_loss: 0.0103, label: 1, bag_size: 13786\n",
      "batch 599, loss: 0.0612, instance_loss: 0.0000, weighted_loss: 0.0428, label: 1, bag_size: 20161\n",
      "batch 619, loss: 0.0169, instance_loss: 0.0000, weighted_loss: 0.0118, label: 0, bag_size: 47866\n",
      "batch 639, loss: 0.0426, instance_loss: 0.0059, weighted_loss: 0.0316, label: 0, bag_size: 23791\n",
      "batch 659, loss: 0.0032, instance_loss: 0.0069, weighted_loss: 0.0043, label: 1, bag_size: 6606\n",
      "batch 679, loss: 0.1111, instance_loss: 0.0099, weighted_loss: 0.0807, label: 1, bag_size: 5454\n",
      "batch 699, loss: 0.4952, instance_loss: 0.0042, weighted_loss: 0.3479, label: 1, bag_size: 9942\n",
      "batch 719, loss: 0.3017, instance_loss: 0.2462, weighted_loss: 0.2850, label: 1, bag_size: 1191\n",
      "batch 739, loss: 1.7822, instance_loss: 0.3943, weighted_loss: 1.3658, label: 0, bag_size: 2467\n",
      "batch 759, loss: 0.1508, instance_loss: 0.1080, weighted_loss: 0.1380, label: 0, bag_size: 2351\n",
      "batch 779, loss: 0.1969, instance_loss: 0.1652, weighted_loss: 0.1874, label: 0, bag_size: 9069\n",
      "batch 799, loss: 0.0018, instance_loss: 0.0000, weighted_loss: 0.0013, label: 0, bag_size: 30751\n",
      "batch 819, loss: 0.0049, instance_loss: 0.0026, weighted_loss: 0.0042, label: 1, bag_size: 4102\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9786344282238443: correct 12871/13152\n",
      "class 1 clustering acc 0.8760644768856448: correct 5761/6576\n",
      "Epoch: 32, train_loss: 0.2282, train_clustering_loss:  0.2158, train_error: 0.0876\n",
      "class 0: acc 0.9095607235142119, correct 352/387\n",
      "class 1: acc 0.9149425287356322, correct 398/435\n",
      "\n",
      "Val Set, val_loss: 0.1363, val_error: 0.0652, auc: 0.9932\n",
      "class 0 clustering acc 0.9891304347826086: correct 1456/1472\n",
      "class 1 clustering acc 0.9578804347826086: correct 705/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.9259259259259259, correct 50/54\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1246, instance_loss: 0.0009, weighted_loss: 0.0874, label: 1, bag_size: 6731\n",
      "batch 39, loss: 0.0004, instance_loss: 0.0033, weighted_loss: 0.0013, label: 0, bag_size: 1984\n",
      "batch 59, loss: 0.0063, instance_loss: 0.0000, weighted_loss: 0.0044, label: 0, bag_size: 11727\n",
      "batch 79, loss: 0.1423, instance_loss: 0.1670, weighted_loss: 0.1497, label: 1, bag_size: 2559\n",
      "batch 99, loss: 0.0331, instance_loss: 0.0010, weighted_loss: 0.0235, label: 0, bag_size: 2920\n",
      "batch 119, loss: 0.0424, instance_loss: 0.0020, weighted_loss: 0.0303, label: 1, bag_size: 7613\n",
      "batch 139, loss: 0.0029, instance_loss: 0.0000, weighted_loss: 0.0021, label: 1, bag_size: 9756\n",
      "batch 159, loss: 0.3055, instance_loss: 0.0870, weighted_loss: 0.2400, label: 0, bag_size: 1690\n",
      "batch 179, loss: 0.3252, instance_loss: 0.0126, weighted_loss: 0.2314, label: 1, bag_size: 7768\n",
      "batch 199, loss: 0.0231, instance_loss: 0.1444, weighted_loss: 0.0595, label: 1, bag_size: 6171\n",
      "batch 219, loss: 0.0056, instance_loss: 0.0002, weighted_loss: 0.0039, label: 0, bag_size: 4465\n",
      "batch 239, loss: 0.0052, instance_loss: 0.4308, weighted_loss: 0.1329, label: 0, bag_size: 3101\n",
      "batch 259, loss: 0.2023, instance_loss: 0.0155, weighted_loss: 0.1462, label: 1, bag_size: 1437\n",
      "batch 279, loss: 0.2545, instance_loss: 0.0005, weighted_loss: 0.1783, label: 1, bag_size: 6599\n",
      "batch 299, loss: 0.0248, instance_loss: 0.1558, weighted_loss: 0.0641, label: 1, bag_size: 1437\n",
      "batch 319, loss: 0.0467, instance_loss: 0.0045, weighted_loss: 0.0340, label: 1, bag_size: 14887\n",
      "batch 339, loss: 0.2697, instance_loss: 5.7037, weighted_loss: 1.8999, label: 0, bag_size: 20555\n",
      "batch 359, loss: 0.0060, instance_loss: 0.0022, weighted_loss: 0.0049, label: 0, bag_size: 20910\n",
      "batch 379, loss: 0.0051, instance_loss: 0.0141, weighted_loss: 0.0078, label: 1, bag_size: 3295\n",
      "batch 399, loss: 0.0041, instance_loss: 0.0000, weighted_loss: 0.0029, label: 0, bag_size: 21404\n",
      "batch 419, loss: 0.2033, instance_loss: 0.1397, weighted_loss: 0.1842, label: 1, bag_size: 2522\n",
      "batch 439, loss: 0.1149, instance_loss: 0.0211, weighted_loss: 0.0867, label: 1, bag_size: 5256\n",
      "batch 459, loss: 0.2823, instance_loss: 0.0010, weighted_loss: 0.1979, label: 1, bag_size: 15689\n",
      "batch 479, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 8812\n",
      "batch 499, loss: 0.0133, instance_loss: 0.0012, weighted_loss: 0.0096, label: 1, bag_size: 7382\n",
      "batch 519, loss: 0.0075, instance_loss: 0.0376, weighted_loss: 0.0165, label: 1, bag_size: 2381\n",
      "batch 539, loss: 0.1519, instance_loss: 1.1241, weighted_loss: 0.4436, label: 0, bag_size: 4523\n",
      "batch 559, loss: 0.0693, instance_loss: 0.2089, weighted_loss: 0.1112, label: 1, bag_size: 3224\n",
      "batch 579, loss: 0.0042, instance_loss: 0.0258, weighted_loss: 0.0107, label: 1, bag_size: 6966\n",
      "batch 599, loss: 0.0728, instance_loss: 0.1468, weighted_loss: 0.0950, label: 0, bag_size: 1416\n",
      "batch 619, loss: 0.0951, instance_loss: 0.3705, weighted_loss: 0.1777, label: 0, bag_size: 10721\n",
      "batch 639, loss: 0.1147, instance_loss: 0.1516, weighted_loss: 0.1258, label: 1, bag_size: 1875\n",
      "batch 659, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 11778\n",
      "batch 679, loss: 0.0071, instance_loss: 0.0000, weighted_loss: 0.0050, label: 1, bag_size: 12095\n",
      "batch 699, loss: 0.3147, instance_loss: 0.3508, weighted_loss: 0.3255, label: 0, bag_size: 1592\n",
      "batch 719, loss: 0.3262, instance_loss: 0.0030, weighted_loss: 0.2293, label: 1, bag_size: 1759\n",
      "batch 739, loss: 0.0664, instance_loss: 0.0015, weighted_loss: 0.0469, label: 1, bag_size: 5629\n",
      "batch 759, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 7191\n",
      "batch 779, loss: 0.0017, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 19667\n",
      "batch 799, loss: 0.1920, instance_loss: 0.6370, weighted_loss: 0.3255, label: 1, bag_size: 12340\n",
      "batch 819, loss: 0.0207, instance_loss: 0.0510, weighted_loss: 0.0298, label: 0, bag_size: 3657\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9778740875912408: correct 12861/13152\n",
      "class 1 clustering acc 0.88838199513382: correct 5842/6576\n",
      "Epoch: 33, train_loss: 0.2297, train_clustering_loss:  0.2190, train_error: 0.0730\n",
      "class 0: acc 0.9344262295081968, correct 399/427\n",
      "class 1: acc 0.9189873417721519, correct 363/395\n",
      "\n",
      "Val Set, val_loss: 0.2030, val_error: 0.0761, auc: 0.9932\n",
      "class 0 clustering acc 0.9735054347826086: correct 1433/1472\n",
      "class 1 clustering acc 0.779891304347826: correct 574/736\n",
      "class 0: acc 1.0, correct 38/38\n",
      "class 1: acc 0.8703703703703703, correct 47/54\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2140, instance_loss: 0.2764, weighted_loss: 0.2327, label: 1, bag_size: 15125\n",
      "batch 39, loss: 0.0085, instance_loss: 0.3077, weighted_loss: 0.0983, label: 0, bag_size: 10751\n",
      "batch 59, loss: 0.4580, instance_loss: 0.2764, weighted_loss: 0.4035, label: 1, bag_size: 1497\n",
      "batch 79, loss: 0.2597, instance_loss: 0.0663, weighted_loss: 0.2017, label: 0, bag_size: 3783\n",
      "batch 99, loss: 0.1309, instance_loss: 0.3358, weighted_loss: 0.1924, label: 1, bag_size: 1609\n",
      "batch 119, loss: 0.0496, instance_loss: 0.0157, weighted_loss: 0.0394, label: 1, bag_size: 8040\n",
      "batch 139, loss: 0.0197, instance_loss: 0.0046, weighted_loss: 0.0151, label: 1, bag_size: 11600\n",
      "batch 159, loss: 0.1090, instance_loss: 0.0878, weighted_loss: 0.1026, label: 0, bag_size: 13023\n",
      "batch 179, loss: 0.3055, instance_loss: 0.0028, weighted_loss: 0.2147, label: 0, bag_size: 13619\n",
      "batch 199, loss: 0.0036, instance_loss: 0.0001, weighted_loss: 0.0026, label: 0, bag_size: 9234\n",
      "batch 219, loss: 0.0060, instance_loss: 0.0001, weighted_loss: 0.0042, label: 0, bag_size: 19466\n",
      "batch 239, loss: 1.0507, instance_loss: 0.0802, weighted_loss: 0.7595, label: 0, bag_size: 2653\n",
      "batch 259, loss: 0.0026, instance_loss: 0.0061, weighted_loss: 0.0036, label: 0, bag_size: 11512\n",
      "batch 279, loss: 0.1262, instance_loss: 1.0423, weighted_loss: 0.4011, label: 0, bag_size: 1483\n",
      "batch 299, loss: 0.1235, instance_loss: 0.1784, weighted_loss: 0.1400, label: 0, bag_size: 3774\n",
      "batch 319, loss: 0.3538, instance_loss: 1.8422, weighted_loss: 0.8003, label: 1, bag_size: 983\n",
      "batch 339, loss: 0.1376, instance_loss: 0.0563, weighted_loss: 0.1132, label: 1, bag_size: 16154\n",
      "batch 359, loss: 0.0059, instance_loss: 0.0115, weighted_loss: 0.0076, label: 0, bag_size: 12731\n",
      "batch 379, loss: 0.0258, instance_loss: 0.0000, weighted_loss: 0.0181, label: 0, bag_size: 24439\n",
      "batch 399, loss: 0.0039, instance_loss: 0.0119, weighted_loss: 0.0063, label: 0, bag_size: 27158\n",
      "batch 419, loss: 0.0059, instance_loss: 0.0012, weighted_loss: 0.0045, label: 1, bag_size: 18095\n",
      "batch 439, loss: 0.0006, instance_loss: 0.0007, weighted_loss: 0.0006, label: 1, bag_size: 10920\n",
      "batch 459, loss: 0.5034, instance_loss: 0.1786, weighted_loss: 0.4059, label: 1, bag_size: 1703\n",
      "batch 479, loss: 0.3060, instance_loss: 0.7277, weighted_loss: 0.4325, label: 1, bag_size: 1963\n",
      "batch 499, loss: 0.0528, instance_loss: 0.0002, weighted_loss: 0.0370, label: 0, bag_size: 2732\n",
      "batch 519, loss: 0.0211, instance_loss: 0.0022, weighted_loss: 0.0154, label: 0, bag_size: 11917\n",
      "batch 539, loss: 0.0186, instance_loss: 0.0941, weighted_loss: 0.0413, label: 1, bag_size: 2356\n",
      "batch 559, loss: 0.0391, instance_loss: 0.1684, weighted_loss: 0.0779, label: 0, bag_size: 3970\n",
      "batch 579, loss: 0.0154, instance_loss: 0.0044, weighted_loss: 0.0121, label: 0, bag_size: 18415\n",
      "batch 599, loss: 0.0472, instance_loss: 0.0064, weighted_loss: 0.0349, label: 0, bag_size: 10942\n",
      "batch 619, loss: 0.0607, instance_loss: 0.0006, weighted_loss: 0.0427, label: 1, bag_size: 7351\n",
      "batch 639, loss: 0.0072, instance_loss: 0.0000, weighted_loss: 0.0050, label: 0, bag_size: 21682\n",
      "batch 659, loss: 0.0008, instance_loss: 0.0001, weighted_loss: 0.0006, label: 0, bag_size: 3190\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15850\n",
      "batch 699, loss: 0.0079, instance_loss: 0.0007, weighted_loss: 0.0057, label: 1, bag_size: 13194\n",
      "batch 719, loss: 0.0565, instance_loss: 0.0974, weighted_loss: 0.0688, label: 1, bag_size: 1051\n",
      "batch 739, loss: 0.0582, instance_loss: 0.0002, weighted_loss: 0.0408, label: 1, bag_size: 13026\n",
      "batch 759, loss: 0.0219, instance_loss: 0.0003, weighted_loss: 0.0154, label: 1, bag_size: 8216\n",
      "batch 779, loss: 0.0097, instance_loss: 0.0009, weighted_loss: 0.0071, label: 0, bag_size: 11654\n",
      "batch 799, loss: 0.1480, instance_loss: 0.0153, weighted_loss: 0.1082, label: 1, bag_size: 7981\n",
      "batch 819, loss: 0.0028, instance_loss: 0.0003, weighted_loss: 0.0021, label: 1, bag_size: 10105\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9841849148418491: correct 12944/13152\n",
      "class 1 clustering acc 0.9265510948905109: correct 6093/6576\n",
      "Epoch: 34, train_loss: 0.1898, train_clustering_loss:  0.1549, train_error: 0.0754\n",
      "class 0: acc 0.9226804123711341, correct 358/388\n",
      "class 1: acc 0.9262672811059908, correct 402/434\n",
      "\n",
      "Val Set, val_loss: 0.1067, val_error: 0.0217, auc: 0.9932\n",
      "class 0 clustering acc 0.9741847826086957: correct 1434/1472\n",
      "class 1 clustering acc 0.9130434782608695: correct 672/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 1.0, correct 54/54\n",
      "Validation loss decreased (0.110581 --> 0.106688).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.8527, instance_loss: 0.7433, weighted_loss: 0.8199, label: 0, bag_size: 1614\n",
      "batch 39, loss: 0.1548, instance_loss: 0.0054, weighted_loss: 0.1100, label: 1, bag_size: 12714\n",
      "batch 59, loss: 0.4332, instance_loss: 0.0462, weighted_loss: 0.3171, label: 0, bag_size: 546\n",
      "batch 79, loss: 0.0006, instance_loss: 0.0112, weighted_loss: 0.0038, label: 1, bag_size: 3004\n",
      "batch 99, loss: 0.0071, instance_loss: 0.0006, weighted_loss: 0.0052, label: 0, bag_size: 12131\n",
      "batch 119, loss: 1.7906, instance_loss: 0.2098, weighted_loss: 1.3163, label: 1, bag_size: 1764\n",
      "batch 139, loss: 0.2524, instance_loss: 0.0583, weighted_loss: 0.1942, label: 0, bag_size: 6624\n",
      "batch 159, loss: 0.0019, instance_loss: 0.0047, weighted_loss: 0.0028, label: 0, bag_size: 17791\n",
      "batch 179, loss: 0.0022, instance_loss: 0.0018, weighted_loss: 0.0021, label: 1, bag_size: 12795\n",
      "batch 199, loss: 0.0435, instance_loss: 0.4361, weighted_loss: 0.1613, label: 1, bag_size: 12697\n",
      "batch 219, loss: 0.0054, instance_loss: 0.0038, weighted_loss: 0.0049, label: 1, bag_size: 1022\n",
      "batch 239, loss: 0.1648, instance_loss: 0.0001, weighted_loss: 0.1154, label: 0, bag_size: 15003\n",
      "batch 259, loss: 0.0004, instance_loss: 0.0011, weighted_loss: 0.0006, label: 1, bag_size: 11642\n",
      "batch 279, loss: 0.0839, instance_loss: 0.1160, weighted_loss: 0.0935, label: 1, bag_size: 5516\n",
      "batch 299, loss: 0.0022, instance_loss: 0.0026, weighted_loss: 0.0023, label: 1, bag_size: 3937\n",
      "batch 319, loss: 0.0311, instance_loss: 0.3327, weighted_loss: 0.1216, label: 0, bag_size: 2652\n",
      "batch 339, loss: 0.1977, instance_loss: 0.1796, weighted_loss: 0.1922, label: 1, bag_size: 5366\n",
      "batch 359, loss: 0.0335, instance_loss: 0.0226, weighted_loss: 0.0302, label: 1, bag_size: 12127\n",
      "batch 379, loss: 0.0150, instance_loss: 0.0007, weighted_loss: 0.0107, label: 0, bag_size: 31085\n",
      "batch 399, loss: 0.1431, instance_loss: 0.1447, weighted_loss: 0.1436, label: 0, bag_size: 1142\n",
      "batch 419, loss: 0.0010, instance_loss: 0.0001, weighted_loss: 0.0007, label: 0, bag_size: 18240\n",
      "batch 439, loss: 0.2469, instance_loss: 0.0031, weighted_loss: 0.1737, label: 0, bag_size: 12840\n",
      "batch 459, loss: 0.0317, instance_loss: 0.0039, weighted_loss: 0.0234, label: 0, bag_size: 1452\n",
      "batch 479, loss: 0.0158, instance_loss: 0.0000, weighted_loss: 0.0110, label: 0, bag_size: 2998\n",
      "batch 499, loss: 0.4968, instance_loss: 0.0052, weighted_loss: 0.3493, label: 0, bag_size: 2959\n",
      "batch 519, loss: 0.1836, instance_loss: 0.0092, weighted_loss: 0.1313, label: 1, bag_size: 5345\n",
      "batch 539, loss: 0.0028, instance_loss: 0.0010, weighted_loss: 0.0023, label: 1, bag_size: 9756\n",
      "batch 559, loss: 1.4319, instance_loss: 0.5479, weighted_loss: 1.1667, label: 0, bag_size: 10381\n",
      "batch 579, loss: 0.0046, instance_loss: 0.0018, weighted_loss: 0.0038, label: 1, bag_size: 1249\n",
      "batch 599, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 17437\n",
      "batch 619, loss: 0.0429, instance_loss: 0.0330, weighted_loss: 0.0399, label: 0, bag_size: 1370\n",
      "batch 639, loss: 0.1019, instance_loss: 0.1043, weighted_loss: 0.1026, label: 0, bag_size: 6727\n",
      "batch 659, loss: 0.0082, instance_loss: 0.1644, weighted_loss: 0.0550, label: 1, bag_size: 2638\n",
      "batch 679, loss: 0.0229, instance_loss: 0.6272, weighted_loss: 0.2042, label: 1, bag_size: 2785\n",
      "batch 699, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 19659\n",
      "batch 719, loss: 0.1492, instance_loss: 0.0067, weighted_loss: 0.1064, label: 1, bag_size: 6016\n",
      "batch 739, loss: 0.0009, instance_loss: 0.0052, weighted_loss: 0.0022, label: 1, bag_size: 6453\n",
      "batch 759, loss: 0.1211, instance_loss: 0.0277, weighted_loss: 0.0931, label: 1, bag_size: 12714\n",
      "batch 779, loss: 0.6462, instance_loss: 0.2192, weighted_loss: 0.5181, label: 0, bag_size: 2959\n",
      "batch 799, loss: 0.0372, instance_loss: 0.0004, weighted_loss: 0.0262, label: 0, bag_size: 14625\n",
      "batch 819, loss: 0.0737, instance_loss: 0.0335, weighted_loss: 0.0617, label: 1, bag_size: 9230\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9799270072992701: correct 12888/13152\n",
      "class 1 clustering acc 0.9070863746958637: correct 5965/6576\n",
      "Epoch: 35, train_loss: 0.2227, train_clustering_loss:  0.2036, train_error: 0.0791\n",
      "class 0: acc 0.910411622276029, correct 376/413\n",
      "class 1: acc 0.9315403422982885, correct 381/409\n",
      "\n",
      "Val Set, val_loss: 0.1640, val_error: 0.0761, auc: 0.9946\n",
      "class 0 clustering acc 0.967391304347826: correct 1424/1472\n",
      "class 1 clustering acc 0.7894021739130435: correct 581/736\n",
      "class 0: acc 1.0, correct 38/38\n",
      "class 1: acc 0.8703703703703703, correct 47/54\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0616, instance_loss: 0.1864, weighted_loss: 0.0990, label: 1, bag_size: 10072\n",
      "batch 39, loss: 0.0756, instance_loss: 0.0027, weighted_loss: 0.0537, label: 1, bag_size: 2848\n",
      "batch 59, loss: 0.0502, instance_loss: 0.0000, weighted_loss: 0.0351, label: 0, bag_size: 3710\n",
      "batch 79, loss: 0.8581, instance_loss: 1.3605, weighted_loss: 1.0088, label: 1, bag_size: 1963\n",
      "batch 99, loss: 0.0161, instance_loss: 0.1773, weighted_loss: 0.0645, label: 1, bag_size: 7932\n",
      "batch 119, loss: 0.0257, instance_loss: 0.0035, weighted_loss: 0.0191, label: 1, bag_size: 2278\n",
      "batch 139, loss: 0.0196, instance_loss: 0.0000, weighted_loss: 0.0137, label: 0, bag_size: 2534\n",
      "batch 159, loss: 0.0175, instance_loss: 0.0016, weighted_loss: 0.0127, label: 0, bag_size: 21319\n",
      "batch 179, loss: 0.3860, instance_loss: 0.0937, weighted_loss: 0.2983, label: 0, bag_size: 9421\n",
      "batch 199, loss: 0.4175, instance_loss: 0.0178, weighted_loss: 0.2976, label: 1, bag_size: 11729\n",
      "batch 219, loss: 0.0727, instance_loss: 0.0168, weighted_loss: 0.0559, label: 1, bag_size: 11729\n",
      "batch 239, loss: 0.0635, instance_loss: 0.0579, weighted_loss: 0.0618, label: 1, bag_size: 7468\n",
      "batch 259, loss: 0.3087, instance_loss: 0.0946, weighted_loss: 0.2445, label: 1, bag_size: 3652\n",
      "batch 279, loss: 0.1394, instance_loss: 0.1784, weighted_loss: 0.1511, label: 1, bag_size: 9330\n",
      "batch 299, loss: 0.7804, instance_loss: 2.7193, weighted_loss: 1.3621, label: 0, bag_size: 1701\n",
      "batch 319, loss: 0.0111, instance_loss: 0.0137, weighted_loss: 0.0119, label: 1, bag_size: 3683\n",
      "batch 339, loss: 0.0050, instance_loss: 0.0000, weighted_loss: 0.0035, label: 1, bag_size: 14030\n",
      "batch 359, loss: 0.4609, instance_loss: 0.8111, weighted_loss: 0.5659, label: 1, bag_size: 1875\n",
      "batch 379, loss: 0.1566, instance_loss: 0.0551, weighted_loss: 0.1261, label: 0, bag_size: 6281\n",
      "batch 399, loss: 0.0146, instance_loss: 0.0007, weighted_loss: 0.0104, label: 0, bag_size: 14625\n",
      "batch 419, loss: 1.9625, instance_loss: 4.0566, weighted_loss: 2.5907, label: 1, bag_size: 6360\n",
      "batch 439, loss: 0.0002, instance_loss: 0.0286, weighted_loss: 0.0087, label: 1, bag_size: 11981\n",
      "batch 459, loss: 0.0475, instance_loss: 0.0000, weighted_loss: 0.0332, label: 0, bag_size: 25027\n",
      "batch 479, loss: 0.0141, instance_loss: 0.0086, weighted_loss: 0.0125, label: 1, bag_size: 5991\n",
      "batch 499, loss: 0.0410, instance_loss: 0.1606, weighted_loss: 0.0769, label: 0, bag_size: 2148\n",
      "batch 519, loss: 0.0082, instance_loss: 0.0000, weighted_loss: 0.0057, label: 0, bag_size: 12593\n",
      "batch 539, loss: 0.0271, instance_loss: 0.0002, weighted_loss: 0.0190, label: 0, bag_size: 13339\n",
      "batch 559, loss: 0.0515, instance_loss: 0.0699, weighted_loss: 0.0570, label: 0, bag_size: 1452\n",
      "batch 579, loss: 0.0010, instance_loss: 0.0016, weighted_loss: 0.0012, label: 1, bag_size: 5221\n",
      "batch 599, loss: 1.6290, instance_loss: 0.5206, weighted_loss: 1.2964, label: 1, bag_size: 1533\n",
      "batch 619, loss: 1.0378, instance_loss: 0.0571, weighted_loss: 0.7436, label: 0, bag_size: 15057\n",
      "batch 639, loss: 0.1609, instance_loss: 0.0274, weighted_loss: 0.1209, label: 0, bag_size: 7557\n",
      "batch 659, loss: 0.0310, instance_loss: 0.0000, weighted_loss: 0.0217, label: 1, bag_size: 16565\n",
      "batch 679, loss: 0.0059, instance_loss: 0.0026, weighted_loss: 0.0049, label: 1, bag_size: 2344\n",
      "batch 699, loss: 0.0008, instance_loss: 0.0026, weighted_loss: 0.0013, label: 1, bag_size: 1412\n",
      "batch 719, loss: 0.0106, instance_loss: 0.0001, weighted_loss: 0.0074, label: 0, bag_size: 8788\n",
      "batch 739, loss: 0.0356, instance_loss: 0.0020, weighted_loss: 0.0255, label: 1, bag_size: 11701\n",
      "batch 759, loss: 0.0627, instance_loss: 0.1226, weighted_loss: 0.0807, label: 0, bag_size: 1142\n",
      "batch 779, loss: 0.0101, instance_loss: 0.0081, weighted_loss: 0.0095, label: 0, bag_size: 2322\n",
      "batch 799, loss: 0.0168, instance_loss: 0.0017, weighted_loss: 0.0123, label: 0, bag_size: 2351\n",
      "batch 819, loss: 0.0599, instance_loss: 0.6462, weighted_loss: 0.2358, label: 0, bag_size: 4345\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9777980535279805: correct 12860/13152\n",
      "class 1 clustering acc 0.8908150851581509: correct 5858/6576\n",
      "Epoch: 36, train_loss: 0.1898, train_clustering_loss:  0.2164, train_error: 0.0657\n",
      "class 0: acc 0.9384236453201971, correct 381/406\n",
      "class 1: acc 0.9302884615384616, correct 387/416\n",
      "\n",
      "Val Set, val_loss: 0.0996, val_error: 0.0217, auc: 0.9951\n",
      "class 0 clustering acc 0.9966032608695652: correct 1467/1472\n",
      "class 1 clustering acc 0.717391304347826: correct 528/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 1.0, correct 54/54\n",
      "Validation loss decreased (0.106688 --> 0.099570).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0028, instance_loss: 0.0281, weighted_loss: 0.0104, label: 0, bag_size: 1120\n",
      "batch 39, loss: 0.0178, instance_loss: 0.0069, weighted_loss: 0.0145, label: 0, bag_size: 2998\n",
      "batch 59, loss: 0.0022, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 21874\n",
      "batch 79, loss: 0.0091, instance_loss: 0.0001, weighted_loss: 0.0064, label: 0, bag_size: 17083\n",
      "batch 99, loss: 0.3787, instance_loss: 0.6689, weighted_loss: 0.4658, label: 1, bag_size: 16514\n",
      "batch 119, loss: 0.0110, instance_loss: 0.0172, weighted_loss: 0.0129, label: 0, bag_size: 11146\n",
      "batch 139, loss: 0.0003, instance_loss: 0.1350, weighted_loss: 0.0407, label: 1, bag_size: 2485\n",
      "batch 159, loss: 0.0061, instance_loss: 0.0024, weighted_loss: 0.0050, label: 1, bag_size: 6781\n",
      "batch 179, loss: 2.3323, instance_loss: 1.1902, weighted_loss: 1.9897, label: 1, bag_size: 6360\n",
      "batch 199, loss: 0.0154, instance_loss: 0.0388, weighted_loss: 0.0224, label: 1, bag_size: 13015\n",
      "batch 219, loss: 0.0172, instance_loss: 0.0036, weighted_loss: 0.0131, label: 1, bag_size: 19832\n",
      "batch 239, loss: 0.1653, instance_loss: 0.0641, weighted_loss: 0.1349, label: 0, bag_size: 9616\n",
      "batch 259, loss: 0.0035, instance_loss: 0.0013, weighted_loss: 0.0029, label: 1, bag_size: 5317\n",
      "batch 279, loss: 0.0270, instance_loss: 0.0088, weighted_loss: 0.0216, label: 1, bag_size: 15609\n",
      "batch 299, loss: 0.0096, instance_loss: 0.0044, weighted_loss: 0.0080, label: 0, bag_size: 9930\n",
      "batch 319, loss: 0.0202, instance_loss: 0.0000, weighted_loss: 0.0141, label: 0, bag_size: 13777\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0006, weighted_loss: 0.0003, label: 0, bag_size: 16782\n",
      "batch 359, loss: 0.4808, instance_loss: 0.1660, weighted_loss: 0.3864, label: 0, bag_size: 6367\n",
      "batch 379, loss: 0.3845, instance_loss: 0.0151, weighted_loss: 0.2737, label: 0, bag_size: 18516\n",
      "batch 399, loss: 0.0038, instance_loss: 0.2900, weighted_loss: 0.0896, label: 1, bag_size: 9533\n",
      "batch 419, loss: 0.0071, instance_loss: 0.0329, weighted_loss: 0.0149, label: 0, bag_size: 9851\n",
      "batch 439, loss: 0.0463, instance_loss: 1.0410, weighted_loss: 0.3447, label: 1, bag_size: 11386\n",
      "batch 459, loss: 0.0012, instance_loss: 0.0017, weighted_loss: 0.0014, label: 1, bag_size: 6745\n",
      "batch 479, loss: 0.1341, instance_loss: 0.1440, weighted_loss: 0.1371, label: 1, bag_size: 2681\n",
      "batch 499, loss: 0.0392, instance_loss: 0.1695, weighted_loss: 0.0783, label: 0, bag_size: 3893\n",
      "batch 519, loss: 0.0017, instance_loss: 0.0003, weighted_loss: 0.0013, label: 0, bag_size: 16720\n",
      "batch 539, loss: 0.0341, instance_loss: 0.5204, weighted_loss: 0.1800, label: 0, bag_size: 1052\n",
      "batch 559, loss: 0.0004, instance_loss: 0.0086, weighted_loss: 0.0028, label: 1, bag_size: 5833\n",
      "batch 579, loss: 0.0097, instance_loss: 0.0043, weighted_loss: 0.0081, label: 0, bag_size: 25027\n",
      "batch 599, loss: 0.0317, instance_loss: 0.0981, weighted_loss: 0.0516, label: 1, bag_size: 10912\n",
      "batch 619, loss: 0.1660, instance_loss: 0.1125, weighted_loss: 0.1500, label: 1, bag_size: 4939\n",
      "batch 639, loss: 0.0031, instance_loss: 0.0000, weighted_loss: 0.0022, label: 0, bag_size: 17919\n",
      "batch 659, loss: 0.0006, instance_loss: 0.0211, weighted_loss: 0.0067, label: 0, bag_size: 7709\n",
      "batch 679, loss: 0.0039, instance_loss: 0.0270, weighted_loss: 0.0109, label: 0, bag_size: 2490\n",
      "batch 699, loss: 0.0093, instance_loss: 0.0161, weighted_loss: 0.0113, label: 0, bag_size: 23791\n",
      "batch 719, loss: 0.0137, instance_loss: 0.0024, weighted_loss: 0.0103, label: 0, bag_size: 13777\n",
      "batch 739, loss: 0.0049, instance_loss: 0.5149, weighted_loss: 0.1579, label: 0, bag_size: 1560\n",
      "batch 759, loss: 0.0129, instance_loss: 0.0066, weighted_loss: 0.0110, label: 0, bag_size: 12687\n",
      "batch 779, loss: 0.0562, instance_loss: 0.0021, weighted_loss: 0.0400, label: 1, bag_size: 11220\n",
      "batch 799, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 8812\n",
      "batch 819, loss: 0.0097, instance_loss: 0.0000, weighted_loss: 0.0068, label: 0, bag_size: 12593\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9779501216545012: correct 12862/13152\n",
      "class 1 clustering acc 0.879257907542579: correct 5782/6576\n",
      "Epoch: 37, train_loss: 0.2098, train_clustering_loss:  0.2115, train_error: 0.0669\n",
      "class 0: acc 0.9280397022332506, correct 374/403\n",
      "class 1: acc 0.9379474940334129, correct 393/419\n",
      "\n",
      "Val Set, val_loss: 0.1093, val_error: 0.0435, auc: 0.9932\n",
      "class 0 clustering acc 0.9823369565217391: correct 1446/1472\n",
      "class 1 clustering acc 0.9157608695652174: correct 674/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0008, instance_loss: 0.0003, weighted_loss: 0.0006, label: 1, bag_size: 9673\n",
      "batch 39, loss: 0.0138, instance_loss: 0.3760, weighted_loss: 0.1225, label: 1, bag_size: 2785\n",
      "batch 59, loss: 1.6631, instance_loss: 0.4456, weighted_loss: 1.2979, label: 1, bag_size: 1683\n",
      "batch 79, loss: 0.4873, instance_loss: 0.1110, weighted_loss: 0.3744, label: 0, bag_size: 15057\n",
      "batch 99, loss: 0.7243, instance_loss: 0.0199, weighted_loss: 0.5130, label: 0, bag_size: 23618\n",
      "batch 119, loss: 0.1673, instance_loss: 0.1633, weighted_loss: 0.1661, label: 0, bag_size: 5161\n",
      "batch 139, loss: 0.0900, instance_loss: 0.0203, weighted_loss: 0.0691, label: 0, bag_size: 12899\n",
      "batch 159, loss: 0.1871, instance_loss: 0.6816, weighted_loss: 0.3355, label: 0, bag_size: 2490\n",
      "batch 179, loss: 0.3356, instance_loss: 0.0163, weighted_loss: 0.2398, label: 0, bag_size: 6356\n",
      "batch 199, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 14515\n",
      "batch 219, loss: 0.0586, instance_loss: 0.0298, weighted_loss: 0.0500, label: 1, bag_size: 5690\n",
      "batch 239, loss: 0.0002, instance_loss: 0.0017, weighted_loss: 0.0007, label: 0, bag_size: 11512\n",
      "batch 259, loss: 0.1043, instance_loss: 0.0304, weighted_loss: 0.0821, label: 1, bag_size: 10848\n",
      "batch 279, loss: 0.0592, instance_loss: 0.0067, weighted_loss: 0.0435, label: 1, bag_size: 4039\n",
      "batch 299, loss: 0.2387, instance_loss: 0.0174, weighted_loss: 0.1723, label: 0, bag_size: 2959\n",
      "batch 319, loss: 0.0041, instance_loss: 0.0004, weighted_loss: 0.0030, label: 1, bag_size: 5317\n",
      "batch 339, loss: 0.1286, instance_loss: 0.0459, weighted_loss: 0.1038, label: 0, bag_size: 11194\n",
      "batch 359, loss: 0.0374, instance_loss: 0.0072, weighted_loss: 0.0283, label: 0, bag_size: 18215\n",
      "batch 379, loss: 0.0113, instance_loss: 0.0049, weighted_loss: 0.0094, label: 1, bag_size: 3937\n",
      "batch 399, loss: 0.0142, instance_loss: 0.0077, weighted_loss: 0.0123, label: 1, bag_size: 21827\n",
      "batch 419, loss: 0.0467, instance_loss: 1.3306, weighted_loss: 0.4319, label: 1, bag_size: 7066\n",
      "batch 439, loss: 0.0015, instance_loss: 0.0047, weighted_loss: 0.0025, label: 1, bag_size: 1781\n",
      "batch 459, loss: 0.0108, instance_loss: 0.0069, weighted_loss: 0.0096, label: 0, bag_size: 19518\n",
      "batch 479, loss: 0.0150, instance_loss: 0.0044, weighted_loss: 0.0118, label: 1, bag_size: 16379\n",
      "batch 499, loss: 0.6946, instance_loss: 0.3868, weighted_loss: 0.6023, label: 1, bag_size: 1236\n",
      "batch 519, loss: 0.0134, instance_loss: 0.0016, weighted_loss: 0.0098, label: 1, bag_size: 12127\n",
      "batch 539, loss: 0.0105, instance_loss: 0.0042, weighted_loss: 0.0086, label: 1, bag_size: 7873\n",
      "batch 559, loss: 0.0016, instance_loss: 0.0279, weighted_loss: 0.0095, label: 1, bag_size: 6950\n",
      "batch 579, loss: 0.0179, instance_loss: 0.0344, weighted_loss: 0.0228, label: 0, bag_size: 14333\n",
      "batch 599, loss: 0.7785, instance_loss: 0.3171, weighted_loss: 0.6401, label: 0, bag_size: 21361\n",
      "batch 619, loss: 0.1094, instance_loss: 0.5360, weighted_loss: 0.2373, label: 0, bag_size: 6727\n",
      "batch 639, loss: 0.6239, instance_loss: 0.2750, weighted_loss: 0.5192, label: 0, bag_size: 2269\n",
      "batch 659, loss: 0.0021, instance_loss: 0.0871, weighted_loss: 0.0276, label: 1, bag_size: 12349\n",
      "batch 679, loss: 1.8999, instance_loss: 0.0233, weighted_loss: 1.3369, label: 1, bag_size: 1819\n",
      "batch 699, loss: 0.7279, instance_loss: 0.2214, weighted_loss: 0.5759, label: 1, bag_size: 2842\n",
      "batch 719, loss: 0.0139, instance_loss: 0.0002, weighted_loss: 0.0098, label: 0, bag_size: 2873\n",
      "batch 739, loss: 0.0091, instance_loss: 0.0000, weighted_loss: 0.0063, label: 1, bag_size: 12095\n",
      "batch 759, loss: 0.1282, instance_loss: 0.0032, weighted_loss: 0.0907, label: 1, bag_size: 16565\n",
      "batch 779, loss: 0.0143, instance_loss: 0.5651, weighted_loss: 0.1795, label: 0, bag_size: 8330\n",
      "batch 799, loss: 0.0024, instance_loss: 0.0000, weighted_loss: 0.0017, label: 0, bag_size: 9455\n",
      "batch 819, loss: 0.0103, instance_loss: 0.0000, weighted_loss: 0.0072, label: 0, bag_size: 17083\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9804592457420924: correct 12895/13152\n",
      "class 1 clustering acc 0.8874695863746959: correct 5836/6576\n",
      "Epoch: 38, train_loss: 0.1942, train_clustering_loss:  0.1948, train_error: 0.0706\n",
      "class 0: acc 0.9240196078431373, correct 377/408\n",
      "class 1: acc 0.9347826086956522, correct 387/414\n",
      "\n",
      "Val Set, val_loss: 0.2252, val_error: 0.0978, auc: 0.9942\n",
      "class 0 clustering acc 0.9762228260869565: correct 1437/1472\n",
      "class 1 clustering acc 0.8614130434782609: correct 634/736\n",
      "class 0: acc 1.0, correct 38/38\n",
      "class 1: acc 0.8333333333333334, correct 45/54\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0117, instance_loss: 0.0038, weighted_loss: 0.0093, label: 1, bag_size: 6090\n",
      "batch 39, loss: 0.0035, instance_loss: 0.0132, weighted_loss: 0.0064, label: 1, bag_size: 617\n",
      "batch 59, loss: 0.0314, instance_loss: 0.1778, weighted_loss: 0.0753, label: 0, bag_size: 10415\n",
      "batch 79, loss: 0.0501, instance_loss: 0.3517, weighted_loss: 0.1406, label: 0, bag_size: 1458\n",
      "batch 99, loss: 0.1680, instance_loss: 0.8485, weighted_loss: 0.3722, label: 0, bag_size: 4523\n",
      "batch 119, loss: 0.0056, instance_loss: 0.1077, weighted_loss: 0.0362, label: 1, bag_size: 9533\n",
      "batch 139, loss: 0.0069, instance_loss: 0.0008, weighted_loss: 0.0050, label: 1, bag_size: 1022\n",
      "batch 159, loss: 0.0176, instance_loss: 0.0401, weighted_loss: 0.0243, label: 1, bag_size: 9983\n",
      "batch 179, loss: 0.0029, instance_loss: 0.1252, weighted_loss: 0.0396, label: 1, bag_size: 19832\n",
      "batch 199, loss: 0.0799, instance_loss: 0.1337, weighted_loss: 0.0960, label: 1, bag_size: 7351\n",
      "batch 219, loss: 1.2047, instance_loss: 1.4929, weighted_loss: 1.2912, label: 1, bag_size: 15563\n",
      "batch 239, loss: 0.1206, instance_loss: 0.6047, weighted_loss: 0.2658, label: 0, bag_size: 2104\n",
      "batch 259, loss: 0.0076, instance_loss: 0.0000, weighted_loss: 0.0053, label: 1, bag_size: 6453\n",
      "batch 279, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 15008\n",
      "batch 299, loss: 0.0137, instance_loss: 0.0000, weighted_loss: 0.0096, label: 1, bag_size: 2092\n",
      "batch 319, loss: 0.0053, instance_loss: 0.0000, weighted_loss: 0.0037, label: 0, bag_size: 8898\n",
      "batch 339, loss: 0.0300, instance_loss: 0.0129, weighted_loss: 0.0249, label: 1, bag_size: 8216\n",
      "batch 359, loss: 0.0408, instance_loss: 0.0000, weighted_loss: 0.0286, label: 1, bag_size: 15931\n",
      "batch 379, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 22264\n",
      "batch 399, loss: 0.1248, instance_loss: 0.1024, weighted_loss: 0.1181, label: 0, bag_size: 9596\n",
      "batch 419, loss: 0.1433, instance_loss: 0.0000, weighted_loss: 0.1003, label: 1, bag_size: 10498\n",
      "batch 439, loss: 0.0284, instance_loss: 0.0000, weighted_loss: 0.0199, label: 1, bag_size: 6205\n",
      "batch 459, loss: 0.0017, instance_loss: 0.0169, weighted_loss: 0.0062, label: 1, bag_size: 1316\n",
      "batch 479, loss: 0.1771, instance_loss: 0.8239, weighted_loss: 0.3711, label: 0, bag_size: 9132\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 18574\n",
      "batch 519, loss: 0.0240, instance_loss: 0.0174, weighted_loss: 0.0220, label: 1, bag_size: 7932\n",
      "batch 539, loss: 0.0039, instance_loss: 0.0011, weighted_loss: 0.0030, label: 0, bag_size: 17368\n",
      "batch 559, loss: 0.2661, instance_loss: 0.2118, weighted_loss: 0.2498, label: 1, bag_size: 1095\n",
      "batch 579, loss: 2.3932, instance_loss: 3.7916, weighted_loss: 2.8127, label: 0, bag_size: 1714\n",
      "batch 599, loss: 0.0107, instance_loss: 0.0008, weighted_loss: 0.0077, label: 1, bag_size: 7119\n",
      "batch 619, loss: 0.0256, instance_loss: 0.0048, weighted_loss: 0.0193, label: 0, bag_size: 14333\n",
      "batch 639, loss: 0.0076, instance_loss: 0.1341, weighted_loss: 0.0455, label: 0, bag_size: 9596\n",
      "batch 659, loss: 0.4109, instance_loss: 0.0100, weighted_loss: 0.2906, label: 1, bag_size: 1764\n",
      "batch 679, loss: 0.0650, instance_loss: 0.0000, weighted_loss: 0.0455, label: 1, bag_size: 13026\n",
      "batch 699, loss: 0.0221, instance_loss: 0.0056, weighted_loss: 0.0171, label: 1, bag_size: 8216\n",
      "batch 719, loss: 0.0046, instance_loss: 0.0022, weighted_loss: 0.0039, label: 1, bag_size: 1838\n",
      "batch 739, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 10725\n",
      "batch 759, loss: 0.9264, instance_loss: 0.6124, weighted_loss: 0.8322, label: 0, bag_size: 4241\n",
      "batch 779, loss: 0.1112, instance_loss: 0.0345, weighted_loss: 0.0882, label: 1, bag_size: 4786\n",
      "batch 799, loss: 0.0092, instance_loss: 0.0000, weighted_loss: 0.0064, label: 1, bag_size: 9062\n",
      "batch 819, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 11865\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9774939172749392: correct 12856/13152\n",
      "class 1 clustering acc 0.8847323600973236: correct 5818/6576\n",
      "Epoch: 39, train_loss: 0.2087, train_clustering_loss:  0.2358, train_error: 0.0730\n",
      "class 0: acc 0.9226804123711341, correct 358/388\n",
      "class 1: acc 0.9308755760368663, correct 404/434\n",
      "\n",
      "Val Set, val_loss: 0.1485, val_error: 0.0652, auc: 0.9942\n",
      "class 0 clustering acc 0.9667119565217391: correct 1423/1472\n",
      "class 1 clustering acc 0.8464673913043478: correct 623/736\n",
      "class 0: acc 1.0, correct 38/38\n",
      "class 1: acc 0.8888888888888888, correct 48/54\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 19470\n",
      "batch 39, loss: 0.2275, instance_loss: 0.1342, weighted_loss: 0.1995, label: 1, bag_size: 10622\n",
      "batch 59, loss: 0.0178, instance_loss: 0.0018, weighted_loss: 0.0130, label: 1, bag_size: 5317\n",
      "batch 79, loss: 0.0539, instance_loss: 0.2753, weighted_loss: 0.1203, label: 0, bag_size: 1614\n",
      "batch 99, loss: 0.0038, instance_loss: 0.0108, weighted_loss: 0.0059, label: 1, bag_size: 10105\n",
      "batch 119, loss: 0.0439, instance_loss: 0.1423, weighted_loss: 0.0734, label: 1, bag_size: 2495\n",
      "batch 139, loss: 0.0110, instance_loss: 0.0022, weighted_loss: 0.0083, label: 1, bag_size: 1838\n",
      "batch 159, loss: 1.2473, instance_loss: 0.6661, weighted_loss: 1.0729, label: 0, bag_size: 10063\n",
      "batch 179, loss: 3.9724, instance_loss: 0.3067, weighted_loss: 2.8727, label: 1, bag_size: 19470\n",
      "batch 199, loss: 0.0020, instance_loss: 0.0032, weighted_loss: 0.0024, label: 1, bag_size: 6453\n",
      "batch 219, loss: 0.5251, instance_loss: 0.6164, weighted_loss: 0.5525, label: 0, bag_size: 20478\n",
      "batch 239, loss: 0.0135, instance_loss: 0.0001, weighted_loss: 0.0095, label: 0, bag_size: 1884\n",
      "batch 259, loss: 0.0141, instance_loss: 0.0001, weighted_loss: 0.0099, label: 1, bag_size: 8602\n",
      "batch 279, loss: 0.0212, instance_loss: 0.0000, weighted_loss: 0.0148, label: 0, bag_size: 5225\n",
      "batch 299, loss: 0.0106, instance_loss: 0.0000, weighted_loss: 0.0074, label: 1, bag_size: 6745\n",
      "batch 319, loss: 0.0479, instance_loss: 0.0000, weighted_loss: 0.0335, label: 0, bag_size: 6898\n",
      "batch 339, loss: 1.7224, instance_loss: 0.0039, weighted_loss: 1.2069, label: 0, bag_size: 5211\n",
      "batch 359, loss: 0.0567, instance_loss: 0.2127, weighted_loss: 0.1035, label: 0, bag_size: 1052\n",
      "batch 379, loss: 0.0337, instance_loss: 0.1330, weighted_loss: 0.0635, label: 1, bag_size: 7669\n",
      "batch 399, loss: 0.0034, instance_loss: 0.0000, weighted_loss: 0.0024, label: 0, bag_size: 24911\n",
      "batch 419, loss: 0.0044, instance_loss: 0.0139, weighted_loss: 0.0073, label: 1, bag_size: 12460\n",
      "batch 439, loss: 0.0039, instance_loss: 0.0502, weighted_loss: 0.0178, label: 0, bag_size: 1712\n",
      "batch 459, loss: 0.0148, instance_loss: 0.0276, weighted_loss: 0.0186, label: 1, bag_size: 2193\n",
      "batch 479, loss: 0.0276, instance_loss: 0.0008, weighted_loss: 0.0196, label: 1, bag_size: 12575\n",
      "batch 499, loss: 0.3136, instance_loss: 0.0446, weighted_loss: 0.2329, label: 0, bag_size: 11212\n",
      "batch 519, loss: 0.0349, instance_loss: 0.0040, weighted_loss: 0.0256, label: 0, bag_size: 15071\n",
      "batch 539, loss: 0.0078, instance_loss: 0.3212, weighted_loss: 0.1018, label: 1, bag_size: 1255\n",
      "batch 559, loss: 0.4509, instance_loss: 0.3176, weighted_loss: 0.4109, label: 1, bag_size: 1845\n",
      "batch 579, loss: 0.0163, instance_loss: 0.2287, weighted_loss: 0.0800, label: 0, bag_size: 19466\n",
      "batch 599, loss: 0.0026, instance_loss: 0.0000, weighted_loss: 0.0018, label: 0, bag_size: 12524\n",
      "batch 619, loss: 0.1586, instance_loss: 0.0327, weighted_loss: 0.1208, label: 1, bag_size: 5292\n",
      "batch 639, loss: 0.1837, instance_loss: 0.0027, weighted_loss: 0.1294, label: 1, bag_size: 1483\n",
      "batch 659, loss: 0.0445, instance_loss: 0.0012, weighted_loss: 0.0315, label: 1, bag_size: 10432\n",
      "batch 679, loss: 0.0030, instance_loss: 0.0385, weighted_loss: 0.0136, label: 1, bag_size: 13194\n",
      "batch 699, loss: 0.1467, instance_loss: 0.0695, weighted_loss: 0.1235, label: 0, bag_size: 9597\n",
      "batch 719, loss: 0.0779, instance_loss: 0.4743, weighted_loss: 0.1968, label: 0, bag_size: 1052\n",
      "batch 739, loss: 0.2355, instance_loss: 0.0569, weighted_loss: 0.1819, label: 1, bag_size: 16548\n",
      "batch 759, loss: 0.0020, instance_loss: 0.0000, weighted_loss: 0.0014, label: 0, bag_size: 11917\n",
      "batch 779, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 10535\n",
      "batch 799, loss: 0.1402, instance_loss: 0.0108, weighted_loss: 0.1014, label: 1, bag_size: 5411\n",
      "batch 819, loss: 0.1665, instance_loss: 0.0108, weighted_loss: 0.1198, label: 0, bag_size: 2006\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9820559610705596: correct 12916/13152\n",
      "class 1 clustering acc 0.9122566909975669: correct 5999/6576\n",
      "Epoch: 40, train_loss: 0.2028, train_clustering_loss:  0.1771, train_error: 0.0620\n",
      "class 0: acc 0.9452380952380952, correct 397/420\n",
      "class 1: acc 0.9303482587064676, correct 374/402\n",
      "\n",
      "Val Set, val_loss: 0.0912, val_error: 0.0217, auc: 0.9966\n",
      "class 0 clustering acc 0.9884510869565217: correct 1455/1472\n",
      "class 1 clustering acc 0.9402173913043478: correct 692/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 1.0, correct 54/54\n",
      "Validation loss decreased (0.099570 --> 0.091167).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0017, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 22800\n",
      "batch 39, loss: 0.0164, instance_loss: 0.0026, weighted_loss: 0.0123, label: 0, bag_size: 2063\n",
      "batch 59, loss: 0.0005, instance_loss: 0.0244, weighted_loss: 0.0076, label: 1, bag_size: 22264\n",
      "batch 79, loss: 0.0013, instance_loss: 0.0731, weighted_loss: 0.0229, label: 1, bag_size: 2405\n",
      "batch 99, loss: 0.0080, instance_loss: 0.0000, weighted_loss: 0.0056, label: 0, bag_size: 3710\n",
      "batch 119, loss: 1.7972, instance_loss: 1.1592, weighted_loss: 1.6058, label: 1, bag_size: 1497\n",
      "batch 139, loss: 0.0023, instance_loss: 0.0932, weighted_loss: 0.0296, label: 1, bag_size: 2344\n",
      "batch 159, loss: 0.0450, instance_loss: 0.1818, weighted_loss: 0.0861, label: 0, bag_size: 2367\n",
      "batch 179, loss: 0.0235, instance_loss: 0.0004, weighted_loss: 0.0166, label: 0, bag_size: 8582\n",
      "batch 199, loss: 0.0038, instance_loss: 0.0134, weighted_loss: 0.0067, label: 1, bag_size: 12719\n",
      "batch 219, loss: 0.0003, instance_loss: 0.0084, weighted_loss: 0.0028, label: 1, bag_size: 8410\n",
      "batch 239, loss: 0.0093, instance_loss: 0.0033, weighted_loss: 0.0075, label: 1, bag_size: 14433\n",
      "batch 259, loss: 0.0363, instance_loss: 0.0171, weighted_loss: 0.0305, label: 1, bag_size: 9062\n",
      "batch 279, loss: 0.0380, instance_loss: 1.0865, weighted_loss: 0.3525, label: 0, bag_size: 16690\n",
      "batch 299, loss: 0.5345, instance_loss: 0.3009, weighted_loss: 0.4644, label: 1, bag_size: 5292\n",
      "batch 319, loss: 0.0069, instance_loss: 0.0151, weighted_loss: 0.0094, label: 1, bag_size: 10558\n",
      "batch 339, loss: 0.0052, instance_loss: 0.0000, weighted_loss: 0.0036, label: 0, bag_size: 16052\n",
      "batch 359, loss: 0.2107, instance_loss: 0.8136, weighted_loss: 0.3916, label: 0, bag_size: 13602\n",
      "batch 379, loss: 0.0402, instance_loss: 0.0303, weighted_loss: 0.0373, label: 0, bag_size: 12793\n",
      "batch 399, loss: 0.4115, instance_loss: 0.0634, weighted_loss: 0.3071, label: 0, bag_size: 2467\n",
      "batch 419, loss: 0.2154, instance_loss: 0.2778, weighted_loss: 0.2342, label: 1, bag_size: 2904\n",
      "batch 439, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 13218\n",
      "batch 459, loss: 0.1282, instance_loss: 0.0054, weighted_loss: 0.0914, label: 1, bag_size: 9561\n",
      "batch 479, loss: 0.0418, instance_loss: 0.2317, weighted_loss: 0.0988, label: 0, bag_size: 931\n",
      "batch 499, loss: 0.0025, instance_loss: 0.0012, weighted_loss: 0.0021, label: 1, bag_size: 5612\n",
      "batch 519, loss: 0.0005, instance_loss: 0.0290, weighted_loss: 0.0091, label: 1, bag_size: 5612\n",
      "batch 539, loss: 0.1263, instance_loss: 0.1205, weighted_loss: 0.1246, label: 1, bag_size: 8438\n",
      "batch 559, loss: 0.0374, instance_loss: 0.0388, weighted_loss: 0.0378, label: 1, bag_size: 13732\n",
      "batch 579, loss: 0.0035, instance_loss: 0.0049, weighted_loss: 0.0039, label: 0, bag_size: 16720\n",
      "batch 599, loss: 0.0622, instance_loss: 0.0125, weighted_loss: 0.0473, label: 0, bag_size: 3502\n",
      "batch 619, loss: 0.0006, instance_loss: 0.0299, weighted_loss: 0.0093, label: 1, bag_size: 6792\n",
      "batch 639, loss: 2.1211, instance_loss: 1.8462, weighted_loss: 2.0386, label: 1, bag_size: 1191\n",
      "batch 659, loss: 0.0555, instance_loss: 0.0378, weighted_loss: 0.0502, label: 1, bag_size: 9955\n",
      "batch 679, loss: 0.0233, instance_loss: 0.0031, weighted_loss: 0.0172, label: 1, bag_size: 3651\n",
      "batch 699, loss: 0.3859, instance_loss: 0.3708, weighted_loss: 0.3814, label: 0, bag_size: 1127\n",
      "batch 719, loss: 0.7319, instance_loss: 2.4317, weighted_loss: 1.2419, label: 0, bag_size: 2070\n",
      "batch 739, loss: 0.0981, instance_loss: 0.0097, weighted_loss: 0.0716, label: 0, bag_size: 10490\n",
      "batch 759, loss: 0.0110, instance_loss: 0.0058, weighted_loss: 0.0094, label: 1, bag_size: 7371\n",
      "batch 779, loss: 0.3037, instance_loss: 0.0006, weighted_loss: 0.2128, label: 1, bag_size: 18699\n",
      "batch 799, loss: 0.0076, instance_loss: 0.0074, weighted_loss: 0.0075, label: 0, bag_size: 19466\n",
      "batch 819, loss: 0.3945, instance_loss: 0.1743, weighted_loss: 0.3285, label: 1, bag_size: 15192\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9779501216545012: correct 12862/13152\n",
      "class 1 clustering acc 0.8914233576642335: correct 5862/6576\n",
      "Epoch: 41, train_loss: 0.2289, train_clustering_loss:  0.2123, train_error: 0.0839\n",
      "class 0: acc 0.9108433734939759, correct 378/415\n",
      "class 1: acc 0.9213759213759214, correct 375/407\n",
      "\n",
      "Val Set, val_loss: 0.1123, val_error: 0.0435, auc: 0.9976\n",
      "class 0 clustering acc 0.985733695652174: correct 1451/1472\n",
      "class 1 clustering acc 0.9307065217391305: correct 685/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0134, instance_loss: 0.0002, weighted_loss: 0.0094, label: 0, bag_size: 10304\n",
      "batch 39, loss: 1.2551, instance_loss: 0.3888, weighted_loss: 0.9952, label: 1, bag_size: 3121\n",
      "batch 59, loss: 0.0934, instance_loss: 0.3963, weighted_loss: 0.1842, label: 0, bag_size: 2367\n",
      "batch 79, loss: 0.1348, instance_loss: 0.3010, weighted_loss: 0.1847, label: 1, bag_size: 8191\n",
      "batch 99, loss: 0.0178, instance_loss: 0.3763, weighted_loss: 0.1254, label: 0, bag_size: 16690\n",
      "batch 119, loss: 0.1777, instance_loss: 0.0012, weighted_loss: 0.1248, label: 0, bag_size: 2160\n",
      "batch 139, loss: 0.0527, instance_loss: 0.0000, weighted_loss: 0.0369, label: 0, bag_size: 3474\n",
      "batch 159, loss: 0.0889, instance_loss: 0.0114, weighted_loss: 0.0657, label: 1, bag_size: 1786\n",
      "batch 179, loss: 0.0066, instance_loss: 0.0476, weighted_loss: 0.0189, label: 1, bag_size: 2904\n",
      "batch 199, loss: 0.0076, instance_loss: 0.0053, weighted_loss: 0.0069, label: 1, bag_size: 11266\n",
      "batch 219, loss: 1.0500, instance_loss: 0.6118, weighted_loss: 0.9185, label: 0, bag_size: 8744\n",
      "batch 239, loss: 0.0471, instance_loss: 0.0786, weighted_loss: 0.0565, label: 0, bag_size: 1370\n",
      "batch 259, loss: 0.0458, instance_loss: 0.0013, weighted_loss: 0.0325, label: 1, bag_size: 9913\n",
      "batch 279, loss: 0.0046, instance_loss: 0.0131, weighted_loss: 0.0072, label: 1, bag_size: 7381\n",
      "batch 299, loss: 0.9970, instance_loss: 0.0031, weighted_loss: 0.6988, label: 0, bag_size: 15057\n",
      "batch 319, loss: 0.0088, instance_loss: 0.0380, weighted_loss: 0.0176, label: 1, bag_size: 11684\n",
      "batch 339, loss: 0.0207, instance_loss: 0.0768, weighted_loss: 0.0376, label: 1, bag_size: 1746\n",
      "batch 359, loss: 0.3565, instance_loss: 0.0226, weighted_loss: 0.2564, label: 1, bag_size: 13362\n",
      "batch 379, loss: 0.0019, instance_loss: 0.0031, weighted_loss: 0.0023, label: 1, bag_size: 15093\n",
      "batch 399, loss: 0.0618, instance_loss: 0.0162, weighted_loss: 0.0481, label: 1, bag_size: 9649\n",
      "batch 419, loss: 0.0266, instance_loss: 0.0101, weighted_loss: 0.0216, label: 0, bag_size: 9786\n",
      "batch 439, loss: 0.0484, instance_loss: 0.0586, weighted_loss: 0.0515, label: 1, bag_size: 16154\n",
      "batch 459, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 21576\n",
      "batch 479, loss: 0.0609, instance_loss: 0.7381, weighted_loss: 0.2640, label: 0, bag_size: 3444\n",
      "batch 499, loss: 0.0576, instance_loss: 0.0233, weighted_loss: 0.0473, label: 0, bag_size: 17482\n",
      "batch 519, loss: 0.1367, instance_loss: 0.0061, weighted_loss: 0.0975, label: 0, bag_size: 21361\n",
      "batch 539, loss: 0.0093, instance_loss: 0.0004, weighted_loss: 0.0066, label: 1, bag_size: 7119\n",
      "batch 559, loss: 1.9110, instance_loss: 0.4649, weighted_loss: 1.4772, label: 1, bag_size: 1845\n",
      "batch 579, loss: 0.4920, instance_loss: 0.0638, weighted_loss: 0.3635, label: 0, bag_size: 3399\n",
      "batch 599, loss: 0.0007, instance_loss: 0.0038, weighted_loss: 0.0017, label: 0, bag_size: 15636\n",
      "batch 619, loss: 0.0665, instance_loss: 0.0096, weighted_loss: 0.0494, label: 1, bag_size: 5907\n",
      "batch 639, loss: 0.0088, instance_loss: 0.0073, weighted_loss: 0.0084, label: 1, bag_size: 2381\n",
      "batch 659, loss: 0.0019, instance_loss: 0.0002, weighted_loss: 0.0014, label: 0, bag_size: 10068\n",
      "batch 679, loss: 0.0220, instance_loss: 0.0015, weighted_loss: 0.0158, label: 0, bag_size: 6898\n",
      "batch 699, loss: 0.0024, instance_loss: 0.0046, weighted_loss: 0.0030, label: 1, bag_size: 16512\n",
      "batch 719, loss: 0.0378, instance_loss: 0.0000, weighted_loss: 0.0265, label: 1, bag_size: 19500\n",
      "batch 739, loss: 0.0972, instance_loss: 0.3085, weighted_loss: 0.1606, label: 1, bag_size: 5366\n",
      "batch 759, loss: 0.0396, instance_loss: 0.0166, weighted_loss: 0.0327, label: 1, bag_size: 6752\n",
      "batch 779, loss: 0.0272, instance_loss: 0.0235, weighted_loss: 0.0261, label: 0, bag_size: 2036\n",
      "batch 799, loss: 0.1243, instance_loss: 0.0016, weighted_loss: 0.0875, label: 1, bag_size: 9548\n",
      "batch 819, loss: 0.0341, instance_loss: 0.0064, weighted_loss: 0.0258, label: 0, bag_size: 1884\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9815997566909975: correct 12910/13152\n",
      "class 1 clustering acc 0.9124087591240876: correct 6000/6576\n",
      "Epoch: 42, train_loss: 0.2134, train_clustering_loss:  0.1685, train_error: 0.0864\n",
      "class 0: acc 0.9148418491484185, correct 376/411\n",
      "class 1: acc 0.9124087591240876, correct 375/411\n",
      "\n",
      "Val Set, val_loss: 0.1032, val_error: 0.0217, auc: 0.9976\n",
      "class 0 clustering acc 0.983016304347826: correct 1447/1472\n",
      "class 1 clustering acc 0.9035326086956522: correct 665/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 1.0, correct 54/54\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0056, instance_loss: 0.0007, weighted_loss: 0.0041, label: 1, bag_size: 7119\n",
      "batch 39, loss: 0.1041, instance_loss: 0.0041, weighted_loss: 0.0741, label: 1, bag_size: 4929\n",
      "batch 59, loss: 0.0113, instance_loss: 0.0001, weighted_loss: 0.0080, label: 1, bag_size: 12895\n",
      "batch 79, loss: 0.0355, instance_loss: 0.0976, weighted_loss: 0.0541, label: 0, bag_size: 5999\n",
      "batch 99, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 21576\n",
      "batch 119, loss: 0.1585, instance_loss: 0.0020, weighted_loss: 0.1116, label: 1, bag_size: 5516\n",
      "batch 139, loss: 0.0351, instance_loss: 0.0037, weighted_loss: 0.0257, label: 1, bag_size: 16379\n",
      "batch 159, loss: 0.0073, instance_loss: 0.0076, weighted_loss: 0.0074, label: 1, bag_size: 11032\n",
      "batch 179, loss: 0.1471, instance_loss: 0.0068, weighted_loss: 0.1050, label: 0, bag_size: 1684\n",
      "batch 199, loss: 0.0029, instance_loss: 0.0636, weighted_loss: 0.0211, label: 1, bag_size: 6269\n",
      "batch 219, loss: 0.0035, instance_loss: 0.0015, weighted_loss: 0.0029, label: 0, bag_size: 3265\n",
      "batch 239, loss: 0.0186, instance_loss: 0.0542, weighted_loss: 0.0293, label: 0, bag_size: 4902\n",
      "batch 259, loss: 0.0530, instance_loss: 0.1251, weighted_loss: 0.0746, label: 1, bag_size: 6825\n",
      "batch 279, loss: 0.0058, instance_loss: 0.0012, weighted_loss: 0.0044, label: 1, bag_size: 19500\n",
      "batch 299, loss: 0.0076, instance_loss: 0.0581, weighted_loss: 0.0228, label: 0, bag_size: 10304\n",
      "batch 319, loss: 0.0609, instance_loss: 0.0000, weighted_loss: 0.0426, label: 1, bag_size: 5256\n",
      "batch 339, loss: 0.0445, instance_loss: 0.1574, weighted_loss: 0.0783, label: 1, bag_size: 1294\n",
      "batch 359, loss: 2.2134, instance_loss: 0.0154, weighted_loss: 1.5540, label: 0, bag_size: 5120\n",
      "batch 379, loss: 0.4797, instance_loss: 2.7255, weighted_loss: 1.1534, label: 1, bag_size: 1867\n",
      "batch 399, loss: 0.0308, instance_loss: 0.0037, weighted_loss: 0.0226, label: 0, bag_size: 8959\n",
      "batch 419, loss: 0.0017, instance_loss: 0.0092, weighted_loss: 0.0039, label: 1, bag_size: 13194\n",
      "batch 439, loss: 0.0208, instance_loss: 0.0216, weighted_loss: 0.0211, label: 0, bag_size: 22681\n",
      "batch 459, loss: 0.2696, instance_loss: 0.5395, weighted_loss: 0.3506, label: 1, bag_size: 983\n",
      "batch 479, loss: 0.0349, instance_loss: 0.0006, weighted_loss: 0.0246, label: 0, bag_size: 7989\n",
      "batch 499, loss: 0.0069, instance_loss: 0.0003, weighted_loss: 0.0049, label: 0, bag_size: 3228\n",
      "batch 519, loss: 0.0019, instance_loss: 0.0175, weighted_loss: 0.0065, label: 1, bag_size: 7650\n",
      "batch 539, loss: 0.0184, instance_loss: 0.0006, weighted_loss: 0.0131, label: 0, bag_size: 3557\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 20666\n",
      "batch 579, loss: 0.8477, instance_loss: 0.5400, weighted_loss: 0.7554, label: 1, bag_size: 1191\n",
      "batch 599, loss: 0.0051, instance_loss: 0.0282, weighted_loss: 0.0120, label: 1, bag_size: 9470\n",
      "batch 619, loss: 0.0031, instance_loss: 0.0534, weighted_loss: 0.0182, label: 1, bag_size: 10392\n",
      "batch 639, loss: 0.0073, instance_loss: 0.0026, weighted_loss: 0.0059, label: 1, bag_size: 14202\n",
      "batch 659, loss: 0.0035, instance_loss: 0.0007, weighted_loss: 0.0026, label: 1, bag_size: 10482\n",
      "batch 679, loss: 0.6800, instance_loss: 0.1116, weighted_loss: 0.5094, label: 0, bag_size: 1953\n",
      "batch 699, loss: 0.0190, instance_loss: 0.2907, weighted_loss: 0.1005, label: 0, bag_size: 1614\n",
      "batch 719, loss: 0.0062, instance_loss: 0.0002, weighted_loss: 0.0044, label: 0, bag_size: 2652\n",
      "batch 739, loss: 0.0932, instance_loss: 0.0109, weighted_loss: 0.0685, label: 1, bag_size: 2137\n",
      "batch 759, loss: 0.3359, instance_loss: 0.4713, weighted_loss: 0.3766, label: 0, bag_size: 1800\n",
      "batch 779, loss: 1.3666, instance_loss: 0.1406, weighted_loss: 0.9988, label: 1, bag_size: 3224\n",
      "batch 799, loss: 0.0238, instance_loss: 0.0008, weighted_loss: 0.0169, label: 1, bag_size: 14230\n",
      "batch 819, loss: 0.0774, instance_loss: 0.3951, weighted_loss: 0.1727, label: 0, bag_size: 1498\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9796989051094891: correct 12885/13152\n",
      "class 1 clustering acc 0.9063260340632603: correct 5960/6576\n",
      "Epoch: 43, train_loss: 0.1833, train_clustering_loss:  0.1921, train_error: 0.0706\n",
      "class 0: acc 0.9280575539568345, correct 387/417\n",
      "class 1: acc 0.9308641975308642, correct 377/405\n",
      "\n",
      "Val Set, val_loss: 0.1559, val_error: 0.0652, auc: 0.9981\n",
      "class 0 clustering acc 0.984375: correct 1449/1472\n",
      "class 1 clustering acc 0.9307065217391305: correct 685/736\n",
      "class 0: acc 1.0, correct 38/38\n",
      "class 1: acc 0.8888888888888888, correct 48/54\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0363, instance_loss: 0.0071, weighted_loss: 0.0275, label: 0, bag_size: 1349\n",
      "batch 39, loss: 0.0029, instance_loss: 0.0084, weighted_loss: 0.0045, label: 1, bag_size: 14030\n",
      "batch 59, loss: 0.0427, instance_loss: 0.0002, weighted_loss: 0.0300, label: 0, bag_size: 17268\n",
      "batch 79, loss: 0.3538, instance_loss: 0.0000, weighted_loss: 0.2477, label: 1, bag_size: 12180\n",
      "batch 99, loss: 0.2006, instance_loss: 0.0796, weighted_loss: 0.1643, label: 0, bag_size: 17482\n",
      "batch 119, loss: 0.9424, instance_loss: 0.2934, weighted_loss: 0.7477, label: 0, bag_size: 9616\n",
      "batch 139, loss: 0.0088, instance_loss: 0.0561, weighted_loss: 0.0230, label: 0, bag_size: 1234\n",
      "batch 159, loss: 0.0009, instance_loss: 0.0164, weighted_loss: 0.0056, label: 1, bag_size: 15233\n",
      "batch 179, loss: 0.0863, instance_loss: 0.2292, weighted_loss: 0.1291, label: 0, bag_size: 5639\n",
      "batch 199, loss: 0.2388, instance_loss: 0.0125, weighted_loss: 0.1709, label: 1, bag_size: 1230\n",
      "batch 219, loss: 0.2508, instance_loss: 0.1874, weighted_loss: 0.2318, label: 0, bag_size: 10415\n",
      "batch 239, loss: 0.0223, instance_loss: 0.2095, weighted_loss: 0.0785, label: 1, bag_size: 9913\n",
      "batch 259, loss: 0.0371, instance_loss: 0.1942, weighted_loss: 0.0842, label: 0, bag_size: 1370\n",
      "batch 279, loss: 0.0038, instance_loss: 0.0092, weighted_loss: 0.0054, label: 1, bag_size: 928\n",
      "batch 299, loss: 0.1889, instance_loss: 0.0000, weighted_loss: 0.1322, label: 0, bag_size: 13602\n",
      "batch 319, loss: 0.1731, instance_loss: 0.2426, weighted_loss: 0.1940, label: 0, bag_size: 1831\n",
      "batch 339, loss: 0.0079, instance_loss: 0.0003, weighted_loss: 0.0056, label: 0, bag_size: 10365\n",
      "batch 359, loss: 0.0124, instance_loss: 0.0047, weighted_loss: 0.0101, label: 1, bag_size: 617\n",
      "batch 379, loss: 0.0514, instance_loss: 0.0194, weighted_loss: 0.0418, label: 1, bag_size: 2092\n",
      "batch 399, loss: 0.0024, instance_loss: 0.0810, weighted_loss: 0.0260, label: 1, bag_size: 5612\n",
      "batch 419, loss: 0.0028, instance_loss: 0.0156, weighted_loss: 0.0066, label: 0, bag_size: 2179\n",
      "batch 439, loss: 0.3127, instance_loss: 0.5360, weighted_loss: 0.3797, label: 1, bag_size: 771\n",
      "batch 459, loss: 0.0029, instance_loss: 0.0143, weighted_loss: 0.0063, label: 0, bag_size: 16211\n",
      "batch 479, loss: 0.0036, instance_loss: 0.0139, weighted_loss: 0.0067, label: 1, bag_size: 10725\n",
      "batch 499, loss: 0.0012, instance_loss: 0.0059, weighted_loss: 0.0026, label: 1, bag_size: 2662\n",
      "batch 519, loss: 0.0119, instance_loss: 0.0026, weighted_loss: 0.0091, label: 0, bag_size: 1639\n",
      "batch 539, loss: 0.0161, instance_loss: 0.0000, weighted_loss: 0.0113, label: 0, bag_size: 5551\n",
      "batch 559, loss: 0.0076, instance_loss: 0.4301, weighted_loss: 0.1343, label: 0, bag_size: 8755\n",
      "batch 579, loss: 0.0126, instance_loss: 0.0000, weighted_loss: 0.0088, label: 0, bag_size: 3876\n",
      "batch 599, loss: 0.0014, instance_loss: 0.0275, weighted_loss: 0.0092, label: 1, bag_size: 6164\n",
      "batch 619, loss: 0.0764, instance_loss: 0.5781, weighted_loss: 0.2269, label: 1, bag_size: 13732\n",
      "batch 639, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 20666\n",
      "batch 659, loss: 0.0114, instance_loss: 0.0000, weighted_loss: 0.0080, label: 0, bag_size: 18777\n",
      "batch 679, loss: 2.7990, instance_loss: 0.3618, weighted_loss: 2.0678, label: 0, bag_size: 2694\n",
      "batch 699, loss: 0.5986, instance_loss: 0.2949, weighted_loss: 0.5075, label: 0, bag_size: 1800\n",
      "batch 719, loss: 0.0012, instance_loss: 0.0024, weighted_loss: 0.0016, label: 1, bag_size: 5049\n",
      "batch 739, loss: 0.0070, instance_loss: 0.1440, weighted_loss: 0.0481, label: 1, bag_size: 2140\n",
      "batch 759, loss: 0.1156, instance_loss: 0.2098, weighted_loss: 0.1439, label: 1, bag_size: 8026\n",
      "batch 779, loss: 0.4613, instance_loss: 1.1034, weighted_loss: 0.6539, label: 0, bag_size: 2815\n",
      "batch 799, loss: 1.3684, instance_loss: 0.3681, weighted_loss: 1.0683, label: 0, bag_size: 2653\n",
      "batch 819, loss: 0.0017, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 11383\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.979470802919708: correct 12882/13152\n",
      "class 1 clustering acc 0.9017639902676399: correct 5930/6576\n",
      "Epoch: 44, train_loss: 0.2011, train_clustering_loss:  0.2021, train_error: 0.0766\n",
      "class 0: acc 0.9230769230769231, correct 384/416\n",
      "class 1: acc 0.9236453201970444, correct 375/406\n",
      "\n",
      "Val Set, val_loss: 0.0999, val_error: 0.0435, auc: 0.9981\n",
      "class 0 clustering acc 0.9802989130434783: correct 1443/1472\n",
      "class 1 clustering acc 0.938858695652174: correct 691/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0153, instance_loss: 0.0054, weighted_loss: 0.0123, label: 1, bag_size: 17579\n",
      "batch 39, loss: 0.0070, instance_loss: 0.0492, weighted_loss: 0.0197, label: 1, bag_size: 6776\n",
      "batch 59, loss: 0.0011, instance_loss: 0.0017, weighted_loss: 0.0013, label: 1, bag_size: 6966\n",
      "batch 79, loss: 0.0703, instance_loss: 0.0011, weighted_loss: 0.0495, label: 1, bag_size: 10460\n",
      "batch 99, loss: 0.1064, instance_loss: 0.0964, weighted_loss: 0.1034, label: 1, bag_size: 10622\n",
      "batch 119, loss: 0.0336, instance_loss: 0.5265, weighted_loss: 0.1815, label: 0, bag_size: 15003\n",
      "batch 139, loss: 0.9610, instance_loss: 0.2835, weighted_loss: 0.7578, label: 0, bag_size: 2270\n",
      "batch 159, loss: 0.0895, instance_loss: 0.0058, weighted_loss: 0.0644, label: 0, bag_size: 14333\n",
      "batch 179, loss: 0.2738, instance_loss: 0.0003, weighted_loss: 0.1917, label: 1, bag_size: 8592\n",
      "batch 199, loss: 0.1759, instance_loss: 0.0005, weighted_loss: 0.1233, label: 1, bag_size: 1875\n",
      "batch 219, loss: 0.0010, instance_loss: 0.0078, weighted_loss: 0.0031, label: 1, bag_size: 10112\n",
      "batch 239, loss: 0.0108, instance_loss: 0.0981, weighted_loss: 0.0370, label: 0, bag_size: 9596\n",
      "batch 259, loss: 0.0019, instance_loss: 0.0000, weighted_loss: 0.0013, label: 0, bag_size: 12593\n",
      "batch 279, loss: 0.0182, instance_loss: 0.0425, weighted_loss: 0.0255, label: 1, bag_size: 8012\n",
      "batch 299, loss: 1.1921, instance_loss: 1.6180, weighted_loss: 1.3199, label: 1, bag_size: 684\n",
      "batch 319, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 11759\n",
      "batch 339, loss: 0.0058, instance_loss: 0.0036, weighted_loss: 0.0052, label: 0, bag_size: 17630\n",
      "batch 359, loss: 0.0443, instance_loss: 0.5113, weighted_loss: 0.1844, label: 0, bag_size: 1213\n",
      "batch 379, loss: 0.0598, instance_loss: 0.7663, weighted_loss: 0.2717, label: 0, bag_size: 1458\n",
      "batch 399, loss: 0.1100, instance_loss: 0.0185, weighted_loss: 0.0826, label: 0, bag_size: 7557\n",
      "batch 419, loss: 0.0004, instance_loss: 0.0004, weighted_loss: 0.0004, label: 0, bag_size: 9252\n",
      "batch 439, loss: 0.0009, instance_loss: 0.0038, weighted_loss: 0.0018, label: 0, bag_size: 17791\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 14266\n",
      "batch 479, loss: 2.1932, instance_loss: 1.5517, weighted_loss: 2.0008, label: 1, bag_size: 13367\n",
      "batch 499, loss: 0.0019, instance_loss: 0.0026, weighted_loss: 0.0021, label: 1, bag_size: 8522\n",
      "batch 519, loss: 0.0178, instance_loss: 0.0095, weighted_loss: 0.0153, label: 1, bag_size: 17579\n",
      "batch 539, loss: 0.0255, instance_loss: 0.0000, weighted_loss: 0.0178, label: 0, bag_size: 19880\n",
      "batch 559, loss: 0.1982, instance_loss: 0.0131, weighted_loss: 0.1427, label: 1, bag_size: 8026\n",
      "batch 579, loss: 0.0340, instance_loss: 0.0591, weighted_loss: 0.0415, label: 1, bag_size: 1339\n",
      "batch 599, loss: 0.2720, instance_loss: 0.7195, weighted_loss: 0.4062, label: 0, bag_size: 1701\n",
      "batch 619, loss: 0.1539, instance_loss: 0.0002, weighted_loss: 0.1078, label: 1, bag_size: 12180\n",
      "batch 639, loss: 0.0214, instance_loss: 0.0057, weighted_loss: 0.0167, label: 1, bag_size: 5921\n",
      "batch 659, loss: 1.8467, instance_loss: 0.3335, weighted_loss: 1.3928, label: 1, bag_size: 6360\n",
      "batch 679, loss: 0.0076, instance_loss: 0.0117, weighted_loss: 0.0089, label: 1, bag_size: 2785\n",
      "batch 699, loss: 0.2309, instance_loss: 0.0596, weighted_loss: 0.1795, label: 0, bag_size: 3654\n",
      "batch 719, loss: 0.0077, instance_loss: 1.2673, weighted_loss: 0.3855, label: 0, bag_size: 1483\n",
      "batch 739, loss: 0.2170, instance_loss: 0.0149, weighted_loss: 0.1564, label: 1, bag_size: 1875\n",
      "batch 759, loss: 0.0001, instance_loss: 0.0017, weighted_loss: 0.0006, label: 1, bag_size: 10112\n",
      "batch 779, loss: 0.4360, instance_loss: 0.5428, weighted_loss: 0.4681, label: 0, bag_size: 2160\n",
      "batch 799, loss: 0.0075, instance_loss: 0.0000, weighted_loss: 0.0053, label: 0, bag_size: 14885\n",
      "batch 819, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 23037\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9807633819951338: correct 12899/13152\n",
      "class 1 clustering acc 0.9110401459854015: correct 5991/6576\n",
      "Epoch: 45, train_loss: 0.1789, train_clustering_loss:  0.1791, train_error: 0.0742\n",
      "class 0: acc 0.9219143576826196, correct 366/397\n",
      "class 1: acc 0.9294117647058824, correct 395/425\n",
      "\n",
      "Val Set, val_loss: 0.0867, val_error: 0.0326, auc: 0.9981\n",
      "class 0 clustering acc 0.9836956521739131: correct 1448/1472\n",
      "class 1 clustering acc 0.936141304347826: correct 689/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.9814814814814815, correct 53/54\n",
      "Validation loss decreased (0.091167 --> 0.086746).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0027, instance_loss: 0.0251, weighted_loss: 0.0094, label: 1, bag_size: 1101\n",
      "batch 39, loss: 0.7114, instance_loss: 0.0205, weighted_loss: 0.5041, label: 0, bag_size: 5161\n",
      "batch 59, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 11512\n",
      "batch 79, loss: 0.0086, instance_loss: 0.0024, weighted_loss: 0.0068, label: 1, bag_size: 2678\n",
      "batch 99, loss: 0.0321, instance_loss: 0.0016, weighted_loss: 0.0229, label: 1, bag_size: 10028\n",
      "batch 119, loss: 0.0180, instance_loss: 0.0000, weighted_loss: 0.0126, label: 0, bag_size: 8025\n",
      "batch 139, loss: 0.0782, instance_loss: 0.0000, weighted_loss: 0.0547, label: 0, bag_size: 11122\n",
      "batch 159, loss: 0.0004, instance_loss: 0.0318, weighted_loss: 0.0098, label: 1, bag_size: 1273\n",
      "batch 179, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 9786\n",
      "batch 199, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 19435\n",
      "batch 219, loss: 0.0998, instance_loss: 0.0078, weighted_loss: 0.0722, label: 1, bag_size: 5256\n",
      "batch 239, loss: 0.0019, instance_loss: 0.0038, weighted_loss: 0.0024, label: 1, bag_size: 9971\n",
      "batch 259, loss: 0.0016, instance_loss: 0.1260, weighted_loss: 0.0389, label: 1, bag_size: 6950\n",
      "batch 279, loss: 0.0240, instance_loss: 0.0042, weighted_loss: 0.0181, label: 1, bag_size: 9470\n",
      "batch 299, loss: 0.1117, instance_loss: 0.0026, weighted_loss: 0.0789, label: 1, bag_size: 9955\n",
      "batch 319, loss: 0.0060, instance_loss: 0.0000, weighted_loss: 0.0042, label: 1, bag_size: 5317\n",
      "batch 339, loss: 0.0419, instance_loss: 0.1712, weighted_loss: 0.0807, label: 1, bag_size: 1969\n",
      "batch 359, loss: 0.0018, instance_loss: 0.0536, weighted_loss: 0.0173, label: 1, bag_size: 3856\n",
      "batch 379, loss: 0.0201, instance_loss: 0.0011, weighted_loss: 0.0144, label: 1, bag_size: 14230\n",
      "batch 399, loss: 0.1241, instance_loss: 0.0000, weighted_loss: 0.0869, label: 0, bag_size: 20230\n",
      "batch 419, loss: 0.0636, instance_loss: 0.0015, weighted_loss: 0.0449, label: 1, bag_size: 4821\n",
      "batch 439, loss: 0.4612, instance_loss: 0.6047, weighted_loss: 0.5042, label: 1, bag_size: 2935\n",
      "batch 459, loss: 0.3422, instance_loss: 0.0084, weighted_loss: 0.2421, label: 0, bag_size: 18777\n",
      "batch 479, loss: 0.0923, instance_loss: 0.0003, weighted_loss: 0.0647, label: 0, bag_size: 2303\n",
      "batch 499, loss: 0.0426, instance_loss: 0.0000, weighted_loss: 0.0298, label: 1, bag_size: 21009\n",
      "batch 519, loss: 0.0193, instance_loss: 0.0000, weighted_loss: 0.0135, label: 0, bag_size: 21032\n",
      "batch 539, loss: 0.0715, instance_loss: 1.4126, weighted_loss: 0.4739, label: 1, bag_size: 1242\n",
      "batch 559, loss: 0.0132, instance_loss: 0.0003, weighted_loss: 0.0093, label: 0, bag_size: 8788\n",
      "batch 579, loss: 0.3202, instance_loss: 0.0000, weighted_loss: 0.2241, label: 1, bag_size: 7445\n",
      "batch 599, loss: 0.0049, instance_loss: 0.0028, weighted_loss: 0.0043, label: 1, bag_size: 11266\n",
      "batch 619, loss: 0.0449, instance_loss: 0.0006, weighted_loss: 0.0316, label: 0, bag_size: 1797\n",
      "batch 639, loss: 0.0664, instance_loss: 0.0000, weighted_loss: 0.0465, label: 1, bag_size: 8012\n",
      "batch 659, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 5965\n",
      "batch 679, loss: 0.0103, instance_loss: 0.0047, weighted_loss: 0.0087, label: 0, bag_size: 4345\n",
      "batch 699, loss: 0.3206, instance_loss: 0.0015, weighted_loss: 0.2249, label: 1, bag_size: 11220\n",
      "batch 719, loss: 0.0214, instance_loss: 0.0000, weighted_loss: 0.0149, label: 0, bag_size: 11194\n",
      "batch 739, loss: 0.0041, instance_loss: 0.0010, weighted_loss: 0.0032, label: 0, bag_size: 15001\n",
      "batch 759, loss: 0.0399, instance_loss: 0.0013, weighted_loss: 0.0284, label: 1, bag_size: 2412\n",
      "batch 779, loss: 0.0130, instance_loss: 0.0007, weighted_loss: 0.0093, label: 1, bag_size: 8868\n",
      "batch 799, loss: 0.0038, instance_loss: 0.0000, weighted_loss: 0.0027, label: 0, bag_size: 1213\n",
      "batch 819, loss: 0.1406, instance_loss: 0.6401, weighted_loss: 0.2905, label: 1, bag_size: 1242\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9849452554744526: correct 12954/13152\n",
      "class 1 clustering acc 0.9066301703163017: correct 5962/6576\n",
      "Epoch: 46, train_loss: 0.1894, train_clustering_loss:  0.1537, train_error: 0.0681\n",
      "class 0: acc 0.9207920792079208, correct 372/404\n",
      "class 1: acc 0.9425837320574163, correct 394/418\n",
      "\n",
      "Val Set, val_loss: 0.1311, val_error: 0.0326, auc: 0.9981\n",
      "class 0 clustering acc 0.9816576086956522: correct 1445/1472\n",
      "class 1 clustering acc 0.9456521739130435: correct 696/736\n",
      "class 0: acc 1.0, correct 38/38\n",
      "class 1: acc 0.9444444444444444, correct 51/54\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 19667\n",
      "batch 39, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 3228\n",
      "batch 59, loss: 0.9130, instance_loss: 0.8109, weighted_loss: 0.8823, label: 1, bag_size: 771\n",
      "batch 79, loss: 0.0211, instance_loss: 0.0000, weighted_loss: 0.0147, label: 0, bag_size: 25027\n",
      "batch 99, loss: 0.0121, instance_loss: 0.0020, weighted_loss: 0.0091, label: 1, bag_size: 2412\n",
      "batch 119, loss: 0.0057, instance_loss: 0.0000, weighted_loss: 0.0040, label: 0, bag_size: 2322\n",
      "batch 139, loss: 0.0020, instance_loss: 0.0000, weighted_loss: 0.0014, label: 0, bag_size: 9888\n",
      "batch 159, loss: 0.0119, instance_loss: 0.2290, weighted_loss: 0.0770, label: 0, bag_size: 2609\n",
      "batch 179, loss: 0.0016, instance_loss: 0.0026, weighted_loss: 0.0019, label: 1, bag_size: 11032\n",
      "batch 199, loss: 0.0668, instance_loss: 0.0000, weighted_loss: 0.0467, label: 1, bag_size: 10622\n",
      "batch 219, loss: 0.1824, instance_loss: 0.0000, weighted_loss: 0.1277, label: 1, bag_size: 8592\n",
      "batch 239, loss: 0.2766, instance_loss: 1.0321, weighted_loss: 0.5032, label: 1, bag_size: 1755\n",
      "batch 259, loss: 0.1706, instance_loss: 0.1523, weighted_loss: 0.1651, label: 1, bag_size: 1755\n",
      "batch 279, loss: 0.6420, instance_loss: 0.2472, weighted_loss: 0.5235, label: 1, bag_size: 11729\n",
      "batch 299, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 13225\n",
      "batch 319, loss: 0.2262, instance_loss: 0.0069, weighted_loss: 0.1604, label: 1, bag_size: 6781\n",
      "batch 339, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 21385\n",
      "batch 359, loss: 0.0684, instance_loss: 0.0036, weighted_loss: 0.0489, label: 1, bag_size: 8395\n",
      "batch 379, loss: 0.1843, instance_loss: 0.0021, weighted_loss: 0.1296, label: 1, bag_size: 8026\n",
      "batch 399, loss: 0.0072, instance_loss: 0.0000, weighted_loss: 0.0050, label: 0, bag_size: 8025\n",
      "batch 419, loss: 0.0626, instance_loss: 0.3355, weighted_loss: 0.1444, label: 0, bag_size: 17482\n",
      "batch 439, loss: 0.3192, instance_loss: 0.0000, weighted_loss: 0.2235, label: 0, bag_size: 11212\n",
      "batch 459, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 15736\n",
      "batch 479, loss: 0.0481, instance_loss: 0.0040, weighted_loss: 0.0349, label: 1, bag_size: 10848\n",
      "batch 499, loss: 0.0125, instance_loss: 0.0006, weighted_loss: 0.0089, label: 1, bag_size: 5629\n",
      "batch 519, loss: 0.0317, instance_loss: 0.0000, weighted_loss: 0.0222, label: 1, bag_size: 6090\n",
      "batch 539, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 11113\n",
      "batch 559, loss: 0.6605, instance_loss: 0.0015, weighted_loss: 0.4628, label: 1, bag_size: 11729\n",
      "batch 579, loss: 0.9197, instance_loss: 0.3006, weighted_loss: 0.7340, label: 0, bag_size: 9421\n",
      "batch 599, loss: 0.0017, instance_loss: 0.0002, weighted_loss: 0.0013, label: 1, bag_size: 6966\n",
      "batch 619, loss: 4.5636, instance_loss: 5.4809, weighted_loss: 4.8388, label: 0, bag_size: 4692\n",
      "batch 639, loss: 0.0092, instance_loss: 0.0000, weighted_loss: 0.0065, label: 0, bag_size: 15464\n",
      "batch 659, loss: 0.0160, instance_loss: 0.0122, weighted_loss: 0.0148, label: 0, bag_size: 2998\n",
      "batch 679, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 11477\n",
      "batch 699, loss: 0.0150, instance_loss: 0.0052, weighted_loss: 0.0121, label: 1, bag_size: 4821\n",
      "batch 719, loss: 0.0001, instance_loss: 0.2578, weighted_loss: 0.0775, label: 1, bag_size: 11642\n",
      "batch 739, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 2322\n",
      "batch 759, loss: 0.0523, instance_loss: 0.0013, weighted_loss: 0.0370, label: 1, bag_size: 11223\n",
      "batch 779, loss: 0.0064, instance_loss: 0.0003, weighted_loss: 0.0046, label: 1, bag_size: 10558\n",
      "batch 799, loss: 0.0205, instance_loss: 0.4557, weighted_loss: 0.1511, label: 0, bag_size: 9583\n",
      "batch 819, loss: 0.4486, instance_loss: 0.0009, weighted_loss: 0.3142, label: 0, bag_size: 15255\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9841088807785888: correct 12943/13152\n",
      "class 1 clustering acc 0.9105839416058394: correct 5988/6576\n",
      "Epoch: 47, train_loss: 0.1754, train_clustering_loss:  0.1648, train_error: 0.0657\n",
      "class 0: acc 0.9316037735849056, correct 395/424\n",
      "class 1: acc 0.9371859296482412, correct 373/398\n",
      "\n",
      "Val Set, val_loss: 0.0961, val_error: 0.0326, auc: 0.9981\n",
      "class 0 clustering acc 0.985733695652174: correct 1451/1472\n",
      "class 1 clustering acc 0.9524456521739131: correct 701/736\n",
      "class 0: acc 0.9736842105263158, correct 37/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0182, instance_loss: 0.0000, weighted_loss: 0.0127, label: 0, bag_size: 3725\n",
      "batch 39, loss: 0.0546, instance_loss: 1.0764, weighted_loss: 0.3611, label: 1, bag_size: 11386\n",
      "batch 59, loss: 0.0019, instance_loss: 0.0000, weighted_loss: 0.0013, label: 0, bag_size: 10581\n",
      "batch 79, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 11654\n",
      "batch 99, loss: 0.0428, instance_loss: 1.1898, weighted_loss: 0.3869, label: 0, bag_size: 1772\n",
      "batch 119, loss: 0.0188, instance_loss: 0.0310, weighted_loss: 0.0224, label: 1, bag_size: 6171\n",
      "batch 139, loss: 0.0101, instance_loss: 0.0000, weighted_loss: 0.0071, label: 0, bag_size: 12561\n",
      "batch 159, loss: 0.0129, instance_loss: 0.0005, weighted_loss: 0.0092, label: 1, bag_size: 8040\n",
      "batch 179, loss: 0.0192, instance_loss: 0.1760, weighted_loss: 0.0662, label: 0, bag_size: 1234\n",
      "batch 199, loss: 0.0753, instance_loss: 0.0000, weighted_loss: 0.0527, label: 0, bag_size: 25814\n",
      "batch 219, loss: 0.1927, instance_loss: 0.0251, weighted_loss: 0.1424, label: 1, bag_size: 12425\n",
      "batch 239, loss: 0.0546, instance_loss: 0.0060, weighted_loss: 0.0400, label: 0, bag_size: 13880\n",
      "batch 259, loss: 0.0145, instance_loss: 0.0047, weighted_loss: 0.0116, label: 1, bag_size: 1888\n",
      "batch 279, loss: 0.0067, instance_loss: 0.0036, weighted_loss: 0.0058, label: 1, bag_size: 5340\n",
      "batch 299, loss: 0.0025, instance_loss: 0.0000, weighted_loss: 0.0017, label: 0, bag_size: 19043\n",
      "batch 319, loss: 0.0029, instance_loss: 0.0000, weighted_loss: 0.0020, label: 1, bag_size: 11684\n",
      "batch 339, loss: 0.0056, instance_loss: 0.0000, weighted_loss: 0.0039, label: 0, bag_size: 8981\n",
      "batch 359, loss: 0.0572, instance_loss: 0.6060, weighted_loss: 0.2219, label: 0, bag_size: 1891\n",
      "batch 379, loss: 0.0378, instance_loss: 0.0797, weighted_loss: 0.0504, label: 0, bag_size: 803\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0582, weighted_loss: 0.0175, label: 1, bag_size: 3004\n",
      "batch 419, loss: 0.9503, instance_loss: 0.0048, weighted_loss: 0.6666, label: 1, bag_size: 1764\n",
      "batch 439, loss: 0.0637, instance_loss: 0.0855, weighted_loss: 0.0703, label: 0, bag_size: 24439\n",
      "batch 459, loss: 0.0081, instance_loss: 0.0003, weighted_loss: 0.0058, label: 0, bag_size: 10304\n",
      "batch 479, loss: 0.0378, instance_loss: 0.0209, weighted_loss: 0.0327, label: 0, bag_size: 5999\n",
      "batch 499, loss: 0.0353, instance_loss: 0.0026, weighted_loss: 0.0255, label: 1, bag_size: 11701\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 17791\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23996\n",
      "batch 559, loss: 0.4572, instance_loss: 0.0000, weighted_loss: 0.3201, label: 0, bag_size: 5211\n",
      "batch 579, loss: 0.0622, instance_loss: 0.0046, weighted_loss: 0.0449, label: 1, bag_size: 8216\n",
      "batch 599, loss: 0.2060, instance_loss: 0.3673, weighted_loss: 0.2544, label: 0, bag_size: 1498\n",
      "batch 619, loss: 0.1700, instance_loss: 0.9472, weighted_loss: 0.4031, label: 1, bag_size: 9330\n",
      "batch 639, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 26271\n",
      "batch 659, loss: 0.0248, instance_loss: 0.4487, weighted_loss: 0.1520, label: 0, bag_size: 11146\n",
      "batch 679, loss: 0.2170, instance_loss: 0.0187, weighted_loss: 0.1575, label: 0, bag_size: 10898\n",
      "batch 699, loss: 0.1052, instance_loss: 0.2466, weighted_loss: 0.1476, label: 0, bag_size: 1438\n",
      "batch 719, loss: 0.3774, instance_loss: 1.8047, weighted_loss: 0.8056, label: 0, bag_size: 1592\n",
      "batch 739, loss: 0.2204, instance_loss: 0.0292, weighted_loss: 0.1631, label: 0, bag_size: 20478\n",
      "batch 759, loss: 0.0940, instance_loss: 0.0000, weighted_loss: 0.0658, label: 0, bag_size: 5297\n",
      "batch 779, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 4902\n",
      "batch 799, loss: 0.0423, instance_loss: 0.2394, weighted_loss: 0.1015, label: 0, bag_size: 19880\n",
      "batch 819, loss: 0.0050, instance_loss: 0.0068, weighted_loss: 0.0056, label: 1, bag_size: 1838\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9809154501216545: correct 12901/13152\n",
      "class 1 clustering acc 0.8968978102189781: correct 5898/6576\n",
      "Epoch: 48, train_loss: 0.1562, train_clustering_loss:  0.1919, train_error: 0.0620\n",
      "class 0: acc 0.9467592592592593, correct 409/432\n",
      "class 1: acc 0.9282051282051282, correct 362/390\n",
      "\n",
      "Val Set, val_loss: 0.0812, val_error: 0.0326, auc: 0.9981\n",
      "class 0 clustering acc 0.970108695652174: correct 1428/1472\n",
      "class 1 clustering acc 0.8600543478260869: correct 633/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.9814814814814815, correct 53/54\n",
      "Validation loss decreased (0.086746 --> 0.081222).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5266, instance_loss: 0.0089, weighted_loss: 0.3713, label: 1, bag_size: 4976\n",
      "batch 39, loss: 0.0025, instance_loss: 0.6712, weighted_loss: 0.2031, label: 1, bag_size: 6950\n",
      "batch 59, loss: 0.0307, instance_loss: 0.2183, weighted_loss: 0.0870, label: 0, bag_size: 9596\n",
      "batch 79, loss: 0.2774, instance_loss: 1.4045, weighted_loss: 0.6156, label: 1, bag_size: 7989\n",
      "batch 99, loss: 0.0039, instance_loss: 0.0449, weighted_loss: 0.0162, label: 1, bag_size: 5763\n",
      "batch 119, loss: 0.0092, instance_loss: 0.0415, weighted_loss: 0.0189, label: 1, bag_size: 2559\n",
      "batch 139, loss: 0.0706, instance_loss: 0.0000, weighted_loss: 0.0494, label: 0, bag_size: 19880\n",
      "batch 159, loss: 0.0716, instance_loss: 0.0286, weighted_loss: 0.0587, label: 1, bag_size: 8264\n",
      "batch 179, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 12524\n",
      "batch 199, loss: 0.0314, instance_loss: 0.2127, weighted_loss: 0.0858, label: 1, bag_size: 1294\n",
      "batch 219, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 17268\n",
      "batch 239, loss: 0.0275, instance_loss: 0.0133, weighted_loss: 0.0232, label: 1, bag_size: 5454\n",
      "batch 259, loss: 0.8940, instance_loss: 1.2207, weighted_loss: 0.9920, label: 1, bag_size: 2935\n",
      "batch 279, loss: 0.0002, instance_loss: 2.2794, weighted_loss: 0.6839, label: 1, bag_size: 10920\n",
      "batch 299, loss: 0.0039, instance_loss: 0.0000, weighted_loss: 0.0027, label: 0, bag_size: 8582\n",
      "batch 319, loss: 0.0244, instance_loss: 0.0155, weighted_loss: 0.0217, label: 1, bag_size: 7873\n",
      "batch 339, loss: 0.0024, instance_loss: 0.0000, weighted_loss: 0.0017, label: 0, bag_size: 2179\n",
      "batch 359, loss: 0.0130, instance_loss: 0.1916, weighted_loss: 0.0666, label: 0, bag_size: 8755\n",
      "batch 379, loss: 0.0200, instance_loss: 0.0061, weighted_loss: 0.0158, label: 0, bag_size: 2814\n",
      "batch 399, loss: 0.0027, instance_loss: 0.0019, weighted_loss: 0.0024, label: 1, bag_size: 6453\n",
      "batch 419, loss: 0.0022, instance_loss: 0.0014, weighted_loss: 0.0020, label: 0, bag_size: 9888\n",
      "batch 439, loss: 0.1923, instance_loss: 0.5104, weighted_loss: 0.2877, label: 1, bag_size: 1794\n",
      "batch 459, loss: 0.0029, instance_loss: 0.2016, weighted_loss: 0.0625, label: 1, bag_size: 3576\n",
      "batch 479, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 7709\n",
      "batch 499, loss: 0.0048, instance_loss: 0.0000, weighted_loss: 0.0034, label: 0, bag_size: 13892\n",
      "batch 519, loss: 0.0170, instance_loss: 0.0641, weighted_loss: 0.0311, label: 0, bag_size: 9596\n",
      "batch 539, loss: 0.0365, instance_loss: 0.1412, weighted_loss: 0.0679, label: 0, bag_size: 5409\n",
      "batch 559, loss: 0.0506, instance_loss: 0.0790, weighted_loss: 0.0591, label: 1, bag_size: 11316\n",
      "batch 579, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 11900\n",
      "batch 599, loss: 0.1495, instance_loss: 0.0004, weighted_loss: 0.1048, label: 1, bag_size: 6731\n",
      "batch 619, loss: 0.0099, instance_loss: 0.0000, weighted_loss: 0.0069, label: 0, bag_size: 12732\n",
      "batch 639, loss: 0.0007, instance_loss: 0.0396, weighted_loss: 0.0124, label: 0, bag_size: 9930\n",
      "batch 659, loss: 0.0943, instance_loss: 0.0018, weighted_loss: 0.0665, label: 1, bag_size: 5723\n",
      "batch 679, loss: 0.0091, instance_loss: 0.5688, weighted_loss: 0.1770, label: 1, bag_size: 4039\n",
      "batch 699, loss: 0.1339, instance_loss: 0.0465, weighted_loss: 0.1077, label: 0, bag_size: 2219\n",
      "batch 719, loss: 0.0020, instance_loss: 0.0000, weighted_loss: 0.0014, label: 1, bag_size: 11387\n",
      "batch 739, loss: 2.3130, instance_loss: 0.0097, weighted_loss: 1.6220, label: 0, bag_size: 5120\n",
      "batch 759, loss: 0.0027, instance_loss: 0.0000, weighted_loss: 0.0019, label: 0, bag_size: 2044\n",
      "batch 779, loss: 0.0249, instance_loss: 0.3090, weighted_loss: 0.1102, label: 1, bag_size: 8264\n",
      "batch 799, loss: 0.0334, instance_loss: 0.0633, weighted_loss: 0.0424, label: 1, bag_size: 1236\n",
      "batch 819, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 21385\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9796228710462287: correct 12884/13152\n",
      "class 1 clustering acc 0.8845802919708029: correct 5817/6576\n",
      "Epoch: 49, train_loss: 0.1782, train_clustering_loss:  0.2024, train_error: 0.0645\n",
      "class 0: acc 0.9396984924623115, correct 374/398\n",
      "class 1: acc 0.9316037735849056, correct 395/424\n",
      "\n",
      "Val Set, val_loss: 0.0797, val_error: 0.0435, auc: 0.9981\n",
      "class 0 clustering acc 0.9741847826086957: correct 1434/1472\n",
      "class 1 clustering acc 0.8057065217391305: correct 593/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "Validation loss decreased (0.081222 --> 0.079723).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0087, instance_loss: 0.1036, weighted_loss: 0.0372, label: 0, bag_size: 3198\n",
      "batch 39, loss: 0.0578, instance_loss: 0.4580, weighted_loss: 0.1779, label: 1, bag_size: 1230\n",
      "batch 59, loss: 0.0041, instance_loss: 0.0613, weighted_loss: 0.0213, label: 1, bag_size: 3409\n",
      "batch 79, loss: 0.0544, instance_loss: 0.2615, weighted_loss: 0.1165, label: 0, bag_size: 9616\n",
      "batch 99, loss: 0.1482, instance_loss: 0.5148, weighted_loss: 0.2582, label: 0, bag_size: 9421\n",
      "batch 119, loss: 0.0031, instance_loss: 0.0000, weighted_loss: 0.0022, label: 1, bag_size: 9470\n",
      "batch 139, loss: 2.2496, instance_loss: 0.4910, weighted_loss: 1.7220, label: 0, bag_size: 2653\n",
      "batch 159, loss: 0.0357, instance_loss: 0.0000, weighted_loss: 0.0250, label: 1, bag_size: 11701\n",
      "batch 179, loss: 0.0175, instance_loss: 0.0000, weighted_loss: 0.0122, label: 0, bag_size: 7381\n",
      "batch 199, loss: 0.0205, instance_loss: 1.0477, weighted_loss: 0.3286, label: 0, bag_size: 1052\n",
      "batch 219, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 23368\n",
      "batch 239, loss: 0.0751, instance_loss: 0.0000, weighted_loss: 0.0526, label: 0, bag_size: 10365\n",
      "batch 259, loss: 0.0090, instance_loss: 0.0000, weighted_loss: 0.0063, label: 1, bag_size: 20161\n",
      "batch 279, loss: 0.1372, instance_loss: 0.2387, weighted_loss: 0.1676, label: 0, bag_size: 2242\n",
      "batch 299, loss: 0.6146, instance_loss: 0.0007, weighted_loss: 0.4304, label: 0, bag_size: 2959\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 15850\n",
      "batch 339, loss: 0.0071, instance_loss: 0.0000, weighted_loss: 0.0049, label: 0, bag_size: 19808\n",
      "batch 359, loss: 0.0002, instance_loss: 0.0009, weighted_loss: 0.0004, label: 0, bag_size: 19390\n",
      "batch 379, loss: 0.0527, instance_loss: 0.2751, weighted_loss: 0.1195, label: 1, bag_size: 34356\n",
      "batch 399, loss: 0.0125, instance_loss: 0.0645, weighted_loss: 0.0281, label: 1, bag_size: 12895\n",
      "batch 419, loss: 1.2864, instance_loss: 0.2566, weighted_loss: 0.9774, label: 1, bag_size: 1875\n",
      "batch 439, loss: 0.1028, instance_loss: 0.1408, weighted_loss: 0.1142, label: 1, bag_size: 16548\n",
      "batch 459, loss: 0.0637, instance_loss: 0.0108, weighted_loss: 0.0478, label: 0, bag_size: 6356\n",
      "batch 479, loss: 0.0122, instance_loss: 0.0000, weighted_loss: 0.0085, label: 0, bag_size: 18954\n",
      "batch 499, loss: 0.0012, instance_loss: 0.0759, weighted_loss: 0.0236, label: 1, bag_size: 18649\n",
      "batch 519, loss: 0.0124, instance_loss: 0.0000, weighted_loss: 0.0087, label: 0, bag_size: 2873\n",
      "batch 539, loss: 0.0002, instance_loss: 0.0005, weighted_loss: 0.0003, label: 1, bag_size: 4102\n",
      "batch 559, loss: 0.1517, instance_loss: 0.0000, weighted_loss: 0.1062, label: 0, bag_size: 3238\n",
      "batch 579, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 21864\n",
      "batch 599, loss: 0.0283, instance_loss: 0.0198, weighted_loss: 0.0257, label: 1, bag_size: 5155\n",
      "batch 619, loss: 0.5655, instance_loss: 1.9070, weighted_loss: 0.9680, label: 0, bag_size: 1714\n",
      "batch 639, loss: 0.1726, instance_loss: 0.0521, weighted_loss: 0.1364, label: 1, bag_size: 9561\n",
      "batch 659, loss: 0.3214, instance_loss: 1.3799, weighted_loss: 0.6390, label: 0, bag_size: 2104\n",
      "batch 679, loss: 0.1018, instance_loss: 0.2782, weighted_loss: 0.1547, label: 0, bag_size: 1614\n",
      "batch 699, loss: 0.0052, instance_loss: 0.0485, weighted_loss: 0.0182, label: 0, bag_size: 1884\n",
      "batch 719, loss: 0.0034, instance_loss: 0.0000, weighted_loss: 0.0024, label: 0, bag_size: 21093\n",
      "batch 739, loss: 0.6320, instance_loss: 0.1203, weighted_loss: 0.4785, label: 0, bag_size: 2918\n",
      "batch 759, loss: 0.0018, instance_loss: 0.0205, weighted_loss: 0.0074, label: 0, bag_size: 2534\n",
      "batch 779, loss: 0.5696, instance_loss: 0.1683, weighted_loss: 0.4492, label: 0, bag_size: 1127\n",
      "batch 799, loss: 0.2834, instance_loss: 0.7995, weighted_loss: 0.4382, label: 1, bag_size: 1095\n",
      "batch 819, loss: 0.0081, instance_loss: 0.3989, weighted_loss: 0.1253, label: 1, bag_size: 1064\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9812195863746959: correct 12905/13152\n",
      "class 1 clustering acc 0.8826034063260341: correct 5804/6576\n",
      "Epoch: 50, train_loss: 0.1615, train_clustering_loss:  0.2041, train_error: 0.0572\n",
      "class 0: acc 0.9477434679334917, correct 399/421\n",
      "class 1: acc 0.9376558603491272, correct 376/401\n",
      "\n",
      "Val Set, val_loss: 0.0825, val_error: 0.0435, auc: 0.9976\n",
      "class 0 clustering acc 0.9836956521739131: correct 1448/1472\n",
      "class 1 clustering acc 0.9171195652173914: correct 675/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7325, instance_loss: 0.8313, weighted_loss: 0.7622, label: 1, bag_size: 1236\n",
      "batch 39, loss: 0.0019, instance_loss: 0.0000, weighted_loss: 0.0013, label: 0, bag_size: 19043\n",
      "batch 59, loss: 0.0469, instance_loss: 0.0000, weighted_loss: 0.0329, label: 0, bag_size: 13205\n",
      "batch 79, loss: 0.0845, instance_loss: 0.0220, weighted_loss: 0.0657, label: 1, bag_size: 8438\n",
      "batch 99, loss: 0.0324, instance_loss: 0.3579, weighted_loss: 0.1301, label: 1, bag_size: 4039\n",
      "batch 119, loss: 0.0032, instance_loss: 0.0443, weighted_loss: 0.0155, label: 0, bag_size: 12593\n",
      "batch 139, loss: 0.0268, instance_loss: 0.0394, weighted_loss: 0.0306, label: 1, bag_size: 4821\n",
      "batch 159, loss: 0.0283, instance_loss: 0.1333, weighted_loss: 0.0598, label: 0, bag_size: 1416\n",
      "batch 179, loss: 0.0008, instance_loss: 0.0237, weighted_loss: 0.0077, label: 0, bag_size: 10751\n",
      "batch 199, loss: 0.0007, instance_loss: 0.8117, weighted_loss: 0.2440, label: 0, bag_size: 13591\n",
      "batch 219, loss: 0.0014, instance_loss: 0.0287, weighted_loss: 0.0096, label: 0, bag_size: 11113\n",
      "batch 239, loss: 0.0500, instance_loss: 0.0090, weighted_loss: 0.0377, label: 1, bag_size: 2936\n",
      "batch 259, loss: 0.0292, instance_loss: 0.0406, weighted_loss: 0.0326, label: 0, bag_size: 13892\n",
      "batch 279, loss: 0.0017, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 27012\n",
      "batch 299, loss: 0.0006, instance_loss: 0.0169, weighted_loss: 0.0055, label: 1, bag_size: 9673\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0170, weighted_loss: 0.0051, label: 0, bag_size: 10068\n",
      "batch 339, loss: 0.1001, instance_loss: 0.0143, weighted_loss: 0.0743, label: 1, bag_size: 1838\n",
      "batch 359, loss: 2.2115, instance_loss: 3.8810, weighted_loss: 2.7123, label: 1, bag_size: 684\n",
      "batch 379, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 22426\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 17437\n",
      "batch 419, loss: 0.0149, instance_loss: 0.0171, weighted_loss: 0.0156, label: 1, bag_size: 15609\n",
      "batch 439, loss: 1.0532, instance_loss: 1.4125, weighted_loss: 1.1610, label: 0, bag_size: 13619\n",
      "batch 459, loss: 0.0039, instance_loss: 0.0483, weighted_loss: 0.0173, label: 1, bag_size: 4308\n",
      "batch 479, loss: 0.0059, instance_loss: 0.0000, weighted_loss: 0.0041, label: 0, bag_size: 13880\n",
      "batch 499, loss: 0.0329, instance_loss: 0.0000, weighted_loss: 0.0231, label: 0, bag_size: 17083\n",
      "batch 519, loss: 0.0028, instance_loss: 0.0442, weighted_loss: 0.0152, label: 1, bag_size: 2904\n",
      "batch 539, loss: 0.0851, instance_loss: 0.0634, weighted_loss: 0.0786, label: 1, bag_size: 1051\n",
      "batch 559, loss: 0.0024, instance_loss: 0.0000, weighted_loss: 0.0017, label: 0, bag_size: 13795\n",
      "batch 579, loss: 0.0070, instance_loss: 0.0000, weighted_loss: 0.0049, label: 0, bag_size: 31085\n",
      "batch 599, loss: 0.2751, instance_loss: 0.0000, weighted_loss: 0.1926, label: 0, bag_size: 14333\n",
      "batch 619, loss: 0.0011, instance_loss: 0.0053, weighted_loss: 0.0023, label: 0, bag_size: 10721\n",
      "batch 639, loss: 0.1588, instance_loss: 0.4219, weighted_loss: 0.2377, label: 1, bag_size: 16154\n",
      "batch 659, loss: 0.0156, instance_loss: 0.0114, weighted_loss: 0.0143, label: 1, bag_size: 3224\n",
      "batch 679, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 20666\n",
      "batch 699, loss: 0.1293, instance_loss: 0.1070, weighted_loss: 0.1226, label: 1, bag_size: 4976\n",
      "batch 719, loss: 0.0134, instance_loss: 0.1521, weighted_loss: 0.0550, label: 0, bag_size: 5639\n",
      "batch 739, loss: 0.0167, instance_loss: 0.0000, weighted_loss: 0.0117, label: 0, bag_size: 13378\n",
      "batch 759, loss: 0.0100, instance_loss: 0.0030, weighted_loss: 0.0079, label: 1, bag_size: 21009\n",
      "batch 779, loss: 0.0627, instance_loss: 1.7262, weighted_loss: 0.5618, label: 1, bag_size: 8982\n",
      "batch 799, loss: 1.6792, instance_loss: 0.0081, weighted_loss: 1.1779, label: 0, bag_size: 15672\n",
      "batch 819, loss: 0.0006, instance_loss: 0.0827, weighted_loss: 0.0252, label: 0, bag_size: 1712\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9781021897810219: correct 12864/13152\n",
      "class 1 clustering acc 0.9040450121654501: correct 5945/6576\n",
      "Epoch: 51, train_loss: 0.2149, train_clustering_loss:  0.2038, train_error: 0.0742\n",
      "class 0: acc 0.9308755760368663, correct 404/434\n",
      "class 1: acc 0.9201030927835051, correct 357/388\n",
      "\n",
      "Val Set, val_loss: 0.1052, val_error: 0.0217, auc: 0.9981\n",
      "class 0 clustering acc 0.9816576086956522: correct 1445/1472\n",
      "class 1 clustering acc 0.9225543478260869: correct 679/736\n",
      "class 0: acc 1.0, correct 38/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0430, instance_loss: 0.4981, weighted_loss: 0.1795, label: 0, bag_size: 10063\n",
      "batch 39, loss: 0.1964, instance_loss: 0.8653, weighted_loss: 0.3971, label: 0, bag_size: 9421\n",
      "batch 59, loss: 0.0019, instance_loss: 0.2043, weighted_loss: 0.0626, label: 0, bag_size: 3101\n",
      "batch 79, loss: 0.0424, instance_loss: 0.0006, weighted_loss: 0.0298, label: 1, bag_size: 2936\n",
      "batch 99, loss: 0.0004, instance_loss: 0.0053, weighted_loss: 0.0019, label: 1, bag_size: 12795\n",
      "batch 119, loss: 0.0073, instance_loss: 0.0539, weighted_loss: 0.0213, label: 1, bag_size: 8191\n",
      "batch 139, loss: 0.0570, instance_loss: 0.0040, weighted_loss: 0.0411, label: 1, bag_size: 11256\n",
      "batch 159, loss: 0.5462, instance_loss: 0.3225, weighted_loss: 0.4791, label: 1, bag_size: 1649\n",
      "batch 179, loss: 0.2392, instance_loss: 0.1938, weighted_loss: 0.2256, label: 0, bag_size: 1127\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0011, weighted_loss: 0.0004, label: 1, bag_size: 2405\n",
      "batch 219, loss: 0.0130, instance_loss: 0.0169, weighted_loss: 0.0141, label: 0, bag_size: 2654\n",
      "batch 239, loss: 2.2634, instance_loss: 1.7971, weighted_loss: 2.1235, label: 0, bag_size: 3897\n",
      "batch 259, loss: 0.0076, instance_loss: 0.0859, weighted_loss: 0.0310, label: 1, bag_size: 19832\n",
      "batch 279, loss: 0.0035, instance_loss: 0.0066, weighted_loss: 0.0044, label: 1, bag_size: 12697\n",
      "batch 299, loss: 0.0302, instance_loss: 0.0067, weighted_loss: 0.0232, label: 1, bag_size: 11363\n",
      "batch 319, loss: 0.0112, instance_loss: 0.0000, weighted_loss: 0.0079, label: 0, bag_size: 21138\n",
      "batch 339, loss: 2.5737, instance_loss: 0.0245, weighted_loss: 1.8090, label: 1, bag_size: 25831\n",
      "batch 359, loss: 0.0029, instance_loss: 0.2723, weighted_loss: 0.0837, label: 1, bag_size: 7381\n",
      "batch 379, loss: 0.0185, instance_loss: 0.0003, weighted_loss: 0.0131, label: 0, bag_size: 21032\n",
      "batch 399, loss: 0.1314, instance_loss: 0.1684, weighted_loss: 0.1425, label: 0, bag_size: 3725\n",
      "batch 419, loss: 0.4198, instance_loss: 0.4755, weighted_loss: 0.4365, label: 0, bag_size: 7141\n",
      "batch 439, loss: 0.0148, instance_loss: 0.1715, weighted_loss: 0.0618, label: 0, bag_size: 3160\n",
      "batch 459, loss: 0.0507, instance_loss: 0.6152, weighted_loss: 0.2201, label: 0, bag_size: 16690\n",
      "batch 479, loss: 0.0092, instance_loss: 0.1799, weighted_loss: 0.0604, label: 0, bag_size: 13691\n",
      "batch 499, loss: 0.4278, instance_loss: 0.2962, weighted_loss: 0.3883, label: 0, bag_size: 24382\n",
      "batch 519, loss: 0.0292, instance_loss: 0.0542, weighted_loss: 0.0367, label: 1, bag_size: 9747\n",
      "batch 539, loss: 0.0392, instance_loss: 1.0846, weighted_loss: 0.3528, label: 0, bag_size: 4959\n",
      "batch 559, loss: 1.7926, instance_loss: 0.6023, weighted_loss: 1.4355, label: 1, bag_size: 12494\n",
      "batch 579, loss: 0.4588, instance_loss: 0.1798, weighted_loss: 0.3751, label: 1, bag_size: 13362\n",
      "batch 599, loss: 0.1915, instance_loss: 0.0374, weighted_loss: 0.1452, label: 0, bag_size: 3399\n",
      "batch 619, loss: 0.1064, instance_loss: 1.2454, weighted_loss: 0.4481, label: 0, bag_size: 2458\n",
      "batch 639, loss: 0.9200, instance_loss: 2.2916, weighted_loss: 1.3315, label: 1, bag_size: 15185\n",
      "batch 659, loss: 0.6623, instance_loss: 0.0666, weighted_loss: 0.4836, label: 0, bag_size: 4241\n",
      "batch 679, loss: 0.0116, instance_loss: 0.0009, weighted_loss: 0.0084, label: 1, bag_size: 8868\n",
      "batch 699, loss: 0.2278, instance_loss: 0.2103, weighted_loss: 0.2226, label: 1, bag_size: 3879\n",
      "batch 719, loss: 0.0040, instance_loss: 0.0079, weighted_loss: 0.0052, label: 1, bag_size: 8475\n",
      "batch 739, loss: 0.0218, instance_loss: 0.1389, weighted_loss: 0.0569, label: 1, bag_size: 5256\n",
      "batch 759, loss: 0.0016, instance_loss: 0.0006, weighted_loss: 0.0013, label: 1, bag_size: 3549\n",
      "batch 779, loss: 0.0143, instance_loss: 0.0258, weighted_loss: 0.0177, label: 1, bag_size: 5155\n",
      "batch 799, loss: 0.0072, instance_loss: 0.0871, weighted_loss: 0.0312, label: 0, bag_size: 2873\n",
      "batch 819, loss: 1.7748, instance_loss: 0.1565, weighted_loss: 1.2893, label: 0, bag_size: 5120\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9772658150851582: correct 12853/13152\n",
      "class 1 clustering acc 0.8797141119221411: correct 5785/6576\n",
      "Epoch: 52, train_loss: 0.1845, train_clustering_loss:  0.2099, train_error: 0.0681\n",
      "class 0: acc 0.927710843373494, correct 385/415\n",
      "class 1: acc 0.9361179361179361, correct 381/407\n",
      "\n",
      "Val Set, val_loss: 0.0848, val_error: 0.0326, auc: 0.9976\n",
      "class 0 clustering acc 0.9809782608695652: correct 1444/1472\n",
      "class 1 clustering acc 0.8994565217391305: correct 662/736\n",
      "class 0: acc 0.9736842105263158, correct 37/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0118, instance_loss: 0.0030, weighted_loss: 0.0092, label: 1, bag_size: 4128\n",
      "batch 39, loss: 0.4264, instance_loss: 0.0623, weighted_loss: 0.3172, label: 0, bag_size: 2959\n",
      "batch 59, loss: 0.0033, instance_loss: 0.1868, weighted_loss: 0.0583, label: 0, bag_size: 10128\n",
      "batch 79, loss: 0.0591, instance_loss: 0.0215, weighted_loss: 0.0478, label: 0, bag_size: 15071\n",
      "batch 99, loss: 0.1289, instance_loss: 0.0040, weighted_loss: 0.0915, label: 1, bag_size: 13477\n",
      "batch 119, loss: 0.0010, instance_loss: 0.0014, weighted_loss: 0.0011, label: 1, bag_size: 6164\n",
      "batch 139, loss: 0.0179, instance_loss: 0.0008, weighted_loss: 0.0128, label: 1, bag_size: 1022\n",
      "batch 159, loss: 0.0071, instance_loss: 0.0022, weighted_loss: 0.0057, label: 1, bag_size: 6453\n",
      "batch 179, loss: 1.1782, instance_loss: 2.0429, weighted_loss: 1.4376, label: 1, bag_size: 3879\n",
      "batch 199, loss: 0.0560, instance_loss: 0.1199, weighted_loss: 0.0752, label: 0, bag_size: 1884\n",
      "batch 219, loss: 0.0746, instance_loss: 0.0216, weighted_loss: 0.0587, label: 1, bag_size: 11223\n",
      "batch 239, loss: 0.1386, instance_loss: 0.2230, weighted_loss: 0.1640, label: 1, bag_size: 7583\n",
      "batch 259, loss: 0.0063, instance_loss: 0.0000, weighted_loss: 0.0044, label: 0, bag_size: 23368\n",
      "batch 279, loss: 0.2029, instance_loss: 0.0125, weighted_loss: 0.1458, label: 1, bag_size: 2137\n",
      "batch 299, loss: 0.0032, instance_loss: 0.1006, weighted_loss: 0.0324, label: 1, bag_size: 6164\n",
      "batch 319, loss: 0.1941, instance_loss: 1.9333, weighted_loss: 0.7159, label: 0, bag_size: 1592\n",
      "batch 339, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 14305\n",
      "batch 359, loss: 0.3543, instance_loss: 0.0184, weighted_loss: 0.2535, label: 1, bag_size: 10591\n",
      "batch 379, loss: 1.0370, instance_loss: 0.9462, weighted_loss: 1.0097, label: 1, bag_size: 1845\n",
      "batch 399, loss: 0.0147, instance_loss: 0.0000, weighted_loss: 0.0103, label: 0, bag_size: 30828\n",
      "batch 419, loss: 0.0181, instance_loss: 0.0060, weighted_loss: 0.0144, label: 1, bag_size: 6842\n",
      "batch 439, loss: 0.3599, instance_loss: 0.1668, weighted_loss: 0.3020, label: 0, bag_size: 2043\n",
      "batch 459, loss: 0.1683, instance_loss: 0.0000, weighted_loss: 0.1178, label: 0, bag_size: 3238\n",
      "batch 479, loss: 0.0010, instance_loss: 0.0112, weighted_loss: 0.0040, label: 1, bag_size: 1165\n",
      "batch 499, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 27012\n",
      "batch 519, loss: 0.9315, instance_loss: 0.2941, weighted_loss: 0.7403, label: 0, bag_size: 8420\n",
      "batch 539, loss: 0.1832, instance_loss: 0.2149, weighted_loss: 0.1927, label: 0, bag_size: 2815\n",
      "batch 559, loss: 0.0132, instance_loss: 0.0022, weighted_loss: 0.0099, label: 0, bag_size: 2652\n",
      "batch 579, loss: 0.0132, instance_loss: 0.0073, weighted_loss: 0.0114, label: 1, bag_size: 7468\n",
      "batch 599, loss: 0.0853, instance_loss: 0.0054, weighted_loss: 0.0613, label: 1, bag_size: 5454\n",
      "batch 619, loss: 0.0582, instance_loss: 0.0191, weighted_loss: 0.0465, label: 0, bag_size: 1797\n",
      "batch 639, loss: 2.8199, instance_loss: 0.4063, weighted_loss: 2.0958, label: 1, bag_size: 25831\n",
      "batch 659, loss: 0.0076, instance_loss: 0.0139, weighted_loss: 0.0095, label: 1, bag_size: 9636\n",
      "batch 679, loss: 0.8752, instance_loss: 0.3554, weighted_loss: 0.7193, label: 1, bag_size: 771\n",
      "batch 699, loss: 0.0543, instance_loss: 0.0020, weighted_loss: 0.0386, label: 1, bag_size: 15125\n",
      "batch 719, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 11199\n",
      "batch 739, loss: 0.0459, instance_loss: 0.1016, weighted_loss: 0.0626, label: 0, bag_size: 1920\n",
      "batch 759, loss: 0.0018, instance_loss: 0.0005, weighted_loss: 0.0014, label: 0, bag_size: 14956\n",
      "batch 779, loss: 0.0015, instance_loss: 0.1832, weighted_loss: 0.0560, label: 1, bag_size: 549\n",
      "batch 799, loss: 0.0029, instance_loss: 0.0088, weighted_loss: 0.0047, label: 0, bag_size: 1962\n",
      "batch 819, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 18944\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.980155109489051: correct 12891/13152\n",
      "class 1 clustering acc 0.9061739659367397: correct 5959/6576\n",
      "Epoch: 53, train_loss: 0.1690, train_clustering_loss:  0.1805, train_error: 0.0706\n",
      "class 0: acc 0.9323671497584541, correct 386/414\n",
      "class 1: acc 0.9264705882352942, correct 378/408\n",
      "\n",
      "Val Set, val_loss: 0.0955, val_error: 0.0217, auc: 0.9976\n",
      "class 0 clustering acc 0.9769021739130435: correct 1438/1472\n",
      "class 1 clustering acc 0.9225543478260869: correct 679/736\n",
      "class 0: acc 1.0, correct 38/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.1410, instance_loss: 0.0050, weighted_loss: 0.8002, label: 0, bag_size: 2918\n",
      "batch 39, loss: 0.0002, instance_loss: 0.0022, weighted_loss: 0.0008, label: 1, bag_size: 12349\n",
      "batch 59, loss: 0.0003, instance_loss: 0.0006, weighted_loss: 0.0004, label: 1, bag_size: 8602\n",
      "batch 79, loss: 0.0824, instance_loss: 0.0000, weighted_loss: 0.0577, label: 1, bag_size: 15689\n",
      "batch 99, loss: 2.6204, instance_loss: 0.3093, weighted_loss: 1.9271, label: 0, bag_size: 2694\n",
      "batch 119, loss: 0.0559, instance_loss: 0.0073, weighted_loss: 0.0413, label: 1, bag_size: 14230\n",
      "batch 139, loss: 3.3738, instance_loss: 4.3370, weighted_loss: 3.6628, label: 1, bag_size: 19439\n",
      "batch 159, loss: 0.0002, instance_loss: 0.1868, weighted_loss: 0.0561, label: 0, bag_size: 3787\n",
      "batch 179, loss: 0.0006, instance_loss: 0.0008, weighted_loss: 0.0007, label: 0, bag_size: 7709\n",
      "batch 199, loss: 0.3800, instance_loss: 0.0540, weighted_loss: 0.2822, label: 0, bag_size: 2070\n",
      "batch 219, loss: 0.0002, instance_loss: 0.2440, weighted_loss: 0.0733, label: 1, bag_size: 1743\n",
      "batch 239, loss: 0.0549, instance_loss: 0.0467, weighted_loss: 0.0525, label: 1, bag_size: 10492\n",
      "batch 259, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 8812\n",
      "batch 279, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 12137\n",
      "batch 299, loss: 0.0031, instance_loss: 0.0000, weighted_loss: 0.0021, label: 0, bag_size: 12593\n",
      "batch 319, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 14681\n",
      "batch 339, loss: 0.4264, instance_loss: 1.4996, weighted_loss: 0.7483, label: 1, bag_size: 8982\n",
      "batch 359, loss: 0.0251, instance_loss: 0.0173, weighted_loss: 0.0228, label: 0, bag_size: 21032\n",
      "batch 379, loss: 0.0094, instance_loss: 0.0326, weighted_loss: 0.0164, label: 0, bag_size: 1234\n",
      "batch 399, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 23996\n",
      "batch 419, loss: 0.0168, instance_loss: 0.0901, weighted_loss: 0.0388, label: 0, bag_size: 1614\n",
      "batch 439, loss: 0.0504, instance_loss: 0.0023, weighted_loss: 0.0359, label: 1, bag_size: 3450\n",
      "batch 459, loss: 0.0015, instance_loss: 0.2115, weighted_loss: 0.0645, label: 0, bag_size: 3265\n",
      "batch 479, loss: 1.6846, instance_loss: 0.5597, weighted_loss: 1.3471, label: 1, bag_size: 19470\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 1, bag_size: 11195\n",
      "batch 519, loss: 0.0712, instance_loss: 0.0691, weighted_loss: 0.0706, label: 0, bag_size: 1920\n",
      "batch 539, loss: 0.0032, instance_loss: 0.0688, weighted_loss: 0.0229, label: 1, bag_size: 7110\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0032, weighted_loss: 0.0010, label: 1, bag_size: 5833\n",
      "batch 579, loss: 0.0025, instance_loss: 0.0001, weighted_loss: 0.0018, label: 0, bag_size: 27158\n",
      "batch 599, loss: 0.0053, instance_loss: 0.0000, weighted_loss: 0.0037, label: 0, bag_size: 12593\n",
      "batch 619, loss: 0.0253, instance_loss: 0.1985, weighted_loss: 0.0773, label: 0, bag_size: 6652\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 1, bag_size: 11600\n",
      "batch 659, loss: 0.0015, instance_loss: 0.0056, weighted_loss: 0.0027, label: 0, bag_size: 9234\n",
      "batch 679, loss: 0.0002, instance_loss: 0.0053, weighted_loss: 0.0017, label: 1, bag_size: 10867\n",
      "batch 699, loss: 0.0520, instance_loss: 0.0116, weighted_loss: 0.0399, label: 1, bag_size: 5256\n",
      "batch 719, loss: 0.0585, instance_loss: 0.0273, weighted_loss: 0.0491, label: 1, bag_size: 1123\n",
      "batch 739, loss: 0.0094, instance_loss: 0.0143, weighted_loss: 0.0109, label: 0, bag_size: 2534\n",
      "batch 759, loss: 0.0020, instance_loss: 0.1159, weighted_loss: 0.0362, label: 1, bag_size: 2455\n",
      "batch 779, loss: 0.0254, instance_loss: 0.0318, weighted_loss: 0.0273, label: 0, bag_size: 2160\n",
      "batch 799, loss: 0.0016, instance_loss: 0.0113, weighted_loss: 0.0045, label: 1, bag_size: 10105\n",
      "batch 819, loss: 0.1700, instance_loss: 0.0336, weighted_loss: 0.1291, label: 0, bag_size: 11122\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9809914841849149: correct 12902/13152\n",
      "class 1 clustering acc 0.9168187347931873: correct 6029/6576\n",
      "Epoch: 54, train_loss: 0.1629, train_clustering_loss:  0.1710, train_error: 0.0596\n",
      "class 0: acc 0.9404466501240695, correct 379/403\n",
      "class 1: acc 0.9403341288782816, correct 394/419\n",
      "\n",
      "Val Set, val_loss: 0.0752, val_error: 0.0326, auc: 0.9976\n",
      "class 0 clustering acc 0.984375: correct 1449/1472\n",
      "class 1 clustering acc 0.9415760869565217: correct 693/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.9814814814814815, correct 53/54\n",
      "Validation loss decreased (0.079723 --> 0.075155).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0311, instance_loss: 0.0049, weighted_loss: 0.0232, label: 1, bag_size: 14887\n",
      "batch 39, loss: 0.0078, instance_loss: 0.0639, weighted_loss: 0.0246, label: 0, bag_size: 2382\n",
      "batch 59, loss: 0.0007, instance_loss: 0.0049, weighted_loss: 0.0020, label: 1, bag_size: 10346\n",
      "batch 79, loss: 0.0231, instance_loss: 0.0057, weighted_loss: 0.0179, label: 1, bag_size: 19500\n",
      "batch 99, loss: 0.0008, instance_loss: 0.0013, weighted_loss: 0.0010, label: 0, bag_size: 11900\n",
      "batch 119, loss: 0.0222, instance_loss: 0.0125, weighted_loss: 0.0193, label: 0, bag_size: 14739\n",
      "batch 139, loss: 0.0036, instance_loss: 0.0000, weighted_loss: 0.0026, label: 0, bag_size: 3876\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0024, weighted_loss: 0.0008, label: 1, bag_size: 7078\n",
      "batch 179, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 24911\n",
      "batch 199, loss: 0.0361, instance_loss: 0.0000, weighted_loss: 0.0253, label: 0, bag_size: 5999\n",
      "batch 219, loss: 0.0003, instance_loss: 0.0064, weighted_loss: 0.0021, label: 1, bag_size: 6164\n",
      "batch 239, loss: 0.0138, instance_loss: 0.1253, weighted_loss: 0.0472, label: 0, bag_size: 14625\n",
      "batch 259, loss: 0.0026, instance_loss: 0.0000, weighted_loss: 0.0018, label: 0, bag_size: 3710\n",
      "batch 279, loss: 0.0071, instance_loss: 0.0204, weighted_loss: 0.0111, label: 1, bag_size: 22286\n",
      "batch 299, loss: 0.0098, instance_loss: 0.0001, weighted_loss: 0.0069, label: 0, bag_size: 11900\n",
      "batch 319, loss: 0.0002, instance_loss: 0.0037, weighted_loss: 0.0012, label: 1, bag_size: 6317\n",
      "batch 339, loss: 0.0135, instance_loss: 0.0751, weighted_loss: 0.0320, label: 0, bag_size: 1891\n",
      "batch 359, loss: 0.0513, instance_loss: 0.0096, weighted_loss: 0.0388, label: 0, bag_size: 24382\n",
      "batch 379, loss: 0.0068, instance_loss: 0.0509, weighted_loss: 0.0201, label: 1, bag_size: 14223\n",
      "batch 399, loss: 0.0126, instance_loss: 0.0078, weighted_loss: 0.0111, label: 1, bag_size: 7515\n",
      "batch 419, loss: 0.0027, instance_loss: 0.0000, weighted_loss: 0.0019, label: 0, bag_size: 5965\n",
      "batch 439, loss: 0.0958, instance_loss: 0.9178, weighted_loss: 0.3424, label: 0, bag_size: 2458\n",
      "batch 459, loss: 0.0895, instance_loss: 0.0671, weighted_loss: 0.0828, label: 1, bag_size: 5903\n",
      "batch 479, loss: 0.7539, instance_loss: 0.5265, weighted_loss: 0.6857, label: 1, bag_size: 12494\n",
      "batch 499, loss: 0.0013, instance_loss: 0.1064, weighted_loss: 0.0329, label: 1, bag_size: 5340\n",
      "batch 519, loss: 0.0018, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 29270\n",
      "batch 539, loss: 0.0350, instance_loss: 0.0381, weighted_loss: 0.0359, label: 0, bag_size: 7923\n",
      "batch 559, loss: 0.1134, instance_loss: 0.0044, weighted_loss: 0.0807, label: 1, bag_size: 2356\n",
      "batch 579, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 22828\n",
      "batch 599, loss: 0.1174, instance_loss: 0.1045, weighted_loss: 0.1135, label: 0, bag_size: 16690\n",
      "batch 619, loss: 0.3007, instance_loss: 0.0213, weighted_loss: 0.2169, label: 1, bag_size: 1786\n",
      "batch 639, loss: 0.0112, instance_loss: 0.0135, weighted_loss: 0.0119, label: 1, bag_size: 2381\n",
      "batch 659, loss: 0.2690, instance_loss: 1.9426, weighted_loss: 0.7711, label: 0, bag_size: 1800\n",
      "batch 679, loss: 0.0147, instance_loss: 0.0014, weighted_loss: 0.0107, label: 0, bag_size: 13880\n",
      "batch 699, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 5965\n",
      "batch 719, loss: 0.0252, instance_loss: 0.0172, weighted_loss: 0.0228, label: 1, bag_size: 7371\n",
      "batch 739, loss: 0.1129, instance_loss: 0.0007, weighted_loss: 0.0792, label: 1, bag_size: 10432\n",
      "batch 759, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 4465\n",
      "batch 779, loss: 0.0037, instance_loss: 0.0895, weighted_loss: 0.0294, label: 0, bag_size: 2351\n",
      "batch 799, loss: 0.0067, instance_loss: 0.0594, weighted_loss: 0.0225, label: 1, bag_size: 10482\n",
      "batch 819, loss: 0.0331, instance_loss: 0.6723, weighted_loss: 0.2249, label: 1, bag_size: 1339\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.98213199513382: correct 12917/13152\n",
      "class 1 clustering acc 0.9215328467153284: correct 6060/6576\n",
      "Epoch: 55, train_loss: 0.1602, train_clustering_loss:  0.1645, train_error: 0.0657\n",
      "class 0: acc 0.9245742092457421, correct 380/411\n",
      "class 1: acc 0.9440389294403893, correct 388/411\n",
      "\n",
      "Val Set, val_loss: 0.1535, val_error: 0.0543, auc: 0.9981\n",
      "class 0 clustering acc 0.9809782608695652: correct 1444/1472\n",
      "class 1 clustering acc 0.9565217391304348: correct 704/736\n",
      "class 0: acc 1.0, correct 38/38\n",
      "class 1: acc 0.9074074074074074, correct 49/54\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0095, instance_loss: 0.0000, weighted_loss: 0.0066, label: 0, bag_size: 14739\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0243, weighted_loss: 0.0074, label: 1, bag_size: 1743\n",
      "batch 59, loss: 0.0905, instance_loss: 0.0349, weighted_loss: 0.0738, label: 1, bag_size: 15609\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 1984\n",
      "batch 99, loss: 0.3655, instance_loss: 0.5387, weighted_loss: 0.4175, label: 1, bag_size: 6171\n",
      "batch 119, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 10581\n",
      "batch 139, loss: 0.0105, instance_loss: 0.0614, weighted_loss: 0.0257, label: 0, bag_size: 1234\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0003, weighted_loss: 0.0002, label: 1, bag_size: 20537\n",
      "batch 179, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 9234\n",
      "batch 199, loss: 0.0271, instance_loss: 0.0727, weighted_loss: 0.0408, label: 1, bag_size: 12095\n",
      "batch 219, loss: 0.0016, instance_loss: 0.0970, weighted_loss: 0.0302, label: 0, bag_size: 3265\n",
      "batch 239, loss: 1.7889, instance_loss: 0.1185, weighted_loss: 1.2877, label: 1, bag_size: 13367\n",
      "batch 259, loss: 0.0113, instance_loss: 0.0000, weighted_loss: 0.0079, label: 1, bag_size: 11220\n",
      "batch 279, loss: 0.0046, instance_loss: 0.0001, weighted_loss: 0.0033, label: 0, bag_size: 7235\n",
      "batch 299, loss: 0.0342, instance_loss: 0.0296, weighted_loss: 0.0328, label: 1, bag_size: 12575\n",
      "batch 319, loss: 0.3845, instance_loss: 0.0014, weighted_loss: 0.2695, label: 0, bag_size: 18738\n",
      "batch 339, loss: 0.0173, instance_loss: 0.0222, weighted_loss: 0.0188, label: 0, bag_size: 10415\n",
      "batch 359, loss: 0.0040, instance_loss: 0.0000, weighted_loss: 0.0028, label: 0, bag_size: 2844\n",
      "batch 379, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 21864\n",
      "batch 399, loss: 0.0855, instance_loss: 0.0032, weighted_loss: 0.0609, label: 0, bag_size: 11922\n",
      "batch 419, loss: 0.5251, instance_loss: 0.0137, weighted_loss: 0.3717, label: 1, bag_size: 8395\n",
      "batch 439, loss: 0.0003, instance_loss: 0.0438, weighted_loss: 0.0133, label: 1, bag_size: 6164\n",
      "batch 459, loss: 0.2202, instance_loss: 0.0098, weighted_loss: 0.1571, label: 1, bag_size: 7583\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0063, weighted_loss: 0.0020, label: 1, bag_size: 7078\n",
      "batch 499, loss: 0.0316, instance_loss: 0.0003, weighted_loss: 0.0222, label: 0, bag_size: 18777\n",
      "batch 519, loss: 0.0265, instance_loss: 0.0476, weighted_loss: 0.0328, label: 1, bag_size: 1867\n",
      "batch 539, loss: 0.0070, instance_loss: 0.1194, weighted_loss: 0.0407, label: 0, bag_size: 2063\n",
      "batch 559, loss: 0.0013, instance_loss: 0.0622, weighted_loss: 0.0196, label: 0, bag_size: 13691\n",
      "batch 579, loss: 0.3298, instance_loss: 0.3222, weighted_loss: 0.3275, label: 1, bag_size: 1831\n",
      "batch 599, loss: 0.0029, instance_loss: 0.0000, weighted_loss: 0.0020, label: 0, bag_size: 8898\n",
      "batch 619, loss: 0.0059, instance_loss: 0.0587, weighted_loss: 0.0217, label: 1, bag_size: 2790\n",
      "batch 639, loss: 0.0868, instance_loss: 0.0008, weighted_loss: 0.0610, label: 0, bag_size: 2269\n",
      "batch 659, loss: 0.0032, instance_loss: 0.0005, weighted_loss: 0.0024, label: 0, bag_size: 8981\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0612, weighted_loss: 0.0184, label: 1, bag_size: 16512\n",
      "batch 699, loss: 0.0005, instance_loss: 0.0002, weighted_loss: 0.0004, label: 1, bag_size: 10558\n",
      "batch 719, loss: 0.0028, instance_loss: 0.0004, weighted_loss: 0.0021, label: 0, bag_size: 10535\n",
      "batch 739, loss: 0.0056, instance_loss: 0.0000, weighted_loss: 0.0039, label: 1, bag_size: 11032\n",
      "batch 759, loss: 0.0027, instance_loss: 0.0000, weighted_loss: 0.0019, label: 0, bag_size: 25027\n",
      "batch 779, loss: 0.0318, instance_loss: 0.1230, weighted_loss: 0.0592, label: 1, bag_size: 6928\n",
      "batch 799, loss: 0.0101, instance_loss: 0.2497, weighted_loss: 0.0819, label: 0, bag_size: 9583\n",
      "batch 819, loss: 0.2075, instance_loss: 0.0073, weighted_loss: 0.1474, label: 1, bag_size: 10432\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9806113138686131: correct 12897/13152\n",
      "class 1 clustering acc 0.9116484184914841: correct 5995/6576\n",
      "Epoch: 56, train_loss: 0.1431, train_clustering_loss:  0.1861, train_error: 0.0523\n",
      "class 0: acc 0.9360613810741688, correct 366/391\n",
      "class 1: acc 0.9582366589327146, correct 413/431\n",
      "\n",
      "Val Set, val_loss: 0.0938, val_error: 0.0217, auc: 0.9976\n",
      "class 0 clustering acc 0.9823369565217391: correct 1446/1472\n",
      "class 1 clustering acc 0.9089673913043478: correct 669/736\n",
      "class 0: acc 1.0, correct 38/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0437, instance_loss: 0.5555, weighted_loss: 0.1972, label: 1, bag_size: 1095\n",
      "batch 39, loss: 0.0032, instance_loss: 0.1004, weighted_loss: 0.0324, label: 1, bag_size: 2140\n",
      "batch 59, loss: 0.0465, instance_loss: 0.0520, weighted_loss: 0.0481, label: 1, bag_size: 9983\n",
      "batch 79, loss: 0.3826, instance_loss: 0.0032, weighted_loss: 0.2688, label: 1, bag_size: 10591\n",
      "batch 99, loss: 1.0741, instance_loss: 0.0025, weighted_loss: 0.7526, label: 1, bag_size: 7748\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0782, weighted_loss: 0.0235, label: 1, bag_size: 2136\n",
      "batch 139, loss: 1.1649, instance_loss: 0.3941, weighted_loss: 0.9337, label: 0, bag_size: 14664\n",
      "batch 159, loss: 0.0497, instance_loss: 0.0035, weighted_loss: 0.0358, label: 0, bag_size: 11281\n",
      "batch 179, loss: 0.1486, instance_loss: 0.0140, weighted_loss: 0.1082, label: 1, bag_size: 1483\n",
      "batch 199, loss: 0.5897, instance_loss: 0.1207, weighted_loss: 0.4490, label: 1, bag_size: 1683\n",
      "batch 219, loss: 0.3246, instance_loss: 0.4564, weighted_loss: 0.3641, label: 0, bag_size: 20555\n",
      "batch 239, loss: 0.0025, instance_loss: 0.0000, weighted_loss: 0.0017, label: 0, bag_size: 12201\n",
      "batch 259, loss: 0.0047, instance_loss: 0.0000, weighted_loss: 0.0033, label: 0, bag_size: 3710\n",
      "batch 279, loss: 0.0323, instance_loss: 0.3923, weighted_loss: 0.1403, label: 0, bag_size: 4523\n",
      "batch 299, loss: 0.0039, instance_loss: 0.0000, weighted_loss: 0.0027, label: 0, bag_size: 22426\n",
      "batch 319, loss: 0.1919, instance_loss: 0.2225, weighted_loss: 0.2010, label: 0, bag_size: 6884\n",
      "batch 339, loss: 2.1044, instance_loss: 2.7490, weighted_loss: 2.2978, label: 0, bag_size: 1714\n",
      "batch 359, loss: 0.0069, instance_loss: 0.0054, weighted_loss: 0.0065, label: 1, bag_size: 2936\n",
      "batch 379, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 25027\n",
      "batch 399, loss: 0.0067, instance_loss: 0.0116, weighted_loss: 0.0082, label: 1, bag_size: 3409\n",
      "batch 419, loss: 0.0018, instance_loss: 0.0070, weighted_loss: 0.0034, label: 1, bag_size: 14433\n",
      "batch 439, loss: 0.0019, instance_loss: 0.0028, weighted_loss: 0.0022, label: 1, bag_size: 8522\n",
      "batch 459, loss: 1.0003, instance_loss: 1.5449, weighted_loss: 1.1637, label: 0, bag_size: 1592\n",
      "batch 479, loss: 0.0005, instance_loss: 0.0055, weighted_loss: 0.0020, label: 1, bag_size: 10392\n",
      "batch 499, loss: 0.0015, instance_loss: 0.0015, weighted_loss: 0.0015, label: 1, bag_size: 7371\n",
      "batch 519, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 8040\n",
      "batch 539, loss: 0.0054, instance_loss: 0.0000, weighted_loss: 0.0038, label: 1, bag_size: 13015\n",
      "batch 559, loss: 0.0063, instance_loss: 0.0030, weighted_loss: 0.0053, label: 1, bag_size: 6928\n",
      "batch 579, loss: 0.0252, instance_loss: 0.0000, weighted_loss: 0.0176, label: 0, bag_size: 9542\n",
      "batch 599, loss: 0.0318, instance_loss: 0.0000, weighted_loss: 0.0223, label: 0, bag_size: 18415\n",
      "batch 619, loss: 0.2165, instance_loss: 0.0009, weighted_loss: 0.1518, label: 0, bag_size: 11122\n",
      "batch 639, loss: 0.0154, instance_loss: 0.1500, weighted_loss: 0.0558, label: 1, bag_size: 10912\n",
      "batch 659, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 21576\n",
      "batch 679, loss: 0.0105, instance_loss: 0.0000, weighted_loss: 0.0074, label: 0, bag_size: 14305\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 13947\n",
      "batch 719, loss: 0.0067, instance_loss: 0.0000, weighted_loss: 0.0047, label: 0, bag_size: 14625\n",
      "batch 739, loss: 0.0000, instance_loss: 0.2014, weighted_loss: 0.0604, label: 1, bag_size: 4102\n",
      "batch 759, loss: 0.0001, instance_loss: 0.0052, weighted_loss: 0.0017, label: 1, bag_size: 14223\n",
      "batch 779, loss: 0.3248, instance_loss: 0.4827, weighted_loss: 0.3722, label: 1, bag_size: 9404\n",
      "batch 799, loss: 0.0200, instance_loss: 0.0000, weighted_loss: 0.0140, label: 0, bag_size: 6898\n",
      "batch 819, loss: 0.0079, instance_loss: 0.0153, weighted_loss: 0.0101, label: 1, bag_size: 14887\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.982816301703163: correct 12926/13152\n",
      "class 1 clustering acc 0.9239659367396593: correct 6076/6576\n",
      "Epoch: 57, train_loss: 0.1631, train_clustering_loss:  0.1570, train_error: 0.0669\n",
      "class 0: acc 0.935064935064935, correct 360/385\n",
      "class 1: acc 0.931350114416476, correct 407/437\n",
      "\n",
      "Val Set, val_loss: 0.0980, val_error: 0.0435, auc: 0.9966\n",
      "class 0 clustering acc 0.967391304347826: correct 1424/1472\n",
      "class 1 clustering acc 0.90625: correct 667/736\n",
      "class 0: acc 0.9210526315789473, correct 35/38\n",
      "class 1: acc 0.9814814814814815, correct 53/54\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0028, instance_loss: 0.0000, weighted_loss: 0.0020, label: 0, bag_size: 3970\n",
      "batch 39, loss: 0.0047, instance_loss: 0.0023, weighted_loss: 0.0040, label: 1, bag_size: 22286\n",
      "batch 59, loss: 0.4280, instance_loss: 0.3074, weighted_loss: 0.3918, label: 1, bag_size: 771\n",
      "batch 79, loss: 0.6414, instance_loss: 1.0966, weighted_loss: 0.7780, label: 0, bag_size: 8744\n",
      "batch 99, loss: 0.0178, instance_loss: 0.0188, weighted_loss: 0.0181, label: 1, bag_size: 3674\n",
      "batch 119, loss: 0.1089, instance_loss: 0.0002, weighted_loss: 0.0763, label: 0, bag_size: 20230\n",
      "batch 139, loss: 1.1220, instance_loss: 1.4269, weighted_loss: 1.2135, label: 0, bag_size: 15898\n",
      "batch 159, loss: 0.0153, instance_loss: 0.0587, weighted_loss: 0.0283, label: 1, bag_size: 21701\n",
      "batch 179, loss: 1.4602, instance_loss: 0.2040, weighted_loss: 1.0833, label: 0, bag_size: 7428\n",
      "batch 199, loss: 0.0044, instance_loss: 0.0632, weighted_loss: 0.0220, label: 1, bag_size: 14433\n",
      "batch 219, loss: 0.0006, instance_loss: 0.0695, weighted_loss: 0.0213, label: 0, bag_size: 1824\n",
      "batch 239, loss: 0.0383, instance_loss: 0.1152, weighted_loss: 0.0614, label: 0, bag_size: 15071\n",
      "batch 259, loss: 0.4228, instance_loss: 0.1474, weighted_loss: 0.3402, label: 0, bag_size: 1370\n",
      "batch 279, loss: 0.0002, instance_loss: 0.0521, weighted_loss: 0.0158, label: 0, bag_size: 4465\n",
      "batch 299, loss: 0.0170, instance_loss: 0.0000, weighted_loss: 0.0119, label: 0, bag_size: 10365\n",
      "batch 319, loss: 2.0246, instance_loss: 0.0268, weighted_loss: 1.4253, label: 0, bag_size: 5211\n",
      "batch 339, loss: 0.0005, instance_loss: 0.0099, weighted_loss: 0.0033, label: 1, bag_size: 3634\n",
      "batch 359, loss: 0.0010, instance_loss: 0.0031, weighted_loss: 0.0017, label: 1, bag_size: 9756\n",
      "batch 379, loss: 0.0005, instance_loss: 0.0304, weighted_loss: 0.0094, label: 1, bag_size: 12349\n",
      "batch 399, loss: 0.0355, instance_loss: 0.0078, weighted_loss: 0.0272, label: 1, bag_size: 2522\n",
      "batch 419, loss: 0.0360, instance_loss: 0.0160, weighted_loss: 0.0300, label: 1, bag_size: 3968\n",
      "batch 439, loss: 0.0125, instance_loss: 0.0025, weighted_loss: 0.0095, label: 1, bag_size: 10912\n",
      "batch 459, loss: 0.1490, instance_loss: 2.8117, weighted_loss: 0.9478, label: 0, bag_size: 2213\n",
      "batch 479, loss: 0.0047, instance_loss: 0.0000, weighted_loss: 0.0033, label: 0, bag_size: 2303\n",
      "batch 499, loss: 0.0248, instance_loss: 0.0002, weighted_loss: 0.0174, label: 0, bag_size: 31085\n",
      "batch 519, loss: 0.0050, instance_loss: 0.0116, weighted_loss: 0.0070, label: 1, bag_size: 16051\n",
      "batch 539, loss: 0.0017, instance_loss: 0.1388, weighted_loss: 0.0428, label: 0, bag_size: 890\n",
      "batch 559, loss: 3.7421, instance_loss: 2.0546, weighted_loss: 3.2359, label: 0, bag_size: 2732\n",
      "batch 579, loss: 0.0655, instance_loss: 0.0246, weighted_loss: 0.0533, label: 1, bag_size: 2522\n",
      "batch 599, loss: 0.0164, instance_loss: 0.0342, weighted_loss: 0.0217, label: 1, bag_size: 13255\n",
      "batch 619, loss: 0.0461, instance_loss: 2.0023, weighted_loss: 0.6329, label: 0, bag_size: 2732\n",
      "batch 639, loss: 0.5073, instance_loss: 0.0357, weighted_loss: 0.3658, label: 1, bag_size: 1764\n",
      "batch 659, loss: 0.0003, instance_loss: 0.0062, weighted_loss: 0.0020, label: 1, bag_size: 12795\n",
      "batch 679, loss: 0.0006, instance_loss: 0.0001, weighted_loss: 0.0005, label: 0, bag_size: 1639\n",
      "batch 699, loss: 0.0670, instance_loss: 0.0997, weighted_loss: 0.0768, label: 0, bag_size: 4997\n",
      "batch 719, loss: 0.0207, instance_loss: 0.0591, weighted_loss: 0.0322, label: 0, bag_size: 3774\n",
      "batch 739, loss: 0.1045, instance_loss: 0.2493, weighted_loss: 0.1479, label: 1, bag_size: 6726\n",
      "batch 759, loss: 0.0095, instance_loss: 0.0002, weighted_loss: 0.0067, label: 1, bag_size: 5023\n",
      "batch 779, loss: 0.6041, instance_loss: 0.1545, weighted_loss: 0.4692, label: 1, bag_size: 12712\n",
      "batch 799, loss: 0.0003, instance_loss: 0.0019, weighted_loss: 0.0008, label: 1, bag_size: 13365\n",
      "batch 819, loss: 0.0433, instance_loss: 0.0021, weighted_loss: 0.0309, label: 1, bag_size: 8264\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9803071776155717: correct 12893/13152\n",
      "class 1 clustering acc 0.9063260340632603: correct 5960/6576\n",
      "Epoch: 58, train_loss: 0.1647, train_clustering_loss:  0.1879, train_error: 0.0608\n",
      "class 0: acc 0.9371980676328503, correct 388/414\n",
      "class 1: acc 0.9411764705882353, correct 384/408\n",
      "\n",
      "Val Set, val_loss: 0.0926, val_error: 0.0543, auc: 0.9956\n",
      "class 0 clustering acc 0.9823369565217391: correct 1446/1472\n",
      "class 1 clustering acc 0.9347826086956522: correct 688/736\n",
      "class 0: acc 0.9210526315789473, correct 35/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0191, weighted_loss: 0.0059, label: 1, bag_size: 15008\n",
      "batch 39, loss: 0.0071, instance_loss: 0.0000, weighted_loss: 0.0050, label: 0, bag_size: 13205\n",
      "batch 59, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 1213\n",
      "batch 79, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 12137\n",
      "batch 99, loss: 0.0383, instance_loss: 0.0000, weighted_loss: 0.0268, label: 0, bag_size: 15003\n",
      "batch 119, loss: 0.1193, instance_loss: 0.1268, weighted_loss: 0.1216, label: 0, bag_size: 15057\n",
      "batch 139, loss: 0.0340, instance_loss: 0.0001, weighted_loss: 0.0238, label: 0, bag_size: 13602\n",
      "batch 159, loss: 2.7622, instance_loss: 0.2121, weighted_loss: 1.9972, label: 1, bag_size: 25831\n",
      "batch 179, loss: 0.0000, instance_loss: 0.1800, weighted_loss: 0.0540, label: 1, bag_size: 3295\n",
      "batch 199, loss: 0.0069, instance_loss: 0.0003, weighted_loss: 0.0049, label: 1, bag_size: 2638\n",
      "batch 219, loss: 0.0670, instance_loss: 0.1832, weighted_loss: 0.1019, label: 1, bag_size: 1794\n",
      "batch 239, loss: 0.0196, instance_loss: 0.2083, weighted_loss: 0.0762, label: 1, bag_size: 2356\n",
      "batch 259, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 21076\n",
      "batch 279, loss: 0.0059, instance_loss: 0.4200, weighted_loss: 0.1301, label: 1, bag_size: 25695\n",
      "batch 299, loss: 0.0172, instance_loss: 0.0233, weighted_loss: 0.0190, label: 1, bag_size: 8680\n",
      "batch 319, loss: 0.2961, instance_loss: 0.1481, weighted_loss: 0.2517, label: 0, bag_size: 1953\n",
      "batch 339, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 21138\n",
      "batch 359, loss: 0.0472, instance_loss: 0.6531, weighted_loss: 0.2289, label: 0, bag_size: 1920\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0504, weighted_loss: 0.0151, label: 1, bag_size: 1360\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0128, weighted_loss: 0.0038, label: 1, bag_size: 2485\n",
      "batch 419, loss: 0.0066, instance_loss: 0.0249, weighted_loss: 0.0121, label: 1, bag_size: 5763\n",
      "batch 439, loss: 0.0004, instance_loss: 0.0151, weighted_loss: 0.0049, label: 1, bag_size: 3980\n",
      "batch 459, loss: 0.0068, instance_loss: 0.0248, weighted_loss: 0.0122, label: 1, bag_size: 5160\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0722, weighted_loss: 0.0217, label: 1, bag_size: 6533\n",
      "batch 499, loss: 0.0095, instance_loss: 0.0420, weighted_loss: 0.0193, label: 1, bag_size: 9649\n",
      "batch 519, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 18944\n",
      "batch 539, loss: 1.0110, instance_loss: 0.0720, weighted_loss: 0.7293, label: 0, bag_size: 14893\n",
      "batch 559, loss: 0.3455, instance_loss: 0.4489, weighted_loss: 0.3765, label: 0, bag_size: 2270\n",
      "batch 579, loss: 1.3409, instance_loss: 2.0722, weighted_loss: 1.5603, label: 1, bag_size: 15185\n",
      "batch 599, loss: 0.0281, instance_loss: 0.0073, weighted_loss: 0.0219, label: 1, bag_size: 9519\n",
      "batch 619, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 11778\n",
      "batch 639, loss: 0.1080, instance_loss: 0.0000, weighted_loss: 0.0756, label: 0, bag_size: 65728\n",
      "batch 659, loss: 2.2286, instance_loss: 0.5107, weighted_loss: 1.7132, label: 1, bag_size: 1845\n",
      "batch 679, loss: 0.0003, instance_loss: 0.0750, weighted_loss: 0.0227, label: 1, bag_size: 18649\n",
      "batch 699, loss: 0.1094, instance_loss: 0.0348, weighted_loss: 0.0870, label: 1, bag_size: 16514\n",
      "batch 719, loss: 0.0587, instance_loss: 0.0455, weighted_loss: 0.0547, label: 1, bag_size: 10460\n",
      "batch 739, loss: 0.0637, instance_loss: 0.0282, weighted_loss: 0.0530, label: 1, bag_size: 10622\n",
      "batch 759, loss: 0.0050, instance_loss: 0.0564, weighted_loss: 0.0204, label: 1, bag_size: 8602\n",
      "batch 779, loss: 3.5279, instance_loss: 3.6513, weighted_loss: 3.5649, label: 0, bag_size: 7239\n",
      "batch 799, loss: 0.1402, instance_loss: 2.2051, weighted_loss: 0.7597, label: 1, bag_size: 12626\n",
      "batch 819, loss: 0.0023, instance_loss: 0.0024, weighted_loss: 0.0023, label: 1, bag_size: 28527\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9784823600973236: correct 12869/13152\n",
      "class 1 clustering acc 0.8932481751824818: correct 5874/6576\n",
      "Epoch: 59, train_loss: 0.1755, train_clustering_loss:  0.2074, train_error: 0.0645\n",
      "class 0: acc 0.9375, correct 375/400\n",
      "class 1: acc 0.933649289099526, correct 394/422\n",
      "\n",
      "Val Set, val_loss: 0.0919, val_error: 0.0326, auc: 0.9961\n",
      "class 0 clustering acc 0.9789402173913043: correct 1441/1472\n",
      "class 1 clustering acc 0.8845108695652174: correct 651/736\n",
      "class 0: acc 0.9736842105263158, correct 37/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.8383, instance_loss: 0.8676, weighted_loss: 1.5471, label: 1, bag_size: 19439\n",
      "batch 39, loss: 0.2144, instance_loss: 0.5220, weighted_loss: 0.3067, label: 1, bag_size: 3879\n",
      "batch 59, loss: 1.0831, instance_loss: 1.4530, weighted_loss: 1.1941, label: 1, bag_size: 2935\n",
      "batch 79, loss: 0.0687, instance_loss: 0.6690, weighted_loss: 0.2488, label: 1, bag_size: 1242\n",
      "batch 99, loss: 0.0255, instance_loss: 0.0609, weighted_loss: 0.0361, label: 0, bag_size: 7612\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0064, weighted_loss: 0.0019, label: 0, bag_size: 23037\n",
      "batch 139, loss: 0.0036, instance_loss: 0.0043, weighted_loss: 0.0039, label: 1, bag_size: 2193\n",
      "batch 159, loss: 0.1662, instance_loss: 0.2101, weighted_loss: 0.1794, label: 0, bag_size: 47866\n",
      "batch 179, loss: 0.0131, instance_loss: 0.0278, weighted_loss: 0.0175, label: 0, bag_size: 12910\n",
      "batch 199, loss: 0.0116, instance_loss: 0.0199, weighted_loss: 0.0141, label: 0, bag_size: 931\n",
      "batch 219, loss: 0.0011, instance_loss: 0.0006, weighted_loss: 0.0009, label: 0, bag_size: 24911\n",
      "batch 239, loss: 0.0006, instance_loss: 0.9479, weighted_loss: 0.2848, label: 1, bag_size: 928\n",
      "batch 259, loss: 0.2083, instance_loss: 0.0615, weighted_loss: 0.1643, label: 0, bag_size: 9597\n",
      "batch 279, loss: 0.0007, instance_loss: 0.0017, weighted_loss: 0.0010, label: 1, bag_size: 11600\n",
      "batch 299, loss: 0.0136, instance_loss: 0.0090, weighted_loss: 0.0122, label: 1, bag_size: 1759\n",
      "batch 319, loss: 0.1052, instance_loss: 0.0670, weighted_loss: 0.0937, label: 0, bag_size: 6884\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 11113\n",
      "batch 359, loss: 0.0003, instance_loss: 0.0010, weighted_loss: 0.0005, label: 0, bag_size: 31780\n",
      "batch 379, loss: 0.2926, instance_loss: 0.0892, weighted_loss: 0.2316, label: 0, bag_size: 2098\n",
      "batch 399, loss: 1.2989, instance_loss: 0.0370, weighted_loss: 0.9204, label: 0, bag_size: 5120\n",
      "batch 419, loss: 0.0047, instance_loss: 0.1603, weighted_loss: 0.0514, label: 0, bag_size: 705\n",
      "batch 439, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 15747\n",
      "batch 459, loss: 0.0290, instance_loss: 0.0009, weighted_loss: 0.0206, label: 0, bag_size: 17083\n",
      "batch 479, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 1, bag_size: 2678\n",
      "batch 499, loss: 0.0902, instance_loss: 0.2203, weighted_loss: 0.1292, label: 0, bag_size: 1831\n",
      "batch 519, loss: 0.0002, instance_loss: 0.0012, weighted_loss: 0.0005, label: 1, bag_size: 12349\n",
      "batch 539, loss: 0.1439, instance_loss: 0.3090, weighted_loss: 0.1934, label: 0, bag_size: 10721\n",
      "batch 559, loss: 0.0000, instance_loss: 0.2915, weighted_loss: 0.0875, label: 1, bag_size: 14515\n",
      "batch 579, loss: 0.0216, instance_loss: 0.4889, weighted_loss: 0.1618, label: 0, bag_size: 1772\n",
      "batch 599, loss: 1.3716, instance_loss: 0.5578, weighted_loss: 1.1275, label: 0, bag_size: 9132\n",
      "batch 619, loss: 0.0010, instance_loss: 0.0165, weighted_loss: 0.0056, label: 0, bag_size: 1072\n",
      "batch 639, loss: 0.0486, instance_loss: 0.0544, weighted_loss: 0.0503, label: 0, bag_size: 7989\n",
      "batch 659, loss: 0.0171, instance_loss: 0.0141, weighted_loss: 0.0162, label: 0, bag_size: 4523\n",
      "batch 679, loss: 0.4176, instance_loss: 0.1784, weighted_loss: 0.3458, label: 0, bag_size: 11122\n",
      "batch 699, loss: 0.0005, instance_loss: 0.1260, weighted_loss: 0.0381, label: 1, bag_size: 1339\n",
      "batch 719, loss: 0.0185, instance_loss: 0.0997, weighted_loss: 0.0429, label: 0, bag_size: 1831\n",
      "batch 739, loss: 0.0913, instance_loss: 0.1893, weighted_loss: 0.1207, label: 1, bag_size: 5292\n",
      "batch 759, loss: 0.0169, instance_loss: 0.0095, weighted_loss: 0.0147, label: 1, bag_size: 6776\n",
      "batch 779, loss: 0.0203, instance_loss: 0.2038, weighted_loss: 0.0754, label: 1, bag_size: 5921\n",
      "batch 799, loss: 0.0452, instance_loss: 0.0036, weighted_loss: 0.0327, label: 1, bag_size: 9942\n",
      "batch 819, loss: 0.1246, instance_loss: 0.0207, weighted_loss: 0.0934, label: 1, bag_size: 7768\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9835006082725061: correct 12935/13152\n",
      "class 1 clustering acc 0.9154501216545012: correct 6020/6576\n",
      "Epoch: 60, train_loss: 0.1712, train_clustering_loss:  0.1591, train_error: 0.0742\n",
      "class 0: acc 0.9284009546539379, correct 389/419\n",
      "class 1: acc 0.9230769230769231, correct 372/403\n",
      "\n",
      "Val Set, val_loss: 0.0991, val_error: 0.0543, auc: 0.9951\n",
      "class 0 clustering acc 0.9735054347826086: correct 1433/1472\n",
      "class 1 clustering acc 0.9103260869565217: correct 670/736\n",
      "class 0: acc 0.9210526315789473, correct 35/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0006, instance_loss: 0.0001, weighted_loss: 0.0004, label: 0, bag_size: 9252\n",
      "batch 39, loss: 0.0229, instance_loss: 0.0406, weighted_loss: 0.0282, label: 0, bag_size: 10814\n",
      "batch 59, loss: 0.0472, instance_loss: 0.0377, weighted_loss: 0.0443, label: 1, bag_size: 5605\n",
      "batch 79, loss: 0.0203, instance_loss: 0.0071, weighted_loss: 0.0164, label: 1, bag_size: 4423\n",
      "batch 99, loss: 0.0607, instance_loss: 0.9282, weighted_loss: 0.3209, label: 0, bag_size: 20555\n",
      "batch 119, loss: 0.0141, instance_loss: 0.1330, weighted_loss: 0.0498, label: 0, bag_size: 2360\n",
      "batch 139, loss: 0.0254, instance_loss: 0.0028, weighted_loss: 0.0186, label: 1, bag_size: 7445\n",
      "batch 159, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 21404\n",
      "batch 179, loss: 0.0051, instance_loss: 0.0001, weighted_loss: 0.0036, label: 1, bag_size: 17579\n",
      "batch 199, loss: 0.6686, instance_loss: 0.3246, weighted_loss: 0.5654, label: 0, bag_size: 2815\n",
      "batch 219, loss: 0.0009, instance_loss: 0.0021, weighted_loss: 0.0012, label: 1, bag_size: 3937\n",
      "batch 239, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 22800\n",
      "batch 259, loss: 0.0088, instance_loss: 0.0000, weighted_loss: 0.0061, label: 0, bag_size: 15636\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 1, bag_size: 1360\n",
      "batch 299, loss: 0.0075, instance_loss: 0.0048, weighted_loss: 0.0067, label: 1, bag_size: 12719\n",
      "batch 319, loss: 3.1760, instance_loss: 0.2242, weighted_loss: 2.2905, label: 1, bag_size: 25831\n",
      "batch 339, loss: 0.2782, instance_loss: 0.0564, weighted_loss: 0.2116, label: 0, bag_size: 3654\n",
      "batch 359, loss: 0.0035, instance_loss: 0.0006, weighted_loss: 0.0026, label: 0, bag_size: 8788\n",
      "batch 379, loss: 0.0426, instance_loss: 0.0021, weighted_loss: 0.0305, label: 1, bag_size: 13026\n",
      "batch 399, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 18240\n",
      "batch 419, loss: 0.0883, instance_loss: 0.0007, weighted_loss: 0.0620, label: 0, bag_size: 14333\n",
      "batch 439, loss: 0.1158, instance_loss: 0.0387, weighted_loss: 0.0927, label: 0, bag_size: 2270\n",
      "batch 459, loss: 0.0017, instance_loss: 0.0000, weighted_loss: 0.0012, label: 1, bag_size: 3937\n",
      "batch 479, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 12793\n",
      "batch 499, loss: 0.0568, instance_loss: 0.0000, weighted_loss: 0.0398, label: 0, bag_size: 18738\n",
      "batch 519, loss: 0.3570, instance_loss: 0.0014, weighted_loss: 0.2503, label: 0, bag_size: 21361\n",
      "batch 539, loss: 0.0030, instance_loss: 0.0000, weighted_loss: 0.0021, label: 1, bag_size: 4128\n",
      "batch 559, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 11917\n",
      "batch 579, loss: 0.0962, instance_loss: 0.1931, weighted_loss: 0.1253, label: 1, bag_size: 12180\n",
      "batch 599, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0015, label: 1, bag_size: 11884\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15841\n",
      "batch 639, loss: 0.0017, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 10942\n",
      "batch 659, loss: 0.0001, instance_loss: 0.0039, weighted_loss: 0.0013, label: 1, bag_size: 629\n",
      "batch 679, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 31085\n",
      "batch 699, loss: 0.0006, instance_loss: 0.0009, weighted_loss: 0.0007, label: 1, bag_size: 4102\n",
      "batch 719, loss: 0.0018, instance_loss: 0.0003, weighted_loss: 0.0013, label: 0, bag_size: 15636\n",
      "batch 739, loss: 0.1016, instance_loss: 0.0087, weighted_loss: 0.0738, label: 1, bag_size: 10848\n",
      "batch 759, loss: 0.0032, instance_loss: 0.0000, weighted_loss: 0.0022, label: 0, bag_size: 8898\n",
      "batch 779, loss: 0.0007, instance_loss: 0.1233, weighted_loss: 0.0375, label: 1, bag_size: 3450\n",
      "batch 799, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 15850\n",
      "batch 819, loss: 0.0192, instance_loss: 0.0104, weighted_loss: 0.0166, label: 0, bag_size: 13602\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9822080291970803: correct 12918/13152\n",
      "class 1 clustering acc 0.9105839416058394: correct 5988/6576\n",
      "Epoch: 61, train_loss: 0.1383, train_clustering_loss:  0.1786, train_error: 0.0633\n",
      "class 0: acc 0.9339853300733496, correct 382/409\n",
      "class 1: acc 0.9394673123486683, correct 388/413\n",
      "\n",
      "Val Set, val_loss: 0.1063, val_error: 0.0326, auc: 0.9937\n",
      "class 0 clustering acc 0.9660326086956522: correct 1422/1472\n",
      "class 1 clustering acc 0.8505434782608695: correct 626/736\n",
      "class 0: acc 0.9736842105263158, correct 37/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0021, instance_loss: 0.0239, weighted_loss: 0.0086, label: 0, bag_size: 1120\n",
      "batch 39, loss: 0.0040, instance_loss: 0.0208, weighted_loss: 0.0090, label: 0, bag_size: 15001\n",
      "batch 59, loss: 0.1204, instance_loss: 0.0240, weighted_loss: 0.0915, label: 0, bag_size: 21032\n",
      "batch 79, loss: 0.1361, instance_loss: 0.0000, weighted_loss: 0.0953, label: 1, bag_size: 8012\n",
      "batch 99, loss: 0.0183, instance_loss: 0.0000, weighted_loss: 0.0128, label: 0, bag_size: 3198\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0014, weighted_loss: 0.0005, label: 1, bag_size: 15233\n",
      "batch 139, loss: 0.5773, instance_loss: 0.0001, weighted_loss: 0.4041, label: 0, bag_size: 5211\n",
      "batch 159, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 13795\n",
      "batch 179, loss: 0.7407, instance_loss: 1.2194, weighted_loss: 0.8843, label: 1, bag_size: 1845\n",
      "batch 199, loss: 0.0432, instance_loss: 0.9399, weighted_loss: 0.3122, label: 0, bag_size: 705\n",
      "batch 219, loss: 0.0985, instance_loss: 0.0010, weighted_loss: 0.0693, label: 1, bag_size: 4939\n",
      "batch 239, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 15464\n",
      "batch 259, loss: 0.4965, instance_loss: 0.4965, weighted_loss: 0.4965, label: 1, bag_size: 9215\n",
      "batch 279, loss: 0.0037, instance_loss: 0.0082, weighted_loss: 0.0050, label: 1, bag_size: 4423\n",
      "batch 299, loss: 0.0031, instance_loss: 0.0075, weighted_loss: 0.0044, label: 1, bag_size: 8522\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0046, weighted_loss: 0.0014, label: 1, bag_size: 3437\n",
      "batch 339, loss: 0.0031, instance_loss: 0.0002, weighted_loss: 0.0022, label: 0, bag_size: 7235\n",
      "batch 359, loss: 0.0006, instance_loss: 0.0986, weighted_loss: 0.0300, label: 1, bag_size: 8935\n",
      "batch 379, loss: 0.1046, instance_loss: 0.0000, weighted_loss: 0.0732, label: 0, bag_size: 10942\n",
      "batch 399, loss: 0.0210, instance_loss: 0.0787, weighted_loss: 0.0383, label: 1, bag_size: 10492\n",
      "batch 419, loss: 0.0039, instance_loss: 0.2542, weighted_loss: 0.0790, label: 1, bag_size: 6950\n",
      "batch 439, loss: 0.0443, instance_loss: 0.2830, weighted_loss: 0.1159, label: 1, bag_size: 10396\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0743, weighted_loss: 0.0223, label: 1, bag_size: 16512\n",
      "batch 479, loss: 0.0112, instance_loss: 0.0414, weighted_loss: 0.0203, label: 1, bag_size: 6842\n",
      "batch 499, loss: 0.0119, instance_loss: 0.0508, weighted_loss: 0.0236, label: 1, bag_size: 8216\n",
      "batch 519, loss: 0.0034, instance_loss: 0.0106, weighted_loss: 0.0056, label: 1, bag_size: 7371\n",
      "batch 539, loss: 0.0160, instance_loss: 0.1445, weighted_loss: 0.0546, label: 1, bag_size: 1242\n",
      "batch 559, loss: 0.0038, instance_loss: 0.0000, weighted_loss: 0.0027, label: 0, bag_size: 10942\n",
      "batch 579, loss: 0.1327, instance_loss: 0.0153, weighted_loss: 0.0975, label: 1, bag_size: 10432\n",
      "batch 599, loss: 0.0008, instance_loss: 0.0829, weighted_loss: 0.0254, label: 1, bag_size: 4128\n",
      "batch 619, loss: 1.0950, instance_loss: 0.0566, weighted_loss: 0.7834, label: 1, bag_size: 2935\n",
      "batch 639, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 9470\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 12524\n",
      "batch 679, loss: 0.0612, instance_loss: 0.0400, weighted_loss: 0.0548, label: 1, bag_size: 2356\n",
      "batch 699, loss: 0.7056, instance_loss: 0.0000, weighted_loss: 0.4939, label: 0, bag_size: 15672\n",
      "batch 719, loss: 0.0220, instance_loss: 0.3099, weighted_loss: 0.1084, label: 0, bag_size: 3160\n",
      "batch 739, loss: 0.2584, instance_loss: 0.0026, weighted_loss: 0.1817, label: 0, bag_size: 11122\n",
      "batch 759, loss: 0.0122, instance_loss: 0.0039, weighted_loss: 0.0097, label: 1, bag_size: 5454\n",
      "batch 779, loss: 0.0045, instance_loss: 0.0000, weighted_loss: 0.0032, label: 0, bag_size: 31085\n",
      "batch 799, loss: 0.1378, instance_loss: 0.0127, weighted_loss: 0.1002, label: 1, bag_size: 1831\n",
      "batch 819, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 12201\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9863138686131386: correct 12972/13152\n",
      "class 1 clustering acc 0.9156021897810219: correct 6021/6576\n",
      "Epoch: 62, train_loss: 0.1457, train_clustering_loss:  0.1503, train_error: 0.0584\n",
      "class 0: acc 0.9452380952380952, correct 397/420\n",
      "class 1: acc 0.9378109452736318, correct 377/402\n",
      "\n",
      "Val Set, val_loss: 0.2047, val_error: 0.0870, auc: 0.9961\n",
      "class 0 clustering acc 0.9680706521739131: correct 1425/1472\n",
      "class 1 clustering acc 0.8491847826086957: correct 625/736\n",
      "class 0: acc 1.0, correct 38/38\n",
      "class 1: acc 0.8518518518518519, correct 46/54\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0009, instance_loss: 0.3222, weighted_loss: 0.0973, label: 1, bag_size: 1255\n",
      "batch 39, loss: 0.0920, instance_loss: 0.0185, weighted_loss: 0.0700, label: 1, bag_size: 11223\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14319\n",
      "batch 79, loss: 0.0363, instance_loss: 1.8472, weighted_loss: 0.5796, label: 0, bag_size: 2814\n",
      "batch 99, loss: 0.0012, instance_loss: 0.0062, weighted_loss: 0.0027, label: 1, bag_size: 4880\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0142, weighted_loss: 0.0043, label: 1, bag_size: 10867\n",
      "batch 139, loss: 0.0706, instance_loss: 0.7243, weighted_loss: 0.2667, label: 1, bag_size: 5366\n",
      "batch 159, loss: 0.5234, instance_loss: 0.0000, weighted_loss: 0.3664, label: 0, bag_size: 13619\n",
      "batch 179, loss: 0.0413, instance_loss: 0.0015, weighted_loss: 0.0294, label: 1, bag_size: 14887\n",
      "batch 199, loss: 0.0038, instance_loss: 0.0000, weighted_loss: 0.0027, label: 0, bag_size: 5965\n",
      "batch 219, loss: 0.0031, instance_loss: 0.0047, weighted_loss: 0.0036, label: 1, bag_size: 11032\n",
      "batch 239, loss: 0.0998, instance_loss: 0.0335, weighted_loss: 0.0799, label: 1, bag_size: 21701\n",
      "batch 259, loss: 0.0187, instance_loss: 0.0028, weighted_loss: 0.0139, label: 0, bag_size: 3502\n",
      "batch 279, loss: 0.5757, instance_loss: 0.1241, weighted_loss: 0.4403, label: 0, bag_size: 4241\n",
      "batch 299, loss: 0.0466, instance_loss: 0.0548, weighted_loss: 0.0491, label: 1, bag_size: 1572\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 21682\n",
      "batch 339, loss: 0.0185, instance_loss: 0.0000, weighted_loss: 0.0130, label: 0, bag_size: 10365\n",
      "batch 359, loss: 0.0654, instance_loss: 0.0254, weighted_loss: 0.0534, label: 1, bag_size: 6599\n",
      "batch 379, loss: 0.0043, instance_loss: 0.0000, weighted_loss: 0.0030, label: 1, bag_size: 10969\n",
      "batch 399, loss: 0.8847, instance_loss: 0.2162, weighted_loss: 0.6842, label: 0, bag_size: 546\n",
      "batch 419, loss: 0.0622, instance_loss: 0.1349, weighted_loss: 0.0840, label: 1, bag_size: 7798\n",
      "batch 439, loss: 0.0884, instance_loss: 0.3144, weighted_loss: 0.1562, label: 1, bag_size: 12712\n",
      "batch 459, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 23398\n",
      "batch 479, loss: 0.0049, instance_loss: 0.0151, weighted_loss: 0.0080, label: 0, bag_size: 10128\n",
      "batch 499, loss: 3.2279, instance_loss: 0.5285, weighted_loss: 2.4181, label: 0, bag_size: 2732\n",
      "batch 519, loss: 0.0061, instance_loss: 0.0040, weighted_loss: 0.0055, label: 0, bag_size: 2609\n",
      "batch 539, loss: 0.0006, instance_loss: 0.0121, weighted_loss: 0.0041, label: 1, bag_size: 865\n",
      "batch 559, loss: 0.0367, instance_loss: 0.0056, weighted_loss: 0.0274, label: 1, bag_size: 9519\n",
      "batch 579, loss: 1.4782, instance_loss: 0.0765, weighted_loss: 1.0577, label: 1, bag_size: 19470\n",
      "batch 599, loss: 0.0171, instance_loss: 0.0000, weighted_loss: 0.0120, label: 0, bag_size: 12201\n",
      "batch 619, loss: 0.0156, instance_loss: 0.0000, weighted_loss: 0.0109, label: 1, bag_size: 20161\n",
      "batch 639, loss: 0.0226, instance_loss: 0.0134, weighted_loss: 0.0199, label: 1, bag_size: 9519\n",
      "batch 659, loss: 0.0060, instance_loss: 0.3891, weighted_loss: 0.1209, label: 0, bag_size: 1234\n",
      "batch 679, loss: 0.0239, instance_loss: 0.0027, weighted_loss: 0.0175, label: 1, bag_size: 10498\n",
      "batch 699, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 11884\n",
      "batch 719, loss: 0.0705, instance_loss: 0.0082, weighted_loss: 0.0518, label: 1, bag_size: 6781\n",
      "batch 739, loss: 0.0002, instance_loss: 0.0176, weighted_loss: 0.0055, label: 1, bag_size: 3634\n",
      "batch 759, loss: 0.3984, instance_loss: 0.0638, weighted_loss: 0.2980, label: 1, bag_size: 15931\n",
      "batch 779, loss: 0.2701, instance_loss: 0.0289, weighted_loss: 0.1978, label: 0, bag_size: 7031\n",
      "batch 799, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 22264\n",
      "batch 819, loss: 0.0047, instance_loss: 0.1707, weighted_loss: 0.0545, label: 0, bag_size: 2104\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9846411192214112: correct 12950/13152\n",
      "class 1 clustering acc 0.9184914841849149: correct 6040/6576\n",
      "Epoch: 63, train_loss: 0.1696, train_clustering_loss:  0.1689, train_error: 0.0657\n",
      "class 0: acc 0.9272237196765498, correct 344/371\n",
      "class 1: acc 0.9401330376940134, correct 424/451\n",
      "\n",
      "Val Set, val_loss: 0.1678, val_error: 0.0761, auc: 0.9927\n",
      "class 0 clustering acc 0.9796195652173914: correct 1442/1472\n",
      "class 1 clustering acc 0.9266304347826086: correct 682/736\n",
      "class 0: acc 0.9736842105263158, correct 37/38\n",
      "class 1: acc 0.8888888888888888, correct 48/54\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0112, instance_loss: 1.0077, weighted_loss: 0.3102, label: 0, bag_size: 2814\n",
      "batch 39, loss: 0.0006, instance_loss: 0.0440, weighted_loss: 0.0136, label: 1, bag_size: 865\n",
      "batch 59, loss: 0.1203, instance_loss: 0.5802, weighted_loss: 0.2583, label: 1, bag_size: 1095\n",
      "batch 79, loss: 0.2825, instance_loss: 0.0031, weighted_loss: 0.1986, label: 1, bag_size: 12626\n",
      "batch 99, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 12796\n",
      "batch 119, loss: 0.0037, instance_loss: 0.0006, weighted_loss: 0.0028, label: 0, bag_size: 8788\n",
      "batch 139, loss: 0.0002, instance_loss: 0.0008, weighted_loss: 0.0004, label: 0, bag_size: 9930\n",
      "batch 159, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 12593\n",
      "batch 179, loss: 0.0129, instance_loss: 0.0074, weighted_loss: 0.0113, label: 1, bag_size: 1888\n",
      "batch 199, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 11113\n",
      "batch 219, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 15914\n",
      "batch 239, loss: 0.0265, instance_loss: 0.0175, weighted_loss: 0.0238, label: 1, bag_size: 4976\n",
      "batch 259, loss: 0.2286, instance_loss: 0.5095, weighted_loss: 0.3129, label: 1, bag_size: 11386\n",
      "batch 279, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0015, label: 0, bag_size: 16087\n",
      "batch 299, loss: 0.2009, instance_loss: 0.6095, weighted_loss: 0.3235, label: 1, bag_size: 2681\n",
      "batch 319, loss: 0.0107, instance_loss: 0.0001, weighted_loss: 0.0076, label: 1, bag_size: 3651\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11642\n",
      "batch 359, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 26271\n",
      "batch 379, loss: 1.5924, instance_loss: 0.1210, weighted_loss: 1.1510, label: 1, bag_size: 8103\n",
      "batch 399, loss: 0.0578, instance_loss: 0.0855, weighted_loss: 0.0661, label: 0, bag_size: 1630\n",
      "batch 419, loss: 0.0004, instance_loss: 0.0052, weighted_loss: 0.0018, label: 0, bag_size: 24911\n",
      "batch 439, loss: 0.6211, instance_loss: 1.0438, weighted_loss: 0.7479, label: 1, bag_size: 1649\n",
      "batch 459, loss: 0.0014, instance_loss: 0.1540, weighted_loss: 0.0472, label: 0, bag_size: 2351\n",
      "batch 479, loss: 0.0140, instance_loss: 0.0077, weighted_loss: 0.0121, label: 0, bag_size: 6884\n",
      "batch 499, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 2179\n",
      "batch 519, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 11146\n",
      "batch 539, loss: 0.0139, instance_loss: 0.0002, weighted_loss: 0.0098, label: 1, bag_size: 4789\n",
      "batch 559, loss: 0.0971, instance_loss: 0.0000, weighted_loss: 0.0680, label: 0, bag_size: 3710\n",
      "batch 579, loss: 0.0207, instance_loss: 0.6209, weighted_loss: 0.2008, label: 0, bag_size: 1370\n",
      "batch 599, loss: 0.0108, instance_loss: 0.0414, weighted_loss: 0.0200, label: 0, bag_size: 1142\n",
      "batch 619, loss: 0.0003, instance_loss: 0.0116, weighted_loss: 0.0037, label: 1, bag_size: 699\n",
      "batch 639, loss: 0.0076, instance_loss: 0.0002, weighted_loss: 0.0054, label: 0, bag_size: 9542\n",
      "batch 659, loss: 0.0855, instance_loss: 0.3615, weighted_loss: 0.1683, label: 0, bag_size: 1772\n",
      "batch 679, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 9851\n",
      "batch 699, loss: 0.0003, instance_loss: 0.0082, weighted_loss: 0.0027, label: 1, bag_size: 7381\n",
      "batch 719, loss: 0.0063, instance_loss: 0.0485, weighted_loss: 0.0189, label: 1, bag_size: 9062\n",
      "batch 739, loss: 0.5858, instance_loss: 0.1727, weighted_loss: 0.4619, label: 1, bag_size: 1230\n",
      "batch 759, loss: 0.0004, instance_loss: 0.0019, weighted_loss: 0.0009, label: 1, bag_size: 7110\n",
      "batch 779, loss: 0.0015, instance_loss: 0.0004, weighted_loss: 0.0012, label: 0, bag_size: 4902\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 10068\n",
      "batch 819, loss: 0.0060, instance_loss: 0.0400, weighted_loss: 0.0162, label: 1, bag_size: 8475\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9840328467153284: correct 12942/13152\n",
      "class 1 clustering acc 0.9229014598540146: correct 6069/6576\n",
      "Epoch: 64, train_loss: 0.1476, train_clustering_loss:  0.1476, train_error: 0.0511\n",
      "class 0: acc 0.9403341288782816, correct 394/419\n",
      "class 1: acc 0.9578163771712159, correct 386/403\n",
      "\n",
      "Val Set, val_loss: 0.1219, val_error: 0.0326, auc: 0.9907\n",
      "class 0 clustering acc 0.9714673913043478: correct 1430/1472\n",
      "class 1 clustering acc 0.907608695652174: correct 668/736\n",
      "class 0: acc 0.9736842105263158, correct 37/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.3047, weighted_loss: 0.0915, label: 1, bag_size: 18468\n",
      "batch 39, loss: 0.0071, instance_loss: 0.0008, weighted_loss: 0.0052, label: 1, bag_size: 9747\n",
      "batch 59, loss: 0.0017, instance_loss: 0.0000, weighted_loss: 0.0012, label: 1, bag_size: 9756\n",
      "batch 79, loss: 0.0326, instance_loss: 0.0094, weighted_loss: 0.0257, label: 0, bag_size: 2360\n",
      "batch 99, loss: 0.2131, instance_loss: 0.0000, weighted_loss: 0.1492, label: 1, bag_size: 7583\n",
      "batch 119, loss: 0.0113, instance_loss: 0.0001, weighted_loss: 0.0080, label: 0, bag_size: 7923\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 4442\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11477\n",
      "batch 179, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 11383\n",
      "batch 199, loss: 0.0038, instance_loss: 0.0249, weighted_loss: 0.0102, label: 1, bag_size: 25695\n",
      "batch 219, loss: 0.0244, instance_loss: 0.1811, weighted_loss: 0.0714, label: 1, bag_size: 2814\n",
      "batch 239, loss: 0.5080, instance_loss: 0.0076, weighted_loss: 0.3579, label: 0, bag_size: 11212\n",
      "batch 259, loss: 0.0010, instance_loss: 0.0033, weighted_loss: 0.0017, label: 0, bag_size: 2844\n",
      "batch 279, loss: 0.0208, instance_loss: 0.0105, weighted_loss: 0.0177, label: 1, bag_size: 4789\n",
      "batch 299, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 9949\n",
      "batch 319, loss: 0.0131, instance_loss: 0.0000, weighted_loss: 0.0092, label: 1, bag_size: 15689\n",
      "batch 339, loss: 0.1868, instance_loss: 0.0228, weighted_loss: 0.1376, label: 0, bag_size: 26208\n",
      "batch 359, loss: 0.0288, instance_loss: 0.0079, weighted_loss: 0.0225, label: 1, bag_size: 15609\n",
      "batch 379, loss: 3.3344, instance_loss: 0.1173, weighted_loss: 2.3692, label: 1, bag_size: 2565\n",
      "batch 399, loss: 0.0005, instance_loss: 0.0003, weighted_loss: 0.0004, label: 1, bag_size: 5731\n",
      "batch 419, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 9455\n",
      "batch 439, loss: 0.5813, instance_loss: 0.0505, weighted_loss: 0.4221, label: 1, bag_size: 1703\n",
      "batch 459, loss: 0.0004, instance_loss: 0.0016, weighted_loss: 0.0008, label: 1, bag_size: 1969\n",
      "batch 479, loss: 0.3803, instance_loss: 0.1192, weighted_loss: 0.3020, label: 1, bag_size: 6665\n",
      "batch 499, loss: 0.5350, instance_loss: 0.0172, weighted_loss: 0.3797, label: 1, bag_size: 3652\n",
      "batch 519, loss: 0.0018, instance_loss: 0.0064, weighted_loss: 0.0032, label: 0, bag_size: 9415\n",
      "batch 539, loss: 0.0085, instance_loss: 0.0047, weighted_loss: 0.0074, label: 1, bag_size: 8040\n",
      "batch 559, loss: 0.0108, instance_loss: 0.1253, weighted_loss: 0.0452, label: 0, bag_size: 4523\n",
      "batch 579, loss: 0.0101, instance_loss: 0.0613, weighted_loss: 0.0255, label: 0, bag_size: 13591\n",
      "batch 599, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 17368\n",
      "batch 619, loss: 0.0000, instance_loss: 0.2356, weighted_loss: 0.0707, label: 1, bag_size: 3295\n",
      "batch 639, loss: 0.0297, instance_loss: 0.0306, weighted_loss: 0.0300, label: 1, bag_size: 6171\n",
      "batch 659, loss: 0.0001, instance_loss: 0.0559, weighted_loss: 0.0169, label: 1, bag_size: 1622\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 13947\n",
      "batch 699, loss: 0.0004, instance_loss: 0.0458, weighted_loss: 0.0140, label: 1, bag_size: 19832\n",
      "batch 719, loss: 0.0013, instance_loss: 0.0026, weighted_loss: 0.0017, label: 1, bag_size: 6533\n",
      "batch 739, loss: 0.0488, instance_loss: 0.0328, weighted_loss: 0.0440, label: 1, bag_size: 10492\n",
      "batch 759, loss: 0.0330, instance_loss: 0.1735, weighted_loss: 0.0752, label: 0, bag_size: 2004\n",
      "batch 779, loss: 0.0055, instance_loss: 0.0024, weighted_loss: 0.0045, label: 1, bag_size: 10912\n",
      "batch 799, loss: 0.0081, instance_loss: 0.0024, weighted_loss: 0.0064, label: 0, bag_size: 1651\n",
      "batch 819, loss: 0.1535, instance_loss: 0.0129, weighted_loss: 0.1113, label: 1, bag_size: 3224\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9839568126520681: correct 12941/13152\n",
      "class 1 clustering acc 0.9250304136253041: correct 6083/6576\n",
      "Epoch: 65, train_loss: 0.1692, train_clustering_loss:  0.1586, train_error: 0.0718\n",
      "class 0: acc 0.9203980099502488, correct 370/402\n",
      "class 1: acc 0.9357142857142857, correct 393/420\n",
      "\n",
      "Val Set, val_loss: 0.1426, val_error: 0.0326, auc: 0.9898\n",
      "class 0 clustering acc 0.9510869565217391: correct 1400/1472\n",
      "class 1 clustering acc 0.8709239130434783: correct 641/736\n",
      "class 0: acc 0.9736842105263158, correct 37/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0029, instance_loss: 0.0205, weighted_loss: 0.0082, label: 1, bag_size: 9533\n",
      "batch 39, loss: 0.2475, instance_loss: 0.7503, weighted_loss: 0.3984, label: 1, bag_size: 2937\n",
      "batch 59, loss: 0.0083, instance_loss: 0.0000, weighted_loss: 0.0058, label: 0, bag_size: 9542\n",
      "batch 79, loss: 0.0043, instance_loss: 0.2597, weighted_loss: 0.0809, label: 0, bag_size: 4523\n",
      "batch 99, loss: 0.0071, instance_loss: 0.0104, weighted_loss: 0.0081, label: 1, bag_size: 14230\n",
      "batch 119, loss: 0.0048, instance_loss: 0.0481, weighted_loss: 0.0178, label: 0, bag_size: 2236\n",
      "batch 139, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 6164\n",
      "batch 159, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 17155\n",
      "batch 179, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 21093\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 10068\n",
      "batch 219, loss: 0.0022, instance_loss: 0.0000, weighted_loss: 0.0015, label: 0, bag_size: 9949\n",
      "batch 239, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 19832\n",
      "batch 259, loss: 0.0399, instance_loss: 0.0011, weighted_loss: 0.0283, label: 0, bag_size: 10814\n",
      "batch 279, loss: 0.0675, instance_loss: 0.0000, weighted_loss: 0.0473, label: 1, bag_size: 11220\n",
      "batch 299, loss: 0.0077, instance_loss: 0.0000, weighted_loss: 0.0054, label: 0, bag_size: 18954\n",
      "batch 319, loss: 0.1587, instance_loss: 0.1350, weighted_loss: 0.1516, label: 1, bag_size: 1963\n",
      "batch 339, loss: 0.0025, instance_loss: 0.0004, weighted_loss: 0.0019, label: 1, bag_size: 8602\n",
      "batch 359, loss: 0.0885, instance_loss: 0.0175, weighted_loss: 0.0672, label: 0, bag_size: 10415\n",
      "batch 379, loss: 0.1430, instance_loss: 0.0417, weighted_loss: 0.1126, label: 0, bag_size: 2467\n",
      "batch 399, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 11735\n",
      "batch 419, loss: 0.0010, instance_loss: 0.0282, weighted_loss: 0.0091, label: 1, bag_size: 1746\n",
      "batch 439, loss: 0.0075, instance_loss: 0.0010, weighted_loss: 0.0056, label: 1, bag_size: 14230\n",
      "batch 459, loss: 0.1370, instance_loss: 0.0683, weighted_loss: 0.1164, label: 1, bag_size: 11160\n",
      "batch 479, loss: 0.0189, instance_loss: 0.0035, weighted_loss: 0.0142, label: 1, bag_size: 15125\n",
      "batch 499, loss: 0.0047, instance_loss: 0.0000, weighted_loss: 0.0033, label: 0, bag_size: 5551\n",
      "batch 519, loss: 0.0022, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 21082\n",
      "batch 539, loss: 0.4856, instance_loss: 0.0000, weighted_loss: 0.3399, label: 0, bag_size: 15672\n",
      "batch 559, loss: 0.0029, instance_loss: 0.0000, weighted_loss: 0.0020, label: 0, bag_size: 9415\n",
      "batch 579, loss: 0.0172, instance_loss: 0.1359, weighted_loss: 0.0528, label: 1, bag_size: 4039\n",
      "batch 599, loss: 2.6311, instance_loss: 2.8781, weighted_loss: 2.7052, label: 1, bag_size: 9162\n",
      "batch 619, loss: 0.0361, instance_loss: 0.1392, weighted_loss: 0.0670, label: 0, bag_size: 12131\n",
      "batch 639, loss: 0.0152, instance_loss: 0.1247, weighted_loss: 0.0481, label: 1, bag_size: 9649\n",
      "batch 659, loss: 0.0249, instance_loss: 0.0621, weighted_loss: 0.0361, label: 0, bag_size: 1831\n",
      "batch 679, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 25027\n",
      "batch 699, loss: 0.0069, instance_loss: 0.0640, weighted_loss: 0.0240, label: 1, bag_size: 2904\n",
      "batch 719, loss: 0.0001, instance_loss: 0.0530, weighted_loss: 0.0160, label: 1, bag_size: 2904\n",
      "batch 739, loss: 0.0423, instance_loss: 0.8417, weighted_loss: 0.2821, label: 1, bag_size: 6726\n",
      "batch 759, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 23368\n",
      "batch 779, loss: 0.0916, instance_loss: 0.2274, weighted_loss: 0.1324, label: 0, bag_size: 16690\n",
      "batch 799, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 22828\n",
      "batch 819, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 16720\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9832725060827251: correct 12932/13152\n",
      "class 1 clustering acc 0.9302007299270073: correct 6117/6576\n",
      "Epoch: 66, train_loss: 0.1279, train_clustering_loss:  0.1550, train_error: 0.0426\n",
      "class 0: acc 0.9603960396039604, correct 388/404\n",
      "class 1: acc 0.9545454545454546, correct 399/418\n",
      "\n",
      "Val Set, val_loss: 0.1397, val_error: 0.0326, auc: 0.9878\n",
      "class 0 clustering acc 0.9633152173913043: correct 1418/1472\n",
      "class 1 clustering acc 0.9035326086956522: correct 665/736\n",
      "class 0: acc 0.9736842105263158, correct 37/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "EarlyStopping counter: 12 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0423, instance_loss: 0.1424, weighted_loss: 0.0723, label: 0, bag_size: 2104\n",
      "batch 39, loss: 0.0003, instance_loss: 0.0066, weighted_loss: 0.0022, label: 1, bag_size: 5221\n",
      "batch 59, loss: 0.0007, instance_loss: 0.1182, weighted_loss: 0.0360, label: 0, bag_size: 3190\n",
      "batch 79, loss: 0.0108, instance_loss: 0.2607, weighted_loss: 0.0857, label: 1, bag_size: 1249\n",
      "batch 99, loss: 0.0168, instance_loss: 0.0000, weighted_loss: 0.0117, label: 0, bag_size: 7637\n",
      "batch 119, loss: 0.3790, instance_loss: 0.0182, weighted_loss: 0.2708, label: 0, bag_size: 4241\n",
      "batch 139, loss: 0.0106, instance_loss: 0.0720, weighted_loss: 0.0290, label: 0, bag_size: 2996\n",
      "batch 159, loss: 0.0123, instance_loss: 0.0250, weighted_loss: 0.0161, label: 0, bag_size: 3670\n",
      "batch 179, loss: 0.0757, instance_loss: 0.0000, weighted_loss: 0.0530, label: 0, bag_size: 12131\n",
      "batch 199, loss: 0.0555, instance_loss: 0.1807, weighted_loss: 0.0931, label: 0, bag_size: 2270\n",
      "batch 219, loss: 0.0794, instance_loss: 0.1680, weighted_loss: 0.1059, label: 1, bag_size: 1022\n",
      "batch 239, loss: 0.0042, instance_loss: 0.0034, weighted_loss: 0.0039, label: 1, bag_size: 3640\n",
      "batch 259, loss: 0.0197, instance_loss: 0.0081, weighted_loss: 0.0162, label: 1, bag_size: 6842\n",
      "batch 279, loss: 0.1908, instance_loss: 0.0745, weighted_loss: 0.1559, label: 1, bag_size: 11223\n",
      "batch 299, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 6606\n",
      "batch 319, loss: 0.0007, instance_loss: 0.0117, weighted_loss: 0.0040, label: 1, bag_size: 9636\n",
      "batch 339, loss: 0.0009, instance_loss: 0.0229, weighted_loss: 0.0075, label: 1, bag_size: 6752\n",
      "batch 359, loss: 0.0027, instance_loss: 0.0024, weighted_loss: 0.0026, label: 0, bag_size: 3657\n",
      "batch 379, loss: 0.0007, instance_loss: 0.0222, weighted_loss: 0.0072, label: 1, bag_size: 7110\n",
      "batch 399, loss: 0.0004, instance_loss: 0.0035, weighted_loss: 0.0013, label: 0, bag_size: 3228\n",
      "batch 419, loss: 0.0003, instance_loss: 0.0027, weighted_loss: 0.0010, label: 1, bag_size: 6016\n",
      "batch 439, loss: 3.1553, instance_loss: 0.1004, weighted_loss: 2.2388, label: 1, bag_size: 25831\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 10581\n",
      "batch 479, loss: 0.1310, instance_loss: 0.0000, weighted_loss: 0.0917, label: 0, bag_size: 20478\n",
      "batch 499, loss: 0.0175, instance_loss: 0.0111, weighted_loss: 0.0156, label: 1, bag_size: 2179\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0019, weighted_loss: 0.0006, label: 1, bag_size: 14515\n",
      "batch 539, loss: 0.2208, instance_loss: 0.1260, weighted_loss: 0.1924, label: 0, bag_size: 1701\n",
      "batch 559, loss: 0.0220, instance_loss: 0.1817, weighted_loss: 0.0699, label: 1, bag_size: 2814\n",
      "batch 579, loss: 0.1118, instance_loss: 0.2287, weighted_loss: 0.1469, label: 0, bag_size: 1772\n",
      "batch 599, loss: 0.4590, instance_loss: 0.0001, weighted_loss: 0.3213, label: 0, bag_size: 13992\n",
      "batch 619, loss: 0.0072, instance_loss: 0.2316, weighted_loss: 0.0745, label: 0, bag_size: 11512\n",
      "batch 639, loss: 0.0906, instance_loss: 0.2432, weighted_loss: 0.1364, label: 0, bag_size: 2458\n",
      "batch 659, loss: 0.1817, instance_loss: 0.0137, weighted_loss: 0.1313, label: 1, bag_size: 6781\n",
      "batch 679, loss: 0.1797, instance_loss: 0.0294, weighted_loss: 0.1346, label: 1, bag_size: 2356\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23996\n",
      "batch 719, loss: 0.1711, instance_loss: 0.0020, weighted_loss: 0.1204, label: 1, bag_size: 10622\n",
      "batch 739, loss: 0.0000, instance_loss: 0.0086, weighted_loss: 0.0026, label: 1, bag_size: 1412\n",
      "batch 759, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 7217\n",
      "batch 779, loss: 0.0637, instance_loss: 0.0085, weighted_loss: 0.0472, label: 0, bag_size: 14333\n",
      "batch 799, loss: 0.0326, instance_loss: 0.0001, weighted_loss: 0.0229, label: 1, bag_size: 5605\n",
      "batch 819, loss: 0.0586, instance_loss: 0.0371, weighted_loss: 0.0521, label: 0, bag_size: 8549\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9858576642335767: correct 12966/13152\n",
      "class 1 clustering acc 0.9352189781021898: correct 6150/6576\n",
      "Epoch: 67, train_loss: 0.1373, train_clustering_loss:  0.1448, train_error: 0.0474\n",
      "class 0: acc 0.9590909090909091, correct 422/440\n",
      "class 1: acc 0.9450261780104712, correct 361/382\n",
      "\n",
      "Val Set, val_loss: 0.1402, val_error: 0.0326, auc: 0.9844\n",
      "class 0 clustering acc 0.983016304347826: correct 1447/1472\n",
      "class 1 clustering acc 0.9483695652173914: correct 698/736\n",
      "class 0: acc 0.9736842105263158, correct 37/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "EarlyStopping counter: 13 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.6210, instance_loss: 0.0174, weighted_loss: 1.1399, label: 0, bag_size: 2694\n",
      "batch 39, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 16720\n",
      "batch 59, loss: 0.0027, instance_loss: 0.3464, weighted_loss: 0.1058, label: 0, bag_size: 14739\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23996\n",
      "batch 99, loss: 0.0082, instance_loss: 0.0409, weighted_loss: 0.0180, label: 0, bag_size: 2351\n",
      "batch 119, loss: 0.0135, instance_loss: 0.0053, weighted_loss: 0.0111, label: 1, bag_size: 9689\n",
      "batch 139, loss: 0.4539, instance_loss: 0.0155, weighted_loss: 0.3224, label: 1, bag_size: 8395\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 11389\n",
      "batch 179, loss: 0.0178, instance_loss: 0.0030, weighted_loss: 0.0134, label: 0, bag_size: 2873\n",
      "batch 199, loss: 0.0063, instance_loss: 0.0117, weighted_loss: 0.0079, label: 1, bag_size: 15609\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0377, weighted_loss: 0.0113, label: 0, bag_size: 3787\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 6317\n",
      "batch 259, loss: 0.0529, instance_loss: 0.0002, weighted_loss: 0.0371, label: 0, bag_size: 12083\n",
      "batch 279, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 23368\n",
      "batch 299, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 8372\n",
      "batch 319, loss: 0.1848, instance_loss: 0.1730, weighted_loss: 0.1813, label: 1, bag_size: 16154\n",
      "batch 339, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 19390\n",
      "batch 359, loss: 0.0025, instance_loss: 0.0000, weighted_loss: 0.0018, label: 0, bag_size: 25027\n",
      "batch 379, loss: 0.0772, instance_loss: 0.0091, weighted_loss: 0.0568, label: 1, bag_size: 5903\n",
      "batch 399, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 15636\n",
      "batch 419, loss: 0.0080, instance_loss: 0.0046, weighted_loss: 0.0069, label: 1, bag_size: 7468\n",
      "batch 439, loss: 0.0627, instance_loss: 0.0667, weighted_loss: 0.0639, label: 0, bag_size: 3774\n",
      "batch 459, loss: 0.0030, instance_loss: 0.0131, weighted_loss: 0.0060, label: 1, bag_size: 5256\n",
      "batch 479, loss: 0.0927, instance_loss: 0.0153, weighted_loss: 0.0695, label: 0, bag_size: 11281\n",
      "batch 499, loss: 0.9520, instance_loss: 1.0966, weighted_loss: 0.9954, label: 1, bag_size: 8103\n",
      "batch 519, loss: 0.0837, instance_loss: 0.0110, weighted_loss: 0.0619, label: 0, bag_size: 4845\n",
      "batch 539, loss: 0.0043, instance_loss: 0.0003, weighted_loss: 0.0031, label: 1, bag_size: 3683\n",
      "batch 559, loss: 0.0115, instance_loss: 0.4543, weighted_loss: 0.1443, label: 1, bag_size: 9747\n",
      "batch 579, loss: 0.5035, instance_loss: 0.0870, weighted_loss: 0.3785, label: 1, bag_size: 2565\n",
      "batch 599, loss: 0.0297, instance_loss: 0.0736, weighted_loss: 0.0429, label: 1, bag_size: 6665\n",
      "batch 619, loss: 0.0006, instance_loss: 0.0194, weighted_loss: 0.0062, label: 1, bag_size: 14433\n",
      "batch 639, loss: 0.0025, instance_loss: 0.0330, weighted_loss: 0.0117, label: 0, bag_size: 2004\n",
      "batch 659, loss: 0.0002, instance_loss: 0.0036, weighted_loss: 0.0012, label: 1, bag_size: 7767\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0126, weighted_loss: 0.0039, label: 1, bag_size: 7381\n",
      "batch 699, loss: 0.0179, instance_loss: 0.1864, weighted_loss: 0.0684, label: 1, bag_size: 6825\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 32227\n",
      "batch 739, loss: 0.1026, instance_loss: 0.0093, weighted_loss: 0.0746, label: 1, bag_size: 16154\n",
      "batch 759, loss: 0.0001, instance_loss: 0.0019, weighted_loss: 0.0007, label: 0, bag_size: 17791\n",
      "batch 779, loss: 0.0032, instance_loss: 0.0270, weighted_loss: 0.0103, label: 0, bag_size: 1483\n",
      "batch 799, loss: 0.0207, instance_loss: 0.0007, weighted_loss: 0.0147, label: 1, bag_size: 7989\n",
      "batch 819, loss: 0.0011, instance_loss: 0.0187, weighted_loss: 0.0064, label: 1, bag_size: 4423\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9854774939172749: correct 12961/13152\n",
      "class 1 clustering acc 0.931265206812652: correct 6124/6576\n",
      "Epoch: 68, train_loss: 0.1306, train_clustering_loss:  0.1404, train_error: 0.0511\n",
      "class 0: acc 0.9502487562189055, correct 382/402\n",
      "class 1: acc 0.9476190476190476, correct 398/420\n",
      "\n",
      "Val Set, val_loss: 0.1523, val_error: 0.0543, auc: 0.9820\n",
      "class 0 clustering acc 0.9850543478260869: correct 1450/1472\n",
      "class 1 clustering acc 0.9347826086956522: correct 688/736\n",
      "class 0: acc 0.8947368421052632, correct 34/38\n",
      "class 1: acc 0.9814814814814815, correct 53/54\n",
      "EarlyStopping counter: 14 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0224, instance_loss: 0.0414, weighted_loss: 0.0281, label: 1, bag_size: 12180\n",
      "batch 39, loss: 0.0655, instance_loss: 0.0029, weighted_loss: 0.0467, label: 0, bag_size: 12899\n",
      "batch 59, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 1884\n",
      "batch 79, loss: 1.8361, instance_loss: 0.0272, weighted_loss: 1.2934, label: 0, bag_size: 3897\n",
      "batch 99, loss: 0.0005, instance_loss: 0.3303, weighted_loss: 0.0994, label: 1, bag_size: 4128\n",
      "batch 119, loss: 0.0117, instance_loss: 0.0002, weighted_loss: 0.0083, label: 0, bag_size: 25558\n",
      "batch 139, loss: 0.2829, instance_loss: 0.0830, weighted_loss: 0.2229, label: 1, bag_size: 15931\n",
      "batch 159, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 8812\n",
      "batch 179, loss: 0.0195, instance_loss: 0.0766, weighted_loss: 0.0366, label: 0, bag_size: 17482\n",
      "batch 199, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 21385\n",
      "batch 219, loss: 0.0854, instance_loss: 0.0471, weighted_loss: 0.0739, label: 0, bag_size: 24382\n",
      "batch 239, loss: 2.7938, instance_loss: 2.1147, weighted_loss: 2.5901, label: 1, bag_size: 20870\n",
      "batch 259, loss: 0.0002, instance_loss: 0.0060, weighted_loss: 0.0020, label: 1, bag_size: 9971\n",
      "batch 279, loss: 0.0015, instance_loss: 0.1319, weighted_loss: 0.0406, label: 0, bag_size: 1891\n",
      "batch 299, loss: 0.2928, instance_loss: 0.0027, weighted_loss: 0.2057, label: 0, bag_size: 5105\n",
      "batch 319, loss: 0.0026, instance_loss: 0.0006, weighted_loss: 0.0020, label: 1, bag_size: 3683\n",
      "batch 339, loss: 0.0419, instance_loss: 0.0000, weighted_loss: 0.0293, label: 0, bag_size: 10365\n",
      "batch 359, loss: 0.0581, instance_loss: 0.0194, weighted_loss: 0.0465, label: 1, bag_size: 13477\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 14515\n",
      "batch 399, loss: 0.0057, instance_loss: 0.0011, weighted_loss: 0.0043, label: 1, bag_size: 9747\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 6792\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0122, weighted_loss: 0.0037, label: 1, bag_size: 1412\n",
      "batch 459, loss: 0.0908, instance_loss: 0.0816, weighted_loss: 0.0880, label: 0, bag_size: 26208\n",
      "batch 479, loss: 0.1640, instance_loss: 0.0130, weighted_loss: 0.1187, label: 1, bag_size: 7768\n",
      "batch 499, loss: 0.0003, instance_loss: 0.0053, weighted_loss: 0.0018, label: 1, bag_size: 7078\n",
      "batch 519, loss: 0.0002, instance_loss: 0.0153, weighted_loss: 0.0047, label: 1, bag_size: 4250\n",
      "batch 539, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 21082\n",
      "batch 559, loss: 0.0540, instance_loss: 0.0000, weighted_loss: 0.0378, label: 0, bag_size: 18738\n",
      "batch 579, loss: 0.3740, instance_loss: 0.0899, weighted_loss: 0.2887, label: 1, bag_size: 12712\n",
      "batch 599, loss: 0.0011, instance_loss: 0.0021, weighted_loss: 0.0014, label: 1, bag_size: 14223\n",
      "batch 619, loss: 0.0046, instance_loss: 0.0073, weighted_loss: 0.0054, label: 0, bag_size: 2457\n",
      "batch 639, loss: 0.5140, instance_loss: 0.1389, weighted_loss: 0.4015, label: 1, bag_size: 1831\n",
      "batch 659, loss: 0.0190, instance_loss: 0.0068, weighted_loss: 0.0153, label: 0, bag_size: 1690\n",
      "batch 679, loss: 0.0000, instance_loss: 0.2661, weighted_loss: 0.0799, label: 1, bag_size: 13194\n",
      "batch 699, loss: 0.2044, instance_loss: 0.4391, weighted_loss: 0.2748, label: 0, bag_size: 546\n",
      "batch 719, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 18240\n",
      "batch 739, loss: 0.0004, instance_loss: 0.0500, weighted_loss: 0.0153, label: 1, bag_size: 10671\n",
      "batch 759, loss: 0.0040, instance_loss: 0.0668, weighted_loss: 0.0228, label: 0, bag_size: 1891\n",
      "batch 779, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 7191\n",
      "batch 799, loss: 0.0283, instance_loss: 0.0125, weighted_loss: 0.0236, label: 1, bag_size: 8264\n",
      "batch 819, loss: 0.3377, instance_loss: 0.1766, weighted_loss: 0.2894, label: 1, bag_size: 7148\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.982816301703163: correct 12926/13152\n",
      "class 1 clustering acc 0.920316301703163: correct 6052/6576\n",
      "Epoch: 69, train_loss: 0.1483, train_clustering_loss:  0.1565, train_error: 0.0523\n",
      "class 0: acc 0.940149625935162, correct 377/401\n",
      "class 1: acc 0.9548693586698337, correct 402/421\n",
      "\n",
      "Val Set, val_loss: 0.1490, val_error: 0.0435, auc: 0.9810\n",
      "class 0 clustering acc 0.9538043478260869: correct 1404/1472\n",
      "class 1 clustering acc 0.8396739130434783: correct 618/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "EarlyStopping counter: 15 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0160, instance_loss: 0.0004, weighted_loss: 0.0113, label: 1, bag_size: 5256\n",
      "batch 39, loss: 0.5220, instance_loss: 0.1054, weighted_loss: 0.3970, label: 1, bag_size: 15192\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0016, weighted_loss: 0.0005, label: 1, bag_size: 3004\n",
      "batch 79, loss: 0.0101, instance_loss: 0.0705, weighted_loss: 0.0282, label: 1, bag_size: 1294\n",
      "batch 99, loss: 0.1209, instance_loss: 0.0104, weighted_loss: 0.0878, label: 1, bag_size: 5629\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 9759\n",
      "batch 139, loss: 0.0229, instance_loss: 0.0092, weighted_loss: 0.0188, label: 1, bag_size: 3674\n",
      "batch 159, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 7709\n",
      "batch 179, loss: 0.0541, instance_loss: 0.1751, weighted_loss: 0.0904, label: 1, bag_size: 9548\n",
      "batch 199, loss: 0.0436, instance_loss: 0.0000, weighted_loss: 0.0305, label: 0, bag_size: 9069\n",
      "batch 219, loss: 0.0000, instance_loss: 0.1751, weighted_loss: 0.0525, label: 1, bag_size: 2136\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0019, weighted_loss: 0.0006, label: 1, bag_size: 4442\n",
      "batch 259, loss: 0.0078, instance_loss: 0.2081, weighted_loss: 0.0679, label: 0, bag_size: 2043\n",
      "batch 279, loss: 0.1181, instance_loss: 0.1284, weighted_loss: 0.1212, label: 1, bag_size: 12712\n",
      "batch 299, loss: 0.0817, instance_loss: 0.1264, weighted_loss: 0.0951, label: 1, bag_size: 15192\n",
      "batch 319, loss: 0.0726, instance_loss: 0.3408, weighted_loss: 0.1530, label: 1, bag_size: 15192\n",
      "batch 339, loss: 3.7185, instance_loss: 3.7783, weighted_loss: 3.7364, label: 0, bag_size: 2732\n",
      "batch 359, loss: 0.1058, instance_loss: 0.2363, weighted_loss: 0.1449, label: 1, bag_size: 8264\n",
      "batch 379, loss: 0.0008, instance_loss: 0.1226, weighted_loss: 0.0373, label: 0, bag_size: 1651\n",
      "batch 399, loss: 0.0165, instance_loss: 0.0343, weighted_loss: 0.0218, label: 0, bag_size: 13023\n",
      "batch 419, loss: 0.0513, instance_loss: 0.0443, weighted_loss: 0.0492, label: 1, bag_size: 13362\n",
      "batch 439, loss: 0.2336, instance_loss: 0.0381, weighted_loss: 0.1750, label: 1, bag_size: 1831\n",
      "batch 459, loss: 0.0004, instance_loss: 2.3341, weighted_loss: 0.7005, label: 1, bag_size: 2904\n",
      "batch 479, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 8981\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19472\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 18944\n",
      "batch 539, loss: 0.0013, instance_loss: 0.0299, weighted_loss: 0.0099, label: 0, bag_size: 22828\n",
      "batch 559, loss: 0.0015, instance_loss: 0.0033, weighted_loss: 0.0021, label: 0, bag_size: 11199\n",
      "batch 579, loss: 0.0505, instance_loss: 0.0059, weighted_loss: 0.0371, label: 1, bag_size: 9942\n",
      "batch 599, loss: 0.0006, instance_loss: 0.0218, weighted_loss: 0.0069, label: 0, bag_size: 13777\n",
      "batch 619, loss: 0.0327, instance_loss: 0.1664, weighted_loss: 0.0728, label: 0, bag_size: 3444\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 0, bag_size: 19472\n",
      "batch 659, loss: 0.0028, instance_loss: 0.0147, weighted_loss: 0.0064, label: 0, bag_size: 1884\n",
      "batch 679, loss: 0.0051, instance_loss: 0.0464, weighted_loss: 0.0175, label: 0, bag_size: 2236\n",
      "batch 699, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 5318\n",
      "batch 719, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 1, bag_size: 13786\n",
      "batch 739, loss: 0.0001, instance_loss: 0.0705, weighted_loss: 0.0212, label: 0, bag_size: 1984\n",
      "batch 759, loss: 0.0009, instance_loss: 0.3723, weighted_loss: 0.1123, label: 1, bag_size: 3856\n",
      "batch 779, loss: 0.0010, instance_loss: 0.0031, weighted_loss: 0.0016, label: 1, bag_size: 9062\n",
      "batch 799, loss: 0.4700, instance_loss: 0.0181, weighted_loss: 0.3344, label: 0, bag_size: 11122\n",
      "batch 819, loss: 0.2850, instance_loss: 0.0014, weighted_loss: 0.2000, label: 1, bag_size: 21450\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9809914841849149: correct 12902/13152\n",
      "class 1 clustering acc 0.9210766423357665: correct 6057/6576\n",
      "Epoch: 70, train_loss: 0.1158, train_clustering_loss:  0.1797, train_error: 0.0401\n",
      "class 0: acc 0.9548872180451128, correct 381/399\n",
      "class 1: acc 0.9645390070921985, correct 408/423\n",
      "\n",
      "Val Set, val_loss: 0.1596, val_error: 0.0435, auc: 0.9800\n",
      "class 0 clustering acc 0.9836956521739131: correct 1448/1472\n",
      "class 1 clustering acc 0.9578804347826086: correct 705/736\n",
      "class 0: acc 0.9210526315789473, correct 35/38\n",
      "class 1: acc 0.9814814814814815, correct 53/54\n",
      "EarlyStopping counter: 16 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1341, instance_loss: 0.1226, weighted_loss: 0.1306, label: 1, bag_size: 2522\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 1, bag_size: 14202\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 14515\n",
      "batch 79, loss: 1.3356, instance_loss: 0.0087, weighted_loss: 0.9376, label: 0, bag_size: 5211\n",
      "batch 99, loss: 0.1376, instance_loss: 0.0477, weighted_loss: 0.1106, label: 1, bag_size: 1294\n",
      "batch 119, loss: 0.0128, instance_loss: 0.0161, weighted_loss: 0.0138, label: 1, bag_size: 1294\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0685, weighted_loss: 0.0206, label: 1, bag_size: 2278\n",
      "batch 159, loss: 0.0048, instance_loss: 0.0001, weighted_loss: 0.0034, label: 1, bag_size: 17579\n",
      "batch 179, loss: 0.0004, instance_loss: 0.0031, weighted_loss: 0.0012, label: 1, bag_size: 2193\n",
      "batch 199, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 29270\n",
      "batch 219, loss: 0.0006, instance_loss: 0.0761, weighted_loss: 0.0232, label: 1, bag_size: 13194\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11865\n",
      "batch 259, loss: 0.0131, instance_loss: 0.1717, weighted_loss: 0.0606, label: 1, bag_size: 29832\n",
      "batch 279, loss: 0.0073, instance_loss: 0.0420, weighted_loss: 0.0177, label: 0, bag_size: 7637\n",
      "batch 299, loss: 0.0345, instance_loss: 0.0434, weighted_loss: 0.0372, label: 0, bag_size: 3089\n",
      "batch 319, loss: 0.0046, instance_loss: 0.0018, weighted_loss: 0.0038, label: 1, bag_size: 2682\n",
      "batch 339, loss: 0.0025, instance_loss: 0.0549, weighted_loss: 0.0182, label: 0, bag_size: 2091\n",
      "batch 359, loss: 0.1744, instance_loss: 0.0028, weighted_loss: 0.1229, label: 0, bag_size: 13992\n",
      "batch 379, loss: 0.0754, instance_loss: 0.0073, weighted_loss: 0.0549, label: 0, bag_size: 17482\n",
      "batch 399, loss: 0.7665, instance_loss: 0.0933, weighted_loss: 0.5645, label: 0, bag_size: 7239\n",
      "batch 419, loss: 0.0003, instance_loss: 0.0053, weighted_loss: 0.0018, label: 1, bag_size: 11387\n",
      "batch 439, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 27012\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 19659\n",
      "batch 479, loss: 0.0154, instance_loss: 1.2336, weighted_loss: 0.3809, label: 0, bag_size: 2367\n",
      "batch 499, loss: 0.0888, instance_loss: 0.0134, weighted_loss: 0.0662, label: 0, bag_size: 10113\n",
      "batch 519, loss: 0.0045, instance_loss: 0.2209, weighted_loss: 0.0694, label: 1, bag_size: 11884\n",
      "batch 539, loss: 0.0292, instance_loss: 0.0410, weighted_loss: 0.0327, label: 0, bag_size: 11122\n",
      "batch 559, loss: 0.0016, instance_loss: 0.0472, weighted_loss: 0.0153, label: 1, bag_size: 2412\n",
      "batch 579, loss: 0.0023, instance_loss: 0.0428, weighted_loss: 0.0144, label: 0, bag_size: 2296\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19659\n",
      "batch 619, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 21874\n",
      "batch 639, loss: 0.0585, instance_loss: 0.1192, weighted_loss: 0.0767, label: 1, bag_size: 10622\n",
      "batch 659, loss: 0.7595, instance_loss: 0.0033, weighted_loss: 0.5326, label: 0, bag_size: 2269\n",
      "batch 679, loss: 0.0002, instance_loss: 0.0959, weighted_loss: 0.0289, label: 0, bag_size: 2652\n",
      "batch 699, loss: 0.0003, instance_loss: 0.2557, weighted_loss: 0.0769, label: 1, bag_size: 6453\n",
      "batch 719, loss: 0.0005, instance_loss: 0.6339, weighted_loss: 0.1905, label: 1, bag_size: 8602\n",
      "batch 739, loss: 0.0018, instance_loss: 0.1102, weighted_loss: 0.0343, label: 0, bag_size: 1483\n",
      "batch 759, loss: 0.0003, instance_loss: 0.0789, weighted_loss: 0.0239, label: 0, bag_size: 9930\n",
      "batch 779, loss: 0.0050, instance_loss: 0.0000, weighted_loss: 0.0035, label: 0, bag_size: 13795\n",
      "batch 799, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 6752\n",
      "batch 819, loss: 0.0167, instance_loss: 0.0008, weighted_loss: 0.0119, label: 1, bag_size: 2412\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9854014598540146: correct 12960/13152\n",
      "class 1 clustering acc 0.930352798053528: correct 6118/6576\n",
      "Epoch: 71, train_loss: 0.1463, train_clustering_loss:  0.1392, train_error: 0.0572\n",
      "class 0: acc 0.9466357308584686, correct 408/431\n",
      "class 1: acc 0.9386189258312021, correct 367/391\n",
      "\n",
      "Val Set, val_loss: 0.1627, val_error: 0.0435, auc: 0.9815\n",
      "class 0 clustering acc 0.9816576086956522: correct 1445/1472\n",
      "class 1 clustering acc 0.9334239130434783: correct 687/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "EarlyStopping counter: 17 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0194, instance_loss: 0.0065, weighted_loss: 0.0156, label: 0, bag_size: 13023\n",
      "batch 39, loss: 1.6283, instance_loss: 0.3924, weighted_loss: 1.2575, label: 1, bag_size: 13367\n",
      "batch 59, loss: 0.0011, instance_loss: 0.3263, weighted_loss: 0.0986, label: 1, bag_size: 3409\n",
      "batch 79, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 24911\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11778\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0013, weighted_loss: 0.0005, label: 1, bag_size: 20537\n",
      "batch 139, loss: 0.0488, instance_loss: 0.0343, weighted_loss: 0.0444, label: 1, bag_size: 8592\n",
      "batch 159, loss: 0.0107, instance_loss: 0.0082, weighted_loss: 0.0100, label: 0, bag_size: 4345\n",
      "batch 179, loss: 0.0044, instance_loss: 0.0454, weighted_loss: 0.0167, label: 0, bag_size: 2548\n",
      "batch 199, loss: 0.0276, instance_loss: 0.0000, weighted_loss: 0.0193, label: 0, bag_size: 3876\n",
      "batch 219, loss: 0.0082, instance_loss: 0.0099, weighted_loss: 0.0087, label: 1, bag_size: 7468\n",
      "batch 239, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 17919\n",
      "batch 259, loss: 0.2706, instance_loss: 0.0108, weighted_loss: 0.1926, label: 0, bag_size: 8420\n",
      "batch 279, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 11917\n",
      "batch 299, loss: 0.0092, instance_loss: 0.1589, weighted_loss: 0.0541, label: 0, bag_size: 8427\n",
      "batch 319, loss: 0.0078, instance_loss: 0.0998, weighted_loss: 0.0354, label: 0, bag_size: 2457\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 1, bag_size: 5612\n",
      "batch 359, loss: 0.3038, instance_loss: 0.7315, weighted_loss: 0.4321, label: 1, bag_size: 2937\n",
      "batch 379, loss: 0.0012, instance_loss: 0.0005, weighted_loss: 0.0010, label: 1, bag_size: 2412\n",
      "batch 399, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 8868\n",
      "batch 419, loss: 0.0003, instance_loss: 0.1574, weighted_loss: 0.0474, label: 1, bag_size: 8935\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 12137\n",
      "batch 459, loss: 0.0011, instance_loss: 0.7533, weighted_loss: 0.2268, label: 0, bag_size: 3101\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 23796\n",
      "batch 499, loss: 0.0575, instance_loss: 0.0087, weighted_loss: 0.0429, label: 0, bag_size: 3908\n",
      "batch 519, loss: 3.6201, instance_loss: 0.1626, weighted_loss: 2.5828, label: 1, bag_size: 20870\n",
      "batch 539, loss: 0.0102, instance_loss: 0.0019, weighted_loss: 0.0077, label: 1, bag_size: 6090\n",
      "batch 559, loss: 0.0405, instance_loss: 0.0084, weighted_loss: 0.0309, label: 0, bag_size: 8549\n",
      "batch 579, loss: 0.0018, instance_loss: 0.0041, weighted_loss: 0.0025, label: 1, bag_size: 5318\n",
      "batch 599, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0014, label: 0, bag_size: 5297\n",
      "batch 619, loss: 0.0752, instance_loss: 0.0005, weighted_loss: 0.0528, label: 1, bag_size: 1230\n",
      "batch 639, loss: 0.0287, instance_loss: 0.0276, weighted_loss: 0.0284, label: 0, bag_size: 10415\n",
      "batch 659, loss: 0.0460, instance_loss: 0.6256, weighted_loss: 0.2199, label: 0, bag_size: 47866\n",
      "batch 679, loss: 0.0046, instance_loss: 0.0108, weighted_loss: 0.0065, label: 0, bag_size: 2044\n",
      "batch 699, loss: 0.0004, instance_loss: 0.1767, weighted_loss: 0.0533, label: 1, bag_size: 12127\n",
      "batch 719, loss: 0.0021, instance_loss: 0.0096, weighted_loss: 0.0043, label: 1, bag_size: 3683\n",
      "batch 739, loss: 0.0174, instance_loss: 0.2533, weighted_loss: 0.0882, label: 1, bag_size: 983\n",
      "batch 759, loss: 0.4405, instance_loss: 0.5160, weighted_loss: 0.4631, label: 1, bag_size: 3121\n",
      "batch 779, loss: 0.0011, instance_loss: 0.0003, weighted_loss: 0.0009, label: 0, bag_size: 16211\n",
      "batch 799, loss: 0.0376, instance_loss: 0.0000, weighted_loss: 0.0263, label: 0, bag_size: 10814\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0225, weighted_loss: 0.0068, label: 1, bag_size: 1609\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9851733576642335: correct 12957/13152\n",
      "class 1 clustering acc 0.9387165450121655: correct 6173/6576\n",
      "Epoch: 72, train_loss: 0.1014, train_clustering_loss:  0.1369, train_error: 0.0328\n",
      "class 0: acc 0.9733009708737864, correct 401/412\n",
      "class 1: acc 0.9609756097560975, correct 394/410\n",
      "\n",
      "Val Set, val_loss: 0.1900, val_error: 0.0543, auc: 0.9781\n",
      "class 0 clustering acc 0.9653532608695652: correct 1421/1472\n",
      "class 1 clustering acc 0.8804347826086957: correct 648/736\n",
      "class 0: acc 0.8947368421052632, correct 34/38\n",
      "class 1: acc 0.9814814814814815, correct 53/54\n",
      "EarlyStopping counter: 18 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0251, instance_loss: 0.0350, weighted_loss: 0.0280, label: 0, bag_size: 1630\n",
      "batch 39, loss: 0.0032, instance_loss: 0.0724, weighted_loss: 0.0240, label: 1, bag_size: 8475\n",
      "batch 59, loss: 0.0052, instance_loss: 0.0002, weighted_loss: 0.0037, label: 1, bag_size: 6781\n",
      "batch 79, loss: 0.0083, instance_loss: 0.0000, weighted_loss: 0.0058, label: 0, bag_size: 11900\n",
      "batch 99, loss: 0.0003, instance_loss: 0.0800, weighted_loss: 0.0242, label: 0, bag_size: 9485\n",
      "batch 119, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 25027\n",
      "batch 139, loss: 0.0015, instance_loss: 0.0009, weighted_loss: 0.0013, label: 0, bag_size: 16341\n",
      "batch 159, loss: 0.0983, instance_loss: 0.0050, weighted_loss: 0.0703, label: 1, bag_size: 2842\n",
      "batch 179, loss: 0.0251, instance_loss: 0.0638, weighted_loss: 0.0367, label: 1, bag_size: 5921\n",
      "batch 199, loss: 0.0014, instance_loss: 0.0135, weighted_loss: 0.0050, label: 0, bag_size: 1149\n",
      "batch 219, loss: 0.0084, instance_loss: 0.0046, weighted_loss: 0.0072, label: 0, bag_size: 1438\n",
      "batch 239, loss: 0.7874, instance_loss: 0.0062, weighted_loss: 0.5530, label: 0, bag_size: 2070\n",
      "batch 259, loss: 0.0357, instance_loss: 0.0086, weighted_loss: 0.0276, label: 0, bag_size: 24382\n",
      "batch 279, loss: 0.0010, instance_loss: 0.0328, weighted_loss: 0.0106, label: 0, bag_size: 1149\n",
      "batch 299, loss: 0.0003, instance_loss: 0.9563, weighted_loss: 0.2871, label: 0, bag_size: 11900\n",
      "batch 319, loss: 0.0002, instance_loss: 0.0234, weighted_loss: 0.0071, label: 1, bag_size: 18095\n",
      "batch 339, loss: 0.0230, instance_loss: 0.3334, weighted_loss: 0.1161, label: 0, bag_size: 6281\n",
      "batch 359, loss: 0.3452, instance_loss: 0.1859, weighted_loss: 0.2974, label: 0, bag_size: 1630\n",
      "batch 379, loss: 0.3420, instance_loss: 3.5273, weighted_loss: 1.2976, label: 1, bag_size: 15185\n",
      "batch 399, loss: 0.5786, instance_loss: 0.0223, weighted_loss: 0.4117, label: 1, bag_size: 1764\n",
      "batch 419, loss: 0.0546, instance_loss: 0.0108, weighted_loss: 0.0415, label: 0, bag_size: 11390\n",
      "batch 439, loss: 0.0364, instance_loss: 0.0464, weighted_loss: 0.0394, label: 0, bag_size: 3321\n",
      "batch 459, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 7110\n",
      "batch 479, loss: 0.1503, instance_loss: 0.5528, weighted_loss: 0.2710, label: 1, bag_size: 2314\n",
      "batch 499, loss: 0.0045, instance_loss: 0.0000, weighted_loss: 0.0031, label: 0, bag_size: 8661\n",
      "batch 519, loss: 0.0169, instance_loss: 0.0244, weighted_loss: 0.0192, label: 1, bag_size: 4929\n",
      "batch 539, loss: 0.0126, instance_loss: 0.0054, weighted_loss: 0.0104, label: 1, bag_size: 12697\n",
      "batch 559, loss: 0.0002, instance_loss: 0.0013, weighted_loss: 0.0005, label: 1, bag_size: 20537\n",
      "batch 579, loss: 0.0503, instance_loss: 0.0133, weighted_loss: 0.0392, label: 1, bag_size: 18699\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0011, weighted_loss: 0.0004, label: 1, bag_size: 10392\n",
      "batch 619, loss: 0.0665, instance_loss: 0.0183, weighted_loss: 0.0520, label: 0, bag_size: 11281\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 16052\n",
      "batch 659, loss: 0.0662, instance_loss: 0.0001, weighted_loss: 0.0464, label: 1, bag_size: 16565\n",
      "batch 679, loss: 0.0125, instance_loss: 0.0000, weighted_loss: 0.0087, label: 0, bag_size: 14333\n",
      "batch 699, loss: 0.0063, instance_loss: 0.0291, weighted_loss: 0.0131, label: 1, bag_size: 1242\n",
      "batch 719, loss: 0.0001, instance_loss: 0.0059, weighted_loss: 0.0018, label: 1, bag_size: 4128\n",
      "batch 739, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 0, bag_size: 11735\n",
      "batch 759, loss: 0.0007, instance_loss: 0.0192, weighted_loss: 0.0063, label: 0, bag_size: 7823\n",
      "batch 779, loss: 0.0000, instance_loss: 0.0110, weighted_loss: 0.0033, label: 0, bag_size: 8898\n",
      "batch 799, loss: 0.0002, instance_loss: 0.0365, weighted_loss: 0.0111, label: 0, bag_size: 11759\n",
      "batch 819, loss: 0.0008, instance_loss: 0.0192, weighted_loss: 0.0063, label: 0, bag_size: 1452\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9825121654501217: correct 12922/13152\n",
      "class 1 clustering acc 0.9180352798053528: correct 6037/6576\n",
      "Epoch: 73, train_loss: 0.1002, train_clustering_loss:  0.1653, train_error: 0.0353\n",
      "class 0: acc 0.9618138424821002, correct 403/419\n",
      "class 1: acc 0.967741935483871, correct 390/403\n",
      "\n",
      "Val Set, val_loss: 0.2430, val_error: 0.0652, auc: 0.9747\n",
      "class 0 clustering acc 0.9809782608695652: correct 1444/1472\n",
      "class 1 clustering acc 0.9130434782608695: correct 672/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.9259259259259259, correct 50/54\n",
      "EarlyStopping counter: 19 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0052, instance_loss: 0.2472, weighted_loss: 0.0778, label: 1, bag_size: 3450\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 27158\n",
      "batch 59, loss: 0.0223, instance_loss: 0.1025, weighted_loss: 0.0463, label: 0, bag_size: 6884\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0027, weighted_loss: 0.0008, label: 0, bag_size: 14266\n",
      "batch 99, loss: 0.2828, instance_loss: 0.0214, weighted_loss: 0.2044, label: 1, bag_size: 7613\n",
      "batch 119, loss: 1.0886, instance_loss: 2.1682, weighted_loss: 1.4125, label: 0, bag_size: 11306\n",
      "batch 139, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 8330\n",
      "batch 159, loss: 0.0003, instance_loss: 0.0049, weighted_loss: 0.0017, label: 0, bag_size: 3657\n",
      "batch 179, loss: 0.0212, instance_loss: 0.2011, weighted_loss: 0.0752, label: 1, bag_size: 1236\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 23037\n",
      "batch 219, loss: 0.1560, instance_loss: 0.0140, weighted_loss: 0.1134, label: 1, bag_size: 7066\n",
      "batch 239, loss: 0.0001, instance_loss: 0.0131, weighted_loss: 0.0040, label: 0, bag_size: 14956\n",
      "batch 259, loss: 0.0964, instance_loss: 0.0233, weighted_loss: 0.0745, label: 1, bag_size: 15192\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0020, weighted_loss: 0.0007, label: 1, bag_size: 8410\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0005, weighted_loss: 0.0002, label: 0, bag_size: 24911\n",
      "batch 319, loss: 0.2109, instance_loss: 0.0269, weighted_loss: 0.1557, label: 1, bag_size: 7613\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0773, weighted_loss: 0.0233, label: 1, bag_size: 2278\n",
      "batch 359, loss: 0.0067, instance_loss: 0.0580, weighted_loss: 0.0221, label: 0, bag_size: 1651\n",
      "batch 379, loss: 0.0005, instance_loss: 0.3760, weighted_loss: 0.1131, label: 0, bag_size: 14305\n",
      "batch 399, loss: 0.0001, instance_loss: 0.0103, weighted_loss: 0.0032, label: 0, bag_size: 19472\n",
      "batch 419, loss: 0.0001, instance_loss: 0.0880, weighted_loss: 0.0265, label: 0, bag_size: 9930\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0489, weighted_loss: 0.0147, label: 1, bag_size: 14604\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0089, weighted_loss: 0.0027, label: 0, bag_size: 14266\n",
      "batch 479, loss: 0.0007, instance_loss: 0.0059, weighted_loss: 0.0022, label: 1, bag_size: 2193\n",
      "batch 499, loss: 0.1164, instance_loss: 4.3235, weighted_loss: 1.3785, label: 1, bag_size: 12180\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0118, weighted_loss: 0.0035, label: 1, bag_size: 6875\n",
      "batch 539, loss: 0.0013, instance_loss: 0.0073, weighted_loss: 0.0031, label: 1, bag_size: 10346\n",
      "batch 559, loss: 0.0147, instance_loss: 0.0273, weighted_loss: 0.0185, label: 0, bag_size: 803\n",
      "batch 579, loss: 0.0221, instance_loss: 0.0198, weighted_loss: 0.0214, label: 0, bag_size: 1800\n",
      "batch 599, loss: 0.0052, instance_loss: 1.9715, weighted_loss: 0.5951, label: 0, bag_size: 2814\n",
      "batch 619, loss: 0.0005, instance_loss: 0.0196, weighted_loss: 0.0062, label: 0, bag_size: 9252\n",
      "batch 639, loss: 0.0343, instance_loss: 0.0014, weighted_loss: 0.0245, label: 1, bag_size: 15192\n",
      "batch 659, loss: 0.0035, instance_loss: 0.0169, weighted_loss: 0.0075, label: 0, bag_size: 10721\n",
      "batch 679, loss: 0.0002, instance_loss: 0.3198, weighted_loss: 0.0960, label: 0, bag_size: 12732\n",
      "batch 699, loss: 0.0003, instance_loss: 0.1397, weighted_loss: 0.0421, label: 1, bag_size: 10105\n",
      "batch 719, loss: 0.0111, instance_loss: 0.0095, weighted_loss: 0.0106, label: 1, bag_size: 3674\n",
      "batch 739, loss: 0.0044, instance_loss: 0.0616, weighted_loss: 0.0215, label: 0, bag_size: 12083\n",
      "batch 759, loss: 0.0001, instance_loss: 0.0030, weighted_loss: 0.0010, label: 0, bag_size: 31780\n",
      "batch 779, loss: 0.0133, instance_loss: 0.0177, weighted_loss: 0.0146, label: 0, bag_size: 1458\n",
      "batch 799, loss: 0.0001, instance_loss: 0.0005, weighted_loss: 0.0002, label: 1, bag_size: 6769\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 7191\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9806873479318735: correct 12898/13152\n",
      "class 1 clustering acc 0.9206204379562044: correct 6054/6576\n",
      "Epoch: 74, train_loss: 0.1237, train_clustering_loss:  0.1813, train_error: 0.0474\n",
      "class 0: acc 0.9561200923787528, correct 414/433\n",
      "class 1: acc 0.9485861182519281, correct 369/389\n",
      "\n",
      "Val Set, val_loss: 0.2063, val_error: 0.0435, auc: 0.9742\n",
      "class 0 clustering acc 0.9626358695652174: correct 1417/1472\n",
      "class 1 clustering acc 0.84375: correct 621/736\n",
      "class 0: acc 0.9473684210526315, correct 36/38\n",
      "class 1: acc 0.9629629629629629, correct 52/54\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Val error: 0.0326, ROC AUC: 0.9976\n",
      "Test error: 0.0619, ROC AUC: 0.9915\n",
      "class 0: acc 0.8627450980392157, correct 44/51\n",
      "class 1: acc 1.0, correct 62/62\n",
      "\n",
      "Training Fold 3!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 830 samples\n",
      "Validating on 97 samples\n",
      "Testing on 100 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Setting tau to 1.0\n",
      "Done!\n",
      "MCBAT_SB(\n",
      "  (instance_loss_fn): SmoothTop1SVM()\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (transformer_low): TransformerEncoder_FLASH(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FLASH(\n",
      "            (attn_fn): LaplacianAttnFn()\n",
      "            (rel_pos_bias): T5RelativePositionBias(\n",
      "              (relative_attention_bias): Embedding(32, 1)\n",
      "            )\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (to_hidden): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (to_qk): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (qk_offset_scale): OffsetScale()\n",
      "            (to_out): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer_high): TransformerEncoder_FLASH(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FLASH(\n",
      "            (attn_fn): LaplacianAttnFn()\n",
      "            (rel_pos_bias): T5RelativePositionBias(\n",
      "              (relative_attention_bias): Embedding(32, 1)\n",
      "            )\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (to_hidden): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (to_qk): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (qk_offset_scale): OffsetScale()\n",
      "            (to_out): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attention): Attn_Net_Gated(\n",
      "    (attention_a): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_b): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (fusion_encoder): FusionEncoder()\n",
      "  (attention_V2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (attention_U2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (attention_weights2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Total number of parameters: 17086091\n",
      "Total number of trainable parameters: 17086091\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... 2\n",
      "Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7612, instance_loss: 1.0962, weighted_loss: 0.8617, label: 0, bag_size: 21138\n",
      "batch 39, loss: 0.4709, instance_loss: 1.6724, weighted_loss: 0.8313, label: 1, bag_size: 1924\n",
      "batch 59, loss: 0.6031, instance_loss: 1.6866, weighted_loss: 0.9282, label: 1, bag_size: 6734\n",
      "batch 79, loss: 0.8074, instance_loss: 0.7042, weighted_loss: 0.7764, label: 0, bag_size: 5551\n",
      "batch 99, loss: 0.6967, instance_loss: 0.7734, weighted_loss: 0.7197, label: 1, bag_size: 1051\n",
      "batch 119, loss: 0.5019, instance_loss: 0.7639, weighted_loss: 0.5805, label: 1, bag_size: 9230\n",
      "batch 139, loss: 0.8358, instance_loss: 3.3002, weighted_loss: 1.5751, label: 0, bag_size: 1498\n",
      "batch 159, loss: 0.9661, instance_loss: 1.7748, weighted_loss: 1.2087, label: 0, bag_size: 6367\n",
      "batch 179, loss: 0.6032, instance_loss: 0.9051, weighted_loss: 0.6938, label: 1, bag_size: 2935\n",
      "batch 199, loss: 0.7904, instance_loss: 2.5815, weighted_loss: 1.3278, label: 0, bag_size: 18777\n",
      "batch 219, loss: 0.6252, instance_loss: 0.7825, weighted_loss: 0.6724, label: 1, bag_size: 5345\n",
      "batch 239, loss: 0.4932, instance_loss: 1.0595, weighted_loss: 0.6631, label: 1, bag_size: 1572\n",
      "batch 259, loss: 0.5808, instance_loss: 1.6193, weighted_loss: 0.8924, label: 1, bag_size: 25831\n",
      "batch 279, loss: 0.6714, instance_loss: 0.5712, weighted_loss: 0.6414, label: 1, bag_size: 9878\n",
      "batch 299, loss: 0.6458, instance_loss: 1.6565, weighted_loss: 0.9490, label: 1, bag_size: 6734\n",
      "batch 319, loss: 0.7186, instance_loss: 0.9366, weighted_loss: 0.7840, label: 0, bag_size: 16341\n",
      "batch 339, loss: 0.4419, instance_loss: 0.5311, weighted_loss: 0.4686, label: 0, bag_size: 2518\n",
      "batch 359, loss: 1.0028, instance_loss: 1.7099, weighted_loss: 1.2149, label: 1, bag_size: 621\n",
      "batch 379, loss: 0.5629, instance_loss: 0.5278, weighted_loss: 0.5524, label: 0, bag_size: 11690\n",
      "batch 399, loss: 0.8853, instance_loss: 2.7672, weighted_loss: 1.4499, label: 1, bag_size: 15332\n",
      "batch 419, loss: 0.7388, instance_loss: 0.7370, weighted_loss: 0.7382, label: 1, bag_size: 11964\n",
      "batch 439, loss: 0.6342, instance_loss: 0.4812, weighted_loss: 0.5883, label: 1, bag_size: 6343\n",
      "batch 459, loss: 1.0850, instance_loss: 0.4105, weighted_loss: 0.8827, label: 0, bag_size: 7557\n",
      "batch 479, loss: 0.8434, instance_loss: 0.8595, weighted_loss: 0.8482, label: 1, bag_size: 1412\n",
      "batch 499, loss: 0.6845, instance_loss: 0.7765, weighted_loss: 0.7121, label: 0, bag_size: 1684\n",
      "batch 519, loss: 0.7501, instance_loss: 0.4425, weighted_loss: 0.6578, label: 1, bag_size: 9321\n",
      "batch 539, loss: 0.6003, instance_loss: 3.4529, weighted_loss: 1.4561, label: 1, bag_size: 10432\n",
      "batch 559, loss: 0.6228, instance_loss: 0.4182, weighted_loss: 0.5614, label: 1, bag_size: 4250\n",
      "batch 579, loss: 0.6948, instance_loss: 0.5958, weighted_loss: 0.6651, label: 1, bag_size: 5310\n",
      "batch 599, loss: 0.4720, instance_loss: 0.0945, weighted_loss: 0.3588, label: 1, bag_size: 5023\n",
      "batch 619, loss: 0.4550, instance_loss: 1.0075, weighted_loss: 0.6208, label: 1, bag_size: 8264\n",
      "batch 639, loss: 0.4731, instance_loss: 0.8092, weighted_loss: 0.5740, label: 1, bag_size: 9877\n",
      "batch 659, loss: 0.8176, instance_loss: 0.4848, weighted_loss: 0.7178, label: 0, bag_size: 1415\n",
      "batch 679, loss: 0.8117, instance_loss: 1.0751, weighted_loss: 0.8907, label: 1, bag_size: 13015\n",
      "batch 699, loss: 0.7779, instance_loss: 0.0003, weighted_loss: 0.5446, label: 0, bag_size: 14435\n",
      "batch 719, loss: 0.7835, instance_loss: 0.6145, weighted_loss: 0.7328, label: 0, bag_size: 1452\n",
      "batch 739, loss: 0.4370, instance_loss: 0.3077, weighted_loss: 0.3982, label: 0, bag_size: 16341\n",
      "batch 759, loss: 0.4594, instance_loss: 0.7357, weighted_loss: 0.5422, label: 0, bag_size: 15914\n",
      "batch 779, loss: 0.8212, instance_loss: 1.6137, weighted_loss: 1.0590, label: 1, bag_size: 8660\n",
      "batch 799, loss: 0.8709, instance_loss: 1.7055, weighted_loss: 1.1212, label: 0, bag_size: 2282\n",
      "batch 819, loss: 0.8784, instance_loss: 0.4710, weighted_loss: 0.7562, label: 0, bag_size: 10304\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.8970632530120481: correct 11913/13280\n",
      "class 1 clustering acc 0.3060240963855422: correct 2032/6640\n",
      "Epoch: 0, train_loss: 0.7049, train_clustering_loss:  1.1451, train_error: 0.5000\n",
      "class 0: acc 0.3598014888337469, correct 145/403\n",
      "class 1: acc 0.6323185011709602, correct 270/427\n",
      "\n",
      "Val Set, val_loss: 0.6930, val_error: 0.4845, auc: 0.3881\n",
      "class 0 clustering acc 0.928479381443299: correct 1441/1552\n",
      "class 1 clustering acc 0.49742268041237114: correct 386/776\n",
      "class 0: acc 0.0, correct 0/47\n",
      "class 1: acc 1.0, correct 50/50\n",
      "Validation loss decreased (inf --> 0.692999).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7577, instance_loss: 0.0765, weighted_loss: 0.5533, label: 1, bag_size: 6731\n",
      "batch 39, loss: 0.6665, instance_loss: 0.5283, weighted_loss: 0.6251, label: 1, bag_size: 9610\n",
      "batch 59, loss: 0.4351, instance_loss: 1.2956, weighted_loss: 0.6933, label: 0, bag_size: 2534\n",
      "batch 79, loss: 0.7853, instance_loss: 1.5690, weighted_loss: 1.0204, label: 0, bag_size: 3725\n",
      "batch 99, loss: 0.5578, instance_loss: 0.0937, weighted_loss: 0.4186, label: 0, bag_size: 1072\n",
      "batch 119, loss: 0.7732, instance_loss: 0.0174, weighted_loss: 0.5465, label: 1, bag_size: 6776\n",
      "batch 139, loss: 0.5633, instance_loss: 0.7314, weighted_loss: 0.6137, label: 1, bag_size: 10501\n",
      "batch 159, loss: 0.7411, instance_loss: 2.0401, weighted_loss: 1.1308, label: 1, bag_size: 12758\n",
      "batch 179, loss: 0.6405, instance_loss: 0.1716, weighted_loss: 0.4998, label: 0, bag_size: 10581\n",
      "batch 199, loss: 0.7324, instance_loss: 0.2584, weighted_loss: 0.5902, label: 0, bag_size: 1588\n",
      "batch 219, loss: 0.8100, instance_loss: 0.9930, weighted_loss: 0.8649, label: 1, bag_size: 12946\n",
      "batch 239, loss: 0.7000, instance_loss: 0.4407, weighted_loss: 0.6222, label: 1, bag_size: 10912\n",
      "batch 259, loss: 0.7385, instance_loss: 0.7200, weighted_loss: 0.7329, label: 0, bag_size: 2336\n",
      "batch 279, loss: 0.6968, instance_loss: 0.3117, weighted_loss: 0.5813, label: 1, bag_size: 11316\n",
      "batch 299, loss: 0.7213, instance_loss: 1.3522, weighted_loss: 0.9106, label: 0, bag_size: 3190\n",
      "batch 319, loss: 0.6682, instance_loss: 0.5968, weighted_loss: 0.6468, label: 1, bag_size: 16417\n",
      "batch 339, loss: 0.7242, instance_loss: 0.1670, weighted_loss: 0.5571, label: 0, bag_size: 15850\n",
      "batch 359, loss: 0.6913, instance_loss: 0.0293, weighted_loss: 0.4927, label: 1, bag_size: 1781\n",
      "batch 379, loss: 0.5893, instance_loss: 1.7420, weighted_loss: 0.9351, label: 1, bag_size: 2695\n",
      "batch 399, loss: 0.6386, instance_loss: 0.1215, weighted_loss: 0.4834, label: 0, bag_size: 10581\n",
      "batch 419, loss: 0.5840, instance_loss: 0.0074, weighted_loss: 0.4110, label: 1, bag_size: 4250\n",
      "batch 439, loss: 1.1085, instance_loss: 0.5415, weighted_loss: 0.9384, label: 0, bag_size: 1920\n",
      "batch 459, loss: 0.5972, instance_loss: 0.0136, weighted_loss: 0.4221, label: 1, bag_size: 6682\n",
      "batch 479, loss: 1.0012, instance_loss: 0.0008, weighted_loss: 0.7010, label: 1, bag_size: 16512\n",
      "batch 499, loss: 1.0547, instance_loss: 0.9751, weighted_loss: 1.0308, label: 1, bag_size: 1786\n",
      "batch 519, loss: 0.8751, instance_loss: 1.1080, weighted_loss: 0.9450, label: 1, bag_size: 4976\n",
      "batch 539, loss: 0.5020, instance_loss: 0.4181, weighted_loss: 0.4768, label: 0, bag_size: 14893\n",
      "batch 559, loss: 0.4038, instance_loss: 0.7229, weighted_loss: 0.4995, label: 0, bag_size: 763\n",
      "batch 579, loss: 0.6774, instance_loss: 0.3122, weighted_loss: 0.5679, label: 1, bag_size: 6317\n",
      "batch 599, loss: 0.5790, instance_loss: 0.0444, weighted_loss: 0.4186, label: 0, bag_size: 17268\n",
      "batch 619, loss: 0.4675, instance_loss: 2.1253, weighted_loss: 0.9649, label: 0, bag_size: 3654\n",
      "batch 639, loss: 0.6987, instance_loss: 0.7146, weighted_loss: 0.7035, label: 0, bag_size: 3444\n",
      "batch 659, loss: 0.8299, instance_loss: 0.2003, weighted_loss: 0.6410, label: 0, bag_size: 9470\n",
      "batch 679, loss: 0.7618, instance_loss: 0.6303, weighted_loss: 0.7224, label: 0, bag_size: 14249\n",
      "batch 699, loss: 0.7945, instance_loss: 0.1377, weighted_loss: 0.5974, label: 1, bag_size: 18649\n",
      "batch 719, loss: 0.6574, instance_loss: 0.5214, weighted_loss: 0.6166, label: 0, bag_size: 24382\n",
      "batch 739, loss: 0.7984, instance_loss: 0.2275, weighted_loss: 0.6272, label: 1, bag_size: 8103\n",
      "batch 759, loss: 0.8209, instance_loss: 0.0404, weighted_loss: 0.5868, label: 0, bag_size: 14956\n",
      "batch 779, loss: 0.7932, instance_loss: 0.0050, weighted_loss: 0.5567, label: 1, bag_size: 15008\n",
      "batch 799, loss: 0.7626, instance_loss: 0.1535, weighted_loss: 0.5799, label: 1, bag_size: 1165\n",
      "batch 819, loss: 0.6113, instance_loss: 0.4531, weighted_loss: 0.5639, label: 1, bag_size: 3634\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9371234939759037: correct 12445/13280\n",
      "class 1 clustering acc 0.5930722891566265: correct 3938/6640\n",
      "Epoch: 1, train_loss: 0.7088, train_clustering_loss:  0.6458, train_error: 0.5133\n",
      "class 0: acc 0.5887850467289719, correct 252/428\n",
      "class 1: acc 0.3781094527363184, correct 152/402\n",
      "\n",
      "Val Set, val_loss: 0.7055, val_error: 0.4845, auc: 0.5247\n",
      "class 0 clustering acc 0.9246134020618557: correct 1435/1552\n",
      "class 1 clustering acc 0.6404639175257731: correct 497/776\n",
      "class 0: acc 0.0, correct 0/47\n",
      "class 1: acc 1.0, correct 50/50\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4399, instance_loss: 0.1704, weighted_loss: 0.3591, label: 1, bag_size: 8003\n",
      "batch 39, loss: 0.6838, instance_loss: 1.9520, weighted_loss: 1.0642, label: 1, bag_size: 1746\n",
      "batch 59, loss: 0.5146, instance_loss: 1.5362, weighted_loss: 0.8211, label: 0, bag_size: 2996\n",
      "batch 79, loss: 0.5916, instance_loss: 0.1549, weighted_loss: 0.4606, label: 0, bag_size: 10365\n",
      "batch 99, loss: 0.5460, instance_loss: 0.0597, weighted_loss: 0.4001, label: 0, bag_size: 4345\n",
      "batch 119, loss: 1.0705, instance_loss: 1.0087, weighted_loss: 1.0519, label: 1, bag_size: 16703\n",
      "batch 139, loss: 0.4078, instance_loss: 0.0089, weighted_loss: 0.2881, label: 0, bag_size: 14319\n",
      "batch 159, loss: 0.8030, instance_loss: 0.0666, weighted_loss: 0.5821, label: 1, bag_size: 1781\n",
      "batch 179, loss: 1.1176, instance_loss: 0.1817, weighted_loss: 0.8368, label: 1, bag_size: 11195\n",
      "batch 199, loss: 0.3925, instance_loss: 0.2141, weighted_loss: 0.3390, label: 0, bag_size: 20230\n",
      "batch 219, loss: 0.4604, instance_loss: 0.6191, weighted_loss: 0.5080, label: 0, bag_size: 1349\n",
      "batch 239, loss: 0.3984, instance_loss: 0.1325, weighted_loss: 0.3187, label: 0, bag_size: 20478\n",
      "batch 259, loss: 0.4549, instance_loss: 1.1331, weighted_loss: 0.6584, label: 0, bag_size: 4692\n",
      "batch 279, loss: 0.5358, instance_loss: 0.8966, weighted_loss: 0.6440, label: 0, bag_size: 10995\n",
      "batch 299, loss: 0.6880, instance_loss: 0.3076, weighted_loss: 0.5739, label: 1, bag_size: 5345\n",
      "batch 319, loss: 0.7949, instance_loss: 0.0276, weighted_loss: 0.5647, label: 1, bag_size: 9689\n",
      "batch 339, loss: 0.3951, instance_loss: 0.0100, weighted_loss: 0.2796, label: 1, bag_size: 5864\n",
      "batch 359, loss: 0.4375, instance_loss: 0.6196, weighted_loss: 0.4921, label: 1, bag_size: 6928\n",
      "batch 379, loss: 0.9832, instance_loss: 0.6684, weighted_loss: 0.8888, label: 0, bag_size: 12732\n",
      "batch 399, loss: 0.4584, instance_loss: 0.2203, weighted_loss: 0.3870, label: 1, bag_size: 20537\n",
      "batch 419, loss: 0.7581, instance_loss: 0.7549, weighted_loss: 0.7571, label: 0, bag_size: 2244\n",
      "batch 439, loss: 0.8310, instance_loss: 0.7048, weighted_loss: 0.7932, label: 1, bag_size: 6478\n",
      "batch 459, loss: 0.8280, instance_loss: 0.1371, weighted_loss: 0.6207, label: 0, bag_size: 5965\n",
      "batch 479, loss: 0.8024, instance_loss: 0.0028, weighted_loss: 0.5625, label: 1, bag_size: 9759\n",
      "batch 499, loss: 0.5714, instance_loss: 0.7437, weighted_loss: 0.6231, label: 0, bag_size: 10113\n",
      "batch 519, loss: 0.4304, instance_loss: 0.1203, weighted_loss: 0.3373, label: 0, bag_size: 12793\n",
      "batch 539, loss: 0.6250, instance_loss: 0.1720, weighted_loss: 0.4891, label: 0, bag_size: 23368\n",
      "batch 559, loss: 0.5332, instance_loss: 0.3812, weighted_loss: 0.4876, label: 1, bag_size: 8660\n",
      "batch 579, loss: 0.4566, instance_loss: 1.0118, weighted_loss: 0.6231, label: 1, bag_size: 1875\n",
      "batch 599, loss: 0.5195, instance_loss: 0.0517, weighted_loss: 0.3791, label: 1, bag_size: 20767\n",
      "batch 619, loss: 0.6806, instance_loss: 0.5989, weighted_loss: 0.6561, label: 0, bag_size: 10063\n",
      "batch 639, loss: 0.6900, instance_loss: 0.3819, weighted_loss: 0.5976, label: 1, bag_size: 21450\n",
      "batch 659, loss: 0.5768, instance_loss: 0.2257, weighted_loss: 0.4715, label: 0, bag_size: 7031\n",
      "batch 679, loss: 0.7854, instance_loss: 0.0509, weighted_loss: 0.5650, label: 1, bag_size: 5921\n",
      "batch 699, loss: 0.3952, instance_loss: 0.0495, weighted_loss: 0.2915, label: 1, bag_size: 21450\n",
      "batch 719, loss: 0.3863, instance_loss: 0.2772, weighted_loss: 0.3535, label: 1, bag_size: 11884\n",
      "batch 739, loss: 0.5794, instance_loss: 0.3023, weighted_loss: 0.4963, label: 1, bag_size: 13692\n",
      "batch 759, loss: 0.6141, instance_loss: 0.0449, weighted_loss: 0.4434, label: 0, bag_size: 30828\n",
      "batch 779, loss: 0.5821, instance_loss: 0.6337, weighted_loss: 0.5976, label: 1, bag_size: 11316\n",
      "batch 799, loss: 0.4989, instance_loss: 0.0000, weighted_loss: 0.3492, label: 1, bag_size: 6966\n",
      "batch 819, loss: 0.4344, instance_loss: 2.0402, weighted_loss: 0.9161, label: 1, bag_size: 983\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9397590361445783: correct 12480/13280\n",
      "class 1 clustering acc 0.611144578313253: correct 4058/6640\n",
      "Epoch: 2, train_loss: 0.6971, train_clustering_loss:  0.6189, train_error: 0.4458\n",
      "class 0: acc 0.5904761904761905, correct 248/420\n",
      "class 1: acc 0.5170731707317073, correct 212/410\n",
      "\n",
      "Val Set, val_loss: 0.7067, val_error: 0.4845, auc: 0.6366\n",
      "class 0 clustering acc 0.9987113402061856: correct 1550/1552\n",
      "class 1 clustering acc 0.22293814432989692: correct 173/776\n",
      "class 0: acc 0.0, correct 0/47\n",
      "class 1: acc 1.0, correct 50/50\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5216, instance_loss: 0.0076, weighted_loss: 0.3674, label: 0, bag_size: 65728\n",
      "batch 39, loss: 0.9675, instance_loss: 0.1160, weighted_loss: 0.7120, label: 1, bag_size: 7186\n",
      "batch 59, loss: 1.0187, instance_loss: 0.0592, weighted_loss: 0.7308, label: 1, bag_size: 10558\n",
      "batch 79, loss: 0.4848, instance_loss: 0.3680, weighted_loss: 0.4498, label: 0, bag_size: 11607\n",
      "batch 99, loss: 0.4254, instance_loss: 0.0755, weighted_loss: 0.3204, label: 0, bag_size: 31106\n",
      "batch 119, loss: 0.5710, instance_loss: 0.2803, weighted_loss: 0.4838, label: 0, bag_size: 14266\n",
      "batch 139, loss: 0.6662, instance_loss: 1.5323, weighted_loss: 0.9260, label: 1, bag_size: 1794\n",
      "batch 159, loss: 0.6938, instance_loss: 0.3054, weighted_loss: 0.5773, label: 1, bag_size: 12626\n",
      "batch 179, loss: 0.9519, instance_loss: 3.0849, weighted_loss: 1.5918, label: 0, bag_size: 7428\n",
      "batch 199, loss: 0.7467, instance_loss: 0.0714, weighted_loss: 0.5441, label: 0, bag_size: 21682\n",
      "batch 219, loss: 0.5989, instance_loss: 0.7725, weighted_loss: 0.6510, label: 0, bag_size: 20478\n",
      "batch 239, loss: 0.6281, instance_loss: 0.0054, weighted_loss: 0.4413, label: 1, bag_size: 22286\n",
      "batch 259, loss: 0.4321, instance_loss: 0.1347, weighted_loss: 0.3429, label: 1, bag_size: 13051\n",
      "batch 279, loss: 0.9526, instance_loss: 0.1042, weighted_loss: 0.6981, label: 0, bag_size: 15841\n",
      "batch 299, loss: 0.7448, instance_loss: 0.2863, weighted_loss: 0.6073, label: 1, bag_size: 6171\n",
      "batch 319, loss: 0.6970, instance_loss: 0.1333, weighted_loss: 0.5279, label: 0, bag_size: 13591\n",
      "batch 339, loss: 0.5422, instance_loss: 0.0840, weighted_loss: 0.4048, label: 0, bag_size: 17919\n",
      "batch 359, loss: 0.8743, instance_loss: 0.6111, weighted_loss: 0.7953, label: 1, bag_size: 2785\n",
      "batch 379, loss: 0.8868, instance_loss: 3.2934, weighted_loss: 1.6087, label: 1, bag_size: 1015\n",
      "batch 399, loss: 0.6864, instance_loss: 0.0661, weighted_loss: 0.5004, label: 1, bag_size: 29832\n",
      "batch 419, loss: 0.4349, instance_loss: 0.3232, weighted_loss: 0.4014, label: 0, bag_size: 803\n",
      "batch 439, loss: 1.0458, instance_loss: 0.0383, weighted_loss: 0.7436, label: 1, bag_size: 15093\n",
      "batch 459, loss: 0.4323, instance_loss: 0.3080, weighted_loss: 0.3950, label: 0, bag_size: 3708\n",
      "batch 479, loss: 0.4076, instance_loss: 2.0564, weighted_loss: 0.9022, label: 0, bag_size: 11128\n",
      "batch 499, loss: 0.6625, instance_loss: 0.0290, weighted_loss: 0.4725, label: 1, bag_size: 15125\n",
      "batch 519, loss: 0.5599, instance_loss: 0.3291, weighted_loss: 0.4907, label: 1, bag_size: 10848\n",
      "batch 539, loss: 0.7503, instance_loss: 0.1135, weighted_loss: 0.5593, label: 0, bag_size: 5225\n",
      "batch 559, loss: 0.8513, instance_loss: 0.6518, weighted_loss: 0.7915, label: 0, bag_size: 1909\n",
      "batch 579, loss: 0.4394, instance_loss: 0.0527, weighted_loss: 0.3234, label: 1, bag_size: 7583\n",
      "batch 599, loss: 0.4228, instance_loss: 0.0175, weighted_loss: 0.3012, label: 1, bag_size: 10492\n",
      "batch 619, loss: 0.9186, instance_loss: 0.3014, weighted_loss: 0.7335, label: 0, bag_size: 1437\n",
      "batch 639, loss: 0.6389, instance_loss: 0.0049, weighted_loss: 0.4487, label: 0, bag_size: 13225\n",
      "batch 659, loss: 0.7564, instance_loss: 0.2697, weighted_loss: 0.6104, label: 0, bag_size: 1415\n",
      "batch 679, loss: 0.8044, instance_loss: 0.0927, weighted_loss: 0.5909, label: 0, bag_size: 16720\n",
      "batch 699, loss: 0.4771, instance_loss: 0.0755, weighted_loss: 0.3566, label: 0, bag_size: 2624\n",
      "batch 719, loss: 0.3863, instance_loss: 0.0869, weighted_loss: 0.2965, label: 0, bag_size: 14266\n",
      "batch 739, loss: 0.9395, instance_loss: 0.0047, weighted_loss: 0.6590, label: 1, bag_size: 18603\n",
      "batch 759, loss: 0.6734, instance_loss: 0.1275, weighted_loss: 0.5096, label: 0, bag_size: 2628\n",
      "batch 779, loss: 0.6786, instance_loss: 0.5411, weighted_loss: 0.6373, label: 1, bag_size: 4480\n",
      "batch 799, loss: 0.7026, instance_loss: 0.3132, weighted_loss: 0.5858, label: 0, bag_size: 2918\n",
      "batch 819, loss: 0.6741, instance_loss: 0.0828, weighted_loss: 0.4967, label: 0, bag_size: 3232\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9459337349397591: correct 12562/13280\n",
      "class 1 clustering acc 0.7206325301204819: correct 4785/6640\n",
      "Epoch: 3, train_loss: 0.6996, train_clustering_loss:  0.4964, train_error: 0.4614\n",
      "class 0: acc 0.6152073732718893, correct 267/434\n",
      "class 1: acc 0.45454545454545453, correct 180/396\n",
      "\n",
      "Val Set, val_loss: 0.7268, val_error: 0.5155, auc: 0.7026\n",
      "class 0 clustering acc 0.8975515463917526: correct 1393/1552\n",
      "class 1 clustering acc 0.645618556701031: correct 501/776\n",
      "class 0: acc 1.0, correct 47/47\n",
      "class 1: acc 0.0, correct 0/50\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5644, instance_loss: 0.0814, weighted_loss: 0.4195, label: 0, bag_size: 10814\n",
      "batch 39, loss: 1.0247, instance_loss: 0.0020, weighted_loss: 0.7179, label: 1, bag_size: 4877\n",
      "batch 59, loss: 0.8347, instance_loss: 0.0103, weighted_loss: 0.5874, label: 1, bag_size: 30675\n",
      "batch 79, loss: 0.4645, instance_loss: 0.1029, weighted_loss: 0.3560, label: 0, bag_size: 30751\n",
      "batch 99, loss: 0.6577, instance_loss: 0.0381, weighted_loss: 0.4718, label: 0, bag_size: 19067\n",
      "batch 119, loss: 0.6144, instance_loss: 0.0168, weighted_loss: 0.4351, label: 0, bag_size: 21138\n",
      "batch 139, loss: 0.5570, instance_loss: 0.2149, weighted_loss: 0.4544, label: 0, bag_size: 8866\n",
      "batch 159, loss: 0.6588, instance_loss: 0.1092, weighted_loss: 0.4939, label: 1, bag_size: 11875\n",
      "batch 179, loss: 0.8090, instance_loss: 1.8673, weighted_loss: 1.1265, label: 0, bag_size: 2070\n",
      "batch 199, loss: 0.9393, instance_loss: 0.1354, weighted_loss: 0.6982, label: 0, bag_size: 9471\n",
      "batch 219, loss: 0.5520, instance_loss: 0.5700, weighted_loss: 0.5574, label: 0, bag_size: 14264\n",
      "batch 239, loss: 0.3864, instance_loss: 1.1647, weighted_loss: 0.6199, label: 0, bag_size: 2266\n",
      "batch 259, loss: 0.7395, instance_loss: 0.6905, weighted_loss: 0.7248, label: 0, bag_size: 12732\n",
      "batch 279, loss: 0.5515, instance_loss: 0.1491, weighted_loss: 0.4308, label: 0, bag_size: 2044\n",
      "batch 299, loss: 0.7976, instance_loss: 0.2213, weighted_loss: 0.6247, label: 1, bag_size: 1249\n",
      "batch 319, loss: 1.0344, instance_loss: 3.1717, weighted_loss: 1.6756, label: 0, bag_size: 15057\n",
      "batch 339, loss: 0.5706, instance_loss: 1.5583, weighted_loss: 0.8669, label: 1, bag_size: 6478\n",
      "batch 359, loss: 0.8482, instance_loss: 0.0217, weighted_loss: 0.6002, label: 1, bag_size: 17486\n",
      "batch 379, loss: 0.9016, instance_loss: 0.0310, weighted_loss: 0.6404, label: 1, bag_size: 8019\n",
      "batch 399, loss: 0.7270, instance_loss: 0.2603, weighted_loss: 0.5869, label: 1, bag_size: 8680\n",
      "batch 419, loss: 0.8107, instance_loss: 0.7086, weighted_loss: 0.7800, label: 0, bag_size: 14249\n",
      "batch 439, loss: 0.7889, instance_loss: 0.2877, weighted_loss: 0.6385, label: 1, bag_size: 1249\n",
      "batch 459, loss: 0.6155, instance_loss: 0.0021, weighted_loss: 0.4315, label: 1, bag_size: 5292\n",
      "batch 479, loss: 0.9297, instance_loss: 0.2217, weighted_loss: 0.7173, label: 0, bag_size: 6093\n",
      "batch 499, loss: 0.7628, instance_loss: 0.0003, weighted_loss: 0.5340, label: 1, bag_size: 12931\n",
      "batch 519, loss: 0.7326, instance_loss: 1.4057, weighted_loss: 0.9345, label: 1, bag_size: 1493\n",
      "batch 539, loss: 0.3958, instance_loss: 0.1524, weighted_loss: 0.3228, label: 0, bag_size: 14885\n",
      "batch 559, loss: 0.8950, instance_loss: 0.0248, weighted_loss: 0.6340, label: 1, bag_size: 5991\n",
      "batch 579, loss: 0.8076, instance_loss: 0.4568, weighted_loss: 0.7024, label: 1, bag_size: 7389\n",
      "batch 599, loss: 0.8661, instance_loss: 0.2800, weighted_loss: 0.6903, label: 0, bag_size: 15967\n",
      "batch 619, loss: 0.7399, instance_loss: 0.0780, weighted_loss: 0.5413, label: 0, bag_size: 19470\n",
      "batch 639, loss: 0.6512, instance_loss: 0.0578, weighted_loss: 0.4732, label: 0, bag_size: 10721\n",
      "batch 659, loss: 0.7384, instance_loss: 0.6431, weighted_loss: 0.7098, label: 1, bag_size: 25695\n",
      "batch 679, loss: 0.5673, instance_loss: 0.1078, weighted_loss: 0.4295, label: 1, bag_size: 11884\n",
      "batch 699, loss: 0.6580, instance_loss: 0.4454, weighted_loss: 0.5942, label: 0, bag_size: 14893\n",
      "batch 719, loss: 0.6307, instance_loss: 0.6312, weighted_loss: 0.6308, label: 0, bag_size: 11281\n",
      "batch 739, loss: 0.7838, instance_loss: 0.0533, weighted_loss: 0.5647, label: 1, bag_size: 19606\n",
      "batch 759, loss: 0.6840, instance_loss: 1.1960, weighted_loss: 0.8376, label: 1, bag_size: 7981\n",
      "batch 779, loss: 0.5381, instance_loss: 0.8253, weighted_loss: 0.6243, label: 0, bag_size: 15255\n",
      "batch 799, loss: 1.0149, instance_loss: 0.0584, weighted_loss: 0.7280, label: 1, bag_size: 10346\n",
      "batch 819, loss: 0.7050, instance_loss: 0.0843, weighted_loss: 0.5188, label: 0, bag_size: 15057\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9478915662650602: correct 12588/13280\n",
      "class 1 clustering acc 0.7121987951807229: correct 4729/6640\n",
      "Epoch: 4, train_loss: 0.7039, train_clustering_loss:  0.4869, train_error: 0.4916\n",
      "class 0: acc 0.6435185185185185, correct 278/432\n",
      "class 1: acc 0.36180904522613067, correct 144/398\n",
      "\n",
      "Val Set, val_loss: 0.6958, val_error: 0.5155, auc: 0.7609\n",
      "class 0 clustering acc 0.9271907216494846: correct 1439/1552\n",
      "class 1 clustering acc 0.7422680412371134: correct 576/776\n",
      "class 0: acc 1.0, correct 47/47\n",
      "class 1: acc 0.0, correct 0/50\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6385, instance_loss: 0.0561, weighted_loss: 0.4637, label: 0, bag_size: 11527\n",
      "batch 39, loss: 0.6236, instance_loss: 0.0249, weighted_loss: 0.4440, label: 1, bag_size: 5991\n",
      "batch 59, loss: 0.7092, instance_loss: 7.2575, weighted_loss: 2.6737, label: 0, bag_size: 5105\n",
      "batch 79, loss: 0.9155, instance_loss: 0.5560, weighted_loss: 0.8077, label: 0, bag_size: 7031\n",
      "batch 99, loss: 0.7050, instance_loss: 0.0869, weighted_loss: 0.5196, label: 0, bag_size: 7557\n",
      "batch 119, loss: 0.8159, instance_loss: 0.0382, weighted_loss: 0.5826, label: 1, bag_size: 25970\n",
      "batch 139, loss: 0.8981, instance_loss: 1.3515, weighted_loss: 1.0341, label: 0, bag_size: 5639\n",
      "batch 159, loss: 0.8019, instance_loss: 0.4949, weighted_loss: 0.7098, label: 0, bag_size: 1826\n",
      "batch 179, loss: 0.4795, instance_loss: 1.2321, weighted_loss: 0.7053, label: 1, bag_size: 9983\n",
      "batch 199, loss: 0.4925, instance_loss: 0.0027, weighted_loss: 0.3455, label: 1, bag_size: 12931\n",
      "batch 219, loss: 0.8360, instance_loss: 0.0919, weighted_loss: 0.6128, label: 0, bag_size: 13591\n",
      "batch 239, loss: 0.6958, instance_loss: 0.1875, weighted_loss: 0.5433, label: 0, bag_size: 7923\n",
      "batch 259, loss: 0.7216, instance_loss: 0.0447, weighted_loss: 0.5186, label: 1, bag_size: 10112\n",
      "batch 279, loss: 0.6250, instance_loss: 0.0086, weighted_loss: 0.4401, label: 0, bag_size: 18954\n",
      "batch 299, loss: 0.7542, instance_loss: 0.4849, weighted_loss: 0.6734, label: 0, bag_size: 17482\n",
      "batch 319, loss: 0.4727, instance_loss: 0.1512, weighted_loss: 0.3762, label: 1, bag_size: 10848\n",
      "batch 339, loss: 0.7782, instance_loss: 0.0196, weighted_loss: 0.5506, label: 0, bag_size: 13225\n",
      "batch 359, loss: 0.8208, instance_loss: 2.9590, weighted_loss: 1.4623, label: 1, bag_size: 2937\n",
      "batch 379, loss: 0.9507, instance_loss: 0.3764, weighted_loss: 0.7784, label: 0, bag_size: 13218\n",
      "batch 399, loss: 0.9211, instance_loss: 0.2848, weighted_loss: 0.7302, label: 0, bag_size: 11527\n",
      "batch 419, loss: 0.7583, instance_loss: 0.6676, weighted_loss: 0.7311, label: 0, bag_size: 2269\n",
      "batch 439, loss: 0.7538, instance_loss: 0.3789, weighted_loss: 0.6414, label: 1, bag_size: 621\n",
      "batch 459, loss: 0.6666, instance_loss: 0.2918, weighted_loss: 0.5541, label: 1, bag_size: 1572\n",
      "batch 479, loss: 0.5072, instance_loss: 1.6971, weighted_loss: 0.8641, label: 1, bag_size: 2681\n",
      "batch 499, loss: 0.6017, instance_loss: 0.6299, weighted_loss: 0.6101, label: 1, bag_size: 10966\n",
      "batch 519, loss: 0.5071, instance_loss: 0.1116, weighted_loss: 0.3884, label: 1, bag_size: 2405\n",
      "batch 539, loss: 0.5714, instance_loss: 4.7049, weighted_loss: 1.8114, label: 1, bag_size: 2759\n",
      "batch 559, loss: 0.6467, instance_loss: 0.8285, weighted_loss: 0.7012, label: 1, bag_size: 5155\n",
      "batch 579, loss: 0.9398, instance_loss: 0.3217, weighted_loss: 0.7544, label: 0, bag_size: 9433\n",
      "batch 599, loss: 0.9359, instance_loss: 1.1559, weighted_loss: 1.0019, label: 0, bag_size: 2959\n",
      "batch 619, loss: 0.9156, instance_loss: 0.2049, weighted_loss: 0.7024, label: 0, bag_size: 18132\n",
      "batch 639, loss: 0.7080, instance_loss: 0.0170, weighted_loss: 0.5007, label: 1, bag_size: 4039\n",
      "batch 659, loss: 0.5990, instance_loss: 0.0436, weighted_loss: 0.4324, label: 0, bag_size: 3459\n",
      "batch 679, loss: 0.4508, instance_loss: 0.1006, weighted_loss: 0.3457, label: 0, bag_size: 2296\n",
      "batch 699, loss: 0.6017, instance_loss: 2.0214, weighted_loss: 1.0276, label: 0, bag_size: 1142\n",
      "batch 719, loss: 0.5030, instance_loss: 0.1114, weighted_loss: 0.3855, label: 0, bag_size: 16690\n",
      "batch 739, loss: 0.7155, instance_loss: 0.3159, weighted_loss: 0.5956, label: 0, bag_size: 17155\n",
      "batch 759, loss: 0.3729, instance_loss: 0.0383, weighted_loss: 0.2725, label: 0, bag_size: 17630\n",
      "batch 779, loss: 0.6149, instance_loss: 0.0384, weighted_loss: 0.4420, label: 0, bag_size: 7557\n",
      "batch 799, loss: 1.0091, instance_loss: 0.5932, weighted_loss: 0.8843, label: 1, bag_size: 1493\n",
      "batch 819, loss: 0.6687, instance_loss: 0.4551, weighted_loss: 0.6046, label: 1, bag_size: 2559\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9533885542168675: correct 12661/13280\n",
      "class 1 clustering acc 0.7423192771084337: correct 4929/6640\n",
      "Epoch: 5, train_loss: 0.6991, train_clustering_loss:  0.4712, train_error: 0.4807\n",
      "class 0: acc 0.42857142857142855, correct 171/399\n",
      "class 1: acc 0.6032482598607889, correct 260/431\n",
      "\n",
      "Val Set, val_loss: 0.6867, val_error: 0.4845, auc: 0.7936\n",
      "class 0 clustering acc 0.8408505154639175: correct 1305/1552\n",
      "class 1 clustering acc 0.5541237113402062: correct 430/776\n",
      "class 0: acc 0.0, correct 0/47\n",
      "class 1: acc 1.0, correct 50/50\n",
      "Validation loss decreased (0.692999 --> 0.686705).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3820, instance_loss: 0.1926, weighted_loss: 0.3252, label: 1, bag_size: 4367\n",
      "batch 39, loss: 0.5484, instance_loss: 0.0503, weighted_loss: 0.3990, label: 1, bag_size: 7371\n",
      "batch 59, loss: 0.6290, instance_loss: 0.0745, weighted_loss: 0.4626, label: 1, bag_size: 8602\n",
      "batch 79, loss: 0.6001, instance_loss: 0.0064, weighted_loss: 0.4220, label: 1, bag_size: 13174\n",
      "batch 99, loss: 0.7438, instance_loss: 0.0401, weighted_loss: 0.5327, label: 0, bag_size: 12510\n",
      "batch 119, loss: 0.7025, instance_loss: 0.0171, weighted_loss: 0.4969, label: 0, bag_size: 16087\n",
      "batch 139, loss: 0.7741, instance_loss: 0.0743, weighted_loss: 0.5642, label: 0, bag_size: 9471\n",
      "batch 159, loss: 0.8476, instance_loss: 1.2993, weighted_loss: 0.9831, label: 0, bag_size: 2968\n",
      "batch 179, loss: 0.7020, instance_loss: 0.0346, weighted_loss: 0.5017, label: 1, bag_size: 14604\n",
      "batch 199, loss: 0.5255, instance_loss: 0.1171, weighted_loss: 0.4030, label: 0, bag_size: 9234\n",
      "batch 219, loss: 0.6394, instance_loss: 0.8613, weighted_loss: 0.7060, label: 0, bag_size: 10063\n",
      "batch 239, loss: 0.4740, instance_loss: 0.2017, weighted_loss: 0.3923, label: 1, bag_size: 15931\n",
      "batch 259, loss: 0.5757, instance_loss: 0.0195, weighted_loss: 0.4088, label: 0, bag_size: 8948\n",
      "batch 279, loss: 0.3846, instance_loss: 0.0165, weighted_loss: 0.2742, label: 0, bag_size: 10581\n",
      "batch 299, loss: 0.8202, instance_loss: 0.1968, weighted_loss: 0.6331, label: 1, bag_size: 11964\n",
      "batch 319, loss: 0.7091, instance_loss: 0.1262, weighted_loss: 0.5342, label: 1, bag_size: 10498\n",
      "batch 339, loss: 0.7356, instance_loss: 0.8105, weighted_loss: 0.7581, label: 1, bag_size: 4308\n",
      "batch 359, loss: 0.4905, instance_loss: 0.0307, weighted_loss: 0.3525, label: 0, bag_size: 23398\n",
      "batch 379, loss: 0.6797, instance_loss: 0.0007, weighted_loss: 0.4760, label: 1, bag_size: 9548\n",
      "batch 399, loss: 0.7571, instance_loss: 0.0397, weighted_loss: 0.5419, label: 0, bag_size: 6624\n",
      "batch 419, loss: 0.7963, instance_loss: 0.1279, weighted_loss: 0.5958, label: 0, bag_size: 31780\n",
      "batch 439, loss: 0.5833, instance_loss: 0.0027, weighted_loss: 0.4091, label: 1, bag_size: 14681\n",
      "batch 459, loss: 0.7894, instance_loss: 0.0111, weighted_loss: 0.5559, label: 1, bag_size: 11642\n",
      "batch 479, loss: 0.5387, instance_loss: 2.9108, weighted_loss: 1.2503, label: 0, bag_size: 3802\n",
      "batch 499, loss: 1.0444, instance_loss: 0.0309, weighted_loss: 0.7404, label: 1, bag_size: 10492\n",
      "batch 519, loss: 0.6740, instance_loss: 0.0008, weighted_loss: 0.4720, label: 1, bag_size: 7583\n",
      "batch 539, loss: 0.6321, instance_loss: 0.0002, weighted_loss: 0.4425, label: 1, bag_size: 10394\n",
      "batch 559, loss: 0.7478, instance_loss: 0.3400, weighted_loss: 0.6254, label: 0, bag_size: 16936\n",
      "batch 579, loss: 0.5465, instance_loss: 0.7437, weighted_loss: 0.6056, label: 0, bag_size: 14893\n",
      "batch 599, loss: 0.8055, instance_loss: 0.0000, weighted_loss: 0.5639, label: 1, bag_size: 15213\n",
      "batch 619, loss: 0.6766, instance_loss: 6.6802, weighted_loss: 2.4777, label: 0, bag_size: 5105\n",
      "batch 639, loss: 0.8887, instance_loss: 0.9184, weighted_loss: 0.8976, label: 0, bag_size: 12731\n",
      "batch 659, loss: 0.7458, instance_loss: 0.7496, weighted_loss: 0.7470, label: 1, bag_size: 4442\n",
      "batch 679, loss: 0.4136, instance_loss: 0.0301, weighted_loss: 0.2986, label: 0, bag_size: 21682\n",
      "batch 699, loss: 1.1594, instance_loss: 0.0342, weighted_loss: 0.8218, label: 1, bag_size: 12714\n",
      "batch 719, loss: 0.7142, instance_loss: 0.1883, weighted_loss: 0.5564, label: 1, bag_size: 2848\n",
      "batch 739, loss: 0.7564, instance_loss: 0.9463, weighted_loss: 0.8134, label: 1, bag_size: 771\n",
      "batch 759, loss: 0.5821, instance_loss: 0.1882, weighted_loss: 0.4639, label: 0, bag_size: 22762\n",
      "batch 779, loss: 0.6562, instance_loss: 0.0908, weighted_loss: 0.4866, label: 1, bag_size: 1244\n",
      "batch 799, loss: 0.6857, instance_loss: 0.0792, weighted_loss: 0.5037, label: 0, bag_size: 7637\n",
      "batch 819, loss: 0.5960, instance_loss: 1.6817, weighted_loss: 0.9217, label: 0, bag_size: 4692\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9531626506024097: correct 12658/13280\n",
      "class 1 clustering acc 0.7704819277108433: correct 5116/6640\n",
      "Epoch: 6, train_loss: 0.6879, train_clustering_loss:  0.4324, train_error: 0.4578\n",
      "class 0: acc 0.6264501160092807, correct 270/431\n",
      "class 1: acc 0.45112781954887216, correct 180/399\n",
      "\n",
      "Val Set, val_loss: 0.6706, val_error: 0.2990, auc: 0.8209\n",
      "class 0 clustering acc 0.9922680412371134: correct 1540/1552\n",
      "class 1 clustering acc 0.3801546391752577: correct 295/776\n",
      "class 0: acc 0.5531914893617021, correct 26/47\n",
      "class 1: acc 0.84, correct 42/50\n",
      "Validation loss decreased (0.686705 --> 0.670581).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5103, instance_loss: 0.1142, weighted_loss: 0.3915, label: 1, bag_size: 14433\n",
      "batch 39, loss: 0.9130, instance_loss: 0.3987, weighted_loss: 0.7587, label: 1, bag_size: 9147\n",
      "batch 59, loss: 0.6449, instance_loss: 0.1835, weighted_loss: 0.5065, label: 0, bag_size: 7637\n",
      "batch 79, loss: 0.7056, instance_loss: 0.6430, weighted_loss: 0.6869, label: 1, bag_size: 699\n",
      "batch 99, loss: 0.8115, instance_loss: 0.2690, weighted_loss: 0.6488, label: 1, bag_size: 19972\n",
      "batch 119, loss: 0.8134, instance_loss: 0.2547, weighted_loss: 0.6458, label: 0, bag_size: 19435\n",
      "batch 139, loss: 0.4707, instance_loss: 0.0391, weighted_loss: 0.3412, label: 1, bag_size: 1969\n",
      "batch 159, loss: 1.0413, instance_loss: 0.5328, weighted_loss: 0.8888, label: 0, bag_size: 11922\n",
      "batch 179, loss: 0.7243, instance_loss: 0.0746, weighted_loss: 0.5294, label: 0, bag_size: 1884\n",
      "batch 199, loss: 0.6602, instance_loss: 0.0174, weighted_loss: 0.4674, label: 1, bag_size: 13365\n",
      "batch 219, loss: 0.7226, instance_loss: 0.8521, weighted_loss: 0.7615, label: 1, bag_size: 7613\n",
      "batch 239, loss: 0.7521, instance_loss: 0.0001, weighted_loss: 0.5265, label: 1, bag_size: 15213\n",
      "batch 259, loss: 0.5175, instance_loss: 1.0121, weighted_loss: 0.6659, label: 1, bag_size: 1794\n",
      "batch 279, loss: 0.5009, instance_loss: 0.0036, weighted_loss: 0.3517, label: 1, bag_size: 3683\n",
      "batch 299, loss: 0.7008, instance_loss: 0.0944, weighted_loss: 0.5189, label: 0, bag_size: 4271\n",
      "batch 319, loss: 0.7283, instance_loss: 0.2470, weighted_loss: 0.5839, label: 0, bag_size: 2382\n",
      "batch 339, loss: 0.8206, instance_loss: 0.2985, weighted_loss: 0.6639, label: 0, bag_size: 5999\n",
      "batch 359, loss: 0.5878, instance_loss: 5.2368, weighted_loss: 1.9825, label: 1, bag_size: 15185\n",
      "batch 379, loss: 0.7851, instance_loss: 0.2454, weighted_loss: 0.6232, label: 0, bag_size: 4523\n",
      "batch 399, loss: 0.4661, instance_loss: 0.0004, weighted_loss: 0.3264, label: 1, bag_size: 30675\n",
      "batch 419, loss: 0.6557, instance_loss: 1.0640, weighted_loss: 0.7782, label: 0, bag_size: 11306\n",
      "batch 439, loss: 0.7239, instance_loss: 1.3564, weighted_loss: 0.9137, label: 1, bag_size: 1191\n",
      "batch 459, loss: 0.5008, instance_loss: 0.0435, weighted_loss: 0.3636, label: 0, bag_size: 15672\n",
      "batch 479, loss: 0.4892, instance_loss: 0.0868, weighted_loss: 0.3685, label: 0, bag_size: 2006\n",
      "batch 499, loss: 0.6675, instance_loss: 0.9983, weighted_loss: 0.7667, label: 1, bag_size: 1294\n",
      "batch 519, loss: 0.6907, instance_loss: 0.0769, weighted_loss: 0.5065, label: 0, bag_size: 9583\n",
      "batch 539, loss: 0.8278, instance_loss: 0.0667, weighted_loss: 0.5994, label: 1, bag_size: 3651\n",
      "batch 559, loss: 0.6130, instance_loss: 0.0000, weighted_loss: 0.4291, label: 1, bag_size: 13732\n",
      "batch 579, loss: 0.6080, instance_loss: 1.5880, weighted_loss: 0.9020, label: 1, bag_size: 2904\n",
      "batch 599, loss: 0.5721, instance_loss: 0.1665, weighted_loss: 0.4504, label: 0, bag_size: 16087\n",
      "batch 619, loss: 0.5648, instance_loss: 0.0318, weighted_loss: 0.4049, label: 1, bag_size: 15093\n",
      "batch 639, loss: 0.5985, instance_loss: 0.0000, weighted_loss: 0.4190, label: 1, bag_size: 15464\n",
      "batch 659, loss: 0.8743, instance_loss: 1.6659, weighted_loss: 1.1118, label: 1, bag_size: 15563\n",
      "batch 679, loss: 0.5818, instance_loss: 0.0002, weighted_loss: 0.4073, label: 1, bag_size: 16417\n",
      "batch 699, loss: 0.8170, instance_loss: 1.1423, weighted_loss: 0.9146, label: 1, bag_size: 3651\n",
      "batch 719, loss: 0.5566, instance_loss: 0.1405, weighted_loss: 0.4317, label: 0, bag_size: 17083\n",
      "batch 739, loss: 0.4954, instance_loss: 0.0130, weighted_loss: 0.3507, label: 1, bag_size: 5561\n",
      "batch 759, loss: 0.7107, instance_loss: 0.1687, weighted_loss: 0.5481, label: 0, bag_size: 3760\n",
      "batch 779, loss: 0.3639, instance_loss: 0.0693, weighted_loss: 0.2755, label: 0, bag_size: 13691\n",
      "batch 799, loss: 0.4117, instance_loss: 0.0000, weighted_loss: 0.2882, label: 1, bag_size: 11195\n",
      "batch 819, loss: 0.8406, instance_loss: 2.1377, weighted_loss: 1.2298, label: 0, bag_size: 19808\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9568524096385542: correct 12707/13280\n",
      "class 1 clustering acc 0.7671686746987951: correct 5094/6640\n",
      "Epoch: 7, train_loss: 0.6581, train_clustering_loss:  0.4451, train_error: 0.3904\n",
      "class 0: acc 0.5896805896805897, correct 240/407\n",
      "class 1: acc 0.6288416075650118, correct 266/423\n",
      "\n",
      "Val Set, val_loss: 0.6326, val_error: 0.2990, auc: 0.8302\n",
      "class 0 clustering acc 0.9375: correct 1455/1552\n",
      "class 1 clustering acc 0.7860824742268041: correct 610/776\n",
      "class 0: acc 0.5531914893617021, correct 26/47\n",
      "class 1: acc 0.84, correct 42/50\n",
      "Validation loss decreased (0.670581 --> 0.632607).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6096, instance_loss: 0.2603, weighted_loss: 0.5048, label: 0, bag_size: 3321\n",
      "batch 39, loss: 0.6989, instance_loss: 0.8199, weighted_loss: 0.7352, label: 1, bag_size: 5366\n",
      "batch 59, loss: 0.7007, instance_loss: 0.6746, weighted_loss: 0.6929, label: 1, bag_size: 10848\n",
      "batch 79, loss: 0.4721, instance_loss: 0.4093, weighted_loss: 0.4533, label: 1, bag_size: 16162\n",
      "batch 99, loss: 0.6611, instance_loss: 0.1425, weighted_loss: 0.5055, label: 0, bag_size: 11212\n",
      "batch 119, loss: 0.3514, instance_loss: 0.0992, weighted_loss: 0.2758, label: 1, bag_size: 2278\n",
      "batch 139, loss: 0.8558, instance_loss: 0.1213, weighted_loss: 0.6355, label: 0, bag_size: 11125\n",
      "batch 159, loss: 0.6556, instance_loss: 0.0188, weighted_loss: 0.4646, label: 1, bag_size: 12801\n",
      "batch 179, loss: 0.4888, instance_loss: 0.1893, weighted_loss: 0.3990, label: 0, bag_size: 9433\n",
      "batch 199, loss: 0.8114, instance_loss: 0.3751, weighted_loss: 0.6805, label: 0, bag_size: 10146\n",
      "batch 219, loss: 0.4177, instance_loss: 0.0812, weighted_loss: 0.3168, label: 1, bag_size: 4054\n",
      "batch 239, loss: 0.2759, instance_loss: 0.2581, weighted_loss: 0.2705, label: 1, bag_size: 9548\n",
      "batch 259, loss: 0.5639, instance_loss: 0.0248, weighted_loss: 0.4022, label: 0, bag_size: 18516\n",
      "batch 279, loss: 0.5119, instance_loss: 0.0000, weighted_loss: 0.3583, label: 1, bag_size: 4789\n",
      "batch 299, loss: 0.4601, instance_loss: 0.0013, weighted_loss: 0.3225, label: 1, bag_size: 11875\n",
      "batch 319, loss: 0.6179, instance_loss: 0.2110, weighted_loss: 0.4959, label: 0, bag_size: 2968\n",
      "batch 339, loss: 0.3161, instance_loss: 0.0070, weighted_loss: 0.2234, label: 0, bag_size: 6851\n",
      "batch 359, loss: 0.7010, instance_loss: 0.0055, weighted_loss: 0.4924, label: 1, bag_size: 25970\n",
      "batch 379, loss: 0.5189, instance_loss: 0.1944, weighted_loss: 0.4215, label: 0, bag_size: 12796\n",
      "batch 399, loss: 0.5333, instance_loss: 0.0142, weighted_loss: 0.3776, label: 0, bag_size: 14305\n",
      "batch 419, loss: 0.7538, instance_loss: 0.5200, weighted_loss: 0.6836, label: 0, bag_size: 20478\n",
      "batch 439, loss: 0.4590, instance_loss: 0.0768, weighted_loss: 0.3443, label: 0, bag_size: 16211\n",
      "batch 459, loss: 0.5588, instance_loss: 0.0657, weighted_loss: 0.4109, label: 0, bag_size: 12510\n",
      "batch 479, loss: 0.7486, instance_loss: 0.1070, weighted_loss: 0.5561, label: 0, bag_size: 4271\n",
      "batch 499, loss: 0.4418, instance_loss: 0.0409, weighted_loss: 0.3215, label: 1, bag_size: 11160\n",
      "batch 519, loss: 0.5858, instance_loss: 0.0457, weighted_loss: 0.4238, label: 1, bag_size: 4880\n",
      "batch 539, loss: 0.8911, instance_loss: 2.5670, weighted_loss: 1.3939, label: 0, bag_size: 1437\n",
      "batch 559, loss: 0.4288, instance_loss: 0.0074, weighted_loss: 0.3024, label: 0, bag_size: 17368\n",
      "batch 579, loss: 0.8388, instance_loss: 0.7192, weighted_loss: 0.8029, label: 1, bag_size: 1172\n",
      "batch 599, loss: 0.5595, instance_loss: 1.6156, weighted_loss: 0.8763, label: 0, bag_size: 15255\n",
      "batch 619, loss: 0.3280, instance_loss: 0.0420, weighted_loss: 0.2422, label: 0, bag_size: 18240\n",
      "batch 639, loss: 0.8456, instance_loss: 0.0188, weighted_loss: 0.5975, label: 1, bag_size: 13477\n",
      "batch 659, loss: 0.8380, instance_loss: 0.0684, weighted_loss: 0.6071, label: 0, bag_size: 3502\n",
      "batch 679, loss: 0.4386, instance_loss: 0.3155, weighted_loss: 0.4017, label: 1, bag_size: 18095\n",
      "batch 699, loss: 0.7710, instance_loss: 0.8501, weighted_loss: 0.7947, label: 1, bag_size: 13477\n",
      "batch 719, loss: 0.2778, instance_loss: 0.4745, weighted_loss: 0.3368, label: 1, bag_size: 8475\n",
      "batch 739, loss: 0.3243, instance_loss: 0.0147, weighted_loss: 0.2314, label: 1, bag_size: 2848\n",
      "batch 759, loss: 0.3930, instance_loss: 0.0327, weighted_loss: 0.2849, label: 0, bag_size: 19067\n",
      "batch 779, loss: 0.5103, instance_loss: 0.0895, weighted_loss: 0.3840, label: 0, bag_size: 3502\n",
      "batch 799, loss: 0.2086, instance_loss: 0.7656, weighted_loss: 0.3757, label: 0, bag_size: 10535\n",
      "batch 819, loss: 0.4346, instance_loss: 1.6009, weighted_loss: 0.7845, label: 0, bag_size: 22800\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9510542168674698: correct 12630/13280\n",
      "class 1 clustering acc 0.7447289156626506: correct 4945/6640\n",
      "Epoch: 8, train_loss: 0.5598, train_clustering_loss:  0.4801, train_error: 0.2361\n",
      "class 0: acc 0.74, correct 296/400\n",
      "class 1: acc 0.786046511627907, correct 338/430\n",
      "\n",
      "Val Set, val_loss: 0.5645, val_error: 0.2268, auc: 0.8289\n",
      "class 0 clustering acc 0.8131443298969072: correct 1262/1552\n",
      "class 1 clustering acc 0.44716494845360827: correct 347/776\n",
      "class 0: acc 0.7659574468085106, correct 36/47\n",
      "class 1: acc 0.78, correct 39/50\n",
      "Validation loss decreased (0.632607 --> 0.564513).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3853, instance_loss: 0.5801, weighted_loss: 0.4437, label: 0, bag_size: 1437\n",
      "batch 39, loss: 0.1934, instance_loss: 0.3482, weighted_loss: 0.2398, label: 0, bag_size: 13795\n",
      "batch 59, loss: 0.2470, instance_loss: 0.2557, weighted_loss: 0.2496, label: 1, bag_size: 2193\n",
      "batch 79, loss: 1.2594, instance_loss: 0.3330, weighted_loss: 0.9815, label: 0, bag_size: 17279\n",
      "batch 99, loss: 0.2760, instance_loss: 0.0698, weighted_loss: 0.2141, label: 1, bag_size: 6792\n",
      "batch 119, loss: 1.2795, instance_loss: 2.4922, weighted_loss: 1.6433, label: 1, bag_size: 13440\n",
      "batch 139, loss: 0.6750, instance_loss: 0.9273, weighted_loss: 0.7507, label: 1, bag_size: 1236\n",
      "batch 159, loss: 0.2456, instance_loss: 0.4865, weighted_loss: 0.3179, label: 0, bag_size: 3787\n",
      "batch 179, loss: 0.5164, instance_loss: 0.5844, weighted_loss: 0.5368, label: 1, bag_size: 8103\n",
      "batch 199, loss: 0.2326, instance_loss: 0.2008, weighted_loss: 0.2230, label: 1, bag_size: 2485\n",
      "batch 219, loss: 0.2040, instance_loss: 0.2826, weighted_loss: 0.2276, label: 1, bag_size: 6665\n",
      "batch 239, loss: 0.1820, instance_loss: 0.0036, weighted_loss: 0.1285, label: 1, bag_size: 11964\n",
      "batch 259, loss: 0.8227, instance_loss: 0.3803, weighted_loss: 0.6900, label: 0, bag_size: 1891\n",
      "batch 279, loss: 0.8868, instance_loss: 1.9380, weighted_loss: 1.2021, label: 1, bag_size: 13089\n",
      "batch 299, loss: 0.4898, instance_loss: 0.3839, weighted_loss: 0.4581, label: 0, bag_size: 2814\n",
      "batch 319, loss: 0.3769, instance_loss: 0.2538, weighted_loss: 0.3399, label: 0, bag_size: 10814\n",
      "batch 339, loss: 0.6781, instance_loss: 0.2281, weighted_loss: 0.5431, label: 1, bag_size: 3211\n",
      "batch 359, loss: 0.1590, instance_loss: 0.7597, weighted_loss: 0.3392, label: 1, bag_size: 15233\n",
      "batch 379, loss: 0.6077, instance_loss: 0.3001, weighted_loss: 0.5154, label: 1, bag_size: 1969\n",
      "batch 399, loss: 1.1611, instance_loss: 1.3761, weighted_loss: 1.2256, label: 0, bag_size: 546\n",
      "batch 419, loss: 0.2961, instance_loss: 1.3164, weighted_loss: 0.6021, label: 0, bag_size: 2336\n",
      "batch 439, loss: 0.5627, instance_loss: 0.2258, weighted_loss: 0.4616, label: 1, bag_size: 5731\n",
      "batch 459, loss: 0.3296, instance_loss: 0.7366, weighted_loss: 0.4517, label: 1, bag_size: 2193\n",
      "batch 479, loss: 0.4048, instance_loss: 0.2357, weighted_loss: 0.3540, label: 0, bag_size: 1592\n",
      "batch 499, loss: 0.3520, instance_loss: 0.5381, weighted_loss: 0.4078, label: 0, bag_size: 3657\n",
      "batch 519, loss: 0.6262, instance_loss: 0.3409, weighted_loss: 0.5406, label: 0, bag_size: 4997\n",
      "batch 539, loss: 0.1075, instance_loss: 0.0457, weighted_loss: 0.0890, label: 0, bag_size: 9470\n",
      "batch 559, loss: 0.1833, instance_loss: 0.0383, weighted_loss: 0.1398, label: 0, bag_size: 8252\n",
      "batch 579, loss: 0.3711, instance_loss: 0.5504, weighted_loss: 0.4249, label: 1, bag_size: 928\n",
      "batch 599, loss: 0.4407, instance_loss: 0.1082, weighted_loss: 0.3409, label: 0, bag_size: 2282\n",
      "batch 619, loss: 0.1057, instance_loss: 0.0403, weighted_loss: 0.0861, label: 0, bag_size: 18132\n",
      "batch 639, loss: 0.2028, instance_loss: 0.3867, weighted_loss: 0.2580, label: 0, bag_size: 3190\n",
      "batch 659, loss: 0.3469, instance_loss: 0.5298, weighted_loss: 0.4018, label: 1, bag_size: 1888\n",
      "batch 679, loss: 0.3860, instance_loss: 0.2994, weighted_loss: 0.3600, label: 1, bag_size: 12127\n",
      "batch 699, loss: 0.1795, instance_loss: 0.1243, weighted_loss: 0.1630, label: 0, bag_size: 9060\n",
      "batch 719, loss: 0.3433, instance_loss: 0.2697, weighted_loss: 0.3212, label: 1, bag_size: 6682\n",
      "batch 739, loss: 0.1942, instance_loss: 0.1350, weighted_loss: 0.1765, label: 0, bag_size: 11865\n",
      "batch 759, loss: 1.0703, instance_loss: 2.5934, weighted_loss: 1.5272, label: 1, bag_size: 20870\n",
      "batch 779, loss: 0.8429, instance_loss: 0.5553, weighted_loss: 0.7567, label: 0, bag_size: 3552\n",
      "batch 799, loss: 0.1620, instance_loss: 0.0426, weighted_loss: 0.1262, label: 0, bag_size: 21682\n",
      "batch 819, loss: 1.3590, instance_loss: 1.5963, weighted_loss: 1.4302, label: 0, bag_size: 2815\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9480421686746988: correct 12590/13280\n",
      "class 1 clustering acc 0.5978915662650602: correct 3970/6640\n",
      "Epoch: 9, train_loss: 0.4922, train_clustering_loss:  0.6008, train_error: 0.2060\n",
      "class 0: acc 0.8205741626794258, correct 343/418\n",
      "class 1: acc 0.7669902912621359, correct 316/412\n",
      "\n",
      "Val Set, val_loss: 0.5438, val_error: 0.3093, auc: 0.8374\n",
      "class 0 clustering acc 0.9813144329896907: correct 1523/1552\n",
      "class 1 clustering acc 0.29510309278350516: correct 229/776\n",
      "class 0: acc 0.5319148936170213, correct 25/47\n",
      "class 1: acc 0.84, correct 42/50\n",
      "Validation loss decreased (0.564513 --> 0.543788).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3452, instance_loss: 0.5212, weighted_loss: 0.3980, label: 0, bag_size: 13332\n",
      "batch 39, loss: 0.3386, instance_loss: 0.2035, weighted_loss: 0.2981, label: 0, bag_size: 1588\n",
      "batch 59, loss: 0.6953, instance_loss: 0.2692, weighted_loss: 0.5675, label: 0, bag_size: 17083\n",
      "batch 79, loss: 0.2062, instance_loss: 0.2570, weighted_loss: 0.2214, label: 0, bag_size: 13880\n",
      "batch 99, loss: 0.1252, instance_loss: 0.0215, weighted_loss: 0.0941, label: 1, bag_size: 10558\n",
      "batch 119, loss: 0.2706, instance_loss: 0.0862, weighted_loss: 0.2153, label: 1, bag_size: 8026\n",
      "batch 139, loss: 0.1040, instance_loss: 0.0525, weighted_loss: 0.0885, label: 0, bag_size: 15747\n",
      "batch 159, loss: 0.3000, instance_loss: 0.2542, weighted_loss: 0.2862, label: 1, bag_size: 5292\n",
      "batch 179, loss: 0.2677, instance_loss: 0.0092, weighted_loss: 0.1901, label: 1, bag_size: 5023\n",
      "batch 199, loss: 0.2516, instance_loss: 0.0275, weighted_loss: 0.1844, label: 0, bag_size: 21032\n",
      "batch 219, loss: 0.0853, instance_loss: 0.0541, weighted_loss: 0.0759, label: 0, bag_size: 7309\n",
      "batch 239, loss: 0.2313, instance_loss: 0.0303, weighted_loss: 0.1710, label: 0, bag_size: 21682\n",
      "batch 259, loss: 0.1636, instance_loss: 0.5586, weighted_loss: 0.2821, label: 1, bag_size: 9004\n",
      "batch 279, loss: 0.5867, instance_loss: 0.0256, weighted_loss: 0.4183, label: 1, bag_size: 12758\n",
      "batch 299, loss: 0.0439, instance_loss: 0.0150, weighted_loss: 0.0352, label: 1, bag_size: 3634\n",
      "batch 319, loss: 0.4199, instance_loss: 0.8410, weighted_loss: 0.5462, label: 1, bag_size: 12626\n",
      "batch 339, loss: 0.1180, instance_loss: 0.0010, weighted_loss: 0.0829, label: 1, bag_size: 3082\n",
      "batch 359, loss: 1.2899, instance_loss: 0.6508, weighted_loss: 1.0981, label: 0, bag_size: 2815\n",
      "batch 379, loss: 0.0871, instance_loss: 0.1228, weighted_loss: 0.0978, label: 0, bag_size: 14435\n",
      "batch 399, loss: 0.6516, instance_loss: 0.6383, weighted_loss: 0.6476, label: 0, bag_size: 3198\n",
      "batch 419, loss: 2.8878, instance_loss: 1.3821, weighted_loss: 2.4361, label: 0, bag_size: 3897\n",
      "batch 439, loss: 0.6620, instance_loss: 0.5026, weighted_loss: 0.6142, label: 1, bag_size: 16548\n",
      "batch 459, loss: 0.4670, instance_loss: 0.2281, weighted_loss: 0.3953, label: 1, bag_size: 13692\n",
      "batch 479, loss: 0.3558, instance_loss: 0.2188, weighted_loss: 0.3147, label: 0, bag_size: 11759\n",
      "batch 499, loss: 0.1224, instance_loss: 0.3668, weighted_loss: 0.1957, label: 1, bag_size: 8216\n",
      "batch 519, loss: 0.4653, instance_loss: 1.1213, weighted_loss: 0.6621, label: 0, bag_size: 10063\n",
      "batch 539, loss: 0.2583, instance_loss: 0.0431, weighted_loss: 0.1938, label: 0, bag_size: 31085\n",
      "batch 559, loss: 1.4648, instance_loss: 1.3200, weighted_loss: 1.4214, label: 0, bag_size: 3810\n",
      "batch 579, loss: 0.3537, instance_loss: 0.3634, weighted_loss: 0.3566, label: 1, bag_size: 16162\n",
      "batch 599, loss: 0.1360, instance_loss: 0.0378, weighted_loss: 0.1065, label: 1, bag_size: 3968\n",
      "batch 619, loss: 0.0974, instance_loss: 0.0394, weighted_loss: 0.0800, label: 1, bag_size: 8438\n",
      "batch 639, loss: 1.1441, instance_loss: 1.7590, weighted_loss: 1.3286, label: 1, bag_size: 1831\n",
      "batch 659, loss: 0.9290, instance_loss: 0.4357, weighted_loss: 0.7811, label: 0, bag_size: 2351\n",
      "batch 679, loss: 0.2668, instance_loss: 0.0738, weighted_loss: 0.2089, label: 0, bag_size: 27012\n",
      "batch 699, loss: 0.1439, instance_loss: 0.0212, weighted_loss: 0.1071, label: 1, bag_size: 10591\n",
      "batch 719, loss: 0.3489, instance_loss: 0.0699, weighted_loss: 0.2652, label: 1, bag_size: 14681\n",
      "batch 739, loss: 0.5507, instance_loss: 0.6483, weighted_loss: 0.5799, label: 1, bag_size: 2759\n",
      "batch 759, loss: 0.0935, instance_loss: 0.0175, weighted_loss: 0.0707, label: 1, bag_size: 6453\n",
      "batch 779, loss: 0.4223, instance_loss: 0.1139, weighted_loss: 0.3298, label: 0, bag_size: 3893\n",
      "batch 799, loss: 0.0882, instance_loss: 0.1080, weighted_loss: 0.0942, label: 1, bag_size: 5864\n",
      "batch 819, loss: 0.5150, instance_loss: 0.0988, weighted_loss: 0.3901, label: 1, bag_size: 14155\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9603162650602409: correct 12753/13280\n",
      "class 1 clustering acc 0.7427710843373494: correct 4932/6640\n",
      "Epoch: 10, train_loss: 0.4040, train_clustering_loss:  0.4384, train_error: 0.1542\n",
      "class 0: acc 0.8325, correct 333/400\n",
      "class 1: acc 0.858139534883721, correct 369/430\n",
      "\n",
      "Val Set, val_loss: 0.5201, val_error: 0.2371, auc: 0.8430\n",
      "class 0 clustering acc 0.9271907216494846: correct 1439/1552\n",
      "class 1 clustering acc 0.6842783505154639: correct 531/776\n",
      "class 0: acc 0.6808510638297872, correct 32/47\n",
      "class 1: acc 0.84, correct 42/50\n",
      "Validation loss decreased (0.543788 --> 0.520056).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0637, instance_loss: 0.2219, weighted_loss: 0.1112, label: 1, bag_size: 3640\n",
      "batch 39, loss: 0.1429, instance_loss: 0.0151, weighted_loss: 0.1045, label: 1, bag_size: 6533\n",
      "batch 59, loss: 0.0740, instance_loss: 0.0938, weighted_loss: 0.0799, label: 1, bag_size: 12095\n",
      "batch 79, loss: 0.2934, instance_loss: 0.7524, weighted_loss: 0.4311, label: 1, bag_size: 29832\n",
      "batch 99, loss: 0.0823, instance_loss: 0.2799, weighted_loss: 0.1416, label: 1, bag_size: 3409\n",
      "batch 119, loss: 0.2074, instance_loss: 0.1079, weighted_loss: 0.1775, label: 0, bag_size: 3774\n",
      "batch 139, loss: 0.5353, instance_loss: 0.0507, weighted_loss: 0.3899, label: 1, bag_size: 10460\n",
      "batch 159, loss: 1.4802, instance_loss: 2.5344, weighted_loss: 1.7965, label: 1, bag_size: 12494\n",
      "batch 179, loss: 0.0895, instance_loss: 0.0012, weighted_loss: 0.0630, label: 1, bag_size: 4367\n",
      "batch 199, loss: 0.1242, instance_loss: 0.0224, weighted_loss: 0.0937, label: 0, bag_size: 11194\n",
      "batch 219, loss: 0.3297, instance_loss: 0.3255, weighted_loss: 0.3284, label: 0, bag_size: 2296\n",
      "batch 239, loss: 0.0898, instance_loss: 0.0787, weighted_loss: 0.0865, label: 0, bag_size: 21682\n",
      "batch 259, loss: 2.2255, instance_loss: 3.6419, weighted_loss: 2.6504, label: 0, bag_size: 17279\n",
      "batch 279, loss: 0.9136, instance_loss: 0.0858, weighted_loss: 0.6652, label: 1, bag_size: 17579\n",
      "batch 299, loss: 0.3981, instance_loss: 0.5715, weighted_loss: 0.4501, label: 0, bag_size: 803\n",
      "batch 319, loss: 0.1249, instance_loss: 0.0327, weighted_loss: 0.0972, label: 0, bag_size: 2282\n",
      "batch 339, loss: 0.1370, instance_loss: 0.0079, weighted_loss: 0.0983, label: 1, bag_size: 1888\n",
      "batch 359, loss: 0.2374, instance_loss: 0.0106, weighted_loss: 0.1694, label: 0, bag_size: 15914\n",
      "batch 379, loss: 0.1449, instance_loss: 0.0298, weighted_loss: 0.1104, label: 0, bag_size: 12524\n",
      "batch 399, loss: 0.0386, instance_loss: 0.0397, weighted_loss: 0.0389, label: 0, bag_size: 23398\n",
      "batch 419, loss: 0.1439, instance_loss: 0.0418, weighted_loss: 0.1133, label: 1, bag_size: 15093\n",
      "batch 439, loss: 0.0804, instance_loss: 0.0005, weighted_loss: 0.0564, label: 1, bag_size: 13732\n",
      "batch 459, loss: 1.5369, instance_loss: 1.3188, weighted_loss: 1.4714, label: 0, bag_size: 2996\n",
      "batch 479, loss: 0.2233, instance_loss: 0.2289, weighted_loss: 0.2250, label: 1, bag_size: 6875\n",
      "batch 499, loss: 0.0357, instance_loss: 0.0005, weighted_loss: 0.0251, label: 1, bag_size: 6769\n",
      "batch 519, loss: 0.0997, instance_loss: 0.0000, weighted_loss: 0.0698, label: 1, bag_size: 9644\n",
      "batch 539, loss: 0.1892, instance_loss: 0.3406, weighted_loss: 0.2346, label: 1, bag_size: 699\n",
      "batch 559, loss: 0.4423, instance_loss: 0.3606, weighted_loss: 0.4178, label: 0, bag_size: 20478\n",
      "batch 579, loss: 0.5747, instance_loss: 1.2732, weighted_loss: 0.7842, label: 0, bag_size: 1690\n",
      "batch 599, loss: 0.4275, instance_loss: 0.3458, weighted_loss: 0.4030, label: 1, bag_size: 3856\n",
      "batch 619, loss: 0.0577, instance_loss: 0.0167, weighted_loss: 0.0454, label: 0, bag_size: 22426\n",
      "batch 639, loss: 0.6278, instance_loss: 2.7734, weighted_loss: 1.2715, label: 0, bag_size: 9421\n",
      "batch 659, loss: 0.2900, instance_loss: 0.2077, weighted_loss: 0.2653, label: 0, bag_size: 2148\n",
      "batch 679, loss: 0.0612, instance_loss: 0.0857, weighted_loss: 0.0686, label: 0, bag_size: 4902\n",
      "batch 699, loss: 4.9175, instance_loss: 3.7123, weighted_loss: 4.5559, label: 0, bag_size: 3897\n",
      "batch 719, loss: 0.4549, instance_loss: 0.3837, weighted_loss: 0.4335, label: 1, bag_size: 3368\n",
      "batch 739, loss: 0.0800, instance_loss: 0.0221, weighted_loss: 0.0626, label: 0, bag_size: 1962\n",
      "batch 759, loss: 0.1180, instance_loss: 0.1226, weighted_loss: 0.1194, label: 0, bag_size: 11735\n",
      "batch 779, loss: 0.0829, instance_loss: 0.1416, weighted_loss: 0.1005, label: 0, bag_size: 11690\n",
      "batch 799, loss: 0.5580, instance_loss: 1.1240, weighted_loss: 0.7278, label: 0, bag_size: 3541\n",
      "batch 819, loss: 0.2400, instance_loss: 0.4109, weighted_loss: 0.2912, label: 0, bag_size: 2518\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9618975903614457: correct 12774/13280\n",
      "class 1 clustering acc 0.8048192771084337: correct 5344/6640\n",
      "Epoch: 11, train_loss: 0.3863, train_clustering_loss:  0.3616, train_error: 0.1554\n",
      "class 0: acc 0.8599033816425121, correct 356/414\n",
      "class 1: acc 0.8293269230769231, correct 345/416\n",
      "\n",
      "Val Set, val_loss: 0.5181, val_error: 0.2680, auc: 0.8460\n",
      "class 0 clustering acc 0.9072164948453608: correct 1408/1552\n",
      "class 1 clustering acc 0.654639175257732: correct 508/776\n",
      "class 0: acc 0.6170212765957447, correct 29/47\n",
      "class 1: acc 0.84, correct 42/50\n",
      "Validation loss decreased (0.520056 --> 0.518074).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 1.4848, instance_loss: 5.8670, weighted_loss: 2.7994, label: 0, bag_size: 16690\n",
      "batch 39, loss: 0.1267, instance_loss: 0.0787, weighted_loss: 0.1123, label: 1, bag_size: 5690\n",
      "batch 59, loss: 0.4184, instance_loss: 0.0747, weighted_loss: 0.3153, label: 1, bag_size: 8602\n",
      "batch 79, loss: 0.2574, instance_loss: 0.2838, weighted_loss: 0.2653, label: 0, bag_size: 763\n",
      "batch 99, loss: 0.6175, instance_loss: 0.4126, weighted_loss: 0.5560, label: 0, bag_size: 1142\n",
      "batch 119, loss: 0.2770, instance_loss: 0.1644, weighted_loss: 0.2432, label: 1, bag_size: 4956\n",
      "batch 139, loss: 0.0523, instance_loss: 0.0611, weighted_loss: 0.0549, label: 1, bag_size: 9519\n",
      "batch 159, loss: 0.0476, instance_loss: 0.0024, weighted_loss: 0.0340, label: 1, bag_size: 6966\n",
      "batch 179, loss: 0.1500, instance_loss: 0.1329, weighted_loss: 0.1449, label: 0, bag_size: 1760\n",
      "batch 199, loss: 0.0806, instance_loss: 0.0180, weighted_loss: 0.0618, label: 1, bag_size: 9644\n",
      "batch 219, loss: 0.2098, instance_loss: 1.4596, weighted_loss: 0.5847, label: 1, bag_size: 9404\n",
      "batch 239, loss: 0.4432, instance_loss: 0.0360, weighted_loss: 0.3210, label: 1, bag_size: 10848\n",
      "batch 259, loss: 0.0604, instance_loss: 0.0142, weighted_loss: 0.0466, label: 1, bag_size: 14433\n",
      "batch 279, loss: 0.0706, instance_loss: 0.0000, weighted_loss: 0.0494, label: 1, bag_size: 11032\n",
      "batch 299, loss: 0.2490, instance_loss: 0.0345, weighted_loss: 0.1847, label: 1, bag_size: 8680\n",
      "batch 319, loss: 0.0661, instance_loss: 0.0559, weighted_loss: 0.0630, label: 0, bag_size: 13225\n",
      "batch 339, loss: 0.0348, instance_loss: 0.7071, weighted_loss: 0.2365, label: 0, bag_size: 10995\n",
      "batch 359, loss: 0.0565, instance_loss: 0.3597, weighted_loss: 0.1474, label: 0, bag_size: 10365\n",
      "batch 379, loss: 0.5524, instance_loss: 1.4347, weighted_loss: 0.8171, label: 0, bag_size: 10410\n",
      "batch 399, loss: 0.1348, instance_loss: 0.0022, weighted_loss: 0.0951, label: 0, bag_size: 25027\n",
      "batch 419, loss: 0.9197, instance_loss: 0.8438, weighted_loss: 0.8969, label: 0, bag_size: 2160\n",
      "batch 439, loss: 0.0540, instance_loss: 0.7927, weighted_loss: 0.2756, label: 1, bag_size: 9747\n",
      "batch 459, loss: 0.7362, instance_loss: 0.5023, weighted_loss: 0.6660, label: 1, bag_size: 1095\n",
      "batch 479, loss: 0.3194, instance_loss: 3.0606, weighted_loss: 1.1417, label: 0, bag_size: 13332\n",
      "batch 499, loss: 0.0750, instance_loss: 0.0727, weighted_loss: 0.0743, label: 0, bag_size: 6851\n",
      "batch 519, loss: 0.9046, instance_loss: 0.1380, weighted_loss: 0.6746, label: 1, bag_size: 15931\n",
      "batch 539, loss: 0.7438, instance_loss: 0.2060, weighted_loss: 0.5824, label: 0, bag_size: 7637\n",
      "batch 559, loss: 0.7046, instance_loss: 0.2371, weighted_loss: 0.5644, label: 1, bag_size: 2137\n",
      "batch 579, loss: 0.1739, instance_loss: 0.3385, weighted_loss: 0.2233, label: 1, bag_size: 9004\n",
      "batch 599, loss: 1.8982, instance_loss: 2.1117, weighted_loss: 1.9622, label: 0, bag_size: 7239\n",
      "batch 619, loss: 0.7822, instance_loss: 0.5093, weighted_loss: 0.7004, label: 0, bag_size: 2968\n",
      "batch 639, loss: 0.3936, instance_loss: 0.1707, weighted_loss: 0.3268, label: 1, bag_size: 5690\n",
      "batch 659, loss: 2.3148, instance_loss: 0.1542, weighted_loss: 1.6666, label: 1, bag_size: 9942\n",
      "batch 679, loss: 0.5881, instance_loss: 0.1683, weighted_loss: 0.4621, label: 0, bag_size: 3654\n",
      "batch 699, loss: 1.4095, instance_loss: 0.1268, weighted_loss: 1.0247, label: 1, bag_size: 15844\n",
      "batch 719, loss: 1.3607, instance_loss: 0.7110, weighted_loss: 1.1658, label: 1, bag_size: 2842\n",
      "batch 739, loss: 0.0366, instance_loss: 0.0048, weighted_loss: 0.0270, label: 0, bag_size: 14305\n",
      "batch 759, loss: 0.2672, instance_loss: 0.1820, weighted_loss: 0.2416, label: 1, bag_size: 12340\n",
      "batch 779, loss: 0.1500, instance_loss: 0.1058, weighted_loss: 0.1367, label: 0, bag_size: 9471\n",
      "batch 799, loss: 0.3079, instance_loss: 0.6374, weighted_loss: 0.4068, label: 0, bag_size: 2322\n",
      "batch 819, loss: 0.0469, instance_loss: 1.0664, weighted_loss: 0.3527, label: 0, bag_size: 10721\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9573795180722892: correct 12714/13280\n",
      "class 1 clustering acc 0.7620481927710844: correct 5060/6640\n",
      "Epoch: 12, train_loss: 0.3826, train_clustering_loss:  0.4277, train_error: 0.1530\n",
      "class 0: acc 0.836272040302267, correct 332/397\n",
      "class 1: acc 0.8568129330254042, correct 371/433\n",
      "\n",
      "Val Set, val_loss: 0.5236, val_error: 0.1856, auc: 0.8506\n",
      "class 0 clustering acc 0.9458762886597938: correct 1468/1552\n",
      "class 1 clustering acc 0.6739690721649485: correct 523/776\n",
      "class 0: acc 0.8936170212765957, correct 42/47\n",
      "class 1: acc 0.74, correct 37/50\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1568, instance_loss: 0.0795, weighted_loss: 0.1336, label: 0, bag_size: 13602\n",
      "batch 39, loss: 0.0929, instance_loss: 0.0480, weighted_loss: 0.0794, label: 1, bag_size: 2848\n",
      "batch 59, loss: 0.5140, instance_loss: 0.3369, weighted_loss: 0.4609, label: 1, bag_size: 1746\n",
      "batch 79, loss: 0.0882, instance_loss: 0.0216, weighted_loss: 0.0682, label: 0, bag_size: 16992\n",
      "batch 99, loss: 0.5993, instance_loss: 0.0116, weighted_loss: 0.4230, label: 0, bag_size: 3670\n",
      "batch 119, loss: 0.0498, instance_loss: 0.1010, weighted_loss: 0.0652, label: 0, bag_size: 9542\n",
      "batch 139, loss: 1.2056, instance_loss: 1.2006, weighted_loss: 1.2041, label: 0, bag_size: 1732\n",
      "batch 159, loss: 0.0925, instance_loss: 0.1290, weighted_loss: 0.1034, label: 1, bag_size: 18161\n",
      "batch 179, loss: 0.8754, instance_loss: 0.4483, weighted_loss: 0.7473, label: 0, bag_size: 2918\n",
      "batch 199, loss: 0.2019, instance_loss: 0.7627, weighted_loss: 0.3701, label: 1, bag_size: 6825\n",
      "batch 219, loss: 0.0820, instance_loss: 0.9862, weighted_loss: 0.3533, label: 0, bag_size: 18944\n",
      "batch 239, loss: 0.1899, instance_loss: 0.0132, weighted_loss: 0.1369, label: 1, bag_size: 4250\n",
      "batch 259, loss: 0.1944, instance_loss: 0.0379, weighted_loss: 0.1475, label: 0, bag_size: 1909\n",
      "batch 279, loss: 0.1065, instance_loss: 0.0420, weighted_loss: 0.0872, label: 1, bag_size: 1249\n",
      "batch 299, loss: 0.3722, instance_loss: 0.1149, weighted_loss: 0.2950, label: 0, bag_size: 3502\n",
      "batch 319, loss: 0.3144, instance_loss: 0.2644, weighted_loss: 0.2994, label: 1, bag_size: 2695\n",
      "batch 339, loss: 1.2683, instance_loss: 0.6535, weighted_loss: 1.0838, label: 1, bag_size: 3211\n",
      "batch 359, loss: 0.1583, instance_loss: 0.1953, weighted_loss: 0.1694, label: 0, bag_size: 13378\n",
      "batch 379, loss: 0.0845, instance_loss: 0.1347, weighted_loss: 0.0995, label: 0, bag_size: 10444\n",
      "batch 399, loss: 0.1948, instance_loss: 0.0768, weighted_loss: 0.1594, label: 1, bag_size: 11394\n",
      "batch 419, loss: 0.0401, instance_loss: 0.0228, weighted_loss: 0.0349, label: 0, bag_size: 11527\n",
      "batch 439, loss: 0.1278, instance_loss: 0.0026, weighted_loss: 0.0902, label: 1, bag_size: 28527\n",
      "batch 459, loss: 0.1800, instance_loss: 0.8022, weighted_loss: 0.3667, label: 1, bag_size: 9404\n",
      "batch 479, loss: 0.0428, instance_loss: 0.1530, weighted_loss: 0.0758, label: 1, bag_size: 8660\n",
      "batch 499, loss: 0.5929, instance_loss: 0.8349, weighted_loss: 0.6655, label: 1, bag_size: 1339\n",
      "batch 519, loss: 0.1416, instance_loss: 0.0000, weighted_loss: 0.0991, label: 1, bag_size: 18699\n",
      "batch 539, loss: 0.1443, instance_loss: 0.0331, weighted_loss: 0.1110, label: 0, bag_size: 16211\n",
      "batch 559, loss: 1.2701, instance_loss: 1.5393, weighted_loss: 1.3509, label: 0, bag_size: 11306\n",
      "batch 579, loss: 0.5240, instance_loss: 0.2866, weighted_loss: 0.4528, label: 0, bag_size: 9597\n",
      "batch 599, loss: 0.0830, instance_loss: 2.4831, weighted_loss: 0.8030, label: 0, bag_size: 21874\n",
      "batch 619, loss: 0.0327, instance_loss: 0.1204, weighted_loss: 0.0590, label: 0, bag_size: 11546\n",
      "batch 639, loss: 0.1172, instance_loss: 0.1132, weighted_loss: 0.1160, label: 0, bag_size: 1560\n",
      "batch 659, loss: 0.0224, instance_loss: 0.0520, weighted_loss: 0.0312, label: 0, bag_size: 16607\n",
      "batch 679, loss: 0.5502, instance_loss: 0.0469, weighted_loss: 0.3992, label: 0, bag_size: 518\n",
      "batch 699, loss: 0.0320, instance_loss: 0.1053, weighted_loss: 0.0540, label: 1, bag_size: 6927\n",
      "batch 719, loss: 0.1256, instance_loss: 0.0006, weighted_loss: 0.0881, label: 1, bag_size: 11421\n",
      "batch 739, loss: 0.2509, instance_loss: 0.2214, weighted_loss: 0.2420, label: 1, bag_size: 771\n",
      "batch 759, loss: 1.9520, instance_loss: 0.1055, weighted_loss: 1.3980, label: 0, bag_size: 1800\n",
      "batch 779, loss: 1.2586, instance_loss: 0.3555, weighted_loss: 0.9877, label: 1, bag_size: 2579\n",
      "batch 799, loss: 0.0689, instance_loss: 0.0085, weighted_loss: 0.0508, label: 0, bag_size: 17268\n",
      "batch 819, loss: 0.0165, instance_loss: 0.0063, weighted_loss: 0.0135, label: 1, bag_size: 9322\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9666415662650603: correct 12837/13280\n",
      "class 1 clustering acc 0.8168674698795181: correct 5424/6640\n",
      "Epoch: 13, train_loss: 0.3328, train_clustering_loss:  0.3428, train_error: 0.1337\n",
      "class 0: acc 0.8849765258215962, correct 377/426\n",
      "class 1: acc 0.8465346534653465, correct 342/404\n",
      "\n",
      "Val Set, val_loss: 0.5048, val_error: 0.2268, auc: 0.8515\n",
      "class 0 clustering acc 0.9265463917525774: correct 1438/1552\n",
      "class 1 clustering acc 0.6984536082474226: correct 542/776\n",
      "class 0: acc 0.723404255319149, correct 34/47\n",
      "class 1: acc 0.82, correct 41/50\n",
      "Validation loss decreased (0.518074 --> 0.504840).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1905, instance_loss: 0.2690, weighted_loss: 0.2140, label: 0, bag_size: 1814\n",
      "batch 39, loss: 0.3569, instance_loss: 0.0453, weighted_loss: 0.2634, label: 1, bag_size: 12178\n",
      "batch 59, loss: 0.0620, instance_loss: 0.0342, weighted_loss: 0.0536, label: 0, bag_size: 15967\n",
      "batch 79, loss: 0.1110, instance_loss: 1.9111, weighted_loss: 0.6510, label: 0, bag_size: 8866\n",
      "batch 99, loss: 0.1137, instance_loss: 0.5479, weighted_loss: 0.2440, label: 1, bag_size: 2146\n",
      "batch 119, loss: 0.3794, instance_loss: 0.1009, weighted_loss: 0.2959, label: 0, bag_size: 24439\n",
      "batch 139, loss: 0.2028, instance_loss: 0.1490, weighted_loss: 0.1867, label: 0, bag_size: 3232\n",
      "batch 159, loss: 1.1197, instance_loss: 0.6201, weighted_loss: 0.9698, label: 0, bag_size: 1437\n",
      "batch 179, loss: 0.0184, instance_loss: 0.0694, weighted_loss: 0.0337, label: 0, bag_size: 11690\n",
      "batch 199, loss: 0.0385, instance_loss: 0.1218, weighted_loss: 0.0635, label: 1, bag_size: 8216\n",
      "batch 219, loss: 0.0537, instance_loss: 0.1226, weighted_loss: 0.0743, label: 0, bag_size: 27012\n",
      "batch 239, loss: 0.1094, instance_loss: 0.0006, weighted_loss: 0.0768, label: 1, bag_size: 9065\n",
      "batch 259, loss: 0.0956, instance_loss: 0.3110, weighted_loss: 0.1602, label: 0, bag_size: 1452\n",
      "batch 279, loss: 0.0263, instance_loss: 0.0177, weighted_loss: 0.0237, label: 1, bag_size: 5441\n",
      "batch 299, loss: 0.5416, instance_loss: 0.5086, weighted_loss: 0.5317, label: 0, bag_size: 17155\n",
      "batch 319, loss: 0.1599, instance_loss: 0.1229, weighted_loss: 0.1488, label: 0, bag_size: 7917\n",
      "batch 339, loss: 0.1985, instance_loss: 0.0002, weighted_loss: 0.1390, label: 1, bag_size: 7389\n",
      "batch 359, loss: 0.5618, instance_loss: 0.7177, weighted_loss: 0.6086, label: 0, bag_size: 1772\n",
      "batch 379, loss: 0.0613, instance_loss: 0.3286, weighted_loss: 0.1415, label: 1, bag_size: 6825\n",
      "batch 399, loss: 0.0042, instance_loss: 0.0000, weighted_loss: 0.0029, label: 1, bag_size: 4877\n",
      "batch 419, loss: 0.0705, instance_loss: 0.0318, weighted_loss: 0.0589, label: 0, bag_size: 13023\n",
      "batch 439, loss: 0.0354, instance_loss: 0.0010, weighted_loss: 0.0251, label: 1, bag_size: 8466\n",
      "batch 459, loss: 0.0172, instance_loss: 0.0347, weighted_loss: 0.0224, label: 0, bag_size: 15626\n",
      "batch 479, loss: 0.0501, instance_loss: 0.0009, weighted_loss: 0.0353, label: 0, bag_size: 21576\n",
      "batch 499, loss: 0.0589, instance_loss: 0.0116, weighted_loss: 0.0448, label: 1, bag_size: 4442\n",
      "batch 519, loss: 0.0806, instance_loss: 0.0196, weighted_loss: 0.0623, label: 0, bag_size: 9471\n",
      "batch 539, loss: 0.9205, instance_loss: 0.2228, weighted_loss: 0.7112, label: 1, bag_size: 7981\n",
      "batch 559, loss: 0.1099, instance_loss: 0.0873, weighted_loss: 0.1031, label: 0, bag_size: 15967\n",
      "batch 579, loss: 0.0037, instance_loss: 0.0036, weighted_loss: 0.0036, label: 0, bag_size: 7191\n",
      "batch 599, loss: 0.0278, instance_loss: 0.0245, weighted_loss: 0.0268, label: 0, bag_size: 4497\n",
      "batch 619, loss: 0.4042, instance_loss: 0.9169, weighted_loss: 0.5580, label: 0, bag_size: 6898\n",
      "batch 639, loss: 0.0560, instance_loss: 0.2559, weighted_loss: 0.1160, label: 1, bag_size: 8438\n",
      "batch 659, loss: 0.1462, instance_loss: 0.0276, weighted_loss: 0.1106, label: 0, bag_size: 13880\n",
      "batch 679, loss: 0.0734, instance_loss: 0.0000, weighted_loss: 0.0514, label: 1, bag_size: 10969\n",
      "batch 699, loss: 0.3958, instance_loss: 0.0345, weighted_loss: 0.2874, label: 0, bag_size: 12131\n",
      "batch 719, loss: 0.0382, instance_loss: 0.0242, weighted_loss: 0.0340, label: 1, bag_size: 4367\n",
      "batch 739, loss: 0.3822, instance_loss: 0.8639, weighted_loss: 0.5267, label: 0, bag_size: 4845\n",
      "batch 759, loss: 0.1272, instance_loss: 0.0680, weighted_loss: 0.1094, label: 0, bag_size: 1909\n",
      "batch 779, loss: 0.1302, instance_loss: 0.0000, weighted_loss: 0.0912, label: 1, bag_size: 12408\n",
      "batch 799, loss: 0.0554, instance_loss: 0.2427, weighted_loss: 0.1116, label: 1, bag_size: 9747\n",
      "batch 819, loss: 0.0370, instance_loss: 0.3032, weighted_loss: 0.1169, label: 1, bag_size: 2405\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9654367469879518: correct 12821/13280\n",
      "class 1 clustering acc 0.8275602409638554: correct 5495/6640\n",
      "Epoch: 14, train_loss: 0.3223, train_clustering_loss:  0.3347, train_error: 0.1373\n",
      "class 0: acc 0.8661800486618005, correct 356/411\n",
      "class 1: acc 0.8591885441527446, correct 360/419\n",
      "\n",
      "Val Set, val_loss: 0.5855, val_error: 0.2062, auc: 0.8506\n",
      "class 0 clustering acc 0.9226804123711341: correct 1432/1552\n",
      "class 1 clustering acc 0.7229381443298969: correct 561/776\n",
      "class 0: acc 0.9787234042553191, correct 46/47\n",
      "class 1: acc 0.62, correct 31/50\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 4.7629, instance_loss: 2.9213, weighted_loss: 4.2104, label: 1, bag_size: 898\n",
      "batch 39, loss: 0.0291, instance_loss: 0.0044, weighted_loss: 0.0217, label: 1, bag_size: 4367\n",
      "batch 59, loss: 0.0164, instance_loss: 0.0294, weighted_loss: 0.0203, label: 0, bag_size: 11654\n",
      "batch 79, loss: 1.6637, instance_loss: 0.5985, weighted_loss: 1.3442, label: 1, bag_size: 2480\n",
      "batch 99, loss: 0.0586, instance_loss: 0.0001, weighted_loss: 0.0411, label: 1, bag_size: 10028\n",
      "batch 119, loss: 0.2615, instance_loss: 0.2447, weighted_loss: 0.2565, label: 0, bag_size: 11212\n",
      "batch 139, loss: 0.2704, instance_loss: 0.1567, weighted_loss: 0.2363, label: 0, bag_size: 1416\n",
      "batch 159, loss: 0.0073, instance_loss: 0.0001, weighted_loss: 0.0051, label: 1, bag_size: 5612\n",
      "batch 179, loss: 0.4308, instance_loss: 0.1579, weighted_loss: 0.3489, label: 1, bag_size: 3980\n",
      "batch 199, loss: 0.1002, instance_loss: 0.0226, weighted_loss: 0.0769, label: 1, bag_size: 1165\n",
      "batch 219, loss: 0.0116, instance_loss: 0.0106, weighted_loss: 0.0113, label: 0, bag_size: 18574\n",
      "batch 239, loss: 0.0435, instance_loss: 0.1323, weighted_loss: 0.0702, label: 1, bag_size: 19972\n",
      "batch 259, loss: 0.1008, instance_loss: 0.0626, weighted_loss: 0.0894, label: 0, bag_size: 1452\n",
      "batch 279, loss: 0.0726, instance_loss: 0.0000, weighted_loss: 0.0508, label: 1, bag_size: 15213\n",
      "batch 299, loss: 0.0958, instance_loss: 0.5597, weighted_loss: 0.2350, label: 1, bag_size: 1294\n",
      "batch 319, loss: 0.0142, instance_loss: 0.0047, weighted_loss: 0.0114, label: 1, bag_size: 11389\n",
      "batch 339, loss: 0.1885, instance_loss: 0.0401, weighted_loss: 0.1440, label: 0, bag_size: 2322\n",
      "batch 359, loss: 0.0286, instance_loss: 0.0837, weighted_loss: 0.0451, label: 0, bag_size: 23796\n",
      "batch 379, loss: 0.0127, instance_loss: 0.0611, weighted_loss: 0.0272, label: 0, bag_size: 13691\n",
      "batch 399, loss: 0.0084, instance_loss: 0.0374, weighted_loss: 0.0171, label: 0, bag_size: 8948\n",
      "batch 419, loss: 0.5059, instance_loss: 0.8728, weighted_loss: 0.6160, label: 0, bag_size: 1953\n",
      "batch 439, loss: 0.0570, instance_loss: 0.0108, weighted_loss: 0.0431, label: 1, bag_size: 3652\n",
      "batch 459, loss: 0.0077, instance_loss: 0.3802, weighted_loss: 0.1194, label: 0, bag_size: 21218\n",
      "batch 479, loss: 0.1084, instance_loss: 0.0005, weighted_loss: 0.0760, label: 1, bag_size: 14202\n",
      "batch 499, loss: 0.2175, instance_loss: 0.0004, weighted_loss: 0.1523, label: 1, bag_size: 16034\n",
      "batch 519, loss: 0.0565, instance_loss: 0.0000, weighted_loss: 0.0395, label: 1, bag_size: 9408\n",
      "batch 539, loss: 0.0548, instance_loss: 0.0383, weighted_loss: 0.0498, label: 0, bag_size: 2748\n",
      "batch 559, loss: 0.0312, instance_loss: 0.0225, weighted_loss: 0.0286, label: 1, bag_size: 4239\n",
      "batch 579, loss: 0.1627, instance_loss: 0.2140, weighted_loss: 0.1781, label: 1, bag_size: 12714\n",
      "batch 599, loss: 0.1441, instance_loss: 0.2678, weighted_loss: 0.1812, label: 0, bag_size: 2609\n",
      "batch 619, loss: 0.1731, instance_loss: 0.0009, weighted_loss: 0.1214, label: 0, bag_size: 18516\n",
      "batch 639, loss: 0.1347, instance_loss: 0.3622, weighted_loss: 0.2030, label: 0, bag_size: 12899\n",
      "batch 659, loss: 0.0277, instance_loss: 0.1660, weighted_loss: 0.0692, label: 0, bag_size: 15636\n",
      "batch 679, loss: 0.6631, instance_loss: 0.2271, weighted_loss: 0.5323, label: 0, bag_size: 25420\n",
      "batch 699, loss: 0.0667, instance_loss: 0.0173, weighted_loss: 0.0519, label: 1, bag_size: 1412\n",
      "batch 719, loss: 0.0429, instance_loss: 0.0245, weighted_loss: 0.0374, label: 0, bag_size: 13218\n",
      "batch 739, loss: 0.1214, instance_loss: 0.0754, weighted_loss: 0.1076, label: 0, bag_size: 4497\n",
      "batch 759, loss: 0.0574, instance_loss: 0.0034, weighted_loss: 0.0412, label: 1, bag_size: 1786\n",
      "batch 779, loss: 0.0358, instance_loss: 0.2872, weighted_loss: 0.1112, label: 0, bag_size: 3101\n",
      "batch 799, loss: 0.0053, instance_loss: 0.3056, weighted_loss: 0.0954, label: 0, bag_size: 16936\n",
      "batch 819, loss: 1.9172, instance_loss: 2.0898, weighted_loss: 1.9690, label: 0, bag_size: 2458\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9625753012048193: correct 12783/13280\n",
      "class 1 clustering acc 0.801355421686747: correct 5321/6640\n",
      "Epoch: 15, train_loss: 0.3498, train_clustering_loss:  0.3650, train_error: 0.1398\n",
      "class 0: acc 0.8791469194312796, correct 371/422\n",
      "class 1: acc 0.8406862745098039, correct 343/408\n",
      "\n",
      "Val Set, val_loss: 0.5980, val_error: 0.3402, auc: 0.8506\n",
      "class 0 clustering acc 0.9800257731958762: correct 1521/1552\n",
      "class 1 clustering acc 0.586340206185567: correct 455/776\n",
      "class 0: acc 0.46808510638297873, correct 22/47\n",
      "class 1: acc 0.84, correct 42/50\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0197, instance_loss: 0.0452, weighted_loss: 0.0273, label: 0, bag_size: 30828\n",
      "batch 39, loss: 0.3891, instance_loss: 0.5620, weighted_loss: 0.4410, label: 1, bag_size: 2579\n",
      "batch 59, loss: 0.0598, instance_loss: 0.0254, weighted_loss: 0.0495, label: 1, bag_size: 19606\n",
      "batch 79, loss: 0.0416, instance_loss: 1.2598, weighted_loss: 0.4071, label: 1, bag_size: 1919\n",
      "batch 99, loss: 0.0162, instance_loss: 0.2684, weighted_loss: 0.0919, label: 1, bag_size: 1919\n",
      "batch 119, loss: 0.0578, instance_loss: 0.0642, weighted_loss: 0.0597, label: 0, bag_size: 10942\n",
      "batch 139, loss: 0.0297, instance_loss: 0.0411, weighted_loss: 0.0331, label: 0, bag_size: 19435\n",
      "batch 159, loss: 4.2531, instance_loss: 3.1248, weighted_loss: 3.9146, label: 0, bag_size: 3802\n",
      "batch 179, loss: 0.9504, instance_loss: 0.1183, weighted_loss: 0.7007, label: 0, bag_size: 2996\n",
      "batch 199, loss: 0.8439, instance_loss: 0.4750, weighted_loss: 0.7332, label: 0, bag_size: 11128\n",
      "batch 219, loss: 0.0607, instance_loss: 0.0566, weighted_loss: 0.0594, label: 0, bag_size: 9542\n",
      "batch 239, loss: 0.0521, instance_loss: 0.6474, weighted_loss: 0.2307, label: 1, bag_size: 2193\n",
      "batch 259, loss: 1.1446, instance_loss: 2.0362, weighted_loss: 1.4121, label: 1, bag_size: 1867\n",
      "batch 279, loss: 0.0015, instance_loss: 0.0549, weighted_loss: 0.0175, label: 0, bag_size: 1984\n",
      "batch 299, loss: 0.0385, instance_loss: 0.0707, weighted_loss: 0.0482, label: 0, bag_size: 8145\n",
      "batch 319, loss: 0.0052, instance_loss: 0.0000, weighted_loss: 0.0036, label: 1, bag_size: 4880\n",
      "batch 339, loss: 0.7577, instance_loss: 1.3580, weighted_loss: 0.9378, label: 0, bag_size: 2458\n",
      "batch 359, loss: 0.0670, instance_loss: 0.0006, weighted_loss: 0.0471, label: 1, bag_size: 3004\n",
      "batch 379, loss: 0.3983, instance_loss: 2.9332, weighted_loss: 1.1588, label: 0, bag_size: 7917\n",
      "batch 399, loss: 1.2238, instance_loss: 0.5748, weighted_loss: 1.0291, label: 0, bag_size: 1701\n",
      "batch 419, loss: 0.5480, instance_loss: 0.8752, weighted_loss: 0.6462, label: 1, bag_size: 7669\n",
      "batch 439, loss: 0.0038, instance_loss: 0.1012, weighted_loss: 0.0330, label: 0, bag_size: 7709\n",
      "batch 459, loss: 0.9322, instance_loss: 0.9620, weighted_loss: 0.9411, label: 1, bag_size: 10460\n",
      "batch 479, loss: 0.4034, instance_loss: 0.7266, weighted_loss: 0.5003, label: 0, bag_size: 1142\n",
      "batch 499, loss: 0.4552, instance_loss: 0.1898, weighted_loss: 0.3756, label: 0, bag_size: 1962\n",
      "batch 519, loss: 0.5804, instance_loss: 0.1067, weighted_loss: 0.4383, label: 0, bag_size: 20555\n",
      "batch 539, loss: 0.9271, instance_loss: 1.2230, weighted_loss: 1.0159, label: 0, bag_size: 10063\n",
      "batch 559, loss: 0.6470, instance_loss: 0.5723, weighted_loss: 0.6246, label: 0, bag_size: 2996\n",
      "batch 579, loss: 0.0161, instance_loss: 0.0357, weighted_loss: 0.0220, label: 0, bag_size: 13691\n",
      "batch 599, loss: 0.0834, instance_loss: 0.0015, weighted_loss: 0.0588, label: 1, bag_size: 5690\n",
      "batch 619, loss: 0.0044, instance_loss: 0.0182, weighted_loss: 0.0085, label: 1, bag_size: 4880\n",
      "batch 639, loss: 0.0906, instance_loss: 0.0763, weighted_loss: 0.0863, label: 0, bag_size: 12732\n",
      "batch 659, loss: 0.0988, instance_loss: 0.0460, weighted_loss: 0.0830, label: 1, bag_size: 8935\n",
      "batch 679, loss: 1.2211, instance_loss: 0.9509, weighted_loss: 1.1400, label: 1, bag_size: 1831\n",
      "batch 699, loss: 0.0340, instance_loss: 0.0861, weighted_loss: 0.0496, label: 0, bag_size: 9252\n",
      "batch 719, loss: 1.2406, instance_loss: 1.9835, weighted_loss: 1.4635, label: 1, bag_size: 983\n",
      "batch 739, loss: 0.0976, instance_loss: 0.0019, weighted_loss: 0.0689, label: 1, bag_size: 17486\n",
      "batch 759, loss: 0.0082, instance_loss: 0.0026, weighted_loss: 0.0066, label: 1, bag_size: 11981\n",
      "batch 779, loss: 0.1200, instance_loss: 0.0425, weighted_loss: 0.0968, label: 0, bag_size: 15967\n",
      "batch 799, loss: 0.0067, instance_loss: 0.0002, weighted_loss: 0.0047, label: 0, bag_size: 23037\n",
      "batch 819, loss: 0.0269, instance_loss: 0.0009, weighted_loss: 0.0191, label: 1, bag_size: 9877\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9651355421686747: correct 12817/13280\n",
      "class 1 clustering acc 0.8096385542168675: correct 5376/6640\n",
      "Epoch: 16, train_loss: 0.3263, train_clustering_loss:  0.3377, train_error: 0.1410\n",
      "class 0: acc 0.8591549295774648, correct 366/426\n",
      "class 1: acc 0.8589108910891089, correct 347/404\n",
      "\n",
      "Val Set, val_loss: 0.5047, val_error: 0.1959, auc: 0.8523\n",
      "class 0 clustering acc 0.946520618556701: correct 1469/1552\n",
      "class 1 clustering acc 0.7126288659793815: correct 553/776\n",
      "class 0: acc 0.8085106382978723, correct 38/47\n",
      "class 1: acc 0.8, correct 40/50\n",
      "Validation loss decreased (0.504840 --> 0.504693).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0241, instance_loss: 0.0530, weighted_loss: 0.0328, label: 0, bag_size: 10898\n",
      "batch 39, loss: 0.9151, instance_loss: 0.2325, weighted_loss: 0.7103, label: 1, bag_size: 7989\n",
      "batch 59, loss: 0.3277, instance_loss: 0.6624, weighted_loss: 0.4281, label: 1, bag_size: 2681\n",
      "batch 79, loss: 0.1677, instance_loss: 0.0000, weighted_loss: 0.1174, label: 1, bag_size: 29832\n",
      "batch 99, loss: 0.0054, instance_loss: 0.0159, weighted_loss: 0.0086, label: 0, bag_size: 11690\n",
      "batch 119, loss: 1.7708, instance_loss: 0.3276, weighted_loss: 1.3379, label: 1, bag_size: 13440\n",
      "batch 139, loss: 0.1898, instance_loss: 0.2508, weighted_loss: 0.2081, label: 0, bag_size: 13602\n",
      "batch 159, loss: 0.0717, instance_loss: 0.3024, weighted_loss: 0.1409, label: 0, bag_size: 13943\n",
      "batch 179, loss: 0.0507, instance_loss: 0.5594, weighted_loss: 0.2033, label: 1, bag_size: 928\n",
      "batch 199, loss: 1.5140, instance_loss: 0.6272, weighted_loss: 1.2480, label: 0, bag_size: 2996\n",
      "batch 219, loss: 0.1477, instance_loss: 0.1719, weighted_loss: 0.1550, label: 0, bag_size: 7637\n",
      "batch 239, loss: 0.1282, instance_loss: 0.0120, weighted_loss: 0.0933, label: 0, bag_size: 16087\n",
      "batch 259, loss: 0.1604, instance_loss: 0.0622, weighted_loss: 0.1309, label: 0, bag_size: 12510\n",
      "batch 279, loss: 0.5611, instance_loss: 0.3435, weighted_loss: 0.4958, label: 1, bag_size: 12340\n",
      "batch 299, loss: 0.1686, instance_loss: 0.5780, weighted_loss: 0.2914, label: 0, bag_size: 9252\n",
      "batch 319, loss: 0.0444, instance_loss: 0.0003, weighted_loss: 0.0311, label: 1, bag_size: 20333\n",
      "batch 339, loss: 0.4608, instance_loss: 0.4625, weighted_loss: 0.4613, label: 1, bag_size: 11729\n",
      "batch 359, loss: 0.0013, instance_loss: 0.0076, weighted_loss: 0.0032, label: 0, bag_size: 15736\n",
      "batch 379, loss: 0.2255, instance_loss: 0.3993, weighted_loss: 0.2776, label: 0, bag_size: 9421\n",
      "batch 399, loss: 0.0237, instance_loss: 0.0473, weighted_loss: 0.0308, label: 0, bag_size: 6851\n",
      "batch 419, loss: 0.0076, instance_loss: 0.0008, weighted_loss: 0.0055, label: 0, bag_size: 12137\n",
      "batch 439, loss: 0.0141, instance_loss: 0.0007, weighted_loss: 0.0101, label: 0, bag_size: 18076\n",
      "batch 459, loss: 0.0059, instance_loss: 0.0017, weighted_loss: 0.0046, label: 0, bag_size: 13218\n",
      "batch 479, loss: 0.0456, instance_loss: 0.0484, weighted_loss: 0.0464, label: 0, bag_size: 3232\n",
      "batch 499, loss: 0.4192, instance_loss: 0.1286, weighted_loss: 0.3320, label: 0, bag_size: 7141\n",
      "batch 519, loss: 0.1052, instance_loss: 0.1654, weighted_loss: 0.1233, label: 1, bag_size: 11394\n",
      "batch 539, loss: 0.0535, instance_loss: 0.0240, weighted_loss: 0.0446, label: 0, bag_size: 1824\n",
      "batch 559, loss: 0.0546, instance_loss: 0.0000, weighted_loss: 0.0383, label: 0, bag_size: 16720\n",
      "batch 579, loss: 0.3908, instance_loss: 0.3108, weighted_loss: 0.3668, label: 0, bag_size: 65728\n",
      "batch 599, loss: 0.3259, instance_loss: 0.0001, weighted_loss: 0.2281, label: 1, bag_size: 8680\n",
      "batch 619, loss: 0.2167, instance_loss: 0.2301, weighted_loss: 0.2207, label: 0, bag_size: 11607\n",
      "batch 639, loss: 0.0926, instance_loss: 0.0005, weighted_loss: 0.0650, label: 1, bag_size: 34356\n",
      "batch 659, loss: 0.0320, instance_loss: 0.0003, weighted_loss: 0.0225, label: 0, bag_size: 9485\n",
      "batch 679, loss: 0.0274, instance_loss: 0.0025, weighted_loss: 0.0199, label: 0, bag_size: 19880\n",
      "batch 699, loss: 0.0085, instance_loss: 0.0044, weighted_loss: 0.0073, label: 0, bag_size: 21076\n",
      "batch 719, loss: 0.0112, instance_loss: 0.0995, weighted_loss: 0.0377, label: 0, bag_size: 12503\n",
      "batch 739, loss: 0.0045, instance_loss: 0.1768, weighted_loss: 0.0562, label: 0, bag_size: 1984\n",
      "batch 759, loss: 0.5031, instance_loss: 0.0238, weighted_loss: 0.3593, label: 0, bag_size: 16087\n",
      "batch 779, loss: 0.3818, instance_loss: 0.1543, weighted_loss: 0.3136, label: 1, bag_size: 11316\n",
      "batch 799, loss: 0.0154, instance_loss: 0.1330, weighted_loss: 0.0507, label: 0, bag_size: 10068\n",
      "batch 819, loss: 0.0623, instance_loss: 0.0000, weighted_loss: 0.0436, label: 1, bag_size: 12931\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9678463855421687: correct 12853/13280\n",
      "class 1 clustering acc 0.8403614457831325: correct 5580/6640\n",
      "Epoch: 17, train_loss: 0.3142, train_clustering_loss:  0.3052, train_error: 0.1277\n",
      "class 0: acc 0.882640586797066, correct 361/409\n",
      "class 1: acc 0.8622327790973872, correct 363/421\n",
      "\n",
      "Val Set, val_loss: 0.5081, val_error: 0.1959, auc: 0.8519\n",
      "class 0 clustering acc 0.946520618556701: correct 1469/1552\n",
      "class 1 clustering acc 0.729381443298969: correct 566/776\n",
      "class 0: acc 0.7872340425531915, correct 37/47\n",
      "class 1: acc 0.82, correct 41/50\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0408, instance_loss: 0.0001, weighted_loss: 0.0286, label: 1, bag_size: 19500\n",
      "batch 39, loss: 0.2801, instance_loss: 0.3305, weighted_loss: 0.2952, label: 0, bag_size: 9132\n",
      "batch 59, loss: 0.0357, instance_loss: 0.0614, weighted_loss: 0.0434, label: 0, bag_size: 11759\n",
      "batch 79, loss: 0.0637, instance_loss: 0.0000, weighted_loss: 0.0446, label: 1, bag_size: 3683\n",
      "batch 99, loss: 0.2435, instance_loss: 0.0619, weighted_loss: 0.1890, label: 0, bag_size: 19043\n",
      "batch 119, loss: 0.0571, instance_loss: 0.3973, weighted_loss: 0.1591, label: 1, bag_size: 928\n",
      "batch 139, loss: 0.0541, instance_loss: 0.0291, weighted_loss: 0.0466, label: 0, bag_size: 8025\n",
      "batch 159, loss: 0.0591, instance_loss: 0.0871, weighted_loss: 0.0675, label: 0, bag_size: 803\n",
      "batch 179, loss: 0.0177, instance_loss: 0.0022, weighted_loss: 0.0130, label: 0, bag_size: 21032\n",
      "batch 199, loss: 0.1107, instance_loss: 0.0448, weighted_loss: 0.0909, label: 1, bag_size: 6205\n",
      "batch 219, loss: 0.1181, instance_loss: 0.0401, weighted_loss: 0.0947, label: 0, bag_size: 8898\n",
      "batch 239, loss: 0.7466, instance_loss: 4.1321, weighted_loss: 1.7623, label: 1, bag_size: 1875\n",
      "batch 259, loss: 0.9293, instance_loss: 0.5898, weighted_loss: 0.8274, label: 0, bag_size: 546\n",
      "batch 279, loss: 0.2883, instance_loss: 0.0231, weighted_loss: 0.2087, label: 1, bag_size: 3980\n",
      "batch 299, loss: 0.0303, instance_loss: 0.0337, weighted_loss: 0.0313, label: 0, bag_size: 13880\n",
      "batch 319, loss: 0.0568, instance_loss: 0.0524, weighted_loss: 0.0554, label: 0, bag_size: 2748\n",
      "batch 339, loss: 0.7966, instance_loss: 0.3303, weighted_loss: 0.6567, label: 1, bag_size: 12719\n",
      "batch 359, loss: 0.0117, instance_loss: 0.0002, weighted_loss: 0.0083, label: 0, bag_size: 11735\n",
      "batch 379, loss: 0.0269, instance_loss: 0.0965, weighted_loss: 0.0478, label: 1, bag_size: 8003\n",
      "batch 399, loss: 0.2645, instance_loss: 0.5614, weighted_loss: 0.3536, label: 0, bag_size: 2367\n",
      "batch 419, loss: 0.2359, instance_loss: 0.0254, weighted_loss: 0.1727, label: 0, bag_size: 7031\n",
      "batch 439, loss: 0.0201, instance_loss: 0.0012, weighted_loss: 0.0144, label: 1, bag_size: 11387\n",
      "batch 459, loss: 0.0491, instance_loss: 0.0118, weighted_loss: 0.0379, label: 0, bag_size: 32227\n",
      "batch 479, loss: 0.2542, instance_loss: 0.1891, weighted_loss: 0.2346, label: 1, bag_size: 3674\n",
      "batch 499, loss: 0.5006, instance_loss: 0.4468, weighted_loss: 0.4844, label: 0, bag_size: 1789\n",
      "batch 519, loss: 0.0176, instance_loss: 0.0004, weighted_loss: 0.0125, label: 1, bag_size: 12095\n",
      "batch 539, loss: 0.0096, instance_loss: 0.0007, weighted_loss: 0.0069, label: 0, bag_size: 18574\n",
      "batch 559, loss: 0.0391, instance_loss: 0.0000, weighted_loss: 0.0274, label: 1, bag_size: 6343\n",
      "batch 579, loss: 0.0923, instance_loss: 0.0313, weighted_loss: 0.0740, label: 0, bag_size: 9171\n",
      "batch 599, loss: 0.0153, instance_loss: 0.2749, weighted_loss: 0.0932, label: 0, bag_size: 11778\n",
      "batch 619, loss: 0.0156, instance_loss: 0.0099, weighted_loss: 0.0139, label: 0, bag_size: 14333\n",
      "batch 639, loss: 0.1430, instance_loss: 0.3567, weighted_loss: 0.2071, label: 0, bag_size: 4418\n",
      "batch 659, loss: 0.9279, instance_loss: 1.0216, weighted_loss: 0.9560, label: 0, bag_size: 7835\n",
      "batch 679, loss: 0.1397, instance_loss: 0.5244, weighted_loss: 0.2551, label: 1, bag_size: 865\n",
      "batch 699, loss: 0.0374, instance_loss: 0.1316, weighted_loss: 0.0657, label: 0, bag_size: 2534\n",
      "batch 719, loss: 1.8707, instance_loss: 0.4744, weighted_loss: 1.4518, label: 1, bag_size: 16514\n",
      "batch 739, loss: 0.0142, instance_loss: 0.0078, weighted_loss: 0.0123, label: 1, bag_size: 9571\n",
      "batch 759, loss: 0.0298, instance_loss: 0.2408, weighted_loss: 0.0931, label: 0, bag_size: 15001\n",
      "batch 779, loss: 0.0022, instance_loss: 0.0552, weighted_loss: 0.0181, label: 0, bag_size: 3787\n",
      "batch 799, loss: 0.0141, instance_loss: 0.1399, weighted_loss: 0.0518, label: 0, bag_size: 11527\n",
      "batch 819, loss: 0.0198, instance_loss: 0.0005, weighted_loss: 0.0140, label: 1, bag_size: 11875\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9700301204819277: correct 12882/13280\n",
      "class 1 clustering acc 0.848644578313253: correct 5635/6640\n",
      "Epoch: 18, train_loss: 0.2845, train_clustering_loss:  0.2955, train_error: 0.1096\n",
      "class 0: acc 0.8875, correct 355/400\n",
      "class 1: acc 0.8930232558139535, correct 384/430\n",
      "\n",
      "Val Set, val_loss: 0.5141, val_error: 0.1959, auc: 0.8502\n",
      "class 0 clustering acc 0.9452319587628866: correct 1467/1552\n",
      "class 1 clustering acc 0.7448453608247423: correct 578/776\n",
      "class 0: acc 0.7872340425531915, correct 37/47\n",
      "class 1: acc 0.82, correct 41/50\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0092, instance_loss: 0.0132, weighted_loss: 0.0104, label: 0, bag_size: 14206\n",
      "batch 39, loss: 0.1336, instance_loss: 0.0220, weighted_loss: 0.1001, label: 1, bag_size: 3683\n",
      "batch 59, loss: 2.2229, instance_loss: 0.5172, weighted_loss: 1.7112, label: 1, bag_size: 1444\n",
      "batch 79, loss: 0.0079, instance_loss: 0.0009, weighted_loss: 0.0058, label: 1, bag_size: 9689\n",
      "batch 99, loss: 1.4603, instance_loss: 0.1130, weighted_loss: 1.0561, label: 0, bag_size: 3760\n",
      "batch 119, loss: 0.0034, instance_loss: 0.0529, weighted_loss: 0.0182, label: 0, bag_size: 19466\n",
      "batch 139, loss: 0.0104, instance_loss: 0.0083, weighted_loss: 0.0098, label: 1, bag_size: 6769\n",
      "batch 159, loss: 0.7851, instance_loss: 0.0667, weighted_loss: 0.5696, label: 1, bag_size: 2395\n",
      "batch 179, loss: 0.5416, instance_loss: 0.0016, weighted_loss: 0.3796, label: 1, bag_size: 13089\n",
      "batch 199, loss: 0.0210, instance_loss: 0.0222, weighted_loss: 0.0214, label: 1, bag_size: 549\n",
      "batch 219, loss: 0.0318, instance_loss: 0.1334, weighted_loss: 0.0623, label: 1, bag_size: 2662\n",
      "batch 239, loss: 0.0462, instance_loss: 0.1456, weighted_loss: 0.0760, label: 1, bag_size: 1014\n",
      "batch 259, loss: 0.0827, instance_loss: 0.0585, weighted_loss: 0.0754, label: 0, bag_size: 2322\n",
      "batch 279, loss: 0.0193, instance_loss: 0.0075, weighted_loss: 0.0158, label: 0, bag_size: 10898\n",
      "batch 299, loss: 0.2657, instance_loss: 0.0221, weighted_loss: 0.1926, label: 0, bag_size: 18215\n",
      "batch 319, loss: 0.0133, instance_loss: 0.0204, weighted_loss: 0.0155, label: 1, bag_size: 1244\n",
      "batch 339, loss: 0.0366, instance_loss: 0.0540, weighted_loss: 0.0418, label: 0, bag_size: 13691\n",
      "batch 359, loss: 0.2129, instance_loss: 0.0532, weighted_loss: 0.1650, label: 0, bag_size: 7835\n",
      "batch 379, loss: 1.4381, instance_loss: 0.4370, weighted_loss: 1.1378, label: 0, bag_size: 1506\n",
      "batch 399, loss: 0.0648, instance_loss: 0.0672, weighted_loss: 0.0655, label: 1, bag_size: 4789\n",
      "batch 419, loss: 0.0065, instance_loss: 0.0022, weighted_loss: 0.0052, label: 0, bag_size: 10995\n",
      "batch 439, loss: 0.3129, instance_loss: 0.0046, weighted_loss: 0.2204, label: 1, bag_size: 10498\n",
      "batch 459, loss: 1.4527, instance_loss: 0.0269, weighted_loss: 1.0250, label: 0, bag_size: 23618\n",
      "batch 479, loss: 0.1710, instance_loss: 0.0045, weighted_loss: 0.1211, label: 1, bag_size: 2308\n",
      "batch 499, loss: 0.1594, instance_loss: 0.3936, weighted_loss: 0.2297, label: 1, bag_size: 18095\n",
      "batch 519, loss: 0.0901, instance_loss: 0.0172, weighted_loss: 0.0682, label: 1, bag_size: 10501\n",
      "batch 539, loss: 0.0488, instance_loss: 0.0017, weighted_loss: 0.0347, label: 1, bag_size: 16034\n",
      "batch 559, loss: 0.5906, instance_loss: 0.1366, weighted_loss: 0.4544, label: 0, bag_size: 2006\n",
      "batch 579, loss: 0.0539, instance_loss: 0.1039, weighted_loss: 0.0689, label: 0, bag_size: 15214\n",
      "batch 599, loss: 0.1627, instance_loss: 0.0920, weighted_loss: 0.1415, label: 0, bag_size: 2382\n",
      "batch 619, loss: 0.0101, instance_loss: 0.0000, weighted_loss: 0.0071, label: 1, bag_size: 14515\n",
      "batch 639, loss: 0.0645, instance_loss: 0.8179, weighted_loss: 0.2905, label: 1, bag_size: 4054\n",
      "batch 659, loss: 0.1063, instance_loss: 0.3669, weighted_loss: 0.1845, label: 0, bag_size: 890\n",
      "batch 679, loss: 0.2805, instance_loss: 0.0690, weighted_loss: 0.2171, label: 0, bag_size: 11607\n",
      "batch 699, loss: 0.2087, instance_loss: 0.1308, weighted_loss: 0.1853, label: 0, bag_size: 5161\n",
      "batch 719, loss: 0.1143, instance_loss: 0.0824, weighted_loss: 0.1047, label: 0, bag_size: 1149\n",
      "batch 739, loss: 0.1263, instance_loss: 0.0015, weighted_loss: 0.0888, label: 1, bag_size: 10394\n",
      "batch 759, loss: 0.1516, instance_loss: 0.0981, weighted_loss: 0.1356, label: 1, bag_size: 12178\n",
      "batch 779, loss: 1.8174, instance_loss: 1.4188, weighted_loss: 1.6978, label: 1, bag_size: 1794\n",
      "batch 799, loss: 0.0272, instance_loss: 0.0002, weighted_loss: 0.0191, label: 0, bag_size: 9485\n",
      "batch 819, loss: 0.9931, instance_loss: 1.5374, weighted_loss: 1.1564, label: 1, bag_size: 1236\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9713102409638554: correct 12899/13280\n",
      "class 1 clustering acc 0.8570783132530121: correct 5691/6640\n",
      "Epoch: 19, train_loss: 0.3130, train_clustering_loss:  0.2736, train_error: 0.1205\n",
      "class 0: acc 0.8793969849246231, correct 350/398\n",
      "class 1: acc 0.8796296296296297, correct 380/432\n",
      "\n",
      "Val Set, val_loss: 0.5048, val_error: 0.2062, auc: 0.8549\n",
      "class 0 clustering acc 0.9426546391752577: correct 1463/1552\n",
      "class 1 clustering acc 0.7654639175257731: correct 594/776\n",
      "class 0: acc 0.7659574468085106, correct 36/47\n",
      "class 1: acc 0.82, correct 41/50\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.8550, instance_loss: 0.7389, weighted_loss: 0.8202, label: 0, bag_size: 2814\n",
      "batch 39, loss: 0.0486, instance_loss: 0.0000, weighted_loss: 0.0340, label: 0, bag_size: 11113\n",
      "batch 59, loss: 0.0229, instance_loss: 0.0070, weighted_loss: 0.0181, label: 0, bag_size: 13225\n",
      "batch 79, loss: 0.0535, instance_loss: 0.0089, weighted_loss: 0.0401, label: 1, bag_size: 3450\n",
      "batch 99, loss: 0.0016, instance_loss: 0.0015, weighted_loss: 0.0016, label: 0, bag_size: 23037\n",
      "batch 119, loss: 0.2193, instance_loss: 0.0018, weighted_loss: 0.1541, label: 1, bag_size: 2682\n",
      "batch 139, loss: 0.2791, instance_loss: 0.2200, weighted_loss: 0.2614, label: 1, bag_size: 11316\n",
      "batch 159, loss: 1.8892, instance_loss: 0.0912, weighted_loss: 1.3498, label: 1, bag_size: 13477\n",
      "batch 179, loss: 0.1766, instance_loss: 0.0589, weighted_loss: 0.1413, label: 1, bag_size: 2904\n",
      "batch 199, loss: 0.0119, instance_loss: 0.0023, weighted_loss: 0.0091, label: 1, bag_size: 8466\n",
      "batch 219, loss: 0.1907, instance_loss: 0.3153, weighted_loss: 0.2280, label: 0, bag_size: 19808\n",
      "batch 239, loss: 0.0064, instance_loss: 0.0201, weighted_loss: 0.0105, label: 0, bag_size: 18240\n",
      "batch 259, loss: 0.0831, instance_loss: 0.0021, weighted_loss: 0.0588, label: 1, bag_size: 9478\n",
      "batch 279, loss: 0.0028, instance_loss: 0.0000, weighted_loss: 0.0020, label: 0, bag_size: 19659\n",
      "batch 299, loss: 0.2117, instance_loss: 0.0971, weighted_loss: 0.1773, label: 0, bag_size: 1962\n",
      "batch 319, loss: 0.0200, instance_loss: 0.0833, weighted_loss: 0.0390, label: 0, bag_size: 3970\n",
      "batch 339, loss: 1.1241, instance_loss: 1.3587, weighted_loss: 1.1945, label: 1, bag_size: 2937\n",
      "batch 359, loss: 0.0499, instance_loss: 0.0457, weighted_loss: 0.0486, label: 0, bag_size: 3657\n",
      "batch 379, loss: 0.0370, instance_loss: 0.0001, weighted_loss: 0.0260, label: 1, bag_size: 12460\n",
      "batch 399, loss: 0.0724, instance_loss: 0.0018, weighted_loss: 0.0512, label: 0, bag_size: 14435\n",
      "batch 419, loss: 6.0397, instance_loss: 3.5395, weighted_loss: 5.2896, label: 0, bag_size: 3802\n",
      "batch 439, loss: 0.1358, instance_loss: 0.0756, weighted_loss: 0.1177, label: 0, bag_size: 2382\n",
      "batch 459, loss: 0.0325, instance_loss: 0.1631, weighted_loss: 0.0717, label: 0, bag_size: 11759\n",
      "batch 479, loss: 0.0407, instance_loss: 0.0772, weighted_loss: 0.0517, label: 0, bag_size: 7605\n",
      "batch 499, loss: 0.0366, instance_loss: 0.0000, weighted_loss: 0.0257, label: 0, bag_size: 19880\n",
      "batch 519, loss: 0.0194, instance_loss: 0.0134, weighted_loss: 0.0176, label: 0, bag_size: 30828\n",
      "batch 539, loss: 0.3730, instance_loss: 0.0178, weighted_loss: 0.2664, label: 0, bag_size: 7031\n",
      "batch 559, loss: 0.1023, instance_loss: 0.0378, weighted_loss: 0.0829, label: 0, bag_size: 4959\n",
      "batch 579, loss: 0.0251, instance_loss: 0.0185, weighted_loss: 0.0231, label: 0, bag_size: 10263\n",
      "batch 599, loss: 1.2245, instance_loss: 0.0078, weighted_loss: 0.8595, label: 1, bag_size: 9942\n",
      "batch 619, loss: 0.0380, instance_loss: 0.0072, weighted_loss: 0.0288, label: 0, bag_size: 5225\n",
      "batch 639, loss: 0.4988, instance_loss: 0.3283, weighted_loss: 0.4477, label: 1, bag_size: 3674\n",
      "batch 659, loss: 0.0278, instance_loss: 0.0000, weighted_loss: 0.0195, label: 0, bag_size: 25558\n",
      "batch 679, loss: 0.0983, instance_loss: 0.0368, weighted_loss: 0.0799, label: 1, bag_size: 10396\n",
      "batch 699, loss: 0.3746, instance_loss: 0.4932, weighted_loss: 0.4102, label: 0, bag_size: 20478\n",
      "batch 719, loss: 0.1551, instance_loss: 0.0835, weighted_loss: 0.1336, label: 0, bag_size: 2004\n",
      "batch 739, loss: 0.0578, instance_loss: 0.1187, weighted_loss: 0.0760, label: 0, bag_size: 1909\n",
      "batch 759, loss: 0.0135, instance_loss: 0.0196, weighted_loss: 0.0154, label: 0, bag_size: 14333\n",
      "batch 779, loss: 0.5983, instance_loss: 0.3998, weighted_loss: 0.5387, label: 1, bag_size: 8475\n",
      "batch 799, loss: 0.0022, instance_loss: 0.0007, weighted_loss: 0.0017, label: 1, bag_size: 3634\n",
      "batch 819, loss: 0.0095, instance_loss: 0.0010, weighted_loss: 0.0070, label: 1, bag_size: 15008\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9748493975903615: correct 12946/13280\n",
      "class 1 clustering acc 0.8771084337349397: correct 5824/6640\n",
      "Epoch: 20, train_loss: 0.2544, train_clustering_loss:  0.2488, train_error: 0.1000\n",
      "class 0: acc 0.8960396039603961, correct 362/404\n",
      "class 1: acc 0.903755868544601, correct 385/426\n",
      "\n",
      "Val Set, val_loss: 0.5033, val_error: 0.1649, auc: 0.8574\n",
      "class 0 clustering acc 0.9252577319587629: correct 1436/1552\n",
      "class 1 clustering acc 0.7551546391752577: correct 586/776\n",
      "class 0: acc 0.851063829787234, correct 40/47\n",
      "class 1: acc 0.82, correct 41/50\n",
      "Validation loss decreased (0.504693 --> 0.503330).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0224, instance_loss: 0.0012, weighted_loss: 0.0160, label: 0, bag_size: 14319\n",
      "batch 39, loss: 0.0638, instance_loss: 0.0380, weighted_loss: 0.0561, label: 1, bag_size: 5690\n",
      "batch 59, loss: 0.0372, instance_loss: 0.2713, weighted_loss: 0.1074, label: 1, bag_size: 1622\n",
      "batch 79, loss: 0.0282, instance_loss: 0.0359, weighted_loss: 0.0305, label: 1, bag_size: 4250\n",
      "batch 99, loss: 0.0240, instance_loss: 0.0325, weighted_loss: 0.0266, label: 1, bag_size: 6171\n",
      "batch 119, loss: 0.0188, instance_loss: 0.0041, weighted_loss: 0.0144, label: 1, bag_size: 14779\n",
      "batch 139, loss: 0.4056, instance_loss: 1.3525, weighted_loss: 0.6897, label: 1, bag_size: 6478\n",
      "batch 159, loss: 0.0081, instance_loss: 0.0386, weighted_loss: 0.0173, label: 0, bag_size: 11778\n",
      "batch 179, loss: 0.3805, instance_loss: 2.6152, weighted_loss: 1.0509, label: 1, bag_size: 8026\n",
      "batch 199, loss: 2.5940, instance_loss: 0.9170, weighted_loss: 2.0909, label: 1, bag_size: 15563\n",
      "batch 219, loss: 0.1063, instance_loss: 0.1705, weighted_loss: 0.1256, label: 1, bag_size: 29832\n",
      "batch 239, loss: 0.4036, instance_loss: 0.2406, weighted_loss: 0.3547, label: 1, bag_size: 21450\n",
      "batch 259, loss: 0.0035, instance_loss: 0.0000, weighted_loss: 0.0024, label: 1, bag_size: 9971\n",
      "batch 279, loss: 0.5646, instance_loss: 0.5614, weighted_loss: 0.5637, label: 0, bag_size: 2070\n",
      "batch 299, loss: 0.0672, instance_loss: 0.0231, weighted_loss: 0.0540, label: 0, bag_size: 17919\n",
      "batch 319, loss: 0.1144, instance_loss: 0.0319, weighted_loss: 0.0896, label: 0, bag_size: 2968\n",
      "batch 339, loss: 0.2028, instance_loss: 0.0518, weighted_loss: 0.1575, label: 0, bag_size: 1920\n",
      "batch 359, loss: 0.0771, instance_loss: 0.0029, weighted_loss: 0.0548, label: 1, bag_size: 4259\n",
      "batch 379, loss: 0.1417, instance_loss: 0.0056, weighted_loss: 0.1008, label: 1, bag_size: 18603\n",
      "batch 399, loss: 0.1632, instance_loss: 0.0746, weighted_loss: 0.1366, label: 1, bag_size: 8012\n",
      "batch 419, loss: 0.0804, instance_loss: 0.1318, weighted_loss: 0.0958, label: 0, bag_size: 4959\n",
      "batch 439, loss: 2.4445, instance_loss: 3.1240, weighted_loss: 2.6483, label: 1, bag_size: 684\n",
      "batch 459, loss: 0.0072, instance_loss: 0.0013, weighted_loss: 0.0054, label: 1, bag_size: 4789\n",
      "batch 479, loss: 0.1061, instance_loss: 0.1603, weighted_loss: 0.1223, label: 0, bag_size: 18954\n",
      "batch 499, loss: 0.5142, instance_loss: 0.6372, weighted_loss: 0.5511, label: 0, bag_size: 4418\n",
      "batch 519, loss: 0.1935, instance_loss: 0.0994, weighted_loss: 0.1653, label: 0, bag_size: 17482\n",
      "batch 539, loss: 0.6527, instance_loss: 0.6220, weighted_loss: 0.6435, label: 1, bag_size: 13367\n",
      "batch 559, loss: 0.9357, instance_loss: 1.3353, weighted_loss: 1.0556, label: 1, bag_size: 1339\n",
      "batch 579, loss: 0.0086, instance_loss: 0.0000, weighted_loss: 0.0060, label: 1, bag_size: 11884\n",
      "batch 599, loss: 0.2230, instance_loss: 0.0350, weighted_loss: 0.1666, label: 1, bag_size: 7389\n",
      "batch 619, loss: 0.0195, instance_loss: 0.0196, weighted_loss: 0.0195, label: 0, bag_size: 9171\n",
      "batch 639, loss: 0.0751, instance_loss: 0.0011, weighted_loss: 0.0529, label: 0, bag_size: 3502\n",
      "batch 659, loss: 0.0316, instance_loss: 0.0218, weighted_loss: 0.0287, label: 0, bag_size: 21093\n",
      "batch 679, loss: 1.3854, instance_loss: 1.8192, weighted_loss: 1.5156, label: 0, bag_size: 11128\n",
      "batch 699, loss: 0.2342, instance_loss: 0.0175, weighted_loss: 0.1692, label: 1, bag_size: 7424\n",
      "batch 719, loss: 0.9136, instance_loss: 0.3775, weighted_loss: 0.7528, label: 0, bag_size: 2959\n",
      "batch 739, loss: 0.1785, instance_loss: 0.0788, weighted_loss: 0.1486, label: 1, bag_size: 8592\n",
      "batch 759, loss: 0.0338, instance_loss: 0.2096, weighted_loss: 0.0866, label: 1, bag_size: 3295\n",
      "batch 779, loss: 0.0610, instance_loss: 0.0020, weighted_loss: 0.0433, label: 1, bag_size: 9408\n",
      "batch 799, loss: 0.0987, instance_loss: 0.0028, weighted_loss: 0.0699, label: 1, bag_size: 1823\n",
      "batch 819, loss: 0.2763, instance_loss: 0.0810, weighted_loss: 0.2177, label: 0, bag_size: 3444\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9696536144578313: correct 12877/13280\n",
      "class 1 clustering acc 0.8400602409638555: correct 5578/6640\n",
      "Epoch: 21, train_loss: 0.2999, train_clustering_loss:  0.2878, train_error: 0.1157\n",
      "class 0: acc 0.8829268292682927, correct 362/410\n",
      "class 1: acc 0.8857142857142857, correct 372/420\n",
      "\n",
      "Val Set, val_loss: 0.4903, val_error: 0.1753, auc: 0.8655\n",
      "class 0 clustering acc 0.9490979381443299: correct 1473/1552\n",
      "class 1 clustering acc 0.7680412371134021: correct 596/776\n",
      "class 0: acc 0.851063829787234, correct 40/47\n",
      "class 1: acc 0.8, correct 40/50\n",
      "Validation loss decreased (0.503330 --> 0.490327).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0871, instance_loss: 0.0131, weighted_loss: 0.0649, label: 1, bag_size: 11160\n",
      "batch 39, loss: 0.0090, instance_loss: 0.0172, weighted_loss: 0.0115, label: 0, bag_size: 12796\n",
      "batch 59, loss: 0.2561, instance_loss: 0.2197, weighted_loss: 0.2452, label: 0, bag_size: 4241\n",
      "batch 79, loss: 1.6645, instance_loss: 3.7534, weighted_loss: 2.2912, label: 0, bag_size: 7239\n",
      "batch 99, loss: 0.0348, instance_loss: 0.4461, weighted_loss: 0.1582, label: 1, bag_size: 1459\n",
      "batch 119, loss: 0.5021, instance_loss: 0.3440, weighted_loss: 0.4547, label: 0, bag_size: 1498\n",
      "batch 139, loss: 0.0246, instance_loss: 0.0436, weighted_loss: 0.0303, label: 0, bag_size: 1415\n",
      "batch 159, loss: 0.0588, instance_loss: 0.0017, weighted_loss: 0.0417, label: 1, bag_size: 9478\n",
      "batch 179, loss: 0.9130, instance_loss: 0.0533, weighted_loss: 0.6551, label: 0, bag_size: 9597\n",
      "batch 199, loss: 0.0020, instance_loss: 0.0000, weighted_loss: 0.0014, label: 1, bag_size: 11981\n",
      "batch 219, loss: 0.0838, instance_loss: 0.0534, weighted_loss: 0.0747, label: 0, bag_size: 21385\n",
      "batch 239, loss: 0.0021, instance_loss: 0.0061, weighted_loss: 0.0033, label: 1, bag_size: 3437\n",
      "batch 259, loss: 0.1358, instance_loss: 0.2249, weighted_loss: 0.1625, label: 0, bag_size: 1142\n",
      "batch 279, loss: 0.0031, instance_loss: 0.0089, weighted_loss: 0.0048, label: 1, bag_size: 5991\n",
      "batch 299, loss: 0.0099, instance_loss: 0.0049, weighted_loss: 0.0084, label: 1, bag_size: 11032\n",
      "batch 319, loss: 0.0349, instance_loss: 1.0211, weighted_loss: 0.3308, label: 1, bag_size: 2405\n",
      "batch 339, loss: 0.0047, instance_loss: 0.0000, weighted_loss: 0.0033, label: 1, bag_size: 6776\n",
      "batch 359, loss: 0.3078, instance_loss: 0.0119, weighted_loss: 0.2190, label: 1, bag_size: 13477\n",
      "batch 379, loss: 1.9434, instance_loss: 2.7182, weighted_loss: 2.1758, label: 1, bag_size: 684\n",
      "batch 399, loss: 0.1414, instance_loss: 0.2371, weighted_loss: 0.1701, label: 1, bag_size: 4786\n",
      "batch 419, loss: 0.3126, instance_loss: 0.1009, weighted_loss: 0.2491, label: 0, bag_size: 16521\n",
      "batch 439, loss: 0.2407, instance_loss: 0.8435, weighted_loss: 0.4215, label: 0, bag_size: 13332\n",
      "batch 459, loss: 0.0222, instance_loss: 0.4386, weighted_loss: 0.1471, label: 1, bag_size: 2904\n",
      "batch 479, loss: 0.0107, instance_loss: 0.0005, weighted_loss: 0.0076, label: 0, bag_size: 13511\n",
      "batch 499, loss: 0.0044, instance_loss: 0.0056, weighted_loss: 0.0047, label: 1, bag_size: 6966\n",
      "batch 519, loss: 0.0057, instance_loss: 0.0056, weighted_loss: 0.0057, label: 0, bag_size: 12137\n",
      "batch 539, loss: 0.0275, instance_loss: 0.0000, weighted_loss: 0.0192, label: 1, bag_size: 4317\n",
      "batch 559, loss: 0.2485, instance_loss: 0.0284, weighted_loss: 0.1824, label: 1, bag_size: 8475\n",
      "batch 579, loss: 0.9012, instance_loss: 0.5698, weighted_loss: 0.8018, label: 0, bag_size: 2213\n",
      "batch 599, loss: 0.4599, instance_loss: 0.3506, weighted_loss: 0.4271, label: 0, bag_size: 4997\n",
      "batch 619, loss: 0.5809, instance_loss: 0.4884, weighted_loss: 0.5532, label: 0, bag_size: 4523\n",
      "batch 639, loss: 0.1161, instance_loss: 0.0017, weighted_loss: 0.0818, label: 0, bag_size: 5965\n",
      "batch 659, loss: 0.0020, instance_loss: 0.1166, weighted_loss: 0.0364, label: 0, bag_size: 16936\n",
      "batch 679, loss: 0.1336, instance_loss: 0.3160, weighted_loss: 0.1883, label: 0, bag_size: 3321\n",
      "batch 699, loss: 0.0397, instance_loss: 0.0355, weighted_loss: 0.0384, label: 0, bag_size: 2004\n",
      "batch 719, loss: 0.5446, instance_loss: 0.4110, weighted_loss: 0.5045, label: 0, bag_size: 1142\n",
      "batch 739, loss: 1.3057, instance_loss: 1.5860, weighted_loss: 1.3898, label: 0, bag_size: 9132\n",
      "batch 759, loss: 0.0115, instance_loss: 0.0330, weighted_loss: 0.0180, label: 0, bag_size: 9851\n",
      "batch 779, loss: 0.0336, instance_loss: 0.0206, weighted_loss: 0.0297, label: 0, bag_size: 12149\n",
      "batch 799, loss: 0.0318, instance_loss: 0.0139, weighted_loss: 0.0264, label: 0, bag_size: 4497\n",
      "batch 819, loss: 0.0374, instance_loss: 0.7183, weighted_loss: 0.2416, label: 1, bag_size: 6875\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9666415662650603: correct 12837/13280\n",
      "class 1 clustering acc 0.8230421686746988: correct 5465/6640\n",
      "Epoch: 22, train_loss: 0.3005, train_clustering_loss:  0.3263, train_error: 0.1145\n",
      "class 0: acc 0.8949880668257757, correct 375/419\n",
      "class 1: acc 0.8759124087591241, correct 360/411\n",
      "\n",
      "Val Set, val_loss: 0.5517, val_error: 0.1649, auc: 0.8715\n",
      "class 0 clustering acc 0.9400773195876289: correct 1459/1552\n",
      "class 1 clustering acc 0.7074742268041238: correct 549/776\n",
      "class 0: acc 1.0, correct 47/47\n",
      "class 1: acc 0.68, correct 34/50\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1320, instance_loss: 0.1240, weighted_loss: 0.1296, label: 0, bag_size: 2043\n",
      "batch 39, loss: 0.8724, instance_loss: 0.1631, weighted_loss: 0.6596, label: 1, bag_size: 21252\n",
      "batch 59, loss: 0.4412, instance_loss: 0.0016, weighted_loss: 0.3093, label: 0, bag_size: 2351\n",
      "batch 79, loss: 0.0049, instance_loss: 0.1971, weighted_loss: 0.0625, label: 1, bag_size: 9759\n",
      "batch 99, loss: 0.5559, instance_loss: 1.3664, weighted_loss: 0.7991, label: 0, bag_size: 2653\n",
      "batch 119, loss: 0.0310, instance_loss: 0.0000, weighted_loss: 0.0217, label: 0, bag_size: 16052\n",
      "batch 139, loss: 0.4017, instance_loss: 0.1720, weighted_loss: 0.3328, label: 0, bag_size: 1498\n",
      "batch 159, loss: 0.0460, instance_loss: 0.0047, weighted_loss: 0.0336, label: 0, bag_size: 1891\n",
      "batch 179, loss: 0.0123, instance_loss: 0.0000, weighted_loss: 0.0086, label: 0, bag_size: 9060\n",
      "batch 199, loss: 0.1579, instance_loss: 0.0028, weighted_loss: 0.1114, label: 0, bag_size: 16720\n",
      "batch 219, loss: 0.0140, instance_loss: 0.0210, weighted_loss: 0.0161, label: 1, bag_size: 1014\n",
      "batch 239, loss: 0.0526, instance_loss: 0.0000, weighted_loss: 0.0368, label: 0, bag_size: 9471\n",
      "batch 259, loss: 0.0610, instance_loss: 0.4938, weighted_loss: 0.1908, label: 0, bag_size: 14377\n",
      "batch 279, loss: 0.0086, instance_loss: 0.0135, weighted_loss: 0.0101, label: 0, bag_size: 14206\n",
      "batch 299, loss: 0.2458, instance_loss: 0.3846, weighted_loss: 0.2875, label: 1, bag_size: 6478\n",
      "batch 319, loss: 0.2240, instance_loss: 0.2364, weighted_loss: 0.2277, label: 0, bag_size: 5009\n",
      "batch 339, loss: 0.0053, instance_loss: 0.0007, weighted_loss: 0.0039, label: 0, bag_size: 16341\n",
      "batch 359, loss: 0.0025, instance_loss: 0.0093, weighted_loss: 0.0045, label: 0, bag_size: 16782\n",
      "batch 379, loss: 0.0558, instance_loss: 0.0574, weighted_loss: 0.0563, label: 1, bag_size: 5516\n",
      "batch 399, loss: 0.0575, instance_loss: 0.0038, weighted_loss: 0.0414, label: 0, bag_size: 21076\n",
      "batch 419, loss: 0.0041, instance_loss: 0.0168, weighted_loss: 0.0079, label: 1, bag_size: 8466\n",
      "batch 439, loss: 0.0063, instance_loss: 0.0118, weighted_loss: 0.0080, label: 0, bag_size: 1588\n",
      "batch 459, loss: 0.0206, instance_loss: 0.0016, weighted_loss: 0.0149, label: 0, bag_size: 8025\n",
      "batch 479, loss: 0.9042, instance_loss: 2.7569, weighted_loss: 1.4600, label: 1, bag_size: 2937\n",
      "batch 499, loss: 0.1734, instance_loss: 0.9757, weighted_loss: 0.4141, label: 0, bag_size: 2490\n",
      "batch 519, loss: 0.0985, instance_loss: 0.0567, weighted_loss: 0.0860, label: 0, bag_size: 9930\n",
      "batch 539, loss: 0.0283, instance_loss: 0.0195, weighted_loss: 0.0257, label: 0, bag_size: 3232\n",
      "batch 559, loss: 0.0143, instance_loss: 0.0065, weighted_loss: 0.0119, label: 0, bag_size: 10068\n",
      "batch 579, loss: 0.0124, instance_loss: 0.0039, weighted_loss: 0.0099, label: 0, bag_size: 19470\n",
      "batch 599, loss: 0.0106, instance_loss: 0.0000, weighted_loss: 0.0074, label: 0, bag_size: 10365\n",
      "batch 619, loss: 0.0131, instance_loss: 0.0845, weighted_loss: 0.0345, label: 0, bag_size: 10535\n",
      "batch 639, loss: 0.8398, instance_loss: 0.0630, weighted_loss: 0.6068, label: 0, bag_size: 16690\n",
      "batch 659, loss: 0.0209, instance_loss: 0.0000, weighted_loss: 0.0146, label: 1, bag_size: 22286\n",
      "batch 679, loss: 0.0126, instance_loss: 0.0007, weighted_loss: 0.0090, label: 0, bag_size: 10304\n",
      "batch 699, loss: 0.0543, instance_loss: 0.2591, weighted_loss: 0.1158, label: 0, bag_size: 1745\n",
      "batch 719, loss: 0.0971, instance_loss: 0.1355, weighted_loss: 0.1086, label: 1, bag_size: 1294\n",
      "batch 739, loss: 0.8415, instance_loss: 0.1961, weighted_loss: 0.6479, label: 0, bag_size: 3654\n",
      "batch 759, loss: 1.2006, instance_loss: 0.3437, weighted_loss: 0.9435, label: 1, bag_size: 15192\n",
      "batch 779, loss: 0.1114, instance_loss: 0.2992, weighted_loss: 0.1677, label: 1, bag_size: 2579\n",
      "batch 799, loss: 0.3766, instance_loss: 0.0362, weighted_loss: 0.2745, label: 1, bag_size: 10460\n",
      "batch 819, loss: 0.0324, instance_loss: 0.0001, weighted_loss: 0.0227, label: 1, bag_size: 9065\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.974472891566265: correct 12941/13280\n",
      "class 1 clustering acc 0.8796686746987952: correct 5841/6640\n",
      "Epoch: 23, train_loss: 0.2517, train_clustering_loss:  0.2353, train_error: 0.1060\n",
      "class 0: acc 0.8943488943488943, correct 364/407\n",
      "class 1: acc 0.8936170212765957, correct 378/423\n",
      "\n",
      "Val Set, val_loss: 0.4874, val_error: 0.1649, auc: 0.8672\n",
      "class 0 clustering acc 0.9259020618556701: correct 1437/1552\n",
      "class 1 clustering acc 0.7332474226804123: correct 569/776\n",
      "class 0: acc 0.851063829787234, correct 40/47\n",
      "class 1: acc 0.82, correct 41/50\n",
      "Validation loss decreased (0.490327 --> 0.487422).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0737, instance_loss: 0.0020, weighted_loss: 0.0522, label: 0, bag_size: 18132\n",
      "batch 39, loss: 0.2574, instance_loss: 0.0022, weighted_loss: 0.1809, label: 0, bag_size: 10146\n",
      "batch 59, loss: 0.7040, instance_loss: 0.1690, weighted_loss: 0.5435, label: 0, bag_size: 11128\n",
      "batch 79, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 15736\n",
      "batch 99, loss: 0.0442, instance_loss: 0.0308, weighted_loss: 0.0402, label: 0, bag_size: 18132\n",
      "batch 119, loss: 0.0437, instance_loss: 0.0773, weighted_loss: 0.0538, label: 0, bag_size: 3657\n",
      "batch 139, loss: 0.0727, instance_loss: 0.0018, weighted_loss: 0.0514, label: 0, bag_size: 13943\n",
      "batch 159, loss: 0.3487, instance_loss: 0.0585, weighted_loss: 0.2617, label: 0, bag_size: 18777\n",
      "batch 179, loss: 0.0031, instance_loss: 0.0006, weighted_loss: 0.0024, label: 0, bag_size: 11477\n",
      "batch 199, loss: 0.3179, instance_loss: 0.4571, weighted_loss: 0.3596, label: 0, bag_size: 7031\n",
      "batch 219, loss: 0.0252, instance_loss: 0.0014, weighted_loss: 0.0181, label: 1, bag_size: 4367\n",
      "batch 239, loss: 0.1713, instance_loss: 0.0112, weighted_loss: 0.1233, label: 0, bag_size: 3552\n",
      "batch 259, loss: 0.0298, instance_loss: 0.0005, weighted_loss: 0.0210, label: 1, bag_size: 6343\n",
      "batch 279, loss: 0.7264, instance_loss: 0.4538, weighted_loss: 0.6446, label: 1, bag_size: 1764\n",
      "batch 299, loss: 0.0146, instance_loss: 0.0102, weighted_loss: 0.0133, label: 0, bag_size: 3190\n",
      "batch 319, loss: 0.1507, instance_loss: 0.0478, weighted_loss: 0.1199, label: 0, bag_size: 4845\n",
      "batch 339, loss: 0.5577, instance_loss: 0.8452, weighted_loss: 0.6439, label: 1, bag_size: 4956\n",
      "batch 359, loss: 0.1856, instance_loss: 0.3873, weighted_loss: 0.2461, label: 0, bag_size: 3444\n",
      "batch 379, loss: 0.0299, instance_loss: 0.0015, weighted_loss: 0.0213, label: 0, bag_size: 16052\n",
      "batch 399, loss: 0.0042, instance_loss: 0.0314, weighted_loss: 0.0124, label: 0, bag_size: 4902\n",
      "batch 419, loss: 0.0054, instance_loss: 0.0000, weighted_loss: 0.0038, label: 1, bag_size: 5864\n",
      "batch 439, loss: 0.3123, instance_loss: 0.2830, weighted_loss: 0.3035, label: 1, bag_size: 1493\n",
      "batch 459, loss: 0.0262, instance_loss: 0.2676, weighted_loss: 0.0986, label: 1, bag_size: 1412\n",
      "batch 479, loss: 0.0054, instance_loss: 0.0001, weighted_loss: 0.0038, label: 0, bag_size: 18574\n",
      "batch 499, loss: 0.0686, instance_loss: 0.0091, weighted_loss: 0.0508, label: 1, bag_size: 2695\n",
      "batch 519, loss: 1.2588, instance_loss: 1.2449, weighted_loss: 1.2546, label: 1, bag_size: 2455\n",
      "batch 539, loss: 0.4711, instance_loss: 0.1201, weighted_loss: 0.3658, label: 1, bag_size: 2904\n",
      "batch 559, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 5612\n",
      "batch 579, loss: 0.0172, instance_loss: 0.0002, weighted_loss: 0.0121, label: 0, bag_size: 11727\n",
      "batch 599, loss: 0.0281, instance_loss: 0.0000, weighted_loss: 0.0197, label: 1, bag_size: 19500\n",
      "batch 619, loss: 0.0067, instance_loss: 0.0092, weighted_loss: 0.0075, label: 1, bag_size: 1244\n",
      "batch 639, loss: 0.0546, instance_loss: 0.0081, weighted_loss: 0.0406, label: 0, bag_size: 12840\n",
      "batch 659, loss: 0.0873, instance_loss: 0.0865, weighted_loss: 0.0871, label: 0, bag_size: 5551\n",
      "batch 679, loss: 0.0141, instance_loss: 0.2453, weighted_loss: 0.0835, label: 1, bag_size: 617\n",
      "batch 699, loss: 0.0149, instance_loss: 0.0033, weighted_loss: 0.0114, label: 1, bag_size: 3409\n",
      "batch 719, loss: 0.2679, instance_loss: 0.0154, weighted_loss: 0.1922, label: 0, bag_size: 14885\n",
      "batch 739, loss: 0.2631, instance_loss: 0.0005, weighted_loss: 0.1843, label: 1, bag_size: 11729\n",
      "batch 759, loss: 0.0106, instance_loss: 0.0064, weighted_loss: 0.0094, label: 0, bag_size: 21032\n",
      "batch 779, loss: 0.0352, instance_loss: 0.0000, weighted_loss: 0.0246, label: 1, bag_size: 21009\n",
      "batch 799, loss: 0.1702, instance_loss: 0.0074, weighted_loss: 0.1214, label: 0, bag_size: 14830\n",
      "batch 819, loss: 0.0754, instance_loss: 0.1346, weighted_loss: 0.0932, label: 1, bag_size: 9519\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9780120481927711: correct 12988/13280\n",
      "class 1 clustering acc 0.8951807228915662: correct 5944/6640\n",
      "Epoch: 24, train_loss: 0.2423, train_clustering_loss:  0.2078, train_error: 0.0952\n",
      "class 0: acc 0.9037037037037037, correct 366/405\n",
      "class 1: acc 0.9058823529411765, correct 385/425\n",
      "\n",
      "Val Set, val_loss: 0.5594, val_error: 0.1546, auc: 0.8685\n",
      "class 0 clustering acc 0.9407216494845361: correct 1460/1552\n",
      "class 1 clustering acc 0.7925257731958762: correct 615/776\n",
      "class 0: acc 0.9574468085106383, correct 45/47\n",
      "class 1: acc 0.74, correct 37/50\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0130, instance_loss: 0.0000, weighted_loss: 0.0091, label: 1, bag_size: 5441\n",
      "batch 39, loss: 0.0984, instance_loss: 0.4766, weighted_loss: 0.2118, label: 0, bag_size: 9252\n",
      "batch 59, loss: 0.0390, instance_loss: 0.0152, weighted_loss: 0.0319, label: 0, bag_size: 10721\n",
      "batch 79, loss: 0.0015, instance_loss: 0.0025, weighted_loss: 0.0018, label: 0, bag_size: 11865\n",
      "batch 99, loss: 1.1089, instance_loss: 0.5127, weighted_loss: 0.9300, label: 1, bag_size: 1831\n",
      "batch 119, loss: 0.0028, instance_loss: 0.0020, weighted_loss: 0.0025, label: 1, bag_size: 1919\n",
      "batch 139, loss: 0.0579, instance_loss: 1.5361, weighted_loss: 0.5014, label: 1, bag_size: 1064\n",
      "batch 159, loss: 0.0153, instance_loss: 0.0058, weighted_loss: 0.0125, label: 1, bag_size: 3082\n",
      "batch 179, loss: 0.2781, instance_loss: 0.0000, weighted_loss: 0.1947, label: 0, bag_size: 5297\n",
      "batch 199, loss: 0.0142, instance_loss: 0.0000, weighted_loss: 0.0100, label: 1, bag_size: 12865\n",
      "batch 219, loss: 0.0500, instance_loss: 0.0425, weighted_loss: 0.0477, label: 0, bag_size: 19880\n",
      "batch 239, loss: 0.8774, instance_loss: 0.4773, weighted_loss: 0.7574, label: 0, bag_size: 2815\n",
      "batch 259, loss: 0.3003, instance_loss: 1.1879, weighted_loss: 0.5666, label: 1, bag_size: 22264\n",
      "batch 279, loss: 0.0840, instance_loss: 0.0229, weighted_loss: 0.0657, label: 1, bag_size: 10396\n",
      "batch 299, loss: 0.0293, instance_loss: 0.1696, weighted_loss: 0.0714, label: 0, bag_size: 1760\n",
      "batch 319, loss: 0.0032, instance_loss: 0.0000, weighted_loss: 0.0022, label: 1, bag_size: 7583\n",
      "batch 339, loss: 0.0507, instance_loss: 0.1212, weighted_loss: 0.0718, label: 0, bag_size: 8025\n",
      "batch 359, loss: 0.0162, instance_loss: 0.0129, weighted_loss: 0.0152, label: 0, bag_size: 4902\n",
      "batch 379, loss: 0.0350, instance_loss: 0.0879, weighted_loss: 0.0509, label: 0, bag_size: 13023\n",
      "batch 399, loss: 0.0261, instance_loss: 0.0052, weighted_loss: 0.0198, label: 1, bag_size: 3968\n",
      "batch 419, loss: 0.1421, instance_loss: 0.1084, weighted_loss: 0.1320, label: 0, bag_size: 19043\n",
      "batch 439, loss: 0.3707, instance_loss: 0.0675, weighted_loss: 0.2797, label: 1, bag_size: 7351\n",
      "batch 459, loss: 0.0139, instance_loss: 0.0103, weighted_loss: 0.0128, label: 1, bag_size: 16162\n",
      "batch 479, loss: 0.0060, instance_loss: 0.1790, weighted_loss: 0.0579, label: 1, bag_size: 14604\n",
      "batch 499, loss: 0.0113, instance_loss: 0.0112, weighted_loss: 0.0112, label: 0, bag_size: 9485\n",
      "batch 519, loss: 0.3504, instance_loss: 0.0308, weighted_loss: 0.2545, label: 1, bag_size: 5310\n",
      "batch 539, loss: 0.0011, instance_loss: 0.0011, weighted_loss: 0.0011, label: 1, bag_size: 6776\n",
      "batch 559, loss: 1.0677, instance_loss: 0.7860, weighted_loss: 0.9832, label: 0, bag_size: 19808\n",
      "batch 579, loss: 0.0157, instance_loss: 0.0122, weighted_loss: 0.0147, label: 0, bag_size: 4497\n",
      "batch 599, loss: 0.0070, instance_loss: 0.0015, weighted_loss: 0.0053, label: 1, bag_size: 15008\n",
      "batch 619, loss: 0.3998, instance_loss: 0.3134, weighted_loss: 0.3739, label: 0, bag_size: 2006\n",
      "batch 639, loss: 3.0804, instance_loss: 2.2724, weighted_loss: 2.8380, label: 0, bag_size: 11151\n",
      "batch 659, loss: 0.2263, instance_loss: 0.1735, weighted_loss: 0.2105, label: 0, bag_size: 518\n",
      "batch 679, loss: 0.0375, instance_loss: 0.0030, weighted_loss: 0.0272, label: 0, bag_size: 21864\n",
      "batch 699, loss: 0.0816, instance_loss: 0.1416, weighted_loss: 0.0996, label: 0, bag_size: 2322\n",
      "batch 719, loss: 0.7246, instance_loss: 1.1490, weighted_loss: 0.8520, label: 1, bag_size: 21252\n",
      "batch 739, loss: 0.0242, instance_loss: 0.0000, weighted_loss: 0.0170, label: 1, bag_size: 15093\n",
      "batch 759, loss: 1.6247, instance_loss: 1.7933, weighted_loss: 1.6753, label: 1, bag_size: 2937\n",
      "batch 779, loss: 0.0054, instance_loss: 0.0044, weighted_loss: 0.0051, label: 0, bag_size: 22426\n",
      "batch 799, loss: 0.1182, instance_loss: 0.0000, weighted_loss: 0.0827, label: 1, bag_size: 25695\n",
      "batch 819, loss: 0.5327, instance_loss: 0.0333, weighted_loss: 0.3829, label: 1, bag_size: 12340\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9702560240963856: correct 12885/13280\n",
      "class 1 clustering acc 0.8462349397590362: correct 5619/6640\n",
      "Epoch: 25, train_loss: 0.2625, train_clustering_loss:  0.2858, train_error: 0.1108\n",
      "class 0: acc 0.9007263922518159, correct 372/413\n",
      "class 1: acc 0.8776978417266187, correct 366/417\n",
      "\n",
      "Val Set, val_loss: 0.4984, val_error: 0.2165, auc: 0.8728\n",
      "class 0 clustering acc 0.907860824742268: correct 1409/1552\n",
      "class 1 clustering acc 0.6932989690721649: correct 538/776\n",
      "class 0: acc 0.723404255319149, correct 34/47\n",
      "class 1: acc 0.84, correct 42/50\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3985, instance_loss: 0.0113, weighted_loss: 0.2823, label: 1, bag_size: 7989\n",
      "batch 39, loss: 0.0661, instance_loss: 0.0494, weighted_loss: 0.0611, label: 0, bag_size: 7923\n",
      "batch 59, loss: 0.0180, instance_loss: 0.0429, weighted_loss: 0.0255, label: 0, bag_size: 11727\n",
      "batch 79, loss: 0.1285, instance_loss: 0.2453, weighted_loss: 0.1635, label: 0, bag_size: 2290\n",
      "batch 99, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 15233\n",
      "batch 119, loss: 0.2090, instance_loss: 0.0000, weighted_loss: 0.1463, label: 1, bag_size: 17579\n",
      "batch 139, loss: 0.0119, instance_loss: 0.0000, weighted_loss: 0.0083, label: 1, bag_size: 6606\n",
      "batch 159, loss: 0.0010, instance_loss: 0.0001, weighted_loss: 0.0007, label: 0, bag_size: 7191\n",
      "batch 179, loss: 0.0191, instance_loss: 0.0850, weighted_loss: 0.0389, label: 0, bag_size: 1452\n",
      "batch 199, loss: 4.4933, instance_loss: 1.1149, weighted_loss: 3.4798, label: 0, bag_size: 5105\n",
      "batch 219, loss: 0.0051, instance_loss: 0.0525, weighted_loss: 0.0193, label: 0, bag_size: 11187\n",
      "batch 239, loss: 0.0257, instance_loss: 0.0819, weighted_loss: 0.0425, label: 0, bag_size: 21032\n",
      "batch 259, loss: 0.0166, instance_loss: 0.0436, weighted_loss: 0.0247, label: 0, bag_size: 9930\n",
      "batch 279, loss: 0.1606, instance_loss: 0.4067, weighted_loss: 0.2344, label: 1, bag_size: 2559\n",
      "batch 299, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0011, label: 1, bag_size: 10112\n",
      "batch 319, loss: 0.3041, instance_loss: 0.0406, weighted_loss: 0.2250, label: 0, bag_size: 7031\n",
      "batch 339, loss: 0.0034, instance_loss: 0.0050, weighted_loss: 0.0039, label: 1, bag_size: 5833\n",
      "batch 359, loss: 0.0539, instance_loss: 0.5643, weighted_loss: 0.2070, label: 1, bag_size: 5921\n",
      "batch 379, loss: 0.0059, instance_loss: 0.0364, weighted_loss: 0.0151, label: 0, bag_size: 30751\n",
      "batch 399, loss: 0.0197, instance_loss: 0.2757, weighted_loss: 0.0965, label: 0, bag_size: 890\n",
      "batch 419, loss: 0.0255, instance_loss: 0.0381, weighted_loss: 0.0293, label: 0, bag_size: 13691\n",
      "batch 439, loss: 0.2217, instance_loss: 0.0312, weighted_loss: 0.1646, label: 0, bag_size: 15672\n",
      "batch 459, loss: 0.0242, instance_loss: 0.1522, weighted_loss: 0.0626, label: 0, bag_size: 10304\n",
      "batch 479, loss: 0.0182, instance_loss: 0.1829, weighted_loss: 0.0676, label: 0, bag_size: 1588\n",
      "batch 499, loss: 0.0139, instance_loss: 0.0148, weighted_loss: 0.0142, label: 0, bag_size: 16992\n",
      "batch 519, loss: 0.0174, instance_loss: 0.0000, weighted_loss: 0.0122, label: 1, bag_size: 12895\n",
      "batch 539, loss: 0.0167, instance_loss: 0.0000, weighted_loss: 0.0117, label: 1, bag_size: 9533\n",
      "batch 559, loss: 0.0875, instance_loss: 0.0361, weighted_loss: 0.0721, label: 0, bag_size: 1684\n",
      "batch 579, loss: 0.0073, instance_loss: 0.0000, weighted_loss: 0.0051, label: 1, bag_size: 7382\n",
      "batch 599, loss: 0.0307, instance_loss: 0.8251, weighted_loss: 0.2690, label: 1, bag_size: 1919\n",
      "batch 619, loss: 0.4049, instance_loss: 0.2036, weighted_loss: 0.3445, label: 0, bag_size: 2918\n",
      "batch 639, loss: 0.4409, instance_loss: 0.0542, weighted_loss: 0.3249, label: 1, bag_size: 8103\n",
      "batch 659, loss: 0.4610, instance_loss: 0.1989, weighted_loss: 0.3823, label: 1, bag_size: 12946\n",
      "batch 679, loss: 0.0220, instance_loss: 0.1423, weighted_loss: 0.0581, label: 0, bag_size: 6367\n",
      "batch 699, loss: 0.0219, instance_loss: 0.0030, weighted_loss: 0.0162, label: 1, bag_size: 4789\n",
      "batch 719, loss: 0.7497, instance_loss: 0.5977, weighted_loss: 0.7041, label: 0, bag_size: 2098\n",
      "batch 739, loss: 0.4994, instance_loss: 2.0662, weighted_loss: 0.9694, label: 1, bag_size: 1242\n",
      "batch 759, loss: 0.2364, instance_loss: 0.4642, weighted_loss: 0.3047, label: 0, bag_size: 17482\n",
      "batch 779, loss: 0.0194, instance_loss: 0.0069, weighted_loss: 0.0156, label: 0, bag_size: 9234\n",
      "batch 799, loss: 0.0489, instance_loss: 0.3117, weighted_loss: 0.1278, label: 1, bag_size: 15464\n",
      "batch 819, loss: 0.1004, instance_loss: 0.0026, weighted_loss: 0.0710, label: 1, bag_size: 3003\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9688253012048192: correct 12866/13280\n",
      "class 1 clustering acc 0.8402108433734939: correct 5579/6640\n",
      "Epoch: 26, train_loss: 0.2819, train_clustering_loss:  0.2837, train_error: 0.0988\n",
      "class 0: acc 0.9028436018957346, correct 381/422\n",
      "class 1: acc 0.8995098039215687, correct 367/408\n",
      "\n",
      "Val Set, val_loss: 0.4881, val_error: 0.1443, auc: 0.8817\n",
      "class 0 clustering acc 0.9220360824742269: correct 1431/1552\n",
      "class 1 clustering acc 0.7680412371134021: correct 596/776\n",
      "class 0: acc 0.9148936170212766, correct 43/47\n",
      "class 1: acc 0.8, correct 40/50\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.2197, instance_loss: 1.6580, weighted_loss: 1.3512, label: 1, bag_size: 8026\n",
      "batch 39, loss: 0.3267, instance_loss: 0.3713, weighted_loss: 0.3401, label: 0, bag_size: 4418\n",
      "batch 59, loss: 0.4635, instance_loss: 0.0899, weighted_loss: 0.3514, label: 1, bag_size: 2682\n",
      "batch 79, loss: 0.1197, instance_loss: 0.0038, weighted_loss: 0.0849, label: 1, bag_size: 8012\n",
      "batch 99, loss: 0.0681, instance_loss: 0.0413, weighted_loss: 0.0600, label: 1, bag_size: 8754\n",
      "batch 119, loss: 0.1056, instance_loss: 0.5279, weighted_loss: 0.2323, label: 0, bag_size: 14249\n",
      "batch 139, loss: 0.0053, instance_loss: 0.0000, weighted_loss: 0.0037, label: 1, bag_size: 4715\n",
      "batch 159, loss: 1.8265, instance_loss: 0.0680, weighted_loss: 1.2989, label: 0, bag_size: 1732\n",
      "batch 179, loss: 0.0253, instance_loss: 0.1075, weighted_loss: 0.0500, label: 0, bag_size: 21385\n",
      "batch 199, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 1, bag_size: 11981\n",
      "batch 219, loss: 0.0769, instance_loss: 0.2523, weighted_loss: 0.1295, label: 0, bag_size: 2534\n",
      "batch 239, loss: 0.9310, instance_loss: 0.3676, weighted_loss: 0.7620, label: 0, bag_size: 2266\n",
      "batch 259, loss: 0.0979, instance_loss: 0.2512, weighted_loss: 0.1439, label: 1, bag_size: 4039\n",
      "batch 279, loss: 0.0604, instance_loss: 0.5217, weighted_loss: 0.1988, label: 0, bag_size: 1127\n",
      "batch 299, loss: 0.1432, instance_loss: 0.0002, weighted_loss: 0.1003, label: 1, bag_size: 19606\n",
      "batch 319, loss: 0.2306, instance_loss: 0.1062, weighted_loss: 0.1933, label: 0, bag_size: 7141\n",
      "batch 339, loss: 0.0336, instance_loss: 0.0189, weighted_loss: 0.0292, label: 1, bag_size: 3656\n",
      "batch 359, loss: 0.2743, instance_loss: 0.2855, weighted_loss: 0.2777, label: 0, bag_size: 2148\n",
      "batch 379, loss: 0.0139, instance_loss: 0.0000, weighted_loss: 0.0098, label: 1, bag_size: 9533\n",
      "batch 399, loss: 0.0138, instance_loss: 0.0000, weighted_loss: 0.0097, label: 1, bag_size: 12408\n",
      "batch 419, loss: 0.0215, instance_loss: 0.0915, weighted_loss: 0.0425, label: 0, bag_size: 9171\n",
      "batch 439, loss: 0.0049, instance_loss: 0.0000, weighted_loss: 0.0035, label: 1, bag_size: 19039\n",
      "batch 459, loss: 0.0239, instance_loss: 0.2183, weighted_loss: 0.0822, label: 0, bag_size: 1120\n",
      "batch 479, loss: 0.0153, instance_loss: 0.0570, weighted_loss: 0.0279, label: 1, bag_size: 2140\n",
      "batch 499, loss: 0.5282, instance_loss: 0.4079, weighted_loss: 0.4921, label: 1, bag_size: 12494\n",
      "batch 519, loss: 0.0846, instance_loss: 0.0000, weighted_loss: 0.0593, label: 1, bag_size: 34356\n",
      "batch 539, loss: 0.0049, instance_loss: 0.0471, weighted_loss: 0.0175, label: 1, bag_size: 1165\n",
      "batch 559, loss: 0.2943, instance_loss: 0.1983, weighted_loss: 0.2655, label: 0, bag_size: 1814\n",
      "batch 579, loss: 0.1151, instance_loss: 0.0623, weighted_loss: 0.0992, label: 1, bag_size: 4976\n",
      "batch 599, loss: 0.0224, instance_loss: 0.8316, weighted_loss: 0.2652, label: 1, bag_size: 928\n",
      "batch 619, loss: 0.0173, instance_loss: 0.0000, weighted_loss: 0.0121, label: 1, bag_size: 3082\n",
      "batch 639, loss: 0.0528, instance_loss: 0.0176, weighted_loss: 0.0422, label: 0, bag_size: 3444\n",
      "batch 659, loss: 0.0154, instance_loss: 0.0607, weighted_loss: 0.0290, label: 1, bag_size: 699\n",
      "batch 679, loss: 0.0223, instance_loss: 0.0414, weighted_loss: 0.0280, label: 0, bag_size: 10365\n",
      "batch 699, loss: 0.0136, instance_loss: 0.6969, weighted_loss: 0.2186, label: 0, bag_size: 10263\n",
      "batch 719, loss: 0.0482, instance_loss: 0.0100, weighted_loss: 0.0367, label: 1, bag_size: 4789\n",
      "batch 739, loss: 0.0207, instance_loss: 0.0331, weighted_loss: 0.0244, label: 0, bag_size: 9470\n",
      "batch 759, loss: 0.0112, instance_loss: 0.0129, weighted_loss: 0.0117, label: 0, bag_size: 11865\n",
      "batch 779, loss: 1.1773, instance_loss: 0.1960, weighted_loss: 0.8829, label: 0, bag_size: 23618\n",
      "batch 799, loss: 0.0191, instance_loss: 0.0003, weighted_loss: 0.0135, label: 1, bag_size: 7110\n",
      "batch 819, loss: 0.0274, instance_loss: 0.0730, weighted_loss: 0.0411, label: 0, bag_size: 3774\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9684487951807229: correct 12861/13280\n",
      "class 1 clustering acc 0.8438253012048192: correct 5603/6640\n",
      "Epoch: 27, train_loss: 0.2847, train_clustering_loss:  0.2965, train_error: 0.1036\n",
      "class 0: acc 0.8854166666666666, correct 340/384\n",
      "class 1: acc 0.905829596412556, correct 404/446\n",
      "\n",
      "Val Set, val_loss: 0.4719, val_error: 0.1443, auc: 0.8864\n",
      "class 0 clustering acc 0.9381443298969072: correct 1456/1552\n",
      "class 1 clustering acc 0.7693298969072165: correct 597/776\n",
      "class 0: acc 0.9148936170212766, correct 43/47\n",
      "class 1: acc 0.8, correct 40/50\n",
      "Validation loss decreased (0.487422 --> 0.471930).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0075, instance_loss: 0.0000, weighted_loss: 0.0053, label: 0, bag_size: 31780\n",
      "batch 39, loss: 0.0107, instance_loss: 0.0002, weighted_loss: 0.0076, label: 1, bag_size: 7382\n",
      "batch 59, loss: 0.0228, instance_loss: 0.3929, weighted_loss: 0.1338, label: 0, bag_size: 2628\n",
      "batch 79, loss: 3.5179, instance_loss: 0.8750, weighted_loss: 2.7250, label: 0, bag_size: 1437\n",
      "batch 99, loss: 0.0121, instance_loss: 0.0000, weighted_loss: 0.0085, label: 0, bag_size: 12524\n",
      "batch 119, loss: 0.0641, instance_loss: 0.0171, weighted_loss: 0.0500, label: 1, bag_size: 5256\n",
      "batch 139, loss: 0.1003, instance_loss: 0.0001, weighted_loss: 0.0702, label: 1, bag_size: 9561\n",
      "batch 159, loss: 0.0038, instance_loss: 0.0004, weighted_loss: 0.0028, label: 0, bag_size: 23037\n",
      "batch 179, loss: 0.0926, instance_loss: 0.2002, weighted_loss: 0.1249, label: 0, bag_size: 7381\n",
      "batch 199, loss: 0.1689, instance_loss: 0.1806, weighted_loss: 0.1724, label: 0, bag_size: 1953\n",
      "batch 219, loss: 1.3682, instance_loss: 2.4796, weighted_loss: 1.7016, label: 0, bag_size: 11151\n",
      "batch 239, loss: 0.0055, instance_loss: 0.0000, weighted_loss: 0.0039, label: 0, bag_size: 18240\n",
      "batch 259, loss: 0.0272, instance_loss: 0.1297, weighted_loss: 0.0579, label: 0, bag_size: 9171\n",
      "batch 279, loss: 0.0087, instance_loss: 0.0166, weighted_loss: 0.0111, label: 0, bag_size: 12217\n",
      "batch 299, loss: 0.0024, instance_loss: 0.0000, weighted_loss: 0.0017, label: 0, bag_size: 10898\n",
      "batch 319, loss: 0.0253, instance_loss: 0.0027, weighted_loss: 0.0185, label: 0, bag_size: 22681\n",
      "batch 339, loss: 0.0070, instance_loss: 0.0067, weighted_loss: 0.0069, label: 1, bag_size: 19039\n",
      "batch 359, loss: 0.0075, instance_loss: 0.0130, weighted_loss: 0.0092, label: 0, bag_size: 3459\n",
      "batch 379, loss: 0.2719, instance_loss: 0.0000, weighted_loss: 0.1903, label: 1, bag_size: 8103\n",
      "batch 399, loss: 0.6227, instance_loss: 0.5591, weighted_loss: 0.6036, label: 1, bag_size: 6682\n",
      "batch 419, loss: 0.0049, instance_loss: 0.0000, weighted_loss: 0.0034, label: 1, bag_size: 15716\n",
      "batch 439, loss: 0.0067, instance_loss: 0.0032, weighted_loss: 0.0057, label: 0, bag_size: 8898\n",
      "batch 459, loss: 0.0531, instance_loss: 0.0078, weighted_loss: 0.0395, label: 0, bag_size: 4271\n",
      "batch 479, loss: 0.0085, instance_loss: 0.0019, weighted_loss: 0.0065, label: 1, bag_size: 11981\n",
      "batch 499, loss: 0.0478, instance_loss: 0.0341, weighted_loss: 0.0436, label: 0, bag_size: 2360\n",
      "batch 519, loss: 0.0630, instance_loss: 0.0045, weighted_loss: 0.0455, label: 1, bag_size: 1459\n",
      "batch 539, loss: 0.5426, instance_loss: 0.0027, weighted_loss: 0.3806, label: 1, bag_size: 15185\n",
      "batch 559, loss: 1.8627, instance_loss: 0.7353, weighted_loss: 1.5245, label: 0, bag_size: 2653\n",
      "batch 579, loss: 0.2239, instance_loss: 0.0383, weighted_loss: 0.1682, label: 0, bag_size: 2467\n",
      "batch 599, loss: 1.4864, instance_loss: 1.2196, weighted_loss: 1.4064, label: 1, bag_size: 12494\n",
      "batch 619, loss: 0.4761, instance_loss: 0.3300, weighted_loss: 0.4323, label: 0, bag_size: 2458\n",
      "batch 639, loss: 0.7382, instance_loss: 0.0142, weighted_loss: 0.5210, label: 1, bag_size: 13440\n",
      "batch 659, loss: 0.0041, instance_loss: 0.0013, weighted_loss: 0.0033, label: 0, bag_size: 10898\n",
      "batch 679, loss: 0.0179, instance_loss: 0.0159, weighted_loss: 0.0173, label: 0, bag_size: 7605\n",
      "batch 699, loss: 0.0017, instance_loss: 0.0023, weighted_loss: 0.0019, label: 0, bag_size: 21218\n",
      "batch 719, loss: 0.6651, instance_loss: 1.2509, weighted_loss: 0.8409, label: 1, bag_size: 1875\n",
      "batch 739, loss: 0.1759, instance_loss: 0.0075, weighted_loss: 0.1254, label: 1, bag_size: 5155\n",
      "batch 759, loss: 0.0606, instance_loss: 0.0102, weighted_loss: 0.0455, label: 0, bag_size: 2844\n",
      "batch 779, loss: 0.0533, instance_loss: 0.0082, weighted_loss: 0.0398, label: 1, bag_size: 30675\n",
      "batch 799, loss: 0.0045, instance_loss: 0.0020, weighted_loss: 0.0038, label: 1, bag_size: 5049\n",
      "batch 819, loss: 2.2830, instance_loss: 2.8482, weighted_loss: 2.4525, label: 1, bag_size: 25831\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.974472891566265: correct 12941/13280\n",
      "class 1 clustering acc 0.8772590361445783: correct 5825/6640\n",
      "Epoch: 28, train_loss: 0.2644, train_clustering_loss:  0.2401, train_error: 0.1048\n",
      "class 0: acc 0.9071428571428571, correct 381/420\n",
      "class 1: acc 0.8829268292682927, correct 362/410\n",
      "\n",
      "Val Set, val_loss: 0.5117, val_error: 0.2371, auc: 0.8881\n",
      "class 0 clustering acc 0.9510309278350515: correct 1476/1552\n",
      "class 1 clustering acc 0.8079896907216495: correct 627/776\n",
      "class 0: acc 0.6382978723404256, correct 30/47\n",
      "class 1: acc 0.88, correct 44/50\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0064, instance_loss: 0.0062, weighted_loss: 0.0063, label: 0, bag_size: 18415\n",
      "batch 39, loss: 0.0161, instance_loss: 0.0008, weighted_loss: 0.0115, label: 1, bag_size: 17486\n",
      "batch 59, loss: 0.0065, instance_loss: 0.0000, weighted_loss: 0.0046, label: 1, bag_size: 6164\n",
      "batch 79, loss: 0.0095, instance_loss: 0.0001, weighted_loss: 0.0067, label: 1, bag_size: 10920\n",
      "batch 99, loss: 0.1115, instance_loss: 0.0226, weighted_loss: 0.0848, label: 0, bag_size: 4845\n",
      "batch 119, loss: 0.0164, instance_loss: 0.0107, weighted_loss: 0.0147, label: 0, bag_size: 4497\n",
      "batch 139, loss: 0.0388, instance_loss: 0.4487, weighted_loss: 0.1618, label: 1, bag_size: 1512\n",
      "batch 159, loss: 0.0018, instance_loss: 0.0000, weighted_loss: 0.0012, label: 1, bag_size: 9644\n",
      "batch 179, loss: 0.0074, instance_loss: 0.0042, weighted_loss: 0.0064, label: 0, bag_size: 17791\n",
      "batch 199, loss: 0.2850, instance_loss: 0.1757, weighted_loss: 0.2522, label: 0, bag_size: 2160\n",
      "batch 219, loss: 0.1581, instance_loss: 0.8443, weighted_loss: 0.3640, label: 1, bag_size: 5292\n",
      "batch 239, loss: 0.0196, instance_loss: 0.0000, weighted_loss: 0.0137, label: 1, bag_size: 9636\n",
      "batch 259, loss: 0.0247, instance_loss: 0.0042, weighted_loss: 0.0186, label: 0, bag_size: 13023\n",
      "batch 279, loss: 0.0202, instance_loss: 0.0030, weighted_loss: 0.0151, label: 0, bag_size: 4345\n",
      "batch 299, loss: 0.1237, instance_loss: 0.0435, weighted_loss: 0.0997, label: 0, bag_size: 4271\n",
      "batch 319, loss: 0.0071, instance_loss: 0.0059, weighted_loss: 0.0067, label: 0, bag_size: 13218\n",
      "batch 339, loss: 0.1638, instance_loss: 0.0009, weighted_loss: 0.1149, label: 1, bag_size: 6665\n",
      "batch 359, loss: 2.3294, instance_loss: 2.4819, weighted_loss: 2.3752, label: 1, bag_size: 19470\n",
      "batch 379, loss: 0.0549, instance_loss: 0.0121, weighted_loss: 0.0421, label: 0, bag_size: 11727\n",
      "batch 399, loss: 0.1933, instance_loss: 0.2965, weighted_loss: 0.2243, label: 1, bag_size: 7981\n",
      "batch 419, loss: 0.0169, instance_loss: 0.0906, weighted_loss: 0.0390, label: 0, bag_size: 1712\n",
      "batch 439, loss: 0.0710, instance_loss: 0.0368, weighted_loss: 0.0607, label: 0, bag_size: 16211\n",
      "batch 459, loss: 0.0095, instance_loss: 0.0024, weighted_loss: 0.0074, label: 0, bag_size: 10791\n",
      "batch 479, loss: 0.0191, instance_loss: 0.0587, weighted_loss: 0.0310, label: 0, bag_size: 11125\n",
      "batch 499, loss: 0.0484, instance_loss: 0.0001, weighted_loss: 0.0339, label: 1, bag_size: 20537\n",
      "batch 519, loss: 0.1580, instance_loss: 0.0438, weighted_loss: 0.1238, label: 0, bag_size: 2148\n",
      "batch 539, loss: 0.0280, instance_loss: 0.0193, weighted_loss: 0.0254, label: 0, bag_size: 6652\n",
      "batch 559, loss: 0.3680, instance_loss: 0.1237, weighted_loss: 0.2947, label: 1, bag_size: 8475\n",
      "batch 579, loss: 0.0098, instance_loss: 0.0000, weighted_loss: 0.0069, label: 1, bag_size: 15093\n",
      "batch 599, loss: 0.0538, instance_loss: 0.0000, weighted_loss: 0.0377, label: 1, bag_size: 2848\n",
      "batch 619, loss: 0.0027, instance_loss: 0.0014, weighted_loss: 0.0023, label: 0, bag_size: 9455\n",
      "batch 639, loss: 0.1739, instance_loss: 0.4924, weighted_loss: 0.2694, label: 0, bag_size: 1690\n",
      "batch 659, loss: 0.1318, instance_loss: 0.0151, weighted_loss: 0.0968, label: 0, bag_size: 9542\n",
      "batch 679, loss: 0.2191, instance_loss: 0.2335, weighted_loss: 0.2234, label: 0, bag_size: 1498\n",
      "batch 699, loss: 0.0548, instance_loss: 0.0486, weighted_loss: 0.0529, label: 1, bag_size: 8003\n",
      "batch 719, loss: 0.0152, instance_loss: 0.0102, weighted_loss: 0.0137, label: 1, bag_size: 3295\n",
      "batch 739, loss: 0.0138, instance_loss: 0.0008, weighted_loss: 0.0099, label: 1, bag_size: 7217\n",
      "batch 759, loss: 0.0387, instance_loss: 0.0041, weighted_loss: 0.0283, label: 0, bag_size: 4497\n",
      "batch 779, loss: 0.0083, instance_loss: 0.0019, weighted_loss: 0.0063, label: 0, bag_size: 9851\n",
      "batch 799, loss: 0.0547, instance_loss: 0.0969, weighted_loss: 0.0674, label: 0, bag_size: 1745\n",
      "batch 819, loss: 0.0569, instance_loss: 0.0075, weighted_loss: 0.0421, label: 0, bag_size: 7557\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9803463855421687: correct 13019/13280\n",
      "class 1 clustering acc 0.9093373493975904: correct 6038/6640\n",
      "Epoch: 29, train_loss: 0.2072, train_clustering_loss:  0.1874, train_error: 0.0771\n",
      "class 0: acc 0.9316037735849056, correct 395/424\n",
      "class 1: acc 0.9137931034482759, correct 371/406\n",
      "\n",
      "Val Set, val_loss: 0.5267, val_error: 0.1649, auc: 0.8885\n",
      "class 0 clustering acc 0.9329896907216495: correct 1448/1552\n",
      "class 1 clustering acc 0.8041237113402062: correct 624/776\n",
      "class 0: acc 0.9574468085106383, correct 45/47\n",
      "class 1: acc 0.72, correct 36/50\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 4.0663, instance_loss: 0.9229, weighted_loss: 3.1233, label: 0, bag_size: 5105\n",
      "batch 39, loss: 0.0058, instance_loss: 0.0000, weighted_loss: 0.0040, label: 0, bag_size: 11194\n",
      "batch 59, loss: 0.5890, instance_loss: 2.8216, weighted_loss: 1.2588, label: 0, bag_size: 15255\n",
      "batch 79, loss: 0.3999, instance_loss: 0.1857, weighted_loss: 0.3356, label: 0, bag_size: 2098\n",
      "batch 99, loss: 0.0873, instance_loss: 0.0000, weighted_loss: 0.0611, label: 1, bag_size: 12758\n",
      "batch 119, loss: 0.2486, instance_loss: 0.6134, weighted_loss: 0.3580, label: 0, bag_size: 13332\n",
      "batch 139, loss: 0.0489, instance_loss: 0.0000, weighted_loss: 0.0342, label: 1, bag_size: 18095\n",
      "batch 159, loss: 1.6617, instance_loss: 1.0994, weighted_loss: 1.4930, label: 0, bag_size: 11306\n",
      "batch 179, loss: 0.0049, instance_loss: 0.0000, weighted_loss: 0.0034, label: 1, bag_size: 14681\n",
      "batch 199, loss: 0.0279, instance_loss: 0.0003, weighted_loss: 0.0196, label: 1, bag_size: 9636\n",
      "batch 219, loss: 0.2028, instance_loss: 0.2541, weighted_loss: 0.2182, label: 0, bag_size: 7989\n",
      "batch 239, loss: 0.0006, instance_loss: 0.0003, weighted_loss: 0.0005, label: 0, bag_size: 3787\n",
      "batch 259, loss: 0.1515, instance_loss: 0.0000, weighted_loss: 0.1061, label: 1, bag_size: 12758\n",
      "batch 279, loss: 1.1275, instance_loss: 2.3552, weighted_loss: 1.4958, label: 1, bag_size: 2937\n",
      "batch 299, loss: 0.0263, instance_loss: 0.0469, weighted_loss: 0.0325, label: 0, bag_size: 16211\n",
      "batch 319, loss: 0.0082, instance_loss: 0.0000, weighted_loss: 0.0057, label: 1, bag_size: 11389\n",
      "batch 339, loss: 0.0083, instance_loss: 0.0010, weighted_loss: 0.0061, label: 1, bag_size: 4128\n",
      "batch 359, loss: 0.1990, instance_loss: 2.1487, weighted_loss: 0.7839, label: 0, bag_size: 11607\n",
      "batch 379, loss: 0.0091, instance_loss: 0.0466, weighted_loss: 0.0203, label: 1, bag_size: 2344\n",
      "batch 399, loss: 0.0081, instance_loss: 0.0365, weighted_loss: 0.0166, label: 0, bag_size: 14956\n",
      "batch 419, loss: 0.0050, instance_loss: 0.0128, weighted_loss: 0.0073, label: 0, bag_size: 12524\n",
      "batch 439, loss: 0.0056, instance_loss: 0.0193, weighted_loss: 0.0097, label: 0, bag_size: 3970\n",
      "batch 459, loss: 0.1601, instance_loss: 0.0623, weighted_loss: 0.1308, label: 0, bag_size: 9234\n",
      "batch 479, loss: 0.0365, instance_loss: 0.0000, weighted_loss: 0.0255, label: 1, bag_size: 10396\n",
      "batch 499, loss: 0.0170, instance_loss: 0.0015, weighted_loss: 0.0123, label: 1, bag_size: 11387\n",
      "batch 519, loss: 0.2971, instance_loss: 0.0401, weighted_loss: 0.2200, label: 1, bag_size: 3937\n",
      "batch 539, loss: 0.0226, instance_loss: 0.0010, weighted_loss: 0.0162, label: 0, bag_size: 6624\n",
      "batch 559, loss: 0.0458, instance_loss: 0.0157, weighted_loss: 0.0368, label: 0, bag_size: 4523\n",
      "batch 579, loss: 0.0026, instance_loss: 0.0171, weighted_loss: 0.0069, label: 0, bag_size: 12503\n",
      "batch 599, loss: 0.0165, instance_loss: 0.0293, weighted_loss: 0.0204, label: 1, bag_size: 18468\n",
      "batch 619, loss: 3.9455, instance_loss: 3.6037, weighted_loss: 3.8429, label: 0, bag_size: 4692\n",
      "batch 639, loss: 0.0154, instance_loss: 0.0044, weighted_loss: 0.0121, label: 1, bag_size: 10867\n",
      "batch 659, loss: 0.0127, instance_loss: 0.0002, weighted_loss: 0.0089, label: 1, bag_size: 1838\n",
      "batch 679, loss: 0.1604, instance_loss: 0.0386, weighted_loss: 0.1239, label: 1, bag_size: 3652\n",
      "batch 699, loss: 0.4002, instance_loss: 0.0233, weighted_loss: 0.2872, label: 1, bag_size: 2137\n",
      "batch 719, loss: 0.0452, instance_loss: 0.0004, weighted_loss: 0.0318, label: 1, bag_size: 3968\n",
      "batch 739, loss: 0.5573, instance_loss: 0.0004, weighted_loss: 0.3902, label: 1, bag_size: 10591\n",
      "batch 759, loss: 0.3036, instance_loss: 0.0086, weighted_loss: 0.2151, label: 0, bag_size: 6884\n",
      "batch 779, loss: 0.2239, instance_loss: 0.9935, weighted_loss: 0.4548, label: 0, bag_size: 10410\n",
      "batch 799, loss: 0.0046, instance_loss: 0.0000, weighted_loss: 0.0032, label: 1, bag_size: 15665\n",
      "batch 819, loss: 0.1178, instance_loss: 0.2734, weighted_loss: 0.1645, label: 1, bag_size: 4789\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9750753012048192: correct 12949/13280\n",
      "class 1 clustering acc 0.8719879518072289: correct 5790/6640\n",
      "Epoch: 30, train_loss: 0.2435, train_clustering_loss:  0.2421, train_error: 0.0892\n",
      "class 0: acc 0.920863309352518, correct 384/417\n",
      "class 1: acc 0.9007263922518159, correct 372/413\n",
      "\n",
      "Val Set, val_loss: 0.5525, val_error: 0.1546, auc: 0.8872\n",
      "class 0 clustering acc 0.9561855670103093: correct 1484/1552\n",
      "class 1 clustering acc 0.7951030927835051: correct 617/776\n",
      "class 0: acc 1.0, correct 47/47\n",
      "class 1: acc 0.7, correct 35/50\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0599, instance_loss: 0.0112, weighted_loss: 0.0453, label: 1, bag_size: 7932\n",
      "batch 39, loss: 0.0044, instance_loss: 0.0004, weighted_loss: 0.0032, label: 1, bag_size: 14433\n",
      "batch 59, loss: 0.2406, instance_loss: 0.0025, weighted_loss: 0.1692, label: 0, bag_size: 15672\n",
      "batch 79, loss: 0.0493, instance_loss: 0.7461, weighted_loss: 0.2583, label: 1, bag_size: 1014\n",
      "batch 99, loss: 0.0030, instance_loss: 0.0012, weighted_loss: 0.0024, label: 1, bag_size: 9078\n",
      "batch 119, loss: 0.0344, instance_loss: 0.0001, weighted_loss: 0.0241, label: 1, bag_size: 12460\n",
      "batch 139, loss: 0.1858, instance_loss: 0.1797, weighted_loss: 0.1840, label: 1, bag_size: 11316\n",
      "batch 159, loss: 0.1536, instance_loss: 0.0879, weighted_loss: 0.1339, label: 1, bag_size: 8103\n",
      "batch 179, loss: 0.0439, instance_loss: 0.0336, weighted_loss: 0.0408, label: 1, bag_size: 3450\n",
      "batch 199, loss: 0.1257, instance_loss: 0.4652, weighted_loss: 0.2275, label: 1, bag_size: 2681\n",
      "batch 219, loss: 0.2084, instance_loss: 0.1300, weighted_loss: 0.1849, label: 0, bag_size: 6884\n",
      "batch 239, loss: 0.2300, instance_loss: 0.7928, weighted_loss: 0.3989, label: 0, bag_size: 1127\n",
      "batch 259, loss: 0.0238, instance_loss: 0.6433, weighted_loss: 0.2096, label: 1, bag_size: 7148\n",
      "batch 279, loss: 0.0034, instance_loss: 0.0103, weighted_loss: 0.0055, label: 0, bag_size: 10791\n",
      "batch 299, loss: 0.0031, instance_loss: 0.0018, weighted_loss: 0.0027, label: 0, bag_size: 10898\n",
      "batch 319, loss: 0.0505, instance_loss: 0.0001, weighted_loss: 0.0354, label: 1, bag_size: 11220\n",
      "batch 339, loss: 0.7280, instance_loss: 0.8092, weighted_loss: 0.7523, label: 0, bag_size: 9132\n",
      "batch 359, loss: 0.0077, instance_loss: 0.0006, weighted_loss: 0.0056, label: 1, bag_size: 7935\n",
      "batch 379, loss: 1.0312, instance_loss: 0.0167, weighted_loss: 0.7269, label: 1, bag_size: 12714\n",
      "batch 399, loss: 0.0192, instance_loss: 0.0425, weighted_loss: 0.0262, label: 0, bag_size: 1452\n",
      "batch 419, loss: 0.0687, instance_loss: 0.0000, weighted_loss: 0.0481, label: 1, bag_size: 25695\n",
      "batch 439, loss: 0.0028, instance_loss: 0.0000, weighted_loss: 0.0020, label: 1, bag_size: 6317\n",
      "batch 459, loss: 0.2718, instance_loss: 0.0010, weighted_loss: 0.1905, label: 1, bag_size: 2682\n",
      "batch 479, loss: 0.2080, instance_loss: 0.2222, weighted_loss: 0.2123, label: 1, bag_size: 14155\n",
      "batch 499, loss: 0.1331, instance_loss: 0.0000, weighted_loss: 0.0932, label: 1, bag_size: 6665\n",
      "batch 519, loss: 0.0178, instance_loss: 0.0951, weighted_loss: 0.0410, label: 1, bag_size: 2495\n",
      "batch 539, loss: 0.2080, instance_loss: 0.1470, weighted_loss: 0.1897, label: 0, bag_size: 10814\n",
      "batch 559, loss: 0.0343, instance_loss: 0.0008, weighted_loss: 0.0242, label: 0, bag_size: 2044\n",
      "batch 579, loss: 0.0053, instance_loss: 0.0030, weighted_loss: 0.0046, label: 0, bag_size: 13795\n",
      "batch 599, loss: 0.0441, instance_loss: 0.0007, weighted_loss: 0.0311, label: 1, bag_size: 13174\n",
      "batch 619, loss: 0.0725, instance_loss: 0.0036, weighted_loss: 0.0519, label: 1, bag_size: 11220\n",
      "batch 639, loss: 1.3085, instance_loss: 2.2828, weighted_loss: 1.6008, label: 1, bag_size: 16514\n",
      "batch 659, loss: 0.0132, instance_loss: 0.0139, weighted_loss: 0.0134, label: 0, bag_size: 10942\n",
      "batch 679, loss: 1.5219, instance_loss: 0.5874, weighted_loss: 1.2416, label: 1, bag_size: 2842\n",
      "batch 699, loss: 0.0977, instance_loss: 0.0519, weighted_loss: 0.0840, label: 0, bag_size: 9387\n",
      "batch 719, loss: 0.0117, instance_loss: 0.0204, weighted_loss: 0.0143, label: 0, bag_size: 19067\n",
      "batch 739, loss: 0.0079, instance_loss: 0.0043, weighted_loss: 0.0068, label: 1, bag_size: 617\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python main_myself.py --drop_out --early_stopping --lr 2e-4 --max_lr 2e-3 --k 5 --label_frac 1 --opt adam\\\n",
    "--exp_code cptac_lung_100_level02_mcbat_sb_depth2_adam_FLASH --weighted_sample --bag_loss ce --inst_loss svm \\\n",
    "--task task_2_tumor_subtyping --model_type mcbat_sb --log_data --data_low_dir /home/sci/Disk2/CPTAC-LUNG/FEATURES_level0 \\\n",
    "--data_high_dir /home/sci/Disk2/CPTAC-LUNG/FEATURES_level1 \\\n",
    "--split_dir /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/cptac_lung_100 --subtyping \\\n",
    "--csv_path dataset_csv/cptac_lung_subtyping.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA_VISIBLE_DEVICES=0 python eval.py \\\n",
    "--drop_out \\\n",
    "--k 10 \\\n",
    "--models_exp_code task_1_tumor_vs_normal_CLAM_50_s1 \\\n",
    "--save_exp_code task_1_tumor_vs_normal_CLAM_50_s1_cv \\\n",
    "--task task_1_tumor_vs_normal \\\n",
    "--model_type clam_sb \\\n",
    "--results_dir results \\\n",
    "--data_root_dir /media/yuansh/14THHD/CLAM/FEATURES_DIRECTORY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': 'task_1_tumor_vs_normal', 'split': 'test', 'save_dir': './eval_results/EVAL_tcga_crc_5_cv', 'models_dir': 'results/tcga_crc_5_s1', 'model_type': 'clam_sb', 'drop_out': True, 'model_size': 'small'}\n",
      "label column: label\n",
      "label dictionary: {'normal_tissue': 0, 'tumor_tissue': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 1    1342\n",
      "0      91\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 3\n",
      "Slide-LVL; Number of samples registered in class 0: 91\n",
      "Patient-LVL; Number of samples registered in class 1: 497\n",
      "Slide-LVL; Number of samples registered in class 1: 1342\n",
      "Init Model\n",
      "CLAM_SB(\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (instance_loss_fn): CrossEntropyLoss()\n",
      ")\n",
      "Total number of parameters: 790791\n",
      "Total number of trainable parameters: 790791\n",
      "Init Loaders\n",
      "133\n",
      "test_error:  0.022556390977443608\n",
      "auc:  0.43333333333333335\n",
      "Init Model\n",
      "CLAM_SB(\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (instance_loss_fn): CrossEntropyLoss()\n",
      ")\n",
      "Total number of parameters: 790791\n",
      "Total number of trainable parameters: 790791\n",
      "Init Loaders\n",
      "140\n",
      "test_error:  0.04285714285714286\n",
      "auc:  0.5634328358208955\n",
      "Init Model\n",
      "CLAM_SB(\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (instance_loss_fn): CrossEntropyLoss()\n",
      ")\n",
      "Total number of parameters: 790791\n",
      "Total number of trainable parameters: 790791\n",
      "Init Loaders\n",
      "146\n",
      "test_error:  0.0821917808219178\n",
      "auc:  0.34079601990049746\n",
      "Init Model\n",
      "CLAM_SB(\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (instance_loss_fn): CrossEntropyLoss()\n",
      ")\n",
      "Total number of parameters: 790791\n",
      "Total number of trainable parameters: 790791\n",
      "Init Loaders\n",
      "149\n",
      "test_error:  0.08053691275167785\n",
      "auc:  0.3652676399026764\n",
      "Init Model\n",
      "CLAM_SB(\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (instance_loss_fn): CrossEntropyLoss()\n",
      ")\n",
      "Total number of parameters: 790791\n",
      "Total number of trainable parameters: 790791\n",
      "Init Loaders\n",
      "153\n",
      "test_error:  0.08496732026143791\n",
      "auc:  0.4582417582417582\n"
     ]
    }
   ],
   "source": [
    "!python eval.py --drop_out --k 5 --models_exp_code tcga_crc_5_s1 \\\n",
    "    --save_exp_code tcga_crc_5_cv \\\n",
    "    --task task_1_tumor_vs_normal --model_type clam_sb --results_dir results \\\n",
    "    --data_root_dir /home/sci/Disk2/tcga_crc/FEATURES_DIRECTORY_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "exp_arguments\n",
      "n_classes : 2\n",
      "save_exp_code : HEATMAP_OUTPUT\n",
      "raw_save_dir : heatmaps/heatmap_raw_results\n",
      "production_save_dir : heatmaps/heatmap_production_results\n",
      "batch_size : 384\n",
      "\n",
      "data_arguments\n",
      "data_dir : heatmaps/demo/slides/\n",
      "data_dir_key : source\n",
      "process_list : myself.csv\n",
      "preset : presets/bwh_brca.csv\n",
      "slide_ext : .svs\n",
      "label_dict : {'tumor': 1, 'normal': 0}\n",
      "\n",
      "patching_arguments\n",
      "patch_size : 256\n",
      "overlap : 0.5\n",
      "patch_level : 0\n",
      "custom_downsample : 1\n",
      "\n",
      "model_arguments\n",
      "ckpt_path : heatmaps/demo/ckpts/s_4_checkpoint.pt\n",
      "model_type : clam_sb\n",
      "initiate_fn : initiate_model\n",
      "model_size : small\n",
      "drop_out : True\n",
      "\n",
      "heatmap_arguments\n",
      "vis_level : 1\n",
      "alpha : 0.4\n",
      "blank_canvas : False\n",
      "save_orig : True\n",
      "save_ext : jpg\n",
      "use_ref_scores : True\n",
      "blur : False\n",
      "use_center_shift : True\n",
      "use_roi : False\n",
      "calc_heatmap : True\n",
      "binarize : False\n",
      "binary_thresh : -1\n",
      "custom_downsample : 1\n",
      "cmap : jet\n",
      "\n",
      "sample_arguments\n",
      "samples : [{'name': 'topk_high_attention', 'sample': True, 'seed': 1, 'k': 15, 'mode': 'topk'}]\n",
      "Continue? Y/N ^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sci/PycharmProjects/chaofan/projects/CLAM/create_heatmaps.py\", line 94, in <module>\n",
      "    decision = input('Continue? Y/N ')\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python create_heatmaps.py --config config_template.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('lcf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dddf4cc44ccb2cf796823d3e6277e76da85a74a2d70ce7f32ed1ef0f62e61e3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
