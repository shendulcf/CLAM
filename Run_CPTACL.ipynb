{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## csv_file process\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df_csv = pd.read_csv('dataset_csv/cptac_lung_subtyping.csv')\n",
    "dataset_dir = '/home/sci/Disk2/CPTAC-LUNG/WSI'\n",
    "index = df_csv.index\n",
    "rows_to_delete = []\n",
    "for name in os.listdir(dataset_dir):\n",
    "    slide_id = os.path.splitext(name)[0]\n",
    "\n",
    "    row_index = df_csv[df_csv['slide_id'] == slide_id].index\n",
    "    rows_to_delete.append(row_index.item())\n",
    "    \n",
    "remaining_index = index.difference(rows_to_delete)\n",
    "df_delete = df_csv.drop(remaining_index).reset_index(drop=True)\n",
    "df_delete\n",
    "# df = df_csv.drop(~rows_to_delete)\n",
    "\n",
    "df_delete.to_csv('dataset_csv/cptac_lung_subtyping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已经删除指定列\n"
     ]
    }
   ],
   "source": [
    "# delete columns\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset_csv/cptac_lung_subtyping.csv')\n",
    "if 'specimen_type' in df.columns:\n",
    "    df.drop('specimen_type', axis=1, inplace=True)\n",
    "\n",
    "df.to_csv('dataset_csv/cptac_lung_subtyping.csv', index=True)\n",
    "\n",
    "print('已经删除指定列')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, 2.0001000528019754, 4.00060037688892, 8.002010593284831, 16.007188719102707, 32.04003318547134)\n",
      "OpenSlide('/home/sci/Disk2/CPTAC-LUNG/WSI/C3L-03642-21.svs')\n",
      "(1.0, 2.0000446548182547, 4.0002679528403, 8.001458840613687, 16.009471545761812, 32.045191308417884)\n",
      "OpenSlide('/home/sci/Disk2/CPTAC-LUNG/WSI/C3L-02513-21.svs')\n",
      "(1.0, 2.0000474383301707, 4.0000948766603415, 8.001227636039935, 16.009647386087043, 32.04810298959235)\n",
      "OpenSlide('/home/sci/Disk2/CPTAC-LUNG/WSI/C3L-02513-22.svs')\n",
      "(1.0, 2.00003452800221, 4.000373461003514, 8.000746922007028, 16.004155409255155, 32.01896771155954)\n",
      "OpenSlide('/home/sci/Disk2/CPTAC-LUNG/WSI/C3L-02515-22.svs')\n",
      "(1.0, 2.0, 4.000173190162799, 8.001280087141657, 16.005332417097122, 32.03672210196119)\n",
      "OpenSlide('/home/sci/Disk2/CPTAC-LUNG/WSI/C3L-02513-23.svs')\n",
      "(1.0, 2.0000335390394417, 4.000226950181202, 8.001093542150732, 16.006894025795802, 32.03263256916757)\n",
      "OpenSlide('/home/sci/Disk2/CPTAC-LUNG/WSI/C3L-02515-21.svs')\n",
      "(1.0, 2.0000776061058305, 4.000465675500808, 8.000931351001617, 16.006832188483017, 32.02634956668237, 64.1035607035607)\n",
      "OpenSlide('/home/sci/Disk2/CPTAC-LUNG/WSI/C3L-02515-23.svs')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WSI 属性查看\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import openslide\n",
    "import os\n",
    "import torch\n",
    "\n",
    "slide_path = r'/home/sci/Disk_data/Datasets/TCGA-NSCLC/WSI/TCGA-05-4244-01A-01-BS1.svs'\n",
    "path = r'/home/sci/Disk2/CPTAC-LUNG/WSI'\n",
    "\n",
    "# # ---->> for single slide\n",
    "# slide = openslide.OpenSlide(slide_path)\n",
    "# slide.dimensions\n",
    "# slide.associated_images\n",
    "# slide.level_dimensions\n",
    "# slide.level_count\n",
    "# slide.properties['aperio.AppMag']\n",
    "# slide.properties['aperio.MPP']\n",
    "\n",
    "# ---->> for slide folder\n",
    "slides = os.listdir(path)\n",
    "dimentions_list = []\n",
    "i = 0 \n",
    "for slide in slides:\n",
    "    slide = os.path.join(path,slide)\n",
    "    slide = openslide.OpenSlide(slide)\n",
    "    # print(slide.level_dimensions)\n",
    "    # print(slide.properties['aperio.AppMag'])\n",
    "    # print(slide.level_count)\n",
    "    # print(slide.properties)\n",
    "    if slide.level_count >4:\n",
    "        print(slide.level_downsamples)\n",
    "        print(slide)\n",
    "i\n",
    "\n",
    "    ## slide thumbnails\n",
    "    # slide = slide.get_thumbnail((slide.level_dimensions[-1]))\n",
    "    # slide.save(r'/home/sci/Disk2/tcga_crc/DATA_DIRECTORY/test')\n",
    "    # print('save_success')\n",
    "    # dimentions_list.append(slide.level_dimensions[-1])\n",
    "# dimentions_list=torch.tensor(dimentions_list)\n",
    "# index = torch.topk(dimentions_list, 1, dim=1)\n",
    "# maxMap = torch.index_select(dimentions_list, index=index, dim=0 )\n",
    "# maxMap\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"coords\": shape (2989, 2), type \"<i4\">\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2989, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "path = r'E:\\Workspace\\Project\\CLAM\\data\\RESULTS_DIRECTORY\\patches\\TCGA-A6-2671-01A-01-BS1.h5'\n",
    "f = h5py.File(path,'r')\n",
    "f.keys()\n",
    "data = f.get('coords')\n",
    "print(data)\n",
    "data.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step_1 get patch\n",
    "\n",
    "source svs 文件地址\\\n",
    "save_dir 结果文件\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python create_patches_fp.py \\\n",
    "--source 'svs_dir' \\\n",
    "--save_dir 'result_dir' \\\n",
    "--patch_size 256 \\\n",
    "--seg \\ # 分割=True\n",
    "--patch \\ # 切分patch=True\n",
    "--stitch \\ # \n",
    "process_list_autogen.csv # 这里记录了每张 image 的处理的参数\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:  /home/sci/Disk2/CPTAC-LUNG/WSI\n",
      "patch_save_dir:  /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "mask_save_dir:  /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/masks\n",
      "stitch_save_dir:  /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/stitches\n",
      "thumbnail_save_dir:  /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/thumbnails\n",
      "source : /home/sci/Disk2/CPTAC-LUNG/WSI\n",
      "save_dir : /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2\n",
      "patch_save_dir : /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "mask_save_dir : /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/masks\n",
      "stitch_save_dir : /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/stitches\n",
      "thumbnail_save_dir : /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/thumbnails\n",
      "{'seg_params': {'seg_level': -1, 'sthresh': 8, 'mthresh': 7, 'close': 4, 'use_otsu': False, 'keep_ids': 'none', 'exclude_ids': 'none'}, 'filter_params': {'a_t': 100, 'a_h': 16, 'max_n_holes': 8}, 'patch_params': {'use_padding': True, 'contour_fn': 'four_pt'}, 'vis_params': {'vis_level': -1, 'line_thickness': 250}}\n",
      "\n",
      "\n",
      "progress: 0.00, 0/1027\n",
      "processing C3L-00001-21.svs\n",
      "Creating patches for:  C3L-00001-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3368 3544 19351 17499\n",
      "Contour Area: 243204952.0\n",
      "Extracted 71 coordinates\n",
      "start stitching C3L-00001-21\n",
      "original size: 25895 x 23643\n",
      "downscaled size for stiching: 3236 x 2955\n",
      "number of patches: 71\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/71 stitched\n",
      "progress: 8/71 stitched\n",
      "progress: 16/71 stitched\n",
      "progress: 24/71 stitched\n",
      "progress: 32/71 stitched\n",
      "progress: 40/71 stitched\n",
      "progress: 48/71 stitched\n",
      "progress: 56/71 stitched\n",
      "progress: 64/71 stitched\n",
      "segmentation took 0.6475393772125244 seconds\n",
      "patching took 0.03014850616455078 seconds\n",
      "stitching took 0.3345913887023926 seconds\n",
      "\n",
      "\n",
      "progress: 0.00, 1/1027\n",
      "processing C3L-00009-21.svs\n",
      "Creating patches for:  C3L-00009-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 2064 2664 15599 16709\n",
      "Contour Area: 141935285.0\n",
      "Extracted 41 coordinates\n",
      "start stitching C3L-00009-21\n",
      "original size: 19919 x 21869\n",
      "downscaled size for stiching: 2489 x 2733\n",
      "number of patches: 41\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/41 stitched\n",
      "progress: 5/41 stitched\n",
      "progress: 10/41 stitched\n",
      "progress: 15/41 stitched\n",
      "progress: 20/41 stitched\n",
      "progress: 25/41 stitched\n",
      "progress: 30/41 stitched\n",
      "progress: 35/41 stitched\n",
      "progress: 40/41 stitched\n",
      "segmentation took 0.25275611877441406 seconds\n",
      "patching took 0.021929502487182617 seconds\n",
      "stitching took 0.17304778099060059 seconds\n",
      "\n",
      "\n",
      "progress: 0.00, 2/1027\n",
      "processing C3L-00080-21.svs\n",
      "Creating patches for:  C3L-00080-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 6001 3409 16758 21817\n",
      "Contour Area: 177846628.0\n",
      "Extracted 14 coordinates\n",
      "start stitching C3L-00080-21\n",
      "original size: 25895 x 31420\n",
      "downscaled size for stiching: 1618 x 1963\n",
      "number of patches: 14\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/14 stitched\n",
      "progress: 2/14 stitched\n",
      "progress: 4/14 stitched\n",
      "progress: 6/14 stitched\n",
      "progress: 8/14 stitched\n",
      "progress: 10/14 stitched\n",
      "progress: 12/14 stitched\n",
      "segmentation took 0.1426231861114502 seconds\n",
      "patching took 0.021519184112548828 seconds\n",
      "stitching took 0.06602621078491211 seconds\n",
      "\n",
      "\n",
      "progress: 0.00, 3/1027\n",
      "processing C3L-00081-21.svs\n",
      "Creating patches for:  C3L-00081-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3648 2128 20823 21125\n",
      "Contour Area: 228335043.0\n",
      "Extracted 19 coordinates\n",
      "start stitching C3L-00081-21\n",
      "original size: 25895 x 24645\n",
      "downscaled size for stiching: 1618 x 1540\n",
      "number of patches: 19\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/19 stitched\n",
      "progress: 2/19 stitched\n",
      "progress: 4/19 stitched\n",
      "progress: 6/19 stitched\n",
      "progress: 8/19 stitched\n",
      "progress: 10/19 stitched\n",
      "progress: 12/19 stitched\n",
      "progress: 14/19 stitched\n",
      "progress: 16/19 stitched\n",
      "progress: 18/19 stitched\n",
      "segmentation took 0.11259198188781738 seconds\n",
      "patching took 0.015716552734375 seconds\n",
      "stitching took 0.07596015930175781 seconds\n",
      "\n",
      "\n",
      "progress: 0.00, 4/1027\n",
      "processing C3L-00083-21.svs\n",
      "Creating patches for:  C3L-00083-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 5906 4288 21579 22834\n",
      "Contour Area: 294955914.0\n",
      "Extracted 23 coordinates\n",
      "start stitching C3L-00083-21\n",
      "original size: 31871 x 31138\n",
      "downscaled size for stiching: 1991 x 1946\n",
      "number of patches: 23\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/23 stitched\n",
      "progress: 3/23 stitched\n",
      "progress: 6/23 stitched\n",
      "progress: 9/23 stitched\n",
      "progress: 12/23 stitched\n",
      "progress: 15/23 stitched\n",
      "progress: 18/23 stitched\n",
      "progress: 21/23 stitched\n",
      "segmentation took 0.17356348037719727 seconds\n",
      "patching took 0.015186548233032227 seconds\n",
      "stitching took 0.09663057327270508 seconds\n",
      "\n",
      "\n",
      "progress: 0.00, 5/1027\n",
      "processing C3L-00093-21.svs\n",
      "Creating patches for:  C3L-00093-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 2856 2824 20087 16041\n",
      "Contour Area: 165708796.0\n",
      "Extracted 48 coordinates\n",
      "start stitching C3L-00093-21\n",
      "original size: 25895 x 20592\n",
      "downscaled size for stiching: 3236 x 2574\n",
      "number of patches: 48\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/48 stitched\n",
      "progress: 5/48 stitched\n",
      "progress: 10/48 stitched\n",
      "progress: 15/48 stitched\n",
      "progress: 20/48 stitched\n",
      "progress: 25/48 stitched\n",
      "progress: 30/48 stitched\n",
      "progress: 35/48 stitched\n",
      "progress: 40/48 stitched\n",
      "progress: 45/48 stitched\n",
      "segmentation took 0.302471399307251 seconds\n",
      "patching took 0.018772602081298828 seconds\n",
      "stitching took 0.20527219772338867 seconds\n",
      "\n",
      "\n",
      "progress: 0.01, 6/1027\n",
      "processing C3L-00094-21.svs\n",
      "Creating patches for:  C3L-00094-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4594 2352 17564 20307\n",
      "Contour Area: 139544298.0\n",
      "Extracted 13 coordinates\n",
      "start stitching C3L-00094-21\n",
      "original size: 23903 x 25971\n",
      "downscaled size for stiching: 1493 x 1623\n",
      "number of patches: 13\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/13 stitched\n",
      "progress: 2/13 stitched\n",
      "progress: 4/13 stitched\n",
      "progress: 6/13 stitched\n",
      "progress: 8/13 stitched\n",
      "progress: 10/13 stitched\n",
      "progress: 12/13 stitched\n",
      "segmentation took 0.10511541366577148 seconds\n",
      "patching took 0.021390676498413086 seconds\n",
      "stitching took 0.06187105178833008 seconds\n",
      "\n",
      "\n",
      "progress: 0.01, 7/1027\n",
      "processing C3L-00095-21.svs\n",
      "Creating patches for:  C3L-00095-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3777 3560 13069 16289\n",
      "Contour Area: 102098984.0\n",
      "Extracted 31 coordinates\n",
      "start stitching C3L-00095-21\n",
      "original size: 19919 x 23441\n",
      "downscaled size for stiching: 2489 x 2930\n",
      "number of patches: 31\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/31 stitched\n",
      "progress: 4/31 stitched\n",
      "progress: 8/31 stitched\n",
      "progress: 12/31 stitched\n",
      "progress: 16/31 stitched\n",
      "progress: 20/31 stitched\n",
      "progress: 24/31 stitched\n",
      "progress: 28/31 stitched\n",
      "segmentation took 0.24553203582763672 seconds\n",
      "patching took 0.020075559616088867 seconds\n",
      "stitching took 0.13573384284973145 seconds\n",
      "\n",
      "\n",
      "progress: 0.01, 8/1027\n",
      "processing C3L-00095-22.svs\n",
      "Creating patches for:  C3L-00095-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3232 5017 20703 14052\n",
      "Contour Area: 144411033.0\n",
      "Extracted 42 coordinates\n",
      "start stitching C3L-00095-22\n",
      "original size: 27887 x 22061\n",
      "downscaled size for stiching: 3485 x 2757\n",
      "number of patches: 42\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/42 stitched\n",
      "progress: 5/42 stitched\n",
      "progress: 10/42 stitched\n",
      "progress: 15/42 stitched\n",
      "progress: 20/42 stitched\n",
      "progress: 25/42 stitched\n",
      "progress: 30/42 stitched\n",
      "progress: 35/42 stitched\n",
      "progress: 40/42 stitched\n",
      "segmentation took 0.3321115970611572 seconds\n",
      "patching took 0.02203655242919922 seconds\n",
      "stitching took 0.18439269065856934 seconds\n",
      "\n",
      "\n",
      "progress: 0.01, 9/1027\n",
      "processing C3L-00095-23.svs\n",
      "Creating patches for:  C3L-00095-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3112 3072 11789 16754\n",
      "Contour Area: 99632018.0\n",
      "Extracted 30 coordinates\n",
      "start stitching C3L-00095-23\n",
      "original size: 23903 x 21922\n",
      "downscaled size for stiching: 2987 x 2740\n",
      "number of patches: 30\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/30 stitched\n",
      "progress: 3/30 stitched\n",
      "progress: 6/30 stitched\n",
      "progress: 9/30 stitched\n",
      "progress: 12/30 stitched\n",
      "progress: 15/30 stitched\n",
      "progress: 18/30 stitched\n",
      "progress: 21/30 stitched\n",
      "progress: 24/30 stitched\n",
      "progress: 27/30 stitched\n",
      "segmentation took 0.27419185638427734 seconds\n",
      "patching took 0.021926403045654297 seconds\n",
      "stitching took 0.1306469440460205 seconds\n",
      "\n",
      "\n",
      "progress: 0.01, 10/1027\n",
      "processing C3L-00140-21.svs\n",
      "Creating patches for:  C3L-00140-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4073 4144 15942 14593\n",
      "Contour Area: 137636052.0\n",
      "Extracted 41 coordinates\n",
      "start stitching C3L-00140-21\n",
      "original size: 21911 x 21360\n",
      "downscaled size for stiching: 2738 x 2670\n",
      "number of patches: 41\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/41 stitched\n",
      "progress: 5/41 stitched\n",
      "progress: 10/41 stitched\n",
      "progress: 15/41 stitched\n",
      "progress: 20/41 stitched\n",
      "progress: 25/41 stitched\n",
      "progress: 30/41 stitched\n",
      "progress: 35/41 stitched\n",
      "progress: 40/41 stitched\n",
      "segmentation took 0.2600092887878418 seconds\n",
      "patching took 0.02239823341369629 seconds\n",
      "stitching took 0.17170333862304688 seconds\n",
      "\n",
      "\n",
      "progress: 0.01, 11/1027\n",
      "processing C3L-00140-22.svs\n",
      "Creating patches for:  C3L-00140-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 2882 3809 14972 19062\n",
      "Contour Area: 148023359.0\n",
      "Extracted 14 coordinates\n",
      "start stitching C3L-00140-22\n",
      "original size: 19919 x 26744\n",
      "downscaled size for stiching: 1244 x 1671\n",
      "number of patches: 14\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4098, 4098)x(4098, 4098)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/14 stitched\n",
      "progress: 2/14 stitched\n",
      "progress: 4/14 stitched\n",
      "progress: 6/14 stitched\n",
      "progress: 8/14 stitched\n",
      "progress: 10/14 stitched\n",
      "progress: 12/14 stitched\n",
      "segmentation took 0.09567904472351074 seconds\n",
      "patching took 0.017320632934570312 seconds\n",
      "stitching took 0.058183908462524414 seconds\n",
      "\n",
      "\n",
      "progress: 0.01, 12/1027\n",
      "processing C3L-00144-21.svs\n",
      "Creating patches for:  C3L-00144-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 2152 2008 15471 11916\n",
      "Contour Area: 94130843.0\n",
      "Extracted 29 coordinates\n",
      "start stitching C3L-00144-21\n",
      "original size: 19919 x 16572\n",
      "downscaled size for stiching: 2489 x 2071\n",
      "number of patches: 29\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/29 stitched\n",
      "progress: 3/29 stitched\n",
      "progress: 6/29 stitched\n",
      "progress: 9/29 stitched\n",
      "progress: 12/29 stitched\n",
      "progress: 15/29 stitched\n",
      "progress: 18/29 stitched\n",
      "progress: 21/29 stitched\n",
      "progress: 24/29 stitched\n",
      "progress: 27/29 stitched\n",
      "segmentation took 0.17776203155517578 seconds\n",
      "patching took 0.018263578414916992 seconds\n",
      "stitching took 0.12624812126159668 seconds\n",
      "\n",
      "\n",
      "progress: 0.01, 13/1027\n",
      "processing C3L-00263-21.svs\n",
      "Creating patches for:  C3L-00263-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 6818 5664 25932 21857\n",
      "Contour Area: 198096288.0\n",
      "Extracted 16 coordinates\n",
      "start stitching C3L-00263-21\n",
      "original size: 35855 x 30785\n",
      "downscaled size for stiching: 2240 x 1924\n",
      "number of patches: 16\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/16 stitched\n",
      "progress: 2/16 stitched\n",
      "progress: 4/16 stitched\n",
      "progress: 6/16 stitched\n",
      "progress: 8/16 stitched\n",
      "progress: 10/16 stitched\n",
      "progress: 12/16 stitched\n",
      "progress: 14/16 stitched\n",
      "segmentation took 0.13966679573059082 seconds\n",
      "patching took 0.02397894859313965 seconds\n",
      "stitching took 0.07262635231018066 seconds\n",
      "\n",
      "\n",
      "progress: 0.01, 14/1027\n",
      "processing C3L-00263-22.svs\n",
      "Creating patches for:  C3L-00263-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 5058 5857 13623 12915\n",
      "Contour Area: 88044639.0\n",
      "Extracted 8 coordinates\n",
      "start stitching C3L-00263-22\n",
      "original size: 31871 x 23028\n",
      "downscaled size for stiching: 1991 x 1439\n",
      "number of patches: 8\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/8 stitched\n",
      "progress: 1/8 stitched\n",
      "progress: 2/8 stitched\n",
      "progress: 3/8 stitched\n",
      "progress: 4/8 stitched\n",
      "progress: 5/8 stitched\n",
      "progress: 6/8 stitched\n",
      "progress: 7/8 stitched\n",
      "segmentation took 0.11998367309570312 seconds\n",
      "patching took 0.0178220272064209 seconds\n",
      "stitching took 0.03916501998901367 seconds\n",
      "\n",
      "\n",
      "progress: 0.01, 15/1027\n",
      "processing C3L-00263-23.svs\n",
      "Creating patches for:  C3L-00263-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 6241 5714 19958 21993\n",
      "Contour Area: 240312867.0\n",
      "Extracted 20 coordinates\n",
      "start stitching C3L-00263-23\n",
      "original size: 29879 x 32892\n",
      "downscaled size for stiching: 1867 x 2055\n",
      "number of patches: 20\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/20 stitched\n",
      "progress: 2/20 stitched\n",
      "progress: 4/20 stitched\n",
      "progress: 6/20 stitched\n",
      "progress: 8/20 stitched\n",
      "progress: 10/20 stitched\n",
      "progress: 12/20 stitched\n",
      "progress: 14/20 stitched\n",
      "progress: 16/20 stitched\n",
      "progress: 18/20 stitched\n",
      "segmentation took 0.17012524604797363 seconds\n",
      "patching took 0.016985177993774414 seconds\n",
      "stitching took 0.08292317390441895 seconds\n",
      "\n",
      "\n",
      "progress: 0.02, 16/1027\n",
      "processing C3L-00263-24.svs\n",
      "Creating patches for:  C3L-00263-24 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4640 4001 25895 12614\n",
      "Contour Area: 197610239.0\n",
      "Extracted 16 coordinates\n",
      "start stitching C3L-00263-24\n",
      "original size: 33863 x 21032\n",
      "downscaled size for stiching: 2116 x 1314\n",
      "number of patches: 16\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/16 stitched\n",
      "progress: 2/16 stitched\n",
      "progress: 4/16 stitched\n",
      "progress: 6/16 stitched\n",
      "progress: 8/16 stitched\n",
      "progress: 10/16 stitched\n",
      "progress: 12/16 stitched\n",
      "progress: 14/16 stitched\n",
      "segmentation took 0.1231236457824707 seconds\n",
      "patching took 0.017810821533203125 seconds\n",
      "stitching took 0.06479859352111816 seconds\n",
      "\n",
      "\n",
      "progress: 0.02, 17/1027\n",
      "processing C3L-00263-25.svs\n",
      "Creating patches for:  C3L-00263-25 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4626 4704 20188 22466\n",
      "Contour Area: 122293155.0\n",
      "Extracted 14 coordinates\n",
      "start stitching C3L-00263-25\n",
      "original size: 27887 x 30418\n",
      "downscaled size for stiching: 1742 x 1901\n",
      "number of patches: 14\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/14 stitched\n",
      "progress: 2/14 stitched\n",
      "progress: 4/14 stitched\n",
      "progress: 6/14 stitched\n",
      "progress: 8/14 stitched\n",
      "progress: 10/14 stitched\n",
      "progress: 12/14 stitched\n",
      "segmentation took 0.1437394618988037 seconds\n",
      "patching took 0.017424821853637695 seconds\n",
      "stitching took 0.06100010871887207 seconds\n",
      "\n",
      "\n",
      "progress: 0.02, 18/1027\n",
      "processing C3L-00279-21.svs\n",
      "Creating patches for:  C3L-00279-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3305 2728 16894 17613\n",
      "Contour Area: 176378874.0\n",
      "Extracted 55 coordinates\n",
      "start stitching C3L-00279-21\n",
      "original size: 21911 x 23173\n",
      "downscaled size for stiching: 2738 x 2896\n",
      "number of patches: 55\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/55 stitched\n",
      "progress: 6/55 stitched\n",
      "progress: 12/55 stitched\n",
      "progress: 18/55 stitched\n",
      "progress: 24/55 stitched\n",
      "progress: 30/55 stitched\n",
      "progress: 36/55 stitched\n",
      "progress: 42/55 stitched\n",
      "progress: 48/55 stitched\n",
      "progress: 54/55 stitched\n",
      "segmentation took 0.4565610885620117 seconds\n",
      "patching took 0.022380352020263672 seconds\n",
      "stitching took 0.22423648834228516 seconds\n",
      "\n",
      "\n",
      "progress: 0.02, 19/1027\n",
      "processing C3L-00279-22.svs\n",
      "Creating patches for:  C3L-00279-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3961 5473 15558 13965\n",
      "Contour Area: 154496517.0\n",
      "Extracted 46 coordinates\n",
      "start stitching C3L-00279-22\n",
      "original size: 21911 x 22510\n",
      "downscaled size for stiching: 2738 x 2813\n",
      "number of patches: 46\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/46 stitched\n",
      "progress: 5/46 stitched\n",
      "progress: 10/46 stitched\n",
      "progress: 15/46 stitched\n",
      "progress: 20/46 stitched\n",
      "progress: 25/46 stitched\n",
      "progress: 30/46 stitched\n",
      "progress: 35/46 stitched\n",
      "progress: 40/46 stitched\n",
      "progress: 45/46 stitched\n",
      "segmentation took 0.5016148090362549 seconds\n",
      "patching took 0.023178815841674805 seconds\n",
      "stitching took 0.19114351272583008 seconds\n",
      "\n",
      "\n",
      "progress: 0.02, 20/1027\n",
      "processing C3L-00279-23.svs\n",
      "Creating patches for:  C3L-00279-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3793 3936 15118 14481\n",
      "Contour Area: 148519420.0\n",
      "Extracted 43 coordinates\n",
      "start stitching C3L-00279-23\n",
      "original size: 21911 x 20944\n",
      "downscaled size for stiching: 2738 x 2618\n",
      "number of patches: 43\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/43 stitched\n",
      "progress: 5/43 stitched\n",
      "progress: 10/43 stitched\n",
      "progress: 15/43 stitched\n",
      "progress: 20/43 stitched\n",
      "progress: 25/43 stitched\n",
      "progress: 30/43 stitched\n",
      "progress: 35/43 stitched\n",
      "progress: 40/43 stitched\n",
      "segmentation took 0.41413092613220215 seconds\n",
      "patching took 0.022522926330566406 seconds\n",
      "stitching took 0.183213472366333 seconds\n",
      "\n",
      "\n",
      "progress: 0.02, 21/1027\n",
      "processing C3L-00368-21.svs\n",
      "Creating patches for:  C3L-00368-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 2896 3201 15606 22093\n",
      "Contour Area: 188973689.0\n",
      "Extracted 16 coordinates\n",
      "start stitching C3L-00368-21\n",
      "original size: 21911 x 27311\n",
      "downscaled size for stiching: 1369 x 1706\n",
      "number of patches: 16\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/16 stitched\n",
      "progress: 2/16 stitched\n",
      "progress: 4/16 stitched\n",
      "progress: 6/16 stitched\n",
      "progress: 8/16 stitched\n",
      "progress: 10/16 stitched\n",
      "progress: 12/16 stitched\n",
      "progress: 14/16 stitched\n",
      "segmentation took 0.10338354110717773 seconds\n",
      "patching took 0.01973867416381836 seconds\n",
      "stitching took 0.0659794807434082 seconds\n",
      "\n",
      "\n",
      "progress: 0.02, 22/1027\n",
      "processing C3L-00368-22.svs\n",
      "Creating patches for:  C3L-00368-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4658 5907 23900 14297\n",
      "Contour Area: 206402856.0\n",
      "Extracted 17 coordinates\n",
      "start stitching C3L-00368-22\n",
      "original size: 31871 x 23614\n",
      "downscaled size for stiching: 1991 x 1475\n",
      "number of patches: 17\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4098, 4098)x(4098, 4098)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/17 stitched\n",
      "progress: 2/17 stitched\n",
      "progress: 4/17 stitched\n",
      "progress: 6/17 stitched\n",
      "progress: 8/17 stitched\n",
      "progress: 10/17 stitched\n",
      "progress: 12/17 stitched\n",
      "progress: 14/17 stitched\n",
      "progress: 16/17 stitched\n",
      "segmentation took 0.13582587242126465 seconds\n",
      "patching took 0.018119096755981445 seconds\n",
      "stitching took 0.07189488410949707 seconds\n",
      "\n",
      "\n",
      "progress: 0.02, 23/1027\n",
      "processing C3L-00412-21.svs\n",
      "Creating patches for:  C3L-00412-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3888 3808 24390 26851\n",
      "Contour Area: 390280663.0\n",
      "Extracted 32 coordinates\n",
      "start stitching C3L-00412-21\n",
      "original size: 33863 x 32867\n",
      "downscaled size for stiching: 2116 x 2054\n",
      "number of patches: 32\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/32 stitched\n",
      "progress: 4/32 stitched\n",
      "progress: 8/32 stitched\n",
      "progress: 12/32 stitched\n",
      "progress: 16/32 stitched\n",
      "progress: 20/32 stitched\n",
      "progress: 24/32 stitched\n",
      "progress: 28/32 stitched\n",
      "segmentation took 0.16008591651916504 seconds\n",
      "patching took 0.018524646759033203 seconds\n",
      "stitching took 0.12952494621276855 seconds\n",
      "\n",
      "\n",
      "progress: 0.02, 24/1027\n",
      "processing C3L-00415-21.svs\n",
      "Creating patches for:  C3L-00415-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 10628 5041 21514 17142\n",
      "Contour Area: 233030072.0\n",
      "Extracted 17 coordinates\n",
      "start stitching C3L-00415-21\n",
      "original size: 35855 x 25576\n",
      "downscaled size for stiching: 2240 x 1598\n",
      "number of patches: 17\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/17 stitched\n",
      "progress: 2/17 stitched\n",
      "progress: 4/17 stitched\n",
      "progress: 6/17 stitched\n",
      "progress: 8/17 stitched\n",
      "progress: 10/17 stitched\n",
      "progress: 12/17 stitched\n",
      "progress: 14/17 stitched\n",
      "progress: 16/17 stitched\n",
      "segmentation took 0.1626746654510498 seconds\n",
      "patching took 0.01876211166381836 seconds\n",
      "stitching took 0.07488179206848145 seconds\n",
      "\n",
      "\n",
      "progress: 0.02, 25/1027\n",
      "processing C3L-00415-23.svs\n",
      "Creating patches for:  C3L-00415-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 5185 5281 29132 28875\n",
      "Contour Area: 441557899.0\n",
      "Extracted 32 coordinates\n",
      "start stitching C3L-00415-23\n",
      "original size: 39839 x 38205\n",
      "downscaled size for stiching: 2489 x 2387\n",
      "number of patches: 32\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/32 stitched\n",
      "progress: 4/32 stitched\n",
      "progress: 8/32 stitched\n",
      "progress: 12/32 stitched\n",
      "progress: 16/32 stitched\n",
      "progress: 20/32 stitched\n",
      "progress: 24/32 stitched\n",
      "progress: 28/32 stitched\n",
      "segmentation took 0.20541787147521973 seconds\n",
      "patching took 0.019397974014282227 seconds\n",
      "stitching took 0.13114261627197266 seconds\n",
      "\n",
      "\n",
      "progress: 0.03, 26/1027\n",
      "processing C3L-00422-21.svs\n",
      "Creating patches for:  C3L-00422-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4505 3736 18222 10770\n",
      "Contour Area: 128689556.0\n",
      "Extracted 40 coordinates\n",
      "start stitching C3L-00422-21\n",
      "original size: 25895 x 17530\n",
      "downscaled size for stiching: 3236 x 2191\n",
      "number of patches: 40\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/40 stitched\n",
      "progress: 4/40 stitched\n",
      "progress: 8/40 stitched\n",
      "progress: 12/40 stitched\n",
      "progress: 16/40 stitched\n",
      "progress: 20/40 stitched\n",
      "progress: 24/40 stitched\n",
      "progress: 28/40 stitched\n",
      "progress: 32/40 stitched\n",
      "progress: 36/40 stitched\n",
      "segmentation took 0.24874186515808105 seconds\n",
      "patching took 0.017719268798828125 seconds\n",
      "stitching took 0.1713113784790039 seconds\n",
      "\n",
      "\n",
      "progress: 0.03, 27/1027\n",
      "processing C3L-00422-22.svs\n",
      "Creating patches for:  C3L-00422-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4081 3048 16590 10361\n",
      "Contour Area: 86984252.0\n",
      "Extracted 30 coordinates\n",
      "start stitching C3L-00422-22\n",
      "original size: 21911 x 15312\n",
      "downscaled size for stiching: 2738 x 1914\n",
      "number of patches: 30\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/30 stitched\n",
      "progress: 3/30 stitched\n",
      "progress: 6/30 stitched\n",
      "progress: 9/30 stitched\n",
      "progress: 12/30 stitched\n",
      "progress: 15/30 stitched\n",
      "progress: 18/30 stitched\n",
      "progress: 21/30 stitched\n",
      "progress: 24/30 stitched\n",
      "progress: 27/30 stitched\n",
      "segmentation took 0.1731107234954834 seconds\n",
      "patching took 0.02031421661376953 seconds\n",
      "stitching took 0.12468242645263672 seconds\n",
      "\n",
      "\n",
      "progress: 0.03, 28/1027\n",
      "processing C3L-00422-23.svs\n",
      "Creating patches for:  C3L-00422-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 11906 6688 13588 12417\n",
      "Contour Area: 61524056.0\n",
      "Extracted 8 coordinates\n",
      "start stitching C3L-00422-23\n",
      "original size: 33863 x 29392\n",
      "downscaled size for stiching: 2116 x 1837\n",
      "number of patches: 8\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/8 stitched\n",
      "progress: 1/8 stitched\n",
      "progress: 2/8 stitched\n",
      "progress: 3/8 stitched\n",
      "progress: 4/8 stitched\n",
      "progress: 5/8 stitched\n",
      "progress: 6/8 stitched\n",
      "progress: 7/8 stitched\n",
      "segmentation took 0.17059612274169922 seconds\n",
      "patching took 0.018346309661865234 seconds\n",
      "stitching took 0.042371511459350586 seconds\n",
      "\n",
      "\n",
      "progress: 0.03, 29/1027\n",
      "processing C3L-00444-21.svs\n",
      "Creating patches for:  C3L-00444-21 ...\n",
      "Total number of contours to process:  3\n",
      "Bounding Box: 12546 11143 30696 32439\n",
      "Contour Area: 648437107.0\n",
      "Extracted 47 coordinates\n",
      "Bounding Box: 61197 7557 31496 31157\n",
      "Contour Area: 649223426.0\n",
      "Extracted 48 coordinates\n",
      "Bounding Box: 90356 96 11203 46303\n",
      "Contour Area: 35571521.0\n",
      "Extracted 5 coordinates\n",
      "start stitching C3L-00444-21\n",
      "original size: 101591 x 46431\n",
      "downscaled size for stiching: 3174 x 1450\n",
      "number of patches: 100\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/100 stitched\n",
      "progress: 10/100 stitched\n",
      "progress: 20/100 stitched\n",
      "progress: 30/100 stitched\n",
      "progress: 40/100 stitched\n",
      "progress: 50/100 stitched\n",
      "progress: 60/100 stitched\n",
      "progress: 70/100 stitched\n",
      "progress: 80/100 stitched\n",
      "progress: 90/100 stitched\n",
      "segmentation took 0.3244209289550781 seconds\n",
      "patching took 0.05075645446777344 seconds\n",
      "stitching took 0.11191987991333008 seconds\n",
      "\n",
      "\n",
      "progress: 0.03, 30/1027\n",
      "processing C3L-00444-22.svs\n",
      "Creating patches for:  C3L-00444-22 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 52682 7137 21285 38155\n",
      "Contour Area: 485535752.0\n",
      "Extracted 36 coordinates\n",
      "Bounding Box: 0 0 75663 35754\n",
      "Contour Area: 623126074.0\n",
      "Extracted 46 coordinates\n",
      "start stitching C3L-00444-22\n",
      "original size: 75695 x 47084\n",
      "downscaled size for stiching: 2365 x 1471\n",
      "number of patches: 82\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/82 stitched\n",
      "progress: 9/82 stitched\n",
      "progress: 18/82 stitched\n",
      "progress: 27/82 stitched\n",
      "progress: 36/82 stitched\n",
      "progress: 45/82 stitched\n",
      "progress: 54/82 stitched\n",
      "progress: 63/82 stitched\n",
      "progress: 72/82 stitched\n",
      "progress: 81/82 stitched\n",
      "segmentation took 0.16433072090148926 seconds\n",
      "patching took 0.03584599494934082 seconds\n",
      "stitching took 0.09243011474609375 seconds\n",
      "\n",
      "\n",
      "progress: 0.03, 31/1027\n",
      "processing C3L-00444-23.svs\n",
      "Creating patches for:  C3L-00444-23 ...\n",
      "Total number of contours to process:  3\n",
      "Bounding Box: 8803 8161 25738 28295\n",
      "Contour Area: 383299181.0\n",
      "Extracted 31 coordinates\n",
      "Bounding Box: 57492 5281 24329 29959\n",
      "Contour Area: 398794877.0\n",
      "Extracted 31 coordinates\n",
      "Bounding Box: 76987 0 10628 45738\n",
      "Contour Area: 51496672.0\n",
      "Extracted 3 coordinates\n",
      "start stitching C3L-00444-23\n",
      "original size: 87647 x 45770\n",
      "downscaled size for stiching: 2738 x 1430\n",
      "number of patches: 65\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/65 stitched\n",
      "progress: 7/65 stitched\n",
      "progress: 14/65 stitched\n",
      "progress: 21/65 stitched\n",
      "progress: 28/65 stitched\n",
      "progress: 35/65 stitched\n",
      "progress: 42/65 stitched\n",
      "progress: 49/65 stitched\n",
      "progress: 56/65 stitched\n",
      "progress: 63/65 stitched\n",
      "segmentation took 0.1920771598815918 seconds\n",
      "patching took 0.049547672271728516 seconds\n",
      "stitching took 0.07448887825012207 seconds\n",
      "\n",
      "\n",
      "progress: 0.03, 32/1027\n",
      "processing C3L-00445-21.svs\n",
      "Creating patches for:  C3L-00445-21 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 0 8230 49806 28342\n",
      "Contour Area: 974405405.0\n",
      "Extracted 64 coordinates\n",
      "Bounding Box: 65042 128 46477 36444\n",
      "Contour Area: 961632448.0\n",
      "Extracted 70 coordinates\n",
      "start stitching C3L-00445-21\n",
      "original size: 111551 x 36604\n",
      "downscaled size for stiching: 3485 x 1143\n",
      "number of patches: 134\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/134 stitched\n",
      "progress: 14/134 stitched\n",
      "progress: 28/134 stitched\n",
      "progress: 42/134 stitched\n",
      "progress: 56/134 stitched\n",
      "progress: 70/134 stitched\n",
      "progress: 84/134 stitched\n",
      "progress: 98/134 stitched\n",
      "progress: 112/134 stitched\n",
      "progress: 126/134 stitched\n",
      "segmentation took 0.19270920753479004 seconds\n",
      "patching took 0.03692793846130371 seconds\n",
      "stitching took 0.13602972030639648 seconds\n",
      "\n",
      "\n",
      "progress: 0.03, 33/1027\n",
      "processing C3L-00445-25.svs\n",
      "Creating patches for:  C3L-00445-25 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 2624 8673 39555 25670\n",
      "Contour Area: 817767498.0\n",
      "Extracted 59 coordinates\n",
      "Bounding Box: 59843 4480 40708 26054\n",
      "Contour Area: 845510218.0\n",
      "Extracted 60 coordinates\n",
      "start stitching C3L-00445-25\n",
      "original size: 105575 x 37479\n",
      "downscaled size for stiching: 3299 x 1171\n",
      "number of patches: 119\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/119 stitched\n",
      "progress: 12/119 stitched\n",
      "progress: 24/119 stitched\n",
      "progress: 36/119 stitched\n",
      "progress: 48/119 stitched\n",
      "progress: 60/119 stitched\n",
      "progress: 72/119 stitched\n",
      "progress: 84/119 stitched\n",
      "progress: 96/119 stitched\n",
      "progress: 108/119 stitched\n",
      "segmentation took 0.17682456970214844 seconds\n",
      "patching took 0.03464388847351074 seconds\n",
      "stitching took 0.12170696258544922 seconds\n",
      "\n",
      "\n",
      "progress: 0.03, 34/1027\n",
      "processing C3L-00445-26.svs\n",
      "Creating patches for:  C3L-00445-26 ...\n",
      "Total number of contours to process:  3\n",
      "Bounding Box: 2848 6692 45707 28756\n",
      "Contour Area: 1083033979.0\n",
      "Extracted 74 coordinates\n",
      "Bounding Box: 59180 4258 47531 28693\n",
      "Contour Area: 1104686128.0\n",
      "Extracted 78 coordinates\n",
      "Bounding Box: 0 0 11203 41468\n",
      "Contour Area: 35430304.0\n",
      "Extracted 0 coordinates\n",
      "start stitching C3L-00445-26\n",
      "original size: 109559 x 41500\n",
      "downscaled size for stiching: 3423 x 1296\n",
      "number of patches: 152\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/152 stitched\n",
      "progress: 16/152 stitched\n",
      "progress: 32/152 stitched\n",
      "progress: 48/152 stitched\n",
      "progress: 64/152 stitched\n",
      "progress: 80/152 stitched\n",
      "progress: 96/152 stitched\n",
      "progress: 112/152 stitched\n",
      "progress: 128/152 stitched\n",
      "progress: 144/152 stitched\n",
      "segmentation took 0.17804265022277832 seconds\n",
      "patching took 0.05149340629577637 seconds\n",
      "stitching took 0.15383172035217285 seconds\n",
      "\n",
      "\n",
      "progress: 0.03, 35/1027\n",
      "processing C3L-00446-21.svs\n",
      "Creating patches for:  C3L-00446-21 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 60136 6273 44199 26053\n",
      "Contour Area: 860186245.0\n",
      "Extracted 62 coordinates\n",
      "Bounding Box: 3584 3680 42247 24581\n",
      "Contour Area: 840203285.0\n",
      "Extracted 57 coordinates\n",
      "start stitching C3L-00446-21\n",
      "original size: 107567 x 35718\n",
      "downscaled size for stiching: 3361 x 1116\n",
      "number of patches: 119\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/119 stitched\n",
      "progress: 12/119 stitched\n",
      "progress: 24/119 stitched\n",
      "progress: 36/119 stitched\n",
      "progress: 48/119 stitched\n",
      "progress: 60/119 stitched\n",
      "progress: 72/119 stitched\n",
      "progress: 84/119 stitched\n",
      "progress: 96/119 stitched\n",
      "progress: 108/119 stitched\n",
      "segmentation took 0.1825270652770996 seconds\n",
      "patching took 0.03565502166748047 seconds\n",
      "stitching took 0.1209862232208252 seconds\n",
      "\n",
      "\n",
      "progress: 0.04, 36/1027\n",
      "processing C3L-00446-22.svs\n",
      "Creating patches for:  C3L-00446-22 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 5441 7172 32045 18831\n",
      "Contour Area: 355112713.0\n",
      "Extracted 31 coordinates\n",
      "Bounding Box: 52690 3618 32557 22064\n",
      "Contour Area: 359104102.0\n",
      "Extracted 27 coordinates\n",
      "start stitching C3L-00446-22\n",
      "original size: 87647 x 27411\n",
      "downscaled size for stiching: 2738 x 856\n",
      "number of patches: 58\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/58 stitched\n",
      "progress: 6/58 stitched\n",
      "progress: 12/58 stitched\n",
      "progress: 18/58 stitched\n",
      "progress: 24/58 stitched\n",
      "progress: 30/58 stitched\n",
      "progress: 36/58 stitched\n",
      "progress: 42/58 stitched\n",
      "progress: 48/58 stitched\n",
      "progress: 54/58 stitched\n",
      "segmentation took 0.11052298545837402 seconds\n",
      "patching took 0.032643795013427734 seconds\n",
      "stitching took 0.06373834609985352 seconds\n",
      "\n",
      "\n",
      "progress: 0.04, 37/1027\n",
      "processing C3L-00446-23.svs\n",
      "Creating patches for:  C3L-00446-23 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 53614 5603 29385 21903\n",
      "Contour Area: 450807869.0\n",
      "Extracted 32 coordinates\n",
      "Bounding Box: 3488 3970 29929 22800\n",
      "Contour Area: 464824674.0\n",
      "Extracted 36 coordinates\n",
      "start stitching C3L-00446-23\n",
      "original size: 85655 x 30900\n",
      "downscaled size for stiching: 2676 x 965\n",
      "number of patches: 68\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/68 stitched\n",
      "progress: 7/68 stitched\n",
      "progress: 14/68 stitched\n",
      "progress: 21/68 stitched\n",
      "progress: 28/68 stitched\n",
      "progress: 35/68 stitched\n",
      "progress: 42/68 stitched\n",
      "progress: 49/68 stitched\n",
      "progress: 56/68 stitched\n",
      "progress: 63/68 stitched\n",
      "segmentation took 0.12198805809020996 seconds\n",
      "patching took 0.0331110954284668 seconds\n",
      "stitching took 0.07343554496765137 seconds\n",
      "\n",
      "\n",
      "progress: 0.04, 38/1027\n",
      "processing C3L-00446-24.svs\n",
      "Creating patches for:  C3L-00446-24 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 5408 4480 25510 28801\n",
      "Contour Area: 555495792.0\n",
      "Extracted 38 coordinates\n",
      "Bounding Box: 54185 4448 25414 29281\n",
      "Contour Area: 556031888.0\n",
      "Extracted 38 coordinates\n",
      "start stitching C3L-00446-24\n",
      "original size: 83663 x 36864\n",
      "downscaled size for stiching: 2614 x 1152\n",
      "number of patches: 76\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/76 stitched\n",
      "progress: 8/76 stitched\n",
      "progress: 16/76 stitched\n",
      "progress: 24/76 stitched\n",
      "progress: 32/76 stitched\n",
      "progress: 40/76 stitched\n",
      "progress: 48/76 stitched\n",
      "progress: 56/76 stitched\n",
      "progress: 64/76 stitched\n",
      "progress: 72/76 stitched\n",
      "segmentation took 0.14055347442626953 seconds\n",
      "patching took 0.03339529037475586 seconds\n",
      "stitching took 0.08171486854553223 seconds\n",
      "\n",
      "\n",
      "progress: 0.04, 39/1027\n",
      "processing C3L-00503-21.svs\n",
      "Creating patches for:  C3L-00503-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 8850 6740 13493 11672\n",
      "Contour Area: 106519768.0\n",
      "Extracted 9 coordinates\n",
      "start stitching C3L-00503-21\n",
      "original size: 25895 x 24895\n",
      "downscaled size for stiching: 1618 x 1555\n",
      "number of patches: 9\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/9 stitched\n",
      "progress: 1/9 stitched\n",
      "progress: 2/9 stitched\n",
      "progress: 3/9 stitched\n",
      "progress: 4/9 stitched\n",
      "progress: 5/9 stitched\n",
      "progress: 6/9 stitched\n",
      "progress: 7/9 stitched\n",
      "progress: 8/9 stitched\n",
      "segmentation took 0.1114661693572998 seconds\n",
      "patching took 0.01711583137512207 seconds\n",
      "stitching took 0.03977560997009277 seconds\n",
      "\n",
      "\n",
      "progress: 0.04, 40/1027\n",
      "processing C3L-00503-22.svs\n",
      "Creating patches for:  C3L-00503-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4321 4768 14934 13371\n",
      "Contour Area: 110632702.0\n",
      "Extracted 33 coordinates\n",
      "start stitching C3L-00503-22\n",
      "original size: 21911 x 20523\n",
      "downscaled size for stiching: 2738 x 2565\n",
      "number of patches: 33\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/33 stitched\n",
      "progress: 4/33 stitched\n",
      "progress: 8/33 stitched\n",
      "progress: 12/33 stitched\n",
      "progress: 16/33 stitched\n",
      "progress: 20/33 stitched\n",
      "progress: 24/33 stitched\n",
      "progress: 28/33 stitched\n",
      "progress: 32/33 stitched\n",
      "segmentation took 0.2370154857635498 seconds\n",
      "patching took 0.01950383186340332 seconds\n",
      "stitching took 0.14223432540893555 seconds\n",
      "\n",
      "\n",
      "progress: 0.04, 41/1027\n",
      "processing C3L-00503-23.svs\n",
      "Creating patches for:  C3L-00503-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3520 4656 22135 20068\n",
      "Contour Area: 247247247.0\n",
      "Extracted 21 coordinates\n",
      "start stitching C3L-00503-23\n",
      "original size: 29879 x 28004\n",
      "downscaled size for stiching: 1867 x 1750\n",
      "number of patches: 21\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/21 stitched\n",
      "progress: 3/21 stitched\n",
      "progress: 6/21 stitched\n",
      "progress: 9/21 stitched\n",
      "progress: 12/21 stitched\n",
      "progress: 15/21 stitched\n",
      "progress: 18/21 stitched\n",
      "segmentation took 0.14906597137451172 seconds\n",
      "patching took 0.01958179473876953 seconds\n",
      "stitching took 0.08906698226928711 seconds\n",
      "\n",
      "\n",
      "progress: 0.04, 42/1027\n",
      "processing C3L-00510-21.svs\n",
      "Creating patches for:  C3L-00510-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 2673 2640 25965 20233\n",
      "Contour Area: 241867582.5\n",
      "Extracted 19 coordinates\n",
      "start stitching C3L-00510-21\n",
      "original size: 31871 x 25353\n",
      "downscaled size for stiching: 1991 x 1584\n",
      "number of patches: 19\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/19 stitched\n",
      "progress: 2/19 stitched\n",
      "progress: 4/19 stitched\n",
      "progress: 6/19 stitched\n",
      "progress: 8/19 stitched\n",
      "progress: 10/19 stitched\n",
      "progress: 12/19 stitched\n",
      "progress: 14/19 stitched\n",
      "progress: 16/19 stitched\n",
      "progress: 18/19 stitched\n",
      "segmentation took 0.14298033714294434 seconds\n",
      "patching took 0.01929450035095215 seconds\n",
      "stitching took 0.07990336418151855 seconds\n",
      "\n",
      "\n",
      "progress: 0.04, 43/1027\n",
      "processing C3L-00510-22.svs\n",
      "Creating patches for:  C3L-00510-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 8722 6449 15476 15461\n",
      "Contour Area: 174122337.0\n",
      "Extracted 14 coordinates\n",
      "start stitching C3L-00510-22\n",
      "original size: 29879 x 29911\n",
      "downscaled size for stiching: 1867 x 1869\n",
      "number of patches: 14\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/14 stitched\n",
      "progress: 2/14 stitched\n",
      "progress: 4/14 stitched\n",
      "progress: 6/14 stitched\n",
      "progress: 8/14 stitched\n",
      "progress: 10/14 stitched\n",
      "progress: 12/14 stitched\n",
      "segmentation took 0.15633082389831543 seconds\n",
      "patching took 0.01891613006591797 seconds\n",
      "stitching took 0.0614931583404541 seconds\n",
      "\n",
      "\n",
      "progress: 0.04, 44/1027\n",
      "processing C3L-00510-23.svs\n",
      "Creating patches for:  C3L-00510-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3336 3960 18103 14249\n",
      "Contour Area: 175110556.0\n",
      "Extracted 51 coordinates\n",
      "start stitching C3L-00510-23\n",
      "original size: 23903 x 22337\n",
      "downscaled size for stiching: 2987 x 2792\n",
      "number of patches: 51\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/51 stitched\n",
      "progress: 6/51 stitched\n",
      "progress: 12/51 stitched\n",
      "progress: 18/51 stitched\n",
      "progress: 24/51 stitched\n",
      "progress: 30/51 stitched\n",
      "progress: 36/51 stitched\n",
      "progress: 42/51 stitched\n",
      "progress: 48/51 stitched\n",
      "segmentation took 0.46851253509521484 seconds\n",
      "patching took 0.021802425384521484 seconds\n",
      "stitching took 0.21260833740234375 seconds\n",
      "\n",
      "\n",
      "progress: 0.04, 45/1027\n",
      "processing C3L-00568-21.svs\n",
      "Creating patches for:  C3L-00568-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3330 4961 15466 16372\n",
      "Contour Area: 121520680.0\n",
      "Extracted 12 coordinates\n",
      "start stitching C3L-00568-21\n",
      "original size: 23903 x 23829\n",
      "downscaled size for stiching: 1493 x 1489\n",
      "number of patches: 12\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/12 stitched\n",
      "progress: 2/12 stitched\n",
      "progress: 4/12 stitched\n",
      "progress: 6/12 stitched\n",
      "progress: 8/12 stitched\n",
      "progress: 10/12 stitched\n",
      "segmentation took 0.09553861618041992 seconds\n",
      "patching took 0.021100282669067383 seconds\n",
      "stitching took 0.05196261405944824 seconds\n",
      "\n",
      "\n",
      "progress: 0.04, 46/1027\n",
      "processing C3L-00568-22.svs\n",
      "Creating patches for:  C3L-00568-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 2865 3873 11317 13188\n",
      "Contour Area: 96895062.0\n",
      "Extracted 29 coordinates\n",
      "start stitching C3L-00568-22\n",
      "original size: 17927 x 20798\n",
      "downscaled size for stiching: 2240 x 2599\n",
      "number of patches: 29\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/29 stitched\n",
      "progress: 3/29 stitched\n",
      "progress: 6/29 stitched\n",
      "progress: 9/29 stitched\n",
      "progress: 12/29 stitched\n",
      "progress: 15/29 stitched\n",
      "progress: 18/29 stitched\n",
      "progress: 21/29 stitched\n",
      "progress: 24/29 stitched\n",
      "progress: 27/29 stitched\n",
      "segmentation took 0.2009894847869873 seconds\n",
      "patching took 0.019212007522583008 seconds\n",
      "stitching took 0.1234889030456543 seconds\n",
      "\n",
      "\n",
      "progress: 0.05, 47/1027\n",
      "processing C3L-00568-23.svs\n",
      "Creating patches for:  C3L-00568-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 8474 4505 7916 8004\n",
      "Contour Area: 38103775.0\n",
      "Extracted 14 coordinates\n",
      "start stitching C3L-00568-23\n",
      "original size: 19919 x 18390\n",
      "downscaled size for stiching: 2489 x 2298\n",
      "number of patches: 14\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/14 stitched\n",
      "progress: 2/14 stitched\n",
      "progress: 4/14 stitched\n",
      "progress: 6/14 stitched\n",
      "progress: 8/14 stitched\n",
      "progress: 10/14 stitched\n",
      "progress: 12/14 stitched\n",
      "segmentation took 0.1841897964477539 seconds\n",
      "patching took 0.02000737190246582 seconds\n",
      "stitching took 0.0706777572631836 seconds\n",
      "\n",
      "\n",
      "progress: 0.05, 48/1027\n",
      "processing C3L-00603-21.svs\n",
      "Creating patches for:  C3L-00603-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3657 2792 8133 11517\n",
      "Contour Area: 53897455.0\n",
      "Extracted 18 coordinates\n",
      "start stitching C3L-00603-21\n",
      "original size: 15935 x 17342\n",
      "downscaled size for stiching: 1991 x 2167\n",
      "number of patches: 18\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/18 stitched\n",
      "progress: 2/18 stitched\n",
      "progress: 4/18 stitched\n",
      "progress: 6/18 stitched\n",
      "progress: 8/18 stitched\n",
      "progress: 10/18 stitched\n",
      "progress: 12/18 stitched\n",
      "progress: 14/18 stitched\n",
      "progress: 16/18 stitched\n",
      "segmentation took 0.13950753211975098 seconds\n",
      "patching took 0.0205686092376709 seconds\n",
      "stitching took 0.07739138603210449 seconds\n",
      "\n",
      "\n",
      "progress: 0.05, 49/1027\n",
      "processing C3L-00603-22.svs\n",
      "Creating patches for:  C3L-00603-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 5489 3296 11437 15373\n",
      "Contour Area: 110819990.0\n",
      "Extracted 35 coordinates\n",
      "start stitching C3L-00603-22\n",
      "original size: 19919 x 22109\n",
      "downscaled size for stiching: 2489 x 2763\n",
      "number of patches: 35\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/35 stitched\n",
      "progress: 4/35 stitched\n",
      "progress: 8/35 stitched\n",
      "progress: 12/35 stitched\n",
      "progress: 16/35 stitched\n",
      "progress: 20/35 stitched\n",
      "progress: 24/35 stitched\n",
      "progress: 28/35 stitched\n",
      "progress: 32/35 stitched\n",
      "segmentation took 0.23727083206176758 seconds\n",
      "patching took 0.02169489860534668 seconds\n",
      "stitching took 0.15041017532348633 seconds\n",
      "\n",
      "\n",
      "progress: 0.05, 50/1027\n",
      "processing C3L-00604-21.svs\n",
      "Creating patches for:  C3L-00604-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3873 3904 12293 14643\n",
      "Contour Area: 102925997.0\n",
      "Extracted 32 coordinates\n",
      "start stitching C3L-00604-21\n",
      "original size: 19919 x 21699\n",
      "downscaled size for stiching: 2489 x 2712\n",
      "number of patches: 32\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/32 stitched\n",
      "progress: 4/32 stitched\n",
      "progress: 8/32 stitched\n",
      "progress: 12/32 stitched\n",
      "progress: 16/32 stitched\n",
      "progress: 20/32 stitched\n",
      "progress: 24/32 stitched\n",
      "progress: 28/32 stitched\n",
      "segmentation took 0.23882579803466797 seconds\n",
      "patching took 0.020304441452026367 seconds\n",
      "stitching took 0.13626551628112793 seconds\n",
      "\n",
      "\n",
      "progress: 0.05, 51/1027\n",
      "processing C3L-00604-22.svs\n",
      "Creating patches for:  C3L-00604-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 2849 3232 14294 13706\n",
      "Contour Area: 94669694.0\n",
      "Extracted 31 coordinates\n",
      "start stitching C3L-00604-22\n",
      "original size: 19919 x 19378\n",
      "downscaled size for stiching: 2489 x 2422\n",
      "number of patches: 31\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/31 stitched\n",
      "progress: 4/31 stitched\n",
      "progress: 8/31 stitched\n",
      "progress: 12/31 stitched\n",
      "progress: 16/31 stitched\n",
      "progress: 20/31 stitched\n",
      "progress: 24/31 stitched\n",
      "progress: 28/31 stitched\n",
      "segmentation took 0.20642375946044922 seconds\n",
      "patching took 0.01584768295288086 seconds\n",
      "stitching took 0.13634729385375977 seconds\n",
      "\n",
      "\n",
      "progress: 0.05, 52/1027\n",
      "processing C3L-00604-23.svs\n",
      "Creating patches for:  C3L-00604-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4089 3064 16237 14073\n",
      "Contour Area: 129175356.0\n",
      "Extracted 38 coordinates\n",
      "start stitching C3L-00604-23\n",
      "original size: 23903 x 20025\n",
      "downscaled size for stiching: 2987 x 2503\n",
      "number of patches: 38\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/38 stitched\n",
      "progress: 4/38 stitched\n",
      "progress: 8/38 stitched\n",
      "progress: 12/38 stitched\n",
      "progress: 16/38 stitched\n",
      "progress: 20/38 stitched\n",
      "progress: 24/38 stitched\n",
      "progress: 28/38 stitched\n",
      "progress: 32/38 stitched\n",
      "progress: 36/38 stitched\n",
      "segmentation took 0.2592952251434326 seconds\n",
      "patching took 0.01960134506225586 seconds\n",
      "stitching took 0.1603400707244873 seconds\n",
      "\n",
      "\n",
      "progress: 0.05, 53/1027\n",
      "processing C3L-00893-21.svs\n",
      "Creating patches for:  C3L-00893-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 2360 3872 13894 12962\n",
      "Contour Area: 71187857.0\n",
      "Extracted 23 coordinates\n",
      "start stitching C3L-00893-21\n",
      "original size: 19919 x 19138\n",
      "downscaled size for stiching: 2489 x 2392\n",
      "number of patches: 23\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/23 stitched\n",
      "progress: 3/23 stitched\n",
      "progress: 6/23 stitched\n",
      "progress: 9/23 stitched\n",
      "progress: 12/23 stitched\n",
      "progress: 15/23 stitched\n",
      "progress: 18/23 stitched\n",
      "progress: 21/23 stitched\n",
      "segmentation took 0.19553709030151367 seconds\n",
      "patching took 0.019864797592163086 seconds\n",
      "stitching took 0.10173845291137695 seconds\n",
      "\n",
      "\n",
      "progress: 0.05, 54/1027\n",
      "processing C3L-00893-22.svs\n",
      "Creating patches for:  C3L-00893-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4881 5577 16981 12749\n",
      "Contour Area: 114209201.0\n",
      "Extracted 34 coordinates\n",
      "start stitching C3L-00893-22\n",
      "original size: 27887 x 22807\n",
      "downscaled size for stiching: 3485 x 2850\n",
      "number of patches: 34\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/34 stitched\n",
      "progress: 4/34 stitched\n",
      "progress: 8/34 stitched\n",
      "progress: 12/34 stitched\n",
      "progress: 16/34 stitched\n",
      "progress: 20/34 stitched\n",
      "progress: 24/34 stitched\n",
      "progress: 28/34 stitched\n",
      "progress: 32/34 stitched\n",
      "segmentation took 0.3358304500579834 seconds\n",
      "patching took 0.02294015884399414 seconds\n",
      "stitching took 0.1543717384338379 seconds\n",
      "\n",
      "\n",
      "progress: 0.05, 55/1027\n",
      "processing C3L-00893-23.svs\n",
      "Creating patches for:  C3L-00893-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3425 4176 14853 17825\n",
      "Contour Area: 98129808.0\n",
      "Extracted 10 coordinates\n",
      "start stitching C3L-00893-23\n",
      "original size: 21911 x 24832\n",
      "downscaled size for stiching: 1369 x 1552\n",
      "number of patches: 10\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/10 stitched\n",
      "progress: 1/10 stitched\n",
      "progress: 2/10 stitched\n",
      "progress: 3/10 stitched\n",
      "progress: 4/10 stitched\n",
      "progress: 5/10 stitched\n",
      "progress: 6/10 stitched\n",
      "progress: 7/10 stitched\n",
      "progress: 8/10 stitched\n",
      "progress: 9/10 stitched\n",
      "segmentation took 0.09462499618530273 seconds\n",
      "patching took 0.021654129028320312 seconds\n",
      "stitching took 0.04145479202270508 seconds\n",
      "\n",
      "\n",
      "progress: 0.05, 56/1027\n",
      "processing C3L-00904-21.svs\n",
      "Creating patches for:  C3L-00904-21 ...\n",
      "Total number of contours to process:  4\n",
      "Bounding Box: 4418 30537 28883 19975\n",
      "Contour Area: 197554775.0\n",
      "Extracted 18 coordinates\n",
      "Bounding Box: 5795 3777 22191 19271\n",
      "Contour Area: 193064292.5\n",
      "Extracted 18 coordinates\n",
      "Bounding Box: 0 1440 37815 3106\n",
      "Contour Area: 41431281.0\n",
      "Extracted 9 coordinates\n",
      "Bounding Box: 0 0 37815 1313\n",
      "Contour Area: 27071648.0\n",
      "Extracted 9 coordinates\n",
      "start stitching C3L-00904-21\n",
      "original size: 37847 x 50544\n",
      "downscaled size for stiching: 1182 x 1579\n",
      "number of patches: 54\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/54 stitched\n",
      "progress: 6/54 stitched\n",
      "progress: 12/54 stitched\n",
      "progress: 18/54 stitched\n",
      "progress: 24/54 stitched\n",
      "progress: 30/54 stitched\n",
      "progress: 36/54 stitched\n",
      "progress: 42/54 stitched\n",
      "progress: 48/54 stitched\n",
      "segmentation took 0.08594179153442383 seconds\n",
      "patching took 0.06189894676208496 seconds\n",
      "stitching took 0.056592702865600586 seconds\n",
      "\n",
      "\n",
      "progress: 0.06, 57/1027\n",
      "processing C3L-00904-22.svs\n",
      "Creating patches for:  C3L-00904-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3584 3888 22247 20903\n",
      "Contour Area: 330302890.0\n",
      "Extracted 24 coordinates\n",
      "start stitching C3L-00904-22\n",
      "original size: 29879 x 28007\n",
      "downscaled size for stiching: 1867 x 1750\n",
      "number of patches: 24\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/24 stitched\n",
      "progress: 3/24 stitched\n",
      "progress: 6/24 stitched\n",
      "progress: 9/24 stitched\n",
      "progress: 12/24 stitched\n",
      "progress: 15/24 stitched\n",
      "progress: 18/24 stitched\n",
      "progress: 21/24 stitched\n",
      "segmentation took 0.14803099632263184 seconds\n",
      "patching took 0.018245697021484375 seconds\n",
      "stitching took 0.0959322452545166 seconds\n",
      "\n",
      "\n",
      "progress: 0.06, 58/1027\n",
      "processing C3L-00913-21.svs\n",
      "Creating patches for:  C3L-00913-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4897 5600 13252 19489\n",
      "Contour Area: 157062072.0\n",
      "Extracted 13 coordinates\n",
      "start stitching C3L-00913-21\n",
      "original size: 25895 x 29520\n",
      "downscaled size for stiching: 1618 x 1845\n",
      "number of patches: 13\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/13 stitched\n",
      "progress: 2/13 stitched\n",
      "progress: 4/13 stitched\n",
      "progress: 6/13 stitched\n",
      "progress: 8/13 stitched\n",
      "progress: 10/13 stitched\n",
      "progress: 12/13 stitched\n",
      "segmentation took 0.1314096450805664 seconds\n",
      "patching took 0.0168609619140625 seconds\n",
      "stitching took 0.055642127990722656 seconds\n",
      "\n",
      "\n",
      "progress: 0.06, 59/1027\n",
      "processing C3L-00913-22.svs\n",
      "Creating patches for:  C3L-00913-22 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 15766 15474 15175 12947\n",
      "Contour Area: 55311884.0\n",
      "Extracted 7 coordinates\n",
      "Bounding Box: 4177 4576 15608 22453\n",
      "Contour Area: 181049847.0\n",
      "Extracted 17 coordinates\n",
      "start stitching C3L-00913-22\n",
      "original size: 35855 x 36902\n",
      "downscaled size for stiching: 2240 x 2306\n",
      "number of patches: 24\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/24 stitched\n",
      "progress: 3/24 stitched\n",
      "progress: 6/24 stitched\n",
      "progress: 9/24 stitched\n",
      "progress: 12/24 stitched\n",
      "progress: 15/24 stitched\n",
      "progress: 18/24 stitched\n",
      "progress: 21/24 stitched\n",
      "segmentation took 0.1624460220336914 seconds\n",
      "patching took 0.040924787521362305 seconds\n",
      "stitching took 0.10896849632263184 seconds\n",
      "\n",
      "\n",
      "progress: 0.06, 60/1027\n",
      "processing C3L-00913-23.svs\n",
      "Creating patches for:  C3L-00913-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 5587 4880 10952 17908\n",
      "Contour Area: 101144509.0\n",
      "Extracted 9 coordinates\n",
      "start stitching C3L-00913-23\n",
      "original size: 23903 x 27716\n",
      "downscaled size for stiching: 1493 x 1732\n",
      "number of patches: 9\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/9 stitched\n",
      "progress: 1/9 stitched\n",
      "progress: 2/9 stitched\n",
      "progress: 3/9 stitched\n",
      "progress: 4/9 stitched\n",
      "progress: 5/9 stitched\n",
      "progress: 6/9 stitched\n",
      "progress: 7/9 stitched\n",
      "progress: 8/9 stitched\n",
      "segmentation took 0.11724686622619629 seconds\n",
      "patching took 0.019071340560913086 seconds\n",
      "stitching took 0.04195356369018555 seconds\n",
      "\n",
      "\n",
      "progress: 0.06, 61/1027\n",
      "processing C3L-00923-23.svs\n",
      "Creating patches for:  C3L-00923-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3473 4656 20076 20116\n",
      "Contour Area: 220317505.0\n",
      "Extracted 18 coordinates\n",
      "start stitching C3L-00923-23\n",
      "original size: 27887 x 27860\n",
      "downscaled size for stiching: 1742 x 1741\n",
      "number of patches: 18\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/18 stitched\n",
      "progress: 2/18 stitched\n",
      "progress: 4/18 stitched\n",
      "progress: 6/18 stitched\n",
      "progress: 8/18 stitched\n",
      "progress: 10/18 stitched\n",
      "progress: 12/18 stitched\n",
      "progress: 14/18 stitched\n",
      "progress: 16/18 stitched\n",
      "segmentation took 0.13390707969665527 seconds\n",
      "patching took 0.01905989646911621 seconds\n",
      "stitching took 0.0760030746459961 seconds\n",
      "\n",
      "\n",
      "progress: 0.06, 62/1027\n",
      "processing C3L-00927-21.svs\n",
      "Creating patches for:  C3L-00927-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4673 2704 12606 11765\n",
      "Contour Area: 112448823.0\n",
      "Extracted 31 coordinates\n",
      "start stitching C3L-00927-21\n",
      "original size: 19919 x 17397\n",
      "downscaled size for stiching: 2489 x 2174\n",
      "number of patches: 31\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/31 stitched\n",
      "progress: 4/31 stitched\n",
      "progress: 8/31 stitched\n",
      "progress: 12/31 stitched\n",
      "progress: 16/31 stitched\n",
      "progress: 20/31 stitched\n",
      "progress: 24/31 stitched\n",
      "progress: 28/31 stitched\n",
      "segmentation took 0.19587349891662598 seconds\n",
      "patching took 0.017693758010864258 seconds\n",
      "stitching took 0.1260056495666504 seconds\n",
      "\n",
      "\n",
      "progress: 0.06, 63/1027\n",
      "processing C3L-00927-22.svs\n",
      "Creating patches for:  C3L-00927-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3369 3993 14398 11477\n",
      "Contour Area: 115405652.0\n",
      "Extracted 34 coordinates\n",
      "start stitching C3L-00927-22\n",
      "original size: 19919 x 18055\n",
      "downscaled size for stiching: 2489 x 2256\n",
      "number of patches: 34\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/34 stitched\n",
      "progress: 4/34 stitched\n",
      "progress: 8/34 stitched\n",
      "progress: 12/34 stitched\n",
      "progress: 16/34 stitched\n",
      "progress: 20/34 stitched\n",
      "progress: 24/34 stitched\n",
      "progress: 28/34 stitched\n",
      "progress: 32/34 stitched\n",
      "segmentation took 0.2017972469329834 seconds\n",
      "patching took 0.017713546752929688 seconds\n",
      "stitching took 0.14133548736572266 seconds\n",
      "\n",
      "\n",
      "progress: 0.06, 64/1027\n",
      "processing C3L-00927-23.svs\n",
      "Creating patches for:  C3L-00927-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4481 3072 13997 15814\n",
      "Contour Area: 105796897.0\n",
      "Extracted 36 coordinates\n",
      "start stitching C3L-00927-23\n",
      "original size: 21911 x 22103\n",
      "downscaled size for stiching: 2738 x 2762\n",
      "number of patches: 36\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/36 stitched\n",
      "progress: 4/36 stitched\n",
      "progress: 8/36 stitched\n",
      "progress: 12/36 stitched\n",
      "progress: 16/36 stitched\n",
      "progress: 20/36 stitched\n",
      "progress: 24/36 stitched\n",
      "progress: 28/36 stitched\n",
      "progress: 32/36 stitched\n",
      "segmentation took 0.2550389766693115 seconds\n",
      "patching took 0.02083277702331543 seconds\n",
      "stitching took 0.1531515121459961 seconds\n",
      "\n",
      "\n",
      "progress: 0.06, 65/1027\n",
      "processing C3L-00965-21.svs\n",
      "Creating patches for:  C3L-00965-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 2800 2976 14303 11468\n",
      "Contour Area: 112785130.0\n",
      "Extracted 33 coordinates\n",
      "start stitching C3L-00965-21\n",
      "original size: 19919 x 16244\n",
      "downscaled size for stiching: 2489 x 2030\n",
      "number of patches: 33\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/33 stitched\n",
      "progress: 4/33 stitched\n",
      "progress: 8/33 stitched\n",
      "progress: 12/33 stitched\n",
      "progress: 16/33 stitched\n",
      "progress: 20/33 stitched\n",
      "progress: 24/33 stitched\n",
      "progress: 28/33 stitched\n",
      "progress: 32/33 stitched\n",
      "segmentation took 0.18700814247131348 seconds\n",
      "patching took 0.01898479461669922 seconds\n",
      "stitching took 0.13387227058410645 seconds\n",
      "\n",
      "\n",
      "progress: 0.06, 66/1027\n",
      "processing C3L-00965-22.svs\n",
      "Creating patches for:  C3L-00965-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3537 3088 11285 11457\n",
      "Contour Area: 99010648.0\n",
      "Extracted 29 coordinates\n",
      "start stitching C3L-00965-22\n",
      "original size: 17927 x 16505\n",
      "downscaled size for stiching: 2240 x 2063\n",
      "number of patches: 29\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/29 stitched\n",
      "progress: 3/29 stitched\n",
      "progress: 6/29 stitched\n",
      "progress: 9/29 stitched\n",
      "progress: 12/29 stitched\n",
      "progress: 15/29 stitched\n",
      "progress: 18/29 stitched\n",
      "progress: 21/29 stitched\n",
      "progress: 24/29 stitched\n",
      "progress: 27/29 stitched\n",
      "segmentation took 0.1623377799987793 seconds\n",
      "patching took 0.018858671188354492 seconds\n",
      "stitching took 0.12048220634460449 seconds\n",
      "\n",
      "\n",
      "progress: 0.07, 67/1027\n",
      "processing C3L-00965-23.svs\n",
      "Creating patches for:  C3L-00965-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 2665 2768 11854 10852\n",
      "Contour Area: 78289356.0\n",
      "Extracted 24 coordinates\n",
      "start stitching C3L-00965-23\n",
      "original size: 15935 x 16012\n",
      "downscaled size for stiching: 1991 x 2001\n",
      "number of patches: 24\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/24 stitched\n",
      "progress: 3/24 stitched\n",
      "progress: 6/24 stitched\n",
      "progress: 9/24 stitched\n",
      "progress: 12/24 stitched\n",
      "progress: 15/24 stitched\n",
      "progress: 18/24 stitched\n",
      "progress: 21/24 stitched\n",
      "segmentation took 0.18129611015319824 seconds\n",
      "patching took 0.017066240310668945 seconds\n",
      "stitching took 0.09786558151245117 seconds\n",
      "\n",
      "\n",
      "progress: 0.07, 68/1027\n",
      "processing C3L-00965-24.svs\n",
      "Creating patches for:  C3L-00965-24 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3617 3240 11733 14741\n",
      "Contour Area: 118505441.0\n",
      "Extracted 34 coordinates\n",
      "start stitching C3L-00965-24\n",
      "original size: 17927 x 20549\n",
      "downscaled size for stiching: 2240 x 2568\n",
      "number of patches: 34\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/34 stitched\n",
      "progress: 4/34 stitched\n",
      "progress: 8/34 stitched\n",
      "progress: 12/34 stitched\n",
      "progress: 16/34 stitched\n",
      "progress: 20/34 stitched\n",
      "progress: 24/34 stitched\n",
      "progress: 28/34 stitched\n",
      "progress: 32/34 stitched\n",
      "segmentation took 0.2048182487487793 seconds\n",
      "patching took 0.019888639450073242 seconds\n",
      "stitching took 0.14063262939453125 seconds\n",
      "\n",
      "\n",
      "progress: 0.07, 69/1027\n",
      "processing C3L-00973-21.svs\n",
      "Creating patches for:  C3L-00973-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 6161 5137 31798 25256\n",
      "Contour Area: 440976549.0\n",
      "Extracted 34 coordinates\n",
      "start stitching C3L-00973-21\n",
      "original size: 41831 x 33721\n",
      "downscaled size for stiching: 2614 x 2107\n",
      "number of patches: 34\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/34 stitched\n",
      "progress: 4/34 stitched\n",
      "progress: 8/34 stitched\n",
      "progress: 12/34 stitched\n",
      "progress: 16/34 stitched\n",
      "progress: 20/34 stitched\n",
      "progress: 24/34 stitched\n",
      "progress: 28/34 stitched\n",
      "progress: 32/34 stitched\n",
      "segmentation took 0.19164776802062988 seconds\n",
      "patching took 0.021710634231567383 seconds\n",
      "stitching took 0.14173483848571777 seconds\n",
      "\n",
      "\n",
      "progress: 0.07, 70/1027\n",
      "processing C3L-00973-22.svs\n",
      "Creating patches for:  C3L-00973-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 5168 5456 35031 25126\n",
      "Contour Area: 653969811.0\n",
      "Extracted 44 coordinates\n",
      "start stitching C3L-00973-22\n",
      "original size: 45815 x 35110\n",
      "downscaled size for stiching: 2863 x 2194\n",
      "number of patches: 44\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/44 stitched\n",
      "progress: 5/44 stitched\n",
      "progress: 10/44 stitched\n",
      "progress: 15/44 stitched\n",
      "progress: 20/44 stitched\n",
      "progress: 25/44 stitched\n",
      "progress: 30/44 stitched\n",
      "progress: 35/44 stitched\n",
      "progress: 40/44 stitched\n",
      "segmentation took 0.24062323570251465 seconds\n",
      "patching took 0.018343687057495117 seconds\n",
      "stitching took 0.18326473236083984 seconds\n",
      "\n",
      "\n",
      "progress: 0.07, 71/1027\n",
      "processing C3L-00973-23.svs\n",
      "Creating patches for:  C3L-00973-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4736 4929 35815 27976\n",
      "Contour Area: 688379397.0\n",
      "Extracted 47 coordinates\n",
      "start stitching C3L-00973-23\n",
      "original size: 45815 x 36602\n",
      "downscaled size for stiching: 2863 x 2287\n",
      "number of patches: 47\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/47 stitched\n",
      "progress: 5/47 stitched\n",
      "progress: 10/47 stitched\n",
      "progress: 15/47 stitched\n",
      "progress: 20/47 stitched\n",
      "progress: 25/47 stitched\n",
      "progress: 30/47 stitched\n",
      "progress: 35/47 stitched\n",
      "progress: 40/47 stitched\n",
      "progress: 45/47 stitched\n",
      "segmentation took 0.2444438934326172 seconds\n",
      "patching took 0.02129817008972168 seconds\n",
      "stitching took 0.19804930686950684 seconds\n",
      "\n",
      "\n",
      "progress: 0.07, 72/1027\n",
      "processing C3L-00973-24.svs\n",
      "Creating patches for:  C3L-00973-24 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 6929 6192 29302 23681\n",
      "Contour Area: 472455040.0\n",
      "Extracted 34 coordinates\n",
      "start stitching C3L-00973-24\n",
      "original size: 41831 x 33232\n",
      "downscaled size for stiching: 2614 x 2077\n",
      "number of patches: 34\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/34 stitched\n",
      "progress: 4/34 stitched\n",
      "progress: 8/34 stitched\n",
      "progress: 12/34 stitched\n",
      "progress: 16/34 stitched\n",
      "progress: 20/34 stitched\n",
      "progress: 24/34 stitched\n",
      "progress: 28/34 stitched\n",
      "progress: 32/34 stitched\n",
      "segmentation took 0.20537638664245605 seconds\n",
      "patching took 0.0190582275390625 seconds\n",
      "stitching took 0.13944172859191895 seconds\n",
      "\n",
      "\n",
      "progress: 0.07, 73/1027\n",
      "processing C3L-00973-25.svs\n",
      "Creating patches for:  C3L-00973-25 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4306 5072 23291 20801\n",
      "Contour Area: 289641616.0\n",
      "Extracted 23 coordinates\n",
      "start stitching C3L-00973-25\n",
      "original size: 31871 x 29569\n",
      "downscaled size for stiching: 1991 x 1848\n",
      "number of patches: 23\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/23 stitched\n",
      "progress: 3/23 stitched\n",
      "progress: 6/23 stitched\n",
      "progress: 9/23 stitched\n",
      "progress: 12/23 stitched\n",
      "progress: 15/23 stitched\n",
      "progress: 18/23 stitched\n",
      "progress: 21/23 stitched\n",
      "segmentation took 0.1630558967590332 seconds\n",
      "patching took 0.015277862548828125 seconds\n",
      "stitching took 0.09670829772949219 seconds\n",
      "\n",
      "\n",
      "progress: 0.07, 74/1027\n",
      "processing C3L-00993-21.svs\n",
      "Creating patches for:  C3L-00993-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4769 4321 21910 17192\n",
      "Contour Area: 253730000.0\n",
      "Extracted 19 coordinates\n",
      "start stitching C3L-00993-21\n",
      "original size: 29879 x 25274\n",
      "downscaled size for stiching: 1867 x 1579\n",
      "number of patches: 19\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/19 stitched\n",
      "progress: 2/19 stitched\n",
      "progress: 4/19 stitched\n",
      "progress: 6/19 stitched\n",
      "progress: 8/19 stitched\n",
      "progress: 10/19 stitched\n",
      "progress: 12/19 stitched\n",
      "progress: 14/19 stitched\n",
      "progress: 16/19 stitched\n",
      "progress: 18/19 stitched\n",
      "segmentation took 0.13121724128723145 seconds\n",
      "patching took 0.014755010604858398 seconds\n",
      "stitching took 0.07498955726623535 seconds\n",
      "\n",
      "\n",
      "progress: 0.07, 75/1027\n",
      "processing C3L-00993-22.svs\n",
      "Creating patches for:  C3L-00993-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4353 4272 26412 27477\n",
      "Contour Area: 339857491.0\n",
      "Extracted 28 coordinates\n",
      "start stitching C3L-00993-22\n",
      "original size: 35855 x 36901\n",
      "downscaled size for stiching: 2240 x 2306\n",
      "number of patches: 28\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/28 stitched\n",
      "progress: 3/28 stitched\n",
      "progress: 6/28 stitched\n",
      "progress: 9/28 stitched\n",
      "progress: 12/28 stitched\n",
      "progress: 15/28 stitched\n",
      "progress: 18/28 stitched\n",
      "progress: 21/28 stitched\n",
      "progress: 24/28 stitched\n",
      "progress: 27/28 stitched\n",
      "segmentation took 0.16884541511535645 seconds\n",
      "patching took 0.019013643264770508 seconds\n",
      "stitching took 0.11847114562988281 seconds\n",
      "\n",
      "\n",
      "progress: 0.07, 76/1027\n",
      "processing C3L-00993-23.svs\n",
      "Creating patches for:  C3L-00993-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4418 4256 21963 25557\n",
      "Contour Area: 309278120.0\n",
      "Extracted 24 coordinates\n",
      "start stitching C3L-00993-23\n",
      "original size: 31871 x 34533\n",
      "downscaled size for stiching: 1991 x 2158\n",
      "number of patches: 24\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/24 stitched\n",
      "progress: 3/24 stitched\n",
      "progress: 6/24 stitched\n",
      "progress: 9/24 stitched\n",
      "progress: 12/24 stitched\n",
      "progress: 15/24 stitched\n",
      "progress: 18/24 stitched\n",
      "progress: 21/24 stitched\n",
      "segmentation took 0.1348421573638916 seconds\n",
      "patching took 0.021596908569335938 seconds\n",
      "stitching took 0.10174751281738281 seconds\n",
      "\n",
      "\n",
      "progress: 0.07, 77/1027\n",
      "processing C3L-00993-24.svs\n",
      "Creating patches for:  C3L-00993-24 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 5473 4592 22565 27361\n",
      "Contour Area: 359999160.0\n",
      "Extracted 27 coordinates\n",
      "start stitching C3L-00993-24\n",
      "original size: 33863 x 36257\n",
      "downscaled size for stiching: 2116 x 2266\n",
      "number of patches: 27\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/27 stitched\n",
      "progress: 3/27 stitched\n",
      "progress: 6/27 stitched\n",
      "progress: 9/27 stitched\n",
      "progress: 12/27 stitched\n",
      "progress: 15/27 stitched\n",
      "progress: 18/27 stitched\n",
      "progress: 21/27 stitched\n",
      "progress: 24/27 stitched\n",
      "segmentation took 0.15639114379882812 seconds\n",
      "patching took 0.016128063201904297 seconds\n",
      "stitching took 0.11727738380432129 seconds\n",
      "\n",
      "\n",
      "progress: 0.08, 78/1027\n",
      "processing C3L-01000-24.svs\n",
      "Creating patches for:  C3L-01000-24 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3072 3265 39031 10869\n",
      "Contour Area: 232422200.0\n",
      "Extracted 21 coordinates\n",
      "start stitching C3L-01000-24\n",
      "original size: 45815 x 16583\n",
      "downscaled size for stiching: 2863 x 1036\n",
      "number of patches: 21\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/21 stitched\n",
      "progress: 3/21 stitched\n",
      "progress: 6/21 stitched\n",
      "progress: 9/21 stitched\n",
      "progress: 12/21 stitched\n",
      "progress: 15/21 stitched\n",
      "progress: 18/21 stitched\n",
      "segmentation took 0.13036465644836426 seconds\n",
      "patching took 0.01672077178955078 seconds\n",
      "stitching took 0.08681583404541016 seconds\n",
      "\n",
      "\n",
      "progress: 0.08, 79/1027\n",
      "processing C3L-01000-25.svs\n",
      "Creating patches for:  C3L-01000-25 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 5169 4032 31309 18258\n",
      "Contour Area: 315476065.0\n",
      "Extracted 26 coordinates\n",
      "start stitching C3L-01000-25\n",
      "original size: 39839 x 26290\n",
      "downscaled size for stiching: 2489 x 1643\n",
      "number of patches: 26\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/26 stitched\n",
      "progress: 3/26 stitched\n",
      "progress: 6/26 stitched\n",
      "progress: 9/26 stitched\n",
      "progress: 12/26 stitched\n",
      "progress: 15/26 stitched\n",
      "progress: 18/26 stitched\n",
      "progress: 21/26 stitched\n",
      "progress: 24/26 stitched\n",
      "segmentation took 0.1782817840576172 seconds\n",
      "patching took 0.01633620262145996 seconds\n",
      "stitching took 0.10756206512451172 seconds\n",
      "\n",
      "\n",
      "progress: 0.08, 80/1027\n",
      "processing C3L-01285-21.svs\n",
      "Creating patches for:  C3L-01285-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4817 3329 12965 18521\n",
      "Contour Area: 117821255.0\n",
      "Extracted 11 coordinates\n",
      "start stitching C3L-01285-21\n",
      "original size: 21911 x 24667\n",
      "downscaled size for stiching: 1369 x 1541\n",
      "number of patches: 11\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/11 stitched\n",
      "progress: 2/11 stitched\n",
      "progress: 4/11 stitched\n",
      "progress: 6/11 stitched\n",
      "progress: 8/11 stitched\n",
      "progress: 10/11 stitched\n",
      "segmentation took 0.09349822998046875 seconds\n",
      "patching took 0.015533924102783203 seconds\n",
      "stitching took 0.04596996307373047 seconds\n",
      "\n",
      "\n",
      "progress: 0.08, 81/1027\n",
      "processing C3L-01285-22.svs\n",
      "Creating patches for:  C3L-01285-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3569 3568 14389 12361\n",
      "Contour Area: 116551104.0\n",
      "Extracted 33 coordinates\n",
      "start stitching C3L-01285-22\n",
      "original size: 21911 x 19128\n",
      "downscaled size for stiching: 2738 x 2391\n",
      "number of patches: 33\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/33 stitched\n",
      "progress: 4/33 stitched\n",
      "progress: 8/33 stitched\n",
      "progress: 12/33 stitched\n",
      "progress: 16/33 stitched\n",
      "progress: 20/33 stitched\n",
      "progress: 24/33 stitched\n",
      "progress: 28/33 stitched\n",
      "progress: 32/33 stitched\n",
      "segmentation took 0.2273252010345459 seconds\n",
      "patching took 0.01931476593017578 seconds\n",
      "stitching took 0.1408538818359375 seconds\n",
      "\n",
      "\n",
      "progress: 0.08, 82/1027\n",
      "processing C3L-01285-23.svs\n",
      "Creating patches for:  C3L-01285-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4161 4257 10933 19001\n",
      "Contour Area: 135343454.0\n",
      "Extracted 12 coordinates\n",
      "start stitching C3L-01285-23\n",
      "original size: 17927 x 27083\n",
      "downscaled size for stiching: 1120 x 1692\n",
      "number of patches: 12\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/12 stitched\n",
      "progress: 2/12 stitched\n",
      "progress: 4/12 stitched\n",
      "progress: 6/12 stitched\n",
      "progress: 8/12 stitched\n",
      "progress: 10/12 stitched\n",
      "segmentation took 0.08422064781188965 seconds\n",
      "patching took 0.016922473907470703 seconds\n",
      "stitching took 0.04997682571411133 seconds\n",
      "\n",
      "\n",
      "progress: 0.08, 83/1027\n",
      "processing C3L-01330-21.svs\n",
      "Creating patches for:  C3L-01330-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 2633 2456 9734 12262\n",
      "Contour Area: 76801789.0\n",
      "Extracted 23 coordinates\n",
      "start stitching C3L-01330-21\n",
      "original size: 13943 x 15534\n",
      "downscaled size for stiching: 1742 x 1941\n",
      "number of patches: 23\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/23 stitched\n",
      "progress: 3/23 stitched\n",
      "progress: 6/23 stitched\n",
      "progress: 9/23 stitched\n",
      "progress: 12/23 stitched\n",
      "progress: 15/23 stitched\n",
      "progress: 18/23 stitched\n",
      "progress: 21/23 stitched\n",
      "segmentation took 0.15242314338684082 seconds\n",
      "patching took 0.01791858673095703 seconds\n",
      "stitching took 0.09563255310058594 seconds\n",
      "\n",
      "\n",
      "progress: 0.08, 84/1027\n",
      "processing C3L-01330-22.svs\n",
      "Creating patches for:  C3L-01330-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3769 5064 14269 7969\n",
      "Contour Area: 56816120.0\n",
      "Extracted 21 coordinates\n",
      "start stitching C3L-01330-22\n",
      "original size: 21911 x 17713\n",
      "downscaled size for stiching: 2738 x 2214\n",
      "number of patches: 21\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/21 stitched\n",
      "progress: 3/21 stitched\n",
      "progress: 6/21 stitched\n",
      "progress: 9/21 stitched\n",
      "progress: 12/21 stitched\n",
      "progress: 15/21 stitched\n",
      "progress: 18/21 stitched\n",
      "segmentation took 0.19176745414733887 seconds\n",
      "patching took 0.01713728904724121 seconds\n",
      "stitching took 0.0942385196685791 seconds\n",
      "\n",
      "\n",
      "progress: 0.08, 85/1027\n",
      "processing C3L-01330-23.svs\n",
      "Creating patches for:  C3L-01330-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 1520 2136 17591 8787\n",
      "Contour Area: 76783112.0\n",
      "Extracted 25 coordinates\n",
      "start stitching C3L-01330-23\n",
      "original size: 19919 x 12099\n",
      "downscaled size for stiching: 2489 x 1512\n",
      "number of patches: 25\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/25 stitched\n",
      "progress: 3/25 stitched\n",
      "progress: 6/25 stitched\n",
      "progress: 9/25 stitched\n",
      "progress: 12/25 stitched\n",
      "progress: 15/25 stitched\n",
      "progress: 18/25 stitched\n",
      "progress: 21/25 stitched\n",
      "progress: 24/25 stitched\n",
      "segmentation took 0.16972708702087402 seconds\n",
      "patching took 0.01789546012878418 seconds\n",
      "stitching took 0.10293912887573242 seconds\n",
      "\n",
      "\n",
      "progress: 0.08, 86/1027\n",
      "processing C3L-01455-21.svs\n",
      "Creating patches for:  C3L-01455-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3939 4754 9658 15833\n",
      "Contour Area: 65592913.0\n",
      "Extracted 7 coordinates\n",
      "start stitching C3L-01455-21\n",
      "original size: 15935 x 24365\n",
      "downscaled size for stiching: 995 x 1522\n",
      "number of patches: 7\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4099, 4099)x(4099, 4099)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/7 stitched\n",
      "progress: 1/7 stitched\n",
      "progress: 2/7 stitched\n",
      "progress: 3/7 stitched\n",
      "progress: 4/7 stitched\n",
      "progress: 5/7 stitched\n",
      "progress: 6/7 stitched\n",
      "segmentation took 0.06765198707580566 seconds\n",
      "patching took 0.017349958419799805 seconds\n",
      "stitching took 0.03244376182556152 seconds\n",
      "\n",
      "\n",
      "progress: 0.08, 87/1027\n",
      "processing C3L-01455-22.svs\n",
      "Creating patches for:  C3L-01455-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3665 4200 7069 13396\n",
      "Contour Area: 61671840.0\n",
      "Extracted 20 coordinates\n",
      "start stitching C3L-01455-22\n",
      "original size: 13943 x 20412\n",
      "downscaled size for stiching: 1742 x 2551\n",
      "number of patches: 20\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/20 stitched\n",
      "progress: 2/20 stitched\n",
      "progress: 4/20 stitched\n",
      "progress: 6/20 stitched\n",
      "progress: 8/20 stitched\n",
      "progress: 10/20 stitched\n",
      "progress: 12/20 stitched\n",
      "progress: 14/20 stitched\n",
      "progress: 16/20 stitched\n",
      "progress: 18/20 stitched\n",
      "segmentation took 0.14696264266967773 seconds\n",
      "patching took 0.01759934425354004 seconds\n",
      "stitching took 0.08707976341247559 seconds\n",
      "\n",
      "\n",
      "progress: 0.09, 88/1027\n",
      "processing C3L-01455-23.svs\n",
      "Creating patches for:  C3L-01455-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4945 8258 11509 16517\n",
      "Contour Area: 76896399.0\n",
      "Extracted 7 coordinates\n",
      "start stitching C3L-01455-23\n",
      "original size: 21911 x 27559\n",
      "downscaled size for stiching: 1369 x 1722\n",
      "number of patches: 7\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/7 stitched\n",
      "progress: 1/7 stitched\n",
      "progress: 2/7 stitched\n",
      "progress: 3/7 stitched\n",
      "progress: 4/7 stitched\n",
      "progress: 5/7 stitched\n",
      "progress: 6/7 stitched\n",
      "segmentation took 0.1010892391204834 seconds\n",
      "patching took 0.016589641571044922 seconds\n",
      "stitching took 0.03290224075317383 seconds\n",
      "\n",
      "\n",
      "progress: 0.09, 89/1027\n",
      "processing C3L-01606-21.svs\n",
      "Creating patches for:  C3L-01606-21 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 4737 15463 15510 8869\n",
      "Contour Area: 72396779.0\n",
      "Extracted 7 coordinates\n",
      "Bounding Box: 5345 3697 11269 12023\n",
      "Contour Area: 68047476.0\n",
      "Extracted 6 coordinates\n",
      "start stitching C3L-01606-21\n",
      "original size: 21911 x 26893\n",
      "downscaled size for stiching: 1369 x 1680\n",
      "number of patches: 13\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/13 stitched\n",
      "progress: 2/13 stitched\n",
      "progress: 4/13 stitched\n",
      "progress: 6/13 stitched\n",
      "progress: 8/13 stitched\n",
      "progress: 10/13 stitched\n",
      "progress: 12/13 stitched\n",
      "segmentation took 0.10468435287475586 seconds\n",
      "patching took 0.039811134338378906 seconds\n",
      "stitching took 0.05610251426696777 seconds\n",
      "\n",
      "\n",
      "progress: 0.09, 90/1027\n",
      "processing C3L-01632-21.svs\n",
      "Creating patches for:  C3L-01632-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4657 3761 13269 24348\n",
      "Contour Area: 220807272.0\n",
      "Extracted 17 coordinates\n",
      "start stitching C3L-01632-21\n",
      "original size: 21911 x 31038\n",
      "downscaled size for stiching: 1369 x 1939\n",
      "number of patches: 17\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/17 stitched\n",
      "progress: 2/17 stitched\n",
      "progress: 4/17 stitched\n",
      "progress: 6/17 stitched\n",
      "progress: 8/17 stitched\n",
      "progress: 10/17 stitched\n",
      "progress: 12/17 stitched\n",
      "progress: 14/17 stitched\n",
      "progress: 16/17 stitched\n",
      "segmentation took 0.11572957038879395 seconds\n",
      "patching took 0.01745438575744629 seconds\n",
      "stitching took 0.07494187355041504 seconds\n",
      "\n",
      "\n",
      "progress: 0.09, 91/1027\n",
      "processing C3L-01632-22.svs\n",
      "Creating patches for:  C3L-01632-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3777 4049 14421 27274\n",
      "Contour Area: 278861580.0\n",
      "Extracted 23 coordinates\n",
      "start stitching C3L-01632-22\n",
      "original size: 21911 x 34091\n",
      "downscaled size for stiching: 1369 x 2130\n",
      "number of patches: 23\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/23 stitched\n",
      "progress: 3/23 stitched\n",
      "progress: 6/23 stitched\n",
      "progress: 9/23 stitched\n",
      "progress: 12/23 stitched\n",
      "progress: 15/23 stitched\n",
      "progress: 18/23 stitched\n",
      "progress: 21/23 stitched\n",
      "segmentation took 0.13131213188171387 seconds\n",
      "patching took 0.018030405044555664 seconds\n",
      "stitching took 0.09217500686645508 seconds\n",
      "\n",
      "\n",
      "progress: 0.09, 92/1027\n",
      "processing C3L-01632-23.svs\n",
      "Creating patches for:  C3L-01632-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 2809 2456 10878 17190\n",
      "Contour Area: 129314079.0\n",
      "Extracted 36 coordinates\n",
      "start stitching C3L-01632-23\n",
      "original size: 15935 x 22382\n",
      "downscaled size for stiching: 1991 x 2797\n",
      "number of patches: 36\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/36 stitched\n",
      "progress: 4/36 stitched\n",
      "progress: 8/36 stitched\n",
      "progress: 12/36 stitched\n",
      "progress: 16/36 stitched\n",
      "progress: 20/36 stitched\n",
      "progress: 24/36 stitched\n",
      "progress: 28/36 stitched\n",
      "progress: 32/36 stitched\n",
      "segmentation took 0.1992664337158203 seconds\n",
      "patching took 0.017350196838378906 seconds\n",
      "stitching took 0.14734458923339844 seconds\n",
      "\n",
      "\n",
      "progress: 0.09, 93/1027\n",
      "processing C3L-01663-22.svs\n",
      "Creating patches for:  C3L-01663-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3801 3816 12381 14804\n",
      "Contour Area: 101538950.0\n",
      "Extracted 30 coordinates\n",
      "start stitching C3L-01663-22\n",
      "original size: 19919 x 22612\n",
      "downscaled size for stiching: 2489 x 2826\n",
      "number of patches: 30\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/30 stitched\n",
      "progress: 3/30 stitched\n",
      "progress: 6/30 stitched\n",
      "progress: 9/30 stitched\n",
      "progress: 12/30 stitched\n",
      "progress: 15/30 stitched\n",
      "progress: 18/30 stitched\n",
      "progress: 21/30 stitched\n",
      "progress: 24/30 stitched\n",
      "progress: 27/30 stitched\n",
      "segmentation took 0.23122501373291016 seconds\n",
      "patching took 0.019036293029785156 seconds\n",
      "stitching took 0.1295630931854248 seconds\n",
      "\n",
      "\n",
      "progress: 0.09, 94/1027\n",
      "processing C3L-01682-21.svs\n",
      "Creating patches for:  C3L-01682-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3345 3080 13806 11329\n",
      "Contour Area: 101796364.0\n",
      "Extracted 32 coordinates\n",
      "start stitching C3L-01682-21\n",
      "original size: 19919 x 18056\n",
      "downscaled size for stiching: 2489 x 2257\n",
      "number of patches: 32\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/32 stitched\n",
      "progress: 4/32 stitched\n",
      "progress: 8/32 stitched\n",
      "progress: 12/32 stitched\n",
      "progress: 16/32 stitched\n",
      "progress: 20/32 stitched\n",
      "progress: 24/32 stitched\n",
      "progress: 28/32 stitched\n",
      "segmentation took 0.19520998001098633 seconds\n",
      "patching took 0.01928234100341797 seconds\n",
      "stitching took 0.1329329013824463 seconds\n",
      "\n",
      "\n",
      "progress: 0.09, 95/1027\n",
      "processing C3L-01682-23.svs\n",
      "Creating patches for:  C3L-01682-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 2792 3056 13854 10684\n",
      "Contour Area: 49084407.0\n",
      "Extracted 17 coordinates\n",
      "start stitching C3L-01682-23\n",
      "original size: 19919 x 17236\n",
      "downscaled size for stiching: 2489 x 2154\n",
      "number of patches: 17\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/17 stitched\n",
      "progress: 2/17 stitched\n",
      "progress: 4/17 stitched\n",
      "progress: 6/17 stitched\n",
      "progress: 8/17 stitched\n",
      "progress: 10/17 stitched\n",
      "progress: 12/17 stitched\n",
      "progress: 14/17 stitched\n",
      "progress: 16/17 stitched\n",
      "segmentation took 0.16419625282287598 seconds\n",
      "patching took 0.020150423049926758 seconds\n",
      "stitching took 0.08000707626342773 seconds\n",
      "\n",
      "\n",
      "progress: 0.09, 96/1027\n",
      "processing C3L-01683-26.svs\n",
      "Creating patches for:  C3L-01683-26 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3817 4064 17229 16172\n",
      "Contour Area: 179055156.0\n",
      "Extracted 49 coordinates\n",
      "start stitching C3L-01683-26\n",
      "original size: 25895 x 23404\n",
      "downscaled size for stiching: 3236 x 2925\n",
      "number of patches: 49\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/49 stitched\n",
      "progress: 5/49 stitched\n",
      "progress: 10/49 stitched\n",
      "progress: 15/49 stitched\n",
      "progress: 20/49 stitched\n",
      "progress: 25/49 stitched\n",
      "progress: 30/49 stitched\n",
      "progress: 35/49 stitched\n",
      "progress: 40/49 stitched\n",
      "progress: 45/49 stitched\n",
      "segmentation took 0.34139180183410645 seconds\n",
      "patching took 0.021791458129882812 seconds\n",
      "stitching took 0.21294951438903809 seconds\n",
      "\n",
      "\n",
      "progress: 0.09, 97/1027\n",
      "processing C3L-01683-27.svs\n",
      "Creating patches for:  C3L-01683-27 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3737 4424 14101 12553\n",
      "Contour Area: 138836036.0\n",
      "Extracted 39 coordinates\n",
      "start stitching C3L-01683-27\n",
      "original size: 21911 x 20121\n",
      "downscaled size for stiching: 2738 x 2515\n",
      "number of patches: 39\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/39 stitched\n",
      "progress: 4/39 stitched\n",
      "progress: 8/39 stitched\n",
      "progress: 12/39 stitched\n",
      "progress: 16/39 stitched\n",
      "progress: 20/39 stitched\n",
      "progress: 24/39 stitched\n",
      "progress: 28/39 stitched\n",
      "progress: 32/39 stitched\n",
      "progress: 36/39 stitched\n",
      "segmentation took 0.23520207405090332 seconds\n",
      "patching took 0.02039933204650879 seconds\n",
      "stitching took 0.15869450569152832 seconds\n",
      "\n",
      "\n",
      "progress: 0.10, 98/1027\n",
      "processing C3L-01838-23.svs\n",
      "Creating patches for:  C3L-01838-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4057 3512 14709 15286\n",
      "Contour Area: 113257053.0\n",
      "Extracted 36 coordinates\n",
      "start stitching C3L-01838-23\n",
      "original size: 21911 x 22014\n",
      "downscaled size for stiching: 2738 x 2751\n",
      "number of patches: 36\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/36 stitched\n",
      "progress: 4/36 stitched\n",
      "progress: 8/36 stitched\n",
      "progress: 12/36 stitched\n",
      "progress: 16/36 stitched\n",
      "progress: 20/36 stitched\n",
      "progress: 24/36 stitched\n",
      "progress: 28/36 stitched\n",
      "progress: 32/36 stitched\n",
      "segmentation took 0.23803329467773438 seconds\n",
      "patching took 0.021299123764038086 seconds\n",
      "stitching took 0.15376901626586914 seconds\n",
      "\n",
      "\n",
      "progress: 0.10, 99/1027\n",
      "processing C3L-01862-21.svs\n",
      "Creating patches for:  C3L-01862-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 5321 3856 16165 16293\n",
      "Contour Area: 136734524.0\n",
      "Extracted 41 coordinates\n",
      "start stitching C3L-01862-21\n",
      "original size: 27887 x 23725\n",
      "downscaled size for stiching: 3485 x 2965\n",
      "number of patches: 41\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/41 stitched\n",
      "progress: 5/41 stitched\n",
      "progress: 10/41 stitched\n",
      "progress: 15/41 stitched\n",
      "progress: 20/41 stitched\n",
      "progress: 25/41 stitched\n",
      "progress: 30/41 stitched\n",
      "progress: 35/41 stitched\n",
      "progress: 40/41 stitched\n",
      "segmentation took 0.3516507148742676 seconds\n",
      "patching took 0.022167205810546875 seconds\n",
      "stitching took 0.17976689338684082 seconds\n",
      "\n",
      "\n",
      "progress: 0.10, 100/1027\n",
      "processing C3L-01862-22.svs\n",
      "Creating patches for:  C3L-01862-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4985 4416 11149 12459\n",
      "Contour Area: 71261760.0\n",
      "Extracted 22 coordinates\n",
      "start stitching C3L-01862-22\n",
      "original size: 19919 x 20619\n",
      "downscaled size for stiching: 2489 x 2577\n",
      "number of patches: 22\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/22 stitched\n",
      "progress: 3/22 stitched\n",
      "progress: 6/22 stitched\n",
      "progress: 9/22 stitched\n",
      "progress: 12/22 stitched\n",
      "progress: 15/22 stitched\n",
      "progress: 18/22 stitched\n",
      "progress: 21/22 stitched\n",
      "segmentation took 0.20439863204956055 seconds\n",
      "patching took 0.02103424072265625 seconds\n",
      "stitching took 0.10021543502807617 seconds\n",
      "\n",
      "\n",
      "progress: 0.10, 101/1027\n",
      "processing C3L-01862-23.svs\n",
      "Creating patches for:  C3L-01862-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3721 3665 12373 13245\n",
      "Contour Area: 107324255.0\n",
      "Extracted 31 coordinates\n",
      "start stitching C3L-01862-23\n",
      "original size: 19919 x 20126\n",
      "downscaled size for stiching: 2489 x 2515\n",
      "number of patches: 31\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/31 stitched\n",
      "progress: 4/31 stitched\n",
      "progress: 8/31 stitched\n",
      "progress: 12/31 stitched\n",
      "progress: 16/31 stitched\n",
      "progress: 20/31 stitched\n",
      "progress: 24/31 stitched\n",
      "progress: 28/31 stitched\n",
      "segmentation took 0.21962904930114746 seconds\n",
      "patching took 0.020087480545043945 seconds\n",
      "stitching took 0.1333019733428955 seconds\n",
      "\n",
      "\n",
      "progress: 0.10, 102/1027\n",
      "processing C3L-01884-21.svs\n",
      "Creating patches for:  C3L-01884-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4161 6626 10260 14661\n",
      "Contour Area: 93366146.0\n",
      "Extracted 11 coordinates\n",
      "start stitching C3L-01884-21\n",
      "original size: 21911 x 25160\n",
      "downscaled size for stiching: 1369 x 1572\n",
      "number of patches: 11\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/11 stitched\n",
      "progress: 2/11 stitched\n",
      "progress: 4/11 stitched\n",
      "progress: 6/11 stitched\n",
      "progress: 8/11 stitched\n",
      "progress: 10/11 stitched\n",
      "segmentation took 0.09450268745422363 seconds\n",
      "patching took 0.021006107330322266 seconds\n",
      "stitching took 0.046967506408691406 seconds\n",
      "\n",
      "\n",
      "progress: 0.10, 103/1027\n",
      "processing C3L-01884-22.svs\n",
      "Creating patches for:  C3L-01884-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 5201 4624 12045 11833\n",
      "Contour Area: 69952680.0\n",
      "Extracted 22 coordinates\n",
      "start stitching C3L-01884-22\n",
      "original size: 21911 x 21273\n",
      "downscaled size for stiching: 2738 x 2659\n",
      "number of patches: 22\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/22 stitched\n",
      "progress: 3/22 stitched\n",
      "progress: 6/22 stitched\n",
      "progress: 9/22 stitched\n",
      "progress: 12/22 stitched\n",
      "progress: 15/22 stitched\n",
      "progress: 18/22 stitched\n",
      "progress: 21/22 stitched\n",
      "segmentation took 0.23137497901916504 seconds\n",
      "patching took 0.018578290939331055 seconds\n",
      "stitching took 0.10556530952453613 seconds\n",
      "\n",
      "\n",
      "progress: 0.10, 104/1027\n",
      "processing C3L-01889-21.svs\n",
      "Creating patches for:  C3L-01889-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 6003 3808 12505 17169\n",
      "Contour Area: 106002232.0\n",
      "Extracted 9 coordinates\n",
      "start stitching C3L-01889-21\n",
      "original size: 23903 x 24736\n",
      "downscaled size for stiching: 1493 x 1546\n",
      "number of patches: 9\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/9 stitched\n",
      "progress: 1/9 stitched\n",
      "progress: 2/9 stitched\n",
      "progress: 3/9 stitched\n",
      "progress: 4/9 stitched\n",
      "progress: 5/9 stitched\n",
      "progress: 6/9 stitched\n",
      "progress: 7/9 stitched\n",
      "progress: 8/9 stitched\n",
      "segmentation took 0.10483622550964355 seconds\n",
      "patching took 0.017843008041381836 seconds\n",
      "stitching took 0.04069352149963379 seconds\n",
      "\n",
      "\n",
      "progress: 0.10, 105/1027\n",
      "processing C3L-01890-22.svs\n",
      "Creating patches for:  C3L-01890-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4481 3616 13501 11530\n",
      "Contour Area: 91088904.0\n",
      "Extracted 29 coordinates\n",
      "start stitching C3L-01890-22\n",
      "original size: 21911 x 18954\n",
      "downscaled size for stiching: 2738 x 2369\n",
      "number of patches: 29\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/29 stitched\n",
      "progress: 3/29 stitched\n",
      "progress: 6/29 stitched\n",
      "progress: 9/29 stitched\n",
      "progress: 12/29 stitched\n",
      "progress: 15/29 stitched\n",
      "progress: 18/29 stitched\n",
      "progress: 21/29 stitched\n",
      "progress: 24/29 stitched\n",
      "progress: 27/29 stitched\n",
      "segmentation took 0.22637581825256348 seconds\n",
      "patching took 0.01767444610595703 seconds\n",
      "stitching took 0.13060688972473145 seconds\n",
      "\n",
      "\n",
      "progress: 0.10, 106/1027\n",
      "processing C3L-01924-21.svs\n",
      "Creating patches for:  C3L-01924-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 560 1184 18193 21124\n",
      "Contour Area: 200861696.0\n",
      "Extracted 58 coordinates\n",
      "start stitching C3L-01924-21\n",
      "original size: 19024 x 22596\n",
      "downscaled size for stiching: 2378 x 2824\n",
      "number of patches: 58\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/58 stitched\n",
      "progress: 6/58 stitched\n",
      "progress: 12/58 stitched\n",
      "progress: 18/58 stitched\n",
      "progress: 24/58 stitched\n",
      "progress: 30/58 stitched\n",
      "progress: 36/58 stitched\n",
      "progress: 42/58 stitched\n",
      "progress: 48/58 stitched\n",
      "progress: 54/58 stitched\n",
      "segmentation took 0.34197115898132324 seconds\n",
      "patching took 0.018851518630981445 seconds\n",
      "stitching took 0.228926420211792 seconds\n",
      "\n",
      "\n",
      "progress: 0.10, 107/1027\n",
      "processing C3L-01924-22.svs\n",
      "Creating patches for:  C3L-01924-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 464 832 27606 25281\n",
      "Contour Area: 360820040.0\n",
      "Extracted 28 coordinates\n",
      "start stitching C3L-01924-22\n",
      "original size: 28438 x 26912\n",
      "downscaled size for stiching: 1777 x 1682\n",
      "number of patches: 28\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/28 stitched\n",
      "progress: 3/28 stitched\n",
      "progress: 6/28 stitched\n",
      "progress: 9/28 stitched\n",
      "progress: 12/28 stitched\n",
      "progress: 15/28 stitched\n",
      "progress: 18/28 stitched\n",
      "progress: 21/28 stitched\n",
      "progress: 24/28 stitched\n",
      "progress: 27/28 stitched\n",
      "segmentation took 0.20071983337402344 seconds\n",
      "patching took 0.019022464752197266 seconds\n",
      "stitching took 0.11275172233581543 seconds\n",
      "\n",
      "\n",
      "progress: 0.11, 108/1027\n",
      "processing C3L-01924-23.svs\n",
      "Creating patches for:  C3L-01924-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 336 1032 11941 18693\n",
      "Contour Area: 146362353.0\n",
      "Extracted 43 coordinates\n",
      "start stitching C3L-01924-23\n",
      "original size: 12597 x 20077\n",
      "downscaled size for stiching: 1574 x 2509\n",
      "number of patches: 43\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/43 stitched\n",
      "progress: 5/43 stitched\n",
      "progress: 10/43 stitched\n",
      "progress: 15/43 stitched\n",
      "progress: 20/43 stitched\n",
      "progress: 25/43 stitched\n",
      "progress: 30/43 stitched\n",
      "progress: 35/43 stitched\n",
      "progress: 40/43 stitched\n",
      "segmentation took 0.24935626983642578 seconds\n",
      "patching took 0.016028881072998047 seconds\n",
      "stitching took 0.1676030158996582 seconds\n",
      "\n",
      "\n",
      "progress: 0.11, 109/1027\n",
      "processing C3L-01924-24.svs\n",
      "Creating patches for:  C3L-01924-24 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 96 272 22520 24263\n",
      "Contour Area: 321842396.0\n",
      "Extracted 24 coordinates\n",
      "start stitching C3L-01924-24\n",
      "original size: 23032 x 24839\n",
      "downscaled size for stiching: 1439 x 1552\n",
      "number of patches: 24\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/24 stitched\n",
      "progress: 3/24 stitched\n",
      "progress: 6/24 stitched\n",
      "progress: 9/24 stitched\n",
      "progress: 12/24 stitched\n",
      "progress: 15/24 stitched\n",
      "progress: 18/24 stitched\n",
      "progress: 21/24 stitched\n",
      "segmentation took 0.13849258422851562 seconds\n",
      "patching took 0.015332221984863281 seconds\n",
      "stitching took 0.09575533866882324 seconds\n",
      "\n",
      "\n",
      "progress: 0.11, 110/1027\n",
      "processing C3L-01924-25.svs\n",
      "Creating patches for:  C3L-01924-25 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 64 1560 22977 18646\n",
      "Contour Area: 247924168.0\n",
      "Extracted 68 coordinates\n",
      "start stitching C3L-01924-25\n",
      "original size: 23393 x 20502\n",
      "downscaled size for stiching: 2924 x 2562\n",
      "number of patches: 68\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/68 stitched\n",
      "progress: 7/68 stitched\n",
      "progress: 14/68 stitched\n",
      "progress: 21/68 stitched\n",
      "progress: 28/68 stitched\n",
      "progress: 35/68 stitched\n",
      "progress: 42/68 stitched\n",
      "progress: 49/68 stitched\n",
      "progress: 56/68 stitched\n",
      "progress: 63/68 stitched\n",
      "segmentation took 0.4279167652130127 seconds\n",
      "patching took 0.0188601016998291 seconds\n",
      "stitching took 0.2639505863189697 seconds\n",
      "\n",
      "\n",
      "progress: 0.11, 111/1027\n",
      "processing C3L-02127-21.svs\n",
      "Creating patches for:  C3L-02127-21 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 52746 5953 17700 18438\n",
      "Contour Area: 245742958.0\n",
      "Extracted 19 coordinates\n",
      "Bounding Box: 4672 5729 17637 18822\n",
      "Contour Area: 256925918.0\n",
      "Extracted 20 coordinates\n",
      "start stitching C3L-02127-21\n",
      "original size: 75695 x 29384\n",
      "downscaled size for stiching: 2365 x 918\n",
      "number of patches: 39\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/39 stitched\n",
      "progress: 4/39 stitched\n",
      "progress: 8/39 stitched\n",
      "progress: 12/39 stitched\n",
      "progress: 16/39 stitched\n",
      "progress: 20/39 stitched\n",
      "progress: 24/39 stitched\n",
      "progress: 28/39 stitched\n",
      "progress: 32/39 stitched\n",
      "progress: 36/39 stitched\n",
      "segmentation took 0.09859633445739746 seconds\n",
      "patching took 0.0348818302154541 seconds\n",
      "stitching took 0.04301023483276367 seconds\n",
      "\n",
      "\n",
      "progress: 0.11, 112/1027\n",
      "processing C3L-02127-22.svs\n",
      "Creating patches for:  C3L-02127-22 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 4448 6533 25539 18193\n",
      "Contour Area: 324402487.0\n",
      "Extracted 26 coordinates\n",
      "Bounding Box: 52516 5636 25603 19698\n",
      "Contour Area: 317241017.0\n",
      "Extracted 27 coordinates\n",
      "start stitching C3L-02127-22\n",
      "original size: 81671 x 30426\n",
      "downscaled size for stiching: 2552 x 950\n",
      "number of patches: 53\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/53 stitched\n",
      "progress: 6/53 stitched\n",
      "progress: 12/53 stitched\n",
      "progress: 18/53 stitched\n",
      "progress: 24/53 stitched\n",
      "progress: 30/53 stitched\n",
      "progress: 36/53 stitched\n",
      "progress: 42/53 stitched\n",
      "progress: 48/53 stitched\n",
      "segmentation took 0.11147880554199219 seconds\n",
      "patching took 0.032958030700683594 seconds\n",
      "stitching took 0.05463552474975586 seconds\n",
      "\n",
      "\n",
      "progress: 0.11, 113/1027\n",
      "processing C3L-02127-23.svs\n",
      "Creating patches for:  C3L-02127-23 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 2992 4402 24372 14635\n",
      "Contour Area: 175994054.0\n",
      "Extracted 16 coordinates\n",
      "Bounding Box: 34627 2865 25332 13194\n",
      "Contour Area: 169374646.0\n",
      "Extracted 14 coordinates\n",
      "start stitching C3L-02127-23\n",
      "original size: 61751 x 20862\n",
      "downscaled size for stiching: 3859 x 1303\n",
      "number of patches: 30\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/30 stitched\n",
      "progress: 3/30 stitched\n",
      "progress: 6/30 stitched\n",
      "progress: 9/30 stitched\n",
      "progress: 12/30 stitched\n",
      "progress: 15/30 stitched\n",
      "progress: 18/30 stitched\n",
      "progress: 21/30 stitched\n",
      "progress: 24/30 stitched\n",
      "progress: 27/30 stitched\n",
      "segmentation took 0.1803891658782959 seconds\n",
      "patching took 0.0403285026550293 seconds\n",
      "stitching took 0.12167930603027344 seconds\n",
      "\n",
      "\n",
      "progress: 0.11, 114/1027\n",
      "processing C3L-02127-24.svs\n",
      "Creating patches for:  C3L-02127-24 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 52500 5638 25003 19511\n",
      "Contour Area: 302686965.5\n",
      "Extracted 23 coordinates\n",
      "Bounding Box: 4001 4388 25611 19544\n",
      "Contour Area: 304517761.0\n",
      "Extracted 24 coordinates\n",
      "start stitching C3L-02127-24\n",
      "original size: 79679 x 27455\n",
      "downscaled size for stiching: 2489 x 857\n",
      "number of patches: 47\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/47 stitched\n",
      "progress: 5/47 stitched\n",
      "progress: 10/47 stitched\n",
      "progress: 15/47 stitched\n",
      "progress: 20/47 stitched\n",
      "progress: 25/47 stitched\n",
      "progress: 30/47 stitched\n",
      "progress: 35/47 stitched\n",
      "progress: 40/47 stitched\n",
      "progress: 45/47 stitched\n",
      "segmentation took 0.09828472137451172 seconds\n",
      "patching took 0.033197879791259766 seconds\n",
      "stitching took 0.05028247833251953 seconds\n",
      "\n",
      "\n",
      "progress: 0.11, 115/1027\n",
      "processing C3L-02130-21.svs\n",
      "Creating patches for:  C3L-02130-21 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 4608 9607 35751 26869\n",
      "Contour Area: 605899900.0\n",
      "Extracted 45 coordinates\n",
      "Bounding Box: 52456 4451 35399 26613\n",
      "Contour Area: 594381869.0\n",
      "Extracted 44 coordinates\n",
      "start stitching C3L-02130-21\n",
      "original size: 91631 x 41119\n",
      "downscaled size for stiching: 2863 x 1284\n",
      "number of patches: 89\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/89 stitched\n",
      "progress: 9/89 stitched\n",
      "progress: 18/89 stitched\n",
      "progress: 27/89 stitched\n",
      "progress: 36/89 stitched\n",
      "progress: 45/89 stitched\n",
      "progress: 54/89 stitched\n",
      "progress: 63/89 stitched\n",
      "progress: 72/89 stitched\n",
      "progress: 81/89 stitched\n",
      "segmentation took 0.18297123908996582 seconds\n",
      "patching took 0.03506731986999512 seconds\n",
      "stitching took 0.0948190689086914 seconds\n",
      "\n",
      "\n",
      "progress: 0.11, 116/1027\n",
      "processing C3L-02130-22.svs\n",
      "Creating patches for:  C3L-02130-22 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 53330 8551 28299 23444\n",
      "Contour Area: 445616976.5\n",
      "Extracted 33 coordinates\n",
      "Bounding Box: 5666 5668 27818 23316\n",
      "Contour Area: 438388511.0\n",
      "Extracted 33 coordinates\n",
      "start stitching C3L-02130-22\n",
      "original size: 87647 x 36510\n",
      "downscaled size for stiching: 2738 x 1140\n",
      "number of patches: 66\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/66 stitched\n",
      "progress: 7/66 stitched\n",
      "progress: 14/66 stitched\n",
      "progress: 21/66 stitched\n",
      "progress: 28/66 stitched\n",
      "progress: 35/66 stitched\n",
      "progress: 42/66 stitched\n",
      "progress: 49/66 stitched\n",
      "progress: 56/66 stitched\n",
      "progress: 63/66 stitched\n",
      "segmentation took 0.14310145378112793 seconds\n",
      "patching took 0.03625082969665527 seconds\n",
      "stitching took 0.07091379165649414 seconds\n",
      "\n",
      "\n",
      "progress: 0.11, 117/1027\n",
      "processing C3L-02130-23.svs\n",
      "Creating patches for:  C3L-02130-23 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 53901 8482 36650 23207\n",
      "Contour Area: 535362575.0\n",
      "Extracted 42 coordinates\n",
      "Bounding Box: 4257 4353 37226 24743\n",
      "Contour Area: 552869796.0\n",
      "Extracted 41 coordinates\n",
      "start stitching C3L-02130-23\n",
      "original size: 93623 x 35145\n",
      "downscaled size for stiching: 2925 x 1098\n",
      "number of patches: 83\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/83 stitched\n",
      "progress: 9/83 stitched\n",
      "progress: 18/83 stitched\n",
      "progress: 27/83 stitched\n",
      "progress: 36/83 stitched\n",
      "progress: 45/83 stitched\n",
      "progress: 54/83 stitched\n",
      "progress: 63/83 stitched\n",
      "progress: 72/83 stitched\n",
      "progress: 81/83 stitched\n",
      "segmentation took 0.14423561096191406 seconds\n",
      "patching took 0.033892154693603516 seconds\n",
      "stitching took 0.08689212799072266 seconds\n",
      "\n",
      "\n",
      "progress: 0.11, 118/1027\n",
      "processing C3L-02130-24.svs\n",
      "Creating patches for:  C3L-02130-24 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 51501 4832 31466 28737\n",
      "Contour Area: 580746000.0\n",
      "Extracted 42 coordinates\n",
      "Bounding Box: 4769 3616 31593 28993\n",
      "Contour Area: 590623536.0\n",
      "Extracted 44 coordinates\n",
      "start stitching C3L-02130-24\n",
      "original size: 85655 x 37344\n",
      "downscaled size for stiching: 2676 x 1167\n",
      "number of patches: 86\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/86 stitched\n",
      "progress: 9/86 stitched\n",
      "progress: 18/86 stitched\n",
      "progress: 27/86 stitched\n",
      "progress: 36/86 stitched\n",
      "progress: 45/86 stitched\n",
      "progress: 54/86 stitched\n",
      "progress: 63/86 stitched\n",
      "progress: 72/86 stitched\n",
      "progress: 81/86 stitched\n",
      "segmentation took 0.14152169227600098 seconds\n",
      "patching took 0.033545494079589844 seconds\n",
      "stitching took 0.08954715728759766 seconds\n",
      "\n",
      "\n",
      "progress: 0.12, 119/1027\n",
      "processing C3L-02164-21.svs\n",
      "Creating patches for:  C3L-02164-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3552 2592 27927 26104\n",
      "Contour Area: 337590571.0\n",
      "Extracted 29 coordinates\n",
      "start stitching C3L-02164-21\n",
      "original size: 33863 x 30136\n",
      "downscaled size for stiching: 2116 x 1883\n",
      "number of patches: 29\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/29 stitched\n",
      "progress: 3/29 stitched\n",
      "progress: 6/29 stitched\n",
      "progress: 9/29 stitched\n",
      "progress: 12/29 stitched\n",
      "progress: 15/29 stitched\n",
      "progress: 18/29 stitched\n",
      "progress: 21/29 stitched\n",
      "progress: 24/29 stitched\n",
      "progress: 27/29 stitched\n",
      "segmentation took 0.18608665466308594 seconds\n",
      "patching took 0.018578529357910156 seconds\n",
      "stitching took 0.11856317520141602 seconds\n",
      "\n",
      "\n",
      "progress: 0.12, 120/1027\n",
      "processing C3L-02164-22.svs\n",
      "Creating patches for:  C3L-02164-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 2161 3154 28478 17164\n",
      "Contour Area: 328356521.5\n",
      "Extracted 26 coordinates\n",
      "start stitching C3L-02164-22\n",
      "original size: 31871 x 22207\n",
      "downscaled size for stiching: 1991 x 1387\n",
      "number of patches: 26\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4098, 4098)x(4098, 4098)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/26 stitched\n",
      "progress: 3/26 stitched\n",
      "progress: 6/26 stitched\n",
      "progress: 9/26 stitched\n",
      "progress: 12/26 stitched\n",
      "progress: 15/26 stitched\n",
      "progress: 18/26 stitched\n",
      "progress: 21/26 stitched\n",
      "progress: 24/26 stitched\n",
      "segmentation took 0.12984085083007812 seconds\n",
      "patching took 0.017020463943481445 seconds\n",
      "stitching took 0.10075998306274414 seconds\n",
      "\n",
      "\n",
      "progress: 0.12, 121/1027\n",
      "processing C3L-02164-23.svs\n",
      "Creating patches for:  C3L-02164-23 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 2384 2353 30839 17370\n",
      "Contour Area: 382396932.0\n",
      "Extracted 28 coordinates\n",
      "Bounding Box: 31989 0 5842 7973\n",
      "Contour Area: 35387618.0\n",
      "Extracted 4 coordinates\n",
      "start stitching C3L-02164-23\n",
      "original size: 37847 x 20683\n",
      "downscaled size for stiching: 2365 x 1292\n",
      "number of patches: 32\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/32 stitched\n",
      "progress: 4/32 stitched\n",
      "progress: 8/32 stitched\n",
      "progress: 12/32 stitched\n",
      "progress: 16/32 stitched\n",
      "progress: 20/32 stitched\n",
      "progress: 24/32 stitched\n",
      "progress: 28/32 stitched\n",
      "segmentation took 0.14341402053833008 seconds\n",
      "patching took 0.036149024963378906 seconds\n",
      "stitching took 0.12049078941345215 seconds\n",
      "\n",
      "\n",
      "progress: 0.12, 122/1027\n",
      "processing C3L-02165-21.svs\n",
      "Creating patches for:  C3L-02165-21 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 3424 7427 51856 22058\n",
      "Contour Area: 847768149.0\n",
      "Extracted 60 coordinates\n",
      "Bounding Box: 60432 2529 49807 22026\n",
      "Contour Area: 821554342.0\n",
      "Extracted 56 coordinates\n",
      "start stitching C3L-02165-21\n",
      "original size: 111551 x 32142\n",
      "downscaled size for stiching: 3485 x 1004\n",
      "number of patches: 116\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/116 stitched\n",
      "progress: 12/116 stitched\n",
      "progress: 24/116 stitched\n",
      "progress: 36/116 stitched\n",
      "progress: 48/116 stitched\n",
      "progress: 60/116 stitched\n",
      "progress: 72/116 stitched\n",
      "progress: 84/116 stitched\n",
      "progress: 96/116 stitched\n",
      "progress: 108/116 stitched\n",
      "segmentation took 0.1711885929107666 seconds\n",
      "patching took 0.03784465789794922 seconds\n",
      "stitching took 0.11918401718139648 seconds\n",
      "\n",
      "\n",
      "progress: 0.12, 123/1027\n",
      "processing C3L-02165-22.svs\n",
      "Creating patches for:  C3L-02165-22 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 3553 4995 42797 18190\n",
      "Contour Area: 593040318.0\n",
      "Extracted 42 coordinates\n",
      "Bounding Box: 57169 3202 44494 19342\n",
      "Contour Area: 615999185.0\n",
      "Extracted 44 coordinates\n",
      "start stitching C3L-02165-22\n",
      "original size: 103583 x 25490\n",
      "downscaled size for stiching: 3236 x 796\n",
      "number of patches: 86\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/86 stitched\n",
      "progress: 9/86 stitched\n",
      "progress: 18/86 stitched\n",
      "progress: 27/86 stitched\n",
      "progress: 36/86 stitched\n",
      "progress: 45/86 stitched\n",
      "progress: 54/86 stitched\n",
      "progress: 63/86 stitched\n",
      "progress: 72/86 stitched\n",
      "progress: 81/86 stitched\n",
      "segmentation took 0.11950254440307617 seconds\n",
      "patching took 0.03540849685668945 seconds\n",
      "stitching took 0.08990693092346191 seconds\n",
      "\n",
      "\n",
      "progress: 0.12, 124/1027\n",
      "processing C3L-02165-23.svs\n",
      "Creating patches for:  C3L-02165-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 1440 3360 44743 17203\n",
      "Contour Area: 582992124.0\n",
      "Extracted 43 coordinates\n",
      "start stitching C3L-02165-23\n",
      "original size: 49799 x 23667\n",
      "downscaled size for stiching: 3112 x 1479\n",
      "number of patches: 43\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/43 stitched\n",
      "progress: 5/43 stitched\n",
      "progress: 10/43 stitched\n",
      "progress: 15/43 stitched\n",
      "progress: 20/43 stitched\n",
      "progress: 25/43 stitched\n",
      "progress: 30/43 stitched\n",
      "progress: 35/43 stitched\n",
      "progress: 40/43 stitched\n",
      "segmentation took 0.1803290843963623 seconds\n",
      "patching took 0.019621849060058594 seconds\n",
      "stitching took 0.1662912368774414 seconds\n",
      "\n",
      "\n",
      "progress: 0.12, 125/1027\n",
      "processing C3L-02168-21.svs\n",
      "Creating patches for:  C3L-02168-21 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 2176 3968 25257 18469\n",
      "Contour Area: 317010022.0\n",
      "Extracted 25 coordinates\n",
      "Bounding Box: 51311 0 26344 20837\n",
      "Contour Area: 358611734.0\n",
      "Extracted 27 coordinates\n",
      "start stitching C3L-02168-21\n",
      "original size: 77687 x 23909\n",
      "downscaled size for stiching: 2427 x 747\n",
      "number of patches: 52\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/52 stitched\n",
      "progress: 6/52 stitched\n",
      "progress: 12/52 stitched\n",
      "progress: 18/52 stitched\n",
      "progress: 24/52 stitched\n",
      "progress: 30/52 stitched\n",
      "progress: 36/52 stitched\n",
      "progress: 42/52 stitched\n",
      "progress: 48/52 stitched\n",
      "segmentation took 0.09227180480957031 seconds\n",
      "patching took 0.03740286827087402 seconds\n",
      "stitching took 0.054428815841674805 seconds\n",
      "\n",
      "\n",
      "progress: 0.12, 126/1027\n",
      "processing C3L-02168-22.svs\n",
      "Creating patches for:  C3L-02168-22 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 2048 8262 31463 31704\n",
      "Contour Area: 667045047.0\n",
      "Extracted 50 coordinates\n",
      "Bounding Box: 52585 1697 28902 32761\n",
      "Contour Area: 659556538.0\n",
      "Extracted 46 coordinates\n",
      "start stitching C3L-02168-22\n",
      "original size: 83663 x 41086\n",
      "downscaled size for stiching: 2614 x 1283\n",
      "number of patches: 96\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/96 stitched\n",
      "progress: 10/96 stitched\n",
      "progress: 20/96 stitched\n",
      "progress: 30/96 stitched\n",
      "progress: 40/96 stitched\n",
      "progress: 50/96 stitched\n",
      "progress: 60/96 stitched\n",
      "progress: 70/96 stitched\n",
      "progress: 80/96 stitched\n",
      "progress: 90/96 stitched\n",
      "segmentation took 0.15940403938293457 seconds\n",
      "patching took 0.035874366760253906 seconds\n",
      "stitching took 0.09768033027648926 seconds\n",
      "\n",
      "\n",
      "progress: 0.12, 127/1027\n",
      "processing C3L-02168-23.svs\n",
      "Creating patches for:  C3L-02168-23 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 54771 0 30092 28406\n",
      "Contour Area: 683776789.0\n",
      "Extracted 49 coordinates\n",
      "Bounding Box: 0 0 34541 29687\n",
      "Contour Area: 746676917.0\n",
      "Extracted 54 coordinates\n",
      "start stitching C3L-02168-23\n",
      "original size: 87647 x 29719\n",
      "downscaled size for stiching: 2738 x 928\n",
      "number of patches: 103\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/103 stitched\n",
      "progress: 11/103 stitched\n",
      "progress: 22/103 stitched\n",
      "progress: 33/103 stitched\n",
      "progress: 44/103 stitched\n",
      "progress: 55/103 stitched\n",
      "progress: 66/103 stitched\n",
      "progress: 77/103 stitched\n",
      "progress: 88/103 stitched\n",
      "progress: 99/103 stitched\n",
      "segmentation took 0.12781691551208496 seconds\n",
      "patching took 0.03681516647338867 seconds\n",
      "stitching took 0.10498499870300293 seconds\n",
      "\n",
      "\n",
      "progress: 0.12, 128/1027\n",
      "processing C3L-02170-21.svs\n",
      "Creating patches for:  C3L-02170-21 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 2752 5029 27465 21144\n",
      "Contour Area: 414010103.0\n",
      "Extracted 32 coordinates\n",
      "Bounding Box: 50030 993 27145 20952\n",
      "Contour Area: 403532758.0\n",
      "Extracted 29 coordinates\n",
      "start stitching C3L-02170-21\n",
      "original size: 77687 x 27134\n",
      "downscaled size for stiching: 2427 x 847\n",
      "number of patches: 61\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/61 stitched\n",
      "progress: 7/61 stitched\n",
      "progress: 14/61 stitched\n",
      "progress: 21/61 stitched\n",
      "progress: 28/61 stitched\n",
      "progress: 35/61 stitched\n",
      "progress: 42/61 stitched\n",
      "progress: 49/61 stitched\n",
      "progress: 56/61 stitched\n",
      "segmentation took 0.09676718711853027 seconds\n",
      "patching took 0.03533172607421875 seconds\n",
      "stitching took 0.06511998176574707 seconds\n",
      "\n",
      "\n",
      "progress: 0.13, 129/1027\n",
      "processing C3L-02170-22.svs\n",
      "Creating patches for:  C3L-02170-22 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 4320 2498 31944 26006\n",
      "Contour Area: 528433607.0\n",
      "Extracted 39 coordinates\n",
      "Bounding Box: 41864 2402 31783 26198\n",
      "Contour Area: 514537413.0\n",
      "Extracted 37 coordinates\n",
      "start stitching C3L-02170-22\n",
      "original size: 75695 x 29881\n",
      "downscaled size for stiching: 2365 x 933\n",
      "number of patches: 76\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/76 stitched\n",
      "progress: 8/76 stitched\n",
      "progress: 16/76 stitched\n",
      "progress: 24/76 stitched\n",
      "progress: 32/76 stitched\n",
      "progress: 40/76 stitched\n",
      "progress: 48/76 stitched\n",
      "progress: 56/76 stitched\n",
      "progress: 64/76 stitched\n",
      "progress: 72/76 stitched\n",
      "segmentation took 0.10251617431640625 seconds\n",
      "patching took 0.035722970962524414 seconds\n",
      "stitching took 0.08097362518310547 seconds\n",
      "\n",
      "\n",
      "progress: 0.13, 130/1027\n",
      "processing C3L-02170-23.svs\n",
      "Creating patches for:  C3L-02170-23 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 2880 2848 35275 33569\n",
      "Contour Area: 682794208.0\n",
      "Extracted 48 coordinates\n",
      "Bounding Box: 48493 2080 35594 33281\n",
      "Contour Area: 702977264.0\n",
      "Extracted 50 coordinates\n",
      "start stitching C3L-02170-23\n",
      "original size: 85655 x 36928\n",
      "downscaled size for stiching: 2676 x 1154\n",
      "number of patches: 98\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/98 stitched\n",
      "progress: 10/98 stitched\n",
      "progress: 20/98 stitched\n",
      "progress: 30/98 stitched\n",
      "progress: 40/98 stitched\n",
      "progress: 50/98 stitched\n",
      "progress: 60/98 stitched\n",
      "progress: 70/98 stitched\n",
      "progress: 80/98 stitched\n",
      "progress: 90/98 stitched\n",
      "segmentation took 0.1478579044342041 seconds\n",
      "patching took 0.035297393798828125 seconds\n",
      "stitching took 0.0989527702331543 seconds\n",
      "\n",
      "\n",
      "progress: 0.13, 131/1027\n",
      "processing C3L-02219-21.svs\n",
      "Creating patches for:  C3L-02219-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4609 5561 18589 14356\n",
      "Contour Area: 143166760.0\n",
      "Extracted 44 coordinates\n",
      "start stitching C3L-02219-21\n",
      "original size: 27887 x 23061\n",
      "downscaled size for stiching: 3485 x 2882\n",
      "number of patches: 44\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/44 stitched\n",
      "progress: 5/44 stitched\n",
      "progress: 10/44 stitched\n",
      "progress: 15/44 stitched\n",
      "progress: 20/44 stitched\n",
      "progress: 25/44 stitched\n",
      "progress: 30/44 stitched\n",
      "progress: 35/44 stitched\n",
      "progress: 40/44 stitched\n",
      "segmentation took 0.35132384300231934 seconds\n",
      "patching took 0.0165865421295166 seconds\n",
      "stitching took 0.19906210899353027 seconds\n",
      "\n",
      "\n",
      "progress: 0.13, 132/1027\n",
      "processing C3L-02219-22.svs\n",
      "Creating patches for:  C3L-02219-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3217 3656 11933 9889\n",
      "Contour Area: 69280068.0\n",
      "Extracted 20 coordinates\n",
      "start stitching C3L-02219-22\n",
      "original size: 17927 x 15592\n",
      "downscaled size for stiching: 2240 x 1949\n",
      "number of patches: 20\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/20 stitched\n",
      "progress: 2/20 stitched\n",
      "progress: 4/20 stitched\n",
      "progress: 6/20 stitched\n",
      "progress: 8/20 stitched\n",
      "progress: 10/20 stitched\n",
      "progress: 12/20 stitched\n",
      "progress: 14/20 stitched\n",
      "progress: 16/20 stitched\n",
      "progress: 18/20 stitched\n",
      "segmentation took 0.14981651306152344 seconds\n",
      "patching took 0.01850128173828125 seconds\n",
      "stitching took 0.0866847038269043 seconds\n",
      "\n",
      "\n",
      "progress: 0.13, 133/1027\n",
      "processing C3L-02345-21.svs\n",
      "Creating patches for:  C3L-02345-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4787 3777 16459 17946\n",
      "Contour Area: 168985459.0\n",
      "Extracted 14 coordinates\n",
      "start stitching C3L-02345-21\n",
      "original size: 23903 x 24156\n",
      "downscaled size for stiching: 1493 x 1509\n",
      "number of patches: 14\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4098, 4098)x(4098, 4098)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/14 stitched\n",
      "progress: 2/14 stitched\n",
      "progress: 4/14 stitched\n",
      "progress: 6/14 stitched\n",
      "progress: 8/14 stitched\n",
      "progress: 10/14 stitched\n",
      "progress: 12/14 stitched\n",
      "segmentation took 0.10052871704101562 seconds\n",
      "patching took 0.018520116806030273 seconds\n",
      "stitching took 0.05780935287475586 seconds\n",
      "\n",
      "\n",
      "progress: 0.13, 134/1027\n",
      "processing C3L-02345-22.svs\n",
      "Creating patches for:  C3L-02345-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3753 3416 17190 14498\n",
      "Contour Area: 189937016.0\n",
      "Extracted 55 coordinates\n",
      "start stitching C3L-02345-22\n",
      "original size: 23903 x 20098\n",
      "downscaled size for stiching: 2987 x 2512\n",
      "number of patches: 55\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/55 stitched\n",
      "progress: 6/55 stitched\n",
      "progress: 12/55 stitched\n",
      "progress: 18/55 stitched\n",
      "progress: 24/55 stitched\n",
      "progress: 30/55 stitched\n",
      "progress: 36/55 stitched\n",
      "progress: 42/55 stitched\n",
      "progress: 48/55 stitched\n",
      "progress: 54/55 stitched\n",
      "segmentation took 0.3918607234954834 seconds\n",
      "patching took 0.018919944763183594 seconds\n",
      "stitching took 0.22406792640686035 seconds\n",
      "\n",
      "\n",
      "progress: 0.13, 135/1027\n",
      "processing C3L-02345-23.svs\n",
      "Creating patches for:  C3L-02345-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4433 3632 13349 18849\n",
      "Contour Area: 179328376.0\n",
      "Extracted 14 coordinates\n",
      "start stitching C3L-02345-23\n",
      "original size: 21911 x 27152\n",
      "downscaled size for stiching: 1369 x 1697\n",
      "number of patches: 14\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/14 stitched\n",
      "progress: 2/14 stitched\n",
      "progress: 4/14 stitched\n",
      "progress: 6/14 stitched\n",
      "progress: 8/14 stitched\n",
      "progress: 10/14 stitched\n",
      "progress: 12/14 stitched\n",
      "segmentation took 0.1137394905090332 seconds\n",
      "patching took 0.016219615936279297 seconds\n",
      "stitching took 0.05706381797790527 seconds\n",
      "\n",
      "\n",
      "progress: 0.13, 136/1027\n",
      "processing C3L-02348-21.svs\n",
      "Creating patches for:  C3L-02348-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4658 3009 23116 16888\n",
      "Contour Area: 223229579.5\n",
      "Extracted 18 coordinates\n",
      "start stitching C3L-02348-21\n",
      "original size: 31871 x 23194\n",
      "downscaled size for stiching: 1991 x 1449\n",
      "number of patches: 18\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/18 stitched\n",
      "progress: 2/18 stitched\n",
      "progress: 4/18 stitched\n",
      "progress: 6/18 stitched\n",
      "progress: 8/18 stitched\n",
      "progress: 10/18 stitched\n",
      "progress: 12/18 stitched\n",
      "progress: 14/18 stitched\n",
      "progress: 16/18 stitched\n",
      "segmentation took 0.1301877498626709 seconds\n",
      "patching took 0.016555309295654297 seconds\n",
      "stitching took 0.07450675964355469 seconds\n",
      "\n",
      "\n",
      "progress: 0.13, 137/1027\n",
      "processing C3L-02348-22.svs\n",
      "Creating patches for:  C3L-02348-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4993 3280 25606 20753\n",
      "Contour Area: 274028240.0\n",
      "Extracted 20 coordinates\n",
      "start stitching C3L-02348-22\n",
      "original size: 33863 x 27648\n",
      "downscaled size for stiching: 2116 x 1728\n",
      "number of patches: 20\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/20 stitched\n",
      "progress: 2/20 stitched\n",
      "progress: 4/20 stitched\n",
      "progress: 6/20 stitched\n",
      "progress: 8/20 stitched\n",
      "progress: 10/20 stitched\n",
      "progress: 12/20 stitched\n",
      "progress: 14/20 stitched\n",
      "progress: 16/20 stitched\n",
      "progress: 18/20 stitched\n",
      "segmentation took 0.16175389289855957 seconds\n",
      "patching took 0.017214298248291016 seconds\n",
      "stitching took 0.07990336418151855 seconds\n",
      "\n",
      "\n",
      "progress: 0.13, 138/1027\n",
      "processing C3L-02349-21.svs\n",
      "Creating patches for:  C3L-02349-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3913 3288 17997 16091\n",
      "Contour Area: 184190078.0\n",
      "Extracted 54 coordinates\n",
      "start stitching C3L-02349-21\n",
      "original size: 25895 x 22331\n",
      "downscaled size for stiching: 3236 x 2791\n",
      "number of patches: 54\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/54 stitched\n",
      "progress: 6/54 stitched\n",
      "progress: 12/54 stitched\n",
      "progress: 18/54 stitched\n",
      "progress: 24/54 stitched\n",
      "progress: 30/54 stitched\n",
      "progress: 36/54 stitched\n",
      "progress: 42/54 stitched\n",
      "progress: 48/54 stitched\n",
      "segmentation took 0.4738452434539795 seconds\n",
      "patching took 0.017894506454467773 seconds\n",
      "stitching took 0.21814608573913574 seconds\n",
      "\n",
      "\n",
      "progress: 0.14, 139/1027\n",
      "processing C3L-02358-21.svs\n",
      "Creating patches for:  C3L-02358-21 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 13811 9074 12020 15524\n",
      "Contour Area: 49039466.0\n",
      "Extracted 6 coordinates\n",
      "Bounding Box: 4289 3520 15924 14404\n",
      "Contour Area: 111744125.0\n",
      "Extracted 11 coordinates\n",
      "start stitching C3L-02358-21\n",
      "original size: 29879 x 26934\n",
      "downscaled size for stiching: 1867 x 1683\n",
      "number of patches: 17\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/17 stitched\n",
      "progress: 2/17 stitched\n",
      "progress: 4/17 stitched\n",
      "progress: 6/17 stitched\n",
      "progress: 8/17 stitched\n",
      "progress: 10/17 stitched\n",
      "progress: 12/17 stitched\n",
      "progress: 14/17 stitched\n",
      "progress: 16/17 stitched\n",
      "segmentation took 0.13469862937927246 seconds\n",
      "patching took 0.034790992736816406 seconds\n",
      "stitching took 0.07021760940551758 seconds\n",
      "\n",
      "\n",
      "progress: 0.14, 140/1027\n",
      "processing C3L-02358-22.svs\n",
      "Creating patches for:  C3L-02358-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3617 2928 13413 15155\n",
      "Contour Area: 127140779.0\n",
      "Extracted 37 coordinates\n",
      "start stitching C3L-02358-22\n",
      "original size: 19919 x 20707\n",
      "downscaled size for stiching: 2489 x 2588\n",
      "number of patches: 37\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/37 stitched\n",
      "progress: 4/37 stitched\n",
      "progress: 8/37 stitched\n",
      "progress: 12/37 stitched\n",
      "progress: 16/37 stitched\n",
      "progress: 20/37 stitched\n",
      "progress: 24/37 stitched\n",
      "progress: 28/37 stitched\n",
      "progress: 32/37 stitched\n",
      "progress: 36/37 stitched\n",
      "segmentation took 0.22900104522705078 seconds\n",
      "patching took 0.01878666877746582 seconds\n",
      "stitching took 0.15709376335144043 seconds\n",
      "\n",
      "\n",
      "progress: 0.14, 141/1027\n",
      "processing C3L-02358-23.svs\n",
      "Creating patches for:  C3L-02358-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4130 5264 17724 16481\n",
      "Contour Area: 191763288.0\n",
      "Extracted 16 coordinates\n",
      "start stitching C3L-02358-23\n",
      "original size: 23903 x 24896\n",
      "downscaled size for stiching: 1493 x 1556\n",
      "number of patches: 16\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/16 stitched\n",
      "progress: 2/16 stitched\n",
      "progress: 4/16 stitched\n",
      "progress: 6/16 stitched\n",
      "progress: 8/16 stitched\n",
      "progress: 10/16 stitched\n",
      "progress: 12/16 stitched\n",
      "progress: 14/16 stitched\n",
      "segmentation took 0.10384249687194824 seconds\n",
      "patching took 0.017407655715942383 seconds\n",
      "stitching took 0.06468629837036133 seconds\n",
      "\n",
      "\n",
      "progress: 0.14, 142/1027\n",
      "processing C3L-02365-21.svs\n",
      "Creating patches for:  C3L-02365-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3056 2560 20351 16465\n",
      "Contour Area: 212657248.0\n",
      "Extracted 60 coordinates\n",
      "start stitching C3L-02365-21\n",
      "original size: 25895 x 21745\n",
      "downscaled size for stiching: 3236 x 2718\n",
      "number of patches: 60\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/60 stitched\n",
      "progress: 6/60 stitched\n",
      "progress: 12/60 stitched\n",
      "progress: 18/60 stitched\n",
      "progress: 24/60 stitched\n",
      "progress: 30/60 stitched\n",
      "progress: 36/60 stitched\n",
      "progress: 42/60 stitched\n",
      "progress: 48/60 stitched\n",
      "progress: 54/60 stitched\n",
      "segmentation took 0.4771261215209961 seconds\n",
      "patching took 0.017319917678833008 seconds\n",
      "stitching took 0.23764491081237793 seconds\n",
      "\n",
      "\n",
      "progress: 0.14, 143/1027\n",
      "processing C3L-02365-24.svs\n",
      "Creating patches for:  C3L-02365-24 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 2952 3648 19319 10715\n",
      "Contour Area: 144673156.0\n",
      "Extracted 41 coordinates\n",
      "start stitching C3L-02365-24\n",
      "original size: 25895 x 16291\n",
      "downscaled size for stiching: 3236 x 2036\n",
      "number of patches: 41\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/41 stitched\n",
      "progress: 5/41 stitched\n",
      "progress: 10/41 stitched\n",
      "progress: 15/41 stitched\n",
      "progress: 20/41 stitched\n",
      "progress: 25/41 stitched\n",
      "progress: 30/41 stitched\n",
      "progress: 35/41 stitched\n",
      "progress: 40/41 stitched\n",
      "segmentation took 0.2309281826019287 seconds\n",
      "patching took 0.02203989028930664 seconds\n",
      "stitching took 0.17141938209533691 seconds\n",
      "\n",
      "\n",
      "progress: 0.14, 144/1027\n",
      "processing C3L-02365-25.svs\n",
      "Creating patches for:  C3L-02365-25 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 2984 2608 19095 10236\n",
      "Contour Area: 132264904.0\n",
      "Extracted 38 coordinates\n",
      "start stitching C3L-02365-25\n",
      "original size: 23903 x 14804\n",
      "downscaled size for stiching: 2987 x 1850\n",
      "number of patches: 38\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/38 stitched\n",
      "progress: 4/38 stitched\n",
      "progress: 8/38 stitched\n",
      "progress: 12/38 stitched\n",
      "progress: 16/38 stitched\n",
      "progress: 20/38 stitched\n",
      "progress: 24/38 stitched\n",
      "progress: 28/38 stitched\n",
      "progress: 32/38 stitched\n",
      "progress: 36/38 stitched\n",
      "segmentation took 0.20020818710327148 seconds\n",
      "patching took 0.021235942840576172 seconds\n",
      "stitching took 0.15517973899841309 seconds\n",
      "\n",
      "\n",
      "progress: 0.14, 145/1027\n",
      "processing C3L-02508-22.svs\n",
      "Creating patches for:  C3L-02508-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3441 4272 16637 11722\n",
      "Contour Area: 127423631.0\n",
      "Extracted 39 coordinates\n",
      "start stitching C3L-02508-22\n",
      "original size: 23903 x 20354\n",
      "downscaled size for stiching: 2987 x 2544\n",
      "number of patches: 39\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/39 stitched\n",
      "progress: 4/39 stitched\n",
      "progress: 8/39 stitched\n",
      "progress: 12/39 stitched\n",
      "progress: 16/39 stitched\n",
      "progress: 20/39 stitched\n",
      "progress: 24/39 stitched\n",
      "progress: 28/39 stitched\n",
      "progress: 32/39 stitched\n",
      "progress: 36/39 stitched\n",
      "segmentation took 0.25845980644226074 seconds\n",
      "patching took 0.02104473114013672 seconds\n",
      "stitching took 0.16863346099853516 seconds\n",
      "\n",
      "\n",
      "progress: 0.14, 146/1027\n",
      "processing C3L-02508-23.svs\n",
      "Creating patches for:  C3L-02508-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4841 5713 12837 9715\n",
      "Contour Area: 70351005.0\n",
      "Extracted 23 coordinates\n",
      "start stitching C3L-02508-23\n",
      "original size: 21911 x 18876\n",
      "downscaled size for stiching: 2738 x 2359\n",
      "number of patches: 23\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/23 stitched\n",
      "progress: 3/23 stitched\n",
      "progress: 6/23 stitched\n",
      "progress: 9/23 stitched\n",
      "progress: 12/23 stitched\n",
      "progress: 15/23 stitched\n",
      "progress: 18/23 stitched\n",
      "progress: 21/23 stitched\n",
      "segmentation took 0.19705963134765625 seconds\n",
      "patching took 0.01693272590637207 seconds\n",
      "stitching took 0.10419368743896484 seconds\n",
      "\n",
      "\n",
      "progress: 0.14, 147/1027\n",
      "processing C3L-02513-21.svs\n",
      "Creating patches for:  C3L-02513-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 0 0 22363 17308\n",
      "Contour Area: 237428628.0\n",
      "Extracted 242 coordinates\n",
      "start stitching C3L-02513-21\n",
      "original size: 22395 x 17340\n",
      "downscaled size for stiching: 699 x 541\n",
      "number of patches: 242\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (1024, 1024)x(1024, 1024)\n",
      "downscaled patch size: 32x32\n",
      "progress: 0/242 stitched\n",
      "progress: 25/242 stitched\n",
      "progress: 50/242 stitched\n",
      "progress: 75/242 stitched\n",
      "progress: 100/242 stitched\n",
      "progress: 125/242 stitched\n",
      "progress: 150/242 stitched\n",
      "progress: 175/242 stitched\n",
      "progress: 200/242 stitched\n",
      "progress: 225/242 stitched\n",
      "segmentation took 0.02407360076904297 seconds\n",
      "patching took 0.020830869674682617 seconds\n",
      "stitching took 0.022500276565551758 seconds\n",
      "\n",
      "\n",
      "progress: 0.14, 148/1027\n",
      "processing C3L-02513-22.svs\n",
      "Creating patches for:  C3L-02513-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 0 0 21049 15388\n",
      "Contour Area: 209905497.0\n",
      "Extracted 212 coordinates\n",
      "start stitching C3L-02513-22\n",
      "original size: 21081 x 15420\n",
      "downscaled size for stiching: 658 x 481\n",
      "number of patches: 212\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (1024, 1024)x(1024, 1024)\n",
      "downscaled patch size: 32x32\n",
      "progress: 0/212 stitched\n",
      "progress: 22/212 stitched\n",
      "progress: 44/212 stitched\n",
      "progress: 66/212 stitched\n",
      "progress: 88/212 stitched\n",
      "progress: 110/212 stitched\n",
      "progress: 132/212 stitched\n",
      "progress: 154/212 stitched\n",
      "progress: 176/212 stitched\n",
      "progress: 198/212 stitched\n",
      "segmentation took 0.019913196563720703 seconds\n",
      "patching took 0.017383098602294922 seconds\n",
      "stitching took 0.019774436950683594 seconds\n",
      "\n",
      "\n",
      "progress: 0.15, 149/1027\n",
      "processing C3L-02513-23.svs\n",
      "Creating patches for:  C3L-02513-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 0 0 17108 23066\n",
      "Contour Area: 248865862.0\n",
      "Extracted 259 coordinates\n",
      "start stitching C3L-02513-23\n",
      "original size: 17140 x 23098\n",
      "downscaled size for stiching: 535 x 721\n",
      "number of patches: 259\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (1024, 1024)x(1024, 1024)\n",
      "downscaled patch size: 32x32\n",
      "progress: 0/259 stitched\n",
      "progress: 26/259 stitched\n",
      "progress: 52/259 stitched\n",
      "progress: 78/259 stitched\n",
      "progress: 104/259 stitched\n",
      "progress: 130/259 stitched\n",
      "progress: 156/259 stitched\n",
      "progress: 182/259 stitched\n",
      "progress: 208/259 stitched\n",
      "progress: 234/259 stitched\n",
      "segmentation took 0.02890181541442871 seconds\n",
      "patching took 0.01694321632385254 seconds\n",
      "stitching took 0.023958683013916016 seconds\n",
      "\n",
      "\n",
      "progress: 0.15, 150/1027\n",
      "processing C3L-02515-21.svs\n",
      "Creating patches for:  C3L-02515-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4164 5220 17430 21619\n",
      "Contour Area: 226574751.0\n",
      "Extracted 238 coordinates\n",
      "start stitching C3L-02515-21\n",
      "original size: 25022 x 29817\n",
      "downscaled size for stiching: 781 x 931\n",
      "number of patches: 238\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (1024, 1024)x(1024, 1024)\n",
      "downscaled patch size: 32x32\n",
      "progress: 0/238 stitched\n",
      "progress: 24/238 stitched\n",
      "progress: 48/238 stitched\n",
      "progress: 72/238 stitched\n",
      "progress: 96/238 stitched\n",
      "progress: 120/238 stitched\n",
      "progress: 144/238 stitched\n",
      "progress: 168/238 stitched\n",
      "progress: 192/238 stitched\n",
      "progress: 216/238 stitched\n",
      "segmentation took 0.032079458236694336 seconds\n",
      "patching took 0.017070293426513672 seconds\n",
      "stitching took 0.02323436737060547 seconds\n",
      "\n",
      "\n",
      "progress: 0.15, 151/1027\n",
      "processing C3L-02515-22.svs\n",
      "Creating patches for:  C3L-02515-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 0 288 28259 23514\n",
      "Contour Area: 341170579.0\n",
      "Extracted 346 coordinates\n",
      "start stitching C3L-02515-22\n",
      "original size: 28963 x 24058\n",
      "downscaled size for stiching: 905 x 751\n",
      "number of patches: 346\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (1024, 1024)x(1024, 1024)\n",
      "downscaled patch size: 32x32\n",
      "progress: 0/346 stitched\n",
      "progress: 35/346 stitched\n",
      "progress: 70/346 stitched\n",
      "progress: 105/346 stitched\n",
      "progress: 140/346 stitched\n",
      "progress: 175/346 stitched\n",
      "progress: 210/346 stitched\n",
      "progress: 245/346 stitched\n",
      "progress: 280/346 stitched\n",
      "progress: 315/346 stitched\n",
      "segmentation took 0.03772115707397461 seconds\n",
      "patching took 0.018576383590698242 seconds\n",
      "stitching took 0.03221869468688965 seconds\n",
      "\n",
      "\n",
      "progress: 0.15, 152/1027\n",
      "processing C3L-02515-23.svs\n",
      "Creating patches for:  C3L-02515-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 1280 608 34219 19451\n",
      "Contour Area: 383971550.0\n",
      "Extracted 393 coordinates\n",
      "start stitching C3L-02515-23\n",
      "original size: 35531 x 20219\n",
      "downscaled size for stiching: 1110 x 631\n",
      "number of patches: 393\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (1024, 1024)x(1024, 1024)\n",
      "downscaled patch size: 32x32\n",
      "progress: 0/393 stitched\n",
      "progress: 40/393 stitched\n",
      "progress: 80/393 stitched\n",
      "progress: 120/393 stitched\n",
      "progress: 160/393 stitched\n",
      "progress: 200/393 stitched\n",
      "progress: 240/393 stitched\n",
      "progress: 280/393 stitched\n",
      "progress: 320/393 stitched\n",
      "progress: 360/393 stitched\n",
      "segmentation took 0.062169790267944336 seconds\n",
      "patching took 0.01841139793395996 seconds\n",
      "stitching took 0.035234928131103516 seconds\n",
      "\n",
      "\n",
      "progress: 0.15, 153/1027\n",
      "processing C3L-02546-21.svs\n",
      "Creating patches for:  C3L-02546-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3056 2560 13966 13081\n",
      "Contour Area: 98757976.0\n",
      "Extracted 30 coordinates\n",
      "start stitching C3L-02546-21\n",
      "original size: 21911 x 17801\n",
      "downscaled size for stiching: 2738 x 2225\n",
      "number of patches: 30\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/30 stitched\n",
      "progress: 3/30 stitched\n",
      "progress: 6/30 stitched\n",
      "progress: 9/30 stitched\n",
      "progress: 12/30 stitched\n",
      "progress: 15/30 stitched\n",
      "progress: 18/30 stitched\n",
      "progress: 21/30 stitched\n",
      "progress: 24/30 stitched\n",
      "progress: 27/30 stitched\n",
      "segmentation took 0.21550607681274414 seconds\n",
      "patching took 0.017206430435180664 seconds\n",
      "stitching took 0.12793874740600586 seconds\n",
      "\n",
      "\n",
      "progress: 0.15, 154/1027\n",
      "processing C3L-02546-23.svs\n",
      "Creating patches for:  C3L-02546-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4105 3608 13126 13853\n",
      "Contour Area: 88004916.0\n",
      "Extracted 27 coordinates\n",
      "start stitching C3L-02546-23\n",
      "original size: 19919 x 19797\n",
      "downscaled size for stiching: 2489 x 2474\n",
      "number of patches: 27\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/27 stitched\n",
      "progress: 3/27 stitched\n",
      "progress: 6/27 stitched\n",
      "progress: 9/27 stitched\n",
      "progress: 12/27 stitched\n",
      "progress: 15/27 stitched\n",
      "progress: 18/27 stitched\n",
      "progress: 21/27 stitched\n",
      "progress: 24/27 stitched\n",
      "segmentation took 0.2115788459777832 seconds\n",
      "patching took 0.021427631378173828 seconds\n",
      "stitching took 0.11671853065490723 seconds\n",
      "\n",
      "\n",
      "progress: 0.15, 155/1027\n",
      "processing C3L-02549-21.svs\n",
      "Creating patches for:  C3L-02549-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4561 4592 18061 12434\n",
      "Contour Area: 144422643.0\n",
      "Extracted 45 coordinates\n",
      "start stitching C3L-02549-21\n",
      "original size: 27887 x 20498\n",
      "downscaled size for stiching: 3485 x 2562\n",
      "number of patches: 45\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/45 stitched\n",
      "progress: 5/45 stitched\n",
      "progress: 10/45 stitched\n",
      "progress: 15/45 stitched\n",
      "progress: 20/45 stitched\n",
      "progress: 25/45 stitched\n",
      "progress: 30/45 stitched\n",
      "progress: 35/45 stitched\n",
      "progress: 40/45 stitched\n",
      "segmentation took 0.3158302307128906 seconds\n",
      "patching took 0.020383358001708984 seconds\n",
      "stitching took 0.191192626953125 seconds\n",
      "\n",
      "\n",
      "progress: 0.15, 156/1027\n",
      "processing C3L-02549-22.svs\n",
      "Creating patches for:  C3L-02549-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 8393 3928 16525 15634\n",
      "Contour Area: 141901415.0\n",
      "Extracted 42 coordinates\n",
      "start stitching C3L-02549-22\n",
      "original size: 29879 x 22554\n",
      "downscaled size for stiching: 3734 x 2819\n",
      "number of patches: 42\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/42 stitched\n",
      "progress: 5/42 stitched\n",
      "progress: 10/42 stitched\n",
      "progress: 15/42 stitched\n",
      "progress: 20/42 stitched\n",
      "progress: 25/42 stitched\n",
      "progress: 30/42 stitched\n",
      "progress: 35/42 stitched\n",
      "progress: 40/42 stitched\n",
      "segmentation took 0.35974860191345215 seconds\n",
      "patching took 0.021244049072265625 seconds\n",
      "stitching took 0.18668174743652344 seconds\n",
      "\n",
      "\n",
      "progress: 0.15, 157/1027\n",
      "processing C3L-02552-22.svs\n",
      "Creating patches for:  C3L-02552-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4009 3225 11782 14806\n",
      "Contour Area: 117334573.0\n",
      "Extracted 35 coordinates\n",
      "start stitching C3L-02552-22\n",
      "original size: 17927 x 20879\n",
      "downscaled size for stiching: 2240 x 2609\n",
      "number of patches: 35\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/35 stitched\n",
      "progress: 4/35 stitched\n",
      "progress: 8/35 stitched\n",
      "progress: 12/35 stitched\n",
      "progress: 16/35 stitched\n",
      "progress: 20/35 stitched\n",
      "progress: 24/35 stitched\n",
      "progress: 28/35 stitched\n",
      "progress: 32/35 stitched\n",
      "segmentation took 0.21144843101501465 seconds\n",
      "patching took 0.020442962646484375 seconds\n",
      "stitching took 0.14598393440246582 seconds\n",
      "\n",
      "\n",
      "progress: 0.15, 158/1027\n",
      "processing C3L-02552-23.svs\n",
      "Creating patches for:  C3L-02552-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4145 3353 14286 12590\n",
      "Contour Area: 114412683.0\n",
      "Extracted 35 coordinates\n",
      "start stitching C3L-02552-23\n",
      "original size: 19919 x 18551\n",
      "downscaled size for stiching: 2489 x 2318\n",
      "number of patches: 35\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/35 stitched\n",
      "progress: 4/35 stitched\n",
      "progress: 8/35 stitched\n",
      "progress: 12/35 stitched\n",
      "progress: 16/35 stitched\n",
      "progress: 20/35 stitched\n",
      "progress: 24/35 stitched\n",
      "progress: 28/35 stitched\n",
      "progress: 32/35 stitched\n",
      "segmentation took 0.20951461791992188 seconds\n",
      "patching took 0.024307966232299805 seconds\n",
      "stitching took 0.14777469635009766 seconds\n",
      "\n",
      "\n",
      "progress: 0.15, 159/1027\n",
      "processing C3L-02560-21.svs\n",
      "Creating patches for:  C3L-02560-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3857 3881 13693 12364\n",
      "Contour Area: 104139125.0\n",
      "Extracted 32 coordinates\n",
      "start stitching C3L-02560-21\n",
      "original size: 21911 x 18629\n",
      "downscaled size for stiching: 2738 x 2328\n",
      "number of patches: 32\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/32 stitched\n",
      "progress: 4/32 stitched\n",
      "progress: 8/32 stitched\n",
      "progress: 12/32 stitched\n",
      "progress: 16/32 stitched\n",
      "progress: 20/32 stitched\n",
      "progress: 24/32 stitched\n",
      "progress: 28/32 stitched\n",
      "segmentation took 0.21577239036560059 seconds\n",
      "patching took 0.015308618545532227 seconds\n",
      "stitching took 0.13742804527282715 seconds\n",
      "\n",
      "\n",
      "progress: 0.16, 160/1027\n",
      "processing C3L-02560-22.svs\n",
      "Creating patches for:  C3L-02560-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4249 3768 13861 12658\n",
      "Contour Area: 105595251.0\n",
      "Extracted 32 coordinates\n",
      "start stitching C3L-02560-22\n",
      "original size: 21911 x 19458\n",
      "downscaled size for stiching: 2738 x 2432\n",
      "number of patches: 32\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/32 stitched\n",
      "progress: 4/32 stitched\n",
      "progress: 8/32 stitched\n",
      "progress: 12/32 stitched\n",
      "progress: 16/32 stitched\n",
      "progress: 20/32 stitched\n",
      "progress: 24/32 stitched\n",
      "progress: 28/32 stitched\n",
      "segmentation took 0.2191298007965088 seconds\n",
      "patching took 0.019270658493041992 seconds\n",
      "stitching took 0.1359424591064453 seconds\n",
      "\n",
      "\n",
      "progress: 0.16, 161/1027\n",
      "processing C3L-02601-21.svs\n",
      "Creating patches for:  C3L-02601-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4241 3297 13550 13413\n",
      "Contour Area: 108095018.0\n",
      "Extracted 34 coordinates\n",
      "start stitching C3L-02601-21\n",
      "original size: 19919 x 19791\n",
      "downscaled size for stiching: 2489 x 2473\n",
      "number of patches: 34\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/34 stitched\n",
      "progress: 4/34 stitched\n",
      "progress: 8/34 stitched\n",
      "progress: 12/34 stitched\n",
      "progress: 16/34 stitched\n",
      "progress: 20/34 stitched\n",
      "progress: 24/34 stitched\n",
      "progress: 28/34 stitched\n",
      "progress: 32/34 stitched\n",
      "segmentation took 0.2061150074005127 seconds\n",
      "patching took 0.018402814865112305 seconds\n",
      "stitching took 0.14283347129821777 seconds\n",
      "\n",
      "\n",
      "progress: 0.16, 162/1027\n",
      "processing C3L-02616-21.svs\n",
      "Creating patches for:  C3L-02616-21 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 4193 3409 15669 15896\n",
      "Contour Area: 116827023.0\n",
      "Extracted 10 coordinates\n",
      "Bounding Box: 32425 3361 14917 16056\n",
      "Contour Area: 117413224.0\n",
      "Extracted 11 coordinates\n",
      "start stitching C3L-02616-21\n",
      "original size: 51791 x 23082\n",
      "downscaled size for stiching: 3236 x 1442\n",
      "number of patches: 21\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/21 stitched\n",
      "progress: 3/21 stitched\n",
      "progress: 6/21 stitched\n",
      "progress: 9/21 stitched\n",
      "progress: 12/21 stitched\n",
      "progress: 15/21 stitched\n",
      "progress: 18/21 stitched\n",
      "segmentation took 0.14784598350524902 seconds\n",
      "patching took 0.03635692596435547 seconds\n",
      "stitching took 0.09159731864929199 seconds\n",
      "\n",
      "\n",
      "progress: 0.16, 163/1027\n",
      "processing C3L-02616-22.svs\n",
      "Creating patches for:  C3L-02616-22 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 5601 7890 16517 21097\n",
      "Contour Area: 247147065.0\n",
      "Extracted 20 coordinates\n",
      "Bounding Box: 34489 5810 16389 21288\n",
      "Contour Area: 242839962.0\n",
      "Extracted 19 coordinates\n",
      "start stitching C3L-02616-22\n",
      "original size: 55775 x 32892\n",
      "downscaled size for stiching: 3485 x 2055\n",
      "number of patches: 39\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/39 stitched\n",
      "progress: 4/39 stitched\n",
      "progress: 8/39 stitched\n",
      "progress: 12/39 stitched\n",
      "progress: 16/39 stitched\n",
      "progress: 20/39 stitched\n",
      "progress: 24/39 stitched\n",
      "progress: 28/39 stitched\n",
      "progress: 32/39 stitched\n",
      "progress: 36/39 stitched\n",
      "segmentation took 0.2624077796936035 seconds\n",
      "patching took 0.03630256652832031 seconds\n",
      "stitching took 0.16544175148010254 seconds\n",
      "\n",
      "\n",
      "progress: 0.16, 164/1027\n",
      "processing C3L-02616-23.svs\n",
      "Creating patches for:  C3L-02616-23 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 5089 5841 21382 20134\n",
      "Contour Area: 223108992.0\n",
      "Extracted 17 coordinates\n",
      "Bounding Box: 33464 5281 21270 20374\n",
      "Contour Area: 221870838.0\n",
      "Extracted 17 coordinates\n",
      "start stitching C3L-02616-23\n",
      "original size: 59759 x 29816\n",
      "downscaled size for stiching: 3734 x 1863\n",
      "number of patches: 34\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/34 stitched\n",
      "progress: 4/34 stitched\n",
      "progress: 8/34 stitched\n",
      "progress: 12/34 stitched\n",
      "progress: 16/34 stitched\n",
      "progress: 20/34 stitched\n",
      "progress: 24/34 stitched\n",
      "progress: 28/34 stitched\n",
      "progress: 32/34 stitched\n",
      "segmentation took 0.2309117317199707 seconds\n",
      "patching took 0.028516292572021484 seconds\n",
      "stitching took 0.15263009071350098 seconds\n",
      "\n",
      "\n",
      "progress: 0.16, 165/1027\n",
      "processing C3L-02616-24.svs\n",
      "Creating patches for:  C3L-02616-24 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 34548 8705 14306 15747\n",
      "Contour Area: 163764370.0\n",
      "Extracted 13 coordinates\n",
      "Bounding Box: 6416 8433 14435 15763\n",
      "Contour Area: 163450196.0\n",
      "Extracted 13 coordinates\n",
      "start stitching C3L-02616-24\n",
      "original size: 57767 x 30324\n",
      "downscaled size for stiching: 3610 x 1895\n",
      "number of patches: 26\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/26 stitched\n",
      "progress: 3/26 stitched\n",
      "progress: 6/26 stitched\n",
      "progress: 9/26 stitched\n",
      "progress: 12/26 stitched\n",
      "progress: 15/26 stitched\n",
      "progress: 18/26 stitched\n",
      "progress: 21/26 stitched\n",
      "progress: 24/26 stitched\n",
      "segmentation took 0.21675848960876465 seconds\n",
      "patching took 0.037865400314331055 seconds\n",
      "stitching took 0.11683487892150879 seconds\n",
      "\n",
      "\n",
      "progress: 0.16, 166/1027\n",
      "processing C3L-02616-25.svs\n",
      "Creating patches for:  C3L-02616-25 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 36292 6322 19395 19336\n",
      "Contour Area: 269369971.0\n",
      "Extracted 23 coordinates\n",
      "Bounding Box: 6736 5602 19587 19256\n",
      "Contour Area: 266886785.0\n",
      "Extracted 23 coordinates\n",
      "start stitching C3L-02616-25\n",
      "original size: 61751 x 31036\n",
      "downscaled size for stiching: 3859 x 1939\n",
      "number of patches: 46\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/46 stitched\n",
      "progress: 5/46 stitched\n",
      "progress: 10/46 stitched\n",
      "progress: 15/46 stitched\n",
      "progress: 20/46 stitched\n",
      "progress: 25/46 stitched\n",
      "progress: 30/46 stitched\n",
      "progress: 35/46 stitched\n",
      "progress: 40/46 stitched\n",
      "progress: 45/46 stitched\n",
      "segmentation took 0.2634148597717285 seconds\n",
      "patching took 0.035386085510253906 seconds\n",
      "stitching took 0.19037628173828125 seconds\n",
      "\n",
      "\n",
      "progress: 0.16, 167/1027\n",
      "processing C3L-02624-21.svs\n",
      "Creating patches for:  C3L-02624-21 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 34904 6592 20983 13491\n",
      "Contour Area: 228333319.0\n",
      "Extracted 16 coordinates\n",
      "Bounding Box: 4145 4768 20278 14531\n",
      "Contour Area: 232160841.0\n",
      "Extracted 18 coordinates\n",
      "start stitching C3L-02624-21\n",
      "original size: 59759 x 24019\n",
      "downscaled size for stiching: 3734 x 1501\n",
      "number of patches: 34\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/34 stitched\n",
      "progress: 4/34 stitched\n",
      "progress: 8/34 stitched\n",
      "progress: 12/34 stitched\n",
      "progress: 16/34 stitched\n",
      "progress: 20/34 stitched\n",
      "progress: 24/34 stitched\n",
      "progress: 28/34 stitched\n",
      "progress: 32/34 stitched\n",
      "segmentation took 0.2092726230621338 seconds\n",
      "patching took 0.03511977195739746 seconds\n",
      "stitching took 0.13959717750549316 seconds\n",
      "\n",
      "\n",
      "progress: 0.16, 168/1027\n",
      "processing C3L-02624-22.svs\n",
      "Creating patches for:  C3L-02624-22 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 41641 4896 23302 15106\n",
      "Contour Area: 251908757.0\n",
      "Extracted 21 coordinates\n",
      "Bounding Box: 4160 3744 23271 15170\n",
      "Contour Area: 255671716.0\n",
      "Extracted 21 coordinates\n",
      "start stitching C3L-02624-22\n",
      "original size: 67727 x 22978\n",
      "downscaled size for stiching: 2116 x 718\n",
      "number of patches: 42\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/42 stitched\n",
      "progress: 5/42 stitched\n",
      "progress: 10/42 stitched\n",
      "progress: 15/42 stitched\n",
      "progress: 20/42 stitched\n",
      "progress: 25/42 stitched\n",
      "progress: 30/42 stitched\n",
      "progress: 35/42 stitched\n",
      "progress: 40/42 stitched\n",
      "segmentation took 0.07106900215148926 seconds\n",
      "patching took 0.03547477722167969 seconds\n",
      "stitching took 0.04598593711853027 seconds\n",
      "\n",
      "\n",
      "progress: 0.16, 169/1027\n",
      "processing C3L-02624-23.svs\n",
      "Creating patches for:  C3L-02624-23 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 5185 9265 21174 20837\n",
      "Contour Area: 281420308.0\n",
      "Extracted 22 coordinates\n",
      "Bounding Box: 34904 5104 21271 19989\n",
      "Contour Area: 290062509.0\n",
      "Extracted 23 coordinates\n",
      "start stitching C3L-02624-23\n",
      "original size: 59759 x 33446\n",
      "downscaled size for stiching: 3734 x 2090\n",
      "number of patches: 45\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/45 stitched\n",
      "progress: 5/45 stitched\n",
      "progress: 10/45 stitched\n",
      "progress: 15/45 stitched\n",
      "progress: 20/45 stitched\n",
      "progress: 25/45 stitched\n",
      "progress: 30/45 stitched\n",
      "progress: 35/45 stitched\n",
      "progress: 40/45 stitched\n",
      "segmentation took 0.2888765335083008 seconds\n",
      "patching took 0.033399343490600586 seconds\n",
      "stitching took 0.1903824806213379 seconds\n",
      "\n",
      "\n",
      "progress: 0.17, 170/1027\n",
      "processing C3L-02624-24.svs\n",
      "Creating patches for:  C3L-02624-24 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 3872 6625 25927 18950\n",
      "Contour Area: 390321017.0\n",
      "Extracted 29 coordinates\n",
      "Bounding Box: 36392 4705 26086 18597\n",
      "Contour Area: 388851828.0\n",
      "Extracted 29 coordinates\n",
      "start stitching C3L-02624-24\n",
      "original size: 67727 x 28359\n",
      "downscaled size for stiching: 2116 x 886\n",
      "number of patches: 58\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/58 stitched\n",
      "progress: 6/58 stitched\n",
      "progress: 12/58 stitched\n",
      "progress: 18/58 stitched\n",
      "progress: 24/58 stitched\n",
      "progress: 30/58 stitched\n",
      "progress: 36/58 stitched\n",
      "progress: 42/58 stitched\n",
      "progress: 48/58 stitched\n",
      "progress: 54/58 stitched\n",
      "segmentation took 0.08840107917785645 seconds\n",
      "patching took 0.037070274353027344 seconds\n",
      "stitching took 0.06073808670043945 seconds\n",
      "\n",
      "\n",
      "progress: 0.17, 171/1027\n",
      "processing C3L-02624-25.svs\n",
      "Creating patches for:  C3L-02624-25 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 5185 5424 20870 19363\n",
      "Contour Area: 322627489.0\n",
      "Extracted 25 coordinates\n",
      "Bounding Box: 35640 4736 19974 19267\n",
      "Contour Area: 300540536.0\n",
      "Extracted 25 coordinates\n",
      "start stitching C3L-02624-25\n",
      "original size: 59759 x 28979\n",
      "downscaled size for stiching: 3734 x 1811\n",
      "number of patches: 50\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/50 stitched\n",
      "progress: 5/50 stitched\n",
      "progress: 10/50 stitched\n",
      "progress: 15/50 stitched\n",
      "progress: 20/50 stitched\n",
      "progress: 25/50 stitched\n",
      "progress: 30/50 stitched\n",
      "progress: 35/50 stitched\n",
      "progress: 40/50 stitched\n",
      "progress: 45/50 stitched\n",
      "segmentation took 0.32175612449645996 seconds\n",
      "patching took 0.03270745277404785 seconds\n",
      "stitching took 0.2045741081237793 seconds\n",
      "\n",
      "\n",
      "progress: 0.17, 172/1027\n",
      "processing C3L-02625-21.svs\n",
      "Creating patches for:  C3L-02625-21 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 32744 7089 19847 14581\n",
      "Contour Area: 190930988.0\n",
      "Extracted 16 coordinates\n",
      "Bounding Box: 3488 3872 19511 14565\n",
      "Contour Area: 188279549.0\n",
      "Extracted 16 coordinates\n",
      "start stitching C3L-02625-21\n",
      "original size: 55775 x 24870\n",
      "downscaled size for stiching: 3485 x 1554\n",
      "number of patches: 32\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/32 stitched\n",
      "progress: 4/32 stitched\n",
      "progress: 8/32 stitched\n",
      "progress: 12/32 stitched\n",
      "progress: 16/32 stitched\n",
      "progress: 20/32 stitched\n",
      "progress: 24/32 stitched\n",
      "progress: 28/32 stitched\n",
      "segmentation took 0.18625926971435547 seconds\n",
      "patching took 0.03270459175109863 seconds\n",
      "stitching took 0.13573360443115234 seconds\n",
      "\n",
      "\n",
      "progress: 0.17, 173/1027\n",
      "processing C3L-02625-22.svs\n",
      "Creating patches for:  C3L-02625-22 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 33177 6034 15462 17032\n",
      "Contour Area: 152671307.0\n",
      "Extracted 12 coordinates\n",
      "Bounding Box: 4065 3905 14597 17304\n",
      "Contour Area: 151817826.0\n",
      "Extracted 13 coordinates\n",
      "start stitching C3L-02625-22\n",
      "original size: 51791 x 26299\n",
      "downscaled size for stiching: 3236 x 1643\n",
      "number of patches: 25\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/25 stitched\n",
      "progress: 3/25 stitched\n",
      "progress: 6/25 stitched\n",
      "progress: 9/25 stitched\n",
      "progress: 12/25 stitched\n",
      "progress: 15/25 stitched\n",
      "progress: 18/25 stitched\n",
      "progress: 21/25 stitched\n",
      "progress: 24/25 stitched\n",
      "segmentation took 0.17806601524353027 seconds\n",
      "patching took 0.032880544662475586 seconds\n",
      "stitching took 0.11165761947631836 seconds\n",
      "\n",
      "\n",
      "progress: 0.17, 174/1027\n",
      "processing C3L-02625-23.svs\n",
      "Creating patches for:  C3L-02625-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3585 3185 18790 13110\n",
      "Contour Area: 170012894.0\n",
      "Extracted 52 coordinates\n",
      "start stitching C3L-02625-23\n",
      "original size: 23903 x 18615\n",
      "downscaled size for stiching: 2987 x 2326\n",
      "number of patches: 52\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/52 stitched\n",
      "progress: 6/52 stitched\n",
      "progress: 12/52 stitched\n",
      "progress: 18/52 stitched\n",
      "progress: 24/52 stitched\n",
      "progress: 30/52 stitched\n",
      "progress: 36/52 stitched\n",
      "progress: 42/52 stitched\n",
      "progress: 48/52 stitched\n",
      "segmentation took 0.25687742233276367 seconds\n",
      "patching took 0.01807236671447754 seconds\n",
      "stitching took 0.20939207077026367 seconds\n",
      "\n",
      "\n",
      "progress: 0.17, 175/1027\n",
      "processing C3L-02625-24.svs\n",
      "Creating patches for:  C3L-02625-24 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3729 2897 18902 18584\n",
      "Contour Area: 240301975.0\n",
      "Extracted 19 coordinates\n",
      "start stitching C3L-02625-24\n",
      "original size: 25895 x 23817\n",
      "downscaled size for stiching: 1618 x 1488\n",
      "number of patches: 19\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/19 stitched\n",
      "progress: 2/19 stitched\n",
      "progress: 4/19 stitched\n",
      "progress: 6/19 stitched\n",
      "progress: 8/19 stitched\n",
      "progress: 10/19 stitched\n",
      "progress: 12/19 stitched\n",
      "progress: 14/19 stitched\n",
      "progress: 16/19 stitched\n",
      "progress: 18/19 stitched\n",
      "segmentation took 0.10829353332519531 seconds\n",
      "patching took 0.020170927047729492 seconds\n",
      "stitching took 0.07968807220458984 seconds\n",
      "\n",
      "\n",
      "progress: 0.17, 176/1027\n",
      "processing C3L-02625-25.svs\n",
      "Creating patches for:  C3L-02625-25 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 37593 7472 19542 17649\n",
      "Contour Area: 211458184.0\n",
      "Extracted 19 coordinates\n",
      "Bounding Box: 6945 4320 19622 17713\n",
      "Contour Area: 214858728.0\n",
      "Extracted 17 coordinates\n",
      "start stitching C3L-02625-25\n",
      "original size: 59759 x 27408\n",
      "downscaled size for stiching: 3734 x 1713\n",
      "number of patches: 36\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/36 stitched\n",
      "progress: 4/36 stitched\n",
      "progress: 8/36 stitched\n",
      "progress: 12/36 stitched\n",
      "progress: 16/36 stitched\n",
      "progress: 20/36 stitched\n",
      "progress: 24/36 stitched\n",
      "progress: 28/36 stitched\n",
      "progress: 32/36 stitched\n",
      "segmentation took 0.22023916244506836 seconds\n",
      "patching took 0.03317737579345703 seconds\n",
      "stitching took 0.15115785598754883 seconds\n",
      "\n",
      "\n",
      "progress: 0.17, 177/1027\n",
      "processing C3L-02627-21.svs\n",
      "Creating patches for:  C3L-02627-21 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 37044 6131 13523 12664\n",
      "Contour Area: 87718808.0\n",
      "Extracted 8 coordinates\n",
      "Bounding Box: 4368 4914 16643 9479\n",
      "Contour Area: 87565815.0\n",
      "Extracted 9 coordinates\n",
      "start stitching C3L-02627-21\n",
      "original size: 57767 x 21548\n",
      "downscaled size for stiching: 3610 x 1346\n",
      "number of patches: 17\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/17 stitched\n",
      "progress: 2/17 stitched\n",
      "progress: 4/17 stitched\n",
      "progress: 6/17 stitched\n",
      "progress: 8/17 stitched\n",
      "progress: 10/17 stitched\n",
      "progress: 12/17 stitched\n",
      "progress: 14/17 stitched\n",
      "progress: 16/17 stitched\n",
      "segmentation took 0.15468192100524902 seconds\n",
      "patching took 0.03851723670959473 seconds\n",
      "stitching took 0.07670450210571289 seconds\n",
      "\n",
      "\n",
      "progress: 0.17, 178/1027\n",
      "processing C3L-02627-22.svs\n",
      "Creating patches for:  C3L-02627-22 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 3481 4256 16053 14681\n",
      "Contour Area: 130787944.0\n",
      "Extracted 39 coordinates\n",
      "start stitching C3L-02627-22\n",
      "original size: 23903 x 21017\n",
      "downscaled size for stiching: 2987 x 2627\n",
      "number of patches: 39\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/39 stitched\n",
      "progress: 4/39 stitched\n",
      "progress: 8/39 stitched\n",
      "progress: 12/39 stitched\n",
      "progress: 16/39 stitched\n",
      "progress: 20/39 stitched\n",
      "progress: 24/39 stitched\n",
      "progress: 28/39 stitched\n",
      "progress: 32/39 stitched\n",
      "progress: 36/39 stitched\n",
      "segmentation took 0.28113722801208496 seconds\n",
      "patching took 0.021405696868896484 seconds\n",
      "stitching took 0.16527199745178223 seconds\n",
      "\n",
      "\n",
      "progress: 0.17, 179/1027\n",
      "processing C3L-02627-23.svs\n",
      "Creating patches for:  C3L-02627-23 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 36681 6467 12741 15626\n",
      "Contour Area: 136794630.0\n",
      "Extracted 10 coordinates\n",
      "Bounding Box: 5441 4610 12804 15706\n",
      "Contour Area: 137476221.0\n",
      "Extracted 10 coordinates\n",
      "start stitching C3L-02627-23\n",
      "original size: 55775 x 25871\n",
      "downscaled size for stiching: 3485 x 1616\n",
      "number of patches: 20\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/20 stitched\n",
      "progress: 2/20 stitched\n",
      "progress: 4/20 stitched\n",
      "progress: 6/20 stitched\n",
      "progress: 8/20 stitched\n",
      "progress: 10/20 stitched\n",
      "progress: 12/20 stitched\n",
      "progress: 14/20 stitched\n",
      "progress: 16/20 stitched\n",
      "progress: 18/20 stitched\n",
      "segmentation took 0.18052387237548828 seconds\n",
      "patching took 0.03382515907287598 seconds\n",
      "stitching took 0.09550094604492188 seconds\n",
      "\n",
      "\n",
      "progress: 0.18, 180/1027\n",
      "processing C3L-02627-24.svs\n",
      "Creating patches for:  C3L-02627-24 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4521 3464 14533 16065\n",
      "Contour Area: 150619116.0\n",
      "Extracted 45 coordinates\n",
      "start stitching C3L-02627-24\n",
      "original size: 23903 x 23168\n",
      "downscaled size for stiching: 2987 x 2896\n",
      "number of patches: 45\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/45 stitched\n",
      "progress: 5/45 stitched\n",
      "progress: 10/45 stitched\n",
      "progress: 15/45 stitched\n",
      "progress: 20/45 stitched\n",
      "progress: 25/45 stitched\n",
      "progress: 30/45 stitched\n",
      "progress: 35/45 stitched\n",
      "progress: 40/45 stitched\n",
      "segmentation took 0.46634697914123535 seconds\n",
      "patching took 0.01923203468322754 seconds\n",
      "stitching took 0.19286751747131348 seconds\n",
      "\n",
      "\n",
      "progress: 0.18, 181/1027\n",
      "processing C3L-02627-25.svs\n",
      "Creating patches for:  C3L-02627-25 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 40681 9056 19557 14401\n",
      "Contour Area: 178872208.0\n",
      "Extracted 15 coordinates\n",
      "Bounding Box: 6273 6592 20934 14529\n",
      "Contour Area: 203509248.0\n",
      "Extracted 16 coordinates\n",
      "start stitching C3L-02627-25\n",
      "original size: 67727 x 27200\n",
      "downscaled size for stiching: 2116 x 850\n",
      "number of patches: 31\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/31 stitched\n",
      "progress: 4/31 stitched\n",
      "progress: 8/31 stitched\n",
      "progress: 12/31 stitched\n",
      "progress: 16/31 stitched\n",
      "progress: 20/31 stitched\n",
      "progress: 24/31 stitched\n",
      "progress: 28/31 stitched\n",
      "segmentation took 0.07963705062866211 seconds\n",
      "patching took 0.033441781997680664 seconds\n",
      "stitching took 0.03543543815612793 seconds\n",
      "\n",
      "\n",
      "progress: 0.18, 182/1027\n",
      "processing C3L-02629-21.svs\n",
      "Creating patches for:  C3L-02629-21 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 48772 6183 20195 16021\n",
      "Contour Area: 246958338.0\n",
      "Extracted 18 coordinates\n",
      "Bounding Box: 4352 3780 20451 16598\n",
      "Contour Area: 249867629.0\n",
      "Extracted 18 coordinates\n",
      "start stitching C3L-02629-21\n",
      "original size: 73703 x 24767\n",
      "downscaled size for stiching: 2303 x 773\n",
      "number of patches: 36\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/36 stitched\n",
      "progress: 4/36 stitched\n",
      "progress: 8/36 stitched\n",
      "progress: 12/36 stitched\n",
      "progress: 16/36 stitched\n",
      "progress: 20/36 stitched\n",
      "progress: 24/36 stitched\n",
      "progress: 28/36 stitched\n",
      "progress: 32/36 stitched\n",
      "segmentation took 0.08356165885925293 seconds\n",
      "patching took 0.02857232093811035 seconds\n",
      "stitching took 0.039450883865356445 seconds\n",
      "\n",
      "\n",
      "progress: 0.18, 183/1027\n",
      "processing C3L-02629-22.svs\n",
      "Creating patches for:  C3L-02629-22 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 4001 8098 20298 16549\n",
      "Contour Area: 235472603.5\n",
      "Extracted 17 coordinates\n",
      "Bounding Box: 49397 3520 20810 18246\n",
      "Contour Area: 216356728.0\n",
      "Extracted 17 coordinates\n",
      "start stitching C3L-02629-22\n",
      "original size: 71711 x 27335\n",
      "downscaled size for stiching: 2240 x 854\n",
      "number of patches: 34\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/34 stitched\n",
      "progress: 4/34 stitched\n",
      "progress: 8/34 stitched\n",
      "progress: 12/34 stitched\n",
      "progress: 16/34 stitched\n",
      "progress: 20/34 stitched\n",
      "progress: 24/34 stitched\n",
      "progress: 28/34 stitched\n",
      "progress: 32/34 stitched\n",
      "segmentation took 0.0858316421508789 seconds\n",
      "patching took 0.026845216751098633 seconds\n",
      "stitching took 0.03875899314880371 seconds\n",
      "\n",
      "\n",
      "progress: 0.18, 184/1027\n",
      "processing C3L-02629-23.svs\n",
      "Creating patches for:  C3L-02629-23 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 51146 5029 20677 14962\n",
      "Contour Area: 237971370.0\n",
      "Extracted 19 coordinates\n",
      "Bounding Box: 4224 4164 20805 15058\n",
      "Contour Area: 240302192.0\n",
      "Extracted 19 coordinates\n",
      "start stitching C3L-02629-23\n",
      "original size: 75695 x 22105\n",
      "downscaled size for stiching: 2365 x 690\n",
      "number of patches: 38\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/38 stitched\n",
      "progress: 4/38 stitched\n",
      "progress: 8/38 stitched\n",
      "progress: 12/38 stitched\n",
      "progress: 16/38 stitched\n",
      "progress: 20/38 stitched\n",
      "progress: 24/38 stitched\n",
      "progress: 28/38 stitched\n",
      "progress: 32/38 stitched\n",
      "progress: 36/38 stitched\n",
      "segmentation took 0.07471752166748047 seconds\n",
      "patching took 0.026955127716064453 seconds\n",
      "stitching took 0.04097700119018555 seconds\n",
      "\n",
      "\n",
      "progress: 0.18, 185/1027\n",
      "processing C3L-02629-24.svs\n",
      "Creating patches for:  C3L-02629-24 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 50276 7398 20835 16976\n",
      "Contour Area: 255552390.0\n",
      "Extracted 19 coordinates\n",
      "Bounding Box: 4032 3298 21443 17553\n",
      "Contour Area: 282854249.0\n",
      "Extracted 19 coordinates\n",
      "start stitching C3L-02629-24\n",
      "original size: 73703 x 27736\n",
      "downscaled size for stiching: 2303 x 866\n",
      "number of patches: 38\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/38 stitched\n",
      "progress: 4/38 stitched\n",
      "progress: 8/38 stitched\n",
      "progress: 12/38 stitched\n",
      "progress: 16/38 stitched\n",
      "progress: 20/38 stitched\n",
      "progress: 24/38 stitched\n",
      "progress: 28/38 stitched\n",
      "progress: 32/38 stitched\n",
      "progress: 36/38 stitched\n",
      "segmentation took 0.09129595756530762 seconds\n",
      "patching took 0.028497695922851562 seconds\n",
      "stitching took 0.042530059814453125 seconds\n",
      "\n",
      "\n",
      "progress: 0.18, 186/1027\n",
      "processing C3L-02629-25.svs\n",
      "Creating patches for:  C3L-02629-25 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 52420 8361 21603 15217\n",
      "Contour Area: 215858588.0\n",
      "Extracted 19 coordinates\n",
      "Bounding Box: 3296 3940 20227 17716\n",
      "Contour Area: 214001148.0\n",
      "Extracted 20 coordinates\n",
      "start stitching C3L-02629-25\n",
      "original size: 81671 x 26621\n",
      "downscaled size for stiching: 2552 x 831\n",
      "number of patches: 39\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/39 stitched\n",
      "progress: 4/39 stitched\n",
      "progress: 8/39 stitched\n",
      "progress: 12/39 stitched\n",
      "progress: 16/39 stitched\n",
      "progress: 20/39 stitched\n",
      "progress: 24/39 stitched\n",
      "progress: 28/39 stitched\n",
      "progress: 32/39 stitched\n",
      "progress: 36/39 stitched\n",
      "segmentation took 0.09365558624267578 seconds\n",
      "patching took 0.028202295303344727 seconds\n",
      "stitching took 0.04362988471984863 seconds\n",
      "\n",
      "\n",
      "progress: 0.18, 187/1027\n",
      "processing C3L-02648-21.svs\n",
      "Creating patches for:  C3L-02648-21 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 51471 6371 23464 17193\n",
      "Contour Area: 299363690.0\n",
      "Extracted 24 coordinates\n",
      "Bounding Box: 3489 3745 25032 24205\n",
      "Contour Area: 319904188.0\n",
      "Extracted 26 coordinates\n",
      "start stitching C3L-02648-21\n",
      "original size: 77687 x 31023\n",
      "downscaled size for stiching: 2427 x 969\n",
      "number of patches: 50\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/50 stitched\n",
      "progress: 5/50 stitched\n",
      "progress: 10/50 stitched\n",
      "progress: 15/50 stitched\n",
      "progress: 20/50 stitched\n",
      "progress: 25/50 stitched\n",
      "progress: 30/50 stitched\n",
      "progress: 35/50 stitched\n",
      "progress: 40/50 stitched\n",
      "progress: 45/50 stitched\n",
      "segmentation took 0.10712218284606934 seconds\n",
      "patching took 0.02605748176574707 seconds\n",
      "stitching took 0.05335426330566406 seconds\n",
      "\n",
      "\n",
      "progress: 0.18, 188/1027\n",
      "processing C3L-02648-22.svs\n",
      "Creating patches for:  C3L-02648-22 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 35075 5216 23780 18849\n",
      "Contour Area: 316761896.0\n",
      "Extracted 25 coordinates\n",
      "Bounding Box: 3504 2992 23156 19169\n",
      "Contour Area: 317200528.0\n",
      "Extracted 26 coordinates\n",
      "start stitching C3L-02648-22\n",
      "original size: 61751 x 26817\n",
      "downscaled size for stiching: 3859 x 1676\n",
      "number of patches: 51\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/51 stitched\n",
      "progress: 6/51 stitched\n",
      "progress: 12/51 stitched\n",
      "progress: 18/51 stitched\n",
      "progress: 24/51 stitched\n",
      "progress: 30/51 stitched\n",
      "progress: 36/51 stitched\n",
      "progress: 42/51 stitched\n",
      "progress: 48/51 stitched\n",
      "segmentation took 0.2522158622741699 seconds\n",
      "patching took 0.035096168518066406 seconds\n",
      "stitching took 0.20352387428283691 seconds\n",
      "\n",
      "\n",
      "progress: 0.18, 189/1027\n",
      "processing C3L-02648-23.svs\n",
      "Creating patches for:  C3L-02648-23 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 43474 8066 21867 19816\n",
      "Contour Area: 295206523.5\n",
      "Extracted 21 coordinates\n",
      "Bounding Box: 4609 4673 21803 20840\n",
      "Contour Area: 302881742.0\n",
      "Extracted 23 coordinates\n",
      "start stitching C3L-02648-23\n",
      "original size: 71711 x 31723\n",
      "downscaled size for stiching: 2240 x 991\n",
      "number of patches: 44\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/44 stitched\n",
      "progress: 5/44 stitched\n",
      "progress: 10/44 stitched\n",
      "progress: 15/44 stitched\n",
      "progress: 20/44 stitched\n",
      "progress: 25/44 stitched\n",
      "progress: 30/44 stitched\n",
      "progress: 35/44 stitched\n",
      "progress: 40/44 stitched\n",
      "segmentation took 0.0979917049407959 seconds\n",
      "patching took 0.03181648254394531 seconds\n",
      "stitching took 0.0492861270904541 seconds\n",
      "\n",
      "\n",
      "progress: 0.19, 190/1027\n",
      "processing C3L-02648-24.svs\n",
      "Creating patches for:  C3L-02648-24 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 47822 4994 25320 30573\n",
      "Contour Area: 417970786.0\n",
      "Extracted 30 coordinates\n",
      "Bounding Box: 3937 3649 26120 30414\n",
      "Contour Area: 440081573.0\n",
      "Extracted 36 coordinates\n",
      "start stitching C3L-02648-24\n",
      "original size: 77687 x 38640\n",
      "downscaled size for stiching: 2427 x 1207\n",
      "number of patches: 66\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/66 stitched\n",
      "progress: 7/66 stitched\n",
      "progress: 14/66 stitched\n",
      "progress: 21/66 stitched\n",
      "progress: 28/66 stitched\n",
      "progress: 35/66 stitched\n",
      "progress: 42/66 stitched\n",
      "progress: 49/66 stitched\n",
      "progress: 56/66 stitched\n",
      "progress: 63/66 stitched\n",
      "segmentation took 0.13376593589782715 seconds\n",
      "patching took 0.042319297790527344 seconds\n",
      "stitching took 0.07037496566772461 seconds\n",
      "\n",
      "\n",
      "progress: 0.19, 191/1027\n",
      "processing C3L-02648-25.svs\n",
      "Creating patches for:  C3L-02648-25 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 3889 5185 16709 16646\n",
      "Contour Area: 183048790.0\n",
      "Extracted 13 coordinates\n",
      "Bounding Box: 34937 4721 18038 18423\n",
      "Contour Area: 184009681.0\n",
      "Extracted 15 coordinates\n",
      "start stitching C3L-02648-25\n",
      "original size: 55775 x 25448\n",
      "downscaled size for stiching: 3485 x 1590\n",
      "number of patches: 28\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/28 stitched\n",
      "progress: 3/28 stitched\n",
      "progress: 6/28 stitched\n",
      "progress: 9/28 stitched\n",
      "progress: 12/28 stitched\n",
      "progress: 15/28 stitched\n",
      "progress: 18/28 stitched\n",
      "progress: 21/28 stitched\n",
      "progress: 24/28 stitched\n",
      "progress: 27/28 stitched\n",
      "segmentation took 0.18457889556884766 seconds\n",
      "patching took 0.03174543380737305 seconds\n",
      "stitching took 0.11969494819641113 seconds\n",
      "\n",
      "\n",
      "progress: 0.19, 192/1027\n",
      "processing C3L-02649-21.svs\n",
      "Creating patches for:  C3L-02649-21 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 51972 12448 18371 12833\n",
      "Contour Area: 135484160.0\n",
      "Extracted 12 coordinates\n",
      "Bounding Box: 3872 2848 17187 15713\n",
      "Contour Area: 133105664.0\n",
      "Extracted 11 coordinates\n",
      "start stitching C3L-02649-21\n",
      "original size: 73703 x 27745\n",
      "downscaled size for stiching: 2303 x 867\n",
      "number of patches: 23\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/23 stitched\n",
      "progress: 3/23 stitched\n",
      "progress: 6/23 stitched\n",
      "progress: 9/23 stitched\n",
      "progress: 12/23 stitched\n",
      "progress: 15/23 stitched\n",
      "progress: 18/23 stitched\n",
      "progress: 21/23 stitched\n",
      "segmentation took 0.08968520164489746 seconds\n",
      "patching took 0.031384944915771484 seconds\n",
      "stitching took 0.029169082641601562 seconds\n",
      "\n",
      "\n",
      "progress: 0.19, 193/1027\n",
      "processing C3L-02649-22.svs\n",
      "Creating patches for:  C3L-02649-22 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 4416 2946 21763 17550\n",
      "Contour Area: 261047047.0\n",
      "Extracted 20 coordinates\n",
      "Bounding Box: 50596 2914 21571 17198\n",
      "Contour Area: 252810193.0\n",
      "Extracted 18 coordinates\n",
      "start stitching C3L-02649-22\n",
      "original size: 73703 x 23602\n",
      "downscaled size for stiching: 2303 x 737\n",
      "number of patches: 38\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/38 stitched\n",
      "progress: 4/38 stitched\n",
      "progress: 8/38 stitched\n",
      "progress: 12/38 stitched\n",
      "progress: 16/38 stitched\n",
      "progress: 20/38 stitched\n",
      "progress: 24/38 stitched\n",
      "progress: 28/38 stitched\n",
      "progress: 32/38 stitched\n",
      "progress: 36/38 stitched\n",
      "segmentation took 0.07805156707763672 seconds\n",
      "patching took 0.03240704536437988 seconds\n",
      "stitching took 0.04134511947631836 seconds\n",
      "\n",
      "\n",
      "progress: 0.19, 194/1027\n",
      "processing C3L-02649-23.svs\n",
      "Creating patches for:  C3L-02649-23 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 3937 6373 19111 17390\n",
      "Contour Area: 234834191.0\n",
      "Extracted 19 coordinates\n",
      "Bounding Box: 48303 3843 18247 17486\n",
      "Contour Area: 227278154.0\n",
      "Extracted 17 coordinates\n",
      "start stitching C3L-02649-23\n",
      "original size: 69719 x 26357\n",
      "downscaled size for stiching: 2178 x 823\n",
      "number of patches: 36\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/36 stitched\n",
      "progress: 4/36 stitched\n",
      "progress: 8/36 stitched\n",
      "progress: 12/36 stitched\n",
      "progress: 16/36 stitched\n",
      "progress: 20/36 stitched\n",
      "progress: 24/36 stitched\n",
      "progress: 28/36 stitched\n",
      "progress: 32/36 stitched\n",
      "segmentation took 0.08286333084106445 seconds\n",
      "patching took 0.032711029052734375 seconds\n",
      "stitching took 0.040418148040771484 seconds\n",
      "\n",
      "\n",
      "progress: 0.19, 195/1027\n",
      "processing C3L-02649-24.svs\n",
      "Creating patches for:  C3L-02649-24 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 3681 5218 18889 18889\n",
      "Contour Area: 270069249.0\n",
      "Extracted 22 coordinates\n",
      "Bounding Box: 51510 4769 18153 18185\n",
      "Contour Area: 249912267.0\n",
      "Extracted 21 coordinates\n",
      "start stitching C3L-02649-24\n",
      "original size: 71711 x 26507\n",
      "downscaled size for stiching: 2240 x 828\n",
      "number of patches: 43\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/43 stitched\n",
      "progress: 5/43 stitched\n",
      "progress: 10/43 stitched\n",
      "progress: 15/43 stitched\n",
      "progress: 20/43 stitched\n",
      "progress: 25/43 stitched\n",
      "progress: 30/43 stitched\n",
      "progress: 35/43 stitched\n",
      "progress: 40/43 stitched\n",
      "segmentation took 0.08911776542663574 seconds\n",
      "patching took 0.03345227241516113 seconds\n",
      "stitching took 0.04610800743103027 seconds\n",
      "\n",
      "\n",
      "progress: 0.19, 196/1027\n",
      "processing C3L-02649-25.svs\n",
      "Creating patches for:  C3L-02649-25 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 4609 9121 20907 16195\n",
      "Contour Area: 276876928.0\n",
      "Extracted 20 coordinates\n",
      "Bounding Box: 44883 3936 21450 15971\n",
      "Contour Area: 274940400.0\n",
      "Extracted 19 coordinates\n",
      "start stitching C3L-02649-25\n",
      "original size: 71711 x 28004\n",
      "downscaled size for stiching: 2240 x 875\n",
      "number of patches: 39\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/39 stitched\n",
      "progress: 4/39 stitched\n",
      "progress: 8/39 stitched\n",
      "progress: 12/39 stitched\n",
      "progress: 16/39 stitched\n",
      "progress: 20/39 stitched\n",
      "progress: 24/39 stitched\n",
      "progress: 28/39 stitched\n",
      "progress: 32/39 stitched\n",
      "progress: 36/39 stitched\n",
      "segmentation took 0.09057497978210449 seconds\n",
      "patching took 0.03195333480834961 seconds\n",
      "stitching took 0.043402671813964844 seconds\n",
      "\n",
      "\n",
      "progress: 0.19, 197/1027\n",
      "processing C3L-02661-21.svs\n",
      "Creating patches for:  C3L-02661-21 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 34664 5121 22599 22857\n",
      "Contour Area: 371519207.0\n",
      "Extracted 26 coordinates\n",
      "Bounding Box: 3568 3249 22727 24664\n",
      "Contour Area: 395536407.0\n",
      "Extracted 29 coordinates\n",
      "start stitching C3L-02661-21\n",
      "original size: 59759 x 31050\n",
      "downscaled size for stiching: 3734 x 1940\n",
      "number of patches: 55\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/55 stitched\n",
      "progress: 6/55 stitched\n",
      "progress: 12/55 stitched\n",
      "progress: 18/55 stitched\n",
      "progress: 24/55 stitched\n",
      "progress: 30/55 stitched\n",
      "progress: 36/55 stitched\n",
      "progress: 42/55 stitched\n",
      "progress: 48/55 stitched\n",
      "progress: 54/55 stitched\n",
      "segmentation took 0.4466743469238281 seconds\n",
      "patching took 0.03435230255126953 seconds\n",
      "stitching took 0.22315549850463867 seconds\n",
      "\n",
      "\n",
      "progress: 0.19, 198/1027\n",
      "processing C3L-02661-22.svs\n",
      "Creating patches for:  C3L-02661-22 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 4976 11537 18019 18740\n",
      "Contour Area: 237684021.0\n",
      "Extracted 21 coordinates\n",
      "Bounding Box: 41252 3616 16371 19956\n",
      "Contour Area: 252036093.0\n",
      "Extracted 19 coordinates\n",
      "start stitching C3L-02661-22\n",
      "original size: 61751 x 33029\n",
      "downscaled size for stiching: 3859 x 2064\n",
      "number of patches: 40\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/40 stitched\n",
      "progress: 4/40 stitched\n",
      "progress: 8/40 stitched\n",
      "progress: 12/40 stitched\n",
      "progress: 16/40 stitched\n",
      "progress: 20/40 stitched\n",
      "progress: 24/40 stitched\n",
      "progress: 28/40 stitched\n",
      "progress: 32/40 stitched\n",
      "progress: 36/40 stitched\n",
      "segmentation took 0.2827608585357666 seconds\n",
      "patching took 0.041123151779174805 seconds\n",
      "stitching took 0.17081689834594727 seconds\n",
      "\n",
      "\n",
      "progress: 0.19, 199/1027\n",
      "processing C3L-02661-23.svs\n",
      "Creating patches for:  C3L-02661-23 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 33060 4657 12835 14501\n",
      "Contour Area: 139126229.0\n",
      "Extracted 12 coordinates\n",
      "Bounding Box: 4176 3776 13475 14261\n",
      "Contour Area: 140595886.0\n",
      "Extracted 13 coordinates\n",
      "start stitching C3L-02661-23\n",
      "original size: 49799 x 22678\n",
      "downscaled size for stiching: 3112 x 1417\n",
      "number of patches: 25\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/25 stitched\n",
      "progress: 3/25 stitched\n",
      "progress: 6/25 stitched\n",
      "progress: 9/25 stitched\n",
      "progress: 12/25 stitched\n",
      "progress: 15/25 stitched\n",
      "progress: 18/25 stitched\n",
      "progress: 21/25 stitched\n",
      "progress: 24/25 stitched\n",
      "segmentation took 0.14586973190307617 seconds\n",
      "patching took 0.03238177299499512 seconds\n",
      "stitching took 0.1055455207824707 seconds\n",
      "\n",
      "\n",
      "progress: 0.19, 200/1027\n",
      "processing C3L-02661-24.svs\n",
      "Creating patches for:  C3L-02661-24 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 5216 3139 23492 22010\n",
      "Contour Area: 331167786.0\n",
      "Extracted 24 coordinates\n",
      "Bounding Box: 40516 2691 24035 21305\n",
      "Contour Area: 338191433.0\n",
      "Extracted 24 coordinates\n",
      "start stitching C3L-02661-24\n",
      "original size: 65735 x 26462\n",
      "downscaled size for stiching: 2054 x 826\n",
      "number of patches: 48\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/48 stitched\n",
      "progress: 5/48 stitched\n",
      "progress: 10/48 stitched\n",
      "progress: 15/48 stitched\n",
      "progress: 20/48 stitched\n",
      "progress: 25/48 stitched\n",
      "progress: 30/48 stitched\n",
      "progress: 35/48 stitched\n",
      "progress: 40/48 stitched\n",
      "progress: 45/48 stitched\n",
      "segmentation took 0.08219790458679199 seconds\n",
      "patching took 0.03240561485290527 seconds\n",
      "stitching took 0.05063891410827637 seconds\n",
      "\n",
      "\n",
      "progress: 0.20, 201/1027\n",
      "processing C3L-02661-25.svs\n",
      "Creating patches for:  C3L-02661-25 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 36995 13385 23588 15468\n",
      "Contour Area: 297761446.0\n",
      "Extracted 24 coordinates\n",
      "Bounding Box: 3968 3810 22723 20784\n",
      "Contour Area: 288489988.0\n",
      "Extracted 24 coordinates\n",
      "start stitching C3L-02661-25\n",
      "original size: 65735 x 32759\n",
      "downscaled size for stiching: 2054 x 1023\n",
      "number of patches: 48\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4096, 4096)x(4096, 4096)\n",
      "downscaled patch size: 128x128\n",
      "progress: 0/48 stitched\n",
      "progress: 5/48 stitched\n",
      "progress: 10/48 stitched\n",
      "progress: 15/48 stitched\n",
      "progress: 20/48 stitched\n",
      "progress: 25/48 stitched\n",
      "progress: 30/48 stitched\n",
      "progress: 35/48 stitched\n",
      "progress: 40/48 stitched\n",
      "progress: 45/48 stitched\n",
      "segmentation took 0.09584403038024902 seconds\n",
      "patching took 0.039556026458740234 seconds\n",
      "stitching took 0.05375933647155762 seconds\n",
      "\n",
      "\n",
      "progress: 0.20, 202/1027\n",
      "processing C3L-02834-21.svs\n",
      "Creating patches for:  C3L-02834-21 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 8676 10965 14105 12328\n",
      "Contour Area: 61484483.0\n",
      "Extracted 7 coordinates\n",
      "Bounding Box: 5090 3665 10663 15434\n",
      "Contour Area: 54608151.0\n",
      "Extracted 8 coordinates\n",
      "start stitching C3L-02834-21\n",
      "original size: 27887 x 26702\n",
      "downscaled size for stiching: 1742 x 1668\n",
      "number of patches: 15\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4098, 4098)x(4098, 4098)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/15 stitched\n",
      "progress: 2/15 stitched\n",
      "progress: 4/15 stitched\n",
      "progress: 6/15 stitched\n",
      "progress: 8/15 stitched\n",
      "progress: 10/15 stitched\n",
      "progress: 12/15 stitched\n",
      "progress: 14/15 stitched\n",
      "segmentation took 0.12403035163879395 seconds\n",
      "patching took 0.03747153282165527 seconds\n",
      "stitching took 0.06308627128601074 seconds\n",
      "\n",
      "\n",
      "progress: 0.20, 203/1027\n",
      "processing C3L-02834-22.svs\n",
      "Creating patches for:  C3L-02834-22 ...\n",
      "Total number of contours to process:  2\n",
      "Bounding Box: 4241 10211 6130 8660\n",
      "Contour Area: 31662491.0\n",
      "Extracted 4 coordinates\n",
      "Bounding Box: 11187 3233 8723 22152\n",
      "Contour Area: 133553988.0\n",
      "Extracted 11 coordinates\n",
      "start stitching C3L-02834-22\n",
      "original size: 25895 x 29610\n",
      "downscaled size for stiching: 1618 x 1850\n",
      "number of patches: 15\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (4097, 4097)x(4097, 4097)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/15 stitched\n",
      "progress: 2/15 stitched\n",
      "progress: 4/15 stitched\n",
      "progress: 6/15 stitched\n",
      "progress: 8/15 stitched\n",
      "progress: 10/15 stitched\n",
      "progress: 12/15 stitched\n",
      "progress: 14/15 stitched\n",
      "segmentation took 0.13703250885009766 seconds\n",
      "patching took 0.03093242645263672 seconds\n",
      "stitching took 0.06394338607788086 seconds\n",
      "\n",
      "\n",
      "progress: 0.20, 204/1027\n",
      "processing C3L-02834-23.svs\n",
      "Creating patches for:  C3L-02834-23 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 4377 3288 13717 15137\n",
      "Contour Area: 113467188.0\n",
      "Extracted 33 coordinates\n",
      "start stitching C3L-02834-23\n",
      "original size: 21911 x 21192\n",
      "downscaled size for stiching: 2738 x 2649\n",
      "number of patches: 33\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/33 stitched\n",
      "progress: 4/33 stitched\n",
      "progress: 8/33 stitched\n",
      "progress: 12/33 stitched\n",
      "progress: 16/33 stitched\n",
      "progress: 20/33 stitched\n",
      "progress: 24/33 stitched\n",
      "progress: 28/33 stitched\n",
      "progress: 32/33 stitched\n",
      "segmentation took 0.24790215492248535 seconds\n",
      "patching took 0.019063234329223633 seconds\n",
      "stitching took 0.14699840545654297 seconds\n",
      "\n",
      "\n",
      "progress: 0.20, 205/1027\n",
      "processing C3L-02891-21.svs\n",
      "Creating patches for:  C3L-02891-21 ...\n",
      "Total number of contours to process:  1\n",
      "Bounding Box: 2056 2392 14631 12166\n",
      "Contour Area: 100399688.0\n",
      "Extracted 31 coordinates\n",
      "start stitching C3L-02891-21\n",
      "original size: 17927 x 16414\n",
      "downscaled size for stiching: 2240 x 2051\n",
      "number of patches: 31\n",
      "patch size: 256x256 patch level: 2\n",
      "ref patch size: (2048, 2048)x(2048, 2048)\n",
      "downscaled patch size: 256x256\n",
      "progress: 0/31 stitched\n",
      "progress: 4/31 stitched\n",
      "progress: 8/31 stitched\n",
      "progress: 12/31 stitched\n",
      "progress: 16/31 stitched\n",
      "progress: 20/31 stitched\n",
      "progress: 24/31 stitched\n",
      "progress: 28/31 stitched\n",
      "segmentation took 0.1647489070892334 seconds\n",
      "patching took 0.01873302459716797 seconds\n",
      "stitching took 0.12718534469604492 seconds\n",
      "\n",
      "\n",
      "progress: 0.20, 206/1027\n",
      "processing C3L-02891-23.svs\n",
      "Creating patches for:  C3L-02891-23 ...\n",
      "Total number of contours to process:  1\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sci/PycharmProjects/chaofan/projects/CLAM/create_patches_fp.py\", line 325, in <module>\n",
      "    seg_times, patch_times = seg_and_patch(**directories, **parameters,\n",
      "  File \"/home/sci/PycharmProjects/chaofan/projects/CLAM/create_patches_fp.py\", line 215, in seg_and_patch\n",
      "    file_path, patch_time_elapsed = patching(WSI_object = WSI_object,  **current_patch_params,)\n",
      "  File \"/home/sci/PycharmProjects/chaofan/projects/CLAM/create_patches_fp.py\", line 42, in patching\n",
      "    file_path = WSI_object.process_contours(**kwargs)\n",
      "  File \"/home/sci/PycharmProjects/chaofan/projects/CLAM/wsi_core/WholeSlideImage.py\", line 383, in process_contours\n",
      "    asset_dict, attr_dict = self.process_contour(cont, self.holes_tissue[idx], patch_level, save_path, patch_size, step_size, **kwargs)\n",
      "  File \"/home/sci/PycharmProjects/chaofan/projects/CLAM/wsi_core/WholeSlideImage.py\", line 398, in process_contour\n",
      "    patch_downsample = (int(self.level_downsamples[patch_level][0]), int(self.level_downsamples[patch_level][1]))\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "##改 savedir and patch_level\n",
    "!python create_patches_fp.py --source /home/sci/Disk2/CPTAC-LUNG/WSI --save_dir /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2 --patch_level 2 --patch_size 256 --seg --patch --stitch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step_2 get patch features\n",
    "data_h5_dir  输出文件地址\\\n",
    "data_slide_dir svs文件地址\\\n",
    "上一步生成的csv csv_path\\\n",
    "feat_dir 输出文件地址\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CUDA_VISIBLE_DEVICES=0 python extract_features_fp.py \\\n",
    "--data_h5_dir data/RESULTS_DIRECTORY/patches \\\n",
    "--data_slide_dir /media/yuansh/14THHD/CLAM/DataSet/toy_example \\\n",
    "--csv_path /media/yuansh/14THHD/CLAM/Step_2.csv \\\n",
    "--feat_dir /media/yuansh/14THHD/CLAM/FEATURES_DIRECTORY \\\n",
    "--batch_size 512 \\\n",
    "--slide_ext .svs\n",
    "'''\n",
    "!python extract_features_fp.py\\\n",
    "    --data_h5_dir E:\\Workspace\\Project\\CLAM\\data\\RESULTS_DIRECTORY \\\n",
    "    --data_slide_dir F:/Download/TCGA \\\n",
    "    --csv_path data\\RESULTS_DIRECTORY\\Step_2.csv \\\n",
    "    --feat_dir E:\\Workspace\\Project\\CLAM\\data\\FEATURES_DIRECTORY \\\n",
    "    --batch_size 256 \\\n",
    "    --slide_ext .svs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成第2步骤需要的csv文件\n",
    "from utils.csv_gen import *\n",
    "\n",
    "csv_dir = r'/home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/process_list_autogen.csv'\n",
    "# sort_csv = pd.read_csv(csv_dir).sort_values('slide_id')\n",
    "result_dir = r'/home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/step2_get_features.csv'\n",
    "patch_dir = r'/home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches'\n",
    "csv_gen_step1(csv_dir,result_dir,patch_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing dataset\n",
      "loading model checkpoint\n",
      "\n",
      "progress: 0/206\n",
      "C3L-00001-21\n",
      "downsample [8.00216316 8.00101523]\n",
      "downsampled_level_dim [3236 2955]\n",
      "level_dim [3236 2955]\n",
      "name C3L-00001-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00001-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00001-21.h5 took 2.953198194503784 s\n",
      "features size:  (71, 1024)\n",
      "coordinates size:  (71, 2)\n",
      "\n",
      "progress: 1/206\n",
      "C3L-00009-21\n",
      "downsample [8.00281237 8.00182949]\n",
      "downsampled_level_dim [2489 2733]\n",
      "level_dim [2489 2733]\n",
      "name C3L-00009-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00009-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00009-21.h5 took 0.49354004859924316 s\n",
      "features size:  (41, 1024)\n",
      "coordinates size:  (41, 2)\n",
      "\n",
      "progress: 2/206\n",
      "C3L-00080-21\n",
      "downsample [16.00432633 16.00611309]\n",
      "downsampled_level_dim [1618 1963]\n",
      "level_dim [1618 1963]\n",
      "name C3L-00080-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00080-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00080-21.h5 took 0.328214168548584 s\n",
      "features size:  (14, 1024)\n",
      "coordinates size:  (14, 2)\n",
      "\n",
      "progress: 3/206\n",
      "C3L-00081-21\n",
      "downsample [16.00432633 16.00324675]\n",
      "downsampled_level_dim [1618 1540]\n",
      "level_dim [1618 1540]\n",
      "name C3L-00081-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00081-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00081-21.h5 took 0.35224294662475586 s\n",
      "features size:  (19, 1024)\n",
      "coordinates size:  (19, 2)\n",
      "\n",
      "progress: 4/206\n",
      "C3L-00083-21\n",
      "downsample [16.0075339  16.00102775]\n",
      "downsampled_level_dim [1991 1946]\n",
      "level_dim [1991 1946]\n",
      "name C3L-00083-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00083-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00083-21.h5 took 0.3948934078216553 s\n",
      "features size:  (23, 1024)\n",
      "coordinates size:  (23, 2)\n",
      "\n",
      "progress: 5/206\n",
      "C3L-00093-21\n",
      "downsample [8.00216316 8.        ]\n",
      "downsampled_level_dim [3236 2574]\n",
      "level_dim [3236 2574]\n",
      "name C3L-00093-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00093-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00093-21.h5 took 0.5396156311035156 s\n",
      "features size:  (48, 1024)\n",
      "coordinates size:  (48, 2)\n",
      "\n",
      "progress: 6/206\n",
      "C3L-00094-21\n",
      "downsample [16.01004689 16.00184843]\n",
      "downsampled_level_dim [1493 1623]\n",
      "level_dim [1493 1623]\n",
      "name C3L-00094-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00094-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00094-21.h5 took 0.31485986709594727 s\n",
      "features size:  (13, 1024)\n",
      "coordinates size:  (13, 2)\n",
      "\n",
      "progress: 7/206\n",
      "C3L-00095-21\n",
      "downsample [8.00281237 8.0003413 ]\n",
      "downsampled_level_dim [2489 2930]\n",
      "level_dim [2489 2930]\n",
      "name C3L-00095-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00095-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00095-21.h5 took 0.4452040195465088 s\n",
      "features size:  (31, 1024)\n",
      "coordinates size:  (31, 2)\n",
      "\n",
      "progress: 8/206\n",
      "C3L-00095-22\n",
      "downsample [8.00200861 8.00181357]\n",
      "downsampled_level_dim [3485 2757]\n",
      "level_dim [3485 2757]\n",
      "name C3L-00095-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00095-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00095-22.h5 took 0.5131120681762695 s\n",
      "features size:  (42, 1024)\n",
      "coordinates size:  (42, 2)\n",
      "\n",
      "progress: 9/206\n",
      "C3L-00095-23\n",
      "downsample [8.00234349 8.00072993]\n",
      "downsampled_level_dim [2987 2740]\n",
      "level_dim [2987 2740]\n",
      "name C3L-00095-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00095-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00095-23.h5 took 0.43389177322387695 s\n",
      "features size:  (30, 1024)\n",
      "coordinates size:  (30, 2)\n",
      "\n",
      "progress: 10/206\n",
      "C3L-00140-21\n",
      "downsample [8.00255661 8.        ]\n",
      "downsampled_level_dim [2738 2670]\n",
      "level_dim [2738 2670]\n",
      "name C3L-00140-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00140-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00140-21.h5 took 0.49988484382629395 s\n",
      "features size:  (41, 1024)\n",
      "coordinates size:  (41, 2)\n",
      "\n",
      "progress: 11/206\n",
      "C3L-00140-22\n",
      "downsample [16.01205788 16.00478755]\n",
      "downsampled_level_dim [1244 1671]\n",
      "level_dim [1244 1671]\n",
      "name C3L-00140-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00140-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00140-22.h5 took 0.30614185333251953 s\n",
      "features size:  (14, 1024)\n",
      "coordinates size:  (14, 2)\n",
      "\n",
      "progress: 12/206\n",
      "C3L-00144-21\n",
      "downsample [8.00281237 8.00193143]\n",
      "downsampled_level_dim [2489 2071]\n",
      "level_dim [2489 2071]\n",
      "name C3L-00144-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00144-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00144-21.h5 took 0.41596102714538574 s\n",
      "features size:  (29, 1024)\n",
      "coordinates size:  (29, 2)\n",
      "\n",
      "progress: 13/206\n",
      "C3L-00263-21\n",
      "downsample [16.00669643 16.00051975]\n",
      "downsampled_level_dim [2240 1924]\n",
      "level_dim [2240 1924]\n",
      "name C3L-00263-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00263-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00263-21.h5 took 0.33431291580200195 s\n",
      "features size:  (16, 1024)\n",
      "coordinates size:  (16, 2)\n",
      "\n",
      "progress: 14/206\n",
      "C3L-00263-22\n",
      "downsample [16.0075339  16.00277971]\n",
      "downsampled_level_dim [1991 1439]\n",
      "level_dim [1991 1439]\n",
      "name C3L-00263-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00263-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00263-22.h5 took 0.2872350215911865 s\n",
      "features size:  (8, 1024)\n",
      "coordinates size:  (8, 2)\n",
      "\n",
      "progress: 15/206\n",
      "C3L-00263-23\n",
      "downsample [16.00374933 16.00583942]\n",
      "downsampled_level_dim [1867 2055]\n",
      "level_dim [1867 2055]\n",
      "name C3L-00263-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00263-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00263-23.h5 took 0.35723280906677246 s\n",
      "features size:  (20, 1024)\n",
      "coordinates size:  (20, 2)\n",
      "\n",
      "progress: 16/206\n",
      "C3L-00263-24\n",
      "downsample [16.00330813 16.00608828]\n",
      "downsampled_level_dim [2116 1314]\n",
      "level_dim [2116 1314]\n",
      "name C3L-00263-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00263-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00263-24.h5 took 0.31761765480041504 s\n",
      "features size:  (16, 1024)\n",
      "coordinates size:  (16, 2)\n",
      "\n",
      "progress: 17/206\n",
      "C3L-00263-25\n",
      "downsample [16.00861079 16.00105208]\n",
      "downsampled_level_dim [1742 1901]\n",
      "level_dim [1742 1901]\n",
      "name C3L-00263-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00263-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00263-25.h5 took 0.30028653144836426 s\n",
      "features size:  (14, 1024)\n",
      "coordinates size:  (14, 2)\n",
      "\n",
      "progress: 18/206\n",
      "C3L-00279-21\n",
      "downsample [8.00255661 8.00172652]\n",
      "downsampled_level_dim [2738 2896]\n",
      "level_dim [2738 2896]\n",
      "name C3L-00279-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00279-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00279-21.h5 took 0.5791141986846924 s\n",
      "features size:  (55, 1024)\n",
      "coordinates size:  (55, 2)\n",
      "\n",
      "progress: 19/206\n",
      "C3L-00279-22\n",
      "downsample [8.00255661 8.00213295]\n",
      "downsampled_level_dim [2738 2813]\n",
      "level_dim [2738 2813]\n",
      "name C3L-00279-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00279-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00279-22.h5 took 0.5303330421447754 s\n",
      "features size:  (46, 1024)\n",
      "coordinates size:  (46, 2)\n",
      "\n",
      "progress: 20/206\n",
      "C3L-00279-23\n",
      "downsample [8.00255661 8.        ]\n",
      "downsampled_level_dim [2738 2618]\n",
      "level_dim [2738 2618]\n",
      "name C3L-00279-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00279-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00279-23.h5 took 0.5018181800842285 s\n",
      "features size:  (43, 1024)\n",
      "coordinates size:  (43, 2)\n",
      "\n",
      "progress: 21/206\n",
      "C3L-00368-21\n",
      "downsample [16.00511322 16.0087925 ]\n",
      "downsampled_level_dim [1369 1706]\n",
      "level_dim [1369 1706]\n",
      "name C3L-00368-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00368-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00368-21.h5 took 0.31067466735839844 s\n",
      "features size:  (16, 1024)\n",
      "coordinates size:  (16, 2)\n",
      "\n",
      "progress: 22/206\n",
      "C3L-00368-22\n",
      "downsample [16.0075339  16.00949153]\n",
      "downsampled_level_dim [1991 1475]\n",
      "level_dim [1991 1475]\n",
      "name C3L-00368-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00368-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00368-22.h5 took 0.3393378257751465 s\n",
      "features size:  (17, 1024)\n",
      "coordinates size:  (17, 2)\n",
      "\n",
      "progress: 23/206\n",
      "C3L-00412-21\n",
      "downsample [16.00330813 16.00146056]\n",
      "downsampled_level_dim [2116 2054]\n",
      "level_dim [2116 2054]\n",
      "name C3L-00412-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00412-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00412-21.h5 took 0.4351472854614258 s\n",
      "features size:  (32, 1024)\n",
      "coordinates size:  (32, 2)\n",
      "\n",
      "progress: 24/206\n",
      "C3L-00415-21\n",
      "downsample [16.00669643 16.00500626]\n",
      "downsampled_level_dim [2240 1598]\n",
      "level_dim [2240 1598]\n",
      "name C3L-00415-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00415-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00415-21.h5 took 0.326460599899292 s\n",
      "features size:  (17, 1024)\n",
      "coordinates size:  (17, 2)\n",
      "\n",
      "progress: 25/206\n",
      "C3L-00415-23\n",
      "downsample [16.00602652 16.00544617]\n",
      "downsampled_level_dim [2489 2387]\n",
      "level_dim [2489 2387]\n",
      "name C3L-00415-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00415-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00415-23.h5 took 0.42401599884033203 s\n",
      "features size:  (32, 1024)\n",
      "coordinates size:  (32, 2)\n",
      "\n",
      "progress: 26/206\n",
      "C3L-00422-21\n",
      "downsample [8.00216316 8.00091283]\n",
      "downsampled_level_dim [3236 2191]\n",
      "level_dim [3236 2191]\n",
      "name C3L-00422-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00422-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00422-21.h5 took 0.4899313449859619 s\n",
      "features size:  (40, 1024)\n",
      "coordinates size:  (40, 2)\n",
      "\n",
      "progress: 27/206\n",
      "C3L-00422-22\n",
      "downsample [8.00255661 8.        ]\n",
      "downsampled_level_dim [2738 1914]\n",
      "level_dim [2738 1914]\n",
      "name C3L-00422-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00422-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00422-22.h5 took 0.411602258682251 s\n",
      "features size:  (30, 1024)\n",
      "coordinates size:  (30, 2)\n",
      "\n",
      "progress: 28/206\n",
      "C3L-00422-23\n",
      "downsample [16.00330813 16.        ]\n",
      "downsampled_level_dim [2116 1837]\n",
      "level_dim [2116 1837]\n",
      "name C3L-00422-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00422-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00422-23.h5 took 0.2766706943511963 s\n",
      "features size:  (8, 1024)\n",
      "coordinates size:  (8, 2)\n",
      "\n",
      "progress: 29/206\n",
      "C3L-00444-21\n",
      "downsample [16.00110254 16.00517063]\n",
      "downsampled_level_dim [6349 2901]\n",
      "level_dim [6349 2901]\n",
      "name C3L-00444-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00444-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00444-21.h5 took 1.1633684635162354 s\n",
      "features size:  (100, 1024)\n",
      "coordinates size:  (100, 2)\n",
      "\n",
      "progress: 30/206\n",
      "C3L-00444-22\n",
      "downsample [16.00317125 16.00407886]\n",
      "downsampled_level_dim [4730 2942]\n",
      "level_dim [4730 2942]\n",
      "name C3L-00444-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00444-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00444-22.h5 took 0.8456614017486572 s\n",
      "features size:  (82, 1024)\n",
      "coordinates size:  (82, 2)\n",
      "\n",
      "progress: 31/206\n",
      "C3L-00444-23\n",
      "downsample [16.00273873 16.0034965 ]\n",
      "downsampled_level_dim [5477 2860]\n",
      "level_dim [5477 2860]\n",
      "name C3L-00444-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00444-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00444-23.h5 took 0.7859649658203125 s\n",
      "features size:  (65, 1024)\n",
      "coordinates size:  (65, 2)\n",
      "\n",
      "progress: 32/206\n",
      "C3L-00445-21\n",
      "downsample [16.00215177 16.00524705]\n",
      "downsampled_level_dim [6971 2287]\n",
      "level_dim [6971 2287]\n",
      "name C3L-00445-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00445-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00445-21.h5 took 1.2385718822479248 s\n",
      "features size:  (134, 1024)\n",
      "coordinates size:  (134, 2)\n",
      "\n",
      "progress: 33/206\n",
      "C3L-00445-25\n",
      "downsample [16.00106093 16.0029889 ]\n",
      "downsampled_level_dim [6598 2342]\n",
      "level_dim [6598 2342]\n",
      "name C3L-00445-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00445-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00445-25.h5 took 1.0781352519989014 s\n",
      "features size:  (119, 1024)\n",
      "coordinates size:  (119, 2)\n",
      "\n",
      "progress: 34/206\n",
      "C3L-00445-26\n",
      "downsample [16.00102235 16.00462784]\n",
      "downsampled_level_dim [6847 2593]\n",
      "level_dim [6847 2593]\n",
      "name C3L-00445-26\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00445-26.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00445-26.h5 took 1.3930692672729492 s\n",
      "features size:  (152, 1024)\n",
      "coordinates size:  (152, 2)\n",
      "\n",
      "progress: 35/206\n",
      "C3L-00446-21\n",
      "downsample [16.00223148 16.00268817]\n",
      "downsampled_level_dim [6722 2232]\n",
      "level_dim [6722 2232]\n",
      "name C3L-00446-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00446-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00446-21.h5 took 1.1882355213165283 s\n",
      "features size:  (119, 1024)\n",
      "coordinates size:  (119, 2)\n",
      "\n",
      "progress: 36/206\n",
      "C3L-00446-22\n",
      "downsample [16.00273873 16.00175131]\n",
      "downsampled_level_dim [5477 1713]\n",
      "level_dim [5477 1713]\n",
      "name C3L-00446-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00446-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00446-22.h5 took 0.6671068668365479 s\n",
      "features size:  (58, 1024)\n",
      "coordinates size:  (58, 2)\n",
      "\n",
      "progress: 37/206\n",
      "C3L-00446-23\n",
      "downsample [16.00130768 16.00207147]\n",
      "downsampled_level_dim [5353 1931]\n",
      "level_dim [5353 1931]\n",
      "name C3L-00446-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00446-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00446-23.h5 took 0.697249174118042 s\n",
      "features size:  (68, 1024)\n",
      "coordinates size:  (68, 2)\n",
      "\n",
      "progress: 38/206\n",
      "C3L-00446-24\n",
      "downsample [16.00286917 16.        ]\n",
      "downsampled_level_dim [5228 2304]\n",
      "level_dim [5228 2304]\n",
      "name C3L-00446-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00446-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00446-24.h5 took 0.813330888748169 s\n",
      "features size:  (76, 1024)\n",
      "coordinates size:  (76, 2)\n",
      "\n",
      "progress: 39/206\n",
      "C3L-00503-21\n",
      "downsample [16.00432633 16.0096463 ]\n",
      "downsampled_level_dim [1618 1555]\n",
      "level_dim [1618 1555]\n",
      "name C3L-00503-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00503-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00503-21.h5 took 0.3134760856628418 s\n",
      "features size:  (9, 1024)\n",
      "coordinates size:  (9, 2)\n",
      "\n",
      "progress: 40/206\n",
      "C3L-00503-22\n",
      "downsample [8.00255661 8.00116959]\n",
      "downsampled_level_dim [2738 2565]\n",
      "level_dim [2738 2565]\n",
      "name C3L-00503-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00503-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00503-22.h5 took 0.4478611946105957 s\n",
      "features size:  (33, 1024)\n",
      "coordinates size:  (33, 2)\n",
      "\n",
      "progress: 41/206\n",
      "C3L-00503-23\n",
      "downsample [16.00374933 16.00228571]\n",
      "downsampled_level_dim [1867 1750]\n",
      "level_dim [1867 1750]\n",
      "name C3L-00503-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00503-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00503-23.h5 took 0.3674173355102539 s\n",
      "features size:  (21, 1024)\n",
      "coordinates size:  (21, 2)\n",
      "\n",
      "progress: 42/206\n",
      "C3L-00510-21\n",
      "downsample [16.0075339  16.00568182]\n",
      "downsampled_level_dim [1991 1584]\n",
      "level_dim [1991 1584]\n",
      "name C3L-00510-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00510-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00510-21.h5 took 0.33763718605041504 s\n",
      "features size:  (19, 1024)\n",
      "coordinates size:  (19, 2)\n",
      "\n",
      "progress: 43/206\n",
      "C3L-00510-22\n",
      "downsample [16.00374933 16.00374532]\n",
      "downsampled_level_dim [1867 1869]\n",
      "level_dim [1867 1869]\n",
      "name C3L-00510-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00510-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00510-22.h5 took 0.3024892807006836 s\n",
      "features size:  (14, 1024)\n",
      "coordinates size:  (14, 2)\n",
      "\n",
      "progress: 44/206\n",
      "C3L-00510-23\n",
      "downsample [8.00234349 8.00035817]\n",
      "downsampled_level_dim [2987 2792]\n",
      "level_dim [2987 2792]\n",
      "name C3L-00510-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00510-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00510-23.h5 took 0.554924726486206 s\n",
      "features size:  (51, 1024)\n",
      "coordinates size:  (51, 2)\n",
      "\n",
      "progress: 45/206\n",
      "C3L-00568-21\n",
      "downsample [16.01004689 16.00335796]\n",
      "downsampled_level_dim [1493 1489]\n",
      "level_dim [1493 1489]\n",
      "name C3L-00568-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00568-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00568-21.h5 took 0.32436680793762207 s\n",
      "features size:  (12, 1024)\n",
      "coordinates size:  (12, 2)\n",
      "\n",
      "progress: 46/206\n",
      "C3L-00568-22\n",
      "downsample [8.003125   8.00230858]\n",
      "downsampled_level_dim [2240 2599]\n",
      "level_dim [2240 2599]\n",
      "name C3L-00568-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00568-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00568-22.h5 took 0.41684460639953613 s\n",
      "features size:  (29, 1024)\n",
      "coordinates size:  (29, 2)\n",
      "\n",
      "progress: 47/206\n",
      "C3L-00568-23\n",
      "downsample [8.00281237 8.00261097]\n",
      "downsampled_level_dim [2489 2298]\n",
      "level_dim [2489 2298]\n",
      "name C3L-00568-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00568-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00568-23.h5 took 0.3132803440093994 s\n",
      "features size:  (14, 1024)\n",
      "coordinates size:  (14, 2)\n",
      "\n",
      "progress: 48/206\n",
      "C3L-00603-21\n",
      "downsample [8.00351582 8.0027688 ]\n",
      "downsampled_level_dim [1991 2167]\n",
      "level_dim [1991 2167]\n",
      "name C3L-00603-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00603-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00603-21.h5 took 0.3523070812225342 s\n",
      "features size:  (18, 1024)\n",
      "coordinates size:  (18, 2)\n",
      "\n",
      "progress: 49/206\n",
      "C3L-00603-22\n",
      "downsample [8.00281237 8.00180963]\n",
      "downsampled_level_dim [2489 2763]\n",
      "level_dim [2489 2763]\n",
      "name C3L-00603-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00603-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00603-22.h5 took 0.46324658393859863 s\n",
      "features size:  (35, 1024)\n",
      "coordinates size:  (35, 2)\n",
      "\n",
      "progress: 50/206\n",
      "C3L-00604-21\n",
      "downsample [8.00281237 8.00110619]\n",
      "downsampled_level_dim [2489 2712]\n",
      "level_dim [2489 2712]\n",
      "name C3L-00604-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00604-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00604-21.h5 took 0.45235705375671387 s\n",
      "features size:  (32, 1024)\n",
      "coordinates size:  (32, 2)\n",
      "\n",
      "progress: 51/206\n",
      "C3L-00604-22\n",
      "downsample [8.00281237 8.00082576]\n",
      "downsampled_level_dim [2489 2422]\n",
      "level_dim [2489 2422]\n",
      "name C3L-00604-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00604-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00604-22.h5 took 0.4244678020477295 s\n",
      "features size:  (31, 1024)\n",
      "coordinates size:  (31, 2)\n",
      "\n",
      "progress: 52/206\n",
      "C3L-00604-23\n",
      "downsample [8.00234349 8.00039952]\n",
      "downsampled_level_dim [2987 2503]\n",
      "level_dim [2987 2503]\n",
      "name C3L-00604-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00604-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00604-23.h5 took 0.485414981842041 s\n",
      "features size:  (38, 1024)\n",
      "coordinates size:  (38, 2)\n",
      "\n",
      "progress: 53/206\n",
      "C3L-00893-21\n",
      "downsample [8.00281237 8.00083612]\n",
      "downsampled_level_dim [2489 2392]\n",
      "level_dim [2489 2392]\n",
      "name C3L-00893-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00893-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00893-21.h5 took 0.3669869899749756 s\n",
      "features size:  (23, 1024)\n",
      "coordinates size:  (23, 2)\n",
      "\n",
      "progress: 54/206\n",
      "C3L-00893-22\n",
      "downsample [8.00200861 8.00245614]\n",
      "downsampled_level_dim [3485 2850]\n",
      "level_dim [3485 2850]\n",
      "name C3L-00893-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00893-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00893-22.h5 took 0.46326112747192383 s\n",
      "features size:  (34, 1024)\n",
      "coordinates size:  (34, 2)\n",
      "\n",
      "progress: 55/206\n",
      "C3L-00893-23\n",
      "downsample [16.00511322 16.        ]\n",
      "downsampled_level_dim [1369 1552]\n",
      "level_dim [1369 1552]\n",
      "name C3L-00893-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00893-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00893-23.h5 took 0.3079719543457031 s\n",
      "features size:  (10, 1024)\n",
      "coordinates size:  (10, 2)\n",
      "\n",
      "progress: 56/206\n",
      "C3L-00904-21\n",
      "downsample [16.00295983 16.        ]\n",
      "downsampled_level_dim [2365 3159]\n",
      "level_dim [2365 3159]\n",
      "name C3L-00904-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00904-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00904-21.h5 took 0.6009085178375244 s\n",
      "features size:  (54, 1024)\n",
      "coordinates size:  (54, 2)\n",
      "\n",
      "progress: 57/206\n",
      "C3L-00904-22\n",
      "downsample [16.00374933 16.004     ]\n",
      "downsampled_level_dim [1867 1750]\n",
      "level_dim [1867 1750]\n",
      "name C3L-00904-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00904-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00904-22.h5 took 0.39412569999694824 s\n",
      "features size:  (24, 1024)\n",
      "coordinates size:  (24, 2)\n",
      "\n",
      "progress: 58/206\n",
      "C3L-00913-21\n",
      "downsample [16.00432633 16.        ]\n",
      "downsampled_level_dim [1618 1845]\n",
      "level_dim [1618 1845]\n",
      "name C3L-00913-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00913-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00913-21.h5 took 0.30746912956237793 s\n",
      "features size:  (13, 1024)\n",
      "coordinates size:  (13, 2)\n",
      "\n",
      "progress: 59/206\n",
      "C3L-00913-22\n",
      "downsample [16.00669643 16.00260191]\n",
      "downsampled_level_dim [2240 2306]\n",
      "level_dim [2240 2306]\n",
      "name C3L-00913-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00913-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00913-22.h5 took 0.37636566162109375 s\n",
      "features size:  (24, 1024)\n",
      "coordinates size:  (24, 2)\n",
      "\n",
      "progress: 60/206\n",
      "C3L-00913-23\n",
      "downsample [16.01004689 16.00230947]\n",
      "downsampled_level_dim [1493 1732]\n",
      "level_dim [1493 1732]\n",
      "name C3L-00913-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00913-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00913-23.h5 took 0.27725934982299805 s\n",
      "features size:  (9, 1024)\n",
      "coordinates size:  (9, 2)\n",
      "\n",
      "progress: 61/206\n",
      "C3L-00923-23\n",
      "downsample [16.00861079 16.00229753]\n",
      "downsampled_level_dim [1742 1741]\n",
      "level_dim [1742 1741]\n",
      "name C3L-00923-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00923-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00923-23.h5 took 0.33798718452453613 s\n",
      "features size:  (18, 1024)\n",
      "coordinates size:  (18, 2)\n",
      "\n",
      "progress: 62/206\n",
      "C3L-00927-21\n",
      "downsample [8.00281237 8.00229991]\n",
      "downsampled_level_dim [2489 2174]\n",
      "level_dim [2489 2174]\n",
      "name C3L-00927-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00927-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00927-21.h5 took 0.4325263500213623 s\n",
      "features size:  (31, 1024)\n",
      "coordinates size:  (31, 2)\n",
      "\n",
      "progress: 63/206\n",
      "C3L-00927-22\n",
      "downsample [8.00281237 8.00310284]\n",
      "downsampled_level_dim [2489 2256]\n",
      "level_dim [2489 2256]\n",
      "name C3L-00927-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00927-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00927-22.h5 took 0.44827866554260254 s\n",
      "features size:  (34, 1024)\n",
      "coordinates size:  (34, 2)\n",
      "\n",
      "progress: 64/206\n",
      "C3L-00927-23\n",
      "downsample [8.00255661 8.0025344 ]\n",
      "downsampled_level_dim [2738 2762]\n",
      "level_dim [2738 2762]\n",
      "name C3L-00927-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00927-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00927-23.h5 took 0.47113823890686035 s\n",
      "features size:  (36, 1024)\n",
      "coordinates size:  (36, 2)\n",
      "\n",
      "progress: 65/206\n",
      "C3L-00965-21\n",
      "downsample [8.00281237 8.00197044]\n",
      "downsampled_level_dim [2489 2030]\n",
      "level_dim [2489 2030]\n",
      "name C3L-00965-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00965-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00965-21.h5 took 0.4380149841308594 s\n",
      "features size:  (33, 1024)\n",
      "coordinates size:  (33, 2)\n",
      "\n",
      "progress: 66/206\n",
      "C3L-00965-22\n",
      "downsample [8.003125   8.00048473]\n",
      "downsampled_level_dim [2240 2063]\n",
      "level_dim [2240 2063]\n",
      "name C3L-00965-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00965-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00965-22.h5 took 0.4134495258331299 s\n",
      "features size:  (29, 1024)\n",
      "coordinates size:  (29, 2)\n",
      "\n",
      "progress: 67/206\n",
      "C3L-00965-23\n",
      "downsample [8.00351582 8.001999  ]\n",
      "downsampled_level_dim [1991 2001]\n",
      "level_dim [1991 2001]\n",
      "name C3L-00965-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00965-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00965-23.h5 took 0.384845495223999 s\n",
      "features size:  (24, 1024)\n",
      "coordinates size:  (24, 2)\n",
      "\n",
      "progress: 68/206\n",
      "C3L-00965-24\n",
      "downsample [8.003125   8.00194704]\n",
      "downsampled_level_dim [2240 2568]\n",
      "level_dim [2240 2568]\n",
      "name C3L-00965-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00965-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00965-24.h5 took 0.44971418380737305 s\n",
      "features size:  (34, 1024)\n",
      "coordinates size:  (34, 2)\n",
      "\n",
      "progress: 69/206\n",
      "C3L-00973-21\n",
      "downsample [16.00267789 16.00427148]\n",
      "downsampled_level_dim [2614 2107]\n",
      "level_dim [2614 2107]\n",
      "name C3L-00973-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00973-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00973-21.h5 took 0.4508092403411865 s\n",
      "features size:  (34, 1024)\n",
      "coordinates size:  (34, 2)\n",
      "\n",
      "progress: 70/206\n",
      "C3L-00973-22\n",
      "downsample [16.00244499 16.00273473]\n",
      "downsampled_level_dim [2863 2194]\n",
      "level_dim [2863 2194]\n",
      "name C3L-00973-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00973-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00973-22.h5 took 0.5305333137512207 s\n",
      "features size:  (44, 1024)\n",
      "coordinates size:  (44, 2)\n",
      "\n",
      "progress: 71/206\n",
      "C3L-00973-23\n",
      "downsample [16.00244499 16.00437254]\n",
      "downsampled_level_dim [2863 2287]\n",
      "level_dim [2863 2287]\n",
      "name C3L-00973-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00973-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00973-23.h5 took 0.5450026988983154 s\n",
      "features size:  (47, 1024)\n",
      "coordinates size:  (47, 2)\n",
      "\n",
      "progress: 72/206\n",
      "C3L-00973-24\n",
      "downsample [16.00267789 16.        ]\n",
      "downsampled_level_dim [2614 2077]\n",
      "level_dim [2614 2077]\n",
      "name C3L-00973-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00973-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00973-24.h5 took 0.4531559944152832 s\n",
      "features size:  (34, 1024)\n",
      "coordinates size:  (34, 2)\n",
      "\n",
      "progress: 73/206\n",
      "C3L-00973-25\n",
      "downsample [16.0075339  16.00054113]\n",
      "downsampled_level_dim [1991 1848]\n",
      "level_dim [1991 1848]\n",
      "name C3L-00973-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00973-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00973-25.h5 took 0.37053775787353516 s\n",
      "features size:  (23, 1024)\n",
      "coordinates size:  (23, 2)\n",
      "\n",
      "progress: 74/206\n",
      "C3L-00993-21\n",
      "downsample [16.00374933 16.00633312]\n",
      "downsampled_level_dim [1867 1579]\n",
      "level_dim [1867 1579]\n",
      "name C3L-00993-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00993-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00993-21.h5 took 0.3506441116333008 s\n",
      "features size:  (19, 1024)\n",
      "coordinates size:  (19, 2)\n",
      "\n",
      "progress: 75/206\n",
      "C3L-00993-22\n",
      "downsample [16.00669643 16.00216826]\n",
      "downsampled_level_dim [2240 2306]\n",
      "level_dim [2240 2306]\n",
      "name C3L-00993-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00993-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00993-22.h5 took 0.42247748374938965 s\n",
      "features size:  (28, 1024)\n",
      "coordinates size:  (28, 2)\n",
      "\n",
      "progress: 76/206\n",
      "C3L-00993-23\n",
      "downsample [16.0075339  16.00231696]\n",
      "downsampled_level_dim [1991 2158]\n",
      "level_dim [1991 2158]\n",
      "name C3L-00993-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00993-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00993-23.h5 took 0.3773837089538574 s\n",
      "features size:  (24, 1024)\n",
      "coordinates size:  (24, 2)\n",
      "\n",
      "progress: 77/206\n",
      "C3L-00993-24\n",
      "downsample [16.00330813 16.00044131]\n",
      "downsampled_level_dim [2116 2266]\n",
      "level_dim [2116 2266]\n",
      "name C3L-00993-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-00993-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-00993-24.h5 took 0.41525840759277344 s\n",
      "features size:  (27, 1024)\n",
      "coordinates size:  (27, 2)\n",
      "\n",
      "progress: 78/206\n",
      "C3L-01000-24\n",
      "downsample [16.00244499 16.00675676]\n",
      "downsampled_level_dim [2863 1036]\n",
      "level_dim [2863 1036]\n",
      "name C3L-01000-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01000-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01000-24.h5 took 0.35448217391967773 s\n",
      "features size:  (21, 1024)\n",
      "coordinates size:  (21, 2)\n",
      "\n",
      "progress: 79/206\n",
      "C3L-01000-25\n",
      "downsample [16.00602652 16.00121729]\n",
      "downsampled_level_dim [2489 1643]\n",
      "level_dim [2489 1643]\n",
      "name C3L-01000-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01000-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01000-25.h5 took 0.4040699005126953 s\n",
      "features size:  (26, 1024)\n",
      "coordinates size:  (26, 2)\n",
      "\n",
      "progress: 80/206\n",
      "C3L-01285-21\n",
      "downsample [16.00511322 16.00713822]\n",
      "downsampled_level_dim [1369 1541]\n",
      "level_dim [1369 1541]\n",
      "name C3L-01285-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01285-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01285-21.h5 took 0.31719517707824707 s\n",
      "features size:  (11, 1024)\n",
      "coordinates size:  (11, 2)\n",
      "\n",
      "progress: 81/206\n",
      "C3L-01285-22\n",
      "downsample [8.00255661 8.        ]\n",
      "downsampled_level_dim [2738 2391]\n",
      "level_dim [2738 2391]\n",
      "name C3L-01285-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01285-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01285-22.h5 took 0.43665242195129395 s\n",
      "features size:  (33, 1024)\n",
      "coordinates size:  (33, 2)\n",
      "\n",
      "progress: 82/206\n",
      "C3L-01285-23\n",
      "downsample [16.00625    16.00650118]\n",
      "downsampled_level_dim [1120 1692]\n",
      "level_dim [1120 1692]\n",
      "name C3L-01285-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01285-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01285-23.h5 took 0.3050210475921631 s\n",
      "features size:  (12, 1024)\n",
      "coordinates size:  (12, 2)\n",
      "\n",
      "progress: 83/206\n",
      "C3L-01330-21\n",
      "downsample [8.00401837 8.00309119]\n",
      "downsampled_level_dim [1742 1941]\n",
      "level_dim [1742 1941]\n",
      "name C3L-01330-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01330-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01330-21.h5 took 0.37706899642944336 s\n",
      "features size:  (23, 1024)\n",
      "coordinates size:  (23, 2)\n",
      "\n",
      "progress: 84/206\n",
      "C3L-01330-22\n",
      "downsample [8.00255661 8.00045167]\n",
      "downsampled_level_dim [2738 2214]\n",
      "level_dim [2738 2214]\n",
      "name C3L-01330-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01330-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01330-22.h5 took 0.3584463596343994 s\n",
      "features size:  (21, 1024)\n",
      "coordinates size:  (21, 2)\n",
      "\n",
      "progress: 85/206\n",
      "C3L-01330-23\n",
      "downsample [8.00281237 8.00198413]\n",
      "downsampled_level_dim [2489 1512]\n",
      "level_dim [2489 1512]\n",
      "name C3L-01330-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01330-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01330-23.h5 took 0.39415597915649414 s\n",
      "features size:  (25, 1024)\n",
      "coordinates size:  (25, 2)\n",
      "\n",
      "progress: 86/206\n",
      "C3L-01455-21\n",
      "downsample [16.01507538 16.00854139]\n",
      "downsampled_level_dim [ 995 1522]\n",
      "level_dim [ 995 1522]\n",
      "name C3L-01455-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01455-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01455-21.h5 took 0.28600144386291504 s\n",
      "features size:  (7, 1024)\n",
      "coordinates size:  (7, 2)\n",
      "\n",
      "progress: 87/206\n",
      "C3L-01455-22\n",
      "downsample [8.00401837 8.00156801]\n",
      "downsampled_level_dim [1742 2551]\n",
      "level_dim [1742 2551]\n",
      "name C3L-01455-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01455-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01455-22.h5 took 0.35356593132019043 s\n",
      "features size:  (20, 1024)\n",
      "coordinates size:  (20, 2)\n",
      "\n",
      "progress: 88/206\n",
      "C3L-01455-23\n",
      "downsample [16.00511322 16.00406504]\n",
      "downsampled_level_dim [1369 1722]\n",
      "level_dim [1369 1722]\n",
      "name C3L-01455-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01455-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01455-23.h5 took 0.263735294342041 s\n",
      "features size:  (7, 1024)\n",
      "coordinates size:  (7, 2)\n",
      "\n",
      "progress: 89/206\n",
      "C3L-01606-21\n",
      "downsample [16.00511322 16.0077381 ]\n",
      "downsampled_level_dim [1369 1680]\n",
      "level_dim [1369 1680]\n",
      "name C3L-01606-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01606-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01606-21.h5 took 0.30367398262023926 s\n",
      "features size:  (13, 1024)\n",
      "coordinates size:  (13, 2)\n",
      "\n",
      "progress: 90/206\n",
      "C3L-01632-21\n",
      "downsample [16.00511322 16.00722022]\n",
      "downsampled_level_dim [1369 1939]\n",
      "level_dim [1369 1939]\n",
      "name C3L-01632-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01632-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01632-21.h5 took 0.3331880569458008 s\n",
      "features size:  (17, 1024)\n",
      "coordinates size:  (17, 2)\n",
      "\n",
      "progress: 91/206\n",
      "C3L-01632-22\n",
      "downsample [16.00511322 16.00516432]\n",
      "downsampled_level_dim [1369 2130]\n",
      "level_dim [1369 2130]\n",
      "name C3L-01632-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01632-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01632-22.h5 took 0.36768269538879395 s\n",
      "features size:  (23, 1024)\n",
      "coordinates size:  (23, 2)\n",
      "\n",
      "progress: 92/206\n",
      "C3L-01632-23\n",
      "downsample [8.00351582 8.00214516]\n",
      "downsampled_level_dim [1991 2797]\n",
      "level_dim [1991 2797]\n",
      "name C3L-01632-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01632-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01632-23.h5 took 0.4650604724884033 s\n",
      "features size:  (36, 1024)\n",
      "coordinates size:  (36, 2)\n",
      "\n",
      "progress: 93/206\n",
      "C3L-01663-22\n",
      "downsample [8.00281237 8.00141543]\n",
      "downsampled_level_dim [2489 2826]\n",
      "level_dim [2489 2826]\n",
      "name C3L-01663-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01663-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01663-22.h5 took 0.4376039505004883 s\n",
      "features size:  (30, 1024)\n",
      "coordinates size:  (30, 2)\n",
      "\n",
      "progress: 94/206\n",
      "C3L-01682-21\n",
      "downsample [8.00281237 8.        ]\n",
      "downsampled_level_dim [2489 2257]\n",
      "level_dim [2489 2257]\n",
      "name C3L-01682-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01682-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01682-21.h5 took 0.43923306465148926 s\n",
      "features size:  (32, 1024)\n",
      "coordinates size:  (32, 2)\n",
      "\n",
      "progress: 95/206\n",
      "C3L-01682-23\n",
      "downsample [8.00281237 8.00185701]\n",
      "downsampled_level_dim [2489 2154]\n",
      "level_dim [2489 2154]\n",
      "name C3L-01682-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01682-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01682-23.h5 took 0.334550142288208 s\n",
      "features size:  (17, 1024)\n",
      "coordinates size:  (17, 2)\n",
      "\n",
      "progress: 96/206\n",
      "C3L-01683-26\n",
      "downsample [8.00216316 8.00136752]\n",
      "downsampled_level_dim [3236 2925]\n",
      "level_dim [3236 2925]\n",
      "name C3L-01683-26\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01683-26.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01683-26.h5 took 0.5617473125457764 s\n",
      "features size:  (49, 1024)\n",
      "coordinates size:  (49, 2)\n",
      "\n",
      "progress: 97/206\n",
      "C3L-01683-27\n",
      "downsample [8.00255661 8.00039761]\n",
      "downsampled_level_dim [2738 2515]\n",
      "level_dim [2738 2515]\n",
      "name C3L-01683-27\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01683-27.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01683-27.h5 took 0.5131685733795166 s\n",
      "features size:  (39, 1024)\n",
      "coordinates size:  (39, 2)\n",
      "\n",
      "progress: 98/206\n",
      "C3L-01838-23\n",
      "downsample [8.00255661 8.00218103]\n",
      "downsampled_level_dim [2738 2751]\n",
      "level_dim [2738 2751]\n",
      "name C3L-01838-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01838-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01838-23.h5 took 0.4671206474304199 s\n",
      "features size:  (36, 1024)\n",
      "coordinates size:  (36, 2)\n",
      "\n",
      "progress: 99/206\n",
      "C3L-01862-21\n",
      "downsample [8.00200861 8.00168634]\n",
      "downsampled_level_dim [3485 2965]\n",
      "level_dim [3485 2965]\n",
      "name C3L-01862-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01862-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01862-21.h5 took 0.498776912689209 s\n",
      "features size:  (41, 1024)\n",
      "coordinates size:  (41, 2)\n",
      "\n",
      "progress: 100/206\n",
      "C3L-01862-22\n",
      "downsample [8.00281237 8.00116414]\n",
      "downsampled_level_dim [2489 2577]\n",
      "level_dim [2489 2577]\n",
      "name C3L-01862-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01862-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01862-22.h5 took 0.37625885009765625 s\n",
      "features size:  (22, 1024)\n",
      "coordinates size:  (22, 2)\n",
      "\n",
      "progress: 101/206\n",
      "C3L-01862-23\n",
      "downsample [8.00281237 8.00238569]\n",
      "downsampled_level_dim [2489 2515]\n",
      "level_dim [2489 2515]\n",
      "name C3L-01862-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01862-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01862-23.h5 took 0.4274260997772217 s\n",
      "features size:  (31, 1024)\n",
      "coordinates size:  (31, 2)\n",
      "\n",
      "progress: 102/206\n",
      "C3L-01884-21\n",
      "downsample [16.00511322 16.00508906]\n",
      "downsampled_level_dim [1369 1572]\n",
      "level_dim [1369 1572]\n",
      "name C3L-01884-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01884-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01884-21.h5 took 0.2912435531616211 s\n",
      "features size:  (11, 1024)\n",
      "coordinates size:  (11, 2)\n",
      "\n",
      "progress: 103/206\n",
      "C3L-01884-22\n",
      "downsample [8.00255661 8.00037608]\n",
      "downsampled_level_dim [2738 2659]\n",
      "level_dim [2738 2659]\n",
      "name C3L-01884-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01884-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01884-22.h5 took 0.378248929977417 s\n",
      "features size:  (22, 1024)\n",
      "coordinates size:  (22, 2)\n",
      "\n",
      "progress: 104/206\n",
      "C3L-01889-21\n",
      "downsample [16.01004689 16.        ]\n",
      "downsampled_level_dim [1493 1546]\n",
      "level_dim [1493 1546]\n",
      "name C3L-01889-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01889-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01889-21.h5 took 0.27780771255493164 s\n",
      "features size:  (9, 1024)\n",
      "coordinates size:  (9, 2)\n",
      "\n",
      "progress: 105/206\n",
      "C3L-01890-22\n",
      "downsample [8.00255661 8.00084424]\n",
      "downsampled_level_dim [2738 2369]\n",
      "level_dim [2738 2369]\n",
      "name C3L-01890-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01890-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01890-22.h5 took 0.4135415554046631 s\n",
      "features size:  (29, 1024)\n",
      "coordinates size:  (29, 2)\n",
      "\n",
      "progress: 106/206\n",
      "C3L-01924-21\n",
      "downsample [8.         8.00141643]\n",
      "downsampled_level_dim [2378 2824]\n",
      "level_dim [2378 2824]\n",
      "name C3L-01924-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01924-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01924-21.h5 took 0.6751134395599365 s\n",
      "features size:  (58, 1024)\n",
      "coordinates size:  (58, 2)\n",
      "\n",
      "progress: 107/206\n",
      "C3L-01924-22\n",
      "downsample [16.00337648 16.        ]\n",
      "downsampled_level_dim [1777 1682]\n",
      "level_dim [1777 1682]\n",
      "name C3L-01924-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01924-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01924-22.h5 took 0.433671236038208 s\n",
      "features size:  (28, 1024)\n",
      "coordinates size:  (28, 2)\n",
      "\n",
      "progress: 108/206\n",
      "C3L-01924-23\n",
      "downsample [8.00317662 8.00199283]\n",
      "downsampled_level_dim [1574 2509]\n",
      "level_dim [1574 2509]\n",
      "name C3L-01924-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01924-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01924-23.h5 took 0.5621843338012695 s\n",
      "features size:  (43, 1024)\n",
      "coordinates size:  (43, 2)\n",
      "\n",
      "progress: 109/206\n",
      "C3L-01924-24\n",
      "downsample [16.00555942 16.00451031]\n",
      "downsampled_level_dim [1439 1552]\n",
      "level_dim [1439 1552]\n",
      "name C3L-01924-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01924-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01924-24.h5 took 0.40117979049682617 s\n",
      "features size:  (24, 1024)\n",
      "coordinates size:  (24, 2)\n",
      "\n",
      "progress: 110/206\n",
      "C3L-01924-25\n",
      "downsample [8.000342   8.00234192]\n",
      "downsampled_level_dim [2924 2562]\n",
      "level_dim [2924 2562]\n",
      "name C3L-01924-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-01924-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-01924-25.h5 took 0.7740817070007324 s\n",
      "features size:  (68, 1024)\n",
      "coordinates size:  (68, 2)\n",
      "\n",
      "progress: 111/206\n",
      "C3L-02127-21\n",
      "downsample [16.00317125 16.0043573 ]\n",
      "downsampled_level_dim [4730 1836]\n",
      "level_dim [4730 1836]\n",
      "name C3L-02127-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02127-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02127-21.h5 took 0.5822112560272217 s\n",
      "features size:  (39, 1024)\n",
      "coordinates size:  (39, 2)\n",
      "\n",
      "progress: 112/206\n",
      "C3L-02127-22\n",
      "downsample [16.00137147 16.00526039]\n",
      "downsampled_level_dim [5104 1901]\n",
      "level_dim [5104 1901]\n",
      "name C3L-02127-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02127-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02127-22.h5 took 0.6340057849884033 s\n",
      "features size:  (53, 1024)\n",
      "coordinates size:  (53, 2)\n",
      "\n",
      "progress: 113/206\n",
      "C3L-02127-23\n",
      "downsample [16.00181394 16.01074444]\n",
      "downsampled_level_dim [3859 1303]\n",
      "level_dim [3859 1303]\n",
      "name C3L-02127-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02127-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02127-23.h5 took 0.42826390266418457 s\n",
      "features size:  (30, 1024)\n",
      "coordinates size:  (30, 2)\n",
      "\n",
      "progress: 114/206\n",
      "C3L-02127-24\n",
      "downsample [16.00301265 16.00874636]\n",
      "downsampled_level_dim [4979 1715]\n",
      "level_dim [4979 1715]\n",
      "name C3L-02127-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02127-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02127-24.h5 took 0.5889055728912354 s\n",
      "features size:  (47, 1024)\n",
      "coordinates size:  (47, 2)\n",
      "\n",
      "progress: 115/206\n",
      "C3L-02130-21\n",
      "downsample [16.00261963 16.00583885]\n",
      "downsampled_level_dim [5726 2569]\n",
      "level_dim [5726 2569]\n",
      "name C3L-02130-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02130-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02130-21.h5 took 0.9192636013031006 s\n",
      "features size:  (89, 1024)\n",
      "coordinates size:  (89, 2)\n",
      "\n",
      "progress: 116/206\n",
      "C3L-02130-22\n",
      "downsample [16.00273873 16.00613766]\n",
      "downsampled_level_dim [5477 2281]\n",
      "level_dim [5477 2281]\n",
      "name C3L-02130-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02130-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02130-22.h5 took 0.7468752861022949 s\n",
      "features size:  (66, 1024)\n",
      "coordinates size:  (66, 2)\n",
      "\n",
      "progress: 117/206\n",
      "C3L-02130-23\n",
      "downsample [16.00119638 16.00409836]\n",
      "downsampled_level_dim [5851 2196]\n",
      "level_dim [5851 2196]\n",
      "name C3L-02130-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02130-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02130-23.h5 took 0.8334052562713623 s\n",
      "features size:  (83, 1024)\n",
      "coordinates size:  (83, 2)\n",
      "\n",
      "progress: 118/206\n",
      "C3L-02130-24\n",
      "downsample [16.00130768 16.        ]\n",
      "downsampled_level_dim [5353 2334]\n",
      "level_dim [5353 2334]\n",
      "name C3L-02130-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02130-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02130-24.h5 took 0.8474154472351074 s\n",
      "features size:  (86, 1024)\n",
      "coordinates size:  (86, 2)\n",
      "\n",
      "progress: 119/206\n",
      "C3L-02164-21\n",
      "downsample [16.00330813 16.00424854]\n",
      "downsampled_level_dim [2116 1883]\n",
      "level_dim [2116 1883]\n",
      "name C3L-02164-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02164-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02164-21.h5 took 0.42244935035705566 s\n",
      "features size:  (29, 1024)\n",
      "coordinates size:  (29, 2)\n",
      "\n",
      "progress: 120/206\n",
      "C3L-02164-22\n",
      "downsample [16.0075339  16.01081471]\n",
      "downsampled_level_dim [1991 1387]\n",
      "level_dim [1991 1387]\n",
      "name C3L-02164-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02164-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02164-22.h5 took 0.40438199043273926 s\n",
      "features size:  (26, 1024)\n",
      "coordinates size:  (26, 2)\n",
      "\n",
      "progress: 121/206\n",
      "C3L-02164-23\n",
      "downsample [16.00295983 16.00851393]\n",
      "downsampled_level_dim [2365 1292]\n",
      "level_dim [2365 1292]\n",
      "name C3L-02164-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02164-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02164-23.h5 took 0.43347883224487305 s\n",
      "features size:  (32, 1024)\n",
      "coordinates size:  (32, 2)\n",
      "\n",
      "progress: 122/206\n",
      "C3L-02165-21\n",
      "downsample [16.00215177 16.00697211]\n",
      "downsampled_level_dim [6971 2008]\n",
      "level_dim [6971 2008]\n",
      "name C3L-02165-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02165-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02165-21.h5 took 1.1048107147216797 s\n",
      "features size:  (116, 1024)\n",
      "coordinates size:  (116, 2)\n",
      "\n",
      "progress: 123/206\n",
      "C3L-02165-22\n",
      "downsample [16.00231732 16.00125549]\n",
      "downsampled_level_dim [6473 1593]\n",
      "level_dim [6473 1593]\n",
      "name C3L-02165-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02165-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02165-22.h5 took 1.133815050125122 s\n",
      "features size:  (86, 1024)\n",
      "coordinates size:  (86, 2)\n",
      "\n",
      "progress: 124/206\n",
      "C3L-02165-23\n",
      "downsample [16.00224936 16.0020284 ]\n",
      "downsampled_level_dim [3112 1479]\n",
      "level_dim [3112 1479]\n",
      "name C3L-02165-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02165-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02165-23.h5 took 0.5211613178253174 s\n",
      "features size:  (43, 1024)\n",
      "coordinates size:  (43, 2)\n",
      "\n",
      "progress: 125/206\n",
      "C3L-02168-21\n",
      "downsample [16.00144181 16.00334672]\n",
      "downsampled_level_dim [4855 1494]\n",
      "level_dim [4855 1494]\n",
      "name C3L-02168-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02168-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02168-21.h5 took 0.6254942417144775 s\n",
      "features size:  (52, 1024)\n",
      "coordinates size:  (52, 2)\n",
      "\n",
      "progress: 126/206\n",
      "C3L-02168-22\n",
      "downsample [16.00286917 16.00545384]\n",
      "downsampled_level_dim [5228 2567]\n",
      "level_dim [5228 2567]\n",
      "name C3L-02168-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02168-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02168-22.h5 took 1.0032835006713867 s\n",
      "features size:  (96, 1024)\n",
      "coordinates size:  (96, 2)\n",
      "\n",
      "progress: 127/206\n",
      "C3L-02168-23\n",
      "downsample [16.00273873 16.00376952]\n",
      "downsampled_level_dim [5477 1857]\n",
      "level_dim [5477 1857]\n",
      "name C3L-02168-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02168-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02168-23.h5 took 0.978339672088623 s\n",
      "features size:  (103, 1024)\n",
      "coordinates size:  (103, 2)\n",
      "\n",
      "progress: 128/206\n",
      "C3L-02170-21\n",
      "downsample [16.00144181 16.00825959]\n",
      "downsampled_level_dim [4855 1695]\n",
      "level_dim [4855 1695]\n",
      "name C3L-02170-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02170-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02170-21.h5 took 0.679572343826294 s\n",
      "features size:  (61, 1024)\n",
      "coordinates size:  (61, 2)\n",
      "\n",
      "progress: 129/206\n",
      "C3L-02170-22\n",
      "downsample [16.00317125 16.00482057]\n",
      "downsampled_level_dim [4730 1867]\n",
      "level_dim [4730 1867]\n",
      "name C3L-02170-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02170-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02170-22.h5 took 0.774327278137207 s\n",
      "features size:  (76, 1024)\n",
      "coordinates size:  (76, 2)\n",
      "\n",
      "progress: 130/206\n",
      "C3L-02170-23\n",
      "downsample [16.00130768 16.        ]\n",
      "downsampled_level_dim [5353 2308]\n",
      "level_dim [5353 2308]\n",
      "name C3L-02170-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02170-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02170-23.h5 took 0.9350957870483398 s\n",
      "features size:  (98, 1024)\n",
      "coordinates size:  (98, 2)\n",
      "\n",
      "progress: 131/206\n",
      "C3L-02219-21\n",
      "downsample [8.00200861 8.00173491]\n",
      "downsampled_level_dim [3485 2882]\n",
      "level_dim [3485 2882]\n",
      "name C3L-02219-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02219-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02219-21.h5 took 0.5229077339172363 s\n",
      "features size:  (44, 1024)\n",
      "coordinates size:  (44, 2)\n",
      "\n",
      "progress: 132/206\n",
      "C3L-02219-22\n",
      "downsample [8.003125 8.      ]\n",
      "downsampled_level_dim [2240 1949]\n",
      "level_dim [2240 1949]\n",
      "name C3L-02219-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02219-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02219-22.h5 took 0.35743165016174316 s\n",
      "features size:  (20, 1024)\n",
      "coordinates size:  (20, 2)\n",
      "\n",
      "progress: 133/206\n",
      "C3L-02345-21\n",
      "downsample [16.01004689 16.00795229]\n",
      "downsampled_level_dim [1493 1509]\n",
      "level_dim [1493 1509]\n",
      "name C3L-02345-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02345-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02345-21.h5 took 0.32340383529663086 s\n",
      "features size:  (14, 1024)\n",
      "coordinates size:  (14, 2)\n",
      "\n",
      "progress: 134/206\n",
      "C3L-02345-22\n",
      "downsample [8.00234349 8.00079618]\n",
      "downsampled_level_dim [2987 2512]\n",
      "level_dim [2987 2512]\n",
      "name C3L-02345-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02345-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02345-22.h5 took 0.5931880474090576 s\n",
      "features size:  (55, 1024)\n",
      "coordinates size:  (55, 2)\n",
      "\n",
      "progress: 135/206\n",
      "C3L-02345-23\n",
      "downsample [16.00511322 16.        ]\n",
      "downsampled_level_dim [1369 1697]\n",
      "level_dim [1369 1697]\n",
      "name C3L-02345-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02345-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02345-23.h5 took 0.3210892677307129 s\n",
      "features size:  (14, 1024)\n",
      "coordinates size:  (14, 2)\n",
      "\n",
      "progress: 136/206\n",
      "C3L-02348-21\n",
      "downsample [16.0075339  16.00690131]\n",
      "downsampled_level_dim [1991 1449]\n",
      "level_dim [1991 1449]\n",
      "name C3L-02348-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02348-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02348-21.h5 took 0.34625887870788574 s\n",
      "features size:  (18, 1024)\n",
      "coordinates size:  (18, 2)\n",
      "\n",
      "progress: 137/206\n",
      "C3L-02348-22\n",
      "downsample [16.00330813 16.        ]\n",
      "downsampled_level_dim [2116 1728]\n",
      "level_dim [2116 1728]\n",
      "name C3L-02348-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02348-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02348-22.h5 took 0.35877346992492676 s\n",
      "features size:  (20, 1024)\n",
      "coordinates size:  (20, 2)\n",
      "\n",
      "progress: 138/206\n",
      "C3L-02349-21\n",
      "downsample [8.00216316 8.00107488]\n",
      "downsampled_level_dim [3236 2791]\n",
      "level_dim [3236 2791]\n",
      "name C3L-02349-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02349-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02349-21.h5 took 0.5740976333618164 s\n",
      "features size:  (54, 1024)\n",
      "coordinates size:  (54, 2)\n",
      "\n",
      "progress: 139/206\n",
      "C3L-02358-21\n",
      "downsample [16.00374933 16.00356506]\n",
      "downsampled_level_dim [1867 1683]\n",
      "level_dim [1867 1683]\n",
      "name C3L-02358-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02358-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02358-21.h5 took 0.3322775363922119 s\n",
      "features size:  (17, 1024)\n",
      "coordinates size:  (17, 2)\n",
      "\n",
      "progress: 140/206\n",
      "C3L-02358-22\n",
      "downsample [8.00281237 8.0011592 ]\n",
      "downsampled_level_dim [2489 2588]\n",
      "level_dim [2489 2588]\n",
      "name C3L-02358-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02358-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02358-22.h5 took 0.494426965713501 s\n",
      "features size:  (37, 1024)\n",
      "coordinates size:  (37, 2)\n",
      "\n",
      "progress: 141/206\n",
      "C3L-02358-23\n",
      "downsample [16.01004689 16.        ]\n",
      "downsampled_level_dim [1493 1556]\n",
      "level_dim [1493 1556]\n",
      "name C3L-02358-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02358-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02358-23.h5 took 0.32719969749450684 s\n",
      "features size:  (16, 1024)\n",
      "coordinates size:  (16, 2)\n",
      "\n",
      "progress: 142/206\n",
      "C3L-02365-21\n",
      "downsample [8.00216316 8.00036792]\n",
      "downsampled_level_dim [3236 2718]\n",
      "level_dim [3236 2718]\n",
      "name C3L-02365-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02365-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02365-21.h5 took 0.6135783195495605 s\n",
      "features size:  (60, 1024)\n",
      "coordinates size:  (60, 2)\n",
      "\n",
      "progress: 143/206\n",
      "C3L-02365-24\n",
      "downsample [8.00216316 8.00147348]\n",
      "downsampled_level_dim [3236 2036]\n",
      "level_dim [3236 2036]\n",
      "name C3L-02365-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02365-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02365-24.h5 took 0.5019280910491943 s\n",
      "features size:  (41, 1024)\n",
      "coordinates size:  (41, 2)\n",
      "\n",
      "progress: 144/206\n",
      "C3L-02365-25\n",
      "downsample [8.00234349 8.00216216]\n",
      "downsampled_level_dim [2987 1850]\n",
      "level_dim [2987 1850]\n",
      "name C3L-02365-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02365-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02365-25.h5 took 0.4930753707885742 s\n",
      "features size:  (38, 1024)\n",
      "coordinates size:  (38, 2)\n",
      "\n",
      "progress: 145/206\n",
      "C3L-02508-22\n",
      "downsample [8.00234349 8.00078616]\n",
      "downsampled_level_dim [2987 2544]\n",
      "level_dim [2987 2544]\n",
      "name C3L-02508-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02508-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02508-22.h5 took 0.48308873176574707 s\n",
      "features size:  (39, 1024)\n",
      "coordinates size:  (39, 2)\n",
      "\n",
      "progress: 146/206\n",
      "C3L-02508-23\n",
      "downsample [8.00255661 8.00169563]\n",
      "downsampled_level_dim [2738 2359]\n",
      "level_dim [2738 2359]\n",
      "name C3L-02508-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02508-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02508-23.h5 took 0.3756546974182129 s\n",
      "features size:  (23, 1024)\n",
      "coordinates size:  (23, 2)\n",
      "\n",
      "progress: 147/206\n",
      "C3L-02513-21\n",
      "downsample [4.00053591 4.        ]\n",
      "downsampled_level_dim [5598 4335]\n",
      "level_dim [5598 4335]\n",
      "name C3L-02513-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02513-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02513-21.h5 took 1.9194142818450928 s\n",
      "features size:  (242, 1024)\n",
      "coordinates size:  (242, 2)\n",
      "\n",
      "progress: 148/206\n",
      "C3L-02513-22\n",
      "downsample [4.00018975 4.        ]\n",
      "downsampled_level_dim [5270 3855]\n",
      "level_dim [5270 3855]\n",
      "name C3L-02513-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02513-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02513-22.h5 took 1.7218201160430908 s\n",
      "features size:  (212, 1024)\n",
      "coordinates size:  (212, 2)\n",
      "\n",
      "progress: 149/206\n",
      "C3L-02513-23\n",
      "downsample [4.         4.00034638]\n",
      "downsampled_level_dim [4285 5774]\n",
      "level_dim [4285 5774]\n",
      "name C3L-02513-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02513-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02513-23.h5 took 2.1401469707489014 s\n",
      "features size:  (259, 1024)\n",
      "coordinates size:  (259, 2)\n",
      "\n",
      "progress: 150/206\n",
      "C3L-02515-21\n",
      "downsample [4.00031974 4.00013416]\n",
      "downsampled_level_dim [6255 7454]\n",
      "level_dim [6255 7454]\n",
      "name C3L-02515-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02515-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02515-21.h5 took 2.0690362453460693 s\n",
      "features size:  (238, 1024)\n",
      "coordinates size:  (238, 2)\n",
      "\n",
      "progress: 151/206\n",
      "C3L-02515-22\n",
      "downsample [4.00041436 4.00033256]\n",
      "downsampled_level_dim [7240 6014]\n",
      "level_dim [7240 6014]\n",
      "name C3L-02515-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02515-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02515-22.h5 took 2.740746021270752 s\n",
      "features size:  (346, 1024)\n",
      "coordinates size:  (346, 2)\n",
      "\n",
      "progress: 152/206\n",
      "C3L-02515-23\n",
      "downsample [4.00033776 4.00059359]\n",
      "downsampled_level_dim [8882 5054]\n",
      "level_dim [8882 5054]\n",
      "name C3L-02515-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02515-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02515-23.h5 took 3.1587753295898438 s\n",
      "features size:  (393, 1024)\n",
      "coordinates size:  (393, 2)\n",
      "\n",
      "progress: 153/206\n",
      "C3L-02546-21\n",
      "downsample [8.00255661 8.00044944]\n",
      "downsampled_level_dim [2738 2225]\n",
      "level_dim [2738 2225]\n",
      "name C3L-02546-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02546-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02546-21.h5 took 0.42901134490966797 s\n",
      "features size:  (30, 1024)\n",
      "coordinates size:  (30, 2)\n",
      "\n",
      "progress: 154/206\n",
      "C3L-02546-23\n",
      "downsample [8.00281237 8.00202102]\n",
      "downsampled_level_dim [2489 2474]\n",
      "level_dim [2489 2474]\n",
      "name C3L-02546-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02546-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02546-23.h5 took 0.40723681449890137 s\n",
      "features size:  (27, 1024)\n",
      "coordinates size:  (27, 2)\n",
      "\n",
      "progress: 155/206\n",
      "C3L-02549-21\n",
      "downsample [8.00200861 8.00078064]\n",
      "downsampled_level_dim [3485 2562]\n",
      "level_dim [3485 2562]\n",
      "name C3L-02549-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02549-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02549-21.h5 took 0.5234365463256836 s\n",
      "features size:  (45, 1024)\n",
      "coordinates size:  (45, 2)\n",
      "\n",
      "progress: 156/206\n",
      "C3L-02549-22\n",
      "downsample [8.00187467 8.00070947]\n",
      "downsampled_level_dim [3734 2819]\n",
      "level_dim [3734 2819]\n",
      "name C3L-02549-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02549-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02549-22.h5 took 0.5064182281494141 s\n",
      "features size:  (42, 1024)\n",
      "coordinates size:  (42, 2)\n",
      "\n",
      "progress: 157/206\n",
      "C3L-02552-22\n",
      "downsample [8.003125   8.00268302]\n",
      "downsampled_level_dim [2240 2609]\n",
      "level_dim [2240 2609]\n",
      "name C3L-02552-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02552-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02552-22.h5 took 0.4615137577056885 s\n",
      "features size:  (35, 1024)\n",
      "coordinates size:  (35, 2)\n",
      "\n",
      "progress: 158/206\n",
      "C3L-02552-23\n",
      "downsample [8.00281237 8.00301984]\n",
      "downsampled_level_dim [2489 2318]\n",
      "level_dim [2489 2318]\n",
      "name C3L-02552-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02552-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02552-23.h5 took 0.46951818466186523 s\n",
      "features size:  (35, 1024)\n",
      "coordinates size:  (35, 2)\n",
      "\n",
      "progress: 159/206\n",
      "C3L-02560-21\n",
      "downsample [8.00255661 8.00214777]\n",
      "downsampled_level_dim [2738 2328]\n",
      "level_dim [2738 2328]\n",
      "name C3L-02560-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02560-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02560-21.h5 took 0.4426143169403076 s\n",
      "features size:  (32, 1024)\n",
      "coordinates size:  (32, 2)\n",
      "\n",
      "progress: 160/206\n",
      "C3L-02560-22\n",
      "downsample [8.00255661 8.00082237]\n",
      "downsampled_level_dim [2738 2432]\n",
      "level_dim [2738 2432]\n",
      "name C3L-02560-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02560-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02560-22.h5 took 0.45064806938171387 s\n",
      "features size:  (32, 1024)\n",
      "coordinates size:  (32, 2)\n",
      "\n",
      "progress: 161/206\n",
      "C3L-02601-21\n",
      "downsample [8.00281237 8.00283057]\n",
      "downsampled_level_dim [2489 2473]\n",
      "level_dim [2489 2473]\n",
      "name C3L-02601-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02601-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02601-21.h5 took 0.45728063583374023 s\n",
      "features size:  (34, 1024)\n",
      "coordinates size:  (34, 2)\n",
      "\n",
      "progress: 162/206\n",
      "C3L-02616-21\n",
      "downsample [16.00463535 16.00693481]\n",
      "downsampled_level_dim [3236 1442]\n",
      "level_dim [3236 1442]\n",
      "name C3L-02616-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02616-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02616-21.h5 took 0.36746692657470703 s\n",
      "features size:  (21, 1024)\n",
      "coordinates size:  (21, 2)\n",
      "\n",
      "progress: 163/206\n",
      "C3L-02616-22\n",
      "downsample [16.00430416 16.00583942]\n",
      "downsampled_level_dim [3485 2055]\n",
      "level_dim [3485 2055]\n",
      "name C3L-02616-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02616-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02616-22.h5 took 0.4920015335083008 s\n",
      "features size:  (39, 1024)\n",
      "coordinates size:  (39, 2)\n",
      "\n",
      "progress: 164/206\n",
      "C3L-02616-23\n",
      "downsample [16.00401714 16.00429415]\n",
      "downsampled_level_dim [3734 1863]\n",
      "level_dim [3734 1863]\n",
      "name C3L-02616-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02616-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02616-23.h5 took 0.45470523834228516 s\n",
      "features size:  (34, 1024)\n",
      "coordinates size:  (34, 2)\n",
      "\n",
      "progress: 165/206\n",
      "C3L-02616-24\n",
      "downsample [16.00193906 16.00211082]\n",
      "downsampled_level_dim [3610 1895]\n",
      "level_dim [3610 1895]\n",
      "name C3L-02616-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02616-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02616-24.h5 took 0.3997330665588379 s\n",
      "features size:  (26, 1024)\n",
      "coordinates size:  (26, 2)\n",
      "\n",
      "progress: 166/206\n",
      "C3L-02616-25\n",
      "downsample [16.00181394 16.00618876]\n",
      "downsampled_level_dim [3859 1939]\n",
      "level_dim [3859 1939]\n",
      "name C3L-02616-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02616-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02616-25.h5 took 0.5445218086242676 s\n",
      "features size:  (46, 1024)\n",
      "coordinates size:  (46, 2)\n",
      "\n",
      "progress: 167/206\n",
      "C3L-02624-21\n",
      "downsample [16.00401714 16.00199867]\n",
      "downsampled_level_dim [3734 1501]\n",
      "level_dim [3734 1501]\n",
      "name C3L-02624-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02624-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02624-21.h5 took 0.46682286262512207 s\n",
      "features size:  (34, 1024)\n",
      "coordinates size:  (34, 2)\n",
      "\n",
      "progress: 168/206\n",
      "C3L-02624-22\n",
      "downsample [16.00354442 16.00139276]\n",
      "downsampled_level_dim [4232 1436]\n",
      "level_dim [4232 1436]\n",
      "name C3L-02624-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02624-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02624-22.h5 took 0.5717496871948242 s\n",
      "features size:  (42, 1024)\n",
      "coordinates size:  (42, 2)\n",
      "\n",
      "progress: 169/206\n",
      "C3L-02624-23\n",
      "downsample [16.00401714 16.00287081]\n",
      "downsampled_level_dim [3734 2090]\n",
      "level_dim [3734 2090]\n",
      "name C3L-02624-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02624-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02624-23.h5 took 0.5449457168579102 s\n",
      "features size:  (45, 1024)\n",
      "coordinates size:  (45, 2)\n",
      "\n",
      "progress: 170/206\n",
      "C3L-02624-24\n",
      "downsample [16.00354442 16.00395034]\n",
      "downsampled_level_dim [4232 1772]\n",
      "level_dim [4232 1772]\n",
      "name C3L-02624-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02624-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02624-24.h5 took 0.7047221660614014 s\n",
      "features size:  (58, 1024)\n",
      "coordinates size:  (58, 2)\n",
      "\n",
      "progress: 171/206\n",
      "C3L-02624-25\n",
      "downsample [16.00401714 16.00165654]\n",
      "downsampled_level_dim [3734 1811]\n",
      "level_dim [3734 1811]\n",
      "name C3L-02624-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02624-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02624-25.h5 took 0.5598065853118896 s\n",
      "features size:  (50, 1024)\n",
      "coordinates size:  (50, 2)\n",
      "\n",
      "progress: 172/206\n",
      "C3L-02625-21\n",
      "downsample [16.00430416 16.003861  ]\n",
      "downsampled_level_dim [3485 1554]\n",
      "level_dim [3485 1554]\n",
      "name C3L-02625-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02625-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02625-21.h5 took 0.45003390312194824 s\n",
      "features size:  (32, 1024)\n",
      "coordinates size:  (32, 2)\n",
      "\n",
      "progress: 173/206\n",
      "C3L-02625-22\n",
      "downsample [16.00463535 16.00669507]\n",
      "downsampled_level_dim [3236 1643]\n",
      "level_dim [3236 1643]\n",
      "name C3L-02625-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02625-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02625-22.h5 took 0.39313340187072754 s\n",
      "features size:  (25, 1024)\n",
      "coordinates size:  (25, 2)\n",
      "\n",
      "progress: 174/206\n",
      "C3L-02625-23\n",
      "downsample [8.00234349 8.00300946]\n",
      "downsampled_level_dim [2987 2326]\n",
      "level_dim [2987 2326]\n",
      "name C3L-02625-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02625-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02625-23.h5 took 0.5834648609161377 s\n",
      "features size:  (52, 1024)\n",
      "coordinates size:  (52, 2)\n",
      "\n",
      "progress: 175/206\n",
      "C3L-02625-24\n",
      "downsample [16.00432633 16.00604839]\n",
      "downsampled_level_dim [1618 1488]\n",
      "level_dim [1618 1488]\n",
      "name C3L-02625-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02625-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02625-24.h5 took 0.3587825298309326 s\n",
      "features size:  (19, 1024)\n",
      "coordinates size:  (19, 2)\n",
      "\n",
      "progress: 176/206\n",
      "C3L-02625-25\n",
      "downsample [16.00401714 16.        ]\n",
      "downsampled_level_dim [3734 1713]\n",
      "level_dim [3734 1713]\n",
      "name C3L-02625-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02625-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02625-25.h5 took 0.4764413833618164 s\n",
      "features size:  (36, 1024)\n",
      "coordinates size:  (36, 2)\n",
      "\n",
      "progress: 177/206\n",
      "C3L-02627-21\n",
      "downsample [16.00193906 16.0089153 ]\n",
      "downsampled_level_dim [3610 1346]\n",
      "level_dim [3610 1346]\n",
      "name C3L-02627-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02627-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02627-21.h5 took 0.3441803455352783 s\n",
      "features size:  (17, 1024)\n",
      "coordinates size:  (17, 2)\n",
      "\n",
      "progress: 178/206\n",
      "C3L-02627-22\n",
      "downsample [8.00234349 8.00038066]\n",
      "downsampled_level_dim [2987 2627]\n",
      "level_dim [2987 2627]\n",
      "name C3L-02627-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02627-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02627-22.h5 took 0.5030868053436279 s\n",
      "features size:  (39, 1024)\n",
      "coordinates size:  (39, 2)\n",
      "\n",
      "progress: 179/206\n",
      "C3L-02627-23\n",
      "downsample [16.00430416 16.00928218]\n",
      "downsampled_level_dim [3485 1616]\n",
      "level_dim [3485 1616]\n",
      "name C3L-02627-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02627-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02627-23.h5 took 0.36171483993530273 s\n",
      "features size:  (20, 1024)\n",
      "coordinates size:  (20, 2)\n",
      "\n",
      "progress: 180/206\n",
      "C3L-02627-24\n",
      "downsample [8.00234349 8.        ]\n",
      "downsampled_level_dim [2987 2896]\n",
      "level_dim [2987 2896]\n",
      "name C3L-02627-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02627-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02627-24.h5 took 0.52976393699646 s\n",
      "features size:  (45, 1024)\n",
      "coordinates size:  (45, 2)\n",
      "\n",
      "progress: 181/206\n",
      "C3L-02627-25\n",
      "downsample [16.00354442 16.        ]\n",
      "downsampled_level_dim [4232 1700]\n",
      "level_dim [4232 1700]\n",
      "name C3L-02627-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02627-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02627-25.h5 took 0.47921133041381836 s\n",
      "features size:  (31, 1024)\n",
      "coordinates size:  (31, 2)\n",
      "\n",
      "progress: 182/206\n",
      "C3L-02629-21\n",
      "downsample [16.00151976 16.00969619]\n",
      "downsampled_level_dim [4606 1547]\n",
      "level_dim [4606 1547]\n",
      "name C3L-02629-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02629-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02629-21.h5 took 0.51908278465271 s\n",
      "features size:  (36, 1024)\n",
      "coordinates size:  (36, 2)\n",
      "\n",
      "progress: 183/206\n",
      "C3L-02629-22\n",
      "downsample [16.00334747 16.00409836]\n",
      "downsampled_level_dim [4481 1708]\n",
      "level_dim [4481 1708]\n",
      "name C3L-02629-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02629-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02629-22.h5 took 0.53914475440979 s\n",
      "features size:  (34, 1024)\n",
      "coordinates size:  (34, 2)\n",
      "\n",
      "progress: 184/206\n",
      "C3L-02629-23\n",
      "downsample [16.00317125 16.00651702]\n",
      "downsampled_level_dim [4730 1381]\n",
      "level_dim [4730 1381]\n",
      "name C3L-02629-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02629-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02629-23.h5 took 0.5308058261871338 s\n",
      "features size:  (38, 1024)\n",
      "coordinates size:  (38, 2)\n",
      "\n",
      "progress: 185/206\n",
      "C3L-02629-24\n",
      "downsample [16.00151976 16.00461627]\n",
      "downsampled_level_dim [4606 1733]\n",
      "level_dim [4606 1733]\n",
      "name C3L-02629-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02629-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02629-24.h5 took 0.5651462078094482 s\n",
      "features size:  (38, 1024)\n",
      "coordinates size:  (38, 2)\n",
      "\n",
      "progress: 186/206\n",
      "C3L-02629-25\n",
      "downsample [16.00137147 16.0078172 ]\n",
      "downsampled_level_dim [5104 1663]\n",
      "level_dim [5104 1663]\n",
      "name C3L-02629-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02629-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02629-25.h5 took 0.5480403900146484 s\n",
      "features size:  (39, 1024)\n",
      "coordinates size:  (39, 2)\n",
      "\n",
      "progress: 187/206\n",
      "C3L-02648-21\n",
      "downsample [16.00144181 16.00773994]\n",
      "downsampled_level_dim [4855 1938]\n",
      "level_dim [4855 1938]\n",
      "name C3L-02648-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02648-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02648-21.h5 took 0.6071641445159912 s\n",
      "features size:  (50, 1024)\n",
      "coordinates size:  (50, 2)\n",
      "\n",
      "progress: 188/206\n",
      "C3L-02648-22\n",
      "downsample [16.00181394 16.00059666]\n",
      "downsampled_level_dim [3859 1676]\n",
      "level_dim [3859 1676]\n",
      "name C3L-02648-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02648-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02648-22.h5 took 0.5794799327850342 s\n",
      "features size:  (51, 1024)\n",
      "coordinates size:  (51, 2)\n",
      "\n",
      "progress: 189/206\n",
      "C3L-02648-23\n",
      "downsample [16.00334747 16.00554995]\n",
      "downsampled_level_dim [4481 1982]\n",
      "level_dim [4481 1982]\n",
      "name C3L-02648-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02648-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02648-23.h5 took 0.5727694034576416 s\n",
      "features size:  (44, 1024)\n",
      "coordinates size:  (44, 2)\n",
      "\n",
      "progress: 190/206\n",
      "C3L-02648-24\n",
      "downsample [16.00144181 16.        ]\n",
      "downsampled_level_dim [4855 2415]\n",
      "level_dim [4855 2415]\n",
      "name C3L-02648-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02648-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02648-24.h5 took 0.715785026550293 s\n",
      "features size:  (66, 1024)\n",
      "coordinates size:  (66, 2)\n",
      "\n",
      "progress: 191/206\n",
      "C3L-02648-25\n",
      "downsample [16.00430416 16.00503145]\n",
      "downsampled_level_dim [3485 1590]\n",
      "level_dim [3485 1590]\n",
      "name C3L-02648-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02648-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02648-25.h5 took 0.41635990142822266 s\n",
      "features size:  (28, 1024)\n",
      "coordinates size:  (28, 2)\n",
      "\n",
      "progress: 192/206\n",
      "C3L-02649-21\n",
      "downsample [16.00151976 16.0005767 ]\n",
      "downsampled_level_dim [4606 1734]\n",
      "level_dim [4606 1734]\n",
      "name C3L-02649-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02649-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02649-21.h5 took 0.412203311920166 s\n",
      "features size:  (23, 1024)\n",
      "coordinates size:  (23, 2)\n",
      "\n",
      "progress: 193/206\n",
      "C3L-02649-22\n",
      "downsample [16.00151976 16.00135593]\n",
      "downsampled_level_dim [4606 1475]\n",
      "level_dim [4606 1475]\n",
      "name C3L-02649-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02649-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02649-22.h5 took 0.5419468879699707 s\n",
      "features size:  (38, 1024)\n",
      "coordinates size:  (38, 2)\n",
      "\n",
      "progress: 194/206\n",
      "C3L-02649-23\n",
      "downsample [16.00160661 16.00303582]\n",
      "downsampled_level_dim [4357 1647]\n",
      "level_dim [4357 1647]\n",
      "name C3L-02649-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02649-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02649-23.h5 took 0.5671663284301758 s\n",
      "features size:  (36, 1024)\n",
      "coordinates size:  (36, 2)\n",
      "\n",
      "progress: 195/206\n",
      "C3L-02649-24\n",
      "downsample [16.00334747 16.00664251]\n",
      "downsampled_level_dim [4481 1656]\n",
      "level_dim [4481 1656]\n",
      "name C3L-02649-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02649-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02649-24.h5 took 0.5500888824462891 s\n",
      "features size:  (43, 1024)\n",
      "coordinates size:  (43, 2)\n",
      "\n",
      "progress: 196/206\n",
      "C3L-02649-25\n",
      "downsample [16.00334747 16.00228571]\n",
      "downsampled_level_dim [4481 1750]\n",
      "level_dim [4481 1750]\n",
      "name C3L-02649-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02649-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02649-25.h5 took 0.5330057144165039 s\n",
      "features size:  (39, 1024)\n",
      "coordinates size:  (39, 2)\n",
      "\n",
      "progress: 197/206\n",
      "C3L-02661-21\n",
      "downsample [16.00401714 16.00515464]\n",
      "downsampled_level_dim [3734 1940]\n",
      "level_dim [3734 1940]\n",
      "name C3L-02661-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02661-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02661-21.h5 took 0.6033086776733398 s\n",
      "features size:  (55, 1024)\n",
      "coordinates size:  (55, 2)\n",
      "\n",
      "progress: 198/206\n",
      "C3L-02661-22\n",
      "downsample [16.00181394 16.00242248]\n",
      "downsampled_level_dim [3859 2064]\n",
      "level_dim [3859 2064]\n",
      "name C3L-02661-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02661-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02661-22.h5 took 0.5069336891174316 s\n",
      "features size:  (40, 1024)\n",
      "coordinates size:  (40, 2)\n",
      "\n",
      "progress: 199/206\n",
      "C3L-02661-23\n",
      "downsample [16.00224936 16.0042343 ]\n",
      "downsampled_level_dim [3112 1417]\n",
      "level_dim [3112 1417]\n",
      "name C3L-02661-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02661-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02661-23.h5 took 0.4003634452819824 s\n",
      "features size:  (25, 1024)\n",
      "coordinates size:  (25, 2)\n",
      "\n",
      "progress: 200/206\n",
      "C3L-02661-24\n",
      "downsample [16.00170399 16.00846945]\n",
      "downsampled_level_dim [4108 1653]\n",
      "level_dim [4108 1653]\n",
      "name C3L-02661-24\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02661-24.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02661-24.h5 took 0.6142513751983643 s\n",
      "features size:  (48, 1024)\n",
      "coordinates size:  (48, 2)\n",
      "\n",
      "progress: 201/206\n",
      "C3L-02661-25\n",
      "downsample [16.00170399 16.00341964]\n",
      "downsampled_level_dim [4108 2047]\n",
      "level_dim [4108 2047]\n",
      "name C3L-02661-25\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02661-25.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02661-25.h5 took 0.6101255416870117 s\n",
      "features size:  (48, 1024)\n",
      "coordinates size:  (48, 2)\n",
      "\n",
      "progress: 202/206\n",
      "C3L-02834-21\n",
      "downsample [16.00861079 16.00839329]\n",
      "downsampled_level_dim [1742 1668]\n",
      "level_dim [1742 1668]\n",
      "name C3L-02834-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02834-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02834-21.h5 took 0.3557136058807373 s\n",
      "features size:  (15, 1024)\n",
      "coordinates size:  (15, 2)\n",
      "\n",
      "progress: 203/206\n",
      "C3L-02834-22\n",
      "downsample [16.00432633 16.00540541]\n",
      "downsampled_level_dim [1618 1850]\n",
      "level_dim [1618 1850]\n",
      "name C3L-02834-22\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02834-22.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02834-22.h5 took 0.33124494552612305 s\n",
      "features size:  (15, 1024)\n",
      "coordinates size:  (15, 2)\n",
      "\n",
      "progress: 204/206\n",
      "C3L-02834-23\n",
      "downsample [8.00255661 8.        ]\n",
      "downsampled_level_dim [2738 2649]\n",
      "level_dim [2738 2649]\n",
      "name C3L-02834-23\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02834-23.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02834-23.h5 took 0.45558977127075195 s\n",
      "features size:  (33, 1024)\n",
      "coordinates size:  (33, 2)\n",
      "\n",
      "progress: 205/206\n",
      "C3L-02891-21\n",
      "downsample [8.003125  8.0029254]\n",
      "downsampled_level_dim [2240 2051]\n",
      "level_dim [2240 2051]\n",
      "name C3L-02891-21\n",
      "patch_level 2\n",
      "patch_size 256\n",
      "save_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches\n",
      "\n",
      "feature extraction settings\n",
      "target patch size:  None\n",
      "pretrained:  True\n",
      "transformations:  Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "processing /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/patches/C3L-02891-21.h5: total of 1 batches\n",
      "batch 0/1, 0 files processed\n",
      "\n",
      "computing features for /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2/h5_files/C3L-02891-21.h5 took 0.44001245498657227 s\n",
      "features size:  (31, 1024)\n",
      "coordinates size:  (31, 2)\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python extract_features_fp.py --data_h5_dir /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2 \\\n",
    "--data_slide_dir /home/sci/Disk2/CPTAC-LUNG/WSI \\\n",
    "--csv_path /home/sci/Disk2/CPTAC-LUNG/BLOCKS_level2/step2_get_features.csv \\\n",
    "--feat_dir /home/sci/Disk2/CPTAC-LUNG/FEATURES_level2 --batch_size 512 --slide_ext .svs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step3 Create split\n",
    "注意修改Create_split_seq.py文件中的csv路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.csv_gen import * \n",
    "path = r'/home/sci/Disk_data/TCGA-NSCLC/WSI'\n",
    "# sort_csv = pd.read_csv(csv_dir).sort_values('slide_id')\n",
    "result_dir = r'/home/sci/Disk_data/TCGA-NSCLC/RESULTS_DIRECTORY/step3_get_splits.csv' ## 5 + 20\n",
    "patch_dir = r'/home/sci/Disk_data/TCGA-NSCLC/RESULTS_DIRECTORY/patches'\n",
    "csv_gen_test(path,result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "/home/sci/PycharmProjects/chaofan/projects/CLAM/datasets/dataset_generic.py:104: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  label = stats.mode(label)[0]\n",
      "label column: label\n",
      "label dictionary: {'LUAD': 0, 'LUSC': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 0    507\n",
      "1    520\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 164\n",
      "Slide-LVL; Number of samples registered in class 0: 507\n",
      "Patient-LVL; Number of samples registered in class 1: 157\n",
      "Slide-LVL; Number of samples registered in class 1: 520\n",
      "\n",
      "number of training samples: 820\n",
      "number of samples in cls 0: 406\n",
      "number of samples in cls 1: 414\n",
      "\n",
      "number of val samples: 110\n",
      "number of samples in cls 0: 52\n",
      "number of samples in cls 1: 58\n",
      "\n",
      "number of test samples: 97\n",
      "number of samples in cls 0: 49\n",
      "number of samples in cls 1: 48\n",
      "\n",
      "\n",
      "\n",
      "number of training samples: 822\n",
      "number of samples in cls 0: 410\n",
      "number of samples in cls 1: 412\n",
      "\n",
      "number of val samples: 109\n",
      "number of samples in cls 0: 46\n",
      "number of samples in cls 1: 63\n",
      "\n",
      "number of test samples: 96\n",
      "number of samples in cls 0: 51\n",
      "number of samples in cls 1: 45\n",
      "\n",
      "\n",
      "\n",
      "number of training samples: 822\n",
      "number of samples in cls 0: 418\n",
      "number of samples in cls 1: 404\n",
      "\n",
      "number of val samples: 92\n",
      "number of samples in cls 0: 38\n",
      "number of samples in cls 1: 54\n",
      "\n",
      "number of test samples: 113\n",
      "number of samples in cls 0: 51\n",
      "number of samples in cls 1: 62\n",
      "\n",
      "\n",
      "\n",
      "number of training samples: 830\n",
      "number of samples in cls 0: 412\n",
      "number of samples in cls 1: 418\n",
      "\n",
      "number of val samples: 97\n",
      "number of samples in cls 0: 47\n",
      "number of samples in cls 1: 50\n",
      "\n",
      "number of test samples: 100\n",
      "number of samples in cls 0: 48\n",
      "number of samples in cls 1: 52\n",
      "\n",
      "\n",
      "\n",
      "number of training samples: 838\n",
      "number of samples in cls 0: 407\n",
      "number of samples in cls 1: 431\n",
      "\n",
      "number of val samples: 90\n",
      "number of samples in cls 0: 44\n",
      "number of samples in cls 1: 46\n",
      "\n",
      "number of test samples: 99\n",
      "number of samples in cls 0: 56\n",
      "number of samples in cls 1: 43\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python create_splits_seq.py --task task_2_tumor_subtyping --seed 1 --label_frac 1 --k 5 --csv_path dataset_csv/cptac_lung_subtyping.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 Train\n",
    "注意修改main.py中的csv路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA_VISIBLE_DEVICES=0 python main.py \\\n",
    "--drop_out \\\n",
    "--early_stopping \\\n",
    "--lr 2e-4 \\\n",
    "--k 10 \\\n",
    "--label_frac 0.75 \\\n",
    "--exp_code task_1_tumor_vs_normal_CLAM_50 --weighted_sample --bag_loss ce --inst_loss svm --task task_1_tumor_vs_normal --model_type clam_sb --log_data \\\n",
    "--data_root_dir /media/yuansh/14THHD/CLAM/FEATURES_DIRECTORY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load Dataset\n",
      "label column: label\n",
      "label dictionary: {'LUAD': 0, 'LUSC': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 0    507\n",
      "1    520\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 164\n",
      "Slide-LVL; Number of samples registered in class 0: 507\n",
      "Patient-LVL; Number of samples registered in class 1: 157\n",
      "Slide-LVL; Number of samples registered in class 1: 520\n",
      "split_dir:  /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/cptac_lung_100\n",
      "################# Settings ###################\n",
      "num_splits:  5\n",
      "k_start:  -1\n",
      "k_end:  -1\n",
      "task:  task_2_tumor_subtyping\n",
      "max_epochs:  200\n",
      "results_dir:  ./results\n",
      "lr:  0.0002\n",
      "experiment:  cptac_lung_100_level0_mil_adam\n",
      "reg:  1e-05\n",
      "label_frac:  1.0\n",
      "bag_loss:  ce\n",
      "seed:  1\n",
      "model_type:  mil\n",
      "model_size:  small\n",
      "use_drop_out:  True\n",
      "weighted_sample:  True\n",
      "opt:  adam\n",
      "split_dir:  /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/cptac_lung_100\n",
      "\n",
      "Training Fold 0!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 820 samples\n",
      "Validating on 110 samples\n",
      "Testing on 97 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Done!\n",
      "MIL_fc(\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 525826\n",
      "Total number of trainable parameters: 525826\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6818, label: 1, bag_size: 4128\n",
      "batch 39, loss: 0.6528, label: 0, bag_size: 1560\n",
      "batch 59, loss: 1.0508, label: 1, bag_size: 9533\n",
      "batch 79, loss: 0.5315, label: 0, bag_size: 16782\n",
      "batch 99, loss: 0.7114, label: 0, bag_size: 11390\n",
      "batch 119, loss: 0.6722, label: 1, bag_size: 15185\n",
      "batch 139, loss: 0.5311, label: 0, bag_size: 11922\n",
      "batch 159, loss: 0.6855, label: 1, bag_size: 11964\n",
      "batch 179, loss: 0.9588, label: 1, bag_size: 7989\n",
      "batch 199, loss: 0.9789, label: 1, bag_size: 11642\n",
      "batch 219, loss: 0.4111, label: 0, bag_size: 1052\n",
      "batch 239, loss: 0.5133, label: 0, bag_size: 20555\n",
      "batch 259, loss: 0.4009, label: 0, bag_size: 2244\n",
      "batch 279, loss: 0.5735, label: 0, bag_size: 890\n",
      "batch 299, loss: 0.6566, label: 0, bag_size: 5409\n",
      "batch 319, loss: 0.5537, label: 1, bag_size: 14433\n",
      "batch 339, loss: 0.5028, label: 1, bag_size: 1038\n",
      "batch 359, loss: 0.5791, label: 0, bag_size: 705\n",
      "batch 379, loss: 0.5226, label: 0, bag_size: 19466\n",
      "batch 399, loss: 0.4973, label: 1, bag_size: 11684\n",
      "batch 419, loss: 0.5023, label: 0, bag_size: 23714\n",
      "batch 439, loss: 0.3495, label: 0, bag_size: 1684\n",
      "batch 459, loss: 0.5029, label: 1, bag_size: 7217\n",
      "batch 479, loss: 0.5905, label: 0, bag_size: 10415\n",
      "batch 499, loss: 0.7250, label: 0, bag_size: 10751\n",
      "batch 519, loss: 0.4037, label: 0, bag_size: 9930\n",
      "batch 539, loss: 0.2425, label: 1, bag_size: 21701\n",
      "batch 559, loss: 0.6770, label: 1, bag_size: 5723\n",
      "batch 579, loss: 0.2968, label: 1, bag_size: 12611\n",
      "batch 599, loss: 1.1325, label: 1, bag_size: 1230\n",
      "batch 619, loss: 0.2039, label: 0, bag_size: 2063\n",
      "batch 639, loss: 0.5092, label: 0, bag_size: 27158\n",
      "batch 659, loss: 0.7305, label: 0, bag_size: 3198\n",
      "batch 679, loss: 0.6867, label: 0, bag_size: 10490\n",
      "batch 699, loss: 1.0698, label: 0, bag_size: 18738\n",
      "batch 719, loss: 0.5962, label: 1, bag_size: 8216\n",
      "batch 739, loss: 0.2825, label: 1, bag_size: 9877\n",
      "batch 759, loss: 0.2634, label: 1, bag_size: 9408\n",
      "batch 779, loss: 0.1863, label: 0, bag_size: 8372\n",
      "batch 799, loss: 0.4088, label: 0, bag_size: 11512\n",
      "batch 819, loss: 0.4462, label: 1, bag_size: 11195\n",
      "Epoch: 0, train_loss: 0.6005, train_error: 0.3195\n",
      "class 0: acc 0.7692307692307693, correct 340/442\n",
      "class 1: acc 0.5767195767195767, correct 218/378\n",
      "\n",
      "Val Set, val_loss: 0.5072, val_error: 0.2273, auc: 0.9586\n",
      "class 0: acc 1.0, correct 52/52\n",
      "class 1: acc 0.5689655172413793, correct 33/58\n",
      "Validation loss decreased (inf --> 0.507219).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6283, label: 1, bag_size: 5629\n",
      "batch 39, loss: 0.5199, label: 0, bag_size: 2270\n",
      "batch 59, loss: 0.1492, label: 1, bag_size: 6090\n",
      "batch 79, loss: 0.6937, label: 0, bag_size: 7835\n",
      "batch 99, loss: 0.0807, label: 1, bag_size: 9955\n",
      "batch 119, loss: 0.7549, label: 1, bag_size: 10622\n",
      "batch 139, loss: 1.3564, label: 1, bag_size: 5256\n",
      "batch 159, loss: 0.7216, label: 0, bag_size: 1458\n",
      "batch 179, loss: 0.6912, label: 0, bag_size: 8866\n",
      "batch 199, loss: 0.3392, label: 0, bag_size: 1437\n",
      "batch 219, loss: 0.0992, label: 1, bag_size: 18794\n",
      "batch 239, loss: 0.4567, label: 0, bag_size: 2213\n",
      "batch 259, loss: 0.5601, label: 0, bag_size: 6356\n",
      "batch 279, loss: 0.2470, label: 0, bag_size: 8948\n",
      "batch 299, loss: 0.2684, label: 1, bag_size: 6927\n",
      "batch 319, loss: 1.1042, label: 0, bag_size: 2815\n",
      "batch 339, loss: 0.1418, label: 0, bag_size: 2244\n",
      "batch 359, loss: 0.1103, label: 0, bag_size: 11865\n",
      "batch 379, loss: 1.2736, label: 1, bag_size: 2682\n",
      "batch 399, loss: 0.2799, label: 1, bag_size: 6606\n",
      "batch 419, loss: 0.1970, label: 1, bag_size: 5231\n",
      "batch 439, loss: 0.0612, label: 0, bag_size: 1760\n",
      "batch 459, loss: 0.1785, label: 0, bag_size: 11122\n",
      "batch 479, loss: 0.4349, label: 1, bag_size: 11421\n",
      "batch 499, loss: 1.1167, label: 0, bag_size: 6850\n",
      "batch 519, loss: 0.3352, label: 1, bag_size: 13947\n",
      "batch 539, loss: 0.2710, label: 0, bag_size: 1213\n",
      "batch 559, loss: 1.5451, label: 1, bag_size: 12340\n",
      "batch 579, loss: 0.2125, label: 0, bag_size: 21093\n",
      "batch 599, loss: 0.1299, label: 0, bag_size: 12149\n",
      "batch 619, loss: 0.1063, label: 0, bag_size: 8812\n",
      "batch 639, loss: 0.3414, label: 0, bag_size: 2367\n",
      "batch 659, loss: 0.6507, label: 0, bag_size: 9069\n",
      "batch 679, loss: 0.1457, label: 0, bag_size: 2322\n",
      "batch 699, loss: 0.7826, label: 0, bag_size: 15747\n",
      "batch 719, loss: 0.1490, label: 0, bag_size: 8981\n",
      "batch 739, loss: 0.1943, label: 1, bag_size: 13051\n",
      "batch 759, loss: 1.5889, label: 1, bag_size: 1831\n",
      "batch 779, loss: 0.6574, label: 0, bag_size: 11128\n",
      "batch 799, loss: 0.1136, label: 0, bag_size: 15001\n",
      "batch 819, loss: 0.6418, label: 1, bag_size: 1764\n",
      "Epoch: 1, train_loss: 0.4389, train_error: 0.1720\n",
      "class 0: acc 0.8747203579418344, correct 391/447\n",
      "class 1: acc 0.7721179624664879, correct 288/373\n",
      "\n",
      "Val Set, val_loss: 0.2945, val_error: 0.0727, auc: 0.9751\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "Validation loss decreased (0.507219 --> 0.294473).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0670, label: 1, bag_size: 13194\n",
      "batch 39, loss: 0.4127, label: 1, bag_size: 6343\n",
      "batch 59, loss: 0.5586, label: 0, bag_size: 3228\n",
      "batch 79, loss: 0.4802, label: 1, bag_size: 2522\n",
      "batch 99, loss: 0.0888, label: 0, bag_size: 1052\n",
      "batch 119, loss: 0.0387, label: 1, bag_size: 13194\n",
      "batch 139, loss: 0.0961, label: 1, bag_size: 5864\n",
      "batch 159, loss: 0.1417, label: 0, bag_size: 11146\n",
      "batch 179, loss: 0.4993, label: 0, bag_size: 10146\n",
      "batch 199, loss: 0.1171, label: 1, bag_size: 16034\n",
      "batch 219, loss: 0.2040, label: 0, bag_size: 2760\n",
      "batch 239, loss: 0.3588, label: 1, bag_size: 9404\n",
      "batch 259, loss: 0.8157, label: 1, bag_size: 2935\n",
      "batch 279, loss: 0.4176, label: 0, bag_size: 5009\n",
      "batch 299, loss: 0.1681, label: 0, bag_size: 10381\n",
      "batch 319, loss: 0.4971, label: 0, bag_size: 2006\n",
      "batch 339, loss: 0.0682, label: 1, bag_size: 6792\n",
      "batch 359, loss: 0.2706, label: 0, bag_size: 763\n",
      "batch 379, loss: 0.1234, label: 0, bag_size: 19067\n",
      "batch 399, loss: 0.1574, label: 0, bag_size: 15672\n",
      "batch 419, loss: 1.2907, label: 1, bag_size: 2937\n",
      "batch 439, loss: 0.2314, label: 0, bag_size: 15672\n",
      "batch 459, loss: 0.4202, label: 0, bag_size: 8330\n",
      "batch 479, loss: 0.2234, label: 1, bag_size: 11394\n",
      "batch 499, loss: 0.2036, label: 0, bag_size: 13591\n",
      "batch 519, loss: 0.0292, label: 0, bag_size: 21082\n",
      "batch 539, loss: 0.7965, label: 1, bag_size: 1255\n",
      "batch 559, loss: 0.0148, label: 1, bag_size: 18794\n",
      "batch 579, loss: 2.1085, label: 1, bag_size: 3121\n",
      "batch 599, loss: 0.1457, label: 1, bag_size: 6731\n",
      "batch 619, loss: 0.5613, label: 1, bag_size: 1064\n",
      "batch 639, loss: 0.3345, label: 1, bag_size: 1230\n",
      "batch 659, loss: 0.0384, label: 1, bag_size: 6736\n",
      "batch 679, loss: 0.0814, label: 1, bag_size: 7119\n",
      "batch 699, loss: 0.1385, label: 1, bag_size: 865\n",
      "batch 719, loss: 0.0728, label: 1, bag_size: 9878\n",
      "batch 739, loss: 0.0339, label: 0, bag_size: 23791\n",
      "batch 759, loss: 1.3738, label: 0, bag_size: 6356\n",
      "batch 779, loss: 0.2280, label: 1, bag_size: 10912\n",
      "batch 799, loss: 0.3569, label: 1, bag_size: 9983\n",
      "batch 819, loss: 0.0095, label: 1, bag_size: 19039\n",
      "Epoch: 2, train_loss: 0.3422, train_error: 0.1232\n",
      "class 0: acc 0.900990099009901, correct 364/404\n",
      "class 1: acc 0.8533653846153846, correct 355/416\n",
      "\n",
      "Val Set, val_loss: 0.2654, val_error: 0.0818, auc: 0.9768\n",
      "class 0: acc 0.8653846153846154, correct 45/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "Validation loss decreased (0.294473 --> 0.265411).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2747, label: 0, bag_size: 2322\n",
      "batch 39, loss: 0.4416, label: 1, bag_size: 21252\n",
      "batch 59, loss: 0.3663, label: 0, bag_size: 3893\n",
      "batch 79, loss: 0.2956, label: 0, bag_size: 15898\n",
      "batch 99, loss: 0.4903, label: 0, bag_size: 1920\n",
      "batch 119, loss: 0.1269, label: 0, bag_size: 11546\n",
      "batch 139, loss: 0.0609, label: 1, bag_size: 20161\n",
      "batch 159, loss: 0.1582, label: 1, bag_size: 7798\n",
      "batch 179, loss: 0.2398, label: 0, bag_size: 12796\n",
      "batch 199, loss: 0.2643, label: 0, bag_size: 2044\n",
      "batch 219, loss: 0.0838, label: 0, bag_size: 1831\n",
      "batch 239, loss: 0.1414, label: 0, bag_size: 1690\n",
      "batch 259, loss: 0.0871, label: 0, bag_size: 1962\n",
      "batch 279, loss: 0.0147, label: 1, bag_size: 4862\n",
      "batch 299, loss: 1.0186, label: 0, bag_size: 7239\n",
      "batch 319, loss: 0.0610, label: 1, bag_size: 689\n",
      "batch 339, loss: 0.6935, label: 1, bag_size: 5256\n",
      "batch 359, loss: 0.9398, label: 1, bag_size: 9942\n",
      "batch 379, loss: 0.1456, label: 0, bag_size: 8788\n",
      "batch 399, loss: 0.5678, label: 1, bag_size: 928\n",
      "batch 419, loss: 0.1222, label: 0, bag_size: 15841\n",
      "batch 439, loss: 0.1108, label: 0, bag_size: 1127\n",
      "batch 459, loss: 0.0609, label: 1, bag_size: 4394\n",
      "batch 479, loss: 0.0359, label: 1, bag_size: 19932\n",
      "batch 499, loss: 0.2752, label: 1, bag_size: 9519\n",
      "batch 519, loss: 0.2966, label: 1, bag_size: 1339\n",
      "batch 539, loss: 0.1179, label: 0, bag_size: 3557\n",
      "batch 559, loss: 0.2129, label: 1, bag_size: 5454\n",
      "batch 579, loss: 0.3848, label: 0, bag_size: 7637\n",
      "batch 599, loss: 0.0178, label: 0, bag_size: 9433\n",
      "batch 619, loss: 0.0747, label: 0, bag_size: 10068\n",
      "batch 639, loss: 0.0731, label: 1, bag_size: 12758\n",
      "batch 659, loss: 0.6387, label: 0, bag_size: 14249\n",
      "batch 679, loss: 0.0798, label: 1, bag_size: 10460\n",
      "batch 699, loss: 0.0249, label: 1, bag_size: 8448\n",
      "batch 719, loss: 0.3240, label: 0, bag_size: 2282\n",
      "batch 739, loss: 0.7381, label: 0, bag_size: 3552\n",
      "batch 759, loss: 1.2488, label: 0, bag_size: 47866\n",
      "batch 779, loss: 0.0338, label: 0, bag_size: 9949\n",
      "batch 799, loss: 0.0242, label: 1, bag_size: 1022\n",
      "batch 819, loss: 0.0323, label: 1, bag_size: 10498\n",
      "Epoch: 3, train_loss: 0.3103, train_error: 0.1171\n",
      "class 0: acc 0.8974358974358975, correct 350/390\n",
      "class 1: acc 0.8697674418604651, correct 374/430\n",
      "\n",
      "Val Set, val_loss: 0.2743, val_error: 0.0909, auc: 0.9728\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0434, label: 1, bag_size: 6752\n",
      "batch 39, loss: 0.3443, label: 1, bag_size: 7389\n",
      "batch 59, loss: 0.0185, label: 1, bag_size: 1101\n",
      "batch 79, loss: 0.1003, label: 1, bag_size: 12178\n",
      "batch 99, loss: 0.0210, label: 0, bag_size: 9885\n",
      "batch 119, loss: 0.0095, label: 0, bag_size: 18154\n",
      "batch 139, loss: 0.0429, label: 1, bag_size: 7669\n",
      "batch 159, loss: 0.0532, label: 0, bag_size: 12148\n",
      "batch 179, loss: 0.0059, label: 1, bag_size: 12349\n",
      "batch 199, loss: 1.0607, label: 1, bag_size: 1294\n",
      "batch 219, loss: 0.1158, label: 0, bag_size: 11151\n",
      "batch 239, loss: 0.0452, label: 0, bag_size: 9060\n",
      "batch 259, loss: 0.2513, label: 1, bag_size: 1493\n",
      "batch 279, loss: 0.7366, label: 1, bag_size: 621\n",
      "batch 299, loss: 0.0491, label: 1, bag_size: 14618\n",
      "batch 319, loss: 0.0811, label: 0, bag_size: 12212\n",
      "batch 339, loss: 0.0521, label: 0, bag_size: 19472\n",
      "batch 359, loss: 0.3779, label: 0, bag_size: 2609\n",
      "batch 379, loss: 0.2474, label: 0, bag_size: 11194\n",
      "batch 399, loss: 0.0366, label: 0, bag_size: 23398\n",
      "batch 419, loss: 1.7901, label: 0, bag_size: 4997\n",
      "batch 439, loss: 0.0231, label: 1, bag_size: 7935\n",
      "batch 459, loss: 0.3442, label: 0, bag_size: 24382\n",
      "batch 479, loss: 0.1990, label: 0, bag_size: 13339\n",
      "batch 499, loss: 0.1500, label: 0, bag_size: 7557\n",
      "batch 519, loss: 0.0874, label: 0, bag_size: 22426\n",
      "batch 539, loss: 0.0150, label: 1, bag_size: 12408\n",
      "batch 559, loss: 0.0511, label: 1, bag_size: 5345\n",
      "batch 579, loss: 0.1270, label: 0, bag_size: 3474\n",
      "batch 599, loss: 0.0552, label: 0, bag_size: 12217\n",
      "batch 619, loss: 0.0245, label: 1, bag_size: 4039\n",
      "batch 639, loss: 0.0130, label: 0, bag_size: 10481\n",
      "batch 659, loss: 0.4278, label: 0, bag_size: 8420\n",
      "batch 679, loss: 0.3226, label: 1, bag_size: 7381\n",
      "batch 699, loss: 0.0976, label: 1, bag_size: 2522\n",
      "batch 719, loss: 0.0719, label: 1, bag_size: 5292\n",
      "batch 739, loss: 0.0819, label: 0, bag_size: 21093\n",
      "batch 759, loss: 0.0432, label: 1, bag_size: 617\n",
      "batch 779, loss: 0.6404, label: 1, bag_size: 1919\n",
      "batch 799, loss: 0.0784, label: 0, bag_size: 12524\n",
      "batch 819, loss: 0.0269, label: 1, bag_size: 1622\n",
      "Epoch: 4, train_loss: 0.3065, train_error: 0.0976\n",
      "class 0: acc 0.9253012048192771, correct 384/415\n",
      "class 1: acc 0.8790123456790123, correct 356/405\n",
      "\n",
      "Val Set, val_loss: 0.2563, val_error: 0.1000, auc: 0.9728\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "Validation loss decreased (0.265411 --> 0.256271).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3500, label: 1, bag_size: 8012\n",
      "batch 39, loss: 0.0665, label: 1, bag_size: 3368\n",
      "batch 59, loss: 0.1611, label: 1, bag_size: 1888\n",
      "batch 79, loss: 0.0448, label: 0, bag_size: 9485\n",
      "batch 99, loss: 1.9704, label: 0, bag_size: 2219\n",
      "batch 119, loss: 0.0869, label: 0, bag_size: 11512\n",
      "batch 139, loss: 0.1272, label: 0, bag_size: 5225\n",
      "batch 159, loss: 0.0377, label: 1, bag_size: 2412\n",
      "batch 179, loss: 0.0430, label: 0, bag_size: 2091\n",
      "batch 199, loss: 0.0221, label: 1, bag_size: 12719\n",
      "batch 219, loss: 0.1808, label: 0, bag_size: 2654\n",
      "batch 239, loss: 0.1898, label: 0, bag_size: 12510\n",
      "batch 259, loss: 0.1090, label: 0, bag_size: 1712\n",
      "batch 279, loss: 0.0030, label: 0, bag_size: 9433\n",
      "batch 299, loss: 0.0673, label: 1, bag_size: 3968\n",
      "batch 319, loss: 0.0885, label: 1, bag_size: 2356\n",
      "batch 339, loss: 0.0784, label: 1, bag_size: 8475\n",
      "batch 359, loss: 0.1090, label: 0, bag_size: 1684\n",
      "batch 379, loss: 0.0170, label: 0, bag_size: 13892\n",
      "batch 399, loss: 0.9427, label: 0, bag_size: 9132\n",
      "batch 419, loss: 0.7592, label: 0, bag_size: 2098\n",
      "batch 439, loss: 0.4059, label: 1, bag_size: 621\n",
      "batch 459, loss: 0.3913, label: 0, bag_size: 3908\n",
      "batch 479, loss: 0.0307, label: 0, bag_size: 31780\n",
      "batch 499, loss: 1.4792, label: 1, bag_size: 1755\n",
      "batch 519, loss: 0.5300, label: 0, bag_size: 21361\n",
      "batch 539, loss: 0.0944, label: 0, bag_size: 1789\n",
      "batch 559, loss: 0.1336, label: 1, bag_size: 5723\n",
      "batch 579, loss: 0.1559, label: 0, bag_size: 3541\n",
      "batch 599, loss: 0.1069, label: 0, bag_size: 2367\n",
      "batch 619, loss: 0.5233, label: 1, bag_size: 21450\n",
      "batch 639, loss: 0.0824, label: 1, bag_size: 10498\n",
      "batch 659, loss: 0.6395, label: 1, bag_size: 20537\n",
      "batch 679, loss: 2.7441, label: 0, bag_size: 17279\n",
      "batch 699, loss: 0.0730, label: 1, bag_size: 621\n",
      "batch 719, loss: 0.0060, label: 0, bag_size: 1984\n",
      "batch 739, loss: 0.2948, label: 1, bag_size: 20537\n",
      "batch 759, loss: 0.1001, label: 0, bag_size: 18225\n",
      "batch 779, loss: 0.0130, label: 1, bag_size: 15213\n",
      "batch 799, loss: 0.0453, label: 0, bag_size: 8145\n",
      "batch 819, loss: 0.0275, label: 0, bag_size: 10415\n",
      "Epoch: 5, train_loss: 0.2815, train_error: 0.1037\n",
      "class 0: acc 0.9114832535885168, correct 381/418\n",
      "class 1: acc 0.8805970149253731, correct 354/402\n",
      "\n",
      "Val Set, val_loss: 0.2145, val_error: 0.0636, auc: 0.9751\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9137931034482759, correct 53/58\n",
      "Validation loss decreased (0.256271 --> 0.214470).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0879, label: 0, bag_size: 1349\n",
      "batch 39, loss: 0.5949, label: 0, bag_size: 3654\n",
      "batch 59, loss: 0.6548, label: 0, bag_size: 3228\n",
      "batch 79, loss: 0.0895, label: 0, bag_size: 21682\n",
      "batch 99, loss: 0.1955, label: 1, bag_size: 9446\n",
      "batch 119, loss: 3.4217, label: 0, bag_size: 5105\n",
      "batch 139, loss: 0.0103, label: 1, bag_size: 6164\n",
      "batch 159, loss: 0.0086, label: 1, bag_size: 10725\n",
      "batch 179, loss: 0.0110, label: 1, bag_size: 2412\n",
      "batch 199, loss: 0.0487, label: 1, bag_size: 617\n",
      "batch 219, loss: 2.1190, label: 0, bag_size: 2815\n",
      "batch 239, loss: 0.1339, label: 0, bag_size: 12510\n",
      "batch 259, loss: 0.0310, label: 1, bag_size: 11421\n",
      "batch 279, loss: 0.0158, label: 1, bag_size: 7110\n",
      "batch 299, loss: 0.1472, label: 1, bag_size: 5292\n",
      "batch 319, loss: 0.1927, label: 1, bag_size: 11223\n",
      "batch 339, loss: 0.0791, label: 1, bag_size: 11220\n",
      "batch 359, loss: 1.5781, label: 0, bag_size: 47866\n",
      "batch 379, loss: 0.0293, label: 0, bag_size: 12910\n",
      "batch 399, loss: 1.9111, label: 1, bag_size: 6360\n",
      "batch 419, loss: 0.6999, label: 0, bag_size: 1506\n",
      "batch 439, loss: 0.0036, label: 1, bag_size: 14618\n",
      "batch 459, loss: 0.0801, label: 1, bag_size: 7389\n",
      "batch 479, loss: 0.1758, label: 0, bag_size: 13332\n",
      "batch 499, loss: 0.2135, label: 0, bag_size: 9252\n",
      "batch 519, loss: 0.0405, label: 0, bag_size: 19466\n",
      "batch 539, loss: 0.0620, label: 1, bag_size: 12178\n",
      "batch 559, loss: 0.1606, label: 0, bag_size: 8549\n",
      "batch 579, loss: 0.4520, label: 1, bag_size: 5366\n",
      "batch 599, loss: 0.0267, label: 1, bag_size: 20333\n",
      "batch 619, loss: 0.3333, label: 0, bag_size: 7428\n",
      "batch 639, loss: 0.0317, label: 1, bag_size: 6343\n",
      "batch 659, loss: 0.9149, label: 1, bag_size: 1095\n",
      "batch 679, loss: 0.0186, label: 1, bag_size: 7669\n",
      "batch 699, loss: 0.0196, label: 1, bag_size: 15609\n",
      "batch 719, loss: 0.2453, label: 0, bag_size: 6367\n",
      "batch 739, loss: 0.0048, label: 1, bag_size: 12931\n",
      "batch 759, loss: 0.2296, label: 0, bag_size: 1458\n",
      "batch 779, loss: 0.6086, label: 0, bag_size: 9132\n",
      "batch 799, loss: 0.2097, label: 1, bag_size: 1759\n",
      "batch 819, loss: 0.3951, label: 1, bag_size: 7246\n",
      "Epoch: 6, train_loss: 0.2510, train_error: 0.0854\n",
      "class 0: acc 0.9177377892030848, correct 357/389\n",
      "class 1: acc 0.9118329466357309, correct 393/431\n",
      "\n",
      "Val Set, val_loss: 0.2014, val_error: 0.0545, auc: 0.9794\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "Validation loss decreased (0.214470 --> 0.201374).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6836, label: 0, bag_size: 23714\n",
      "batch 39, loss: 0.0352, label: 0, bag_size: 1127\n",
      "batch 59, loss: 0.0643, label: 1, bag_size: 1759\n",
      "batch 79, loss: 0.0590, label: 1, bag_size: 14230\n",
      "batch 99, loss: 0.3421, label: 0, bag_size: 5409\n",
      "batch 119, loss: 0.2349, label: 1, bag_size: 9004\n",
      "batch 139, loss: 0.0647, label: 1, bag_size: 5907\n",
      "batch 159, loss: 0.2576, label: 0, bag_size: 7823\n",
      "batch 179, loss: 0.0004, label: 1, bag_size: 11195\n",
      "batch 199, loss: 0.1354, label: 1, bag_size: 3856\n",
      "batch 219, loss: 0.1287, label: 1, bag_size: 1920\n",
      "batch 239, loss: 0.0731, label: 0, bag_size: 16607\n",
      "batch 259, loss: 0.0121, label: 0, bag_size: 16782\n",
      "batch 279, loss: 0.3247, label: 1, bag_size: 5516\n",
      "batch 299, loss: 0.0217, label: 1, bag_size: 14202\n",
      "batch 319, loss: 0.1925, label: 1, bag_size: 11032\n",
      "batch 339, loss: 0.0128, label: 1, bag_size: 5991\n",
      "batch 359, loss: 0.0262, label: 0, bag_size: 11187\n",
      "batch 379, loss: 0.1633, label: 1, bag_size: 10622\n",
      "batch 399, loss: 0.0195, label: 1, bag_size: 3968\n",
      "batch 419, loss: 0.0121, label: 0, bag_size: 16341\n",
      "batch 439, loss: 0.0574, label: 0, bag_size: 21864\n",
      "batch 459, loss: 0.0299, label: 1, bag_size: 13051\n",
      "batch 479, loss: 0.0511, label: 1, bag_size: 3674\n",
      "batch 499, loss: 0.3023, label: 0, bag_size: 2303\n",
      "batch 519, loss: 3.3473, label: 1, bag_size: 3879\n",
      "batch 539, loss: 0.3831, label: 0, bag_size: 11727\n",
      "batch 559, loss: 0.0097, label: 1, bag_size: 8475\n",
      "batch 579, loss: 0.0278, label: 0, bag_size: 10942\n",
      "batch 599, loss: 0.0516, label: 1, bag_size: 18161\n",
      "batch 619, loss: 0.0020, label: 1, bag_size: 1638\n",
      "batch 639, loss: 0.1023, label: 1, bag_size: 10492\n",
      "batch 659, loss: 0.8693, label: 1, bag_size: 9215\n",
      "batch 679, loss: 0.0035, label: 1, bag_size: 19039\n",
      "batch 699, loss: 0.1014, label: 1, bag_size: 11220\n",
      "batch 719, loss: 0.2936, label: 0, bag_size: 4523\n",
      "batch 739, loss: 0.0061, label: 1, bag_size: 15716\n",
      "batch 759, loss: 0.0662, label: 1, bag_size: 1888\n",
      "batch 779, loss: 0.0308, label: 0, bag_size: 10898\n",
      "batch 799, loss: 0.2742, label: 1, bag_size: 11256\n",
      "batch 819, loss: 0.0007, label: 1, bag_size: 5317\n",
      "Epoch: 7, train_loss: 0.2659, train_error: 0.1037\n",
      "class 0: acc 0.898989898989899, correct 356/396\n",
      "class 1: acc 0.8938679245283019, correct 379/424\n",
      "\n",
      "Val Set, val_loss: 0.2305, val_error: 0.1000, auc: 0.9801\n",
      "class 0: acc 0.8076923076923077, correct 42/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0136, label: 1, bag_size: 4394\n",
      "batch 39, loss: 0.5817, label: 1, bag_size: 12626\n",
      "batch 59, loss: 0.3865, label: 1, bag_size: 6360\n",
      "batch 79, loss: 0.0061, label: 0, bag_size: 12217\n",
      "batch 99, loss: 0.0050, label: 1, bag_size: 13255\n",
      "batch 119, loss: 0.0017, label: 1, bag_size: 6090\n",
      "batch 139, loss: 0.1916, label: 0, bag_size: 9069\n",
      "batch 159, loss: 0.0211, label: 1, bag_size: 5723\n",
      "batch 179, loss: 0.0051, label: 1, bag_size: 13947\n",
      "batch 199, loss: 0.0114, label: 1, bag_size: 15332\n",
      "batch 219, loss: 0.1444, label: 1, bag_size: 7768\n",
      "batch 239, loss: 0.0081, label: 0, bag_size: 8372\n",
      "batch 259, loss: 0.3435, label: 1, bag_size: 2522\n",
      "batch 279, loss: 0.0127, label: 1, bag_size: 3968\n",
      "batch 299, loss: 0.0684, label: 0, bag_size: 3670\n",
      "batch 319, loss: 0.0080, label: 0, bag_size: 6624\n",
      "batch 339, loss: 0.0753, label: 1, bag_size: 1493\n",
      "batch 359, loss: 0.0061, label: 0, bag_size: 1415\n",
      "batch 379, loss: 0.9417, label: 0, bag_size: 1714\n",
      "batch 399, loss: 0.0071, label: 1, bag_size: 8019\n",
      "batch 419, loss: 0.0955, label: 0, bag_size: 2160\n",
      "batch 439, loss: 0.1122, label: 1, bag_size: 10460\n",
      "batch 459, loss: 0.0507, label: 1, bag_size: 16267\n",
      "batch 479, loss: 0.0108, label: 0, bag_size: 11512\n",
      "batch 499, loss: 0.0280, label: 0, bag_size: 10068\n",
      "batch 519, loss: 0.0388, label: 0, bag_size: 14625\n",
      "batch 539, loss: 0.1251, label: 0, bag_size: 3265\n",
      "batch 559, loss: 0.0386, label: 0, bag_size: 22870\n",
      "batch 579, loss: 0.7729, label: 1, bag_size: 2344\n",
      "batch 599, loss: 0.0494, label: 0, bag_size: 2322\n",
      "batch 619, loss: 0.9299, label: 1, bag_size: 2314\n",
      "batch 639, loss: 0.0216, label: 0, bag_size: 5551\n",
      "batch 659, loss: 0.0233, label: 1, bag_size: 6736\n",
      "batch 679, loss: 0.3677, label: 0, bag_size: 2920\n",
      "batch 699, loss: 0.0122, label: 0, bag_size: 19518\n",
      "batch 719, loss: 0.0120, label: 0, bag_size: 9851\n",
      "batch 739, loss: 0.1747, label: 0, bag_size: 9485\n",
      "batch 759, loss: 0.0019, label: 1, bag_size: 9078\n",
      "batch 779, loss: 0.0006, label: 1, bag_size: 629\n",
      "batch 799, loss: 0.0001, label: 1, bag_size: 9644\n",
      "batch 819, loss: 0.0321, label: 0, bag_size: 9930\n",
      "Epoch: 8, train_loss: 0.2296, train_error: 0.0866\n",
      "class 0: acc 0.9212410501193318, correct 386/419\n",
      "class 1: acc 0.9052369077306733, correct 363/401\n",
      "\n",
      "Val Set, val_loss: 0.1922, val_error: 0.0636, auc: 0.9788\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "Validation loss decreased (0.201374 --> 0.192198).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1383, label: 0, bag_size: 1498\n",
      "batch 39, loss: 0.0076, label: 0, bag_size: 23791\n",
      "batch 59, loss: 0.1839, label: 1, bag_size: 1822\n",
      "batch 79, loss: 0.0108, label: 1, bag_size: 10969\n",
      "batch 99, loss: 0.0556, label: 1, bag_size: 13015\n",
      "batch 119, loss: 0.0212, label: 0, bag_size: 2179\n",
      "batch 139, loss: 0.0044, label: 0, bag_size: 1052\n",
      "batch 159, loss: 0.0281, label: 1, bag_size: 10072\n",
      "batch 179, loss: 0.1114, label: 0, bag_size: 10415\n",
      "batch 199, loss: 0.0084, label: 1, bag_size: 11600\n",
      "batch 219, loss: 0.0274, label: 1, bag_size: 8754\n",
      "batch 239, loss: 0.1873, label: 0, bag_size: 2760\n",
      "batch 259, loss: 0.0078, label: 0, bag_size: 2282\n",
      "batch 279, loss: 1.3652, label: 0, bag_size: 25814\n",
      "batch 299, loss: 0.7689, label: 0, bag_size: 17279\n",
      "batch 319, loss: 0.0031, label: 0, bag_size: 12732\n",
      "batch 339, loss: 0.0222, label: 1, bag_size: 11394\n",
      "batch 359, loss: 0.0016, label: 1, bag_size: 12865\n",
      "batch 379, loss: 0.2695, label: 1, bag_size: 19972\n",
      "batch 399, loss: 0.0145, label: 0, bag_size: 22828\n",
      "batch 419, loss: 0.0012, label: 1, bag_size: 3409\n",
      "batch 439, loss: 0.0819, label: 1, bag_size: 5292\n",
      "batch 459, loss: 0.0928, label: 0, bag_size: 1920\n",
      "batch 479, loss: 0.0312, label: 0, bag_size: 12524\n",
      "batch 499, loss: 0.0028, label: 1, bag_size: 4039\n",
      "batch 519, loss: 0.0535, label: 0, bag_size: 31106\n",
      "batch 539, loss: 0.8447, label: 1, bag_size: 16514\n",
      "batch 559, loss: 0.0160, label: 1, bag_size: 7873\n",
      "batch 579, loss: 0.0441, label: 0, bag_size: 1127\n",
      "batch 599, loss: 0.0260, label: 1, bag_size: 10460\n",
      "batch 619, loss: 0.0296, label: 1, bag_size: 14681\n",
      "batch 639, loss: 0.0033, label: 1, bag_size: 12795\n",
      "batch 659, loss: 0.1048, label: 1, bag_size: 16890\n",
      "batch 679, loss: 0.1299, label: 1, bag_size: 10501\n",
      "batch 699, loss: 0.0025, label: 1, bag_size: 5494\n",
      "batch 719, loss: 0.4230, label: 0, bag_size: 15898\n",
      "batch 739, loss: 0.0094, label: 0, bag_size: 14333\n",
      "batch 759, loss: 0.0437, label: 0, bag_size: 15841\n",
      "batch 779, loss: 0.0029, label: 1, bag_size: 12931\n",
      "batch 799, loss: 0.5541, label: 0, bag_size: 9387\n",
      "batch 819, loss: 0.0201, label: 1, bag_size: 6731\n",
      "Epoch: 9, train_loss: 0.2333, train_error: 0.0866\n",
      "class 0: acc 0.9209183673469388, correct 361/392\n",
      "class 1: acc 0.9065420560747663, correct 388/428\n",
      "\n",
      "Val Set, val_loss: 0.1827, val_error: 0.0545, auc: 0.9801\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "Validation loss decreased (0.192198 --> 0.182672).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0117, label: 1, bag_size: 14681\n",
      "batch 39, loss: 0.0073, label: 1, bag_size: 12795\n",
      "batch 59, loss: 1.5340, label: 1, bag_size: 2937\n",
      "batch 79, loss: 0.1457, label: 0, bag_size: 2534\n",
      "batch 99, loss: 0.0124, label: 0, bag_size: 13777\n",
      "batch 119, loss: 0.0141, label: 0, bag_size: 16720\n",
      "batch 139, loss: 0.0001, label: 1, bag_size: 629\n",
      "batch 159, loss: 0.0469, label: 0, bag_size: 2844\n",
      "batch 179, loss: 0.1678, label: 0, bag_size: 2998\n",
      "batch 199, loss: 0.0767, label: 0, bag_size: 2360\n",
      "batch 219, loss: 0.0396, label: 0, bag_size: 11778\n",
      "batch 239, loss: 3.5862, label: 0, bag_size: 3897\n",
      "batch 259, loss: 0.0144, label: 0, bag_size: 9888\n",
      "batch 279, loss: 0.9874, label: 0, bag_size: 7835\n",
      "batch 299, loss: 0.0087, label: 0, bag_size: 21076\n",
      "batch 319, loss: 0.0061, label: 1, bag_size: 2412\n",
      "batch 339, loss: 0.0113, label: 0, bag_size: 1962\n",
      "batch 359, loss: 0.4663, label: 0, bag_size: 3783\n",
      "batch 379, loss: 0.0537, label: 1, bag_size: 2455\n",
      "batch 399, loss: 0.0038, label: 1, bag_size: 11387\n",
      "batch 419, loss: 0.0004, label: 0, bag_size: 10481\n",
      "batch 439, loss: 0.0039, label: 1, bag_size: 5731\n",
      "batch 459, loss: 0.1146, label: 0, bag_size: 14249\n",
      "batch 479, loss: 0.4910, label: 1, bag_size: 15125\n",
      "batch 499, loss: 0.1501, label: 0, bag_size: 16211\n",
      "batch 519, loss: 0.0040, label: 0, bag_size: 1483\n",
      "batch 539, loss: 0.0046, label: 0, bag_size: 2628\n",
      "batch 559, loss: 0.0183, label: 1, bag_size: 3368\n",
      "batch 579, loss: 0.0563, label: 1, bag_size: 2412\n",
      "batch 599, loss: 0.0448, label: 1, bag_size: 25970\n",
      "batch 619, loss: 0.4528, label: 0, bag_size: 19808\n",
      "batch 639, loss: 0.0019, label: 1, bag_size: 9971\n",
      "batch 659, loss: 0.0602, label: 1, bag_size: 2522\n",
      "batch 679, loss: 0.0500, label: 1, bag_size: 13015\n",
      "batch 699, loss: 0.0012, label: 1, bag_size: 7078\n",
      "batch 719, loss: 0.5059, label: 0, bag_size: 5211\n",
      "batch 739, loss: 0.0041, label: 1, bag_size: 9571\n",
      "batch 759, loss: 0.0695, label: 1, bag_size: 5690\n",
      "batch 779, loss: 0.0139, label: 1, bag_size: 6606\n",
      "batch 799, loss: 0.3257, label: 0, bag_size: 14249\n",
      "batch 819, loss: 0.0460, label: 1, bag_size: 8438\n",
      "Epoch: 10, train_loss: 0.2676, train_error: 0.0951\n",
      "class 0: acc 0.9088607594936708, correct 359/395\n",
      "class 1: acc 0.9011764705882352, correct 383/425\n",
      "\n",
      "Val Set, val_loss: 0.2416, val_error: 0.1091, auc: 0.9811\n",
      "class 0: acc 0.7884615384615384, correct 41/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0089, label: 1, bag_size: 5612\n",
      "batch 39, loss: 0.5268, label: 0, bag_size: 9616\n",
      "batch 59, loss: 0.4461, label: 1, bag_size: 6726\n",
      "batch 79, loss: 0.0369, label: 0, bag_size: 23791\n",
      "batch 99, loss: 0.0063, label: 0, bag_size: 8948\n",
      "batch 119, loss: 0.0111, label: 0, bag_size: 12212\n",
      "batch 139, loss: 0.2553, label: 0, bag_size: 18516\n",
      "batch 159, loss: 0.0165, label: 1, bag_size: 16267\n",
      "batch 179, loss: 0.0256, label: 1, bag_size: 6736\n",
      "batch 199, loss: 0.3400, label: 1, bag_size: 1437\n",
      "batch 219, loss: 0.0372, label: 0, bag_size: 18954\n",
      "batch 239, loss: 0.0046, label: 1, bag_size: 9877\n",
      "batch 259, loss: 0.0068, label: 0, bag_size: 13964\n",
      "batch 279, loss: 0.0090, label: 0, bag_size: 6851\n",
      "batch 299, loss: 0.0255, label: 0, bag_size: 12524\n",
      "batch 319, loss: 1.5575, label: 0, bag_size: 7612\n",
      "batch 339, loss: 0.0488, label: 0, bag_size: 3774\n",
      "batch 359, loss: 0.0644, label: 1, bag_size: 9470\n",
      "batch 379, loss: 0.2775, label: 1, bag_size: 8026\n",
      "batch 399, loss: 0.0020, label: 1, bag_size: 6090\n",
      "batch 419, loss: 0.0367, label: 0, bag_size: 16782\n",
      "batch 439, loss: 0.0279, label: 0, bag_size: 8025\n",
      "batch 459, loss: 0.3142, label: 0, bag_size: 1508\n",
      "batch 479, loss: 0.0254, label: 0, bag_size: 1909\n",
      "batch 499, loss: 0.1693, label: 0, bag_size: 22498\n",
      "batch 519, loss: 0.1475, label: 0, bag_size: 15914\n",
      "batch 539, loss: 0.0214, label: 0, bag_size: 10898\n",
      "batch 559, loss: 0.0604, label: 1, bag_size: 9955\n",
      "batch 579, loss: 0.0042, label: 1, bag_size: 15233\n",
      "batch 599, loss: 0.0258, label: 1, bag_size: 9519\n",
      "batch 619, loss: 0.0423, label: 1, bag_size: 7119\n",
      "batch 639, loss: 0.1937, label: 1, bag_size: 12895\n",
      "batch 659, loss: 0.0306, label: 1, bag_size: 10396\n",
      "batch 679, loss: 0.4496, label: 0, bag_size: 7637\n",
      "batch 699, loss: 0.3646, label: 0, bag_size: 7835\n",
      "batch 719, loss: 0.0006, label: 0, bag_size: 10481\n",
      "batch 739, loss: 0.0011, label: 1, bag_size: 12611\n",
      "batch 759, loss: 0.0259, label: 1, bag_size: 1255\n",
      "batch 779, loss: 0.0028, label: 1, bag_size: 14433\n",
      "batch 799, loss: 0.0072, label: 0, bag_size: 12524\n",
      "batch 819, loss: 0.0227, label: 0, bag_size: 4902\n",
      "Epoch: 11, train_loss: 0.2591, train_error: 0.0927\n",
      "class 0: acc 0.9148418491484185, correct 376/411\n",
      "class 1: acc 0.8997555012224939, correct 368/409\n",
      "\n",
      "Val Set, val_loss: 0.2001, val_error: 0.0545, auc: 0.9838\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.1701, label: 0, bag_size: 14664\n",
      "batch 39, loss: 0.0176, label: 0, bag_size: 23368\n",
      "batch 59, loss: 0.0034, label: 1, bag_size: 17486\n",
      "batch 79, loss: 0.0138, label: 1, bag_size: 19039\n",
      "batch 99, loss: 0.0169, label: 1, bag_size: 11600\n",
      "batch 119, loss: 0.0078, label: 0, bag_size: 18225\n",
      "batch 139, loss: 0.3034, label: 1, bag_size: 4789\n",
      "batch 159, loss: 0.0076, label: 1, bag_size: 3453\n",
      "batch 179, loss: 0.0002, label: 1, bag_size: 5221\n",
      "batch 199, loss: 0.0300, label: 1, bag_size: 16890\n",
      "batch 219, loss: 0.0512, label: 0, bag_size: 1824\n",
      "batch 239, loss: 0.6132, label: 0, bag_size: 10113\n",
      "batch 259, loss: 0.0667, label: 1, bag_size: 7424\n",
      "batch 279, loss: 0.2005, label: 1, bag_size: 6478\n",
      "batch 299, loss: 0.0141, label: 0, bag_size: 16992\n",
      "batch 319, loss: 0.0053, label: 0, bag_size: 14956\n",
      "batch 339, loss: 0.0484, label: 1, bag_size: 2308\n",
      "batch 359, loss: 0.1932, label: 0, bag_size: 7557\n",
      "batch 379, loss: 0.0174, label: 1, bag_size: 8019\n",
      "batch 399, loss: 1.4442, label: 1, bag_size: 1242\n",
      "batch 419, loss: 0.0261, label: 1, bag_size: 11220\n",
      "batch 439, loss: 0.6506, label: 0, bag_size: 2920\n",
      "batch 459, loss: 0.0040, label: 1, bag_size: 13026\n",
      "batch 479, loss: 0.0728, label: 0, bag_size: 3089\n",
      "batch 499, loss: 0.2692, label: 0, bag_size: 10029\n",
      "batch 519, loss: 0.0033, label: 1, bag_size: 12931\n",
      "batch 539, loss: 0.0011, label: 1, bag_size: 13947\n",
      "batch 559, loss: 0.0313, label: 0, bag_size: 11527\n",
      "batch 579, loss: 0.0286, label: 1, bag_size: 10671\n",
      "batch 599, loss: 0.0466, label: 1, bag_size: 10501\n",
      "batch 619, loss: 0.0169, label: 0, bag_size: 1149\n",
      "batch 639, loss: 0.0603, label: 0, bag_size: 15747\n",
      "batch 659, loss: 0.0220, label: 0, bag_size: 9234\n",
      "batch 679, loss: 0.0190, label: 0, bag_size: 1651\n",
      "batch 699, loss: 0.0025, label: 0, bag_size: 10995\n",
      "batch 719, loss: 0.0164, label: 0, bag_size: 31106\n",
      "batch 739, loss: 0.0788, label: 1, bag_size: 16565\n",
      "batch 759, loss: 0.0006, label: 1, bag_size: 9065\n",
      "batch 779, loss: 0.0011, label: 1, bag_size: 7110\n",
      "batch 799, loss: 0.0156, label: 0, bag_size: 18415\n",
      "batch 819, loss: 0.0016, label: 1, bag_size: 15213\n",
      "Epoch: 12, train_loss: 0.2015, train_error: 0.0829\n",
      "class 0: acc 0.9314420803782506, correct 394/423\n",
      "class 1: acc 0.9017632241813602, correct 358/397\n",
      "\n",
      "Val Set, val_loss: 0.1729, val_error: 0.0455, auc: 0.9818\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "Validation loss decreased (0.182672 --> 0.172875).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3364, label: 0, bag_size: 26208\n",
      "batch 39, loss: 0.0567, label: 1, bag_size: 2455\n",
      "batch 59, loss: 0.1082, label: 0, bag_size: 24911\n",
      "batch 79, loss: 0.0005, label: 1, bag_size: 18794\n",
      "batch 99, loss: 0.1445, label: 0, bag_size: 5120\n",
      "batch 119, loss: 0.0272, label: 0, bag_size: 15914\n",
      "batch 139, loss: 0.0805, label: 0, bag_size: 3774\n",
      "batch 159, loss: 0.0103, label: 0, bag_size: 11113\n",
      "batch 179, loss: 0.0413, label: 0, bag_size: 3774\n",
      "batch 199, loss: 0.7845, label: 0, bag_size: 24382\n",
      "batch 219, loss: 0.4090, label: 1, bag_size: 11256\n",
      "batch 239, loss: 0.2783, label: 0, bag_size: 18215\n",
      "batch 259, loss: 0.0001, label: 1, bag_size: 9610\n",
      "batch 279, loss: 1.1699, label: 0, bag_size: 1701\n",
      "batch 299, loss: 0.1687, label: 0, bag_size: 2918\n",
      "batch 319, loss: 0.0619, label: 0, bag_size: 11187\n",
      "batch 339, loss: 0.1355, label: 1, bag_size: 1759\n",
      "batch 359, loss: 0.0124, label: 1, bag_size: 9878\n",
      "batch 379, loss: 0.0162, label: 0, bag_size: 32227\n",
      "batch 399, loss: 0.2976, label: 0, bag_size: 2043\n",
      "batch 419, loss: 0.0079, label: 1, bag_size: 9478\n",
      "batch 439, loss: 0.2270, label: 0, bag_size: 3783\n",
      "batch 459, loss: 0.4659, label: 1, bag_size: 10622\n",
      "batch 479, loss: 0.0290, label: 1, bag_size: 3450\n",
      "batch 499, loss: 0.3277, label: 0, bag_size: 11607\n",
      "batch 519, loss: 0.8468, label: 1, bag_size: 6360\n",
      "batch 539, loss: 0.0076, label: 0, bag_size: 14828\n",
      "batch 559, loss: 0.0053, label: 1, bag_size: 2904\n",
      "batch 579, loss: 0.0957, label: 1, bag_size: 13015\n",
      "batch 599, loss: 0.0119, label: 0, bag_size: 11187\n",
      "batch 619, loss: 0.0006, label: 1, bag_size: 10592\n",
      "batch 639, loss: 0.0016, label: 1, bag_size: 6752\n",
      "batch 659, loss: 0.9379, label: 1, bag_size: 2344\n",
      "batch 679, loss: 0.0227, label: 0, bag_size: 10791\n",
      "batch 699, loss: 0.0662, label: 1, bag_size: 7798\n",
      "batch 719, loss: 0.1139, label: 1, bag_size: 29832\n",
      "batch 739, loss: 0.0332, label: 0, bag_size: 11778\n",
      "batch 759, loss: 0.0032, label: 0, bag_size: 9786\n",
      "batch 779, loss: 0.3392, label: 0, bag_size: 25814\n",
      "batch 799, loss: 0.2203, label: 1, bag_size: 12178\n",
      "batch 819, loss: 0.5388, label: 1, bag_size: 1609\n",
      "Epoch: 13, train_loss: 0.2087, train_error: 0.0817\n",
      "class 0: acc 0.9234567901234568, correct 374/405\n",
      "class 1: acc 0.9132530120481928, correct 379/415\n",
      "\n",
      "Val Set, val_loss: 0.1658, val_error: 0.0636, auc: 0.9818\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9137931034482759, correct 53/58\n",
      "Validation loss decreased (0.172875 --> 0.165818).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.9577, label: 0, bag_size: 1732\n",
      "batch 39, loss: 0.0008, label: 1, bag_size: 6317\n",
      "batch 59, loss: 0.2496, label: 1, bag_size: 7768\n",
      "batch 79, loss: 0.0498, label: 1, bag_size: 8680\n",
      "batch 99, loss: 0.0004, label: 1, bag_size: 11389\n",
      "batch 119, loss: 0.0252, label: 0, bag_size: 12201\n",
      "batch 139, loss: 0.0027, label: 1, bag_size: 11884\n",
      "batch 159, loss: 0.0118, label: 0, bag_size: 1884\n",
      "batch 179, loss: 0.0209, label: 0, bag_size: 9888\n",
      "batch 199, loss: 0.0667, label: 1, bag_size: 1867\n",
      "batch 219, loss: 0.0155, label: 0, bag_size: 17791\n",
      "batch 239, loss: 0.3590, label: 0, bag_size: 23996\n",
      "batch 259, loss: 0.0008, label: 1, bag_size: 5833\n",
      "batch 279, loss: 0.0086, label: 0, bag_size: 2548\n",
      "batch 299, loss: 0.4416, label: 1, bag_size: 1822\n",
      "batch 319, loss: 0.0308, label: 0, bag_size: 13880\n",
      "batch 339, loss: 0.0015, label: 0, bag_size: 1962\n",
      "batch 359, loss: 0.0118, label: 1, bag_size: 1014\n",
      "batch 379, loss: 0.0310, label: 0, bag_size: 16720\n",
      "batch 399, loss: 0.0645, label: 1, bag_size: 12626\n",
      "batch 419, loss: 0.0157, label: 0, bag_size: 21138\n",
      "batch 439, loss: 0.0235, label: 0, bag_size: 5120\n",
      "batch 459, loss: 0.0520, label: 1, bag_size: 1015\n",
      "batch 479, loss: 0.2761, label: 0, bag_size: 11212\n",
      "batch 499, loss: 0.0025, label: 1, bag_size: 12931\n",
      "batch 519, loss: 0.2021, label: 0, bag_size: 15898\n",
      "batch 539, loss: 0.4675, label: 1, bag_size: 10622\n",
      "batch 559, loss: 0.2752, label: 0, bag_size: 11607\n",
      "batch 579, loss: 0.8641, label: 0, bag_size: 2070\n",
      "batch 599, loss: 0.0013, label: 0, bag_size: 3459\n",
      "batch 619, loss: 0.0057, label: 0, bag_size: 1909\n",
      "batch 639, loss: 0.0070, label: 0, bag_size: 12687\n",
      "batch 659, loss: 0.0001, label: 1, bag_size: 5221\n",
      "batch 679, loss: 0.0003, label: 1, bag_size: 13368\n",
      "batch 699, loss: 0.6913, label: 1, bag_size: 11964\n",
      "batch 719, loss: 0.0648, label: 0, bag_size: 4418\n",
      "batch 739, loss: 0.0012, label: 0, bag_size: 18154\n",
      "batch 759, loss: 0.1226, label: 0, bag_size: 3321\n",
      "batch 779, loss: 0.0238, label: 0, bag_size: 1831\n",
      "batch 799, loss: 0.1854, label: 1, bag_size: 1244\n",
      "batch 819, loss: 0.0735, label: 0, bag_size: 9866\n",
      "Epoch: 14, train_loss: 0.2398, train_error: 0.0890\n",
      "class 0: acc 0.9196217494089834, correct 389/423\n",
      "class 1: acc 0.9017632241813602, correct 358/397\n",
      "\n",
      "Val Set, val_loss: 0.1881, val_error: 0.0727, auc: 0.9847\n",
      "class 0: acc 0.9807692307692307, correct 51/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0114, label: 1, bag_size: 5991\n",
      "batch 39, loss: 0.0309, label: 0, bag_size: 6624\n",
      "batch 59, loss: 0.0236, label: 1, bag_size: 6731\n",
      "batch 79, loss: 0.0227, label: 1, bag_size: 10498\n",
      "batch 99, loss: 0.0053, label: 1, bag_size: 7110\n",
      "batch 119, loss: 0.8042, label: 1, bag_size: 12714\n",
      "batch 139, loss: 0.0079, label: 0, bag_size: 8025\n",
      "batch 159, loss: 0.0051, label: 1, bag_size: 12349\n",
      "batch 179, loss: 0.3277, label: 1, bag_size: 5907\n",
      "batch 199, loss: 0.1989, label: 1, bag_size: 2682\n",
      "batch 219, loss: 0.0061, label: 1, bag_size: 5991\n",
      "batch 239, loss: 0.0062, label: 0, bag_size: 13225\n",
      "batch 259, loss: 0.0183, label: 0, bag_size: 5225\n",
      "batch 279, loss: 0.0057, label: 0, bag_size: 21082\n",
      "batch 299, loss: 0.0800, label: 0, bag_size: 2652\n",
      "batch 319, loss: 0.0076, label: 1, bag_size: 16267\n",
      "batch 339, loss: 0.2606, label: 1, bag_size: 1064\n",
      "batch 359, loss: 0.0121, label: 0, bag_size: 23368\n",
      "batch 379, loss: 0.0110, label: 1, bag_size: 12460\n",
      "batch 399, loss: 0.0757, label: 0, bag_size: 2998\n",
      "batch 419, loss: 0.1302, label: 1, bag_size: 5903\n",
      "batch 439, loss: 0.0800, label: 0, bag_size: 10898\n",
      "batch 459, loss: 0.0744, label: 1, bag_size: 4786\n",
      "batch 479, loss: 0.1949, label: 1, bag_size: 8680\n",
      "batch 499, loss: 0.0070, label: 0, bag_size: 1452\n",
      "batch 519, loss: 0.0175, label: 1, bag_size: 11642\n",
      "batch 539, loss: 0.0064, label: 0, bag_size: 12212\n",
      "batch 559, loss: 0.0633, label: 0, bag_size: 16211\n",
      "batch 579, loss: 0.0088, label: 0, bag_size: 1438\n",
      "batch 599, loss: 0.0831, label: 1, bag_size: 7119\n",
      "batch 619, loss: 0.9279, label: 0, bag_size: 21361\n",
      "batch 639, loss: 0.0415, label: 1, bag_size: 20333\n",
      "batch 659, loss: 0.3178, label: 0, bag_size: 3552\n",
      "batch 679, loss: 3.0257, label: 1, bag_size: 2314\n",
      "batch 699, loss: 0.0962, label: 1, bag_size: 15609\n",
      "batch 719, loss: 0.0027, label: 1, bag_size: 4039\n",
      "batch 739, loss: 0.0364, label: 0, bag_size: 4523\n",
      "batch 759, loss: 0.3099, label: 0, bag_size: 2959\n",
      "batch 779, loss: 0.9663, label: 1, bag_size: 1919\n",
      "batch 799, loss: 0.2812, label: 0, bag_size: 3228\n",
      "batch 819, loss: 0.0094, label: 0, bag_size: 1483\n",
      "Epoch: 15, train_loss: 0.2619, train_error: 0.1012\n",
      "class 0: acc 0.9114219114219114, correct 391/429\n",
      "class 1: acc 0.8849104859335039, correct 346/391\n",
      "\n",
      "Val Set, val_loss: 0.2033, val_error: 0.0545, auc: 0.9841\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7023, label: 1, bag_size: 1284\n",
      "batch 39, loss: 0.3448, label: 0, bag_size: 11922\n",
      "batch 59, loss: 0.5542, label: 1, bag_size: 2356\n",
      "batch 79, loss: 2.7626, label: 0, bag_size: 2694\n",
      "batch 99, loss: 0.9232, label: 0, bag_size: 2815\n",
      "batch 119, loss: 0.4122, label: 1, bag_size: 1244\n",
      "batch 139, loss: 1.3195, label: 0, bag_size: 12840\n",
      "batch 159, loss: 0.1505, label: 0, bag_size: 15747\n",
      "batch 179, loss: 0.0071, label: 1, bag_size: 12712\n",
      "batch 199, loss: 0.0195, label: 1, bag_size: 14030\n",
      "batch 219, loss: 0.0134, label: 0, bag_size: 9888\n",
      "batch 239, loss: 0.1128, label: 0, bag_size: 2044\n",
      "batch 259, loss: 0.0637, label: 0, bag_size: 18415\n",
      "batch 279, loss: 0.0136, label: 1, bag_size: 5991\n",
      "batch 299, loss: 0.0530, label: 1, bag_size: 13786\n",
      "batch 319, loss: 0.0069, label: 1, bag_size: 4929\n",
      "batch 339, loss: 0.0539, label: 0, bag_size: 2266\n",
      "batch 359, loss: 0.0174, label: 0, bag_size: 14377\n",
      "batch 379, loss: 0.4956, label: 0, bag_size: 4598\n",
      "batch 399, loss: 0.0048, label: 0, bag_size: 17630\n",
      "batch 419, loss: 0.0058, label: 0, bag_size: 3970\n",
      "batch 439, loss: 0.0066, label: 1, bag_size: 9478\n",
      "batch 459, loss: 0.7492, label: 1, bag_size: 12494\n",
      "batch 479, loss: 0.0180, label: 1, bag_size: 13051\n",
      "batch 499, loss: 0.0044, label: 0, bag_size: 10942\n",
      "batch 519, loss: 0.0098, label: 0, bag_size: 18240\n",
      "batch 539, loss: 0.0188, label: 0, bag_size: 5965\n",
      "batch 559, loss: 0.0026, label: 1, bag_size: 7650\n",
      "batch 579, loss: 0.0741, label: 0, bag_size: 1508\n",
      "batch 599, loss: 0.0124, label: 0, bag_size: 12524\n",
      "batch 619, loss: 0.5430, label: 0, bag_size: 1732\n",
      "batch 639, loss: 1.4793, label: 1, bag_size: 1703\n",
      "batch 659, loss: 0.1003, label: 1, bag_size: 12425\n",
      "batch 679, loss: 0.0455, label: 0, bag_size: 14681\n",
      "batch 699, loss: 0.0010, label: 1, bag_size: 11389\n",
      "batch 719, loss: 0.0067, label: 0, bag_size: 2748\n",
      "batch 739, loss: 0.0154, label: 0, bag_size: 5551\n",
      "batch 759, loss: 0.0182, label: 0, bag_size: 9060\n",
      "batch 779, loss: 0.1392, label: 1, bag_size: 13732\n",
      "batch 799, loss: 0.0440, label: 0, bag_size: 9234\n",
      "batch 819, loss: 0.1565, label: 0, bag_size: 15747\n",
      "Epoch: 16, train_loss: 0.2005, train_error: 0.0768\n",
      "class 0: acc 0.9236641221374046, correct 363/393\n",
      "class 1: acc 0.9227166276346604, correct 394/427\n",
      "\n",
      "Val Set, val_loss: 0.1823, val_error: 0.0545, auc: 0.9857\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0062, label: 1, bag_size: 3437\n",
      "batch 39, loss: 0.0358, label: 0, bag_size: 10263\n",
      "batch 59, loss: 0.0065, label: 1, bag_size: 5494\n",
      "batch 79, loss: 0.3956, label: 1, bag_size: 1123\n",
      "batch 99, loss: 0.2773, label: 0, bag_size: 15003\n",
      "batch 119, loss: 0.0001, label: 1, bag_size: 6792\n",
      "batch 139, loss: 0.0030, label: 0, bag_size: 10995\n",
      "batch 159, loss: 0.0009, label: 1, bag_size: 9673\n",
      "batch 179, loss: 0.1490, label: 1, bag_size: 549\n",
      "batch 199, loss: 0.0773, label: 0, bag_size: 1831\n",
      "batch 219, loss: 0.0231, label: 0, bag_size: 22426\n",
      "batch 239, loss: 0.0176, label: 0, bag_size: 10128\n",
      "batch 259, loss: 0.0117, label: 1, bag_size: 6731\n",
      "batch 279, loss: 0.0891, label: 1, bag_size: 9322\n",
      "batch 299, loss: 0.4592, label: 0, bag_size: 24382\n",
      "batch 319, loss: 0.0005, label: 1, bag_size: 9065\n",
      "batch 339, loss: 0.1341, label: 1, bag_size: 7351\n",
      "batch 359, loss: 0.0432, label: 0, bag_size: 9470\n",
      "batch 379, loss: 0.0071, label: 1, bag_size: 1638\n",
      "batch 399, loss: 2.5099, label: 1, bag_size: 1051\n",
      "batch 419, loss: 0.0095, label: 1, bag_size: 5256\n",
      "batch 439, loss: 0.0368, label: 1, bag_size: 15125\n",
      "batch 459, loss: 0.0240, label: 0, bag_size: 23796\n",
      "batch 479, loss: 0.0130, label: 0, bag_size: 13880\n",
      "batch 499, loss: 0.0018, label: 1, bag_size: 14618\n",
      "batch 519, loss: 1.6293, label: 1, bag_size: 3879\n",
      "batch 539, loss: 0.1496, label: 0, bag_size: 21319\n",
      "batch 559, loss: 0.0110, label: 1, bag_size: 8019\n",
      "batch 579, loss: 0.0001, label: 1, bag_size: 9644\n",
      "batch 599, loss: 0.0221, label: 0, bag_size: 4271\n",
      "batch 619, loss: 0.0143, label: 0, bag_size: 4959\n",
      "batch 639, loss: 0.1499, label: 0, bag_size: 4598\n",
      "batch 659, loss: 0.0004, label: 1, bag_size: 9610\n",
      "batch 679, loss: 0.1707, label: 0, bag_size: 2004\n",
      "batch 699, loss: 0.0390, label: 1, bag_size: 18603\n",
      "batch 719, loss: 0.0569, label: 0, bag_size: 10381\n",
      "batch 739, loss: 1.9242, label: 1, bag_size: 3879\n",
      "batch 759, loss: 0.0251, label: 0, bag_size: 6367\n",
      "batch 779, loss: 0.2468, label: 1, bag_size: 13732\n",
      "batch 799, loss: 0.0228, label: 1, bag_size: 6927\n",
      "batch 819, loss: 0.6691, label: 0, bag_size: 1701\n",
      "Epoch: 17, train_loss: 0.2187, train_error: 0.0768\n",
      "class 0: acc 0.9183168316831684, correct 371/404\n",
      "class 1: acc 0.9278846153846154, correct 386/416\n",
      "\n",
      "Val Set, val_loss: 0.1783, val_error: 0.0455, auc: 0.9841\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3399, label: 0, bag_size: 10113\n",
      "batch 39, loss: 0.0085, label: 1, bag_size: 5454\n",
      "batch 59, loss: 0.0021, label: 0, bag_size: 2044\n",
      "batch 79, loss: 0.1354, label: 0, bag_size: 15898\n",
      "batch 99, loss: 0.0709, label: 0, bag_size: 1909\n",
      "batch 119, loss: 0.1057, label: 0, bag_size: 8788\n",
      "batch 139, loss: 0.0791, label: 0, bag_size: 5297\n",
      "batch 159, loss: 0.0048, label: 0, bag_size: 11199\n",
      "batch 179, loss: 0.0204, label: 1, bag_size: 5629\n",
      "batch 199, loss: 0.0103, label: 0, bag_size: 11122\n",
      "batch 219, loss: 0.3028, label: 0, bag_size: 2043\n",
      "batch 239, loss: 0.1215, label: 1, bag_size: 13732\n",
      "batch 259, loss: 0.1296, label: 0, bag_size: 15914\n",
      "batch 279, loss: 1.1827, label: 1, bag_size: 2344\n",
      "batch 299, loss: 0.0005, label: 1, bag_size: 6317\n",
      "batch 319, loss: 2.4237, label: 1, bag_size: 2731\n",
      "batch 339, loss: 0.0133, label: 0, bag_size: 10304\n",
      "batch 359, loss: 0.0292, label: 1, bag_size: 5907\n",
      "batch 379, loss: 0.9250, label: 0, bag_size: 1437\n",
      "batch 399, loss: 0.3724, label: 1, bag_size: 1924\n",
      "batch 419, loss: 0.0173, label: 0, bag_size: 10751\n",
      "batch 439, loss: 0.0386, label: 1, bag_size: 9689\n",
      "batch 459, loss: 0.8004, label: 1, bag_size: 1703\n",
      "batch 479, loss: 0.0084, label: 1, bag_size: 4239\n",
      "batch 499, loss: 4.6946, label: 1, bag_size: 2565\n",
      "batch 519, loss: 0.0000, label: 1, bag_size: 12611\n",
      "batch 539, loss: 0.0488, label: 0, bag_size: 15841\n",
      "batch 559, loss: 0.1513, label: 1, bag_size: 1064\n",
      "batch 579, loss: 0.0031, label: 1, bag_size: 7650\n",
      "batch 599, loss: 0.0015, label: 0, bag_size: 3787\n",
      "batch 619, loss: 0.1294, label: 1, bag_size: 8012\n",
      "batch 639, loss: 0.0152, label: 1, bag_size: 16890\n",
      "batch 659, loss: 0.0986, label: 1, bag_size: 2140\n",
      "batch 679, loss: 0.0011, label: 1, bag_size: 9321\n",
      "batch 699, loss: 0.0716, label: 1, bag_size: 9062\n",
      "batch 719, loss: 0.0015, label: 0, bag_size: 3787\n",
      "batch 739, loss: 0.0132, label: 1, bag_size: 3980\n",
      "batch 759, loss: 0.0037, label: 1, bag_size: 12865\n",
      "batch 779, loss: 0.3015, label: 1, bag_size: 10622\n",
      "batch 799, loss: 0.0199, label: 1, bag_size: 1609\n",
      "batch 819, loss: 0.3182, label: 1, bag_size: 12340\n",
      "Epoch: 18, train_loss: 0.2377, train_error: 0.0878\n",
      "class 0: acc 0.9183673469387755, correct 360/392\n",
      "class 1: acc 0.9065420560747663, correct 388/428\n",
      "\n",
      "Val Set, val_loss: 0.2777, val_error: 0.1545, auc: 0.9831\n",
      "class 0: acc 0.6923076923076923, correct 36/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0474, label: 0, bag_size: 2367\n",
      "batch 39, loss: 0.0178, label: 1, bag_size: 12575\n",
      "batch 59, loss: 0.0019, label: 1, bag_size: 4394\n",
      "batch 79, loss: 0.5211, label: 1, bag_size: 6360\n",
      "batch 99, loss: 0.2736, label: 0, bag_size: 7637\n",
      "batch 119, loss: 0.0053, label: 0, bag_size: 23796\n",
      "batch 139, loss: 0.0002, label: 1, bag_size: 10112\n",
      "batch 159, loss: 0.0017, label: 0, bag_size: 19518\n",
      "batch 179, loss: 1.2121, label: 0, bag_size: 14249\n",
      "batch 199, loss: 0.0041, label: 1, bag_size: 4821\n",
      "batch 219, loss: 0.0107, label: 0, bag_size: 1415\n",
      "batch 239, loss: 2.7377, label: 1, bag_size: 1051\n",
      "batch 259, loss: 0.0150, label: 0, bag_size: 12731\n",
      "batch 279, loss: 1.0665, label: 0, bag_size: 4692\n",
      "batch 299, loss: 0.0182, label: 0, bag_size: 16782\n",
      "batch 319, loss: 0.0001, label: 1, bag_size: 18468\n",
      "batch 339, loss: 0.0005, label: 1, bag_size: 12611\n",
      "batch 359, loss: 1.1820, label: 1, bag_size: 4939\n",
      "batch 379, loss: 0.0043, label: 1, bag_size: 8466\n",
      "batch 399, loss: 0.0066, label: 0, bag_size: 2548\n",
      "batch 419, loss: 0.0056, label: 0, bag_size: 21138\n",
      "batch 439, loss: 0.0011, label: 0, bag_size: 7191\n",
      "batch 459, loss: 0.0013, label: 0, bag_size: 1984\n",
      "batch 479, loss: 4.4370, label: 0, bag_size: 3468\n",
      "batch 499, loss: 0.0003, label: 1, bag_size: 15464\n",
      "batch 519, loss: 0.0642, label: 0, bag_size: 1234\n",
      "batch 539, loss: 0.0047, label: 1, bag_size: 9878\n",
      "batch 559, loss: 0.0285, label: 1, bag_size: 4956\n",
      "batch 579, loss: 0.0371, label: 1, bag_size: 13026\n",
      "batch 599, loss: 0.0352, label: 0, bag_size: 13332\n",
      "batch 619, loss: 0.0388, label: 1, bag_size: 1493\n",
      "batch 639, loss: 0.0022, label: 1, bag_size: 11266\n",
      "batch 659, loss: 0.1996, label: 1, bag_size: 6665\n",
      "batch 679, loss: 0.3817, label: 0, bag_size: 24382\n",
      "batch 699, loss: 0.0156, label: 1, bag_size: 15689\n",
      "batch 719, loss: 1.9393, label: 1, bag_size: 1831\n",
      "batch 739, loss: 0.0077, label: 1, bag_size: 4423\n",
      "batch 759, loss: 0.0012, label: 1, bag_size: 15008\n",
      "batch 779, loss: 0.0041, label: 0, bag_size: 21218\n",
      "batch 799, loss: 0.2408, label: 0, bag_size: 18738\n",
      "batch 819, loss: 0.0215, label: 0, bag_size: 8959\n",
      "Epoch: 19, train_loss: 0.2324, train_error: 0.0878\n",
      "class 0: acc 0.9160671462829736, correct 382/417\n",
      "class 1: acc 0.9081885856079405, correct 366/403\n",
      "\n",
      "Val Set, val_loss: 0.2106, val_error: 0.0818, auc: 0.9828\n",
      "class 0: acc 0.9807692307692307, correct 51/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0069, label: 0, bag_size: 9060\n",
      "batch 39, loss: 0.0503, label: 1, bag_size: 8475\n",
      "batch 59, loss: 0.7659, label: 0, bag_size: 7428\n",
      "batch 79, loss: 0.0055, label: 1, bag_size: 14604\n",
      "batch 99, loss: 0.0498, label: 0, bag_size: 2511\n",
      "batch 119, loss: 0.0011, label: 1, bag_size: 12931\n",
      "batch 139, loss: 1.2553, label: 1, bag_size: 12494\n",
      "batch 159, loss: 0.0096, label: 0, bag_size: 12148\n",
      "batch 179, loss: 0.0096, label: 0, bag_size: 14828\n",
      "batch 199, loss: 0.0014, label: 1, bag_size: 5340\n",
      "batch 219, loss: 0.3101, label: 0, bag_size: 9616\n",
      "batch 239, loss: 0.0005, label: 1, bag_size: 30675\n",
      "batch 259, loss: 1.5877, label: 1, bag_size: 2937\n",
      "batch 279, loss: 0.0119, label: 1, bag_size: 9561\n",
      "batch 299, loss: 0.0352, label: 1, bag_size: 9519\n",
      "batch 319, loss: 0.0016, label: 1, bag_size: 5340\n",
      "batch 339, loss: 0.0059, label: 1, bag_size: 6950\n",
      "batch 359, loss: 0.0275, label: 0, bag_size: 22498\n",
      "batch 379, loss: 0.1008, label: 1, bag_size: 1683\n",
      "batch 399, loss: 0.0676, label: 0, bag_size: 6624\n",
      "batch 419, loss: 0.0886, label: 0, bag_size: 30751\n",
      "batch 439, loss: 0.0928, label: 0, bag_size: 2457\n",
      "batch 459, loss: 0.0340, label: 0, bag_size: 1614\n",
      "batch 479, loss: 0.0011, label: 0, bag_size: 20150\n",
      "batch 499, loss: 0.0034, label: 0, bag_size: 22828\n",
      "batch 519, loss: 0.2412, label: 1, bag_size: 1437\n",
      "batch 539, loss: 0.0008, label: 1, bag_size: 7381\n",
      "batch 559, loss: 0.0196, label: 1, bag_size: 13477\n",
      "batch 579, loss: 0.0006, label: 1, bag_size: 9478\n",
      "batch 599, loss: 0.0929, label: 0, bag_size: 19390\n",
      "batch 619, loss: 0.6758, label: 0, bag_size: 2070\n",
      "batch 639, loss: 0.1148, label: 0, bag_size: 18516\n",
      "batch 659, loss: 0.0449, label: 1, bag_size: 5025\n",
      "batch 679, loss: 0.8834, label: 0, bag_size: 1953\n",
      "batch 699, loss: 0.0112, label: 0, bag_size: 11199\n",
      "batch 719, loss: 0.0108, label: 0, bag_size: 1452\n",
      "batch 739, loss: 0.1168, label: 0, bag_size: 24911\n",
      "batch 759, loss: 0.0040, label: 1, bag_size: 13026\n",
      "batch 779, loss: 0.0212, label: 0, bag_size: 10898\n",
      "batch 799, loss: 0.0045, label: 0, bag_size: 12201\n",
      "batch 819, loss: 0.0095, label: 0, bag_size: 12910\n",
      "Epoch: 20, train_loss: 0.1930, train_error: 0.0695\n",
      "class 0: acc 0.9464285714285714, correct 371/392\n",
      "class 1: acc 0.9158878504672897, correct 392/428\n",
      "\n",
      "Val Set, val_loss: 0.1705, val_error: 0.0455, auc: 0.9838\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3730, label: 1, bag_size: 2179\n",
      "batch 39, loss: 0.0170, label: 1, bag_size: 13051\n",
      "batch 59, loss: 0.2318, label: 1, bag_size: 1683\n",
      "batch 79, loss: 0.2139, label: 1, bag_size: 1823\n",
      "batch 99, loss: 0.0183, label: 0, bag_size: 12796\n",
      "batch 119, loss: 0.0158, label: 0, bag_size: 11900\n",
      "batch 139, loss: 0.0011, label: 1, bag_size: 15213\n",
      "batch 159, loss: 0.0885, label: 1, bag_size: 29832\n",
      "batch 179, loss: 1.0583, label: 0, bag_size: 2219\n",
      "batch 199, loss: 0.8678, label: 0, bag_size: 2815\n",
      "batch 219, loss: 0.0106, label: 0, bag_size: 11727\n",
      "batch 239, loss: 0.1283, label: 0, bag_size: 1789\n",
      "batch 259, loss: 0.0048, label: 1, bag_size: 13026\n",
      "batch 279, loss: 0.0061, label: 1, bag_size: 14230\n",
      "batch 299, loss: 0.0532, label: 1, bag_size: 7768\n",
      "batch 319, loss: 0.0008, label: 0, bag_size: 18154\n",
      "batch 339, loss: 0.0083, label: 0, bag_size: 10898\n",
      "batch 359, loss: 0.0266, label: 0, bag_size: 1814\n",
      "batch 379, loss: 0.0038, label: 0, bag_size: 18415\n",
      "batch 399, loss: 0.0374, label: 0, bag_size: 3160\n",
      "batch 419, loss: 0.0461, label: 1, bag_size: 1888\n",
      "batch 439, loss: 0.0317, label: 0, bag_size: 14625\n",
      "batch 459, loss: 0.0603, label: 0, bag_size: 5965\n",
      "batch 479, loss: 0.0073, label: 0, bag_size: 8661\n",
      "batch 499, loss: 0.0110, label: 1, bag_size: 4956\n",
      "batch 519, loss: 0.4118, label: 0, bag_size: 23996\n",
      "batch 539, loss: 0.0763, label: 0, bag_size: 10029\n",
      "batch 559, loss: 0.0060, label: 0, bag_size: 518\n",
      "batch 579, loss: 0.0283, label: 0, bag_size: 2609\n",
      "batch 599, loss: 0.0424, label: 0, bag_size: 8959\n",
      "batch 619, loss: 0.9868, label: 0, bag_size: 1701\n",
      "batch 639, loss: 0.0030, label: 0, bag_size: 16341\n",
      "batch 659, loss: 0.0020, label: 0, bag_size: 17791\n",
      "batch 679, loss: 0.3398, label: 1, bag_size: 2146\n",
      "batch 699, loss: 0.6905, label: 0, bag_size: 14264\n",
      "batch 719, loss: 0.0007, label: 0, bag_size: 13964\n",
      "batch 739, loss: 0.0782, label: 0, bag_size: 14956\n",
      "batch 759, loss: 0.0277, label: 1, bag_size: 9955\n",
      "batch 779, loss: 0.0028, label: 1, bag_size: 9971\n",
      "batch 799, loss: 0.4562, label: 1, bag_size: 1919\n",
      "batch 819, loss: 0.0109, label: 0, bag_size: 11727\n",
      "Epoch: 21, train_loss: 0.2212, train_error: 0.0805\n",
      "class 0: acc 0.9205955334987593, correct 371/403\n",
      "class 1: acc 0.9184652278177458, correct 383/417\n",
      "\n",
      "Val Set, val_loss: 0.1769, val_error: 0.0364, auc: 0.9844\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0019, label: 0, bag_size: 8948\n",
      "batch 39, loss: 0.0718, label: 0, bag_size: 17155\n",
      "batch 59, loss: 0.2973, label: 0, bag_size: 2266\n",
      "batch 79, loss: 0.2143, label: 0, bag_size: 2336\n",
      "batch 99, loss: 0.0953, label: 0, bag_size: 1213\n",
      "batch 119, loss: 0.0002, label: 1, bag_size: 5221\n",
      "batch 139, loss: 0.0100, label: 0, bag_size: 14681\n",
      "batch 159, loss: 0.0307, label: 0, bag_size: 10365\n",
      "batch 179, loss: 0.0038, label: 0, bag_size: 10263\n",
      "batch 199, loss: 0.0377, label: 0, bag_size: 2244\n",
      "batch 219, loss: 0.0110, label: 1, bag_size: 15093\n",
      "batch 239, loss: 0.5546, label: 0, bag_size: 7612\n",
      "batch 259, loss: 0.0323, label: 1, bag_size: 4239\n",
      "batch 279, loss: 1.1402, label: 0, bag_size: 47866\n",
      "batch 299, loss: 1.7480, label: 0, bag_size: 2815\n",
      "batch 319, loss: 0.0101, label: 1, bag_size: 12712\n",
      "batch 339, loss: 0.0273, label: 1, bag_size: 5155\n",
      "batch 359, loss: 0.6282, label: 0, bag_size: 2213\n",
      "batch 379, loss: 0.0182, label: 0, bag_size: 11527\n",
      "batch 399, loss: 0.0518, label: 0, bag_size: 22870\n",
      "batch 419, loss: 0.0674, label: 0, bag_size: 4845\n",
      "batch 439, loss: 0.2432, label: 1, bag_size: 7989\n",
      "batch 459, loss: 0.0021, label: 1, bag_size: 5561\n",
      "batch 479, loss: 0.0024, label: 0, bag_size: 6851\n",
      "batch 499, loss: 0.0626, label: 0, bag_size: 1824\n",
      "batch 519, loss: 0.1324, label: 0, bag_size: 931\n",
      "batch 539, loss: 0.0012, label: 1, bag_size: 10105\n",
      "batch 559, loss: 0.0011, label: 0, bag_size: 3459\n",
      "batch 579, loss: 0.1607, label: 1, bag_size: 2480\n",
      "batch 599, loss: 0.0039, label: 1, bag_size: 8019\n",
      "batch 619, loss: 0.0258, label: 0, bag_size: 8025\n",
      "batch 639, loss: 0.0973, label: 1, bag_size: 8026\n",
      "batch 659, loss: 0.0018, label: 0, bag_size: 2282\n",
      "batch 679, loss: 0.0020, label: 0, bag_size: 15967\n",
      "batch 699, loss: 0.1763, label: 1, bag_size: 2278\n",
      "batch 719, loss: 0.0080, label: 1, bag_size: 10592\n",
      "batch 739, loss: 2.6294, label: 0, bag_size: 2815\n",
      "batch 759, loss: 0.0021, label: 0, bag_size: 4497\n",
      "batch 779, loss: 0.0156, label: 0, bag_size: 763\n",
      "batch 799, loss: 0.0001, label: 1, bag_size: 10867\n",
      "batch 819, loss: 0.0034, label: 0, bag_size: 2091\n",
      "Epoch: 22, train_loss: 0.1953, train_error: 0.0756\n",
      "class 0: acc 0.9276807980049875, correct 372/401\n",
      "class 1: acc 0.9212410501193318, correct 386/419\n",
      "\n",
      "Val Set, val_loss: 0.1697, val_error: 0.0364, auc: 0.9841\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0005, label: 1, bag_size: 16417\n",
      "batch 39, loss: 0.0071, label: 0, bag_size: 17268\n",
      "batch 59, loss: 0.4028, label: 0, bag_size: 26208\n",
      "batch 79, loss: 0.0258, label: 1, bag_size: 3683\n",
      "batch 99, loss: 0.0499, label: 0, bag_size: 1814\n",
      "batch 119, loss: 0.8764, label: 1, bag_size: 1284\n",
      "batch 139, loss: 0.0102, label: 1, bag_size: 21701\n",
      "batch 159, loss: 0.0121, label: 1, bag_size: 3437\n",
      "batch 179, loss: 0.1851, label: 1, bag_size: 13367\n",
      "batch 199, loss: 0.0605, label: 0, bag_size: 4845\n",
      "batch 219, loss: 0.1260, label: 1, bag_size: 8216\n",
      "batch 239, loss: 0.0166, label: 0, bag_size: 2873\n",
      "batch 259, loss: 0.0012, label: 1, bag_size: 2936\n",
      "batch 279, loss: 0.0053, label: 1, bag_size: 2495\n",
      "batch 299, loss: 2.0981, label: 0, bag_size: 2959\n",
      "batch 319, loss: 0.4441, label: 1, bag_size: 4308\n",
      "batch 339, loss: 0.0134, label: 0, bag_size: 11865\n",
      "batch 359, loss: 0.0094, label: 1, bag_size: 13692\n",
      "batch 379, loss: 0.1351, label: 0, bag_size: 2457\n",
      "batch 399, loss: 0.0027, label: 0, bag_size: 21093\n",
      "batch 419, loss: 0.0187, label: 1, bag_size: 13692\n",
      "batch 439, loss: 0.0871, label: 1, bag_size: 1845\n",
      "batch 459, loss: 0.2649, label: 0, bag_size: 18215\n",
      "batch 479, loss: 0.0032, label: 1, bag_size: 4259\n",
      "batch 499, loss: 0.0068, label: 0, bag_size: 11187\n",
      "batch 519, loss: 0.0918, label: 1, bag_size: 4929\n",
      "batch 539, loss: 0.0060, label: 0, bag_size: 22870\n",
      "batch 559, loss: 0.0633, label: 1, bag_size: 3683\n",
      "batch 579, loss: 0.0127, label: 0, bag_size: 2036\n",
      "batch 599, loss: 0.0782, label: 1, bag_size: 8026\n",
      "batch 619, loss: 0.0149, label: 1, bag_size: 20537\n",
      "batch 639, loss: 1.2443, label: 0, bag_size: 2270\n",
      "batch 659, loss: 0.0010, label: 1, bag_size: 12795\n",
      "batch 679, loss: 0.2312, label: 0, bag_size: 15898\n",
      "batch 699, loss: 0.1133, label: 1, bag_size: 7768\n",
      "batch 719, loss: 0.0021, label: 1, bag_size: 7381\n",
      "batch 739, loss: 0.0007, label: 1, bag_size: 10592\n",
      "batch 759, loss: 0.0614, label: 1, bag_size: 16565\n",
      "batch 779, loss: 0.0107, label: 0, bag_size: 3228\n",
      "batch 799, loss: 0.0014, label: 1, bag_size: 19039\n",
      "batch 819, loss: 0.0131, label: 1, bag_size: 12575\n",
      "Epoch: 23, train_loss: 0.1806, train_error: 0.0695\n",
      "class 0: acc 0.9405940594059405, correct 380/404\n",
      "class 1: acc 0.9206730769230769, correct 383/416\n",
      "\n",
      "Val Set, val_loss: 0.1761, val_error: 0.0727, auc: 0.9861\n",
      "class 0: acc 0.8653846153846154, correct 45/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0050, label: 0, bag_size: 9888\n",
      "batch 39, loss: 0.0080, label: 1, bag_size: 11684\n",
      "batch 59, loss: 0.0537, label: 1, bag_size: 1339\n",
      "batch 79, loss: 0.0003, label: 1, bag_size: 689\n",
      "batch 99, loss: 0.0000, label: 1, bag_size: 9644\n",
      "batch 119, loss: 1.3805, label: 1, bag_size: 1819\n",
      "batch 139, loss: 0.0008, label: 1, bag_size: 12349\n",
      "batch 159, loss: 0.0112, label: 0, bag_size: 13880\n",
      "batch 179, loss: 0.0791, label: 0, bag_size: 9597\n",
      "batch 199, loss: 0.1025, label: 1, bag_size: 8680\n",
      "batch 219, loss: 0.0152, label: 1, bag_size: 11220\n",
      "batch 239, loss: 0.0466, label: 1, bag_size: 9230\n",
      "batch 259, loss: 0.0466, label: 0, bag_size: 14377\n",
      "batch 279, loss: 0.0015, label: 1, bag_size: 10498\n",
      "batch 299, loss: 0.5962, label: 0, bag_size: 4418\n",
      "batch 319, loss: 0.0250, label: 0, bag_size: 4845\n",
      "batch 339, loss: 0.0006, label: 1, bag_size: 20767\n",
      "batch 359, loss: 0.0077, label: 1, bag_size: 9004\n",
      "batch 379, loss: 0.6494, label: 1, bag_size: 6726\n",
      "batch 399, loss: 5.7019, label: 1, bag_size: 3121\n",
      "batch 419, loss: 0.0487, label: 0, bag_size: 2457\n",
      "batch 439, loss: 0.0037, label: 0, bag_size: 16052\n",
      "batch 459, loss: 0.2103, label: 0, bag_size: 2070\n",
      "batch 479, loss: 0.0273, label: 1, bag_size: 12946\n",
      "batch 499, loss: 0.0537, label: 1, bag_size: 11386\n",
      "batch 519, loss: 0.0378, label: 1, bag_size: 865\n",
      "batch 539, loss: 0.0029, label: 1, bag_size: 14223\n",
      "batch 559, loss: 0.0529, label: 0, bag_size: 9596\n",
      "batch 579, loss: 1.2377, label: 0, bag_size: 1732\n",
      "batch 599, loss: 0.0001, label: 1, bag_size: 5317\n",
      "batch 619, loss: 0.3195, label: 0, bag_size: 3444\n",
      "batch 639, loss: 0.0113, label: 0, bag_size: 2748\n",
      "batch 659, loss: 0.0892, label: 1, bag_size: 29832\n",
      "batch 679, loss: 0.0565, label: 0, bag_size: 6850\n",
      "batch 699, loss: 0.2918, label: 0, bag_size: 2609\n",
      "batch 719, loss: 0.0045, label: 1, bag_size: 9971\n",
      "batch 739, loss: 0.0169, label: 0, bag_size: 19466\n",
      "batch 759, loss: 0.6961, label: 0, bag_size: 18777\n",
      "batch 779, loss: 0.0831, label: 0, bag_size: 9060\n",
      "batch 799, loss: 0.0159, label: 1, bag_size: 6736\n",
      "batch 819, loss: 0.0306, label: 1, bag_size: 8438\n",
      "Epoch: 24, train_loss: 0.2245, train_error: 0.0732\n",
      "class 0: acc 0.9312039312039312, correct 379/407\n",
      "class 1: acc 0.9225181598062954, correct 381/413\n",
      "\n",
      "Val Set, val_loss: 0.2121, val_error: 0.0727, auc: 0.9841\n",
      "class 0: acc 0.8653846153846154, correct 45/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0528, label: 1, bag_size: 8475\n",
      "batch 39, loss: 0.0072, label: 1, bag_size: 5256\n",
      "batch 59, loss: 0.0389, label: 0, bag_size: 3783\n",
      "batch 79, loss: 0.6608, label: 1, bag_size: 1230\n",
      "batch 99, loss: 0.0001, label: 1, bag_size: 9644\n",
      "batch 119, loss: 0.0019, label: 1, bag_size: 5991\n",
      "batch 139, loss: 0.1378, label: 1, bag_size: 12425\n",
      "batch 159, loss: 0.0054, label: 0, bag_size: 518\n",
      "batch 179, loss: 1.2281, label: 1, bag_size: 2842\n",
      "batch 199, loss: 0.0295, label: 0, bag_size: 10068\n",
      "batch 219, loss: 0.0213, label: 1, bag_size: 13026\n",
      "batch 239, loss: 0.0078, label: 0, bag_size: 3657\n",
      "batch 259, loss: 0.0040, label: 1, bag_size: 8040\n",
      "batch 279, loss: 0.0019, label: 1, bag_size: 4039\n",
      "batch 299, loss: 0.0120, label: 1, bag_size: 13692\n",
      "batch 319, loss: 0.0258, label: 0, bag_size: 7011\n",
      "batch 339, loss: 0.0100, label: 0, bag_size: 15636\n",
      "batch 359, loss: 0.1615, label: 0, bag_size: 8744\n",
      "batch 379, loss: 0.2703, label: 1, bag_size: 1924\n",
      "batch 399, loss: 0.5438, label: 1, bag_size: 2842\n",
      "batch 419, loss: 0.0133, label: 0, bag_size: 4902\n",
      "batch 439, loss: 0.0702, label: 1, bag_size: 3082\n",
      "batch 459, loss: 0.1499, label: 0, bag_size: 2043\n",
      "batch 479, loss: 0.0032, label: 0, bag_size: 20150\n",
      "batch 499, loss: 0.0170, label: 1, bag_size: 1920\n",
      "batch 519, loss: 0.1075, label: 1, bag_size: 3211\n",
      "batch 539, loss: 0.1188, label: 1, bag_size: 4054\n",
      "batch 559, loss: 0.2030, label: 1, bag_size: 7989\n",
      "batch 579, loss: 0.1594, label: 0, bag_size: 16087\n",
      "batch 599, loss: 0.0006, label: 1, bag_size: 14223\n",
      "batch 619, loss: 0.0016, label: 1, bag_size: 6453\n",
      "batch 639, loss: 0.0594, label: 1, bag_size: 1823\n",
      "batch 659, loss: 1.2670, label: 1, bag_size: 771\n",
      "batch 679, loss: 0.0001, label: 1, bag_size: 3295\n",
      "batch 699, loss: 0.0125, label: 0, bag_size: 15071\n",
      "batch 719, loss: 0.0099, label: 1, bag_size: 19606\n",
      "batch 739, loss: 0.2201, label: 0, bag_size: 2652\n",
      "batch 759, loss: 0.0044, label: 1, bag_size: 22264\n",
      "batch 779, loss: 0.0052, label: 1, bag_size: 11316\n",
      "batch 799, loss: 0.0202, label: 0, bag_size: 2534\n",
      "batch 819, loss: 0.0024, label: 0, bag_size: 9455\n",
      "Epoch: 25, train_loss: 0.1925, train_error: 0.0720\n",
      "class 0: acc 0.9324009324009324, correct 400/429\n",
      "class 1: acc 0.9232736572890026, correct 361/391\n",
      "\n",
      "Val Set, val_loss: 0.1941, val_error: 0.0909, auc: 0.9834\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "EarlyStopping counter: 12 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4402, label: 1, bag_size: 5903\n",
      "batch 39, loss: 0.0060, label: 0, bag_size: 13777\n",
      "batch 59, loss: 0.0058, label: 0, bag_size: 11187\n",
      "batch 79, loss: 0.0024, label: 1, bag_size: 7381\n",
      "batch 99, loss: 0.0037, label: 0, bag_size: 9851\n",
      "batch 119, loss: 0.0023, label: 0, bag_size: 16052\n",
      "batch 139, loss: 0.0079, label: 1, bag_size: 3437\n",
      "batch 159, loss: 0.0297, label: 0, bag_size: 1814\n",
      "batch 179, loss: 0.1062, label: 0, bag_size: 931\n",
      "batch 199, loss: 0.8179, label: 0, bag_size: 2815\n",
      "batch 219, loss: 0.0019, label: 0, bag_size: 1415\n",
      "batch 239, loss: 0.0048, label: 1, bag_size: 1459\n",
      "batch 259, loss: 0.3882, label: 1, bag_size: 12340\n",
      "batch 279, loss: 0.0205, label: 1, bag_size: 9519\n",
      "batch 299, loss: 0.0099, label: 0, bag_size: 6851\n",
      "batch 319, loss: 0.1937, label: 0, bag_size: 1684\n",
      "batch 339, loss: 0.0014, label: 1, bag_size: 16417\n",
      "batch 359, loss: 0.0034, label: 0, bag_size: 21404\n",
      "batch 379, loss: 0.0137, label: 1, bag_size: 14202\n",
      "batch 399, loss: 0.0490, label: 0, bag_size: 1881\n",
      "batch 419, loss: 0.0217, label: 1, bag_size: 9470\n",
      "batch 439, loss: 0.0018, label: 0, bag_size: 2006\n",
      "batch 459, loss: 0.0098, label: 0, bag_size: 21082\n",
      "batch 479, loss: 0.0030, label: 0, bag_size: 19518\n",
      "batch 499, loss: 0.0017, label: 0, bag_size: 1052\n",
      "batch 519, loss: 0.0147, label: 0, bag_size: 11146\n",
      "batch 539, loss: 0.0686, label: 0, bag_size: 1438\n",
      "batch 559, loss: 0.0038, label: 1, bag_size: 11642\n",
      "batch 579, loss: 0.0008, label: 1, bag_size: 15716\n",
      "batch 599, loss: 0.0184, label: 0, bag_size: 2748\n",
      "batch 619, loss: 0.0313, label: 1, bag_size: 5025\n",
      "batch 639, loss: 0.0105, label: 1, bag_size: 1493\n",
      "batch 659, loss: 0.0006, label: 0, bag_size: 2628\n",
      "batch 679, loss: 0.0189, label: 1, bag_size: 1244\n",
      "batch 699, loss: 0.6105, label: 1, bag_size: 8868\n",
      "batch 719, loss: 0.0631, label: 1, bag_size: 7424\n",
      "batch 739, loss: 0.0706, label: 0, bag_size: 10365\n",
      "batch 759, loss: 0.0004, label: 1, bag_size: 7873\n",
      "batch 779, loss: 0.0051, label: 1, bag_size: 1622\n",
      "batch 799, loss: 0.0005, label: 0, bag_size: 9433\n",
      "batch 819, loss: 0.0870, label: 1, bag_size: 8660\n",
      "Epoch: 26, train_loss: 0.1763, train_error: 0.0695\n",
      "class 0: acc 0.9465116279069767, correct 407/430\n",
      "class 1: acc 0.9128205128205128, correct 356/390\n",
      "\n",
      "Val Set, val_loss: 0.2244, val_error: 0.0909, auc: 0.9801\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 13 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0901, label: 0, bag_size: 8959\n",
      "batch 39, loss: 0.1604, label: 1, bag_size: 22286\n",
      "batch 59, loss: 0.0524, label: 1, bag_size: 3980\n",
      "batch 79, loss: 0.4552, label: 0, bag_size: 10410\n",
      "batch 99, loss: 1.4797, label: 1, bag_size: 6726\n",
      "batch 119, loss: 0.0142, label: 0, bag_size: 4465\n",
      "batch 139, loss: 0.7495, label: 0, bag_size: 3654\n",
      "batch 159, loss: 0.0233, label: 0, bag_size: 13880\n",
      "batch 179, loss: 0.0087, label: 0, bag_size: 12510\n",
      "batch 199, loss: 0.0125, label: 0, bag_size: 10128\n",
      "batch 219, loss: 0.0029, label: 1, bag_size: 2278\n",
      "batch 239, loss: 0.0286, label: 0, bag_size: 5225\n",
      "batch 259, loss: 0.1927, label: 0, bag_size: 13339\n",
      "batch 279, loss: 0.1446, label: 0, bag_size: 9132\n",
      "batch 299, loss: 0.0031, label: 0, bag_size: 20796\n",
      "batch 319, loss: 0.0162, label: 1, bag_size: 21827\n",
      "batch 339, loss: 0.0010, label: 1, bag_size: 9065\n",
      "batch 359, loss: 0.0546, label: 0, bag_size: 15071\n",
      "batch 379, loss: 0.2153, label: 0, bag_size: 1831\n",
      "batch 399, loss: 0.4475, label: 0, bag_size: 10029\n",
      "batch 419, loss: 0.3226, label: 0, bag_size: 2219\n",
      "batch 439, loss: 0.0006, label: 1, bag_size: 2904\n",
      "batch 459, loss: 0.0161, label: 0, bag_size: 32227\n",
      "batch 479, loss: 0.0402, label: 0, bag_size: 3552\n",
      "batch 499, loss: 0.0060, label: 0, bag_size: 11727\n",
      "batch 519, loss: 0.0029, label: 0, bag_size: 8981\n",
      "batch 539, loss: 0.1687, label: 1, bag_size: 5516\n",
      "batch 559, loss: 0.0433, label: 0, bag_size: 1349\n",
      "batch 579, loss: 0.1078, label: 0, bag_size: 11306\n",
      "batch 599, loss: 0.0085, label: 1, bag_size: 18095\n",
      "batch 619, loss: 0.0002, label: 1, bag_size: 6875\n",
      "batch 639, loss: 0.2915, label: 0, bag_size: 9485\n",
      "batch 659, loss: 0.0016, label: 1, bag_size: 15213\n",
      "batch 679, loss: 0.4128, label: 0, bag_size: 3557\n",
      "batch 699, loss: 0.0030, label: 0, bag_size: 13777\n",
      "batch 719, loss: 0.4574, label: 0, bag_size: 18738\n",
      "batch 739, loss: 0.1203, label: 1, bag_size: 7981\n",
      "batch 759, loss: 3.3871, label: 0, bag_size: 3897\n",
      "batch 779, loss: 0.0001, label: 1, bag_size: 9644\n",
      "batch 799, loss: 0.0051, label: 1, bag_size: 1459\n",
      "batch 819, loss: 0.0462, label: 0, bag_size: 2244\n",
      "Epoch: 27, train_loss: 0.1949, train_error: 0.0744\n",
      "class 0: acc 0.9315403422982885, correct 381/409\n",
      "class 1: acc 0.9197080291970803, correct 378/411\n",
      "\n",
      "Val Set, val_loss: 0.2401, val_error: 0.0818, auc: 0.9788\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 14 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0271, label: 1, bag_size: 10072\n",
      "batch 39, loss: 0.0352, label: 0, bag_size: 1498\n",
      "batch 59, loss: 0.0014, label: 0, bag_size: 8252\n",
      "batch 79, loss: 0.0115, label: 1, bag_size: 20333\n",
      "batch 99, loss: 0.0143, label: 0, bag_size: 8145\n",
      "batch 119, loss: 0.0094, label: 0, bag_size: 2004\n",
      "batch 139, loss: 0.2036, label: 0, bag_size: 13619\n",
      "batch 159, loss: 1.9017, label: 1, bag_size: 2344\n",
      "batch 179, loss: 0.0472, label: 1, bag_size: 9519\n",
      "batch 199, loss: 0.0506, label: 0, bag_size: 1831\n",
      "batch 219, loss: 0.0104, label: 0, bag_size: 10751\n",
      "batch 239, loss: 0.0007, label: 1, bag_size: 2136\n",
      "batch 259, loss: 0.0210, label: 0, bag_size: 22800\n",
      "batch 279, loss: 0.0267, label: 0, bag_size: 17633\n",
      "batch 299, loss: 0.1361, label: 1, bag_size: 2785\n",
      "batch 319, loss: 0.0124, label: 0, bag_size: 16607\n",
      "batch 339, loss: 0.3547, label: 1, bag_size: 4308\n",
      "batch 359, loss: 0.0417, label: 0, bag_size: 14377\n",
      "batch 379, loss: 0.5273, label: 0, bag_size: 6850\n",
      "batch 399, loss: 0.0007, label: 1, bag_size: 15213\n",
      "batch 419, loss: 0.0186, label: 0, bag_size: 2511\n",
      "batch 439, loss: 0.0461, label: 1, bag_size: 9942\n",
      "batch 459, loss: 0.0006, label: 1, bag_size: 10394\n",
      "batch 479, loss: 0.0036, label: 0, bag_size: 2091\n",
      "batch 499, loss: 0.0000, label: 1, bag_size: 18468\n",
      "batch 519, loss: 0.0294, label: 1, bag_size: 10033\n",
      "batch 539, loss: 0.0795, label: 1, bag_size: 2662\n",
      "batch 559, loss: 0.0069, label: 1, bag_size: 13477\n",
      "batch 579, loss: 0.0026, label: 1, bag_size: 11316\n",
      "batch 599, loss: 0.0003, label: 1, bag_size: 7767\n",
      "batch 619, loss: 0.0012, label: 0, bag_size: 18154\n",
      "batch 639, loss: 0.2505, label: 0, bag_size: 9252\n",
      "batch 659, loss: 0.0011, label: 1, bag_size: 11266\n",
      "batch 679, loss: 0.0014, label: 1, bag_size: 11875\n",
      "batch 699, loss: 0.0039, label: 1, bag_size: 10281\n",
      "batch 719, loss: 0.0038, label: 0, bag_size: 2006\n",
      "batch 739, loss: 0.1001, label: 1, bag_size: 8592\n",
      "batch 759, loss: 0.0062, label: 1, bag_size: 10498\n",
      "batch 779, loss: 0.7612, label: 1, bag_size: 1609\n",
      "batch 799, loss: 0.0353, label: 0, bag_size: 4523\n",
      "batch 819, loss: 0.0029, label: 1, bag_size: 2136\n",
      "Epoch: 28, train_loss: 0.1866, train_error: 0.0561\n",
      "class 0: acc 0.9424657534246575, correct 344/365\n",
      "class 1: acc 0.945054945054945, correct 430/455\n",
      "\n",
      "Val Set, val_loss: 0.2066, val_error: 0.0727, auc: 0.9814\n",
      "class 0: acc 0.8653846153846154, correct 45/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 15 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0177, label: 1, bag_size: 1525\n",
      "batch 39, loss: 0.0905, label: 0, bag_size: 6884\n",
      "batch 59, loss: 0.0001, label: 1, bag_size: 18468\n",
      "batch 79, loss: 0.0068, label: 0, bag_size: 9060\n",
      "batch 99, loss: 1.8060, label: 1, bag_size: 15563\n",
      "batch 119, loss: 0.7577, label: 0, bag_size: 8788\n",
      "batch 139, loss: 0.0010, label: 1, bag_size: 16512\n",
      "batch 159, loss: 0.0072, label: 0, bag_size: 10791\n",
      "batch 179, loss: 0.0921, label: 1, bag_size: 6927\n",
      "batch 199, loss: 0.0004, label: 1, bag_size: 15665\n",
      "batch 219, loss: 0.0571, label: 1, bag_size: 3082\n",
      "batch 239, loss: 0.0913, label: 0, bag_size: 5639\n",
      "batch 259, loss: 0.1048, label: 1, bag_size: 8026\n",
      "batch 279, loss: 0.0124, label: 1, bag_size: 8448\n",
      "batch 299, loss: 0.0000, label: 1, bag_size: 12931\n",
      "batch 319, loss: 0.0002, label: 1, bag_size: 20767\n",
      "batch 339, loss: 0.0030, label: 1, bag_size: 9732\n",
      "batch 359, loss: 0.0413, label: 1, bag_size: 13015\n",
      "batch 379, loss: 0.0459, label: 0, bag_size: 23996\n",
      "batch 399, loss: 0.0038, label: 1, bag_size: 1244\n",
      "batch 419, loss: 0.0160, label: 1, bag_size: 6745\n",
      "batch 439, loss: 0.0799, label: 1, bag_size: 9519\n",
      "batch 459, loss: 0.2492, label: 1, bag_size: 2179\n",
      "batch 479, loss: 0.0053, label: 0, bag_size: 763\n",
      "batch 499, loss: 0.0014, label: 1, bag_size: 20161\n",
      "batch 519, loss: 0.0159, label: 1, bag_size: 6731\n",
      "batch 539, loss: 0.0050, label: 0, bag_size: 12687\n",
      "batch 559, loss: 0.0001, label: 1, bag_size: 10920\n",
      "batch 579, loss: 0.0089, label: 0, bag_size: 14333\n",
      "batch 599, loss: 0.0001, label: 1, bag_size: 8410\n",
      "batch 619, loss: 0.1788, label: 0, bag_size: 11128\n",
      "batch 639, loss: 0.0040, label: 1, bag_size: 2412\n",
      "batch 659, loss: 0.0669, label: 1, bag_size: 7798\n",
      "batch 679, loss: 0.4239, label: 1, bag_size: 1512\n",
      "batch 699, loss: 0.0600, label: 0, bag_size: 9597\n",
      "batch 719, loss: 0.0115, label: 0, bag_size: 1891\n",
      "batch 739, loss: 0.0072, label: 1, bag_size: 5454\n",
      "batch 759, loss: 0.0043, label: 0, bag_size: 9455\n",
      "batch 779, loss: 0.6695, label: 0, bag_size: 47866\n",
      "batch 799, loss: 0.0095, label: 0, bag_size: 21138\n",
      "batch 819, loss: 0.0026, label: 1, bag_size: 11518\n",
      "Epoch: 29, train_loss: 0.1501, train_error: 0.0512\n",
      "class 0: acc 0.9533169533169533, correct 388/407\n",
      "class 1: acc 0.9443099273607748, correct 390/413\n",
      "\n",
      "Val Set, val_loss: 0.2326, val_error: 0.1091, auc: 0.9834\n",
      "class 0: acc 0.7884615384615384, correct 41/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 16 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0144, label: 0, bag_size: 10415\n",
      "batch 39, loss: 0.0238, label: 1, bag_size: 12719\n",
      "batch 59, loss: 0.0141, label: 0, bag_size: 7557\n",
      "batch 79, loss: 0.1399, label: 0, bag_size: 18215\n",
      "batch 99, loss: 0.0001, label: 1, bag_size: 5731\n",
      "batch 119, loss: 0.1788, label: 0, bag_size: 3541\n",
      "batch 139, loss: 0.8633, label: 0, bag_size: 4692\n",
      "batch 159, loss: 0.0002, label: 1, bag_size: 4862\n",
      "batch 179, loss: 0.1281, label: 1, bag_size: 11256\n",
      "batch 199, loss: 0.0240, label: 1, bag_size: 9955\n",
      "batch 219, loss: 0.0417, label: 1, bag_size: 3453\n",
      "batch 239, loss: 0.0268, label: 0, bag_size: 11259\n",
      "batch 259, loss: 1.3077, label: 0, bag_size: 3654\n",
      "batch 279, loss: 0.3635, label: 1, bag_size: 5160\n",
      "batch 299, loss: 0.1101, label: 0, bag_size: 5120\n",
      "batch 319, loss: 0.0886, label: 1, bag_size: 2681\n",
      "batch 339, loss: 0.0057, label: 0, bag_size: 10942\n",
      "batch 359, loss: 0.0999, label: 0, bag_size: 7428\n",
      "batch 379, loss: 0.0005, label: 0, bag_size: 9433\n",
      "batch 399, loss: 0.0715, label: 0, bag_size: 2654\n",
      "batch 419, loss: 0.0876, label: 1, bag_size: 5025\n",
      "batch 439, loss: 0.0029, label: 1, bag_size: 11518\n",
      "batch 459, loss: 0.0339, label: 0, bag_size: 21864\n",
      "batch 479, loss: 0.0718, label: 1, bag_size: 2140\n",
      "batch 499, loss: 0.2841, label: 0, bag_size: 2624\n",
      "batch 519, loss: 0.0163, label: 0, bag_size: 17791\n",
      "batch 539, loss: 0.0499, label: 0, bag_size: 4465\n",
      "batch 559, loss: 0.0001, label: 1, bag_size: 5221\n",
      "batch 579, loss: 0.0289, label: 0, bag_size: 7011\n",
      "batch 599, loss: 0.0357, label: 0, bag_size: 12796\n",
      "batch 619, loss: 0.0113, label: 1, bag_size: 1493\n",
      "batch 639, loss: 0.0037, label: 1, bag_size: 2695\n",
      "batch 659, loss: 0.0082, label: 0, bag_size: 19043\n",
      "batch 679, loss: 0.0049, label: 0, bag_size: 3160\n",
      "batch 699, loss: 0.1000, label: 1, bag_size: 2146\n",
      "batch 719, loss: 0.0239, label: 0, bag_size: 2006\n",
      "batch 739, loss: 0.0720, label: 1, bag_size: 1244\n",
      "batch 759, loss: 0.0151, label: 0, bag_size: 1508\n",
      "batch 779, loss: 0.0006, label: 1, bag_size: 4394\n",
      "batch 799, loss: 0.0637, label: 0, bag_size: 15672\n",
      "batch 819, loss: 0.3653, label: 1, bag_size: 2395\n",
      "Epoch: 30, train_loss: 0.1855, train_error: 0.0634\n",
      "class 0: acc 0.9356435643564357, correct 378/404\n",
      "class 1: acc 0.9375, correct 390/416\n",
      "\n",
      "Val Set, val_loss: 0.1922, val_error: 0.0727, auc: 0.9804\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 17 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0589, label: 1, bag_size: 7371\n",
      "batch 39, loss: 0.0017, label: 1, bag_size: 15332\n",
      "batch 59, loss: 0.0105, label: 0, bag_size: 3670\n",
      "batch 79, loss: 0.5066, label: 1, bag_size: 15185\n",
      "batch 99, loss: 0.0002, label: 1, bag_size: 3409\n",
      "batch 119, loss: 0.0000, label: 1, bag_size: 11389\n",
      "batch 139, loss: 0.0003, label: 1, bag_size: 6343\n",
      "batch 159, loss: 0.0175, label: 1, bag_size: 3652\n",
      "batch 179, loss: 0.0616, label: 1, bag_size: 11032\n",
      "batch 199, loss: 0.3389, label: 0, bag_size: 9949\n",
      "batch 219, loss: 0.9129, label: 0, bag_size: 2098\n",
      "batch 239, loss: 0.0817, label: 0, bag_size: 3774\n",
      "batch 259, loss: 1.6624, label: 1, bag_size: 2759\n",
      "batch 279, loss: 0.0001, label: 1, bag_size: 9321\n",
      "batch 299, loss: 1.3456, label: 1, bag_size: 771\n",
      "batch 319, loss: 0.0962, label: 0, bag_size: 12910\n",
      "batch 339, loss: 0.0002, label: 1, bag_size: 6343\n",
      "batch 359, loss: 0.5266, label: 1, bag_size: 5231\n",
      "batch 379, loss: 0.0564, label: 1, bag_size: 5723\n",
      "batch 399, loss: 0.2090, label: 0, bag_size: 18738\n",
      "batch 419, loss: 0.0062, label: 0, bag_size: 11917\n",
      "batch 439, loss: 0.0070, label: 0, bag_size: 1891\n",
      "batch 459, loss: 0.0140, label: 1, bag_size: 12946\n",
      "batch 479, loss: 0.0068, label: 0, bag_size: 10751\n",
      "batch 499, loss: 0.0005, label: 1, bag_size: 5561\n",
      "batch 519, loss: 0.0279, label: 0, bag_size: 3552\n",
      "batch 539, loss: 0.0001, label: 0, bag_size: 2820\n",
      "batch 559, loss: 0.0003, label: 0, bag_size: 10535\n",
      "batch 579, loss: 0.0133, label: 0, bag_size: 1052\n",
      "batch 599, loss: 0.0038, label: 1, bag_size: 11684\n",
      "batch 619, loss: 0.0025, label: 1, bag_size: 1255\n",
      "batch 639, loss: 0.0465, label: 0, bag_size: 15914\n",
      "batch 659, loss: 0.0063, label: 1, bag_size: 9478\n",
      "batch 679, loss: 0.1541, label: 0, bag_size: 13332\n",
      "batch 699, loss: 0.0111, label: 0, bag_size: 2296\n",
      "batch 719, loss: 0.0023, label: 0, bag_size: 9851\n",
      "batch 739, loss: 0.0013, label: 0, bag_size: 1560\n",
      "batch 759, loss: 0.0088, label: 1, bag_size: 3651\n",
      "batch 779, loss: 0.0008, label: 1, bag_size: 19606\n",
      "batch 799, loss: 0.0039, label: 1, bag_size: 20333\n",
      "batch 819, loss: 0.0043, label: 1, bag_size: 7513\n",
      "Epoch: 31, train_loss: 0.2059, train_error: 0.0780\n",
      "class 0: acc 0.9225, correct 369/400\n",
      "class 1: acc 0.9214285714285714, correct 387/420\n",
      "\n",
      "Val Set, val_loss: 0.1864, val_error: 0.0636, auc: 0.9814\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "EarlyStopping counter: 18 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0146, label: 1, bag_size: 1022\n",
      "batch 39, loss: 0.0022, label: 1, bag_size: 4877\n",
      "batch 59, loss: 0.0167, label: 0, bag_size: 23368\n",
      "batch 79, loss: 3.4316, label: 1, bag_size: 9162\n",
      "batch 99, loss: 0.0912, label: 0, bag_size: 3089\n",
      "batch 119, loss: 0.0976, label: 0, bag_size: 11865\n",
      "batch 139, loss: 0.1950, label: 0, bag_size: 6281\n",
      "batch 159, loss: 0.0119, label: 0, bag_size: 22800\n",
      "batch 179, loss: 0.0208, label: 0, bag_size: 13880\n",
      "batch 199, loss: 0.0103, label: 0, bag_size: 1370\n",
      "batch 219, loss: 0.3245, label: 0, bag_size: 10415\n",
      "batch 239, loss: 0.0019, label: 1, bag_size: 9147\n",
      "batch 259, loss: 0.0151, label: 0, bag_size: 6884\n",
      "batch 279, loss: 0.7670, label: 1, bag_size: 8191\n",
      "batch 299, loss: 0.2897, label: 0, bag_size: 25814\n",
      "batch 319, loss: 0.1071, label: 1, bag_size: 5231\n",
      "batch 339, loss: 0.1013, label: 0, bag_size: 4271\n",
      "batch 359, loss: 0.4437, label: 0, bag_size: 11128\n",
      "batch 379, loss: 0.1690, label: 1, bag_size: 11394\n",
      "batch 399, loss: 0.0003, label: 1, bag_size: 13365\n",
      "batch 419, loss: 0.0309, label: 0, bag_size: 22870\n",
      "batch 439, loss: 0.0015, label: 0, bag_size: 1416\n",
      "batch 459, loss: 0.0004, label: 0, bag_size: 2820\n",
      "batch 479, loss: 0.6954, label: 0, bag_size: 3802\n",
      "batch 499, loss: 0.0046, label: 0, bag_size: 12201\n",
      "batch 519, loss: 0.2341, label: 1, bag_size: 4789\n",
      "batch 539, loss: 0.0356, label: 0, bag_size: 6898\n",
      "batch 559, loss: 1.8662, label: 0, bag_size: 65728\n",
      "batch 579, loss: 0.0040, label: 1, bag_size: 13440\n",
      "batch 599, loss: 0.0001, label: 1, bag_size: 6875\n",
      "batch 619, loss: 0.0001, label: 1, bag_size: 6792\n",
      "batch 639, loss: 0.3772, label: 0, bag_size: 6850\n",
      "batch 659, loss: 0.1081, label: 1, bag_size: 8592\n",
      "batch 679, loss: 0.0030, label: 1, bag_size: 12575\n",
      "batch 699, loss: 0.0145, label: 1, bag_size: 5605\n",
      "batch 719, loss: 0.3024, label: 1, bag_size: 13786\n",
      "batch 739, loss: 0.0007, label: 1, bag_size: 10105\n",
      "batch 759, loss: 0.0089, label: 1, bag_size: 13015\n",
      "batch 779, loss: 0.0033, label: 0, bag_size: 13892\n",
      "batch 799, loss: 0.1257, label: 0, bag_size: 1701\n",
      "batch 819, loss: 0.3462, label: 1, bag_size: 3211\n",
      "Epoch: 32, train_loss: 0.1556, train_error: 0.0537\n",
      "class 0: acc 0.9460093896713615, correct 403/426\n",
      "class 1: acc 0.9467005076142132, correct 373/394\n",
      "\n",
      "Val Set, val_loss: 0.2044, val_error: 0.0909, auc: 0.9791\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "EarlyStopping counter: 19 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0063, label: 1, bag_size: 21827\n",
      "batch 39, loss: 0.0001, label: 1, bag_size: 8410\n",
      "batch 59, loss: 0.7680, label: 1, bag_size: 15185\n",
      "batch 79, loss: 0.0108, label: 0, bag_size: 22800\n",
      "batch 99, loss: 0.0073, label: 0, bag_size: 1814\n",
      "batch 119, loss: 0.2307, label: 0, bag_size: 2219\n",
      "batch 139, loss: 0.0008, label: 1, bag_size: 13947\n",
      "batch 159, loss: 0.0064, label: 1, bag_size: 7669\n",
      "batch 179, loss: 0.0216, label: 1, bag_size: 10492\n",
      "batch 199, loss: 0.0144, label: 0, bag_size: 1438\n",
      "batch 219, loss: 0.2132, label: 0, bag_size: 20555\n",
      "batch 239, loss: 0.0137, label: 0, bag_size: 11865\n",
      "batch 259, loss: 0.2196, label: 1, bag_size: 1845\n",
      "batch 279, loss: 0.0001, label: 1, bag_size: 9065\n",
      "batch 299, loss: 0.0488, label: 0, bag_size: 2104\n",
      "batch 319, loss: 0.6645, label: 1, bag_size: 13732\n",
      "batch 339, loss: 0.0000, label: 1, bag_size: 4862\n",
      "batch 359, loss: 0.3194, label: 0, bag_size: 12840\n",
      "batch 379, loss: 0.0034, label: 1, bag_size: 10498\n",
      "batch 399, loss: 1.0702, label: 1, bag_size: 1284\n",
      "batch 419, loss: 0.0246, label: 1, bag_size: 2356\n",
      "batch 439, loss: 0.0002, label: 1, bag_size: 9571\n",
      "batch 459, loss: 0.1973, label: 1, bag_size: 7981\n",
      "batch 479, loss: 0.0013, label: 0, bag_size: 1760\n",
      "batch 499, loss: 0.0600, label: 0, bag_size: 5120\n",
      "batch 519, loss: 0.0849, label: 1, bag_size: 8216\n",
      "batch 539, loss: 0.2392, label: 1, bag_size: 7246\n",
      "batch 559, loss: 0.0000, label: 1, bag_size: 14515\n",
      "batch 579, loss: 0.1450, label: 1, bag_size: 16154\n",
      "batch 599, loss: 0.0075, label: 1, bag_size: 11421\n",
      "batch 619, loss: 1.7490, label: 1, bag_size: 2759\n",
      "batch 639, loss: 0.0757, label: 0, bag_size: 3657\n",
      "batch 659, loss: 0.1067, label: 1, bag_size: 4250\n",
      "batch 679, loss: 0.0386, label: 1, bag_size: 15689\n",
      "batch 699, loss: 0.0239, label: 1, bag_size: 14779\n",
      "batch 719, loss: 2.4614, label: 1, bag_size: 1497\n",
      "batch 739, loss: 0.0031, label: 0, bag_size: 13225\n",
      "batch 759, loss: 0.0317, label: 0, bag_size: 2036\n",
      "batch 779, loss: 0.0040, label: 1, bag_size: 14681\n",
      "batch 799, loss: 0.0313, label: 0, bag_size: 2732\n",
      "batch 819, loss: 0.0336, label: 1, bag_size: 5864\n",
      "Epoch: 33, train_loss: 0.1847, train_error: 0.0720\n",
      "class 0: acc 0.9310344827586207, correct 378/406\n",
      "class 1: acc 0.9251207729468599, correct 383/414\n",
      "\n",
      "Val Set, val_loss: 0.1809, val_error: 0.0636, auc: 0.9824\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "EarlyStopping counter: 20 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1253, label: 1, bag_size: 1920\n",
      "batch 39, loss: 0.1074, label: 0, bag_size: 3541\n",
      "batch 59, loss: 0.0456, label: 1, bag_size: 12178\n",
      "batch 79, loss: 0.0035, label: 0, bag_size: 13795\n",
      "batch 99, loss: 0.4544, label: 0, bag_size: 25420\n",
      "batch 119, loss: 0.0429, label: 0, bag_size: 1800\n",
      "batch 139, loss: 0.2888, label: 1, bag_size: 1512\n",
      "batch 159, loss: 0.2554, label: 0, bag_size: 2266\n",
      "batch 179, loss: 0.0598, label: 1, bag_size: 1437\n",
      "batch 199, loss: 0.0055, label: 0, bag_size: 21319\n",
      "batch 219, loss: 0.0332, label: 1, bag_size: 13477\n",
      "batch 239, loss: 0.0012, label: 1, bag_size: 689\n",
      "batch 259, loss: 0.0194, label: 1, bag_size: 9446\n",
      "batch 279, loss: 0.0306, label: 1, bag_size: 3368\n",
      "batch 299, loss: 0.0311, label: 1, bag_size: 4239\n",
      "batch 319, loss: 0.0040, label: 0, bag_size: 19466\n",
      "batch 339, loss: 0.0177, label: 0, bag_size: 4465\n",
      "batch 359, loss: 0.0023, label: 1, bag_size: 9478\n",
      "batch 379, loss: 0.0255, label: 1, bag_size: 2814\n",
      "batch 399, loss: 0.1292, label: 0, bag_size: 23996\n",
      "batch 419, loss: 0.0202, label: 0, bag_size: 2063\n",
      "batch 439, loss: 0.2246, label: 1, bag_size: 11223\n",
      "batch 459, loss: 0.0864, label: 0, bag_size: 20555\n",
      "batch 479, loss: 0.0000, label: 1, bag_size: 10867\n",
      "batch 499, loss: 0.0580, label: 0, bag_size: 1690\n",
      "batch 519, loss: 0.0279, label: 1, bag_size: 10460\n",
      "batch 539, loss: 0.0091, label: 0, bag_size: 2036\n",
      "batch 559, loss: 0.3441, label: 1, bag_size: 12340\n",
      "batch 579, loss: 0.0768, label: 1, bag_size: 1525\n",
      "batch 599, loss: 0.0442, label: 1, bag_size: 8216\n",
      "batch 619, loss: 0.0537, label: 1, bag_size: 4789\n",
      "batch 639, loss: 0.1074, label: 0, bag_size: 11212\n",
      "batch 659, loss: 0.0417, label: 1, bag_size: 9561\n",
      "batch 679, loss: 0.1204, label: 1, bag_size: 7468\n",
      "batch 699, loss: 0.0188, label: 1, bag_size: 9446\n",
      "batch 719, loss: 1.3536, label: 0, bag_size: 4345\n",
      "batch 739, loss: 0.3595, label: 1, bag_size: 10622\n",
      "batch 759, loss: 0.0006, label: 0, bag_size: 2844\n",
      "batch 779, loss: 0.0001, label: 1, bag_size: 1781\n",
      "batch 799, loss: 0.0001, label: 1, bag_size: 2136\n",
      "batch 819, loss: 0.0076, label: 1, bag_size: 14202\n",
      "Epoch: 34, train_loss: 0.1701, train_error: 0.0622\n",
      "class 0: acc 0.9407407407407408, correct 381/405\n",
      "class 1: acc 0.9349397590361446, correct 388/415\n",
      "\n",
      "Val Set, val_loss: 0.1840, val_error: 0.0636, auc: 0.9808\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "EarlyStopping counter: 21 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1934, label: 1, bag_size: 12180\n",
      "batch 39, loss: 0.0090, label: 0, bag_size: 18240\n",
      "batch 59, loss: 0.0061, label: 0, bag_size: 12910\n",
      "batch 79, loss: 0.5531, label: 0, bag_size: 3444\n",
      "batch 99, loss: 0.0387, label: 1, bag_size: 11223\n",
      "batch 119, loss: 0.0013, label: 0, bag_size: 20150\n",
      "batch 139, loss: 0.0010, label: 0, bag_size: 1962\n",
      "batch 159, loss: 0.0066, label: 1, bag_size: 621\n",
      "batch 179, loss: 0.0060, label: 0, bag_size: 32227\n",
      "batch 199, loss: 0.0059, label: 1, bag_size: 8466\n",
      "batch 219, loss: 0.0006, label: 1, bag_size: 11642\n",
      "batch 239, loss: 0.0013, label: 1, bag_size: 9955\n",
      "batch 259, loss: 0.0004, label: 1, bag_size: 7650\n",
      "batch 279, loss: 0.1255, label: 0, bag_size: 2219\n",
      "batch 299, loss: 0.1212, label: 0, bag_size: 4523\n",
      "batch 319, loss: 0.0001, label: 1, bag_size: 3640\n",
      "batch 339, loss: 0.1361, label: 1, bag_size: 1525\n",
      "batch 359, loss: 0.0010, label: 1, bag_size: 4442\n",
      "batch 379, loss: 0.0620, label: 0, bag_size: 3552\n",
      "batch 399, loss: 0.0028, label: 1, bag_size: 5340\n",
      "batch 419, loss: 0.0031, label: 0, bag_size: 16992\n",
      "batch 439, loss: 0.1848, label: 0, bag_size: 23714\n",
      "batch 459, loss: 0.0000, label: 1, bag_size: 6752\n",
      "batch 479, loss: 0.2387, label: 1, bag_size: 8191\n",
      "batch 499, loss: 0.0037, label: 0, bag_size: 16782\n",
      "batch 519, loss: 0.0047, label: 0, bag_size: 21093\n",
      "batch 539, loss: 1.2880, label: 1, bag_size: 1284\n",
      "batch 559, loss: 0.0158, label: 0, bag_size: 6624\n",
      "batch 579, loss: 0.0072, label: 1, bag_size: 14230\n",
      "batch 599, loss: 0.0012, label: 0, bag_size: 12212\n",
      "batch 619, loss: 0.0771, label: 1, bag_size: 5231\n",
      "batch 639, loss: 1.0251, label: 1, bag_size: 6360\n",
      "batch 659, loss: 0.2046, label: 0, bag_size: 2920\n",
      "batch 679, loss: 0.0184, label: 0, bag_size: 1213\n",
      "batch 699, loss: 0.0000, label: 1, bag_size: 5221\n",
      "batch 719, loss: 0.0166, label: 0, bag_size: 9471\n",
      "batch 739, loss: 0.0329, label: 1, bag_size: 4956\n",
      "batch 759, loss: 0.0244, label: 1, bag_size: 14779\n",
      "batch 779, loss: 0.0581, label: 0, bag_size: 2266\n",
      "batch 799, loss: 0.0027, label: 1, bag_size: 15689\n",
      "batch 819, loss: 0.0881, label: 1, bag_size: 7798\n",
      "Epoch: 35, train_loss: 0.1531, train_error: 0.0561\n",
      "class 0: acc 0.9542168674698795, correct 396/415\n",
      "class 1: acc 0.9333333333333333, correct 378/405\n",
      "\n",
      "Val Set, val_loss: 0.1811, val_error: 0.0727, auc: 0.9818\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 22 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0048, label: 0, bag_size: 17630\n",
      "batch 39, loss: 0.0011, label: 1, bag_size: 3968\n",
      "batch 59, loss: 0.0408, label: 0, bag_size: 22498\n",
      "batch 79, loss: 0.0037, label: 0, bag_size: 11735\n",
      "batch 99, loss: 0.0243, label: 1, bag_size: 5612\n",
      "batch 119, loss: 0.0052, label: 0, bag_size: 22828\n",
      "batch 139, loss: 0.0013, label: 1, bag_size: 4442\n",
      "batch 159, loss: 0.3281, label: 0, bag_size: 23714\n",
      "batch 179, loss: 0.0033, label: 0, bag_size: 12201\n",
      "batch 199, loss: 0.0152, label: 0, bag_size: 2303\n",
      "batch 219, loss: 0.0108, label: 1, bag_size: 6606\n",
      "batch 239, loss: 0.7404, label: 0, bag_size: 5105\n",
      "batch 259, loss: 0.0044, label: 0, bag_size: 2296\n",
      "batch 279, loss: 0.0045, label: 0, bag_size: 21682\n",
      "batch 299, loss: 0.0009, label: 1, bag_size: 19606\n",
      "batch 319, loss: 0.0034, label: 1, bag_size: 1622\n",
      "batch 339, loss: 0.7527, label: 0, bag_size: 14664\n",
      "batch 359, loss: 0.1737, label: 1, bag_size: 12895\n",
      "batch 379, loss: 0.0034, label: 1, bag_size: 5723\n",
      "batch 399, loss: 0.0001, label: 1, bag_size: 8003\n",
      "batch 419, loss: 0.3242, label: 0, bag_size: 5409\n",
      "batch 439, loss: 0.0067, label: 1, bag_size: 7650\n",
      "batch 459, loss: 0.0048, label: 0, bag_size: 2844\n",
      "batch 479, loss: 0.4079, label: 1, bag_size: 7246\n",
      "batch 499, loss: 0.0088, label: 0, bag_size: 3190\n",
      "batch 519, loss: 0.1250, label: 0, bag_size: 3725\n",
      "batch 539, loss: 0.0611, label: 0, bag_size: 11212\n",
      "batch 559, loss: 0.1040, label: 1, bag_size: 2140\n",
      "batch 579, loss: 0.0005, label: 0, bag_size: 705\n",
      "batch 599, loss: 0.0942, label: 1, bag_size: 8012\n",
      "batch 619, loss: 0.0132, label: 1, bag_size: 3856\n",
      "batch 639, loss: 0.0583, label: 0, bag_size: 1508\n",
      "batch 659, loss: 0.8769, label: 0, bag_size: 2290\n",
      "batch 679, loss: 0.1354, label: 0, bag_size: 2043\n",
      "batch 699, loss: 0.0244, label: 1, bag_size: 2140\n",
      "batch 719, loss: 0.0043, label: 0, bag_size: 9470\n",
      "batch 739, loss: 0.0068, label: 1, bag_size: 13051\n",
      "batch 759, loss: 0.0001, label: 1, bag_size: 4394\n",
      "batch 779, loss: 0.7003, label: 1, bag_size: 21450\n",
      "batch 799, loss: 3.3493, label: 1, bag_size: 1038\n",
      "batch 819, loss: 0.0048, label: 0, bag_size: 10263\n",
      "Epoch: 36, train_loss: 0.1743, train_error: 0.0622\n",
      "class 0: acc 0.9432098765432099, correct 382/405\n",
      "class 1: acc 0.9325301204819277, correct 387/415\n",
      "\n",
      "Val Set, val_loss: 0.2220, val_error: 0.0909, auc: 0.9804\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 23 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0127, label: 0, bag_size: 11259\n",
      "batch 39, loss: 0.0013, label: 0, bag_size: 8252\n",
      "batch 59, loss: 0.0025, label: 0, bag_size: 14828\n",
      "batch 79, loss: 0.0501, label: 0, bag_size: 21864\n",
      "batch 99, loss: 1.8670, label: 1, bag_size: 1533\n",
      "batch 119, loss: 0.0010, label: 0, bag_size: 11917\n",
      "batch 139, loss: 0.0145, label: 0, bag_size: 5965\n",
      "batch 159, loss: 0.0047, label: 0, bag_size: 10751\n",
      "batch 179, loss: 0.0479, label: 0, bag_size: 7823\n",
      "batch 199, loss: 0.3685, label: 0, bag_size: 10381\n",
      "batch 219, loss: 0.0296, label: 1, bag_size: 1437\n",
      "batch 239, loss: 0.0002, label: 1, bag_size: 15213\n",
      "batch 259, loss: 0.0010, label: 0, bag_size: 14206\n",
      "batch 279, loss: 0.0000, label: 1, bag_size: 12611\n",
      "batch 299, loss: 0.0066, label: 1, bag_size: 11160\n",
      "batch 319, loss: 0.0547, label: 0, bag_size: 17155\n",
      "batch 339, loss: 0.0075, label: 0, bag_size: 705\n",
      "batch 359, loss: 1.3686, label: 0, bag_size: 2959\n",
      "batch 379, loss: 0.0555, label: 0, bag_size: 3810\n",
      "batch 399, loss: 0.4516, label: 1, bag_size: 12714\n",
      "batch 419, loss: 0.0066, label: 0, bag_size: 1452\n",
      "batch 439, loss: 0.0131, label: 1, bag_size: 4877\n",
      "batch 459, loss: 0.0003, label: 1, bag_size: 5833\n",
      "batch 479, loss: 0.0000, label: 1, bag_size: 6875\n",
      "batch 499, loss: 0.0001, label: 1, bag_size: 19039\n",
      "batch 519, loss: 0.5011, label: 1, bag_size: 4308\n",
      "batch 539, loss: 0.2039, label: 1, bag_size: 11223\n",
      "batch 559, loss: 0.0050, label: 0, bag_size: 11759\n",
      "batch 579, loss: 0.0141, label: 1, bag_size: 2662\n",
      "batch 599, loss: 0.0098, label: 0, bag_size: 8981\n",
      "batch 619, loss: 0.0035, label: 0, bag_size: 10791\n",
      "batch 639, loss: 0.0019, label: 0, bag_size: 14305\n",
      "batch 659, loss: 0.0325, label: 0, bag_size: 11259\n",
      "batch 679, loss: 0.4418, label: 0, bag_size: 2609\n",
      "batch 699, loss: 0.0325, label: 1, bag_size: 8660\n",
      "batch 719, loss: 0.0100, label: 0, bag_size: 11477\n",
      "batch 739, loss: 0.0045, label: 0, bag_size: 1909\n",
      "batch 759, loss: 0.2678, label: 1, bag_size: 21252\n",
      "batch 779, loss: 0.0000, label: 1, bag_size: 9673\n",
      "batch 799, loss: 0.0347, label: 1, bag_size: 7981\n",
      "batch 819, loss: 0.0002, label: 0, bag_size: 10481\n",
      "Epoch: 37, train_loss: 0.1626, train_error: 0.0659\n",
      "class 0: acc 0.9370277078085643, correct 372/397\n",
      "class 1: acc 0.9314420803782506, correct 394/423\n",
      "\n",
      "Val Set, val_loss: 0.1833, val_error: 0.0727, auc: 0.9811\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 24 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0022, label: 0, bag_size: 3657\n",
      "batch 39, loss: 0.0142, label: 1, bag_size: 4880\n",
      "batch 59, loss: 0.0024, label: 0, bag_size: 23037\n",
      "batch 79, loss: 0.0021, label: 0, bag_size: 11654\n",
      "batch 99, loss: 0.0013, label: 1, bag_size: 617\n",
      "batch 119, loss: 0.0004, label: 1, bag_size: 3640\n",
      "batch 139, loss: 0.0000, label: 1, bag_size: 7078\n",
      "batch 159, loss: 0.0179, label: 0, bag_size: 10381\n",
      "batch 179, loss: 0.0019, label: 1, bag_size: 21827\n",
      "batch 199, loss: 0.0002, label: 1, bag_size: 12712\n",
      "batch 219, loss: 0.1280, label: 0, bag_size: 2242\n",
      "batch 239, loss: 0.0114, label: 1, bag_size: 2678\n",
      "batch 259, loss: 0.3979, label: 0, bag_size: 9596\n",
      "batch 279, loss: 0.0055, label: 0, bag_size: 1891\n",
      "batch 299, loss: 0.0007, label: 1, bag_size: 4259\n",
      "batch 319, loss: 0.0016, label: 0, bag_size: 12731\n",
      "batch 339, loss: 0.0601, label: 0, bag_size: 3541\n",
      "batch 359, loss: 0.0011, label: 1, bag_size: 15213\n",
      "batch 379, loss: 0.0137, label: 0, bag_size: 2760\n",
      "batch 399, loss: 0.0002, label: 0, bag_size: 2424\n",
      "batch 419, loss: 0.0022, label: 0, bag_size: 5551\n",
      "batch 439, loss: 0.9282, label: 0, bag_size: 47866\n",
      "batch 459, loss: 0.0000, label: 1, bag_size: 14515\n",
      "batch 479, loss: 0.2276, label: 0, bag_size: 11922\n",
      "batch 499, loss: 0.0013, label: 0, bag_size: 1760\n",
      "batch 519, loss: 0.0256, label: 0, bag_size: 22681\n",
      "batch 539, loss: 0.0001, label: 1, bag_size: 15716\n",
      "batch 559, loss: 0.0222, label: 1, bag_size: 3450\n",
      "batch 579, loss: 0.0024, label: 0, bag_size: 3787\n",
      "batch 599, loss: 0.0358, label: 1, bag_size: 7798\n",
      "batch 619, loss: 0.3178, label: 0, bag_size: 10410\n",
      "batch 639, loss: 0.0130, label: 0, bag_size: 1814\n",
      "batch 659, loss: 0.0082, label: 0, bag_size: 2814\n",
      "batch 679, loss: 0.0000, label: 1, bag_size: 6752\n",
      "batch 699, loss: 0.0129, label: 1, bag_size: 20537\n",
      "batch 719, loss: 0.0007, label: 1, bag_size: 621\n",
      "batch 739, loss: 0.0274, label: 0, bag_size: 3228\n",
      "batch 759, loss: 0.0004, label: 1, bag_size: 8522\n",
      "batch 779, loss: 0.0001, label: 1, bag_size: 13947\n",
      "batch 799, loss: 0.2218, label: 0, bag_size: 7637\n",
      "batch 819, loss: 0.1481, label: 0, bag_size: 11128\n",
      "Epoch: 38, train_loss: 0.1917, train_error: 0.0707\n",
      "class 0: acc 0.9452380952380952, correct 397/420\n",
      "class 1: acc 0.9125, correct 365/400\n",
      "\n",
      "Val Set, val_loss: 0.1885, val_error: 0.0818, auc: 0.9791\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "EarlyStopping counter: 25 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0095, label: 0, bag_size: 9234\n",
      "batch 39, loss: 0.4116, label: 0, bag_size: 2270\n",
      "batch 59, loss: 0.0084, label: 0, bag_size: 23368\n",
      "batch 79, loss: 0.0695, label: 1, bag_size: 2179\n",
      "batch 99, loss: 0.0181, label: 1, bag_size: 1493\n",
      "batch 119, loss: 0.1598, label: 0, bag_size: 7239\n",
      "batch 139, loss: 0.0446, label: 1, bag_size: 2842\n",
      "batch 159, loss: 0.0955, label: 0, bag_size: 13619\n",
      "batch 179, loss: 0.5736, label: 0, bag_size: 1637\n",
      "batch 199, loss: 0.0023, label: 0, bag_size: 2282\n",
      "batch 219, loss: 0.0546, label: 1, bag_size: 16514\n",
      "batch 239, loss: 1.2264, label: 0, bag_size: 1701\n",
      "batch 259, loss: 0.0367, label: 1, bag_size: 4308\n",
      "batch 279, loss: 0.0003, label: 1, bag_size: 6453\n",
      "batch 299, loss: 0.0606, label: 1, bag_size: 2356\n",
      "batch 319, loss: 1.8536, label: 1, bag_size: 1051\n",
      "batch 339, loss: 0.5724, label: 0, bag_size: 18738\n",
      "batch 359, loss: 0.0101, label: 0, bag_size: 11654\n",
      "batch 379, loss: 0.0144, label: 0, bag_size: 9542\n",
      "batch 399, loss: 0.0016, label: 1, bag_size: 5723\n",
      "batch 419, loss: 0.0029, label: 0, bag_size: 8981\n",
      "batch 439, loss: 0.0015, label: 0, bag_size: 2820\n",
      "batch 459, loss: 0.0137, label: 1, bag_size: 5612\n",
      "batch 479, loss: 0.3194, label: 0, bag_size: 6367\n",
      "batch 499, loss: 0.0437, label: 0, bag_size: 2732\n",
      "batch 519, loss: 0.3075, label: 1, bag_size: 3450\n",
      "batch 539, loss: 0.1186, label: 1, bag_size: 12895\n",
      "batch 559, loss: 0.0000, label: 1, bag_size: 12349\n",
      "batch 579, loss: 0.0013, label: 1, bag_size: 4821\n",
      "batch 599, loss: 0.0053, label: 0, bag_size: 1984\n",
      "batch 619, loss: 0.0002, label: 1, bag_size: 14433\n",
      "batch 639, loss: 0.0074, label: 1, bag_size: 2638\n",
      "batch 659, loss: 0.0003, label: 1, bag_size: 14604\n",
      "batch 679, loss: 0.0060, label: 0, bag_size: 6851\n",
      "batch 699, loss: 0.0004, label: 0, bag_size: 2820\n",
      "batch 719, loss: 0.0003, label: 0, bag_size: 3459\n",
      "batch 739, loss: 0.0000, label: 1, bag_size: 10867\n",
      "batch 759, loss: 0.0002, label: 1, bag_size: 6453\n",
      "batch 779, loss: 0.0093, label: 1, bag_size: 4308\n",
      "batch 799, loss: 0.0116, label: 0, bag_size: 1438\n",
      "batch 819, loss: 0.0002, label: 1, bag_size: 6343\n",
      "Epoch: 39, train_loss: 0.1689, train_error: 0.0683\n",
      "class 0: acc 0.9439024390243902, correct 387/410\n",
      "class 1: acc 0.9195121951219513, correct 377/410\n",
      "\n",
      "Val Set, val_loss: 0.1805, val_error: 0.0636, auc: 0.9821\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9137931034482759, correct 53/58\n",
      "EarlyStopping counter: 26 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0501, label: 1, bag_size: 2814\n",
      "batch 39, loss: 1.1041, label: 0, bag_size: 2070\n",
      "batch 59, loss: 0.0023, label: 0, bag_size: 1483\n",
      "batch 79, loss: 1.2456, label: 1, bag_size: 1242\n",
      "batch 99, loss: 0.7672, label: 1, bag_size: 2344\n",
      "batch 119, loss: 0.4746, label: 1, bag_size: 983\n",
      "batch 139, loss: 0.0143, label: 0, bag_size: 2244\n",
      "batch 159, loss: 0.0000, label: 1, bag_size: 5731\n",
      "batch 179, loss: 0.0125, label: 0, bag_size: 14625\n",
      "batch 199, loss: 0.0029, label: 0, bag_size: 1349\n",
      "batch 219, loss: 0.0010, label: 0, bag_size: 4902\n",
      "batch 239, loss: 0.0001, label: 1, bag_size: 3295\n",
      "batch 259, loss: 0.3760, label: 0, bag_size: 18215\n",
      "batch 279, loss: 0.1888, label: 1, bag_size: 2140\n",
      "batch 299, loss: 0.0436, label: 0, bag_size: 8330\n",
      "batch 319, loss: 0.3661, label: 1, bag_size: 15563\n",
      "batch 339, loss: 0.0006, label: 1, bag_size: 7381\n",
      "batch 359, loss: 0.0209, label: 0, bag_size: 11187\n",
      "batch 379, loss: 0.2161, label: 0, bag_size: 5211\n",
      "batch 399, loss: 0.0183, label: 1, bag_size: 8592\n",
      "batch 419, loss: 0.0000, label: 1, bag_size: 7078\n",
      "batch 439, loss: 0.0030, label: 0, bag_size: 15636\n",
      "batch 459, loss: 0.0317, label: 1, bag_size: 2480\n",
      "batch 479, loss: 0.0111, label: 0, bag_size: 12593\n",
      "batch 499, loss: 0.2995, label: 0, bag_size: 8788\n",
      "batch 519, loss: 0.0493, label: 0, bag_size: 21319\n",
      "batch 539, loss: 0.0151, label: 0, bag_size: 9866\n",
      "batch 559, loss: 0.0008, label: 1, bag_size: 5629\n",
      "batch 579, loss: 0.0208, label: 1, bag_size: 10072\n",
      "batch 599, loss: 0.0058, label: 1, bag_size: 5723\n",
      "batch 619, loss: 0.4153, label: 0, bag_size: 2098\n",
      "batch 639, loss: 0.0592, label: 0, bag_size: 3810\n",
      "batch 659, loss: 0.0016, label: 0, bag_size: 12201\n",
      "batch 679, loss: 0.0014, label: 1, bag_size: 2136\n",
      "batch 699, loss: 0.4949, label: 0, bag_size: 3783\n",
      "batch 719, loss: 0.1226, label: 0, bag_size: 8661\n",
      "batch 739, loss: 0.0002, label: 1, bag_size: 10920\n",
      "batch 759, loss: 0.1273, label: 1, bag_size: 5894\n",
      "batch 779, loss: 0.0046, label: 0, bag_size: 15841\n",
      "batch 799, loss: 0.0277, label: 0, bag_size: 3552\n",
      "batch 819, loss: 1.5431, label: 1, bag_size: 1822\n",
      "Epoch: 40, train_loss: 0.1832, train_error: 0.0695\n",
      "class 0: acc 0.9382716049382716, correct 380/405\n",
      "class 1: acc 0.9228915662650602, correct 383/415\n",
      "\n",
      "Val Set, val_loss: 0.1710, val_error: 0.0545, auc: 0.9828\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "EarlyStopping counter: 27 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0030, label: 1, bag_size: 7217\n",
      "batch 39, loss: 0.9307, label: 1, bag_size: 2937\n",
      "batch 59, loss: 0.0032, label: 1, bag_size: 6606\n",
      "batch 79, loss: 0.0015, label: 0, bag_size: 5551\n",
      "batch 99, loss: 0.0199, label: 1, bag_size: 20333\n",
      "batch 119, loss: 0.7783, label: 0, bag_size: 2959\n",
      "batch 139, loss: 0.0010, label: 0, bag_size: 23791\n",
      "batch 159, loss: 2.4843, label: 0, bag_size: 2815\n",
      "batch 179, loss: 0.3581, label: 1, bag_size: 3453\n",
      "batch 199, loss: 0.0010, label: 0, bag_size: 13964\n",
      "batch 219, loss: 0.0109, label: 1, bag_size: 3368\n",
      "batch 239, loss: 1.1361, label: 1, bag_size: 15563\n",
      "batch 259, loss: 0.0010, label: 1, bag_size: 19932\n",
      "batch 279, loss: 0.3480, label: 0, bag_size: 6356\n",
      "batch 299, loss: 0.0087, label: 1, bag_size: 11600\n",
      "batch 319, loss: 0.0024, label: 1, bag_size: 2638\n",
      "batch 339, loss: 0.0988, label: 0, bag_size: 22498\n",
      "batch 359, loss: 0.0527, label: 0, bag_size: 5211\n",
      "batch 379, loss: 0.0042, label: 1, bag_size: 9877\n",
      "batch 399, loss: 0.0035, label: 1, bag_size: 13015\n",
      "batch 419, loss: 0.1100, label: 0, bag_size: 1592\n",
      "batch 439, loss: 0.0033, label: 0, bag_size: 1438\n",
      "batch 459, loss: 0.0105, label: 0, bag_size: 14956\n",
      "batch 479, loss: 0.0209, label: 0, bag_size: 2814\n",
      "batch 499, loss: 0.0004, label: 1, bag_size: 7873\n",
      "batch 519, loss: 0.3819, label: 0, bag_size: 14249\n",
      "batch 539, loss: 0.0078, label: 1, bag_size: 11684\n",
      "batch 559, loss: 0.0579, label: 0, bag_size: 1213\n",
      "batch 579, loss: 2.4034, label: 1, bag_size: 2344\n",
      "batch 599, loss: 0.0054, label: 1, bag_size: 16565\n",
      "batch 619, loss: 0.0151, label: 1, bag_size: 8438\n",
      "batch 639, loss: 0.0021, label: 1, bag_size: 6731\n",
      "batch 659, loss: 0.0057, label: 0, bag_size: 23037\n",
      "batch 679, loss: 0.0200, label: 0, bag_size: 2920\n",
      "batch 699, loss: 0.0180, label: 0, bag_size: 2244\n",
      "batch 719, loss: 0.0919, label: 0, bag_size: 15747\n",
      "batch 739, loss: 0.3292, label: 0, bag_size: 9252\n",
      "batch 759, loss: 0.0026, label: 0, bag_size: 19043\n",
      "batch 779, loss: 0.0229, label: 1, bag_size: 13477\n",
      "batch 799, loss: 0.0122, label: 0, bag_size: 14625\n",
      "batch 819, loss: 0.0763, label: 0, bag_size: 11194\n",
      "Epoch: 41, train_loss: 0.1798, train_error: 0.0720\n",
      "class 0: acc 0.9383561643835616, correct 411/438\n",
      "class 1: acc 0.9162303664921466, correct 350/382\n",
      "\n",
      "Val Set, val_loss: 0.2108, val_error: 0.0909, auc: 0.9798\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 28 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0090, label: 0, bag_size: 8755\n",
      "batch 39, loss: 0.0012, label: 1, bag_size: 1014\n",
      "batch 59, loss: 0.0013, label: 0, bag_size: 11199\n",
      "batch 79, loss: 0.0042, label: 1, bag_size: 11600\n",
      "batch 99, loss: 0.0031, label: 0, bag_size: 12217\n",
      "batch 119, loss: 0.1177, label: 1, bag_size: 1123\n",
      "batch 139, loss: 0.0020, label: 0, bag_size: 14625\n",
      "batch 159, loss: 0.0135, label: 1, bag_size: 11518\n",
      "batch 179, loss: 0.0353, label: 1, bag_size: 8103\n",
      "batch 199, loss: 0.2930, label: 0, bag_size: 5297\n",
      "batch 219, loss: 0.0053, label: 0, bag_size: 890\n",
      "batch 239, loss: 0.6183, label: 1, bag_size: 2935\n",
      "batch 259, loss: 0.4263, label: 0, bag_size: 12510\n",
      "batch 279, loss: 0.0137, label: 0, bag_size: 1349\n",
      "batch 299, loss: 0.0106, label: 0, bag_size: 1884\n",
      "batch 319, loss: 0.1066, label: 0, bag_size: 6356\n",
      "batch 339, loss: 0.0066, label: 0, bag_size: 10791\n",
      "batch 359, loss: 0.0004, label: 1, bag_size: 621\n",
      "batch 379, loss: 0.0869, label: 0, bag_size: 16087\n",
      "batch 399, loss: 0.0171, label: 1, bag_size: 16514\n",
      "batch 419, loss: 0.0096, label: 1, bag_size: 10492\n",
      "batch 439, loss: 0.0067, label: 1, bag_size: 20333\n",
      "batch 459, loss: 0.0075, label: 1, bag_size: 928\n",
      "batch 479, loss: 0.0459, label: 1, bag_size: 7389\n",
      "batch 499, loss: 0.0004, label: 1, bag_size: 4442\n",
      "batch 519, loss: 0.0537, label: 1, bag_size: 1064\n",
      "batch 539, loss: 0.0078, label: 0, bag_size: 9455\n",
      "batch 559, loss: 0.0820, label: 0, bag_size: 21864\n",
      "batch 579, loss: 0.0009, label: 0, bag_size: 3101\n",
      "batch 599, loss: 0.1658, label: 1, bag_size: 4054\n",
      "batch 619, loss: 0.0396, label: 1, bag_size: 16514\n",
      "batch 639, loss: 0.0008, label: 1, bag_size: 11220\n",
      "batch 659, loss: 0.0951, label: 0, bag_size: 3541\n",
      "batch 679, loss: 0.0388, label: 0, bag_size: 2624\n",
      "batch 699, loss: 0.0212, label: 0, bag_size: 2814\n",
      "batch 719, loss: 0.0105, label: 1, bag_size: 1493\n",
      "batch 739, loss: 0.0138, label: 1, bag_size: 5454\n",
      "batch 759, loss: 0.0295, label: 0, bag_size: 7381\n",
      "batch 779, loss: 0.0003, label: 0, bag_size: 10481\n",
      "batch 799, loss: 0.4670, label: 0, bag_size: 4418\n",
      "batch 819, loss: 0.4158, label: 0, bag_size: 12840\n",
      "Epoch: 42, train_loss: 0.1701, train_error: 0.0585\n",
      "class 0: acc 0.9382716049382716, correct 380/405\n",
      "class 1: acc 0.944578313253012, correct 392/415\n",
      "\n",
      "Val Set, val_loss: 0.1862, val_error: 0.0727, auc: 0.9821\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.896551724137931, correct 52/58\n",
      "EarlyStopping counter: 29 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0304, label: 0, bag_size: 4465\n",
      "batch 39, loss: 0.0008, label: 0, bag_size: 3787\n",
      "batch 59, loss: 0.0673, label: 1, bag_size: 1920\n",
      "batch 79, loss: 0.0089, label: 0, bag_size: 14305\n",
      "batch 99, loss: 0.0589, label: 0, bag_size: 3089\n",
      "batch 119, loss: 0.8802, label: 0, bag_size: 3802\n",
      "batch 139, loss: 0.0291, label: 1, bag_size: 1339\n",
      "batch 159, loss: 0.1146, label: 0, bag_size: 16087\n",
      "batch 179, loss: 0.0207, label: 1, bag_size: 22286\n",
      "batch 199, loss: 0.0192, label: 1, bag_size: 12603\n",
      "batch 219, loss: 0.0107, label: 0, bag_size: 14681\n",
      "batch 239, loss: 0.0197, label: 0, bag_size: 10415\n",
      "batch 259, loss: 0.1286, label: 1, bag_size: 9322\n",
      "batch 279, loss: 0.0115, label: 0, bag_size: 1772\n",
      "batch 299, loss: 0.0128, label: 1, bag_size: 25970\n",
      "batch 319, loss: 0.2933, label: 0, bag_size: 18777\n",
      "batch 339, loss: 4.0719, label: 1, bag_size: 1963\n",
      "batch 359, loss: 0.0478, label: 1, bag_size: 13732\n",
      "batch 379, loss: 0.1332, label: 0, bag_size: 2920\n",
      "batch 399, loss: 0.1642, label: 1, bag_size: 8680\n",
      "batch 419, loss: 0.0000, label: 1, bag_size: 12611\n",
      "batch 439, loss: 0.0033, label: 0, bag_size: 9171\n",
      "batch 459, loss: 0.0034, label: 0, bag_size: 18154\n",
      "batch 479, loss: 0.0090, label: 0, bag_size: 31780\n",
      "batch 499, loss: 0.0000, label: 1, bag_size: 11195\n",
      "batch 519, loss: 3.4896, label: 1, bag_size: 3879\n",
      "batch 539, loss: 0.1432, label: 0, bag_size: 4997\n",
      "batch 559, loss: 0.2064, label: 1, bag_size: 10912\n",
      "batch 579, loss: 0.0472, label: 0, bag_size: 24911\n",
      "batch 599, loss: 0.0040, label: 0, bag_size: 11199\n",
      "batch 619, loss: 0.0062, label: 0, bag_size: 11122\n",
      "batch 639, loss: 0.0052, label: 0, bag_size: 16341\n",
      "batch 659, loss: 0.0002, label: 1, bag_size: 9971\n",
      "batch 679, loss: 0.0203, label: 1, bag_size: 29832\n",
      "batch 699, loss: 0.0013, label: 1, bag_size: 15125\n",
      "batch 719, loss: 0.0187, label: 0, bag_size: 2534\n",
      "batch 739, loss: 0.0006, label: 0, bag_size: 16782\n",
      "batch 759, loss: 0.0060, label: 0, bag_size: 3657\n",
      "batch 779, loss: 0.3916, label: 0, bag_size: 6356\n",
      "batch 799, loss: 0.0002, label: 1, bag_size: 9078\n",
      "batch 819, loss: 0.0047, label: 0, bag_size: 11546\n",
      "Epoch: 43, train_loss: 0.1534, train_error: 0.0512\n",
      "class 0: acc 0.948905109489051, correct 390/411\n",
      "class 1: acc 0.9486552567237164, correct 388/409\n",
      "\n",
      "Val Set, val_loss: 0.2091, val_error: 0.0818, auc: 0.9811\n",
      "class 0: acc 0.9807692307692307, correct 51/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "EarlyStopping counter: 30 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0633, label: 0, bag_size: 8959\n",
      "batch 39, loss: 0.0002, label: 1, bag_size: 9065\n",
      "batch 59, loss: 1.1317, label: 0, bag_size: 4345\n",
      "batch 79, loss: 0.4709, label: 0, bag_size: 18738\n",
      "batch 99, loss: 0.5030, label: 0, bag_size: 7835\n",
      "batch 119, loss: 0.0022, label: 0, bag_size: 1984\n",
      "batch 139, loss: 1.0298, label: 1, bag_size: 2344\n",
      "batch 159, loss: 0.0073, label: 1, bag_size: 25970\n",
      "batch 179, loss: 0.0013, label: 1, bag_size: 1622\n",
      "batch 199, loss: 0.0008, label: 1, bag_size: 12795\n",
      "batch 219, loss: 4.9640, label: 0, bag_size: 2815\n",
      "batch 239, loss: 0.3083, label: 1, bag_size: 1819\n",
      "batch 259, loss: 0.0019, label: 1, bag_size: 19606\n",
      "batch 279, loss: 0.0014, label: 1, bag_size: 5256\n",
      "batch 299, loss: 0.0099, label: 0, bag_size: 3265\n",
      "batch 319, loss: 1.9867, label: 0, bag_size: 21361\n",
      "batch 339, loss: 0.0005, label: 0, bag_size: 9433\n",
      "batch 359, loss: 0.0020, label: 0, bag_size: 11199\n",
      "batch 379, loss: 0.0038, label: 0, bag_size: 18240\n",
      "batch 399, loss: 0.4601, label: 0, bag_size: 7835\n",
      "batch 419, loss: 0.9842, label: 0, bag_size: 4345\n",
      "batch 439, loss: 0.0066, label: 1, bag_size: 11316\n",
      "batch 459, loss: 0.0459, label: 0, bag_size: 11187\n",
      "batch 479, loss: 0.0053, label: 1, bag_size: 2678\n",
      "batch 499, loss: 0.0005, label: 1, bag_size: 11266\n",
      "batch 519, loss: 0.0073, label: 1, bag_size: 11642\n",
      "batch 539, loss: 0.0861, label: 0, bag_size: 13591\n",
      "batch 559, loss: 0.0005, label: 0, bag_size: 7191\n",
      "batch 579, loss: 0.0058, label: 0, bag_size: 23796\n",
      "batch 599, loss: 0.0006, label: 1, bag_size: 19039\n",
      "batch 619, loss: 0.0048, label: 1, bag_size: 12719\n",
      "batch 639, loss: 0.0143, label: 0, bag_size: 17268\n",
      "batch 659, loss: 0.0014, label: 0, bag_size: 9433\n",
      "batch 679, loss: 0.0014, label: 1, bag_size: 1014\n",
      "batch 699, loss: 0.1267, label: 1, bag_size: 2140\n",
      "batch 719, loss: 0.0036, label: 1, bag_size: 10392\n",
      "batch 739, loss: 0.0381, label: 0, bag_size: 2360\n",
      "batch 759, loss: 0.0189, label: 1, bag_size: 10028\n",
      "batch 779, loss: 0.0975, label: 1, bag_size: 2140\n",
      "batch 799, loss: 0.0291, label: 1, bag_size: 13089\n",
      "batch 819, loss: 0.0998, label: 1, bag_size: 1823\n",
      "Epoch: 44, train_loss: 0.1693, train_error: 0.0585\n",
      "class 0: acc 0.9543269230769231, correct 397/416\n",
      "class 1: acc 0.9282178217821783, correct 375/404\n",
      "\n",
      "Val Set, val_loss: 0.1777, val_error: 0.0636, auc: 0.9824\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 31 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3560, label: 0, bag_size: 9252\n",
      "batch 39, loss: 0.0016, label: 1, bag_size: 12575\n",
      "batch 59, loss: 0.0000, label: 1, bag_size: 7078\n",
      "batch 79, loss: 0.0588, label: 0, bag_size: 8959\n",
      "batch 99, loss: 0.0024, label: 0, bag_size: 23037\n",
      "batch 119, loss: 0.0007, label: 1, bag_size: 7935\n",
      "batch 139, loss: 0.2400, label: 1, bag_size: 21450\n",
      "batch 159, loss: 0.0241, label: 0, bag_size: 21218\n",
      "batch 179, loss: 0.0089, label: 1, bag_size: 4956\n",
      "batch 199, loss: 0.0038, label: 1, bag_size: 2278\n",
      "batch 219, loss: 0.0042, label: 1, bag_size: 1683\n",
      "batch 239, loss: 0.0021, label: 1, bag_size: 10392\n",
      "batch 259, loss: 0.0012, label: 1, bag_size: 4821\n",
      "batch 279, loss: 0.0092, label: 0, bag_size: 2814\n",
      "batch 299, loss: 0.0538, label: 0, bag_size: 2351\n",
      "batch 319, loss: 0.0009, label: 0, bag_size: 1416\n",
      "batch 339, loss: 0.1028, label: 1, bag_size: 6682\n",
      "batch 359, loss: 0.0646, label: 0, bag_size: 1592\n",
      "batch 379, loss: 0.0002, label: 1, bag_size: 5561\n",
      "batch 399, loss: 0.0021, label: 0, bag_size: 8252\n",
      "batch 419, loss: 0.0102, label: 0, bag_size: 23796\n",
      "batch 439, loss: 0.0020, label: 0, bag_size: 5225\n",
      "batch 459, loss: 3.0593, label: 1, bag_size: 684\n",
      "batch 479, loss: 0.0462, label: 0, bag_size: 1639\n",
      "batch 499, loss: 0.0014, label: 0, bag_size: 2296\n",
      "batch 519, loss: 0.0013, label: 1, bag_size: 3968\n",
      "batch 539, loss: 0.0040, label: 0, bag_size: 22828\n",
      "batch 559, loss: 0.0002, label: 0, bag_size: 2628\n",
      "batch 579, loss: 0.0000, label: 1, bag_size: 15233\n",
      "batch 599, loss: 0.0027, label: 0, bag_size: 6851\n",
      "batch 619, loss: 0.0643, label: 1, bag_size: 8982\n",
      "batch 639, loss: 0.0222, label: 1, bag_size: 16565\n",
      "batch 659, loss: 0.1344, label: 1, bag_size: 10432\n",
      "batch 679, loss: 0.1068, label: 0, bag_size: 9069\n",
      "batch 699, loss: 0.0000, label: 1, bag_size: 13947\n",
      "batch 719, loss: 0.0073, label: 1, bag_size: 7768\n",
      "batch 739, loss: 0.0046, label: 0, bag_size: 2367\n",
      "batch 759, loss: 0.0865, label: 0, bag_size: 11151\n",
      "batch 779, loss: 0.0001, label: 1, bag_size: 10392\n",
      "batch 799, loss: 0.0115, label: 1, bag_size: 8592\n",
      "batch 819, loss: 0.3957, label: 1, bag_size: 5160\n",
      "Epoch: 45, train_loss: 0.1448, train_error: 0.0610\n",
      "class 0: acc 0.9418886198547215, correct 389/413\n",
      "class 1: acc 0.9361179361179361, correct 381/407\n",
      "\n",
      "Val Set, val_loss: 0.1739, val_error: 0.0545, auc: 0.9831\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 32 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0042, label: 1, bag_size: 1022\n",
      "batch 39, loss: 0.6101, label: 0, bag_size: 25420\n",
      "batch 59, loss: 0.0085, label: 0, bag_size: 16992\n",
      "batch 79, loss: 0.1279, label: 0, bag_size: 20555\n",
      "batch 99, loss: 0.0082, label: 1, bag_size: 9877\n",
      "batch 119, loss: 0.0033, label: 0, bag_size: 12793\n",
      "batch 139, loss: 0.0831, label: 0, bag_size: 2104\n",
      "batch 159, loss: 0.0045, label: 0, bag_size: 18240\n",
      "batch 179, loss: 0.0000, label: 1, bag_size: 7767\n",
      "batch 199, loss: 0.0384, label: 0, bag_size: 4598\n",
      "batch 219, loss: 0.0044, label: 0, bag_size: 10490\n",
      "batch 239, loss: 0.0036, label: 1, bag_size: 12460\n",
      "batch 259, loss: 0.0075, label: 0, bag_size: 23791\n",
      "batch 279, loss: 0.0322, label: 1, bag_size: 11032\n",
      "batch 299, loss: 1.3303, label: 1, bag_size: 1284\n",
      "batch 319, loss: 0.0926, label: 1, bag_size: 7798\n",
      "batch 339, loss: 0.0138, label: 1, bag_size: 11256\n",
      "batch 359, loss: 0.0000, label: 1, bag_size: 14515\n",
      "batch 379, loss: 0.0014, label: 0, bag_size: 19518\n",
      "batch 399, loss: 0.0032, label: 0, bag_size: 17791\n",
      "batch 419, loss: 0.0138, label: 1, bag_size: 4789\n",
      "batch 439, loss: 0.0217, label: 1, bag_size: 9322\n",
      "batch 459, loss: 0.0176, label: 1, bag_size: 8448\n",
      "batch 479, loss: 0.0000, label: 1, bag_size: 11875\n",
      "batch 499, loss: 0.0974, label: 0, bag_size: 22681\n",
      "batch 519, loss: 0.0546, label: 1, bag_size: 10912\n",
      "batch 539, loss: 0.0000, label: 1, bag_size: 1781\n",
      "batch 559, loss: 0.0018, label: 0, bag_size: 23368\n",
      "batch 579, loss: 0.0022, label: 0, bag_size: 1438\n",
      "batch 599, loss: 0.0075, label: 1, bag_size: 8026\n",
      "batch 619, loss: 0.0067, label: 1, bag_size: 8592\n",
      "batch 639, loss: 0.0052, label: 1, bag_size: 3683\n",
      "batch 659, loss: 0.5669, label: 0, bag_size: 26208\n",
      "batch 679, loss: 0.0685, label: 0, bag_size: 11922\n",
      "batch 699, loss: 0.0375, label: 0, bag_size: 1920\n",
      "batch 719, loss: 0.3464, label: 1, bag_size: 8982\n",
      "batch 739, loss: 0.0024, label: 0, bag_size: 17630\n",
      "batch 759, loss: 0.0021, label: 1, bag_size: 5723\n",
      "batch 779, loss: 2.0323, label: 1, bag_size: 9162\n",
      "batch 799, loss: 0.0668, label: 0, bag_size: 9949\n",
      "batch 819, loss: 0.0820, label: 0, bag_size: 3810\n",
      "Epoch: 46, train_loss: 0.1650, train_error: 0.0573\n",
      "class 0: acc 0.9514066496163683, correct 372/391\n",
      "class 1: acc 0.9347319347319347, correct 401/429\n",
      "\n",
      "Val Set, val_loss: 0.2037, val_error: 0.0818, auc: 0.9824\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 33 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.0450, label: 1, bag_size: 2731\n",
      "batch 39, loss: 0.0006, label: 1, bag_size: 1412\n",
      "batch 59, loss: 0.0113, label: 0, bag_size: 763\n",
      "batch 79, loss: 0.0285, label: 0, bag_size: 24439\n",
      "batch 99, loss: 0.0000, label: 1, bag_size: 6875\n",
      "batch 119, loss: 0.2347, label: 1, bag_size: 8103\n",
      "batch 139, loss: 0.0027, label: 0, bag_size: 3101\n",
      "batch 159, loss: 0.0219, label: 1, bag_size: 16514\n",
      "batch 179, loss: 0.0024, label: 0, bag_size: 803\n",
      "batch 199, loss: 0.0002, label: 1, bag_size: 12712\n",
      "batch 219, loss: 0.3642, label: 1, bag_size: 1533\n",
      "batch 239, loss: 0.0000, label: 1, bag_size: 12349\n",
      "batch 259, loss: 0.0830, label: 1, bag_size: 549\n",
      "batch 279, loss: 0.3078, label: 0, bag_size: 5009\n",
      "batch 299, loss: 0.1761, label: 1, bag_size: 9561\n",
      "batch 319, loss: 0.0000, label: 1, bag_size: 14515\n",
      "batch 339, loss: 0.0839, label: 0, bag_size: 5409\n",
      "batch 359, loss: 0.0015, label: 1, bag_size: 5690\n",
      "batch 379, loss: 0.0124, label: 1, bag_size: 621\n",
      "batch 399, loss: 0.8347, label: 0, bag_size: 3468\n",
      "batch 419, loss: 0.2547, label: 1, bag_size: 645\n",
      "batch 439, loss: 0.3413, label: 1, bag_size: 1609\n",
      "batch 459, loss: 0.0003, label: 1, bag_size: 18794\n",
      "batch 479, loss: 0.0020, label: 1, bag_size: 12719\n",
      "batch 499, loss: 0.4366, label: 1, bag_size: 1242\n",
      "batch 519, loss: 0.1843, label: 1, bag_size: 7981\n",
      "batch 539, loss: 0.0016, label: 0, bag_size: 11654\n",
      "batch 559, loss: 0.0113, label: 1, bag_size: 10028\n",
      "batch 579, loss: 0.0002, label: 1, bag_size: 3409\n",
      "batch 599, loss: 0.0058, label: 0, bag_size: 8812\n",
      "batch 619, loss: 0.2472, label: 0, bag_size: 12840\n",
      "batch 639, loss: 0.0025, label: 1, bag_size: 19606\n",
      "batch 659, loss: 0.0016, label: 1, bag_size: 14202\n",
      "batch 679, loss: 0.0015, label: 1, bag_size: 16267\n",
      "batch 699, loss: 0.0031, label: 1, bag_size: 14230\n",
      "batch 719, loss: 0.0038, label: 0, bag_size: 15077\n",
      "batch 739, loss: 0.0032, label: 0, bag_size: 16607\n",
      "batch 759, loss: 0.0001, label: 1, bag_size: 7381\n",
      "batch 779, loss: 0.0318, label: 0, bag_size: 7823\n",
      "batch 799, loss: 1.9754, label: 0, bag_size: 3468\n",
      "batch 819, loss: 0.0016, label: 0, bag_size: 3265\n",
      "Epoch: 47, train_loss: 0.1685, train_error: 0.0646\n",
      "class 0: acc 0.9359605911330049, correct 380/406\n",
      "class 1: acc 0.9347826086956522, correct 387/414\n",
      "\n",
      "Val Set, val_loss: 0.2861, val_error: 0.1273, auc: 0.9847\n",
      "class 0: acc 0.9807692307692307, correct 51/52\n",
      "class 1: acc 0.7758620689655172, correct 45/58\n",
      "EarlyStopping counter: 34 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1290, label: 1, bag_size: 10912\n",
      "batch 39, loss: 0.0008, label: 1, bag_size: 11642\n",
      "batch 59, loss: 0.0525, label: 1, bag_size: 8754\n",
      "batch 79, loss: 0.1046, label: 1, bag_size: 8216\n",
      "batch 99, loss: 0.0461, label: 0, bag_size: 4418\n",
      "batch 119, loss: 0.0003, label: 1, bag_size: 6966\n",
      "batch 139, loss: 0.0319, label: 0, bag_size: 21138\n",
      "batch 159, loss: 0.1345, label: 1, bag_size: 4786\n",
      "batch 179, loss: 0.0209, label: 1, bag_size: 7613\n",
      "batch 199, loss: 0.0005, label: 1, bag_size: 5340\n",
      "batch 219, loss: 0.0285, label: 0, bag_size: 3228\n",
      "batch 239, loss: 0.0539, label: 0, bag_size: 9471\n",
      "batch 259, loss: 0.0422, label: 0, bag_size: 11113\n",
      "batch 279, loss: 0.5020, label: 1, bag_size: 10912\n",
      "batch 299, loss: 0.0000, label: 1, bag_size: 1781\n",
      "batch 319, loss: 0.0257, label: 0, bag_size: 21093\n",
      "batch 339, loss: 0.1102, label: 1, bag_size: 13732\n",
      "batch 359, loss: 0.0489, label: 0, bag_size: 11151\n",
      "batch 379, loss: 0.0647, label: 0, bag_size: 11259\n",
      "batch 399, loss: 0.0031, label: 1, bag_size: 11220\n",
      "batch 419, loss: 2.0215, label: 1, bag_size: 2565\n",
      "batch 439, loss: 0.1763, label: 0, bag_size: 22498\n",
      "batch 459, loss: 0.0242, label: 0, bag_size: 7989\n",
      "batch 479, loss: 1.5290, label: 1, bag_size: 1533\n",
      "batch 499, loss: 0.0219, label: 1, bag_size: 4239\n",
      "batch 519, loss: 0.0002, label: 1, bag_size: 8003\n",
      "batch 539, loss: 0.0207, label: 0, bag_size: 14319\n",
      "batch 559, loss: 0.0120, label: 0, bag_size: 30751\n",
      "batch 579, loss: 0.0214, label: 0, bag_size: 1498\n",
      "batch 599, loss: 0.0011, label: 1, bag_size: 12460\n",
      "batch 619, loss: 0.0030, label: 0, bag_size: 9455\n",
      "batch 639, loss: 1.6954, label: 0, bag_size: 2959\n",
      "batch 659, loss: 0.0062, label: 0, bag_size: 12524\n",
      "batch 679, loss: 0.8880, label: 1, bag_size: 12714\n",
      "batch 699, loss: 0.0051, label: 0, bag_size: 12593\n",
      "batch 719, loss: 0.0044, label: 0, bag_size: 15967\n",
      "batch 739, loss: 0.0008, label: 1, bag_size: 1638\n",
      "batch 759, loss: 0.0002, label: 1, bag_size: 12611\n",
      "batch 779, loss: 0.0700, label: 0, bag_size: 13880\n",
      "batch 799, loss: 0.0474, label: 0, bag_size: 2814\n",
      "batch 819, loss: 0.0003, label: 1, bag_size: 4394\n",
      "Epoch: 48, train_loss: 0.1829, train_error: 0.0720\n",
      "class 0: acc 0.95, correct 399/420\n",
      "class 1: acc 0.905, correct 362/400\n",
      "\n",
      "Val Set, val_loss: 0.1886, val_error: 0.0727, auc: 0.9841\n",
      "class 0: acc 0.9807692307692307, correct 51/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "EarlyStopping counter: 35 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0261, label: 0, bag_size: 9415\n",
      "batch 39, loss: 0.6810, label: 0, bag_size: 2815\n",
      "batch 59, loss: 0.0020, label: 0, bag_size: 14333\n",
      "batch 79, loss: 0.0013, label: 0, bag_size: 17268\n",
      "batch 99, loss: 0.0539, label: 0, bag_size: 10068\n",
      "batch 119, loss: 0.0292, label: 0, bag_size: 11727\n",
      "batch 139, loss: 0.0881, label: 0, bag_size: 22498\n",
      "batch 159, loss: 0.0110, label: 0, bag_size: 10128\n",
      "batch 179, loss: 0.1549, label: 0, bag_size: 1684\n",
      "batch 199, loss: 0.0883, label: 0, bag_size: 1614\n",
      "batch 219, loss: 0.0143, label: 0, bag_size: 24911\n",
      "batch 239, loss: 0.0000, label: 1, bag_size: 9610\n",
      "batch 259, loss: 0.0221, label: 0, bag_size: 27158\n",
      "batch 279, loss: 0.0002, label: 1, bag_size: 8019\n",
      "batch 299, loss: 0.0854, label: 0, bag_size: 4523\n",
      "batch 319, loss: 0.0343, label: 1, bag_size: 3224\n",
      "batch 339, loss: 0.0038, label: 0, bag_size: 23037\n",
      "batch 359, loss: 0.0020, label: 0, bag_size: 13964\n",
      "batch 379, loss: 0.1072, label: 1, bag_size: 2480\n",
      "batch 399, loss: 0.0078, label: 0, bag_size: 19808\n",
      "batch 419, loss: 0.5235, label: 0, bag_size: 2242\n",
      "batch 439, loss: 0.0011, label: 0, bag_size: 5551\n",
      "batch 459, loss: 0.0036, label: 0, bag_size: 17268\n",
      "batch 479, loss: 0.0489, label: 1, bag_size: 6927\n",
      "batch 499, loss: 0.7120, label: 0, bag_size: 11306\n",
      "batch 519, loss: 0.0059, label: 1, bag_size: 13051\n",
      "batch 539, loss: 0.0188, label: 1, bag_size: 12603\n",
      "batch 559, loss: 0.0046, label: 1, bag_size: 9942\n",
      "batch 579, loss: 0.0083, label: 1, bag_size: 4250\n",
      "batch 599, loss: 0.0000, label: 1, bag_size: 5317\n",
      "batch 619, loss: 0.1031, label: 1, bag_size: 1242\n",
      "batch 639, loss: 0.0894, label: 1, bag_size: 11701\n",
      "batch 659, loss: 0.0127, label: 0, bag_size: 2063\n",
      "batch 679, loss: 0.1299, label: 1, bag_size: 11701\n",
      "batch 699, loss: 0.0062, label: 0, bag_size: 17630\n",
      "batch 719, loss: 0.0054, label: 0, bag_size: 11527\n",
      "batch 739, loss: 0.1714, label: 1, bag_size: 9561\n",
      "batch 759, loss: 0.0017, label: 0, bag_size: 9885\n",
      "batch 779, loss: 0.0062, label: 1, bag_size: 1888\n",
      "batch 799, loss: 0.0165, label: 0, bag_size: 10444\n",
      "batch 819, loss: 1.0814, label: 0, bag_size: 2290\n",
      "Epoch: 49, train_loss: 0.1565, train_error: 0.0561\n",
      "class 0: acc 0.9543147208121827, correct 376/394\n",
      "class 1: acc 0.9342723004694836, correct 398/426\n",
      "\n",
      "Val Set, val_loss: 0.1805, val_error: 0.0636, auc: 0.9804\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "EarlyStopping counter: 36 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0021, label: 0, bag_size: 3670\n",
      "batch 39, loss: 0.0717, label: 0, bag_size: 13339\n",
      "batch 59, loss: 1.0481, label: 0, bag_size: 14264\n",
      "batch 79, loss: 0.0016, label: 1, bag_size: 18095\n",
      "batch 99, loss: 0.2135, label: 0, bag_size: 9252\n",
      "batch 119, loss: 0.0889, label: 0, bag_size: 9597\n",
      "batch 139, loss: 0.0001, label: 1, bag_size: 7381\n",
      "batch 159, loss: 0.0043, label: 1, bag_size: 13174\n",
      "batch 179, loss: 0.0000, label: 1, bag_size: 15233\n",
      "batch 199, loss: 0.1814, label: 1, bag_size: 11701\n",
      "batch 219, loss: 0.0062, label: 0, bag_size: 3893\n",
      "batch 239, loss: 0.3485, label: 1, bag_size: 1437\n",
      "batch 259, loss: 0.4589, label: 1, bag_size: 21450\n",
      "batch 279, loss: 0.0256, label: 0, bag_size: 22426\n",
      "batch 299, loss: 0.0000, label: 1, bag_size: 9673\n",
      "batch 319, loss: 0.0000, label: 1, bag_size: 5221\n",
      "batch 339, loss: 0.0250, label: 1, bag_size: 5907\n",
      "batch 359, loss: 0.0007, label: 1, bag_size: 2904\n",
      "batch 379, loss: 0.1296, label: 1, bag_size: 3211\n",
      "batch 399, loss: 0.0333, label: 1, bag_size: 3224\n",
      "batch 419, loss: 0.0804, label: 0, bag_size: 3321\n",
      "batch 439, loss: 0.0078, label: 0, bag_size: 3670\n",
      "batch 459, loss: 0.0134, label: 0, bag_size: 8330\n",
      "batch 479, loss: 0.0033, label: 1, bag_size: 14887\n",
      "batch 499, loss: 0.0049, label: 0, bag_size: 10263\n",
      "batch 519, loss: 0.0001, label: 1, bag_size: 6453\n",
      "batch 539, loss: 0.0564, label: 0, bag_size: 11922\n",
      "batch 559, loss: 0.0108, label: 0, bag_size: 23398\n",
      "batch 579, loss: 0.1594, label: 1, bag_size: 8982\n",
      "batch 599, loss: 0.0000, label: 1, bag_size: 12611\n",
      "batch 619, loss: 0.0092, label: 0, bag_size: 1614\n",
      "batch 639, loss: 0.0011, label: 1, bag_size: 5494\n",
      "batch 659, loss: 0.0722, label: 1, bag_size: 13477\n",
      "batch 679, loss: 0.0622, label: 1, bag_size: 3652\n",
      "batch 699, loss: 0.0003, label: 1, bag_size: 4442\n",
      "batch 719, loss: 0.3158, label: 0, bag_size: 7239\n",
      "batch 739, loss: 0.0020, label: 0, bag_size: 14681\n",
      "batch 759, loss: 0.0023, label: 0, bag_size: 518\n",
      "batch 779, loss: 0.0003, label: 0, bag_size: 2382\n",
      "batch 799, loss: 0.0013, label: 1, bag_size: 2638\n",
      "batch 819, loss: 0.0000, label: 1, bag_size: 10920\n",
      "Epoch: 50, train_loss: 0.1263, train_error: 0.0573\n",
      "class 0: acc 0.9396984924623115, correct 374/398\n",
      "class 1: acc 0.9454976303317536, correct 399/422\n",
      "\n",
      "Val Set, val_loss: 0.3415, val_error: 0.1545, auc: 0.9788\n",
      "class 0: acc 0.6923076923076923, correct 36/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 37 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0041, label: 1, bag_size: 11394\n",
      "batch 39, loss: 0.0001, label: 0, bag_size: 10481\n",
      "batch 59, loss: 0.3334, label: 0, bag_size: 2098\n",
      "batch 79, loss: 0.0001, label: 1, bag_size: 16512\n",
      "batch 99, loss: 0.0000, label: 1, bag_size: 5221\n",
      "batch 119, loss: 0.0000, label: 1, bag_size: 13368\n",
      "batch 139, loss: 0.0001, label: 0, bag_size: 803\n",
      "batch 159, loss: 0.0655, label: 0, bag_size: 13880\n",
      "batch 179, loss: 0.4086, label: 0, bag_size: 26208\n",
      "batch 199, loss: 0.0889, label: 1, bag_size: 19972\n",
      "batch 219, loss: 0.2672, label: 0, bag_size: 2219\n",
      "batch 239, loss: 0.0007, label: 1, bag_size: 7873\n",
      "batch 259, loss: 0.0006, label: 1, bag_size: 6453\n",
      "batch 279, loss: 0.0270, label: 1, bag_size: 549\n",
      "batch 299, loss: 0.0004, label: 1, bag_size: 5629\n",
      "batch 319, loss: 0.0560, label: 0, bag_size: 6281\n",
      "batch 339, loss: 0.2309, label: 0, bag_size: 4418\n",
      "batch 359, loss: 0.3248, label: 0, bag_size: 14264\n",
      "batch 379, loss: 0.0103, label: 0, bag_size: 2044\n",
      "batch 399, loss: 0.0325, label: 1, bag_size: 12895\n",
      "batch 419, loss: 0.0125, label: 0, bag_size: 1824\n",
      "batch 439, loss: 0.0093, label: 0, bag_size: 21138\n",
      "batch 459, loss: 1.9730, label: 1, bag_size: 2937\n",
      "batch 479, loss: 0.0073, label: 0, bag_size: 9455\n",
      "batch 499, loss: 0.0350, label: 0, bag_size: 16690\n",
      "batch 519, loss: 0.0712, label: 1, bag_size: 5516\n",
      "batch 539, loss: 0.0052, label: 1, bag_size: 13089\n",
      "batch 559, loss: 0.8626, label: 0, bag_size: 11306\n",
      "batch 579, loss: 0.0839, label: 1, bag_size: 7989\n",
      "batch 599, loss: 0.0060, label: 0, bag_size: 6624\n",
      "batch 619, loss: 0.3209, label: 1, bag_size: 8216\n",
      "batch 639, loss: 0.0567, label: 1, bag_size: 8982\n",
      "batch 659, loss: 0.0456, label: 0, bag_size: 5639\n",
      "batch 679, loss: 0.0104, label: 0, bag_size: 8948\n",
      "batch 699, loss: 0.0358, label: 0, bag_size: 25814\n",
      "batch 719, loss: 0.0016, label: 1, bag_size: 16162\n",
      "batch 739, loss: 0.0604, label: 0, bag_size: 12083\n",
      "batch 759, loss: 0.4953, label: 0, bag_size: 17279\n",
      "batch 779, loss: 0.0014, label: 0, bag_size: 3970\n",
      "batch 799, loss: 0.0058, label: 1, bag_size: 1888\n",
      "batch 819, loss: 5.2001, label: 1, bag_size: 3121\n",
      "Epoch: 51, train_loss: 0.1544, train_error: 0.0561\n",
      "class 0: acc 0.9588100686498856, correct 419/437\n",
      "class 1: acc 0.9268929503916449, correct 355/383\n",
      "\n",
      "Val Set, val_loss: 0.1869, val_error: 0.0818, auc: 0.9824\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "EarlyStopping counter: 38 out of 20\n",
      "Early stopping\n",
      "Val error: 0.0636, ROC AUC: 0.9818\n",
      "Test error: 0.0722, ROC AUC: 0.9592\n",
      "class 0: acc 0.9795918367346939, correct 48/49\n",
      "class 1: acc 0.875, correct 42/48\n",
      "\n",
      "Training Fold 1!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 822 samples\n",
      "Validating on 109 samples\n",
      "Testing on 96 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Done!\n",
      "MIL_fc(\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 525826\n",
      "Total number of trainable parameters: 525826\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7287, label: 1, bag_size: 30675\n",
      "batch 39, loss: 0.6778, label: 0, bag_size: 2036\n",
      "batch 59, loss: 1.0573, label: 1, bag_size: 8522\n",
      "batch 79, loss: 0.5717, label: 0, bag_size: 18240\n",
      "batch 99, loss: 0.6767, label: 0, bag_size: 20666\n",
      "batch 119, loss: 0.7198, label: 1, bag_size: 12494\n",
      "batch 139, loss: 0.6025, label: 0, bag_size: 2624\n",
      "batch 159, loss: 0.6583, label: 1, bag_size: 9065\n",
      "batch 179, loss: 1.1534, label: 1, bag_size: 11386\n",
      "batch 199, loss: 0.9626, label: 1, bag_size: 10920\n",
      "batch 219, loss: 0.3633, label: 0, bag_size: 1052\n",
      "batch 239, loss: 0.4451, label: 0, bag_size: 23368\n",
      "batch 259, loss: 0.5708, label: 0, bag_size: 4418\n",
      "batch 279, loss: 0.9712, label: 0, bag_size: 1549\n",
      "batch 299, loss: 0.4637, label: 0, bag_size: 11477\n",
      "batch 319, loss: 0.3456, label: 1, bag_size: 10969\n",
      "batch 339, loss: 0.3489, label: 1, bag_size: 6745\n",
      "batch 359, loss: 0.9420, label: 0, bag_size: 5485\n",
      "batch 379, loss: 0.4830, label: 0, bag_size: 2266\n",
      "batch 399, loss: 0.8535, label: 1, bag_size: 2455\n",
      "batch 419, loss: 0.3914, label: 0, bag_size: 11146\n",
      "batch 439, loss: 0.4665, label: 0, bag_size: 10410\n",
      "batch 459, loss: 0.9502, label: 1, bag_size: 1437\n",
      "batch 479, loss: 0.5562, label: 0, bag_size: 12510\n",
      "batch 499, loss: 0.6589, label: 0, bag_size: 3198\n",
      "batch 519, loss: 0.4391, label: 0, bag_size: 23996\n",
      "batch 539, loss: 0.4595, label: 1, bag_size: 3224\n",
      "batch 559, loss: 0.4709, label: 1, bag_size: 5256\n",
      "batch 579, loss: 0.1929, label: 1, bag_size: 10112\n",
      "batch 599, loss: 0.8501, label: 1, bag_size: 1051\n",
      "batch 619, loss: 0.4460, label: 0, bag_size: 3708\n",
      "batch 639, loss: 0.4045, label: 0, bag_size: 9171\n",
      "batch 659, loss: 0.7308, label: 0, bag_size: 14266\n",
      "batch 679, loss: 0.7799, label: 0, bag_size: 1814\n",
      "batch 699, loss: 0.5711, label: 0, bag_size: 2098\n",
      "batch 719, loss: 0.4823, label: 1, bag_size: 2638\n",
      "batch 739, loss: 0.2901, label: 1, bag_size: 21009\n",
      "batch 759, loss: 0.5014, label: 1, bag_size: 15665\n",
      "batch 779, loss: 0.4072, label: 0, bag_size: 9387\n",
      "batch 799, loss: 0.7833, label: 0, bag_size: 8755\n",
      "batch 819, loss: 0.9041, label: 1, bag_size: 645\n",
      "Epoch: 0, train_loss: 0.5768, train_error: 0.2725\n",
      "class 0: acc 0.8284424379232506, correct 367/443\n",
      "class 1: acc 0.6094986807387863, correct 231/379\n",
      "\n",
      "Val Set, val_loss: 0.6550, val_error: 0.3945, auc: 0.8892\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.3333333333333333, correct 21/63\n",
      "Validation loss decreased (inf --> 0.654953).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6298, label: 1, bag_size: 16514\n",
      "batch 39, loss: 0.2953, label: 0, bag_size: 2091\n",
      "batch 59, loss: 0.8826, label: 1, bag_size: 2314\n",
      "batch 79, loss: 1.2875, label: 1, bag_size: 21252\n",
      "batch 99, loss: 0.6316, label: 0, bag_size: 9596\n",
      "batch 119, loss: 0.1761, label: 0, bag_size: 15736\n",
      "batch 139, loss: 1.0356, label: 1, bag_size: 12946\n",
      "batch 159, loss: 1.0842, label: 0, bag_size: 7923\n",
      "batch 179, loss: 0.8806, label: 0, bag_size: 2959\n",
      "batch 199, loss: 1.4324, label: 1, bag_size: 1794\n",
      "batch 219, loss: 0.1724, label: 1, bag_size: 5731\n",
      "batch 239, loss: 0.8153, label: 0, bag_size: 25814\n",
      "batch 259, loss: 0.2928, label: 1, bag_size: 14618\n",
      "batch 279, loss: 0.1589, label: 1, bag_size: 15665\n",
      "batch 299, loss: 0.1702, label: 0, bag_size: 11113\n",
      "batch 319, loss: 0.1042, label: 0, bag_size: 10481\n",
      "batch 339, loss: 0.3362, label: 0, bag_size: 1684\n",
      "batch 359, loss: 0.8807, label: 1, bag_size: 5629\n",
      "batch 379, loss: 0.3895, label: 0, bag_size: 1483\n",
      "batch 399, loss: 0.4018, label: 0, bag_size: 1549\n",
      "batch 419, loss: 0.2464, label: 1, bag_size: 1920\n",
      "batch 439, loss: 0.2378, label: 1, bag_size: 14779\n",
      "batch 459, loss: 0.5565, label: 0, bag_size: 2079\n",
      "batch 479, loss: 0.0683, label: 0, bag_size: 8372\n",
      "batch 499, loss: 0.2496, label: 0, bag_size: 12083\n",
      "batch 519, loss: 0.5074, label: 0, bag_size: 19808\n",
      "batch 539, loss: 1.6151, label: 1, bag_size: 1533\n",
      "batch 559, loss: 0.0932, label: 0, bag_size: 10535\n",
      "batch 579, loss: 1.7565, label: 1, bag_size: 2935\n",
      "batch 599, loss: 0.3824, label: 0, bag_size: 9596\n",
      "batch 619, loss: 0.2290, label: 0, bag_size: 7235\n",
      "batch 639, loss: 0.6791, label: 1, bag_size: 1339\n",
      "batch 659, loss: 0.2171, label: 0, bag_size: 3375\n",
      "batch 679, loss: 0.4784, label: 1, bag_size: 13015\n",
      "batch 699, loss: 0.1531, label: 1, bag_size: 8660\n",
      "batch 719, loss: 0.1148, label: 0, bag_size: 10942\n",
      "batch 739, loss: 0.3857, label: 1, bag_size: 5723\n",
      "batch 759, loss: 0.2763, label: 0, bag_size: 12561\n",
      "batch 779, loss: 1.3968, label: 1, bag_size: 2935\n",
      "batch 799, loss: 0.2006, label: 1, bag_size: 6745\n",
      "batch 819, loss: 0.1608, label: 1, bag_size: 5612\n",
      "Epoch: 1, train_loss: 0.4410, train_error: 0.1727\n",
      "class 0: acc 0.8769574944071589, correct 392/447\n",
      "class 1: acc 0.768, correct 288/375\n",
      "\n",
      "Val Set, val_loss: 0.3805, val_error: 0.1560, auc: 0.9355\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.7619047619047619, correct 48/63\n",
      "Validation loss decreased (0.654953 --> 0.380491).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1331, label: 1, bag_size: 12758\n",
      "batch 39, loss: 0.3070, label: 0, bag_size: 4465\n",
      "batch 59, loss: 0.4579, label: 0, bag_size: 16607\n",
      "batch 79, loss: 0.7161, label: 1, bag_size: 2092\n",
      "batch 99, loss: 0.1841, label: 0, bag_size: 3101\n",
      "batch 119, loss: 0.0579, label: 1, bag_size: 14618\n",
      "batch 139, loss: 0.2103, label: 0, bag_size: 11865\n",
      "batch 159, loss: 0.2188, label: 1, bag_size: 9062\n",
      "batch 179, loss: 0.0570, label: 1, bag_size: 4862\n",
      "batch 199, loss: 0.2034, label: 0, bag_size: 16211\n",
      "batch 219, loss: 1.1493, label: 1, bag_size: 2935\n",
      "batch 239, loss: 0.0675, label: 1, bag_size: 14604\n",
      "batch 259, loss: 0.3415, label: 1, bag_size: 9330\n",
      "batch 279, loss: 0.6609, label: 1, bag_size: 2678\n",
      "batch 299, loss: 0.6144, label: 1, bag_size: 16703\n",
      "batch 319, loss: 0.1854, label: 0, bag_size: 1202\n",
      "batch 339, loss: 0.1087, label: 1, bag_size: 20333\n",
      "batch 359, loss: 0.1042, label: 1, bag_size: 7935\n",
      "batch 379, loss: 0.3274, label: 1, bag_size: 1255\n",
      "batch 399, loss: 0.1413, label: 1, bag_size: 7935\n",
      "batch 419, loss: 0.1923, label: 0, bag_size: 1953\n",
      "batch 439, loss: 0.1180, label: 0, bag_size: 13964\n",
      "batch 459, loss: 0.3546, label: 1, bag_size: 8012\n",
      "batch 479, loss: 0.1423, label: 1, bag_size: 5441\n",
      "batch 499, loss: 0.1284, label: 1, bag_size: 6171\n",
      "batch 519, loss: 0.0389, label: 0, bag_size: 2760\n",
      "batch 539, loss: 0.0639, label: 0, bag_size: 2006\n",
      "batch 559, loss: 0.0805, label: 1, bag_size: 3003\n",
      "batch 579, loss: 0.2074, label: 0, bag_size: 9583\n",
      "batch 599, loss: 0.1035, label: 1, bag_size: 10969\n",
      "batch 619, loss: 0.3190, label: 1, bag_size: 1920\n",
      "batch 639, loss: 0.1408, label: 1, bag_size: 8395\n",
      "batch 659, loss: 0.6089, label: 0, bag_size: 13332\n",
      "batch 679, loss: 0.0495, label: 1, bag_size: 12758\n",
      "batch 699, loss: 0.0550, label: 1, bag_size: 20161\n",
      "batch 719, loss: 0.1609, label: 1, bag_size: 1572\n",
      "batch 739, loss: 0.7379, label: 1, bag_size: 4821\n",
      "batch 759, loss: 0.7917, label: 0, bag_size: 18215\n",
      "batch 779, loss: 0.4942, label: 1, bag_size: 1838\n",
      "batch 799, loss: 0.5379, label: 0, bag_size: 22800\n",
      "batch 819, loss: 0.2584, label: 1, bag_size: 1014\n",
      "Epoch: 2, train_loss: 0.3654, train_error: 0.1314\n",
      "class 0: acc 0.8910891089108911, correct 360/404\n",
      "class 1: acc 0.84688995215311, correct 354/418\n",
      "\n",
      "Val Set, val_loss: 0.3155, val_error: 0.1376, auc: 0.9455\n",
      "class 0: acc 0.9130434782608695, correct 42/46\n",
      "class 1: acc 0.8253968253968254, correct 52/63\n",
      "Validation loss decreased (0.380491 --> 0.315499).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3114, label: 1, bag_size: 34356\n",
      "batch 39, loss: 0.8281, label: 1, bag_size: 1819\n",
      "batch 59, loss: 0.1738, label: 1, bag_size: 4330\n",
      "batch 79, loss: 1.0772, label: 1, bag_size: 1051\n",
      "batch 99, loss: 0.0943, label: 1, bag_size: 16379\n",
      "batch 119, loss: 0.1385, label: 1, bag_size: 15125\n",
      "batch 139, loss: 0.0253, label: 1, bag_size: 8040\n",
      "batch 159, loss: 0.2544, label: 0, bag_size: 30828\n",
      "batch 179, loss: 0.3103, label: 0, bag_size: 16087\n",
      "batch 199, loss: 0.0450, label: 1, bag_size: 15213\n",
      "batch 219, loss: 0.3995, label: 0, bag_size: 18777\n",
      "batch 239, loss: 0.1708, label: 1, bag_size: 9689\n",
      "batch 259, loss: 0.5407, label: 0, bag_size: 8427\n",
      "batch 279, loss: 0.0880, label: 1, bag_size: 7148\n",
      "batch 299, loss: 0.2230, label: 0, bag_size: 2079\n",
      "batch 319, loss: 1.1110, label: 1, bag_size: 1095\n",
      "batch 339, loss: 0.2019, label: 0, bag_size: 2322\n",
      "batch 359, loss: 0.9832, label: 0, bag_size: 6281\n",
      "batch 379, loss: 0.0789, label: 1, bag_size: 4715\n",
      "batch 399, loss: 0.0598, label: 1, bag_size: 8040\n",
      "batch 419, loss: 1.3979, label: 1, bag_size: 7989\n",
      "batch 439, loss: 0.1021, label: 1, bag_size: 3674\n",
      "batch 459, loss: 0.0565, label: 1, bag_size: 6927\n",
      "batch 479, loss: 0.2466, label: 0, bag_size: 6850\n",
      "batch 499, loss: 0.0421, label: 0, bag_size: 17482\n",
      "batch 519, loss: 0.0489, label: 1, bag_size: 5894\n",
      "batch 539, loss: 0.1053, label: 0, bag_size: 29270\n",
      "batch 559, loss: 0.0349, label: 1, bag_size: 15213\n",
      "batch 579, loss: 0.2462, label: 1, bag_size: 4821\n",
      "batch 599, loss: 0.2562, label: 1, bag_size: 9649\n",
      "batch 619, loss: 0.1059, label: 0, bag_size: 6727\n",
      "batch 639, loss: 0.6847, label: 1, bag_size: 2682\n",
      "batch 659, loss: 0.1804, label: 0, bag_size: 29270\n",
      "batch 679, loss: 1.7560, label: 1, bag_size: 2937\n",
      "batch 699, loss: 0.3285, label: 0, bag_size: 4271\n",
      "batch 719, loss: 0.7884, label: 1, bag_size: 15563\n",
      "batch 739, loss: 0.0179, label: 1, bag_size: 2485\n",
      "batch 759, loss: 0.0101, label: 0, bag_size: 8372\n",
      "batch 779, loss: 0.0442, label: 0, bag_size: 25027\n",
      "batch 799, loss: 0.7672, label: 1, bag_size: 12714\n",
      "batch 819, loss: 3.8467, label: 0, bag_size: 3802\n",
      "Epoch: 3, train_loss: 0.2981, train_error: 0.1046\n",
      "class 0: acc 0.9111675126903553, correct 359/394\n",
      "class 1: acc 0.8808411214953271, correct 377/428\n",
      "\n",
      "Val Set, val_loss: 0.3026, val_error: 0.1284, auc: 0.9496\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.8095238095238095, correct 51/63\n",
      "Validation loss decreased (0.315499 --> 0.302573).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1103, label: 1, bag_size: 9230\n",
      "batch 39, loss: 0.1659, label: 1, bag_size: 9533\n",
      "batch 59, loss: 0.6879, label: 1, bag_size: 2146\n",
      "batch 79, loss: 0.1820, label: 0, bag_size: 25558\n",
      "batch 99, loss: 0.3208, label: 1, bag_size: 3856\n",
      "batch 119, loss: 0.1976, label: 1, bag_size: 8012\n",
      "batch 139, loss: 0.0109, label: 1, bag_size: 3453\n",
      "batch 159, loss: 0.0433, label: 0, bag_size: 3232\n",
      "batch 179, loss: 0.2287, label: 1, bag_size: 6752\n",
      "batch 199, loss: 0.0119, label: 0, bag_size: 13964\n",
      "batch 219, loss: 0.0532, label: 1, bag_size: 19932\n",
      "batch 239, loss: 0.0386, label: 1, bag_size: 4877\n",
      "batch 259, loss: 0.2466, label: 0, bag_size: 8420\n",
      "batch 279, loss: 2.9789, label: 0, bag_size: 3468\n",
      "batch 299, loss: 1.9816, label: 1, bag_size: 1284\n",
      "batch 319, loss: 1.9991, label: 0, bag_size: 47866\n",
      "batch 339, loss: 0.6042, label: 0, bag_size: 11922\n",
      "batch 359, loss: 0.0232, label: 0, bag_size: 3459\n",
      "batch 379, loss: 0.0473, label: 0, bag_size: 1881\n",
      "batch 399, loss: 0.1473, label: 1, bag_size: 7798\n",
      "batch 419, loss: 0.0569, label: 0, bag_size: 19435\n",
      "batch 439, loss: 0.4596, label: 1, bag_size: 16514\n",
      "batch 459, loss: 0.0205, label: 1, bag_size: 4367\n",
      "batch 479, loss: 0.1531, label: 1, bag_size: 2356\n",
      "batch 499, loss: 0.0239, label: 1, bag_size: 8040\n",
      "batch 519, loss: 0.0543, label: 0, bag_size: 4497\n",
      "batch 539, loss: 0.0217, label: 1, bag_size: 9971\n",
      "batch 559, loss: 0.0042, label: 0, bag_size: 1052\n",
      "batch 579, loss: 0.0224, label: 1, bag_size: 12095\n",
      "batch 599, loss: 0.0448, label: 0, bag_size: 11146\n",
      "batch 619, loss: 0.0200, label: 1, bag_size: 699\n",
      "batch 639, loss: 0.1530, label: 0, bag_size: 1142\n",
      "batch 659, loss: 0.4856, label: 1, bag_size: 7515\n",
      "batch 679, loss: 0.9925, label: 1, bag_size: 21252\n",
      "batch 699, loss: 0.1596, label: 0, bag_size: 9060\n",
      "batch 719, loss: 0.0846, label: 0, bag_size: 7235\n",
      "batch 739, loss: 1.3993, label: 1, bag_size: 1191\n",
      "batch 759, loss: 0.1168, label: 1, bag_size: 8754\n",
      "batch 779, loss: 0.1684, label: 1, bag_size: 8680\n",
      "batch 799, loss: 0.0332, label: 0, bag_size: 1826\n",
      "batch 819, loss: 0.6429, label: 0, bag_size: 4997\n",
      "Epoch: 4, train_loss: 0.3053, train_error: 0.1034\n",
      "class 0: acc 0.9016786570743405, correct 376/417\n",
      "class 1: acc 0.891358024691358, correct 361/405\n",
      "\n",
      "Val Set, val_loss: 0.3449, val_error: 0.1376, auc: 0.9538\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.7777777777777778, correct 49/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0065, label: 0, bag_size: 8372\n",
      "batch 39, loss: 0.1580, label: 0, bag_size: 7989\n",
      "batch 59, loss: 0.5098, label: 0, bag_size: 2270\n",
      "batch 79, loss: 0.0123, label: 0, bag_size: 1438\n",
      "batch 99, loss: 0.0170, label: 1, bag_size: 13786\n",
      "batch 119, loss: 0.0585, label: 1, bag_size: 10072\n",
      "batch 139, loss: 0.0389, label: 0, bag_size: 2063\n",
      "batch 159, loss: 0.0186, label: 0, bag_size: 16782\n",
      "batch 179, loss: 0.0983, label: 1, bag_size: 4423\n",
      "batch 199, loss: 0.1781, label: 1, bag_size: 7583\n",
      "batch 219, loss: 0.0723, label: 1, bag_size: 4054\n",
      "batch 239, loss: 0.0470, label: 1, bag_size: 9446\n",
      "batch 259, loss: 1.0611, label: 0, bag_size: 20555\n",
      "batch 279, loss: 0.0304, label: 1, bag_size: 12349\n",
      "batch 299, loss: 0.0091, label: 1, bag_size: 1412\n",
      "batch 319, loss: 0.0285, label: 1, bag_size: 9971\n",
      "batch 339, loss: 0.0249, label: 1, bag_size: 10392\n",
      "batch 359, loss: 0.0163, label: 1, bag_size: 2412\n",
      "batch 379, loss: 0.0773, label: 1, bag_size: 21701\n",
      "batch 399, loss: 0.1045, label: 0, bag_size: 3670\n",
      "batch 419, loss: 0.5425, label: 0, bag_size: 1732\n",
      "batch 439, loss: 0.0492, label: 0, bag_size: 18215\n",
      "batch 459, loss: 0.0125, label: 1, bag_size: 4715\n",
      "batch 479, loss: 0.0024, label: 0, bag_size: 9433\n",
      "batch 499, loss: 0.7942, label: 0, bag_size: 20230\n",
      "batch 519, loss: 0.2587, label: 0, bag_size: 2043\n",
      "batch 539, loss: 0.1966, label: 0, bag_size: 2624\n",
      "batch 559, loss: 0.0057, label: 1, bag_size: 2904\n",
      "batch 579, loss: 0.0205, label: 0, bag_size: 18954\n",
      "batch 599, loss: 0.2303, label: 1, bag_size: 15192\n",
      "batch 619, loss: 0.2067, label: 0, bag_size: 6850\n",
      "batch 639, loss: 0.0143, label: 0, bag_size: 31106\n",
      "batch 659, loss: 0.1345, label: 1, bag_size: 8438\n",
      "batch 679, loss: 0.2137, label: 0, bag_size: 31085\n",
      "batch 699, loss: 0.0141, label: 1, bag_size: 12349\n",
      "batch 719, loss: 0.1186, label: 0, bag_size: 15898\n",
      "batch 739, loss: 0.0103, label: 1, bag_size: 7873\n",
      "batch 759, loss: 0.0216, label: 0, bag_size: 14956\n",
      "batch 779, loss: 0.2843, label: 0, bag_size: 11390\n",
      "batch 799, loss: 0.2470, label: 0, bag_size: 6281\n",
      "batch 819, loss: 0.2449, label: 1, bag_size: 9548\n",
      "Epoch: 5, train_loss: 0.2475, train_error: 0.0803\n",
      "class 0: acc 0.9333333333333333, correct 392/420\n",
      "class 1: acc 0.9054726368159204, correct 364/402\n",
      "\n",
      "Val Set, val_loss: 0.6757, val_error: 0.2477, auc: 0.9465\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.5873015873015873, correct 37/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0949, label: 1, bag_size: 6736\n",
      "batch 39, loss: 0.0019, label: 1, bag_size: 5731\n",
      "batch 59, loss: 0.9960, label: 1, bag_size: 2937\n",
      "batch 79, loss: 0.0033, label: 1, bag_size: 11122\n",
      "batch 99, loss: 0.1228, label: 0, bag_size: 2760\n",
      "batch 119, loss: 0.1077, label: 0, bag_size: 25420\n",
      "batch 139, loss: 0.0142, label: 1, bag_size: 3453\n",
      "batch 159, loss: 0.0023, label: 1, bag_size: 13255\n",
      "batch 179, loss: 0.1667, label: 1, bag_size: 1867\n",
      "batch 199, loss: 0.0343, label: 0, bag_size: 9583\n",
      "batch 219, loss: 0.1487, label: 1, bag_size: 1123\n",
      "batch 239, loss: 0.1304, label: 0, bag_size: 18516\n",
      "batch 259, loss: 0.5393, label: 1, bag_size: 13089\n",
      "batch 279, loss: 0.0143, label: 1, bag_size: 4039\n",
      "batch 299, loss: 0.0169, label: 0, bag_size: 22828\n",
      "batch 319, loss: 0.2320, label: 1, bag_size: 15192\n",
      "batch 339, loss: 0.0206, label: 0, bag_size: 4959\n",
      "batch 359, loss: 0.0102, label: 0, bag_size: 2063\n",
      "batch 379, loss: 0.0580, label: 0, bag_size: 1920\n",
      "batch 399, loss: 0.0945, label: 0, bag_size: 15464\n",
      "batch 419, loss: 0.0045, label: 1, bag_size: 5991\n",
      "batch 439, loss: 0.0679, label: 0, bag_size: 10721\n",
      "batch 459, loss: 0.0121, label: 1, bag_size: 3450\n",
      "batch 479, loss: 1.0457, label: 0, bag_size: 15003\n",
      "batch 499, loss: 0.0134, label: 1, bag_size: 21009\n",
      "batch 519, loss: 0.0323, label: 0, bag_size: 19472\n",
      "batch 539, loss: 0.1210, label: 0, bag_size: 1592\n",
      "batch 559, loss: 0.0163, label: 1, bag_size: 14887\n",
      "batch 579, loss: 0.0926, label: 1, bag_size: 13015\n",
      "batch 599, loss: 0.0362, label: 1, bag_size: 10033\n",
      "batch 619, loss: 0.0129, label: 0, bag_size: 18225\n",
      "batch 639, loss: 0.1747, label: 0, bag_size: 2004\n",
      "batch 659, loss: 0.0035, label: 1, bag_size: 1412\n",
      "batch 679, loss: 0.0020, label: 1, bag_size: 8019\n",
      "batch 699, loss: 0.1660, label: 0, bag_size: 3474\n",
      "batch 719, loss: 0.0036, label: 1, bag_size: 13368\n",
      "batch 739, loss: 0.0060, label: 1, bag_size: 9971\n",
      "batch 759, loss: 0.0903, label: 1, bag_size: 1014\n",
      "batch 779, loss: 0.0999, label: 1, bag_size: 11266\n",
      "batch 799, loss: 0.0326, label: 0, bag_size: 13225\n",
      "batch 819, loss: 2.0689, label: 1, bag_size: 1038\n",
      "Epoch: 6, train_loss: 0.2923, train_error: 0.1168\n",
      "class 0: acc 0.8891752577319587, correct 345/388\n",
      "class 1: acc 0.8778801843317973, correct 381/434\n",
      "\n",
      "Val Set, val_loss: 0.3661, val_error: 0.1376, auc: 0.9472\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.7777777777777778, correct 49/63\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1467, label: 0, bag_size: 24911\n",
      "batch 39, loss: 0.2302, label: 1, bag_size: 4308\n",
      "batch 59, loss: 0.0655, label: 1, bag_size: 7613\n",
      "batch 79, loss: 0.2763, label: 0, bag_size: 15898\n",
      "batch 99, loss: 0.7598, label: 1, bag_size: 7389\n",
      "batch 119, loss: 0.0515, label: 0, bag_size: 10898\n",
      "batch 139, loss: 0.0050, label: 1, bag_size: 14618\n",
      "batch 159, loss: 0.1031, label: 1, bag_size: 2848\n",
      "batch 179, loss: 0.0033, label: 1, bag_size: 2904\n",
      "batch 199, loss: 0.0136, label: 0, bag_size: 23037\n",
      "batch 219, loss: 0.2494, label: 0, bag_size: 10721\n",
      "batch 239, loss: 0.4036, label: 0, bag_size: 4523\n",
      "batch 259, loss: 0.0034, label: 1, bag_size: 629\n",
      "batch 279, loss: 1.5544, label: 1, bag_size: 1038\n",
      "batch 299, loss: 0.0922, label: 0, bag_size: 19043\n",
      "batch 319, loss: 0.0571, label: 0, bag_size: 15057\n",
      "batch 339, loss: 0.1361, label: 1, bag_size: 13732\n",
      "batch 359, loss: 0.0259, label: 1, bag_size: 7381\n",
      "batch 379, loss: 0.0151, label: 1, bag_size: 8935\n",
      "batch 399, loss: 0.5126, label: 1, bag_size: 21252\n",
      "batch 419, loss: 0.1128, label: 1, bag_size: 3980\n",
      "batch 439, loss: 0.1075, label: 0, bag_size: 7011\n",
      "batch 459, loss: 0.0152, label: 0, bag_size: 18954\n",
      "batch 479, loss: 0.1450, label: 0, bag_size: 11390\n",
      "batch 499, loss: 0.6229, label: 1, bag_size: 11256\n",
      "batch 519, loss: 0.0338, label: 1, bag_size: 7371\n",
      "batch 539, loss: 0.0188, label: 1, bag_size: 8592\n",
      "batch 559, loss: 0.8632, label: 0, bag_size: 1701\n",
      "batch 579, loss: 1.8374, label: 1, bag_size: 11386\n",
      "batch 599, loss: 0.0268, label: 1, bag_size: 3656\n",
      "batch 619, loss: 0.0232, label: 1, bag_size: 5763\n",
      "batch 639, loss: 0.0870, label: 1, bag_size: 13732\n",
      "batch 659, loss: 0.0025, label: 1, bag_size: 3003\n",
      "batch 679, loss: 0.0232, label: 1, bag_size: 10033\n",
      "batch 699, loss: 0.0401, label: 0, bag_size: 15077\n",
      "batch 719, loss: 0.0009, label: 1, bag_size: 5317\n",
      "batch 739, loss: 0.2767, label: 0, bag_size: 3654\n",
      "batch 759, loss: 0.0402, label: 1, bag_size: 8660\n",
      "batch 779, loss: 0.0416, label: 1, bag_size: 13015\n",
      "batch 799, loss: 0.0037, label: 1, bag_size: 9955\n",
      "batch 819, loss: 0.1173, label: 0, bag_size: 12561\n",
      "Epoch: 7, train_loss: 0.2438, train_error: 0.0803\n",
      "class 0: acc 0.9314720812182741, correct 367/394\n",
      "class 1: acc 0.9088785046728972, correct 389/428\n",
      "\n",
      "Val Set, val_loss: 0.2812, val_error: 0.1101, auc: 0.9503\n",
      "class 0: acc 0.8913043478260869, correct 41/46\n",
      "class 1: acc 0.8888888888888888, correct 56/63\n",
      "Validation loss decreased (0.302573 --> 0.281245).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0305, label: 0, bag_size: 4959\n",
      "batch 39, loss: 0.0939, label: 0, bag_size: 6093\n",
      "batch 59, loss: 0.6462, label: 0, bag_size: 15003\n",
      "batch 79, loss: 2.5271, label: 1, bag_size: 2937\n",
      "batch 99, loss: 0.1012, label: 0, bag_size: 2918\n",
      "batch 119, loss: 0.2185, label: 1, bag_size: 9649\n",
      "batch 139, loss: 0.1195, label: 0, bag_size: 17268\n",
      "batch 159, loss: 0.0006, label: 1, bag_size: 7078\n",
      "batch 179, loss: 0.0373, label: 1, bag_size: 9747\n",
      "batch 199, loss: 0.1139, label: 1, bag_size: 9446\n",
      "batch 219, loss: 0.0026, label: 1, bag_size: 15665\n",
      "batch 239, loss: 0.0057, label: 0, bag_size: 10535\n",
      "batch 259, loss: 0.2367, label: 1, bag_size: 5110\n",
      "batch 279, loss: 0.0035, label: 0, bag_size: 1052\n",
      "batch 299, loss: 0.4432, label: 0, bag_size: 2458\n",
      "batch 319, loss: 0.0490, label: 0, bag_size: 3474\n",
      "batch 339, loss: 0.1748, label: 0, bag_size: 29270\n",
      "batch 359, loss: 0.0253, label: 1, bag_size: 5155\n",
      "batch 379, loss: 0.2803, label: 1, bag_size: 4929\n",
      "batch 399, loss: 0.5966, label: 1, bag_size: 1497\n",
      "batch 419, loss: 1.1912, label: 1, bag_size: 2814\n",
      "batch 439, loss: 0.0034, label: 0, bag_size: 9885\n",
      "batch 459, loss: 0.0062, label: 0, bag_size: 9786\n",
      "batch 479, loss: 0.0494, label: 0, bag_size: 931\n",
      "batch 499, loss: 0.0771, label: 1, bag_size: 7445\n",
      "batch 519, loss: 0.0077, label: 1, bag_size: 19500\n",
      "batch 539, loss: 0.0071, label: 1, bag_size: 2638\n",
      "batch 559, loss: 0.0476, label: 1, bag_size: 5690\n",
      "batch 579, loss: 0.2535, label: 1, bag_size: 10501\n",
      "batch 599, loss: 0.1632, label: 1, bag_size: 7613\n",
      "batch 619, loss: 0.0813, label: 0, bag_size: 16087\n",
      "batch 639, loss: 0.4197, label: 0, bag_size: 30828\n",
      "batch 659, loss: 0.1542, label: 0, bag_size: 4523\n",
      "batch 679, loss: 0.0283, label: 1, bag_size: 5441\n",
      "batch 699, loss: 0.0582, label: 0, bag_size: 1415\n",
      "batch 719, loss: 0.0059, label: 1, bag_size: 13786\n",
      "batch 739, loss: 0.0034, label: 0, bag_size: 21218\n",
      "batch 759, loss: 0.0608, label: 0, bag_size: 5161\n",
      "batch 779, loss: 0.0207, label: 0, bag_size: 17368\n",
      "batch 799, loss: 0.1063, label: 1, bag_size: 34356\n",
      "batch 819, loss: 1.2328, label: 1, bag_size: 1230\n",
      "Epoch: 8, train_loss: 0.2651, train_error: 0.0973\n",
      "class 0: acc 0.9212410501193318, correct 386/419\n",
      "class 1: acc 0.8833746898263027, correct 356/403\n",
      "\n",
      "Val Set, val_loss: 0.2949, val_error: 0.1193, auc: 0.9545\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.8095238095238095, correct 51/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0279, label: 1, bag_size: 5991\n",
      "batch 39, loss: 0.0021, label: 1, bag_size: 7110\n",
      "batch 59, loss: 1.4099, label: 0, bag_size: 7428\n",
      "batch 79, loss: 0.0060, label: 0, bag_size: 17155\n",
      "batch 99, loss: 0.0130, label: 1, bag_size: 699\n",
      "batch 119, loss: 0.0051, label: 1, bag_size: 21009\n",
      "batch 139, loss: 0.2501, label: 1, bag_size: 7989\n",
      "batch 159, loss: 0.0069, label: 1, bag_size: 14223\n",
      "batch 179, loss: 0.0119, label: 0, bag_size: 1483\n",
      "batch 199, loss: 0.0256, label: 0, bag_size: 2760\n",
      "batch 219, loss: 0.2887, label: 1, bag_size: 11701\n",
      "batch 239, loss: 0.0453, label: 1, bag_size: 7515\n",
      "batch 259, loss: 0.0087, label: 1, bag_size: 7371\n",
      "batch 279, loss: 0.0667, label: 1, bag_size: 6205\n",
      "batch 299, loss: 0.1252, label: 1, bag_size: 13015\n",
      "batch 319, loss: 0.2426, label: 0, bag_size: 15003\n",
      "batch 339, loss: 0.0057, label: 1, bag_size: 1022\n",
      "batch 359, loss: 0.0491, label: 0, bag_size: 8549\n",
      "batch 379, loss: 0.0016, label: 1, bag_size: 2485\n",
      "batch 399, loss: 0.1728, label: 0, bag_size: 23714\n",
      "batch 419, loss: 0.0641, label: 0, bag_size: 21138\n",
      "batch 439, loss: 0.1648, label: 0, bag_size: 14739\n",
      "batch 459, loss: 0.0060, label: 1, bag_size: 7110\n",
      "batch 479, loss: 0.0901, label: 1, bag_size: 15125\n",
      "batch 499, loss: 0.0869, label: 0, bag_size: 1772\n",
      "batch 519, loss: 0.0293, label: 0, bag_size: 3541\n",
      "batch 539, loss: 0.7785, label: 1, bag_size: 1242\n",
      "batch 559, loss: 0.1522, label: 0, bag_size: 1202\n",
      "batch 579, loss: 0.1563, label: 1, bag_size: 4956\n",
      "batch 599, loss: 1.2418, label: 1, bag_size: 21450\n",
      "batch 619, loss: 0.0080, label: 0, bag_size: 14206\n",
      "batch 639, loss: 0.0482, label: 1, bag_size: 5612\n",
      "batch 659, loss: 0.2836, label: 0, bag_size: 18738\n",
      "batch 679, loss: 0.1545, label: 0, bag_size: 3876\n",
      "batch 699, loss: 0.0041, label: 1, bag_size: 4480\n",
      "batch 719, loss: 0.0390, label: 0, bag_size: 2732\n",
      "batch 739, loss: 0.0025, label: 0, bag_size: 11122\n",
      "batch 759, loss: 0.0373, label: 1, bag_size: 3004\n",
      "batch 779, loss: 0.2643, label: 1, bag_size: 6928\n",
      "batch 799, loss: 0.0068, label: 1, bag_size: 2904\n",
      "batch 819, loss: 0.0577, label: 0, bag_size: 6367\n",
      "Epoch: 9, train_loss: 0.2340, train_error: 0.0876\n",
      "class 0: acc 0.9045226130653267, correct 360/398\n",
      "class 1: acc 0.9198113207547169, correct 390/424\n",
      "\n",
      "Val Set, val_loss: 0.3954, val_error: 0.1560, auc: 0.9562\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.746031746031746, correct 47/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0386, label: 1, bag_size: 20333\n",
      "batch 39, loss: 0.8511, label: 1, bag_size: 2314\n",
      "batch 59, loss: 0.0582, label: 0, bag_size: 8755\n",
      "batch 79, loss: 0.1004, label: 0, bag_size: 3557\n",
      "batch 99, loss: 0.0115, label: 0, bag_size: 10898\n",
      "batch 119, loss: 0.0050, label: 1, bag_size: 2405\n",
      "batch 139, loss: 0.3251, label: 0, bag_size: 15071\n",
      "batch 159, loss: 0.7397, label: 0, bag_size: 3710\n",
      "batch 179, loss: 0.1685, label: 0, bag_size: 6093\n",
      "batch 199, loss: 0.2955, label: 0, bag_size: 15003\n",
      "batch 219, loss: 1.1000, label: 0, bag_size: 7835\n",
      "batch 239, loss: 0.0045, label: 0, bag_size: 3459\n",
      "batch 259, loss: 0.0044, label: 0, bag_size: 12217\n",
      "batch 279, loss: 0.0124, label: 0, bag_size: 15967\n",
      "batch 299, loss: 0.0420, label: 1, bag_size: 11684\n",
      "batch 319, loss: 0.2154, label: 0, bag_size: 12083\n",
      "batch 339, loss: 0.5347, label: 0, bag_size: 3783\n",
      "batch 359, loss: 0.0870, label: 1, bag_size: 5921\n",
      "batch 379, loss: 0.0015, label: 1, bag_size: 5340\n",
      "batch 399, loss: 0.1170, label: 0, bag_size: 2458\n",
      "batch 419, loss: 0.0032, label: 1, bag_size: 15665\n",
      "batch 439, loss: 1.2501, label: 0, bag_size: 4692\n",
      "batch 459, loss: 0.1505, label: 1, bag_size: 9561\n",
      "batch 479, loss: 0.2074, label: 0, bag_size: 23618\n",
      "batch 499, loss: 0.0014, label: 0, bag_size: 1483\n",
      "batch 519, loss: 0.0929, label: 0, bag_size: 18516\n",
      "batch 539, loss: 0.0355, label: 1, bag_size: 10498\n",
      "batch 559, loss: 0.3841, label: 1, bag_size: 11684\n",
      "batch 579, loss: 0.0131, label: 1, bag_size: 16034\n",
      "batch 599, loss: 0.4896, label: 0, bag_size: 3228\n",
      "batch 619, loss: 0.0043, label: 1, bag_size: 14223\n",
      "batch 639, loss: 0.0558, label: 1, bag_size: 8680\n",
      "batch 659, loss: 0.0279, label: 1, bag_size: 8685\n",
      "batch 679, loss: 0.0044, label: 1, bag_size: 15716\n",
      "batch 699, loss: 0.0207, label: 0, bag_size: 30751\n",
      "batch 719, loss: 0.0257, label: 1, bag_size: 13026\n",
      "batch 739, loss: 0.0145, label: 1, bag_size: 5690\n",
      "batch 759, loss: 0.0000, label: 1, bag_size: 11195\n",
      "batch 779, loss: 1.6601, label: 0, bag_size: 4692\n",
      "batch 799, loss: 0.3402, label: 1, bag_size: 10622\n",
      "batch 819, loss: 0.2590, label: 0, bag_size: 3198\n",
      "Epoch: 10, train_loss: 0.2427, train_error: 0.0998\n",
      "class 0: acc 0.910941475826972, correct 358/393\n",
      "class 1: acc 0.8904428904428905, correct 382/429\n",
      "\n",
      "Val Set, val_loss: 0.2741, val_error: 0.1193, auc: 0.9531\n",
      "class 0: acc 0.8478260869565217, correct 39/46\n",
      "class 1: acc 0.9047619047619048, correct 57/63\n",
      "Validation loss decreased (0.281245 --> 0.274097).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0370, label: 0, bag_size: 11187\n",
      "batch 39, loss: 0.1837, label: 1, bag_size: 3651\n",
      "batch 59, loss: 0.0321, label: 1, bag_size: 2193\n",
      "batch 79, loss: 0.0223, label: 0, bag_size: 14681\n",
      "batch 99, loss: 0.2345, label: 0, bag_size: 11922\n",
      "batch 119, loss: 0.0944, label: 0, bag_size: 4241\n",
      "batch 139, loss: 0.6672, label: 1, bag_size: 5310\n",
      "batch 159, loss: 0.0322, label: 0, bag_size: 14681\n",
      "batch 179, loss: 0.0822, label: 0, bag_size: 1920\n",
      "batch 199, loss: 0.0038, label: 0, bag_size: 20150\n",
      "batch 219, loss: 0.0724, label: 0, bag_size: 4465\n",
      "batch 239, loss: 0.0350, label: 0, bag_size: 11527\n",
      "batch 259, loss: 0.1384, label: 0, bag_size: 14885\n",
      "batch 279, loss: 0.0021, label: 0, bag_size: 16341\n",
      "batch 299, loss: 0.1412, label: 1, bag_size: 2455\n",
      "batch 319, loss: 0.0571, label: 1, bag_size: 1746\n",
      "batch 339, loss: 0.2876, label: 0, bag_size: 9132\n",
      "batch 359, loss: 0.0044, label: 0, bag_size: 3190\n",
      "batch 379, loss: 0.0103, label: 0, bag_size: 14266\n",
      "batch 399, loss: 0.2906, label: 1, bag_size: 3224\n",
      "batch 419, loss: 0.0038, label: 1, bag_size: 9571\n",
      "batch 439, loss: 0.0228, label: 1, bag_size: 10848\n",
      "batch 459, loss: 0.0075, label: 1, bag_size: 2405\n",
      "batch 479, loss: 0.0483, label: 0, bag_size: 13218\n",
      "batch 499, loss: 0.0028, label: 1, bag_size: 3437\n",
      "batch 519, loss: 0.0459, label: 0, bag_size: 19880\n",
      "batch 539, loss: 0.1865, label: 1, bag_size: 2681\n",
      "batch 559, loss: 0.0050, label: 1, bag_size: 13194\n",
      "batch 579, loss: 0.0132, label: 1, bag_size: 1746\n",
      "batch 599, loss: 0.1886, label: 0, bag_size: 18738\n",
      "batch 619, loss: 0.1708, label: 1, bag_size: 7148\n",
      "batch 639, loss: 0.0253, label: 0, bag_size: 21138\n",
      "batch 659, loss: 0.0007, label: 1, bag_size: 10482\n",
      "batch 679, loss: 0.1625, label: 1, bag_size: 1437\n",
      "batch 699, loss: 0.0027, label: 1, bag_size: 8410\n",
      "batch 719, loss: 0.0005, label: 0, bag_size: 8372\n",
      "batch 739, loss: 0.0203, label: 1, bag_size: 2695\n",
      "batch 759, loss: 0.4060, label: 0, bag_size: 3444\n",
      "batch 779, loss: 0.0414, label: 0, bag_size: 2004\n",
      "batch 799, loss: 0.0143, label: 1, bag_size: 9913\n",
      "batch 819, loss: 0.0245, label: 0, bag_size: 11735\n",
      "Epoch: 11, train_loss: 0.2222, train_error: 0.0888\n",
      "class 0: acc 0.9227053140096618, correct 382/414\n",
      "class 1: acc 0.8995098039215687, correct 367/408\n",
      "\n",
      "Val Set, val_loss: 0.2748, val_error: 0.1193, auc: 0.9538\n",
      "class 0: acc 0.8478260869565217, correct 39/46\n",
      "class 1: acc 0.9047619047619048, correct 57/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0535, label: 0, bag_size: 7923\n",
      "batch 39, loss: 0.0378, label: 0, bag_size: 8981\n",
      "batch 59, loss: 0.0330, label: 1, bag_size: 16703\n",
      "batch 79, loss: 0.2094, label: 1, bag_size: 7583\n",
      "batch 99, loss: 0.0018, label: 0, bag_size: 2748\n",
      "batch 119, loss: 0.0203, label: 0, bag_size: 19390\n",
      "batch 139, loss: 1.1914, label: 1, bag_size: 1051\n",
      "batch 159, loss: 0.0020, label: 1, bag_size: 2936\n",
      "batch 179, loss: 0.0229, label: 1, bag_size: 21701\n",
      "batch 199, loss: 0.3754, label: 0, bag_size: 2043\n",
      "batch 219, loss: 0.1471, label: 0, bag_size: 23714\n",
      "batch 239, loss: 0.2354, label: 0, bag_size: 9616\n",
      "batch 259, loss: 0.0003, label: 1, bag_size: 5221\n",
      "batch 279, loss: 0.0058, label: 0, bag_size: 803\n",
      "batch 299, loss: 0.0521, label: 0, bag_size: 2160\n",
      "batch 319, loss: 0.1776, label: 0, bag_size: 1800\n",
      "batch 339, loss: 0.0065, label: 0, bag_size: 17368\n",
      "batch 359, loss: 0.0338, label: 0, bag_size: 11727\n",
      "batch 379, loss: 0.1036, label: 1, bag_size: 15093\n",
      "batch 399, loss: 0.0077, label: 0, bag_size: 31780\n",
      "batch 419, loss: 0.0075, label: 0, bag_size: 11690\n",
      "batch 439, loss: 0.0012, label: 1, bag_size: 19606\n",
      "batch 459, loss: 0.1913, label: 1, bag_size: 29832\n",
      "batch 479, loss: 0.1031, label: 1, bag_size: 16703\n",
      "batch 499, loss: 0.0031, label: 1, bag_size: 13194\n",
      "batch 519, loss: 0.0024, label: 1, bag_size: 11642\n",
      "batch 539, loss: 0.0170, label: 0, bag_size: 19067\n",
      "batch 559, loss: 0.0890, label: 0, bag_size: 8959\n",
      "batch 579, loss: 0.0994, label: 0, bag_size: 4465\n",
      "batch 599, loss: 0.0022, label: 1, bag_size: 5049\n",
      "batch 619, loss: 0.0132, label: 0, bag_size: 10721\n",
      "batch 639, loss: 0.0217, label: 0, bag_size: 11512\n",
      "batch 659, loss: 0.0188, label: 1, bag_size: 10033\n",
      "batch 679, loss: 0.0322, label: 1, bag_size: 22264\n",
      "batch 699, loss: 0.2710, label: 0, bag_size: 9387\n",
      "batch 719, loss: 0.1105, label: 1, bag_size: 9649\n",
      "batch 739, loss: 0.2923, label: 0, bag_size: 3557\n",
      "batch 759, loss: 0.0354, label: 0, bag_size: 2732\n",
      "batch 779, loss: 0.0082, label: 1, bag_size: 19500\n",
      "batch 799, loss: 0.4701, label: 0, bag_size: 20478\n",
      "batch 819, loss: 0.0253, label: 1, bag_size: 15689\n",
      "Epoch: 12, train_loss: 0.2188, train_error: 0.0815\n",
      "class 0: acc 0.9295774647887324, correct 396/426\n",
      "class 1: acc 0.9065656565656566, correct 359/396\n",
      "\n",
      "Val Set, val_loss: 0.3564, val_error: 0.1376, auc: 0.9517\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.7777777777777778, correct 49/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0277, label: 1, bag_size: 3450\n",
      "batch 39, loss: 0.0086, label: 0, bag_size: 16607\n",
      "batch 59, loss: 0.0129, label: 0, bag_size: 1213\n",
      "batch 79, loss: 0.0162, label: 1, bag_size: 3937\n",
      "batch 99, loss: 0.0036, label: 0, bag_size: 3190\n",
      "batch 119, loss: 0.0005, label: 0, bag_size: 18574\n",
      "batch 139, loss: 0.0027, label: 0, bag_size: 17368\n",
      "batch 159, loss: 0.0096, label: 1, bag_size: 4394\n",
      "batch 179, loss: 0.0020, label: 0, bag_size: 12137\n",
      "batch 199, loss: 0.0113, label: 0, bag_size: 14305\n",
      "batch 219, loss: 0.0407, label: 1, bag_size: 13015\n",
      "batch 239, loss: 0.0104, label: 0, bag_size: 11865\n",
      "batch 259, loss: 0.0204, label: 1, bag_size: 1786\n",
      "batch 279, loss: 0.0846, label: 0, bag_size: 15736\n",
      "batch 299, loss: 0.0299, label: 1, bag_size: 7119\n",
      "batch 319, loss: 0.0500, label: 0, bag_size: 16211\n",
      "batch 339, loss: 0.1344, label: 0, bag_size: 20555\n",
      "batch 359, loss: 0.0393, label: 1, bag_size: 8438\n",
      "batch 379, loss: 0.0367, label: 0, bag_size: 11727\n",
      "batch 399, loss: 0.0129, label: 0, bag_size: 10898\n",
      "batch 419, loss: 0.0570, label: 0, bag_size: 2351\n",
      "batch 439, loss: 0.0074, label: 1, bag_size: 16512\n",
      "batch 459, loss: 0.5742, label: 0, bag_size: 11607\n",
      "batch 479, loss: 0.0132, label: 0, bag_size: 19472\n",
      "batch 499, loss: 0.0273, label: 1, bag_size: 2405\n",
      "batch 519, loss: 0.4066, label: 1, bag_size: 2842\n",
      "batch 539, loss: 0.0280, label: 0, bag_size: 23714\n",
      "batch 559, loss: 0.0193, label: 1, bag_size: 5454\n",
      "batch 579, loss: 0.0234, label: 0, bag_size: 2079\n",
      "batch 599, loss: 0.0175, label: 0, bag_size: 4497\n",
      "batch 619, loss: 1.0431, label: 0, bag_size: 11922\n",
      "batch 639, loss: 0.1098, label: 1, bag_size: 6599\n",
      "batch 659, loss: 0.0203, label: 1, bag_size: 9971\n",
      "batch 679, loss: 0.0005, label: 0, bag_size: 2091\n",
      "batch 699, loss: 0.1492, label: 0, bag_size: 14893\n",
      "batch 719, loss: 0.0039, label: 0, bag_size: 20150\n",
      "batch 739, loss: 0.0007, label: 0, bag_size: 3459\n",
      "batch 759, loss: 0.6259, label: 0, bag_size: 2959\n",
      "batch 779, loss: 0.0038, label: 1, bag_size: 20161\n",
      "batch 799, loss: 0.0048, label: 0, bag_size: 11199\n",
      "batch 819, loss: 0.0317, label: 0, bag_size: 763\n",
      "Epoch: 13, train_loss: 0.2340, train_error: 0.0742\n",
      "class 0: acc 0.9285714285714286, correct 377/406\n",
      "class 1: acc 0.9230769230769231, correct 384/416\n",
      "\n",
      "Val Set, val_loss: 0.3572, val_error: 0.1468, auc: 0.9538\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.7619047619047619, correct 48/63\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0032, label: 0, bag_size: 27158\n",
      "batch 39, loss: 0.0175, label: 0, bag_size: 14828\n",
      "batch 59, loss: 0.0009, label: 0, bag_size: 13964\n",
      "batch 79, loss: 0.1022, label: 0, bag_size: 19470\n",
      "batch 99, loss: 0.0433, label: 1, bag_size: 2193\n",
      "batch 119, loss: 0.0494, label: 0, bag_size: 3670\n",
      "batch 139, loss: 0.2067, label: 1, bag_size: 13089\n",
      "batch 159, loss: 0.0390, label: 1, bag_size: 7515\n",
      "batch 179, loss: 0.0031, label: 0, bag_size: 13964\n",
      "batch 199, loss: 0.0157, label: 1, bag_size: 9548\n",
      "batch 219, loss: 0.0022, label: 0, bag_size: 20150\n",
      "batch 239, loss: 0.0014, label: 0, bag_size: 18574\n",
      "batch 259, loss: 0.0578, label: 0, bag_size: 18215\n",
      "batch 279, loss: 0.0122, label: 1, bag_size: 16034\n",
      "batch 299, loss: 0.0803, label: 1, bag_size: 6205\n",
      "batch 319, loss: 0.1070, label: 0, bag_size: 5161\n",
      "batch 339, loss: 0.0062, label: 1, bag_size: 5723\n",
      "batch 359, loss: 0.0442, label: 0, bag_size: 14377\n",
      "batch 379, loss: 0.0186, label: 0, bag_size: 24911\n",
      "batch 399, loss: 0.1554, label: 1, bag_size: 1242\n",
      "batch 419, loss: 0.0022, label: 0, bag_size: 12687\n",
      "batch 439, loss: 0.0684, label: 0, bag_size: 8959\n",
      "batch 459, loss: 0.0800, label: 0, bag_size: 763\n",
      "batch 479, loss: 0.1224, label: 1, bag_size: 1920\n",
      "batch 499, loss: 1.5976, label: 1, bag_size: 12340\n",
      "batch 519, loss: 0.0270, label: 0, bag_size: 11122\n",
      "batch 539, loss: 3.0597, label: 0, bag_size: 2815\n",
      "batch 559, loss: 0.3060, label: 1, bag_size: 4715\n",
      "batch 579, loss: 0.8397, label: 1, bag_size: 2681\n",
      "batch 599, loss: 0.0166, label: 0, bag_size: 19390\n",
      "batch 619, loss: 0.0314, label: 0, bag_size: 2160\n",
      "batch 639, loss: 0.0626, label: 0, bag_size: 2079\n",
      "batch 659, loss: 0.0516, label: 0, bag_size: 16087\n",
      "batch 679, loss: 0.0030, label: 0, bag_size: 21218\n",
      "batch 699, loss: 0.0083, label: 0, bag_size: 17633\n",
      "batch 719, loss: 0.0170, label: 0, bag_size: 10490\n",
      "batch 739, loss: 0.0981, label: 0, bag_size: 21361\n",
      "batch 759, loss: 0.0828, label: 1, bag_size: 9689\n",
      "batch 779, loss: 0.0068, label: 1, bag_size: 10394\n",
      "batch 799, loss: 0.0723, label: 1, bag_size: 14681\n",
      "batch 819, loss: 0.0042, label: 0, bag_size: 31106\n",
      "Epoch: 14, train_loss: 0.2349, train_error: 0.0888\n",
      "class 0: acc 0.9158653846153846, correct 381/416\n",
      "class 1: acc 0.9064039408866995, correct 368/406\n",
      "\n",
      "Val Set, val_loss: 0.2787, val_error: 0.1101, auc: 0.9555\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0059, label: 1, bag_size: 14618\n",
      "batch 39, loss: 0.0924, label: 1, bag_size: 21827\n",
      "batch 59, loss: 0.3617, label: 0, bag_size: 7612\n",
      "batch 79, loss: 0.0989, label: 0, bag_size: 2996\n",
      "batch 99, loss: 0.5142, label: 0, bag_size: 3444\n",
      "batch 119, loss: 0.0011, label: 0, bag_size: 17630\n",
      "batch 139, loss: 3.8222, label: 1, bag_size: 1038\n",
      "batch 159, loss: 0.0110, label: 1, bag_size: 3674\n",
      "batch 179, loss: 0.0227, label: 0, bag_size: 1712\n",
      "batch 199, loss: 0.0415, label: 0, bag_size: 30751\n",
      "batch 219, loss: 0.7187, label: 0, bag_size: 8427\n",
      "batch 239, loss: 0.0944, label: 0, bag_size: 1142\n",
      "batch 259, loss: 0.0216, label: 0, bag_size: 2548\n",
      "batch 279, loss: 0.2434, label: 0, bag_size: 5409\n",
      "batch 299, loss: 0.3401, label: 1, bag_size: 15192\n",
      "batch 319, loss: 1.6040, label: 0, bag_size: 47866\n",
      "batch 339, loss: 0.0026, label: 0, bag_size: 16341\n",
      "batch 359, loss: 0.2525, label: 0, bag_size: 3708\n",
      "batch 379, loss: 0.0095, label: 1, bag_size: 4250\n",
      "batch 399, loss: 0.0007, label: 1, bag_size: 6752\n",
      "batch 419, loss: 0.0053, label: 1, bag_size: 8602\n",
      "batch 439, loss: 2.2993, label: 0, bag_size: 2732\n",
      "batch 459, loss: 0.0537, label: 0, bag_size: 11390\n",
      "batch 479, loss: 0.0095, label: 0, bag_size: 21138\n",
      "batch 499, loss: 0.0184, label: 1, bag_size: 2278\n",
      "batch 519, loss: 0.0060, label: 1, bag_size: 2412\n",
      "batch 539, loss: 0.0028, label: 0, bag_size: 2820\n",
      "batch 559, loss: 0.3251, label: 0, bag_size: 10410\n",
      "batch 579, loss: 0.0018, label: 0, bag_size: 7191\n",
      "batch 599, loss: 0.3686, label: 0, bag_size: 2270\n",
      "batch 619, loss: 0.0013, label: 0, bag_size: 12148\n",
      "batch 639, loss: 0.0237, label: 1, bag_size: 9955\n",
      "batch 659, loss: 0.0344, label: 1, bag_size: 5345\n",
      "batch 679, loss: 0.0018, label: 1, bag_size: 4317\n",
      "batch 699, loss: 0.0436, label: 0, bag_size: 2236\n",
      "batch 719, loss: 0.2326, label: 0, bag_size: 5211\n",
      "batch 739, loss: 0.0098, label: 0, bag_size: 1712\n",
      "batch 759, loss: 0.0206, label: 1, bag_size: 13786\n",
      "batch 779, loss: 0.0299, label: 0, bag_size: 2490\n",
      "batch 799, loss: 0.0736, label: 1, bag_size: 9877\n",
      "batch 819, loss: 0.0007, label: 1, bag_size: 1360\n",
      "Epoch: 15, train_loss: 0.2645, train_error: 0.0998\n",
      "class 0: acc 0.9147465437788018, correct 397/434\n",
      "class 1: acc 0.884020618556701, correct 343/388\n",
      "\n",
      "Val Set, val_loss: 0.2751, val_error: 0.1284, auc: 0.9565\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.8253968253968254, correct 52/63\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0016, label: 1, bag_size: 16512\n",
      "batch 39, loss: 0.0180, label: 0, bag_size: 32227\n",
      "batch 59, loss: 0.0329, label: 1, bag_size: 1746\n",
      "batch 79, loss: 0.1851, label: 0, bag_size: 931\n",
      "batch 99, loss: 0.0056, label: 1, bag_size: 2638\n",
      "batch 119, loss: 0.1892, label: 0, bag_size: 1800\n",
      "batch 139, loss: 0.3176, label: 1, bag_size: 1294\n",
      "batch 159, loss: 0.0172, label: 0, bag_size: 22681\n",
      "batch 179, loss: 0.0016, label: 0, bag_size: 9433\n",
      "batch 199, loss: 0.1019, label: 1, bag_size: 1064\n",
      "batch 219, loss: 0.0010, label: 1, bag_size: 8522\n",
      "batch 239, loss: 0.1679, label: 1, bag_size: 3224\n",
      "batch 259, loss: 0.0534, label: 1, bag_size: 4821\n",
      "batch 279, loss: 0.0231, label: 1, bag_size: 14681\n",
      "batch 299, loss: 0.0122, label: 0, bag_size: 18954\n",
      "batch 319, loss: 0.0066, label: 1, bag_size: 19932\n",
      "batch 339, loss: 0.0048, label: 1, bag_size: 5256\n",
      "batch 359, loss: 0.1263, label: 0, bag_size: 25814\n",
      "batch 379, loss: 0.0266, label: 1, bag_size: 7119\n",
      "batch 399, loss: 0.3160, label: 0, bag_size: 20230\n",
      "batch 419, loss: 0.1642, label: 1, bag_size: 7148\n",
      "batch 439, loss: 0.0843, label: 1, bag_size: 2682\n",
      "batch 459, loss: 0.0049, label: 0, bag_size: 11690\n",
      "batch 479, loss: 0.0519, label: 1, bag_size: 2848\n",
      "batch 499, loss: 0.0028, label: 1, bag_size: 6734\n",
      "batch 519, loss: 0.0025, label: 0, bag_size: 3459\n",
      "batch 539, loss: 0.0571, label: 0, bag_size: 9930\n",
      "batch 559, loss: 0.1628, label: 0, bag_size: 9471\n",
      "batch 579, loss: 0.6095, label: 0, bag_size: 1714\n",
      "batch 599, loss: 0.0042, label: 1, bag_size: 2412\n",
      "batch 619, loss: 0.0401, label: 1, bag_size: 1064\n",
      "batch 639, loss: 0.0167, label: 0, bag_size: 4497\n",
      "batch 659, loss: 1.0116, label: 1, bag_size: 1497\n",
      "batch 679, loss: 0.0906, label: 0, bag_size: 19808\n",
      "batch 699, loss: 0.0685, label: 1, bag_size: 12697\n",
      "batch 719, loss: 0.2079, label: 1, bag_size: 7066\n",
      "batch 739, loss: 0.0056, label: 0, bag_size: 16607\n",
      "batch 759, loss: 1.3957, label: 0, bag_size: 7835\n",
      "batch 779, loss: 0.0342, label: 0, bag_size: 13332\n",
      "batch 799, loss: 0.0191, label: 1, bag_size: 5155\n",
      "batch 819, loss: 0.0017, label: 0, bag_size: 10995\n",
      "Epoch: 16, train_loss: 0.1947, train_error: 0.0657\n",
      "class 0: acc 0.9335038363171355, correct 365/391\n",
      "class 1: acc 0.9350348027842227, correct 403/431\n",
      "\n",
      "Val Set, val_loss: 0.3059, val_error: 0.1193, auc: 0.9569\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.8253968253968254, correct 52/63\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0033, label: 0, bag_size: 2236\n",
      "batch 39, loss: 0.0182, label: 0, bag_size: 16782\n",
      "batch 59, loss: 0.6277, label: 0, bag_size: 9597\n",
      "batch 79, loss: 0.1561, label: 0, bag_size: 3541\n",
      "batch 99, loss: 0.0126, label: 1, bag_size: 5605\n",
      "batch 119, loss: 0.0439, label: 0, bag_size: 11187\n",
      "batch 139, loss: 0.0382, label: 0, bag_size: 3198\n",
      "batch 159, loss: 0.8750, label: 0, bag_size: 3876\n",
      "batch 179, loss: 0.1540, label: 1, bag_size: 11684\n",
      "batch 199, loss: 0.0240, label: 1, bag_size: 1459\n",
      "batch 219, loss: 3.1235, label: 0, bag_size: 2815\n",
      "batch 239, loss: 0.1564, label: 1, bag_size: 7445\n",
      "batch 259, loss: 0.0761, label: 1, bag_size: 12895\n",
      "batch 279, loss: 0.0280, label: 0, bag_size: 1920\n",
      "batch 299, loss: 0.0162, label: 0, bag_size: 2179\n",
      "batch 319, loss: 0.0092, label: 1, bag_size: 6927\n",
      "batch 339, loss: 1.0019, label: 1, bag_size: 12494\n",
      "batch 359, loss: 0.0039, label: 0, bag_size: 21093\n",
      "batch 379, loss: 0.0083, label: 0, bag_size: 14305\n",
      "batch 399, loss: 0.1148, label: 0, bag_size: 7031\n",
      "batch 419, loss: 0.0043, label: 0, bag_size: 32227\n",
      "batch 439, loss: 0.0010, label: 1, bag_size: 10112\n",
      "batch 459, loss: 0.0006, label: 1, bag_size: 17486\n",
      "batch 479, loss: 0.0758, label: 1, bag_size: 14230\n",
      "batch 499, loss: 0.0370, label: 1, bag_size: 15609\n",
      "batch 519, loss: 0.0092, label: 1, bag_size: 9971\n",
      "batch 539, loss: 0.0100, label: 0, bag_size: 11735\n",
      "batch 559, loss: 0.0108, label: 0, bag_size: 17791\n",
      "batch 579, loss: 0.0387, label: 0, bag_size: 31085\n",
      "batch 599, loss: 0.0037, label: 1, bag_size: 6343\n",
      "batch 619, loss: 0.3863, label: 1, bag_size: 7445\n",
      "batch 639, loss: 0.0483, label: 1, bag_size: 12425\n",
      "batch 659, loss: 1.1464, label: 0, bag_size: 2270\n",
      "batch 679, loss: 0.0165, label: 0, bag_size: 15898\n",
      "batch 699, loss: 0.0255, label: 1, bag_size: 6781\n",
      "batch 719, loss: 0.1063, label: 1, bag_size: 3980\n",
      "batch 739, loss: 0.0811, label: 0, bag_size: 9387\n",
      "batch 759, loss: 0.1623, label: 0, bag_size: 2652\n",
      "batch 779, loss: 0.0467, label: 1, bag_size: 4054\n",
      "batch 799, loss: 0.0143, label: 0, bag_size: 3670\n",
      "batch 819, loss: 0.0171, label: 0, bag_size: 18240\n",
      "Epoch: 17, train_loss: 0.2162, train_error: 0.0693\n",
      "class 0: acc 0.9344660194174758, correct 385/412\n",
      "class 1: acc 0.926829268292683, correct 380/410\n",
      "\n",
      "Val Set, val_loss: 0.3692, val_error: 0.1376, auc: 0.9569\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.7777777777777778, correct 49/63\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0039, label: 1, bag_size: 4959\n",
      "batch 39, loss: 0.3095, label: 1, bag_size: 2682\n",
      "batch 59, loss: 0.0138, label: 1, bag_size: 4715\n",
      "batch 79, loss: 0.0038, label: 1, bag_size: 6734\n",
      "batch 99, loss: 0.0075, label: 0, bag_size: 22828\n",
      "batch 119, loss: 0.0291, label: 0, bag_size: 1825\n",
      "batch 139, loss: 0.0787, label: 0, bag_size: 9387\n",
      "batch 159, loss: 0.9140, label: 0, bag_size: 2458\n",
      "batch 179, loss: 0.2031, label: 1, bag_size: 7798\n",
      "batch 199, loss: 0.1203, label: 1, bag_size: 4054\n",
      "batch 219, loss: 0.0295, label: 0, bag_size: 16211\n",
      "batch 239, loss: 0.0334, label: 0, bag_size: 8812\n",
      "batch 259, loss: 0.2273, label: 0, bag_size: 21864\n",
      "batch 279, loss: 0.0360, label: 0, bag_size: 13332\n",
      "batch 299, loss: 0.0105, label: 0, bag_size: 10898\n",
      "batch 319, loss: 0.3521, label: 1, bag_size: 2681\n",
      "batch 339, loss: 0.0679, label: 1, bag_size: 15931\n",
      "batch 359, loss: 0.1581, label: 1, bag_size: 13786\n",
      "batch 379, loss: 0.0064, label: 1, bag_size: 6769\n",
      "batch 399, loss: 0.2095, label: 1, bag_size: 1064\n",
      "batch 419, loss: 0.0256, label: 1, bag_size: 3856\n",
      "batch 439, loss: 0.0010, label: 1, bag_size: 10394\n",
      "batch 459, loss: 0.0246, label: 1, bag_size: 16565\n",
      "batch 479, loss: 0.0405, label: 0, bag_size: 26271\n",
      "batch 499, loss: 0.0072, label: 1, bag_size: 10969\n",
      "batch 519, loss: 0.0464, label: 1, bag_size: 3674\n",
      "batch 539, loss: 0.3686, label: 0, bag_size: 5211\n",
      "batch 559, loss: 0.0469, label: 0, bag_size: 8866\n",
      "batch 579, loss: 0.0852, label: 1, bag_size: 12575\n",
      "batch 599, loss: 0.0175, label: 0, bag_size: 6727\n",
      "batch 619, loss: 0.0031, label: 0, bag_size: 14828\n",
      "batch 639, loss: 0.0198, label: 0, bag_size: 21076\n",
      "batch 659, loss: 0.0044, label: 0, bag_size: 20666\n",
      "batch 679, loss: 0.0077, label: 0, bag_size: 9433\n",
      "batch 699, loss: 0.0112, label: 0, bag_size: 13777\n",
      "batch 719, loss: 0.1919, label: 1, bag_size: 7468\n",
      "batch 739, loss: 0.0093, label: 1, bag_size: 2278\n",
      "batch 759, loss: 0.0574, label: 1, bag_size: 2092\n",
      "batch 779, loss: 0.4177, label: 1, bag_size: 2140\n",
      "batch 799, loss: 0.0230, label: 1, bag_size: 3937\n",
      "batch 819, loss: 0.7196, label: 0, bag_size: 2270\n",
      "Epoch: 18, train_loss: 0.2030, train_error: 0.0730\n",
      "class 0: acc 0.9265822784810127, correct 366/395\n",
      "class 1: acc 0.927400468384075, correct 396/427\n",
      "\n",
      "Val Set, val_loss: 0.3779, val_error: 0.1468, auc: 0.9555\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.7619047619047619, correct 48/63\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0012, label: 1, bag_size: 6343\n",
      "batch 39, loss: 0.0004, label: 1, bag_size: 4128\n",
      "batch 59, loss: 0.2108, label: 1, bag_size: 7066\n",
      "batch 79, loss: 0.0027, label: 0, bag_size: 20150\n",
      "batch 99, loss: 0.0006, label: 1, bag_size: 2579\n",
      "batch 119, loss: 0.0272, label: 1, bag_size: 18603\n",
      "batch 139, loss: 0.0936, label: 0, bag_size: 23618\n",
      "batch 159, loss: 0.0942, label: 1, bag_size: 9689\n",
      "batch 179, loss: 0.0178, label: 0, bag_size: 1920\n",
      "batch 199, loss: 0.2825, label: 1, bag_size: 6928\n",
      "batch 219, loss: 0.2549, label: 0, bag_size: 2457\n",
      "batch 239, loss: 0.0317, label: 0, bag_size: 7557\n",
      "batch 259, loss: 0.8378, label: 1, bag_size: 1444\n",
      "batch 279, loss: 0.0021, label: 1, bag_size: 6734\n",
      "batch 299, loss: 0.1584, label: 0, bag_size: 3725\n",
      "batch 319, loss: 0.0202, label: 0, bag_size: 7637\n",
      "batch 339, loss: 0.0238, label: 1, bag_size: 2480\n",
      "batch 359, loss: 0.0292, label: 0, bag_size: 9470\n",
      "batch 379, loss: 0.0303, label: 0, bag_size: 15841\n",
      "batch 399, loss: 0.0123, label: 0, bag_size: 16992\n",
      "batch 419, loss: 0.1071, label: 1, bag_size: 8448\n",
      "batch 439, loss: 0.0015, label: 0, bag_size: 2628\n",
      "batch 459, loss: 0.0246, label: 0, bag_size: 17268\n",
      "batch 479, loss: 0.0467, label: 0, bag_size: 1789\n",
      "batch 499, loss: 0.0105, label: 0, bag_size: 2282\n",
      "batch 519, loss: 0.0690, label: 0, bag_size: 2920\n",
      "batch 539, loss: 2.1217, label: 1, bag_size: 1703\n",
      "batch 559, loss: 0.3028, label: 1, bag_size: 771\n",
      "batch 579, loss: 0.0028, label: 1, bag_size: 2904\n",
      "batch 599, loss: 0.0405, label: 0, bag_size: 10263\n",
      "batch 619, loss: 0.4545, label: 0, bag_size: 10146\n",
      "batch 639, loss: 0.0197, label: 1, bag_size: 16379\n",
      "batch 659, loss: 0.0007, label: 1, bag_size: 2966\n",
      "batch 679, loss: 1.7686, label: 1, bag_size: 1038\n",
      "batch 699, loss: 0.0285, label: 1, bag_size: 34356\n",
      "batch 719, loss: 0.0139, label: 1, bag_size: 14779\n",
      "batch 739, loss: 0.0006, label: 1, bag_size: 10482\n",
      "batch 759, loss: 0.0329, label: 0, bag_size: 2367\n",
      "batch 779, loss: 0.0014, label: 0, bag_size: 14305\n",
      "batch 799, loss: 0.0415, label: 0, bag_size: 6281\n",
      "batch 819, loss: 0.0428, label: 0, bag_size: 15057\n",
      "Epoch: 19, train_loss: 0.2194, train_error: 0.0827\n",
      "class 0: acc 0.9158653846153846, correct 381/416\n",
      "class 1: acc 0.9187192118226601, correct 373/406\n",
      "\n",
      "Val Set, val_loss: 0.3814, val_error: 0.1284, auc: 0.9551\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.7936507936507936, correct 50/63\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3217, label: 0, bag_size: 1701\n",
      "batch 39, loss: 0.0011, label: 1, bag_size: 11642\n",
      "batch 59, loss: 0.3700, label: 0, bag_size: 7428\n",
      "batch 79, loss: 0.0796, label: 1, bag_size: 13015\n",
      "batch 99, loss: 0.0597, label: 1, bag_size: 15192\n",
      "batch 119, loss: 0.0052, label: 0, bag_size: 4959\n",
      "batch 139, loss: 0.0067, label: 0, bag_size: 31085\n",
      "batch 159, loss: 0.0013, label: 1, bag_size: 6090\n",
      "batch 179, loss: 0.0278, label: 0, bag_size: 11735\n",
      "batch 199, loss: 0.0080, label: 1, bag_size: 8003\n",
      "batch 219, loss: 0.2008, label: 1, bag_size: 13089\n",
      "batch 239, loss: 0.2189, label: 1, bag_size: 2559\n",
      "batch 259, loss: 0.0142, label: 1, bag_size: 12697\n",
      "batch 279, loss: 0.0018, label: 1, bag_size: 6090\n",
      "batch 299, loss: 0.0032, label: 1, bag_size: 4102\n",
      "batch 319, loss: 0.0050, label: 0, bag_size: 12131\n",
      "batch 339, loss: 0.0104, label: 1, bag_size: 1249\n",
      "batch 359, loss: 0.0312, label: 0, bag_size: 17268\n",
      "batch 379, loss: 0.1462, label: 0, bag_size: 13023\n",
      "batch 399, loss: 0.1665, label: 0, bag_size: 6093\n",
      "batch 419, loss: 0.0019, label: 0, bag_size: 803\n",
      "batch 439, loss: 0.0084, label: 0, bag_size: 4465\n",
      "batch 459, loss: 0.0002, label: 0, bag_size: 9433\n",
      "batch 479, loss: 0.0151, label: 1, bag_size: 22264\n",
      "batch 499, loss: 0.0011, label: 1, bag_size: 7381\n",
      "batch 519, loss: 0.0209, label: 1, bag_size: 2480\n",
      "batch 539, loss: 0.0276, label: 1, bag_size: 2785\n",
      "batch 559, loss: 0.0089, label: 0, bag_size: 19466\n",
      "batch 579, loss: 3.8295, label: 0, bag_size: 3897\n",
      "batch 599, loss: 0.0005, label: 0, bag_size: 19659\n",
      "batch 619, loss: 0.0016, label: 1, bag_size: 19932\n",
      "batch 639, loss: 0.6395, label: 0, bag_size: 21361\n",
      "batch 659, loss: 0.0016, label: 0, bag_size: 7191\n",
      "batch 679, loss: 1.6521, label: 0, bag_size: 2959\n",
      "batch 699, loss: 0.0160, label: 0, bag_size: 18132\n",
      "batch 719, loss: 0.1754, label: 1, bag_size: 4308\n",
      "batch 739, loss: 0.0071, label: 0, bag_size: 8582\n",
      "batch 759, loss: 0.0256, label: 0, bag_size: 8866\n",
      "batch 779, loss: 0.0239, label: 0, bag_size: 1592\n",
      "batch 799, loss: 2.7649, label: 1, bag_size: 1533\n",
      "batch 819, loss: 0.3958, label: 1, bag_size: 3879\n",
      "Epoch: 20, train_loss: 0.1935, train_error: 0.0633\n",
      "class 0: acc 0.9358974358974359, correct 365/390\n",
      "class 1: acc 0.9375, correct 405/432\n",
      "\n",
      "Val Set, val_loss: 0.2557, val_error: 0.0917, auc: 0.9596\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.8888888888888888, correct 56/63\n",
      "Validation loss decreased (0.274097 --> 0.255700).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0033, label: 1, bag_size: 11220\n",
      "batch 39, loss: 0.3689, label: 0, bag_size: 4241\n",
      "batch 59, loss: 0.0031, label: 0, bag_size: 14305\n",
      "batch 79, loss: 0.0017, label: 1, bag_size: 7873\n",
      "batch 99, loss: 0.0639, label: 0, bag_size: 26208\n",
      "batch 119, loss: 0.1079, label: 1, bag_size: 4789\n",
      "batch 139, loss: 0.2460, label: 0, bag_size: 1690\n",
      "batch 159, loss: 0.0005, label: 1, bag_size: 10112\n",
      "batch 179, loss: 0.0233, label: 1, bag_size: 3368\n",
      "batch 199, loss: 0.0152, label: 1, bag_size: 1888\n",
      "batch 219, loss: 0.0003, label: 1, bag_size: 5340\n",
      "batch 239, loss: 0.0237, label: 0, bag_size: 11383\n",
      "batch 259, loss: 0.0005, label: 1, bag_size: 9065\n",
      "batch 279, loss: 0.0280, label: 1, bag_size: 7613\n",
      "batch 299, loss: 0.0086, label: 1, bag_size: 12460\n",
      "batch 319, loss: 0.0018, label: 1, bag_size: 11642\n",
      "batch 339, loss: 0.0987, label: 1, bag_size: 3656\n",
      "batch 359, loss: 0.0045, label: 1, bag_size: 11884\n",
      "batch 379, loss: 0.0373, label: 1, bag_size: 5763\n",
      "batch 399, loss: 0.0608, label: 1, bag_size: 7066\n",
      "batch 419, loss: 0.0001, label: 1, bag_size: 7110\n",
      "batch 439, loss: 0.0010, label: 1, bag_size: 9571\n",
      "batch 459, loss: 0.0015, label: 1, bag_size: 1412\n",
      "batch 479, loss: 0.0006, label: 0, bag_size: 1052\n",
      "batch 499, loss: 0.2200, label: 1, bag_size: 1483\n",
      "batch 519, loss: 0.0013, label: 1, bag_size: 3968\n",
      "batch 539, loss: 0.0010, label: 1, bag_size: 4317\n",
      "batch 559, loss: 0.0255, label: 0, bag_size: 6884\n",
      "batch 579, loss: 0.0001, label: 1, bag_size: 9571\n",
      "batch 599, loss: 0.0272, label: 1, bag_size: 5441\n",
      "batch 619, loss: 0.0277, label: 0, bag_size: 11512\n",
      "batch 639, loss: 0.1241, label: 0, bag_size: 23714\n",
      "batch 659, loss: 0.0838, label: 1, bag_size: 2455\n",
      "batch 679, loss: 0.0812, label: 0, bag_size: 1213\n",
      "batch 699, loss: 0.0027, label: 1, bag_size: 20537\n",
      "batch 719, loss: 0.0000, label: 1, bag_size: 10867\n",
      "batch 739, loss: 0.0291, label: 0, bag_size: 2351\n",
      "batch 759, loss: 0.7067, label: 1, bag_size: 9330\n",
      "batch 779, loss: 0.2368, label: 1, bag_size: 6478\n",
      "batch 799, loss: 0.1035, label: 0, bag_size: 9387\n",
      "batch 819, loss: 0.0797, label: 0, bag_size: 7557\n",
      "Epoch: 21, train_loss: 0.1745, train_error: 0.0681\n",
      "class 0: acc 0.9384236453201971, correct 381/406\n",
      "class 1: acc 0.9254807692307693, correct 385/416\n",
      "\n",
      "Val Set, val_loss: 0.3575, val_error: 0.1376, auc: 0.9579\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.7777777777777778, correct 49/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1714, label: 0, bag_size: 3876\n",
      "batch 39, loss: 0.0037, label: 1, bag_size: 15213\n",
      "batch 59, loss: 0.0277, label: 1, bag_size: 13440\n",
      "batch 79, loss: 1.2496, label: 1, bag_size: 1703\n",
      "batch 99, loss: 0.0014, label: 0, bag_size: 13225\n",
      "batch 119, loss: 0.0116, label: 1, bag_size: 2686\n",
      "batch 139, loss: 0.0012, label: 0, bag_size: 12212\n",
      "batch 159, loss: 0.0430, label: 0, bag_size: 25558\n",
      "batch 179, loss: 0.2056, label: 1, bag_size: 2814\n",
      "batch 199, loss: 0.0225, label: 1, bag_size: 6731\n",
      "batch 219, loss: 0.0101, label: 0, bag_size: 17268\n",
      "batch 239, loss: 0.0004, label: 0, bag_size: 7191\n",
      "batch 259, loss: 0.2219, label: 0, bag_size: 2609\n",
      "batch 279, loss: 0.3432, label: 1, bag_size: 15931\n",
      "batch 299, loss: 0.0007, label: 1, bag_size: 28527\n",
      "batch 319, loss: 0.0003, label: 1, bag_size: 3409\n",
      "batch 339, loss: 0.1283, label: 1, bag_size: 1242\n",
      "batch 359, loss: 0.0146, label: 0, bag_size: 21385\n",
      "batch 379, loss: 0.0052, label: 0, bag_size: 27158\n",
      "batch 399, loss: 0.2420, label: 1, bag_size: 7989\n",
      "batch 419, loss: 0.0310, label: 1, bag_size: 9533\n",
      "batch 439, loss: 0.0021, label: 1, bag_size: 4480\n",
      "batch 459, loss: 0.0265, label: 1, bag_size: 12946\n",
      "batch 479, loss: 0.0577, label: 1, bag_size: 8660\n",
      "batch 499, loss: 0.0387, label: 0, bag_size: 9485\n",
      "batch 519, loss: 0.0341, label: 0, bag_size: 1498\n",
      "batch 539, loss: 0.0012, label: 0, bag_size: 14956\n",
      "batch 559, loss: 0.0024, label: 0, bag_size: 16782\n",
      "batch 579, loss: 0.1747, label: 1, bag_size: 6842\n",
      "batch 599, loss: 0.0016, label: 0, bag_size: 11527\n",
      "batch 619, loss: 0.0263, label: 1, bag_size: 8660\n",
      "batch 639, loss: 0.0070, label: 1, bag_size: 6927\n",
      "batch 659, loss: 0.0082, label: 1, bag_size: 10396\n",
      "batch 679, loss: 0.0824, label: 1, bag_size: 621\n",
      "batch 699, loss: 0.0119, label: 0, bag_size: 3725\n",
      "batch 719, loss: 0.0003, label: 1, bag_size: 3576\n",
      "batch 739, loss: 0.0094, label: 1, bag_size: 7217\n",
      "batch 759, loss: 2.7149, label: 0, bag_size: 5105\n",
      "batch 779, loss: 0.0499, label: 1, bag_size: 16890\n",
      "batch 799, loss: 0.0001, label: 0, bag_size: 10481\n",
      "batch 819, loss: 0.0105, label: 0, bag_size: 10898\n",
      "Epoch: 22, train_loss: 0.1760, train_error: 0.0584\n",
      "class 0: acc 0.9433497536945813, correct 383/406\n",
      "class 1: acc 0.9399038461538461, correct 391/416\n",
      "\n",
      "Val Set, val_loss: 0.2820, val_error: 0.1009, auc: 0.9617\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0014, label: 1, bag_size: 6734\n",
      "batch 39, loss: 0.1479, label: 0, bag_size: 12083\n",
      "batch 59, loss: 0.0744, label: 0, bag_size: 8330\n",
      "batch 79, loss: 0.0300, label: 0, bag_size: 2079\n",
      "batch 99, loss: 0.0008, label: 0, bag_size: 17368\n",
      "batch 119, loss: 0.0019, label: 0, bag_size: 9786\n",
      "batch 139, loss: 0.0013, label: 0, bag_size: 1416\n",
      "batch 159, loss: 0.0010, label: 0, bag_size: 11146\n",
      "batch 179, loss: 0.0141, label: 0, bag_size: 2179\n",
      "batch 199, loss: 0.0005, label: 1, bag_size: 3576\n",
      "batch 219, loss: 0.0003, label: 1, bag_size: 9065\n",
      "batch 239, loss: 0.0279, label: 1, bag_size: 8395\n",
      "batch 259, loss: 0.0013, label: 0, bag_size: 12687\n",
      "batch 279, loss: 0.4218, label: 1, bag_size: 4786\n",
      "batch 299, loss: 0.0083, label: 0, bag_size: 14319\n",
      "batch 319, loss: 0.0012, label: 1, bag_size: 3409\n",
      "batch 339, loss: 0.0005, label: 0, bag_size: 13964\n",
      "batch 359, loss: 0.0006, label: 1, bag_size: 5833\n",
      "batch 379, loss: 0.0452, label: 1, bag_size: 3651\n",
      "batch 399, loss: 0.0023, label: 0, bag_size: 15850\n",
      "batch 419, loss: 0.0078, label: 0, bag_size: 19472\n",
      "batch 439, loss: 0.0634, label: 0, bag_size: 30751\n",
      "batch 459, loss: 0.1853, label: 1, bag_size: 13089\n",
      "batch 479, loss: 0.0478, label: 0, bag_size: 18738\n",
      "batch 499, loss: 0.0015, label: 1, bag_size: 19606\n",
      "batch 519, loss: 0.4250, label: 0, bag_size: 11128\n",
      "batch 539, loss: 0.0019, label: 1, bag_size: 6734\n",
      "batch 559, loss: 0.0074, label: 1, bag_size: 9955\n",
      "batch 579, loss: 0.0305, label: 1, bag_size: 5894\n",
      "batch 599, loss: 0.0022, label: 1, bag_size: 13194\n",
      "batch 619, loss: 0.0509, label: 0, bag_size: 1639\n",
      "batch 639, loss: 0.0135, label: 1, bag_size: 16703\n",
      "batch 659, loss: 0.2502, label: 0, bag_size: 9421\n",
      "batch 679, loss: 0.1464, label: 1, bag_size: 1236\n",
      "batch 699, loss: 0.0047, label: 0, bag_size: 11146\n",
      "batch 719, loss: 0.0089, label: 1, bag_size: 5256\n",
      "batch 739, loss: 0.0280, label: 0, bag_size: 14885\n",
      "batch 759, loss: 0.0121, label: 0, bag_size: 1826\n",
      "batch 779, loss: 0.0009, label: 1, bag_size: 3207\n",
      "batch 799, loss: 0.2395, label: 0, bag_size: 1592\n",
      "batch 819, loss: 1.3405, label: 0, bag_size: 2219\n",
      "Epoch: 23, train_loss: 0.1925, train_error: 0.0693\n",
      "class 0: acc 0.9271356783919598, correct 369/398\n",
      "class 1: acc 0.9339622641509434, correct 396/424\n",
      "\n",
      "Val Set, val_loss: 0.2670, val_error: 0.0917, auc: 0.9610\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 3.6948, label: 0, bag_size: 2732\n",
      "batch 39, loss: 0.0130, label: 0, bag_size: 16782\n",
      "batch 59, loss: 0.0156, label: 0, bag_size: 14828\n",
      "batch 79, loss: 0.0075, label: 0, bag_size: 11546\n",
      "batch 99, loss: 0.0009, label: 1, bag_size: 4394\n",
      "batch 119, loss: 0.0132, label: 0, bag_size: 1458\n",
      "batch 139, loss: 0.0176, label: 0, bag_size: 22800\n",
      "batch 159, loss: 0.1454, label: 0, bag_size: 2652\n",
      "batch 179, loss: 0.0013, label: 0, bag_size: 17630\n",
      "batch 199, loss: 0.0023, label: 1, bag_size: 10394\n",
      "batch 219, loss: 0.0025, label: 1, bag_size: 7382\n",
      "batch 239, loss: 0.0003, label: 1, bag_size: 4039\n",
      "batch 259, loss: 0.0599, label: 0, bag_size: 14885\n",
      "batch 279, loss: 0.0305, label: 1, bag_size: 10969\n",
      "batch 299, loss: 0.1926, label: 0, bag_size: 30828\n",
      "batch 319, loss: 0.0205, label: 0, bag_size: 16087\n",
      "batch 339, loss: 0.0329, label: 0, bag_size: 7989\n",
      "batch 359, loss: 0.0087, label: 1, bag_size: 12603\n",
      "batch 379, loss: 0.0004, label: 0, bag_size: 9433\n",
      "batch 399, loss: 0.0153, label: 1, bag_size: 2278\n",
      "batch 419, loss: 0.0028, label: 1, bag_size: 11875\n",
      "batch 439, loss: 0.0064, label: 0, bag_size: 13225\n",
      "batch 459, loss: 0.0003, label: 1, bag_size: 7110\n",
      "batch 479, loss: 0.0033, label: 0, bag_size: 19659\n",
      "batch 499, loss: 0.0018, label: 1, bag_size: 11642\n",
      "batch 519, loss: 0.0064, label: 0, bag_size: 3502\n",
      "batch 539, loss: 0.0076, label: 0, bag_size: 5551\n",
      "batch 559, loss: 0.0029, label: 0, bag_size: 2820\n",
      "batch 579, loss: 0.4650, label: 1, bag_size: 1822\n",
      "batch 599, loss: 0.1932, label: 0, bag_size: 16521\n",
      "batch 619, loss: 0.0065, label: 1, bag_size: 8019\n",
      "batch 639, loss: 0.0480, label: 0, bag_size: 10444\n",
      "batch 659, loss: 0.0017, label: 0, bag_size: 14206\n",
      "batch 679, loss: 0.0027, label: 0, bag_size: 27158\n",
      "batch 699, loss: 0.0122, label: 0, bag_size: 22762\n",
      "batch 719, loss: 0.0108, label: 0, bag_size: 10490\n",
      "batch 739, loss: 0.0314, label: 1, bag_size: 13692\n",
      "batch 759, loss: 0.1934, label: 0, bag_size: 7557\n",
      "batch 779, loss: 0.1491, label: 0, bag_size: 3708\n",
      "batch 799, loss: 0.0009, label: 1, bag_size: 11122\n",
      "batch 819, loss: 0.0006, label: 1, bag_size: 14223\n",
      "Epoch: 24, train_loss: 0.1853, train_error: 0.0645\n",
      "class 0: acc 0.9513381995133819, correct 391/411\n",
      "class 1: acc 0.9197080291970803, correct 378/411\n",
      "\n",
      "Val Set, val_loss: 0.3183, val_error: 0.0917, auc: 0.9579\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0064, label: 0, bag_size: 5225\n",
      "batch 39, loss: 0.0384, label: 1, bag_size: 2790\n",
      "batch 59, loss: 0.0803, label: 0, bag_size: 5161\n",
      "batch 79, loss: 0.0096, label: 1, bag_size: 9747\n",
      "batch 99, loss: 0.0355, label: 0, bag_size: 11122\n",
      "batch 119, loss: 0.0082, label: 0, bag_size: 2652\n",
      "batch 139, loss: 0.0165, label: 1, bag_size: 7613\n",
      "batch 159, loss: 0.0005, label: 0, bag_size: 8252\n",
      "batch 179, loss: 0.0157, label: 0, bag_size: 15636\n",
      "batch 199, loss: 0.0108, label: 1, bag_size: 11600\n",
      "batch 219, loss: 0.1216, label: 1, bag_size: 2842\n",
      "batch 239, loss: 0.1433, label: 0, bag_size: 2920\n",
      "batch 259, loss: 0.0086, label: 0, bag_size: 19043\n",
      "batch 279, loss: 0.0108, label: 1, bag_size: 20333\n",
      "batch 299, loss: 0.0276, label: 1, bag_size: 12575\n",
      "batch 319, loss: 0.0001, label: 1, bag_size: 17486\n",
      "batch 339, loss: 0.0357, label: 0, bag_size: 13332\n",
      "batch 359, loss: 0.4547, label: 0, bag_size: 2070\n",
      "batch 379, loss: 0.0006, label: 1, bag_size: 9759\n",
      "batch 399, loss: 0.0021, label: 0, bag_size: 20150\n",
      "batch 419, loss: 0.0031, label: 1, bag_size: 2579\n",
      "batch 439, loss: 0.0002, label: 1, bag_size: 21009\n",
      "batch 459, loss: 0.0519, label: 0, bag_size: 1142\n",
      "batch 479, loss: 0.0007, label: 1, bag_size: 6731\n",
      "batch 499, loss: 0.0401, label: 0, bag_size: 14377\n",
      "batch 519, loss: 0.0808, label: 0, bag_size: 1814\n",
      "batch 539, loss: 1.2790, label: 0, bag_size: 3710\n",
      "batch 559, loss: 0.0047, label: 1, bag_size: 11220\n",
      "batch 579, loss: 1.4629, label: 0, bag_size: 2070\n",
      "batch 599, loss: 0.0045, label: 0, bag_size: 16720\n",
      "batch 619, loss: 1.2642, label: 1, bag_size: 1444\n",
      "batch 639, loss: 0.3051, label: 0, bag_size: 2458\n",
      "batch 659, loss: 0.0247, label: 0, bag_size: 1549\n",
      "batch 679, loss: 0.3768, label: 1, bag_size: 1444\n",
      "batch 699, loss: 0.0447, label: 0, bag_size: 7235\n",
      "batch 719, loss: 0.3690, label: 0, bag_size: 3238\n",
      "batch 739, loss: 0.0806, label: 0, bag_size: 1690\n",
      "batch 759, loss: 1.8943, label: 1, bag_size: 1703\n",
      "batch 779, loss: 1.1869, label: 0, bag_size: 10113\n",
      "batch 799, loss: 0.0919, label: 0, bag_size: 9387\n",
      "batch 819, loss: 0.0121, label: 0, bag_size: 14319\n",
      "Epoch: 25, train_loss: 0.1661, train_error: 0.0474\n",
      "class 0: acc 0.9563218390804598, correct 416/435\n",
      "class 1: acc 0.9483204134366925, correct 367/387\n",
      "\n",
      "Val Set, val_loss: 0.2861, val_error: 0.0917, auc: 0.9607\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.873015873015873, correct 55/63\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5260, label: 1, bag_size: 21252\n",
      "batch 39, loss: 1.8120, label: 0, bag_size: 11306\n",
      "batch 59, loss: 0.0087, label: 0, bag_size: 1202\n",
      "batch 79, loss: 0.0015, label: 0, bag_size: 8661\n",
      "batch 99, loss: 0.0031, label: 0, bag_size: 9060\n",
      "batch 119, loss: 0.0016, label: 1, bag_size: 3634\n",
      "batch 139, loss: 0.0103, label: 0, bag_size: 25558\n",
      "batch 159, loss: 0.0156, label: 0, bag_size: 21218\n",
      "batch 179, loss: 0.3835, label: 0, bag_size: 25420\n",
      "batch 199, loss: 0.1085, label: 0, bag_size: 2098\n",
      "batch 219, loss: 0.0211, label: 0, bag_size: 1639\n",
      "batch 239, loss: 0.0125, label: 0, bag_size: 19470\n",
      "batch 259, loss: 0.0057, label: 0, bag_size: 11383\n",
      "batch 279, loss: 0.0195, label: 1, bag_size: 1022\n",
      "batch 299, loss: 1.4405, label: 1, bag_size: 15185\n",
      "batch 319, loss: 0.0487, label: 1, bag_size: 12946\n",
      "batch 339, loss: 0.0004, label: 1, bag_size: 14515\n",
      "batch 359, loss: 0.7644, label: 1, bag_size: 1822\n",
      "batch 379, loss: 0.0315, label: 0, bag_size: 9866\n",
      "batch 399, loss: 0.0086, label: 1, bag_size: 13692\n",
      "batch 419, loss: 0.0130, label: 1, bag_size: 16379\n",
      "batch 439, loss: 0.2146, label: 0, bag_size: 7835\n",
      "batch 459, loss: 0.0767, label: 1, bag_size: 7424\n",
      "batch 479, loss: 0.0100, label: 0, bag_size: 4465\n",
      "batch 499, loss: 0.0833, label: 0, bag_size: 24439\n",
      "batch 519, loss: 0.0724, label: 1, bag_size: 4239\n",
      "batch 539, loss: 0.0030, label: 1, bag_size: 18699\n",
      "batch 559, loss: 0.0334, label: 0, bag_size: 7557\n",
      "batch 579, loss: 0.4691, label: 0, bag_size: 3541\n",
      "batch 599, loss: 0.0095, label: 0, bag_size: 1202\n",
      "batch 619, loss: 0.0004, label: 0, bag_size: 2628\n",
      "batch 639, loss: 0.0313, label: 1, bag_size: 12575\n",
      "batch 659, loss: 0.0024, label: 1, bag_size: 12408\n",
      "batch 679, loss: 1.2715, label: 0, bag_size: 26208\n",
      "batch 699, loss: 0.0024, label: 1, bag_size: 6731\n",
      "batch 719, loss: 0.3249, label: 0, bag_size: 16690\n",
      "batch 739, loss: 0.0155, label: 1, bag_size: 3619\n",
      "batch 759, loss: 0.0008, label: 1, bag_size: 10394\n",
      "batch 779, loss: 0.4024, label: 1, bag_size: 1236\n",
      "batch 799, loss: 0.0073, label: 1, bag_size: 19606\n",
      "batch 819, loss: 3.9652, label: 0, bag_size: 2815\n",
      "Epoch: 26, train_loss: 0.1894, train_error: 0.0742\n",
      "class 0: acc 0.933649289099526, correct 394/422\n",
      "class 1: acc 0.9175, correct 367/400\n",
      "\n",
      "Val Set, val_loss: 0.3245, val_error: 0.1009, auc: 0.9545\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.8412698412698413, correct 53/63\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0274, label: 0, bag_size: 1825\n",
      "batch 39, loss: 0.0291, label: 0, bag_size: 2548\n",
      "batch 59, loss: 0.0063, label: 0, bag_size: 12137\n",
      "batch 79, loss: 0.1305, label: 1, bag_size: 8592\n",
      "batch 99, loss: 0.0088, label: 0, bag_size: 10535\n",
      "batch 119, loss: 0.0741, label: 1, bag_size: 3619\n",
      "batch 139, loss: 0.0570, label: 0, bag_size: 15841\n",
      "batch 159, loss: 0.6742, label: 1, bag_size: 1191\n",
      "batch 179, loss: 0.0049, label: 1, bag_size: 7935\n",
      "batch 199, loss: 0.0011, label: 0, bag_size: 12148\n",
      "batch 219, loss: 0.9700, label: 1, bag_size: 7748\n",
      "batch 239, loss: 0.0754, label: 1, bag_size: 2522\n",
      "batch 259, loss: 0.0424, label: 1, bag_size: 25695\n",
      "batch 279, loss: 0.1121, label: 1, bag_size: 1920\n",
      "batch 299, loss: 0.0029, label: 0, bag_size: 2628\n",
      "batch 319, loss: 0.0022, label: 0, bag_size: 14266\n",
      "batch 339, loss: 0.1243, label: 1, bag_size: 3656\n",
      "batch 359, loss: 0.0064, label: 0, bag_size: 12217\n",
      "batch 379, loss: 0.0046, label: 1, bag_size: 4128\n",
      "batch 399, loss: 0.2314, label: 1, bag_size: 2785\n",
      "batch 419, loss: 0.0034, label: 1, bag_size: 15093\n",
      "batch 439, loss: 0.0036, label: 1, bag_size: 1255\n",
      "batch 459, loss: 0.0056, label: 1, bag_size: 1022\n",
      "batch 479, loss: 0.0093, label: 0, bag_size: 2104\n",
      "batch 499, loss: 0.0037, label: 1, bag_size: 15609\n",
      "batch 519, loss: 0.0606, label: 0, bag_size: 2732\n",
      "batch 539, loss: 0.0029, label: 1, bag_size: 6090\n",
      "batch 559, loss: 0.0065, label: 1, bag_size: 6769\n",
      "batch 579, loss: 0.0103, label: 0, bag_size: 9866\n",
      "batch 599, loss: 0.5969, label: 1, bag_size: 7989\n",
      "batch 619, loss: 0.0007, label: 1, bag_size: 18649\n",
      "batch 639, loss: 0.1507, label: 1, bag_size: 6825\n",
      "batch 659, loss: 0.0625, label: 0, bag_size: 15841\n",
      "batch 679, loss: 1.5115, label: 1, bag_size: 21252\n",
      "batch 699, loss: 0.0004, label: 1, bag_size: 4394\n",
      "batch 719, loss: 0.0002, label: 1, bag_size: 11387\n",
      "batch 739, loss: 0.0370, label: 1, bag_size: 1875\n",
      "batch 759, loss: 0.0063, label: 0, bag_size: 10942\n",
      "batch 779, loss: 0.1845, label: 1, bag_size: 16154\n",
      "batch 799, loss: 0.1530, label: 0, bag_size: 1684\n",
      "batch 819, loss: 0.2063, label: 0, bag_size: 3238\n",
      "Epoch: 27, train_loss: 0.1746, train_error: 0.0681\n",
      "class 0: acc 0.9394673123486683, correct 388/413\n",
      "class 1: acc 0.9242053789731052, correct 378/409\n",
      "\n",
      "Val Set, val_loss: 0.3019, val_error: 0.1101, auc: 0.9586\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.8412698412698413, correct 53/63\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0005, label: 1, bag_size: 3409\n",
      "batch 39, loss: 0.0035, label: 1, bag_size: 21827\n",
      "batch 59, loss: 0.3436, label: 0, bag_size: 2968\n",
      "batch 79, loss: 0.0113, label: 1, bag_size: 2405\n",
      "batch 99, loss: 0.0257, label: 0, bag_size: 19067\n",
      "batch 119, loss: 0.0268, label: 0, bag_size: 2760\n",
      "batch 139, loss: 0.0147, label: 1, bag_size: 4956\n",
      "batch 159, loss: 0.1184, label: 0, bag_size: 1592\n",
      "batch 179, loss: 0.0209, label: 0, bag_size: 14681\n",
      "batch 199, loss: 0.0134, label: 0, bag_size: 12217\n",
      "batch 219, loss: 0.3706, label: 0, bag_size: 4997\n",
      "batch 239, loss: 0.0009, label: 1, bag_size: 4317\n",
      "batch 259, loss: 0.3548, label: 0, bag_size: 9616\n",
      "batch 279, loss: 0.0929, label: 1, bag_size: 549\n",
      "batch 299, loss: 0.0598, label: 1, bag_size: 4789\n",
      "batch 319, loss: 0.0000, label: 1, bag_size: 4862\n",
      "batch 339, loss: 0.0267, label: 1, bag_size: 9561\n",
      "batch 359, loss: 0.0149, label: 0, bag_size: 2044\n",
      "batch 379, loss: 0.0010, label: 1, bag_size: 4259\n",
      "batch 399, loss: 0.1438, label: 1, bag_size: 12178\n",
      "batch 419, loss: 0.7460, label: 0, bag_size: 30828\n",
      "batch 439, loss: 0.8635, label: 0, bag_size: 5105\n",
      "batch 459, loss: 0.0441, label: 0, bag_size: 22800\n",
      "batch 479, loss: 0.3874, label: 0, bag_size: 11212\n",
      "batch 499, loss: 0.0324, label: 1, bag_size: 15125\n",
      "batch 519, loss: 0.0411, label: 0, bag_size: 2266\n",
      "batch 539, loss: 1.1275, label: 0, bag_size: 10410\n",
      "batch 559, loss: 0.1656, label: 1, bag_size: 1609\n",
      "batch 579, loss: 0.0028, label: 1, bag_size: 2579\n",
      "batch 599, loss: 0.1525, label: 0, bag_size: 7381\n",
      "batch 619, loss: 0.0693, label: 1, bag_size: 12178\n",
      "batch 639, loss: 0.0062, label: 0, bag_size: 22828\n",
      "batch 659, loss: 0.0001, label: 1, bag_size: 15716\n",
      "batch 679, loss: 0.0039, label: 1, bag_size: 5763\n",
      "batch 699, loss: 0.0190, label: 1, bag_size: 9955\n",
      "batch 719, loss: 0.0024, label: 0, bag_size: 10898\n",
      "batch 739, loss: 0.0024, label: 0, bag_size: 20666\n",
      "batch 759, loss: 0.0006, label: 1, bag_size: 4394\n",
      "batch 779, loss: 0.0009, label: 1, bag_size: 5690\n",
      "batch 799, loss: 0.0994, label: 0, bag_size: 2360\n",
      "batch 819, loss: 0.0013, label: 1, bag_size: 8522\n",
      "Epoch: 28, train_loss: 0.2088, train_error: 0.0766\n",
      "class 0: acc 0.9164420485175202, correct 340/371\n",
      "class 1: acc 0.9290465631929047, correct 419/451\n",
      "\n",
      "Val Set, val_loss: 0.3569, val_error: 0.1284, auc: 0.9579\n",
      "class 0: acc 0.9782608695652174, correct 45/46\n",
      "class 1: acc 0.7936507936507936, correct 50/63\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0487, label: 1, bag_size: 2678\n",
      "batch 39, loss: 0.0013, label: 1, bag_size: 11875\n",
      "batch 59, loss: 0.5525, label: 1, bag_size: 1497\n",
      "batch 79, loss: 0.0617, label: 0, bag_size: 3399\n",
      "batch 99, loss: 0.0332, label: 1, bag_size: 10969\n",
      "batch 119, loss: 0.0005, label: 0, bag_size: 11690\n",
      "batch 139, loss: 0.0224, label: 0, bag_size: 15898\n",
      "batch 159, loss: 0.0099, label: 1, bag_size: 2495\n",
      "batch 179, loss: 0.0291, label: 0, bag_size: 2732\n",
      "batch 199, loss: 0.1234, label: 1, bag_size: 1437\n",
      "batch 219, loss: 0.0144, label: 1, bag_size: 5441\n",
      "batch 239, loss: 0.0080, label: 1, bag_size: 4715\n",
      "batch 259, loss: 0.0021, label: 1, bag_size: 3437\n",
      "batch 279, loss: 0.0147, label: 1, bag_size: 2790\n",
      "batch 299, loss: 0.0009, label: 1, bag_size: 6343\n",
      "batch 319, loss: 0.0361, label: 0, bag_size: 7637\n",
      "batch 339, loss: 0.0004, label: 0, bag_size: 23037\n",
      "batch 359, loss: 0.7069, label: 0, bag_size: 13023\n",
      "batch 379, loss: 0.0008, label: 0, bag_size: 8372\n",
      "batch 399, loss: 0.0204, label: 1, bag_size: 16565\n",
      "batch 419, loss: 0.0024, label: 0, bag_size: 23398\n",
      "batch 439, loss: 0.2410, label: 1, bag_size: 1867\n",
      "batch 459, loss: 0.1193, label: 0, bag_size: 1800\n",
      "batch 479, loss: 0.1186, label: 1, bag_size: 7066\n",
      "batch 499, loss: 0.0281, label: 1, bag_size: 12425\n",
      "batch 519, loss: 0.0051, label: 0, bag_size: 26271\n",
      "batch 539, loss: 0.3095, label: 1, bag_size: 6682\n",
      "batch 559, loss: 0.1085, label: 0, bag_size: 2732\n",
      "batch 579, loss: 0.0039, label: 1, bag_size: 12095\n",
      "batch 599, loss: 0.0058, label: 0, bag_size: 11512\n",
      "batch 619, loss: 1.0879, label: 1, bag_size: 5903\n",
      "batch 639, loss: 0.3030, label: 1, bag_size: 7389\n",
      "batch 659, loss: 0.1897, label: 0, bag_size: 7557\n",
      "batch 679, loss: 0.0205, label: 1, bag_size: 3453\n",
      "batch 699, loss: 1.0138, label: 0, bag_size: 2815\n",
      "batch 719, loss: 0.2861, label: 1, bag_size: 1683\n",
      "batch 739, loss: 0.0011, label: 0, bag_size: 12687\n",
      "batch 759, loss: 0.1699, label: 1, bag_size: 13732\n",
      "batch 779, loss: 0.0047, label: 0, bag_size: 14828\n",
      "batch 799, loss: 0.0021, label: 0, bag_size: 1881\n",
      "batch 819, loss: 0.0003, label: 1, bag_size: 4394\n",
      "Epoch: 29, train_loss: 0.1760, train_error: 0.0693\n",
      "class 0: acc 0.9336609336609336, correct 380/407\n",
      "class 1: acc 0.927710843373494, correct 385/415\n",
      "\n",
      "Val Set, val_loss: 0.2688, val_error: 0.1101, auc: 0.9607\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3740, label: 0, bag_size: 9597\n",
      "batch 39, loss: 0.0012, label: 1, bag_size: 10394\n",
      "batch 59, loss: 1.2355, label: 0, bag_size: 3876\n",
      "batch 79, loss: 0.0541, label: 0, bag_size: 3725\n",
      "batch 99, loss: 0.0370, label: 1, bag_size: 9519\n",
      "batch 119, loss: 0.3774, label: 1, bag_size: 1444\n",
      "batch 139, loss: 0.0012, label: 1, bag_size: 11122\n",
      "batch 159, loss: 0.0045, label: 1, bag_size: 7798\n",
      "batch 179, loss: 0.0042, label: 0, bag_size: 12593\n",
      "batch 199, loss: 0.0669, label: 0, bag_size: 24911\n",
      "batch 219, loss: 0.8645, label: 1, bag_size: 2935\n",
      "batch 239, loss: 0.0495, label: 0, bag_size: 15841\n",
      "batch 259, loss: 0.0023, label: 1, bag_size: 19832\n",
      "batch 279, loss: 0.0063, label: 0, bag_size: 13205\n",
      "batch 299, loss: 0.1730, label: 0, bag_size: 1701\n",
      "batch 319, loss: 0.0146, label: 0, bag_size: 2004\n",
      "batch 339, loss: 0.0053, label: 0, bag_size: 1127\n",
      "batch 359, loss: 0.0005, label: 1, bag_size: 19932\n",
      "batch 379, loss: 0.0087, label: 1, bag_size: 12575\n",
      "batch 399, loss: 0.0295, label: 0, bag_size: 14893\n",
      "batch 419, loss: 0.0593, label: 1, bag_size: 1888\n",
      "batch 439, loss: 0.0023, label: 0, bag_size: 3265\n",
      "batch 459, loss: 0.1597, label: 0, bag_size: 10063\n",
      "batch 479, loss: 0.0188, label: 0, bag_size: 31085\n",
      "batch 499, loss: 0.0009, label: 1, bag_size: 20161\n",
      "batch 519, loss: 0.0586, label: 0, bag_size: 11194\n",
      "batch 539, loss: 0.0027, label: 0, bag_size: 21682\n",
      "batch 559, loss: 0.1416, label: 1, bag_size: 4786\n",
      "batch 579, loss: 5.9929, label: 1, bag_size: 3121\n",
      "batch 599, loss: 0.0263, label: 0, bag_size: 21093\n",
      "batch 619, loss: 0.5629, label: 0, bag_size: 18738\n",
      "batch 639, loss: 0.0083, label: 1, bag_size: 3004\n",
      "batch 659, loss: 0.0393, label: 0, bag_size: 2873\n",
      "batch 679, loss: 0.0066, label: 1, bag_size: 2381\n",
      "batch 699, loss: 0.0107, label: 0, bag_size: 2748\n",
      "batch 719, loss: 0.0671, label: 1, bag_size: 2682\n",
      "batch 739, loss: 0.0787, label: 0, bag_size: 12840\n",
      "batch 759, loss: 1.3329, label: 1, bag_size: 2395\n",
      "batch 779, loss: 0.0001, label: 1, bag_size: 9065\n",
      "batch 799, loss: 1.5671, label: 1, bag_size: 1230\n",
      "batch 819, loss: 0.0146, label: 1, bag_size: 6927\n",
      "Epoch: 30, train_loss: 0.1933, train_error: 0.0693\n",
      "class 0: acc 0.9392405063291139, correct 371/395\n",
      "class 1: acc 0.9227166276346604, correct 394/427\n",
      "\n",
      "Val Set, val_loss: 0.2711, val_error: 0.1101, auc: 0.9579\n",
      "class 0: acc 0.9347826086956522, correct 43/46\n",
      "class 1: acc 0.8571428571428571, correct 54/63\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0025, label: 1, bag_size: 11220\n",
      "batch 39, loss: 0.0138, label: 1, bag_size: 7932\n",
      "batch 59, loss: 0.0597, label: 0, bag_size: 1651\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python main.py --drop_out --early_stopping --lr 2e-4 --k 5 --label_frac 1\\\n",
    "--exp_code cptac_lung_100_level0_mil_adam --weighted_sample --bag_loss ce --inst_loss svm \\\n",
    "--task task_2_tumor_subtyping --model_type mil --log_data --data_root_dir /home/sci/Disk2/CPTAC-LUNG/FEATURES_level0 \\\n",
    "--split_dir /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/cptac_lung_100 --subtyping \\\n",
    "--csv_path dataset_csv/cptac_lung_subtyping.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransformerMIL实验结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load Dataset\n",
      "label column: label\n",
      "label dictionary: {'LUAD': 0, 'LUSC': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 0    507\n",
      "1    520\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 164\n",
      "Slide-LVL; Number of samples registered in class 0: 507\n",
      "Patient-LVL; Number of samples registered in class 1: 157\n",
      "Slide-LVL; Number of samples registered in class 1: 520\n",
      "split_dir:  /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/cptac_lung_100\n",
      "################# Settings ###################\n",
      "num_splits:  5\n",
      "k_start:  -1\n",
      "k_end:  -1\n",
      "task:  task_2_tumor_subtyping\n",
      "max_epochs:  200\n",
      "results_dir:  ./results\n",
      "lr:  0.0002\n",
      "experiment:  cptac_lung_100_level0_transformer_adam\n",
      "reg:  1e-05\n",
      "label_frac:  1.0\n",
      "bag_loss:  ce\n",
      "seed:  1\n",
      "model_type:  transmil\n",
      "model_size:  small\n",
      "use_drop_out:  True\n",
      "weighted_sample:  True\n",
      "opt:  adam\n",
      "bag_weight:  0.7\n",
      "inst_loss:  svm\n",
      "B:  8\n",
      "split_dir:  /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/cptac_lung_100\n",
      "\n",
      "Training Fold 0!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 820 samples\n",
      "Validating on 110 samples\n",
      "Testing on 97 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Setting tau to 1.0\n",
      "Done!\n",
      "TransformerMIL_SB(\n",
      "  (instance_loss_fn): SmoothTop1SVM()\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (transformer): TransformerEncoder_PerformerAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): SelfAttention(\n",
      "            (fast_attention): FastAttention(\n",
      "              (kernel_fn): ReLU()\n",
      "            )\n",
      "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attention): Attn_Net_Gated(\n",
      "    (attention_a): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_b): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (attention_V2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (attention_U2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (attention_weights2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Total number of parameters: 8406537\n",
      "Total number of trainable parameters: 8406537\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 12.8556, instance_loss: 1.6393, weighted_loss: 9.4907, label: 0, bag_size: 21138\n",
      "batch 39, loss: 1.6271, instance_loss: 3.1113, weighted_loss: 2.0724, label: 1, bag_size: 4039\n",
      "batch 59, loss: 0.1468, instance_loss: 1.7191, weighted_loss: 0.6185, label: 0, bag_size: 11281\n",
      "batch 79, loss: 1.2280, instance_loss: 1.1473, weighted_loss: 1.2038, label: 1, bag_size: 6745\n",
      "batch 99, loss: 2.3620, instance_loss: 0.9209, weighted_loss: 1.9297, label: 1, bag_size: 5025\n",
      "batch 119, loss: 2.2441, instance_loss: 0.7176, weighted_loss: 1.7861, label: 1, bag_size: 11389\n",
      "batch 139, loss: 0.1107, instance_loss: 0.6791, weighted_loss: 0.2812, label: 0, bag_size: 2609\n",
      "batch 159, loss: 2.4096, instance_loss: 0.7044, weighted_loss: 1.8980, label: 0, bag_size: 13619\n",
      "batch 179, loss: 3.6146, instance_loss: 1.1896, weighted_loss: 2.8871, label: 1, bag_size: 15563\n",
      "batch 199, loss: 0.4196, instance_loss: 1.0541, weighted_loss: 0.6099, label: 0, bag_size: 16087\n",
      "batch 219, loss: 5.5958, instance_loss: 1.6074, weighted_loss: 4.3993, label: 0, bag_size: 763\n",
      "batch 239, loss: 0.8047, instance_loss: 0.9872, weighted_loss: 0.8594, label: 0, bag_size: 12840\n",
      "batch 259, loss: 0.0028, instance_loss: 0.6637, weighted_loss: 0.2011, label: 0, bag_size: 3657\n",
      "batch 279, loss: 0.1030, instance_loss: 1.0209, weighted_loss: 0.3784, label: 0, bag_size: 8948\n",
      "batch 299, loss: 2.6045, instance_loss: 2.0158, weighted_loss: 2.4279, label: 0, bag_size: 22498\n",
      "batch 319, loss: 0.1709, instance_loss: 1.2870, weighted_loss: 0.5057, label: 1, bag_size: 3856\n",
      "batch 339, loss: 0.0774, instance_loss: 1.3309, weighted_loss: 0.4534, label: 1, bag_size: 8466\n",
      "batch 359, loss: 0.0664, instance_loss: 1.6557, weighted_loss: 0.5432, label: 1, bag_size: 4054\n",
      "batch 379, loss: 0.0983, instance_loss: 0.8201, weighted_loss: 0.3149, label: 1, bag_size: 6453\n",
      "batch 399, loss: 0.0995, instance_loss: 0.7635, weighted_loss: 0.2987, label: 0, bag_size: 26208\n",
      "batch 419, loss: 0.9718, instance_loss: 0.8194, weighted_loss: 0.9261, label: 1, bag_size: 9533\n",
      "batch 439, loss: 0.1982, instance_loss: 0.7024, weighted_loss: 0.3495, label: 1, bag_size: 12611\n",
      "batch 459, loss: 0.0285, instance_loss: 0.7456, weighted_loss: 0.2436, label: 1, bag_size: 5256\n",
      "batch 479, loss: 0.1518, instance_loss: 1.1304, weighted_loss: 0.4454, label: 0, bag_size: 23618\n",
      "batch 499, loss: 0.0716, instance_loss: 0.8999, weighted_loss: 0.3201, label: 1, bag_size: 2935\n",
      "batch 519, loss: 1.2603, instance_loss: 0.7254, weighted_loss: 1.0999, label: 0, bag_size: 21093\n",
      "batch 539, loss: 0.0719, instance_loss: 0.6957, weighted_loss: 0.2590, label: 0, bag_size: 9786\n",
      "batch 559, loss: 0.0109, instance_loss: 0.6924, weighted_loss: 0.2153, label: 0, bag_size: 12148\n",
      "batch 579, loss: 2.0382, instance_loss: 1.2904, weighted_loss: 1.8138, label: 1, bag_size: 2937\n",
      "batch 599, loss: 0.0055, instance_loss: 0.6591, weighted_loss: 0.2016, label: 0, bag_size: 22870\n",
      "batch 619, loss: 0.0051, instance_loss: 0.7145, weighted_loss: 0.2179, label: 1, bag_size: 16890\n",
      "batch 639, loss: 0.0002, instance_loss: 0.6640, weighted_loss: 0.1993, label: 1, bag_size: 7246\n",
      "batch 659, loss: 0.0057, instance_loss: 0.7146, weighted_loss: 0.2184, label: 0, bag_size: 9455\n",
      "batch 679, loss: 3.5353, instance_loss: 1.0165, weighted_loss: 2.7796, label: 0, bag_size: 12731\n",
      "batch 699, loss: 0.6830, instance_loss: 0.6787, weighted_loss: 0.6817, label: 0, bag_size: 3502\n",
      "batch 719, loss: 0.3889, instance_loss: 0.7181, weighted_loss: 0.4876, label: 0, bag_size: 11259\n",
      "batch 739, loss: 5.9555, instance_loss: 1.3302, weighted_loss: 4.5679, label: 1, bag_size: 3856\n",
      "batch 759, loss: 0.1525, instance_loss: 1.1564, weighted_loss: 0.4537, label: 1, bag_size: 1764\n",
      "batch 779, loss: 5.8810, instance_loss: 1.9159, weighted_loss: 4.6914, label: 0, bag_size: 6281\n",
      "batch 799, loss: 5.3985, instance_loss: 1.3697, weighted_loss: 4.1899, label: 1, bag_size: 12178\n",
      "batch 819, loss: 0.2648, instance_loss: 0.8345, weighted_loss: 0.4357, label: 1, bag_size: 11729\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.879420731707317: correct 11538/13120\n",
      "class 1 clustering acc 0.2013719512195122: correct 1321/6560\n",
      "Epoch: 0, train_loss: 0.9830, train_clustering_loss:  1.0481, train_error: 0.3512\n",
      "class 0: acc 0.6339066339066339, correct 258/407\n",
      "class 1: acc 0.6634382566585957, correct 274/413\n",
      "\n",
      "Val Set, val_loss: 0.3988, val_error: 0.2000, auc: 0.9486\n",
      "class 0 clustering acc 0.9477272727272728: correct 1668/1760\n",
      "class 1 clustering acc 0.014772727272727272: correct 13/880\n",
      "class 0: acc 0.5961538461538461, correct 31/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "Validation loss decreased (inf --> 0.398787).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0125, instance_loss: 0.8617, weighted_loss: 0.2672, label: 0, bag_size: 18045\n",
      "batch 39, loss: 0.5818, instance_loss: 1.4505, weighted_loss: 0.8424, label: 1, bag_size: 12180\n",
      "batch 59, loss: 0.0025, instance_loss: 0.7226, weighted_loss: 0.2185, label: 1, bag_size: 689\n",
      "batch 79, loss: 0.0003, instance_loss: 0.7072, weighted_loss: 0.2124, label: 1, bag_size: 25970\n",
      "batch 99, loss: 1.0563, instance_loss: 1.7015, weighted_loss: 1.2499, label: 1, bag_size: 4956\n",
      "batch 119, loss: 0.0009, instance_loss: 1.0014, weighted_loss: 0.3011, label: 0, bag_size: 9234\n",
      "batch 139, loss: 0.0967, instance_loss: 0.6367, weighted_loss: 0.2587, label: 1, bag_size: 25970\n",
      "batch 159, loss: 0.0588, instance_loss: 0.7394, weighted_loss: 0.2630, label: 1, bag_size: 2179\n",
      "batch 179, loss: 0.0000, instance_loss: 0.6811, weighted_loss: 0.2044, label: 1, bag_size: 6875\n",
      "batch 199, loss: 0.1487, instance_loss: 2.4665, weighted_loss: 0.8440, label: 1, bag_size: 15185\n",
      "batch 219, loss: 0.2636, instance_loss: 1.3684, weighted_loss: 0.5950, label: 0, bag_size: 4465\n",
      "batch 239, loss: 0.3959, instance_loss: 1.5892, weighted_loss: 0.7539, label: 1, bag_size: 12178\n",
      "batch 259, loss: 0.0179, instance_loss: 0.8975, weighted_loss: 0.2818, label: 1, bag_size: 5345\n",
      "batch 279, loss: 0.0187, instance_loss: 0.8041, weighted_loss: 0.2543, label: 0, bag_size: 2322\n",
      "batch 299, loss: 0.0362, instance_loss: 0.6051, weighted_loss: 0.2069, label: 0, bag_size: 9252\n",
      "batch 319, loss: 0.0413, instance_loss: 0.6263, weighted_loss: 0.2168, label: 0, bag_size: 12687\n",
      "batch 339, loss: 0.0107, instance_loss: 0.6664, weighted_loss: 0.2074, label: 1, bag_size: 10394\n",
      "batch 359, loss: 0.9876, instance_loss: 0.9397, weighted_loss: 0.9732, label: 1, bag_size: 22264\n",
      "batch 379, loss: 2.0850, instance_loss: 1.2826, weighted_loss: 1.8443, label: 0, bag_size: 65728\n",
      "batch 399, loss: 0.0145, instance_loss: 0.6158, weighted_loss: 0.1949, label: 1, bag_size: 25970\n",
      "batch 419, loss: 0.0177, instance_loss: 0.6785, weighted_loss: 0.2159, label: 1, bag_size: 865\n",
      "batch 439, loss: 0.1953, instance_loss: 0.8356, weighted_loss: 0.3874, label: 1, bag_size: 8602\n",
      "batch 459, loss: 0.0007, instance_loss: 0.6082, weighted_loss: 0.1830, label: 0, bag_size: 3190\n",
      "batch 479, loss: 1.3375, instance_loss: 0.6937, weighted_loss: 1.1444, label: 1, bag_size: 11220\n",
      "batch 499, loss: 0.0016, instance_loss: 0.6446, weighted_loss: 0.1945, label: 0, bag_size: 15077\n",
      "batch 519, loss: 0.0000, instance_loss: 1.1521, weighted_loss: 0.3456, label: 0, bag_size: 8866\n",
      "batch 539, loss: 0.3281, instance_loss: 0.7469, weighted_loss: 0.4537, label: 0, bag_size: 3810\n",
      "batch 559, loss: 0.0644, instance_loss: 1.1540, weighted_loss: 0.3913, label: 1, bag_size: 1683\n",
      "batch 579, loss: 0.0272, instance_loss: 0.6874, weighted_loss: 0.2253, label: 0, bag_size: 1789\n",
      "batch 599, loss: 0.0750, instance_loss: 1.1601, weighted_loss: 0.4005, label: 0, bag_size: 1560\n",
      "batch 619, loss: 0.1534, instance_loss: 0.6818, weighted_loss: 0.3119, label: 0, bag_size: 14305\n",
      "batch 639, loss: 0.0069, instance_loss: 0.6970, weighted_loss: 0.2140, label: 0, bag_size: 11917\n",
      "batch 659, loss: 0.5064, instance_loss: 0.9215, weighted_loss: 0.6309, label: 1, bag_size: 5690\n",
      "batch 679, loss: 0.9684, instance_loss: 1.4072, weighted_loss: 1.1000, label: 0, bag_size: 1772\n",
      "batch 699, loss: 0.1103, instance_loss: 0.7844, weighted_loss: 0.3126, label: 1, bag_size: 4259\n",
      "batch 719, loss: 0.0776, instance_loss: 0.7460, weighted_loss: 0.2781, label: 0, bag_size: 4902\n",
      "batch 739, loss: 1.1168, instance_loss: 1.3481, weighted_loss: 1.1862, label: 1, bag_size: 2395\n",
      "batch 759, loss: 0.0984, instance_loss: 1.1704, weighted_loss: 0.4200, label: 0, bag_size: 2534\n",
      "batch 779, loss: 0.1178, instance_loss: 0.8051, weighted_loss: 0.3240, label: 0, bag_size: 2179\n",
      "batch 799, loss: 0.4688, instance_loss: 1.1406, weighted_loss: 0.6704, label: 1, bag_size: 10460\n",
      "batch 819, loss: 0.0188, instance_loss: 0.6991, weighted_loss: 0.2229, label: 1, bag_size: 16417\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.8610518292682927: correct 11297/13120\n",
      "class 1 clustering acc 0.25228658536585363: correct 1655/6560\n",
      "Epoch: 1, train_loss: 0.8171, train_clustering_loss:  1.0350, train_error: 0.2707\n",
      "class 0: acc 0.6928934010152284, correct 273/394\n",
      "class 1: acc 0.7629107981220657, correct 325/426\n",
      "\n",
      "Val Set, val_loss: 0.2699, val_error: 0.1182, auc: 0.9572\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 0.8448275862068966, correct 49/58\n",
      "Validation loss decreased (0.398787 --> 0.269856).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0023, instance_loss: 0.6534, weighted_loss: 0.1976, label: 0, bag_size: 2336\n",
      "batch 39, loss: 0.4411, instance_loss: 1.2320, weighted_loss: 0.6784, label: 1, bag_size: 16514\n",
      "batch 59, loss: 0.1253, instance_loss: 0.8500, weighted_loss: 0.3427, label: 1, bag_size: 15464\n",
      "batch 79, loss: 0.0020, instance_loss: 0.8066, weighted_loss: 0.2434, label: 0, bag_size: 23398\n",
      "batch 99, loss: 0.0221, instance_loss: 0.7066, weighted_loss: 0.2274, label: 1, bag_size: 3652\n",
      "batch 119, loss: 0.0182, instance_loss: 0.6732, weighted_loss: 0.2147, label: 0, bag_size: 21319\n",
      "batch 139, loss: 0.0019, instance_loss: 0.6317, weighted_loss: 0.1909, label: 1, bag_size: 13051\n",
      "batch 159, loss: 0.1733, instance_loss: 0.6617, weighted_loss: 0.3198, label: 0, bag_size: 5225\n",
      "batch 179, loss: 0.4584, instance_loss: 0.7556, weighted_loss: 0.5476, label: 0, bag_size: 12149\n",
      "batch 199, loss: 7.7491, instance_loss: 2.5610, weighted_loss: 6.1927, label: 0, bag_size: 9132\n",
      "batch 219, loss: 0.0533, instance_loss: 0.7987, weighted_loss: 0.2769, label: 1, bag_size: 10592\n",
      "batch 239, loss: 4.7322, instance_loss: 3.3352, weighted_loss: 4.3131, label: 0, bag_size: 3468\n",
      "batch 259, loss: 0.2038, instance_loss: 0.6813, weighted_loss: 0.3471, label: 1, bag_size: 699\n",
      "batch 279, loss: 0.0697, instance_loss: 0.8619, weighted_loss: 0.3073, label: 0, bag_size: 8582\n",
      "batch 299, loss: 2.4543, instance_loss: 0.8928, weighted_loss: 1.9858, label: 0, bag_size: 3783\n",
      "batch 319, loss: 0.0015, instance_loss: 0.7001, weighted_loss: 0.2111, label: 1, bag_size: 5894\n",
      "batch 339, loss: 0.0479, instance_loss: 0.6171, weighted_loss: 0.2187, label: 0, bag_size: 12148\n",
      "batch 359, loss: 0.0335, instance_loss: 0.8364, weighted_loss: 0.2744, label: 1, bag_size: 17769\n",
      "batch 379, loss: 0.1754, instance_loss: 0.6958, weighted_loss: 0.3315, label: 1, bag_size: 2759\n",
      "batch 399, loss: 0.9604, instance_loss: 0.7488, weighted_loss: 0.8969, label: 1, bag_size: 10920\n",
      "batch 419, loss: 0.1398, instance_loss: 0.6557, weighted_loss: 0.2946, label: 1, bag_size: 10920\n",
      "batch 439, loss: 0.0324, instance_loss: 0.7293, weighted_loss: 0.2415, label: 1, bag_size: 2140\n",
      "batch 459, loss: 1.9879, instance_loss: 1.0546, weighted_loss: 1.7079, label: 0, bag_size: 5409\n",
      "batch 479, loss: 0.0056, instance_loss: 0.3426, weighted_loss: 0.1067, label: 0, bag_size: 8145\n",
      "batch 499, loss: 0.0065, instance_loss: 0.6249, weighted_loss: 0.1920, label: 1, bag_size: 7798\n",
      "batch 519, loss: 0.2481, instance_loss: 0.8140, weighted_loss: 0.4179, label: 0, bag_size: 4523\n",
      "batch 539, loss: 1.9163, instance_loss: 0.7005, weighted_loss: 1.5516, label: 1, bag_size: 9147\n",
      "batch 559, loss: 1.8246, instance_loss: 1.3278, weighted_loss: 1.6755, label: 0, bag_size: 1437\n",
      "batch 579, loss: 1.6684, instance_loss: 1.1798, weighted_loss: 1.5218, label: 0, bag_size: 1714\n",
      "batch 599, loss: 5.2492, instance_loss: 3.2408, weighted_loss: 4.6467, label: 1, bag_size: 16514\n",
      "batch 619, loss: 0.2579, instance_loss: 0.8605, weighted_loss: 0.4387, label: 1, bag_size: 4039\n",
      "batch 639, loss: 0.0942, instance_loss: 0.1258, weighted_loss: 0.1037, label: 0, bag_size: 3787\n",
      "batch 659, loss: 0.1285, instance_loss: 0.6693, weighted_loss: 0.2907, label: 0, bag_size: 3502\n",
      "batch 679, loss: 0.0408, instance_loss: 0.0873, weighted_loss: 0.0548, label: 0, bag_size: 5999\n",
      "batch 699, loss: 0.0356, instance_loss: 0.6292, weighted_loss: 0.2137, label: 1, bag_size: 13174\n",
      "batch 719, loss: 0.0477, instance_loss: 0.1843, weighted_loss: 0.0887, label: 0, bag_size: 11194\n",
      "batch 739, loss: 0.4646, instance_loss: 0.6609, weighted_loss: 0.5235, label: 1, bag_size: 8191\n",
      "batch 759, loss: 1.6555, instance_loss: 0.7915, weighted_loss: 1.3963, label: 1, bag_size: 689\n",
      "batch 779, loss: 1.6580, instance_loss: 0.9239, weighted_loss: 1.4377, label: 1, bag_size: 4039\n",
      "batch 799, loss: 0.0035, instance_loss: 0.0269, weighted_loss: 0.0106, label: 0, bag_size: 11900\n",
      "batch 819, loss: 0.0240, instance_loss: 0.6507, weighted_loss: 0.2120, label: 1, bag_size: 14515\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.8984756097560975: correct 11788/13120\n",
      "class 1 clustering acc 0.3358231707317073: correct 2203/6560\n",
      "Epoch: 2, train_loss: 0.5101, train_clustering_loss:  0.8025, train_error: 0.2000\n",
      "class 0: acc 0.7964824120603015, correct 317/398\n",
      "class 1: acc 0.8033175355450237, correct 339/422\n",
      "\n",
      "Val Set, val_loss: 0.3025, val_error: 0.1182, auc: 0.9489\n",
      "class 0 clustering acc 0.9875: correct 1738/1760\n",
      "class 1 clustering acc 0.14772727272727273: correct 130/880\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0076, instance_loss: 0.0184, weighted_loss: 0.0109, label: 0, bag_size: 18415\n",
      "batch 39, loss: 0.0226, instance_loss: 0.3808, weighted_loss: 0.1301, label: 0, bag_size: 4598\n",
      "batch 59, loss: 0.4051, instance_loss: 0.7509, weighted_loss: 0.5089, label: 1, bag_size: 1095\n",
      "batch 79, loss: 0.0212, instance_loss: 0.5689, weighted_loss: 0.1855, label: 1, bag_size: 2356\n",
      "batch 99, loss: 0.0291, instance_loss: 0.6264, weighted_loss: 0.2083, label: 1, bag_size: 5731\n",
      "batch 119, loss: 0.1789, instance_loss: 0.1173, weighted_loss: 0.1605, label: 0, bag_size: 8549\n",
      "batch 139, loss: 0.0011, instance_loss: 0.6722, weighted_loss: 0.2024, label: 1, bag_size: 17769\n",
      "batch 159, loss: 0.6044, instance_loss: 0.4166, weighted_loss: 0.5480, label: 0, bag_size: 15464\n",
      "batch 179, loss: 0.9157, instance_loss: 0.9301, weighted_loss: 0.9200, label: 0, bag_size: 9069\n",
      "batch 199, loss: 0.2122, instance_loss: 0.1785, weighted_loss: 0.2021, label: 0, bag_size: 11512\n",
      "batch 219, loss: 0.0078, instance_loss: 0.0996, weighted_loss: 0.0353, label: 0, bag_size: 16936\n",
      "batch 239, loss: 3.9191, instance_loss: 2.1367, weighted_loss: 3.3844, label: 1, bag_size: 13089\n",
      "batch 259, loss: 0.0011, instance_loss: 0.0447, weighted_loss: 0.0142, label: 0, bag_size: 18415\n",
      "batch 279, loss: 0.0852, instance_loss: 0.5115, weighted_loss: 0.2131, label: 0, bag_size: 1690\n",
      "batch 299, loss: 0.0054, instance_loss: 0.0271, weighted_loss: 0.0119, label: 0, bag_size: 17791\n",
      "batch 319, loss: 0.0042, instance_loss: 0.7525, weighted_loss: 0.2287, label: 1, bag_size: 6731\n",
      "batch 339, loss: 0.0009, instance_loss: 0.0189, weighted_loss: 0.0063, label: 0, bag_size: 10898\n",
      "batch 359, loss: 0.1318, instance_loss: 0.8178, weighted_loss: 0.3376, label: 1, bag_size: 7468\n",
      "batch 379, loss: 0.0663, instance_loss: 0.8730, weighted_loss: 0.3083, label: 1, bag_size: 18649\n",
      "batch 399, loss: 0.0035, instance_loss: 0.6235, weighted_loss: 0.1895, label: 1, bag_size: 1525\n",
      "batch 419, loss: 0.0005, instance_loss: 0.0477, weighted_loss: 0.0147, label: 0, bag_size: 19466\n",
      "batch 439, loss: 0.0177, instance_loss: 0.0199, weighted_loss: 0.0183, label: 0, bag_size: 6624\n",
      "batch 459, loss: 0.0203, instance_loss: 0.5501, weighted_loss: 0.1792, label: 1, bag_size: 10622\n",
      "batch 479, loss: 4.5646, instance_loss: 3.5684, weighted_loss: 4.2658, label: 1, bag_size: 898\n",
      "batch 499, loss: 0.0175, instance_loss: 0.0794, weighted_loss: 0.0361, label: 0, bag_size: 10068\n",
      "batch 519, loss: 0.2172, instance_loss: 0.5264, weighted_loss: 0.3099, label: 0, bag_size: 3502\n",
      "batch 539, loss: 0.0320, instance_loss: 0.4013, weighted_loss: 0.1428, label: 0, bag_size: 1506\n",
      "batch 559, loss: 0.8709, instance_loss: 1.1261, weighted_loss: 0.9475, label: 0, bag_size: 9387\n",
      "batch 579, loss: 0.3964, instance_loss: 1.3908, weighted_loss: 0.6947, label: 1, bag_size: 4929\n",
      "batch 599, loss: 0.0158, instance_loss: 0.7055, weighted_loss: 0.2227, label: 1, bag_size: 6090\n",
      "batch 619, loss: 0.0126, instance_loss: 0.6022, weighted_loss: 0.1895, label: 1, bag_size: 10028\n",
      "batch 639, loss: 0.1730, instance_loss: 1.1049, weighted_loss: 0.4526, label: 1, bag_size: 6734\n",
      "batch 659, loss: 0.3535, instance_loss: 1.1359, weighted_loss: 0.5882, label: 1, bag_size: 865\n",
      "batch 679, loss: 0.0180, instance_loss: 0.2636, weighted_loss: 0.0917, label: 0, bag_size: 19390\n",
      "batch 699, loss: 0.0024, instance_loss: 0.0359, weighted_loss: 0.0124, label: 0, bag_size: 2179\n",
      "batch 719, loss: 2.3077, instance_loss: 2.0434, weighted_loss: 2.2284, label: 0, bag_size: 1701\n",
      "batch 739, loss: 0.0724, instance_loss: 0.1400, weighted_loss: 0.0927, label: 0, bag_size: 16211\n",
      "batch 759, loss: 0.0260, instance_loss: 0.6572, weighted_loss: 0.2154, label: 1, bag_size: 12575\n",
      "batch 779, loss: 0.0080, instance_loss: 0.6931, weighted_loss: 0.2135, label: 1, bag_size: 617\n",
      "batch 799, loss: 1.1348, instance_loss: 0.9465, weighted_loss: 1.0783, label: 1, bag_size: 2785\n",
      "batch 819, loss: 1.6735, instance_loss: 0.9106, weighted_loss: 1.4446, label: 0, bag_size: 1714\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9310213414634146: correct 12215/13120\n",
      "class 1 clustering acc 0.469359756097561: correct 3079/6560\n",
      "Epoch: 3, train_loss: 0.4752, train_clustering_loss:  0.6956, train_error: 0.1866\n",
      "class 0: acc 0.8091787439613527, correct 335/414\n",
      "class 1: acc 0.8177339901477833, correct 332/406\n",
      "\n",
      "Val Set, val_loss: 0.6049, val_error: 0.3000, auc: 0.9542\n",
      "class 0 clustering acc 0.8306818181818182: correct 1462/1760\n",
      "class 1 clustering acc 0.38636363636363635: correct 340/880\n",
      "class 0: acc 0.36538461538461536, correct 19/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0239, instance_loss: 0.2329, weighted_loss: 0.0866, label: 0, bag_size: 23996\n",
      "batch 39, loss: 1.3129, instance_loss: 1.8650, weighted_loss: 1.4786, label: 0, bag_size: 1800\n",
      "batch 59, loss: 0.1516, instance_loss: 1.1921, weighted_loss: 0.4638, label: 1, bag_size: 5366\n",
      "batch 79, loss: 4.7066, instance_loss: 3.8323, weighted_loss: 4.4443, label: 1, bag_size: 16514\n",
      "batch 99, loss: 0.0620, instance_loss: 0.4577, weighted_loss: 0.1807, label: 0, bag_size: 2918\n",
      "batch 119, loss: 0.4215, instance_loss: 0.5059, weighted_loss: 0.4468, label: 0, bag_size: 2367\n",
      "batch 139, loss: 0.0929, instance_loss: 0.5053, weighted_loss: 0.2166, label: 1, bag_size: 4862\n",
      "batch 159, loss: 0.0090, instance_loss: 0.0260, weighted_loss: 0.0141, label: 0, bag_size: 5225\n",
      "batch 179, loss: 0.2202, instance_loss: 0.3618, weighted_loss: 0.2627, label: 0, bag_size: 11194\n",
      "batch 199, loss: 0.0002, instance_loss: 0.5426, weighted_loss: 0.1630, label: 1, bag_size: 4250\n",
      "batch 219, loss: 0.4354, instance_loss: 1.3046, weighted_loss: 0.6961, label: 0, bag_size: 3160\n",
      "batch 239, loss: 1.2135, instance_loss: 0.3108, weighted_loss: 0.9427, label: 0, bag_size: 2609\n",
      "batch 259, loss: 0.0116, instance_loss: 0.5880, weighted_loss: 0.1845, label: 1, bag_size: 4039\n",
      "batch 279, loss: 0.2074, instance_loss: 0.4782, weighted_loss: 0.2886, label: 0, bag_size: 1213\n",
      "batch 299, loss: 0.0145, instance_loss: 0.5971, weighted_loss: 0.1893, label: 1, bag_size: 3409\n",
      "batch 319, loss: 0.8873, instance_loss: 0.6417, weighted_loss: 0.8136, label: 0, bag_size: 1370\n",
      "batch 339, loss: 0.3882, instance_loss: 1.5246, weighted_loss: 0.7291, label: 0, bag_size: 14249\n",
      "batch 359, loss: 0.0165, instance_loss: 0.5030, weighted_loss: 0.1625, label: 0, bag_size: 9786\n",
      "batch 379, loss: 0.0795, instance_loss: 0.4519, weighted_loss: 0.1912, label: 0, bag_size: 11735\n",
      "batch 399, loss: 0.2209, instance_loss: 0.5716, weighted_loss: 0.3261, label: 1, bag_size: 11964\n",
      "batch 419, loss: 0.0112, instance_loss: 0.5697, weighted_loss: 0.1787, label: 1, bag_size: 16565\n",
      "batch 439, loss: 0.0306, instance_loss: 0.4707, weighted_loss: 0.1626, label: 1, bag_size: 12719\n",
      "batch 459, loss: 0.1757, instance_loss: 0.5943, weighted_loss: 0.3013, label: 1, bag_size: 2179\n",
      "batch 479, loss: 0.0320, instance_loss: 0.3640, weighted_loss: 0.1316, label: 0, bag_size: 2070\n",
      "batch 499, loss: 0.0241, instance_loss: 0.0663, weighted_loss: 0.0367, label: 0, bag_size: 23996\n",
      "batch 519, loss: 0.0020, instance_loss: 0.0062, weighted_loss: 0.0032, label: 0, bag_size: 15077\n",
      "batch 539, loss: 0.0660, instance_loss: 0.0375, weighted_loss: 0.0575, label: 0, bag_size: 15003\n",
      "batch 559, loss: 0.0262, instance_loss: 0.7508, weighted_loss: 0.2436, label: 1, bag_size: 4423\n",
      "batch 579, loss: 1.3572, instance_loss: 1.7994, weighted_loss: 1.4899, label: 1, bag_size: 1123\n",
      "batch 599, loss: 0.0043, instance_loss: 0.0769, weighted_loss: 0.0261, label: 0, bag_size: 10481\n",
      "batch 619, loss: 0.0751, instance_loss: 0.1176, weighted_loss: 0.0878, label: 0, bag_size: 2036\n",
      "batch 639, loss: 0.0446, instance_loss: 0.4198, weighted_loss: 0.1572, label: 1, bag_size: 7613\n",
      "batch 659, loss: 0.0177, instance_loss: 0.4734, weighted_loss: 0.1544, label: 0, bag_size: 1881\n",
      "batch 679, loss: 0.0513, instance_loss: 0.6624, weighted_loss: 0.2346, label: 0, bag_size: 1213\n",
      "batch 699, loss: 0.0859, instance_loss: 0.5344, weighted_loss: 0.2204, label: 0, bag_size: 6624\n",
      "batch 719, loss: 0.7281, instance_loss: 1.0667, weighted_loss: 0.8296, label: 1, bag_size: 2937\n",
      "batch 739, loss: 0.0481, instance_loss: 0.0222, weighted_loss: 0.0403, label: 0, bag_size: 12796\n",
      "batch 759, loss: 0.0160, instance_loss: 0.1054, weighted_loss: 0.0428, label: 0, bag_size: 9234\n",
      "batch 779, loss: 0.9621, instance_loss: 0.6643, weighted_loss: 0.8728, label: 0, bag_size: 11212\n",
      "batch 799, loss: 0.2531, instance_loss: 0.7835, weighted_loss: 0.4122, label: 1, bag_size: 7989\n",
      "batch 819, loss: 0.2365, instance_loss: 0.6599, weighted_loss: 0.3635, label: 1, bag_size: 8868\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9293445121951219: correct 12193/13120\n",
      "class 1 clustering acc 0.5525914634146342: correct 3625/6560\n",
      "Epoch: 4, train_loss: 0.4191, train_clustering_loss:  0.6415, train_error: 0.1585\n",
      "class 0: acc 0.8483412322274881, correct 358/422\n",
      "class 1: acc 0.8341708542713567, correct 332/398\n",
      "\n",
      "Val Set, val_loss: 0.3916, val_error: 0.1818, auc: 0.9476\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9807692307692307, correct 51/52\n",
      "class 1: acc 0.6724137931034483, correct 39/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6840, instance_loss: 0.6188, weighted_loss: 0.6644, label: 1, bag_size: 1284\n",
      "batch 39, loss: 0.0125, instance_loss: 0.5183, weighted_loss: 0.1643, label: 1, bag_size: 14618\n",
      "batch 59, loss: 0.3416, instance_loss: 1.1976, weighted_loss: 0.5984, label: 1, bag_size: 1015\n",
      "batch 79, loss: 0.0255, instance_loss: 0.4052, weighted_loss: 0.1394, label: 1, bag_size: 8592\n",
      "batch 99, loss: 0.0123, instance_loss: 0.1121, weighted_loss: 0.0423, label: 0, bag_size: 2732\n",
      "batch 119, loss: 0.0697, instance_loss: 0.2076, weighted_loss: 0.1110, label: 0, bag_size: 2382\n",
      "batch 139, loss: 0.0015, instance_loss: 0.5703, weighted_loss: 0.1721, label: 1, bag_size: 14202\n",
      "batch 159, loss: 0.0006, instance_loss: 0.5347, weighted_loss: 0.1608, label: 1, bag_size: 9408\n",
      "batch 179, loss: 0.0087, instance_loss: 0.0048, weighted_loss: 0.0075, label: 0, bag_size: 11125\n",
      "batch 199, loss: 0.3278, instance_loss: 0.7080, weighted_loss: 0.4419, label: 1, bag_size: 11386\n",
      "batch 219, loss: 0.0677, instance_loss: 0.4825, weighted_loss: 0.1921, label: 1, bag_size: 11964\n",
      "batch 239, loss: 0.2109, instance_loss: 0.5922, weighted_loss: 0.3253, label: 1, bag_size: 1015\n",
      "batch 259, loss: 0.0449, instance_loss: 0.5577, weighted_loss: 0.1988, label: 1, bag_size: 3651\n",
      "batch 279, loss: 0.0916, instance_loss: 0.6483, weighted_loss: 0.2587, label: 1, bag_size: 2278\n",
      "batch 299, loss: 0.0722, instance_loss: 0.1171, weighted_loss: 0.0857, label: 0, bag_size: 1884\n",
      "batch 319, loss: 0.7225, instance_loss: 0.3404, weighted_loss: 0.6079, label: 1, bag_size: 699\n",
      "batch 339, loss: 0.1174, instance_loss: 0.5545, weighted_loss: 0.2486, label: 1, bag_size: 12603\n",
      "batch 359, loss: 0.9351, instance_loss: 1.4250, weighted_loss: 1.0821, label: 1, bag_size: 4786\n",
      "batch 379, loss: 0.0318, instance_loss: 0.5506, weighted_loss: 0.1875, label: 0, bag_size: 9415\n",
      "batch 399, loss: 2.6765, instance_loss: 1.8519, weighted_loss: 2.4291, label: 0, bag_size: 1498\n",
      "batch 419, loss: 0.0466, instance_loss: 0.4172, weighted_loss: 0.1578, label: 0, bag_size: 8145\n",
      "batch 439, loss: 0.0301, instance_loss: 0.2976, weighted_loss: 0.1104, label: 0, bag_size: 1072\n",
      "batch 459, loss: 0.3401, instance_loss: 0.5038, weighted_loss: 0.3892, label: 0, bag_size: 8788\n",
      "batch 479, loss: 1.0704, instance_loss: 1.4358, weighted_loss: 1.1800, label: 1, bag_size: 1867\n",
      "batch 499, loss: 0.0163, instance_loss: 0.2135, weighted_loss: 0.0755, label: 0, bag_size: 3725\n",
      "batch 519, loss: 0.0068, instance_loss: 0.0120, weighted_loss: 0.0084, label: 0, bag_size: 11900\n",
      "batch 539, loss: 0.0122, instance_loss: 0.4968, weighted_loss: 0.1576, label: 1, bag_size: 5864\n",
      "batch 559, loss: 0.0066, instance_loss: 0.0642, weighted_loss: 0.0239, label: 0, bag_size: 2652\n",
      "batch 579, loss: 0.0015, instance_loss: 0.0043, weighted_loss: 0.0024, label: 0, bag_size: 9252\n",
      "batch 599, loss: 0.1928, instance_loss: 0.2650, weighted_loss: 0.2144, label: 1, bag_size: 15332\n",
      "batch 619, loss: 1.0861, instance_loss: 0.8442, weighted_loss: 1.0135, label: 0, bag_size: 1416\n",
      "batch 639, loss: 0.3465, instance_loss: 1.0630, weighted_loss: 0.5614, label: 1, bag_size: 865\n",
      "batch 659, loss: 0.0002, instance_loss: 0.3926, weighted_loss: 0.1179, label: 1, bag_size: 20767\n",
      "batch 679, loss: 0.1294, instance_loss: 0.4062, weighted_loss: 0.2125, label: 0, bag_size: 1202\n",
      "batch 699, loss: 0.0311, instance_loss: 0.2584, weighted_loss: 0.0993, label: 1, bag_size: 5231\n",
      "batch 719, loss: 0.4679, instance_loss: 1.7070, weighted_loss: 0.8396, label: 1, bag_size: 10671\n",
      "batch 739, loss: 0.0181, instance_loss: 0.0762, weighted_loss: 0.0355, label: 0, bag_size: 14266\n",
      "batch 759, loss: 1.1774, instance_loss: 0.7062, weighted_loss: 1.0361, label: 0, bag_size: 1370\n",
      "batch 779, loss: 0.0261, instance_loss: 1.2362, weighted_loss: 0.3891, label: 1, bag_size: 12626\n",
      "batch 799, loss: 0.0310, instance_loss: 0.2733, weighted_loss: 0.1037, label: 0, bag_size: 8788\n",
      "batch 819, loss: 0.0111, instance_loss: 0.4114, weighted_loss: 0.1312, label: 1, bag_size: 6453\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9377286585365854: correct 12303/13120\n",
      "class 1 clustering acc 0.572560975609756: correct 3756/6560\n",
      "Epoch: 5, train_loss: 0.4004, train_clustering_loss:  0.6327, train_error: 0.1646\n",
      "class 0: acc 0.8337349397590361, correct 346/415\n",
      "class 1: acc 0.837037037037037, correct 339/405\n",
      "\n",
      "Val Set, val_loss: 0.2952, val_error: 0.1182, auc: 0.9586\n",
      "class 0 clustering acc 0.9659090909090909: correct 1700/1760\n",
      "class 1 clustering acc 0.3704545454545455: correct 326/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.8275862068965517, correct 48/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1404, instance_loss: 0.5694, weighted_loss: 0.2691, label: 1, bag_size: 12931\n",
      "batch 39, loss: 0.0563, instance_loss: 0.2726, weighted_loss: 0.1212, label: 0, bag_size: 14305\n",
      "batch 59, loss: 0.0004, instance_loss: 0.4551, weighted_loss: 0.1368, label: 1, bag_size: 2638\n",
      "batch 79, loss: 0.0223, instance_loss: 1.1668, weighted_loss: 0.3657, label: 1, bag_size: 9644\n",
      "batch 99, loss: 0.0376, instance_loss: 0.0207, weighted_loss: 0.0325, label: 0, bag_size: 9786\n",
      "batch 119, loss: 0.0036, instance_loss: 0.6052, weighted_loss: 0.1841, label: 1, bag_size: 1014\n",
      "batch 139, loss: 0.0251, instance_loss: 0.5994, weighted_loss: 0.1974, label: 1, bag_size: 2140\n",
      "batch 159, loss: 0.0070, instance_loss: 0.5044, weighted_loss: 0.1563, label: 1, bag_size: 15716\n",
      "batch 179, loss: 0.7284, instance_loss: 0.6992, weighted_loss: 0.7196, label: 1, bag_size: 3211\n",
      "batch 199, loss: 0.0016, instance_loss: 0.1506, weighted_loss: 0.0463, label: 0, bag_size: 763\n",
      "batch 219, loss: 0.0358, instance_loss: 0.4297, weighted_loss: 0.1540, label: 1, bag_size: 3453\n",
      "batch 239, loss: 0.4703, instance_loss: 0.7699, weighted_loss: 0.5602, label: 0, bag_size: 18777\n",
      "batch 259, loss: 1.2384, instance_loss: 1.4024, weighted_loss: 1.2876, label: 1, bag_size: 5903\n",
      "batch 279, loss: 0.0379, instance_loss: 0.5690, weighted_loss: 0.1972, label: 1, bag_size: 5345\n",
      "batch 299, loss: 2.4450, instance_loss: 1.2655, weighted_loss: 2.0911, label: 0, bag_size: 2351\n",
      "batch 319, loss: 0.2172, instance_loss: 0.6659, weighted_loss: 0.3518, label: 1, bag_size: 1838\n",
      "batch 339, loss: 0.0115, instance_loss: 0.4514, weighted_loss: 0.1435, label: 1, bag_size: 20767\n",
      "batch 359, loss: 0.0006, instance_loss: 0.0057, weighted_loss: 0.0021, label: 0, bag_size: 6652\n",
      "batch 379, loss: 0.1450, instance_loss: 0.1496, weighted_loss: 0.1464, label: 0, bag_size: 12149\n",
      "batch 399, loss: 0.2833, instance_loss: 0.6299, weighted_loss: 0.3873, label: 1, bag_size: 16548\n",
      "batch 419, loss: 0.1601, instance_loss: 0.4829, weighted_loss: 0.2569, label: 1, bag_size: 8103\n",
      "batch 439, loss: 0.0198, instance_loss: 0.0919, weighted_loss: 0.0414, label: 0, bag_size: 11477\n",
      "batch 459, loss: 0.0361, instance_loss: 0.1701, weighted_loss: 0.0763, label: 0, bag_size: 1712\n",
      "batch 479, loss: 0.0625, instance_loss: 0.2523, weighted_loss: 0.1195, label: 0, bag_size: 5551\n",
      "batch 499, loss: 0.0028, instance_loss: 0.4659, weighted_loss: 0.1418, label: 1, bag_size: 2278\n",
      "batch 519, loss: 0.4086, instance_loss: 0.8610, weighted_loss: 0.5443, label: 1, bag_size: 3968\n",
      "batch 539, loss: 0.1292, instance_loss: 0.4564, weighted_loss: 0.2274, label: 1, bag_size: 6734\n",
      "batch 559, loss: 0.0016, instance_loss: 0.0039, weighted_loss: 0.0023, label: 0, bag_size: 31106\n",
      "batch 579, loss: 0.0494, instance_loss: 0.3768, weighted_loss: 0.1476, label: 1, bag_size: 9689\n",
      "batch 599, loss: 0.0363, instance_loss: 0.4556, weighted_loss: 0.1621, label: 1, bag_size: 16051\n",
      "batch 619, loss: 0.0052, instance_loss: 0.4441, weighted_loss: 0.1368, label: 1, bag_size: 6090\n",
      "batch 639, loss: 1.0288, instance_loss: 1.2786, weighted_loss: 1.1037, label: 1, bag_size: 10460\n",
      "batch 659, loss: 0.2992, instance_loss: 0.9242, weighted_loss: 0.4867, label: 0, bag_size: 23618\n",
      "batch 679, loss: 0.0399, instance_loss: 0.0214, weighted_loss: 0.0343, label: 0, bag_size: 18415\n",
      "batch 699, loss: 0.0052, instance_loss: 0.4427, weighted_loss: 0.1365, label: 0, bag_size: 15313\n",
      "batch 719, loss: 0.0057, instance_loss: 0.0670, weighted_loss: 0.0241, label: 0, bag_size: 19470\n",
      "batch 739, loss: 0.3598, instance_loss: 0.3096, weighted_loss: 0.3447, label: 0, bag_size: 2732\n",
      "batch 759, loss: 0.3005, instance_loss: 0.4691, weighted_loss: 0.3511, label: 1, bag_size: 5340\n",
      "batch 779, loss: 0.0084, instance_loss: 0.0159, weighted_loss: 0.0107, label: 0, bag_size: 19470\n",
      "batch 799, loss: 0.2573, instance_loss: 0.3064, weighted_loss: 0.2720, label: 0, bag_size: 11212\n",
      "batch 819, loss: 0.0144, instance_loss: 0.2112, weighted_loss: 0.0734, label: 1, bag_size: 10969\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9483993902439024: correct 12443/13120\n",
      "class 1 clustering acc 0.626219512195122: correct 4108/6560\n",
      "Epoch: 6, train_loss: 0.3898, train_clustering_loss:  0.5665, train_error: 0.1488\n",
      "class 0: acc 0.851581508515815, correct 350/411\n",
      "class 1: acc 0.8508557457212714, correct 348/409\n",
      "\n",
      "Val Set, val_loss: 0.2629, val_error: 0.1091, auc: 0.9605\n",
      "class 0 clustering acc 0.9926136363636363: correct 1747/1760\n",
      "class 1 clustering acc 0.35454545454545455: correct 312/880\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "Validation loss decreased (0.269856 --> 0.262937).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1937, instance_loss: 0.4821, weighted_loss: 0.2802, label: 0, bag_size: 3783\n",
      "batch 39, loss: 0.0044, instance_loss: 0.0087, weighted_loss: 0.0057, label: 0, bag_size: 18045\n",
      "batch 59, loss: 0.0179, instance_loss: 0.0637, weighted_loss: 0.0317, label: 0, bag_size: 10535\n",
      "batch 79, loss: 0.0425, instance_loss: 0.1368, weighted_loss: 0.0708, label: 0, bag_size: 1684\n",
      "batch 99, loss: 0.0744, instance_loss: 0.2440, weighted_loss: 0.1253, label: 1, bag_size: 16162\n",
      "batch 119, loss: 0.4356, instance_loss: 0.8055, weighted_loss: 0.5466, label: 1, bag_size: 6478\n",
      "batch 139, loss: 0.2623, instance_loss: 0.3653, weighted_loss: 0.2932, label: 1, bag_size: 6781\n",
      "batch 159, loss: 0.0342, instance_loss: 0.3208, weighted_loss: 0.1202, label: 0, bag_size: 2160\n",
      "batch 179, loss: 1.9229, instance_loss: 1.5649, weighted_loss: 1.8155, label: 0, bag_size: 2179\n",
      "batch 199, loss: 0.0115, instance_loss: 0.4275, weighted_loss: 0.1363, label: 1, bag_size: 8466\n",
      "batch 219, loss: 0.0935, instance_loss: 0.1592, weighted_loss: 0.1132, label: 0, bag_size: 1712\n",
      "batch 239, loss: 2.7086, instance_loss: 1.3253, weighted_loss: 2.2936, label: 1, bag_size: 9162\n",
      "batch 259, loss: 0.5215, instance_loss: 1.3724, weighted_loss: 0.7768, label: 0, bag_size: 11306\n",
      "batch 279, loss: 0.1165, instance_loss: 0.4272, weighted_loss: 0.2097, label: 0, bag_size: 11212\n",
      "batch 299, loss: 0.0407, instance_loss: 0.2252, weighted_loss: 0.0960, label: 0, bag_size: 6850\n",
      "batch 319, loss: 4.2476, instance_loss: 3.7514, weighted_loss: 4.0988, label: 0, bag_size: 1637\n",
      "batch 339, loss: 0.0556, instance_loss: 0.1775, weighted_loss: 0.0922, label: 0, bag_size: 31780\n",
      "batch 359, loss: 0.2264, instance_loss: 0.4407, weighted_loss: 0.2907, label: 0, bag_size: 3657\n",
      "batch 379, loss: 0.0057, instance_loss: 0.2630, weighted_loss: 0.0829, label: 1, bag_size: 8685\n",
      "batch 399, loss: 0.2585, instance_loss: 0.7779, weighted_loss: 0.4143, label: 0, bag_size: 2043\n",
      "batch 419, loss: 0.0094, instance_loss: 0.3055, weighted_loss: 0.0982, label: 1, bag_size: 4128\n",
      "batch 439, loss: 0.0042, instance_loss: 0.0119, weighted_loss: 0.0065, label: 0, bag_size: 8252\n",
      "batch 459, loss: 0.0009, instance_loss: 0.3074, weighted_loss: 0.0928, label: 1, bag_size: 10920\n",
      "batch 479, loss: 0.0337, instance_loss: 0.2764, weighted_loss: 0.1065, label: 0, bag_size: 13795\n",
      "batch 499, loss: 0.0673, instance_loss: 0.2023, weighted_loss: 0.1078, label: 0, bag_size: 7011\n",
      "batch 519, loss: 0.0651, instance_loss: 0.1741, weighted_loss: 0.0978, label: 1, bag_size: 10033\n",
      "batch 539, loss: 0.0795, instance_loss: 0.4256, weighted_loss: 0.1833, label: 1, bag_size: 13692\n",
      "batch 559, loss: 0.0082, instance_loss: 0.1708, weighted_loss: 0.0569, label: 1, bag_size: 15233\n",
      "batch 579, loss: 0.0028, instance_loss: 0.0145, weighted_loss: 0.0063, label: 0, bag_size: 2652\n",
      "batch 599, loss: 0.0017, instance_loss: 0.0109, weighted_loss: 0.0045, label: 0, bag_size: 22681\n",
      "batch 619, loss: 1.1012, instance_loss: 1.4360, weighted_loss: 1.2017, label: 0, bag_size: 9597\n",
      "batch 639, loss: 1.5655, instance_loss: 2.2442, weighted_loss: 1.7691, label: 1, bag_size: 2681\n",
      "batch 659, loss: 0.0286, instance_loss: 0.3615, weighted_loss: 0.1285, label: 0, bag_size: 2036\n",
      "batch 679, loss: 0.0010, instance_loss: 0.2500, weighted_loss: 0.0757, label: 1, bag_size: 645\n",
      "batch 699, loss: 0.0516, instance_loss: 0.1120, weighted_loss: 0.0697, label: 0, bag_size: 2367\n",
      "batch 719, loss: 0.0379, instance_loss: 0.1481, weighted_loss: 0.0710, label: 0, bag_size: 17155\n",
      "batch 739, loss: 0.0011, instance_loss: 0.1082, weighted_loss: 0.0332, label: 1, bag_size: 7798\n",
      "batch 759, loss: 0.0118, instance_loss: 0.0748, weighted_loss: 0.0307, label: 1, bag_size: 8602\n",
      "batch 779, loss: 0.0449, instance_loss: 0.0990, weighted_loss: 0.0611, label: 0, bag_size: 8755\n",
      "batch 799, loss: 0.0889, instance_loss: 0.1594, weighted_loss: 0.1100, label: 0, bag_size: 15967\n",
      "batch 819, loss: 0.0085, instance_loss: 0.2304, weighted_loss: 0.0751, label: 0, bag_size: 10444\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.947560975609756: correct 12432/13120\n",
      "class 1 clustering acc 0.6490853658536585: correct 4258/6560\n",
      "Epoch: 7, train_loss: 0.3430, train_clustering_loss:  0.5351, train_error: 0.1451\n",
      "class 0: acc 0.8535353535353535, correct 338/396\n",
      "class 1: acc 0.8561320754716981, correct 363/424\n",
      "\n",
      "Val Set, val_loss: 0.2916, val_error: 0.1364, auc: 0.9642\n",
      "class 0 clustering acc 0.9863636363636363: correct 1736/1760\n",
      "class 1 clustering acc 0.33636363636363636: correct 296/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.7758620689655172, correct 45/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1680, instance_loss: 0.2755, weighted_loss: 0.2002, label: 1, bag_size: 18468\n",
      "batch 39, loss: 0.2666, instance_loss: 0.3696, weighted_loss: 0.2975, label: 0, bag_size: 11390\n",
      "batch 59, loss: 0.7854, instance_loss: 0.5077, weighted_loss: 0.7021, label: 1, bag_size: 22264\n",
      "batch 79, loss: 0.0285, instance_loss: 0.4202, weighted_loss: 0.1460, label: 1, bag_size: 18161\n",
      "batch 99, loss: 0.0035, instance_loss: 0.1755, weighted_loss: 0.0551, label: 0, bag_size: 16782\n",
      "batch 119, loss: 0.0148, instance_loss: 0.1111, weighted_loss: 0.0437, label: 1, bag_size: 10392\n",
      "batch 139, loss: 0.0112, instance_loss: 0.0750, weighted_loss: 0.0303, label: 1, bag_size: 21701\n",
      "batch 159, loss: 0.1545, instance_loss: 0.3454, weighted_loss: 0.2118, label: 0, bag_size: 25814\n",
      "batch 179, loss: 0.0027, instance_loss: 0.1065, weighted_loss: 0.0338, label: 0, bag_size: 14956\n",
      "batch 199, loss: 0.0302, instance_loss: 0.2246, weighted_loss: 0.0885, label: 1, bag_size: 7613\n",
      "batch 219, loss: 0.0076, instance_loss: 0.0229, weighted_loss: 0.0122, label: 0, bag_size: 21404\n",
      "batch 239, loss: 0.5198, instance_loss: 0.5954, weighted_loss: 0.5425, label: 0, bag_size: 2382\n",
      "batch 259, loss: 0.6655, instance_loss: 1.9767, weighted_loss: 1.0588, label: 0, bag_size: 17279\n",
      "batch 279, loss: 0.0051, instance_loss: 0.3566, weighted_loss: 0.1106, label: 1, bag_size: 7798\n",
      "batch 299, loss: 0.0323, instance_loss: 0.0499, weighted_loss: 0.0376, label: 1, bag_size: 15093\n",
      "batch 319, loss: 0.0481, instance_loss: 0.0548, weighted_loss: 0.0501, label: 1, bag_size: 9878\n",
      "batch 339, loss: 0.0331, instance_loss: 0.2036, weighted_loss: 0.0843, label: 0, bag_size: 11122\n",
      "batch 359, loss: 0.0733, instance_loss: 0.4218, weighted_loss: 0.1779, label: 0, bag_size: 12524\n",
      "batch 379, loss: 0.0419, instance_loss: 0.3484, weighted_loss: 0.1338, label: 1, bag_size: 6745\n",
      "batch 399, loss: 0.5501, instance_loss: 1.9297, weighted_loss: 0.9640, label: 0, bag_size: 1701\n",
      "batch 419, loss: 0.6271, instance_loss: 1.7877, weighted_loss: 0.9753, label: 1, bag_size: 11729\n",
      "batch 439, loss: 2.1179, instance_loss: 2.8192, weighted_loss: 2.3283, label: 1, bag_size: 1493\n",
      "batch 459, loss: 1.2811, instance_loss: 1.4952, weighted_loss: 1.3454, label: 1, bag_size: 2937\n",
      "batch 479, loss: 0.0377, instance_loss: 0.6495, weighted_loss: 0.2212, label: 0, bag_size: 1438\n",
      "batch 499, loss: 0.0655, instance_loss: 0.5184, weighted_loss: 0.2014, label: 0, bag_size: 22800\n",
      "batch 519, loss: 0.3436, instance_loss: 1.1377, weighted_loss: 0.5818, label: 1, bag_size: 1845\n",
      "batch 539, loss: 0.1369, instance_loss: 0.5621, weighted_loss: 0.2644, label: 1, bag_size: 4128\n",
      "batch 559, loss: 0.0024, instance_loss: 0.1415, weighted_loss: 0.0441, label: 1, bag_size: 11032\n",
      "batch 579, loss: 0.4999, instance_loss: 0.9195, weighted_loss: 0.6258, label: 1, bag_size: 4394\n",
      "batch 599, loss: 0.0733, instance_loss: 0.5500, weighted_loss: 0.2163, label: 1, bag_size: 8475\n",
      "batch 619, loss: 0.1199, instance_loss: 0.4949, weighted_loss: 0.2324, label: 1, bag_size: 15093\n",
      "batch 639, loss: 0.7748, instance_loss: 2.2185, weighted_loss: 1.2079, label: 0, bag_size: 7428\n",
      "batch 659, loss: 0.8245, instance_loss: 0.9429, weighted_loss: 0.8600, label: 1, bag_size: 4956\n",
      "batch 679, loss: 0.2021, instance_loss: 0.5995, weighted_loss: 0.3213, label: 0, bag_size: 2296\n",
      "batch 699, loss: 0.0002, instance_loss: 0.3686, weighted_loss: 0.1107, label: 0, bag_size: 10535\n",
      "batch 719, loss: 0.1727, instance_loss: 0.3614, weighted_loss: 0.2293, label: 1, bag_size: 4929\n",
      "batch 739, loss: 0.0019, instance_loss: 0.1267, weighted_loss: 0.0393, label: 1, bag_size: 9673\n",
      "batch 759, loss: 0.0681, instance_loss: 0.2878, weighted_loss: 0.1340, label: 1, bag_size: 1823\n",
      "batch 779, loss: 0.0005, instance_loss: 0.2301, weighted_loss: 0.0694, label: 1, bag_size: 13026\n",
      "batch 799, loss: 0.1442, instance_loss: 0.3077, weighted_loss: 0.1933, label: 1, bag_size: 7371\n",
      "batch 819, loss: 0.3108, instance_loss: 0.8153, weighted_loss: 0.4621, label: 1, bag_size: 4929\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9304115853658537: correct 12207/13120\n",
      "class 1 clustering acc 0.583079268292683: correct 3825/6560\n",
      "Epoch: 8, train_loss: 0.3691, train_clustering_loss:  0.6397, train_error: 0.1402\n",
      "class 0: acc 0.8542713567839196, correct 340/398\n",
      "class 1: acc 0.8649289099526066, correct 365/422\n",
      "\n",
      "Val Set, val_loss: 0.2658, val_error: 0.1273, auc: 0.9625\n",
      "class 0 clustering acc 0.9954545454545455: correct 1752/1760\n",
      "class 1 clustering acc 0.3886363636363636: correct 342/880\n",
      "class 0: acc 0.8076923076923077, correct 42/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 2.2064, instance_loss: 2.6292, weighted_loss: 2.3332, label: 1, bag_size: 16514\n",
      "batch 39, loss: 0.0010, instance_loss: 0.1816, weighted_loss: 0.0551, label: 0, bag_size: 10481\n",
      "batch 59, loss: 0.6167, instance_loss: 0.5983, weighted_loss: 0.6112, label: 0, bag_size: 1560\n",
      "batch 79, loss: 1.4023, instance_loss: 1.2627, weighted_loss: 1.3604, label: 1, bag_size: 8040\n",
      "batch 99, loss: 0.0033, instance_loss: 0.1552, weighted_loss: 0.0489, label: 0, bag_size: 13225\n",
      "batch 119, loss: 0.0604, instance_loss: 0.1443, weighted_loss: 0.0856, label: 0, bag_size: 22426\n",
      "batch 139, loss: 0.2538, instance_loss: 0.3052, weighted_loss: 0.2692, label: 1, bag_size: 12719\n",
      "batch 159, loss: 3.1402, instance_loss: 1.9987, weighted_loss: 2.7977, label: 0, bag_size: 1498\n",
      "batch 179, loss: 0.0567, instance_loss: 0.3953, weighted_loss: 0.1583, label: 1, bag_size: 11642\n",
      "batch 199, loss: 0.4431, instance_loss: 0.7109, weighted_loss: 0.5234, label: 0, bag_size: 931\n",
      "batch 219, loss: 0.1787, instance_loss: 0.3915, weighted_loss: 0.2425, label: 1, bag_size: 12178\n",
      "batch 239, loss: 0.0120, instance_loss: 0.1483, weighted_loss: 0.0529, label: 1, bag_size: 8012\n",
      "batch 259, loss: 0.0008, instance_loss: 0.1232, weighted_loss: 0.0375, label: 1, bag_size: 5612\n",
      "batch 279, loss: 0.0220, instance_loss: 0.3374, weighted_loss: 0.1166, label: 0, bag_size: 2424\n",
      "batch 299, loss: 0.0152, instance_loss: 0.1031, weighted_loss: 0.0416, label: 0, bag_size: 8866\n",
      "batch 319, loss: 0.1064, instance_loss: 0.1545, weighted_loss: 0.1208, label: 0, bag_size: 24911\n",
      "batch 339, loss: 0.4473, instance_loss: 0.9705, weighted_loss: 0.6043, label: 0, bag_size: 18215\n",
      "batch 359, loss: 0.0529, instance_loss: 0.2726, weighted_loss: 0.1188, label: 0, bag_size: 25814\n",
      "batch 379, loss: 0.7261, instance_loss: 1.3285, weighted_loss: 0.9068, label: 1, bag_size: 13089\n",
      "batch 399, loss: 8.7831, instance_loss: 3.9470, weighted_loss: 7.3322, label: 0, bag_size: 3468\n",
      "batch 419, loss: 0.0640, instance_loss: 0.4536, weighted_loss: 0.1809, label: 0, bag_size: 9455\n",
      "batch 439, loss: 0.0234, instance_loss: 0.0912, weighted_loss: 0.0438, label: 1, bag_size: 1622\n",
      "batch 459, loss: 0.0107, instance_loss: 0.0876, weighted_loss: 0.0338, label: 1, bag_size: 4880\n",
      "batch 479, loss: 0.4460, instance_loss: 0.3843, weighted_loss: 0.4275, label: 1, bag_size: 16890\n",
      "batch 499, loss: 0.0419, instance_loss: 0.2727, weighted_loss: 0.1111, label: 1, bag_size: 11421\n",
      "batch 519, loss: 0.2201, instance_loss: 0.4928, weighted_loss: 0.3019, label: 0, bag_size: 1690\n",
      "batch 539, loss: 0.0290, instance_loss: 0.1752, weighted_loss: 0.0729, label: 1, bag_size: 22286\n",
      "batch 559, loss: 0.2449, instance_loss: 0.4422, weighted_loss: 0.3041, label: 1, bag_size: 5256\n",
      "batch 579, loss: 0.0083, instance_loss: 0.1231, weighted_loss: 0.0427, label: 1, bag_size: 17769\n",
      "batch 599, loss: 0.0083, instance_loss: 0.2733, weighted_loss: 0.0878, label: 1, bag_size: 699\n",
      "batch 619, loss: 0.0052, instance_loss: 0.0901, weighted_loss: 0.0307, label: 0, bag_size: 1234\n",
      "batch 639, loss: 0.0493, instance_loss: 0.1942, weighted_loss: 0.0928, label: 1, bag_size: 6726\n",
      "batch 659, loss: 0.0125, instance_loss: 0.0797, weighted_loss: 0.0327, label: 0, bag_size: 11527\n",
      "batch 679, loss: 0.0002, instance_loss: 0.0317, weighted_loss: 0.0097, label: 0, bag_size: 1984\n",
      "batch 699, loss: 0.0008, instance_loss: 0.0063, weighted_loss: 0.0025, label: 0, bag_size: 20150\n",
      "batch 719, loss: 0.6820, instance_loss: 0.6888, weighted_loss: 0.6840, label: 0, bag_size: 10381\n",
      "batch 739, loss: 0.0926, instance_loss: 0.1416, weighted_loss: 0.1073, label: 1, bag_size: 10591\n",
      "batch 759, loss: 0.1771, instance_loss: 0.3219, weighted_loss: 0.2205, label: 1, bag_size: 7351\n",
      "batch 779, loss: 0.0346, instance_loss: 0.1714, weighted_loss: 0.0757, label: 1, bag_size: 7119\n",
      "batch 799, loss: 0.0035, instance_loss: 0.0397, weighted_loss: 0.0144, label: 0, bag_size: 24911\n",
      "batch 819, loss: 0.2353, instance_loss: 0.3340, weighted_loss: 0.2649, label: 1, bag_size: 3450\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9578506097560976: correct 12567/13120\n",
      "class 1 clustering acc 0.7077743902439024: correct 4643/6560\n",
      "Epoch: 9, train_loss: 0.3166, train_clustering_loss:  0.4635, train_error: 0.1329\n",
      "class 0: acc 0.8664921465968587, correct 331/382\n",
      "class 1: acc 0.867579908675799, correct 380/438\n",
      "\n",
      "Val Set, val_loss: 0.2426, val_error: 0.1091, auc: 0.9682\n",
      "class 0 clustering acc 0.9846590909090909: correct 1733/1760\n",
      "class 1 clustering acc 0.7079545454545455: correct 623/880\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "Validation loss decreased (0.262937 --> 0.242626).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0453, instance_loss: 0.1039, weighted_loss: 0.0629, label: 1, bag_size: 14604\n",
      "batch 39, loss: 0.0039, instance_loss: 0.0126, weighted_loss: 0.0065, label: 0, bag_size: 14249\n",
      "batch 59, loss: 0.1328, instance_loss: 0.1256, weighted_loss: 0.1306, label: 0, bag_size: 11281\n",
      "batch 79, loss: 0.0033, instance_loss: 0.0338, weighted_loss: 0.0125, label: 1, bag_size: 22264\n",
      "batch 99, loss: 0.2523, instance_loss: 0.1973, weighted_loss: 0.2358, label: 0, bag_size: 7835\n",
      "batch 119, loss: 0.1998, instance_loss: 0.3791, weighted_loss: 0.2536, label: 1, bag_size: 2278\n",
      "batch 139, loss: 0.0108, instance_loss: 0.0394, weighted_loss: 0.0194, label: 1, bag_size: 7217\n",
      "batch 159, loss: 0.3135, instance_loss: 0.4022, weighted_loss: 0.3401, label: 0, bag_size: 11128\n",
      "batch 179, loss: 0.0527, instance_loss: 0.0893, weighted_loss: 0.0637, label: 1, bag_size: 18161\n",
      "batch 199, loss: 0.0362, instance_loss: 0.1407, weighted_loss: 0.0675, label: 0, bag_size: 12732\n",
      "batch 219, loss: 0.0252, instance_loss: 0.0418, weighted_loss: 0.0302, label: 1, bag_size: 19606\n",
      "batch 239, loss: 0.0017, instance_loss: 0.1361, weighted_loss: 0.0421, label: 1, bag_size: 1781\n",
      "batch 259, loss: 0.0022, instance_loss: 0.0274, weighted_loss: 0.0097, label: 1, bag_size: 6317\n",
      "batch 279, loss: 0.8780, instance_loss: 1.1089, weighted_loss: 0.9473, label: 1, bag_size: 12946\n",
      "batch 299, loss: 0.0376, instance_loss: 0.0592, weighted_loss: 0.0441, label: 1, bag_size: 5292\n",
      "batch 319, loss: 0.0198, instance_loss: 0.0194, weighted_loss: 0.0197, label: 0, bag_size: 9888\n",
      "batch 339, loss: 2.7884, instance_loss: 3.2785, weighted_loss: 2.9354, label: 1, bag_size: 11316\n",
      "batch 359, loss: 0.0122, instance_loss: 0.0071, weighted_loss: 0.0107, label: 1, bag_size: 9147\n",
      "batch 379, loss: 0.0176, instance_loss: 0.4884, weighted_loss: 0.1588, label: 1, bag_size: 8019\n",
      "batch 399, loss: 0.0263, instance_loss: 0.1023, weighted_loss: 0.0491, label: 1, bag_size: 11220\n",
      "batch 419, loss: 0.0025, instance_loss: 0.0420, weighted_loss: 0.0143, label: 0, bag_size: 16052\n",
      "batch 439, loss: 0.3112, instance_loss: 0.1692, weighted_loss: 0.2686, label: 0, bag_size: 18777\n",
      "batch 459, loss: 0.0061, instance_loss: 0.0257, weighted_loss: 0.0120, label: 0, bag_size: 11194\n",
      "batch 479, loss: 0.0851, instance_loss: 0.0891, weighted_loss: 0.0863, label: 0, bag_size: 10444\n",
      "batch 499, loss: 0.2012, instance_loss: 0.8984, weighted_loss: 0.4104, label: 0, bag_size: 1789\n",
      "batch 519, loss: 0.0110, instance_loss: 0.3039, weighted_loss: 0.0988, label: 0, bag_size: 2303\n",
      "batch 539, loss: 0.0540, instance_loss: 0.1996, weighted_loss: 0.0977, label: 0, bag_size: 2624\n",
      "batch 559, loss: 0.0027, instance_loss: 0.0114, weighted_loss: 0.0053, label: 0, bag_size: 18154\n",
      "batch 579, loss: 0.1236, instance_loss: 0.0689, weighted_loss: 0.1072, label: 1, bag_size: 11701\n",
      "batch 599, loss: 0.0042, instance_loss: 0.0623, weighted_loss: 0.0216, label: 0, bag_size: 4902\n",
      "batch 619, loss: 0.2470, instance_loss: 0.5596, weighted_loss: 0.3408, label: 0, bag_size: 20555\n",
      "batch 639, loss: 0.0077, instance_loss: 0.0773, weighted_loss: 0.0286, label: 1, bag_size: 5494\n",
      "batch 659, loss: 0.0072, instance_loss: 0.0181, weighted_loss: 0.0105, label: 0, bag_size: 9415\n",
      "batch 679, loss: 2.7415, instance_loss: 2.0787, weighted_loss: 2.5426, label: 0, bag_size: 2219\n",
      "batch 699, loss: 0.0260, instance_loss: 0.0311, weighted_loss: 0.0275, label: 0, bag_size: 11113\n",
      "batch 719, loss: 0.0001, instance_loss: 0.1583, weighted_loss: 0.0476, label: 0, bag_size: 3787\n",
      "batch 739, loss: 1.1189, instance_loss: 1.2064, weighted_loss: 1.1452, label: 0, bag_size: 5009\n",
      "batch 759, loss: 0.0054, instance_loss: 0.0150, weighted_loss: 0.0083, label: 0, bag_size: 1962\n",
      "batch 779, loss: 1.0494, instance_loss: 1.9290, weighted_loss: 1.3133, label: 1, bag_size: 13367\n",
      "batch 799, loss: 2.4505, instance_loss: 2.4308, weighted_loss: 2.4446, label: 1, bag_size: 13089\n",
      "batch 819, loss: 0.1967, instance_loss: 0.5054, weighted_loss: 0.2893, label: 1, bag_size: 7989\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9596798780487805: correct 12591/13120\n",
      "class 1 clustering acc 0.7698170731707317: correct 5050/6560\n",
      "Epoch: 10, train_loss: 0.3252, train_clustering_loss:  0.4317, train_error: 0.1232\n",
      "class 0: acc 0.8716049382716049, correct 353/405\n",
      "class 1: acc 0.8819277108433735, correct 366/415\n",
      "\n",
      "Val Set, val_loss: 0.2920, val_error: 0.1273, auc: 0.9645\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.3375: correct 297/880\n",
      "class 0: acc 0.7692307692307693, correct 40/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0103, instance_loss: 0.0638, weighted_loss: 0.0263, label: 0, bag_size: 18240\n",
      "batch 39, loss: 0.0338, instance_loss: 0.0187, weighted_loss: 0.0292, label: 1, bag_size: 16512\n",
      "batch 59, loss: 0.1261, instance_loss: 0.4830, weighted_loss: 0.2331, label: 1, bag_size: 1014\n",
      "batch 79, loss: 0.1468, instance_loss: 0.2737, weighted_loss: 0.1849, label: 0, bag_size: 3101\n",
      "batch 99, loss: 0.0002, instance_loss: 0.2465, weighted_loss: 0.0741, label: 0, bag_size: 5225\n",
      "batch 119, loss: 0.0023, instance_loss: 0.0077, weighted_loss: 0.0039, label: 1, bag_size: 1781\n",
      "batch 139, loss: 0.4294, instance_loss: 0.5744, weighted_loss: 0.4729, label: 0, bag_size: 18516\n",
      "batch 159, loss: 0.0183, instance_loss: 0.0186, weighted_loss: 0.0184, label: 0, bag_size: 1588\n",
      "batch 179, loss: 0.0034, instance_loss: 0.0758, weighted_loss: 0.0252, label: 1, bag_size: 4054\n",
      "batch 199, loss: 0.1333, instance_loss: 0.2009, weighted_loss: 0.1536, label: 1, bag_size: 14515\n",
      "batch 219, loss: 4.9247, instance_loss: 5.0839, weighted_loss: 4.9725, label: 1, bag_size: 1703\n",
      "batch 239, loss: 0.0362, instance_loss: 0.1685, weighted_loss: 0.0759, label: 1, bag_size: 6665\n",
      "batch 259, loss: 0.0022, instance_loss: 0.2539, weighted_loss: 0.0777, label: 1, bag_size: 617\n",
      "batch 279, loss: 3.2657, instance_loss: 3.7100, weighted_loss: 3.3990, label: 1, bag_size: 13089\n",
      "batch 299, loss: 0.0821, instance_loss: 0.0122, weighted_loss: 0.0612, label: 1, bag_size: 4128\n",
      "batch 319, loss: 0.0004, instance_loss: 0.1043, weighted_loss: 0.0315, label: 0, bag_size: 1234\n",
      "batch 339, loss: 0.0441, instance_loss: 0.0949, weighted_loss: 0.0593, label: 0, bag_size: 12217\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0088, weighted_loss: 0.0027, label: 0, bag_size: 8981\n",
      "batch 379, loss: 0.0397, instance_loss: 0.0537, weighted_loss: 0.0439, label: 1, bag_size: 8680\n",
      "batch 399, loss: 0.1888, instance_loss: 0.0433, weighted_loss: 0.1452, label: 1, bag_size: 8754\n",
      "batch 419, loss: 0.0778, instance_loss: 0.0928, weighted_loss: 0.0823, label: 0, bag_size: 2104\n",
      "batch 439, loss: 0.0086, instance_loss: 0.0101, weighted_loss: 0.0090, label: 1, bag_size: 9004\n",
      "batch 459, loss: 4.7758, instance_loss: 4.5058, weighted_loss: 4.6948, label: 0, bag_size: 2694\n",
      "batch 479, loss: 0.1700, instance_loss: 0.1031, weighted_loss: 0.1499, label: 0, bag_size: 65728\n",
      "batch 499, loss: 0.1167, instance_loss: 0.1513, weighted_loss: 0.1271, label: 0, bag_size: 3893\n",
      "batch 519, loss: 0.0584, instance_loss: 0.0291, weighted_loss: 0.0496, label: 0, bag_size: 19043\n",
      "batch 539, loss: 0.2822, instance_loss: 0.1245, weighted_loss: 0.2349, label: 1, bag_size: 6205\n",
      "batch 559, loss: 0.0087, instance_loss: 0.0921, weighted_loss: 0.0337, label: 1, bag_size: 9408\n",
      "batch 579, loss: 0.2600, instance_loss: 0.4675, weighted_loss: 0.3223, label: 1, bag_size: 1483\n",
      "batch 599, loss: 3.0371, instance_loss: 2.7439, weighted_loss: 2.9492, label: 0, bag_size: 15898\n",
      "batch 619, loss: 0.0212, instance_loss: 0.0834, weighted_loss: 0.0398, label: 1, bag_size: 7217\n",
      "batch 639, loss: 0.1368, instance_loss: 0.3452, weighted_loss: 0.1993, label: 0, bag_size: 3810\n",
      "batch 659, loss: 0.0135, instance_loss: 0.1041, weighted_loss: 0.0407, label: 0, bag_size: 2732\n",
      "batch 679, loss: 0.6839, instance_loss: 0.4874, weighted_loss: 0.6249, label: 0, bag_size: 7381\n",
      "batch 699, loss: 0.0190, instance_loss: 0.0152, weighted_loss: 0.0179, label: 1, bag_size: 8868\n",
      "batch 719, loss: 0.1996, instance_loss: 0.5848, weighted_loss: 0.3151, label: 0, bag_size: 11122\n",
      "batch 739, loss: 0.0217, instance_loss: 0.0315, weighted_loss: 0.0246, label: 1, bag_size: 9065\n",
      "batch 759, loss: 0.0302, instance_loss: 0.0504, weighted_loss: 0.0363, label: 0, bag_size: 9234\n",
      "batch 779, loss: 0.4017, instance_loss: 0.6067, weighted_loss: 0.4632, label: 1, bag_size: 8475\n",
      "batch 799, loss: 0.1748, instance_loss: 0.2254, weighted_loss: 0.1900, label: 1, bag_size: 9470\n",
      "batch 819, loss: 0.0078, instance_loss: 0.0056, weighted_loss: 0.0071, label: 0, bag_size: 931\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9636432926829268: correct 12643/13120\n",
      "class 1 clustering acc 0.806859756097561: correct 5293/6560\n",
      "Epoch: 11, train_loss: 0.2936, train_clustering_loss:  0.3637, train_error: 0.1085\n",
      "class 0: acc 0.8848920863309353, correct 369/417\n",
      "class 1: acc 0.8982630272952854, correct 362/403\n",
      "\n",
      "Val Set, val_loss: 0.2786, val_error: 0.1182, auc: 0.9649\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.36363636363636365: correct 320/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8103448275862069, correct 47/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0007, instance_loss: 0.0014, weighted_loss: 0.0009, label: 1, bag_size: 8466\n",
      "batch 39, loss: 0.0019, instance_loss: 0.0015, weighted_loss: 0.0018, label: 0, bag_size: 13225\n",
      "batch 59, loss: 0.0339, instance_loss: 0.0329, weighted_loss: 0.0336, label: 0, bag_size: 2091\n",
      "batch 79, loss: 0.0009, instance_loss: 0.0034, weighted_loss: 0.0016, label: 0, bag_size: 8948\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0271, weighted_loss: 0.0082, label: 0, bag_size: 10898\n",
      "batch 119, loss: 0.1660, instance_loss: 0.2157, weighted_loss: 0.1809, label: 1, bag_size: 1015\n",
      "batch 139, loss: 0.3869, instance_loss: 0.4223, weighted_loss: 0.3976, label: 1, bag_size: 2136\n",
      "batch 159, loss: 0.0870, instance_loss: 0.1628, weighted_loss: 0.1098, label: 0, bag_size: 2322\n",
      "batch 179, loss: 1.6849, instance_loss: 2.3795, weighted_loss: 1.8933, label: 1, bag_size: 2395\n",
      "batch 199, loss: 0.5437, instance_loss: 0.4752, weighted_loss: 0.5231, label: 0, bag_size: 1789\n",
      "batch 219, loss: 0.0331, instance_loss: 0.0555, weighted_loss: 0.0398, label: 1, bag_size: 4821\n",
      "batch 239, loss: 1.2916, instance_loss: 1.0846, weighted_loss: 1.2295, label: 1, bag_size: 12180\n",
      "batch 259, loss: 5.3670, instance_loss: 4.1906, weighted_loss: 5.0141, label: 0, bag_size: 2694\n",
      "batch 279, loss: 0.7779, instance_loss: 0.7688, weighted_loss: 0.7752, label: 0, bag_size: 1506\n",
      "batch 299, loss: 1.1052, instance_loss: 1.0588, weighted_loss: 1.0913, label: 0, bag_size: 1592\n",
      "batch 319, loss: 0.0027, instance_loss: 0.0066, weighted_loss: 0.0038, label: 0, bag_size: 15747\n",
      "batch 339, loss: 4.0061, instance_loss: 4.4326, weighted_loss: 4.1341, label: 1, bag_size: 3879\n",
      "batch 359, loss: 0.0276, instance_loss: 0.0241, weighted_loss: 0.0265, label: 1, bag_size: 6453\n",
      "batch 379, loss: 0.0180, instance_loss: 0.0218, weighted_loss: 0.0192, label: 1, bag_size: 13194\n",
      "batch 399, loss: 0.0026, instance_loss: 0.0040, weighted_loss: 0.0030, label: 0, bag_size: 15001\n",
      "batch 419, loss: 0.0714, instance_loss: 0.1275, weighted_loss: 0.0882, label: 0, bag_size: 5409\n",
      "batch 439, loss: 0.0037, instance_loss: 0.0033, weighted_loss: 0.0036, label: 0, bag_size: 10304\n",
      "batch 459, loss: 0.0244, instance_loss: 0.0075, weighted_loss: 0.0193, label: 1, bag_size: 12719\n",
      "batch 479, loss: 2.1818, instance_loss: 2.7123, weighted_loss: 2.3409, label: 1, bag_size: 1867\n",
      "batch 499, loss: 0.0956, instance_loss: 0.0083, weighted_loss: 0.0694, label: 1, bag_size: 12611\n",
      "batch 519, loss: 0.0078, instance_loss: 0.0074, weighted_loss: 0.0077, label: 1, bag_size: 3640\n",
      "batch 539, loss: 0.0098, instance_loss: 0.0255, weighted_loss: 0.0145, label: 0, bag_size: 16052\n",
      "batch 559, loss: 0.0049, instance_loss: 0.0015, weighted_loss: 0.0039, label: 0, bag_size: 23996\n",
      "batch 579, loss: 0.1297, instance_loss: 0.3576, weighted_loss: 0.1981, label: 1, bag_size: 6682\n",
      "batch 599, loss: 0.0029, instance_loss: 0.0157, weighted_loss: 0.0068, label: 0, bag_size: 1234\n",
      "batch 619, loss: 0.0117, instance_loss: 0.0091, weighted_loss: 0.0109, label: 1, bag_size: 5025\n",
      "batch 639, loss: 1.3307, instance_loss: 1.4742, weighted_loss: 1.3738, label: 1, bag_size: 7989\n",
      "batch 659, loss: 0.0070, instance_loss: 0.1840, weighted_loss: 0.0601, label: 1, bag_size: 14618\n",
      "batch 679, loss: 0.1709, instance_loss: 0.4527, weighted_loss: 0.2554, label: 1, bag_size: 8191\n",
      "batch 699, loss: 0.3540, instance_loss: 0.1846, weighted_loss: 0.3032, label: 0, bag_size: 4959\n",
      "batch 719, loss: 0.1833, instance_loss: 0.1335, weighted_loss: 0.1683, label: 1, bag_size: 5690\n",
      "batch 739, loss: 0.0448, instance_loss: 0.0512, weighted_loss: 0.0467, label: 0, bag_size: 2004\n",
      "batch 759, loss: 0.0707, instance_loss: 0.0630, weighted_loss: 0.0684, label: 0, bag_size: 3190\n",
      "batch 779, loss: 0.0573, instance_loss: 0.0453, weighted_loss: 0.0537, label: 1, bag_size: 20161\n",
      "batch 799, loss: 0.0245, instance_loss: 0.0092, weighted_loss: 0.0199, label: 0, bag_size: 8252\n",
      "batch 819, loss: 0.0072, instance_loss: 0.0094, weighted_loss: 0.0079, label: 0, bag_size: 2760\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9636432926829268: correct 12643/13120\n",
      "class 1 clustering acc 0.8228658536585366: correct 5398/6560\n",
      "Epoch: 12, train_loss: 0.2893, train_clustering_loss:  0.3481, train_error: 0.1171\n",
      "class 0: acc 0.8963133640552995, correct 389/434\n",
      "class 1: acc 0.8678756476683938, correct 335/386\n",
      "\n",
      "Val Set, val_loss: 0.3334, val_error: 0.1273, auc: 0.9708\n",
      "class 0 clustering acc 0.9926136363636363: correct 1747/1760\n",
      "class 1 clustering acc 0.08181818181818182: correct 72/880\n",
      "class 0: acc 1.0, correct 52/52\n",
      "class 1: acc 0.7586206896551724, correct 44/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0070, instance_loss: 0.0112, weighted_loss: 0.0083, label: 1, bag_size: 3409\n",
      "batch 39, loss: 0.0299, instance_loss: 0.0243, weighted_loss: 0.0282, label: 0, bag_size: 1881\n",
      "batch 59, loss: 0.0020, instance_loss: 0.0076, weighted_loss: 0.0036, label: 1, bag_size: 5317\n",
      "batch 79, loss: 1.9541, instance_loss: 3.3827, weighted_loss: 2.3827, label: 1, bag_size: 15185\n",
      "batch 99, loss: 0.2267, instance_loss: 0.9091, weighted_loss: 0.4314, label: 0, bag_size: 1953\n",
      "batch 119, loss: 1.0488, instance_loss: 1.4516, weighted_loss: 1.1697, label: 1, bag_size: 2731\n",
      "batch 139, loss: 0.0011, instance_loss: 0.0055, weighted_loss: 0.0024, label: 1, bag_size: 6745\n",
      "batch 159, loss: 0.0108, instance_loss: 0.0143, weighted_loss: 0.0118, label: 1, bag_size: 3082\n",
      "batch 179, loss: 1.6988, instance_loss: 2.2396, weighted_loss: 1.8611, label: 1, bag_size: 21252\n",
      "batch 199, loss: 1.5083, instance_loss: 1.7062, weighted_loss: 1.5677, label: 1, bag_size: 1493\n",
      "batch 219, loss: 0.0034, instance_loss: 0.0075, weighted_loss: 0.0046, label: 1, bag_size: 11875\n",
      "batch 239, loss: 3.2265, instance_loss: 2.9882, weighted_loss: 3.1550, label: 1, bag_size: 2455\n",
      "batch 259, loss: 0.0261, instance_loss: 0.0087, weighted_loss: 0.0209, label: 1, bag_size: 15689\n",
      "batch 279, loss: 0.0139, instance_loss: 0.0124, weighted_loss: 0.0134, label: 1, bag_size: 11256\n",
      "batch 299, loss: 0.0251, instance_loss: 0.0431, weighted_loss: 0.0305, label: 1, bag_size: 9408\n",
      "batch 319, loss: 0.5718, instance_loss: 0.7401, weighted_loss: 0.6223, label: 0, bag_size: 1684\n",
      "batch 339, loss: 0.0106, instance_loss: 0.0254, weighted_loss: 0.0150, label: 0, bag_size: 18738\n",
      "batch 359, loss: 0.0763, instance_loss: 0.0506, weighted_loss: 0.0686, label: 0, bag_size: 8788\n",
      "batch 379, loss: 0.0145, instance_loss: 0.0043, weighted_loss: 0.0114, label: 0, bag_size: 4465\n",
      "batch 399, loss: 0.3019, instance_loss: 0.5807, weighted_loss: 0.3856, label: 0, bag_size: 23796\n",
      "batch 419, loss: 4.9633, instance_loss: 4.7565, weighted_loss: 4.9013, label: 1, bag_size: 2731\n",
      "batch 439, loss: 2.3967, instance_loss: 2.4751, weighted_loss: 2.4202, label: 0, bag_size: 3897\n",
      "batch 459, loss: 0.0035, instance_loss: 0.0846, weighted_loss: 0.0279, label: 0, bag_size: 1824\n",
      "batch 479, loss: 0.0100, instance_loss: 0.0048, weighted_loss: 0.0084, label: 1, bag_size: 8868\n",
      "batch 499, loss: 0.0716, instance_loss: 0.0294, weighted_loss: 0.0589, label: 0, bag_size: 4271\n",
      "batch 519, loss: 0.0546, instance_loss: 0.0311, weighted_loss: 0.0476, label: 1, bag_size: 9610\n",
      "batch 539, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0015, label: 0, bag_size: 18225\n",
      "batch 559, loss: 0.0097, instance_loss: 0.0370, weighted_loss: 0.0179, label: 1, bag_size: 11884\n",
      "batch 579, loss: 0.3134, instance_loss: 0.5241, weighted_loss: 0.3766, label: 1, bag_size: 2785\n",
      "batch 599, loss: 0.0300, instance_loss: 0.0133, weighted_loss: 0.0250, label: 0, bag_size: 10128\n",
      "batch 619, loss: 0.0150, instance_loss: 0.0197, weighted_loss: 0.0164, label: 0, bag_size: 21138\n",
      "batch 639, loss: 0.0037, instance_loss: 0.0045, weighted_loss: 0.0039, label: 0, bag_size: 3725\n",
      "batch 659, loss: 0.0384, instance_loss: 0.0515, weighted_loss: 0.0423, label: 0, bag_size: 3774\n",
      "batch 679, loss: 0.0004, instance_loss: 0.0009, weighted_loss: 0.0006, label: 0, bag_size: 9470\n",
      "batch 699, loss: 0.2071, instance_loss: 0.6248, weighted_loss: 0.3324, label: 1, bag_size: 2480\n",
      "batch 719, loss: 0.0128, instance_loss: 0.4304, weighted_loss: 0.1381, label: 1, bag_size: 6090\n",
      "batch 739, loss: 0.0443, instance_loss: 0.0444, weighted_loss: 0.0443, label: 0, bag_size: 1202\n",
      "batch 759, loss: 0.3736, instance_loss: 0.2651, weighted_loss: 0.3411, label: 0, bag_size: 6884\n",
      "batch 779, loss: 0.1558, instance_loss: 0.0879, weighted_loss: 0.1354, label: 0, bag_size: 1498\n",
      "batch 799, loss: 0.0407, instance_loss: 0.0297, weighted_loss: 0.0374, label: 1, bag_size: 15665\n",
      "batch 819, loss: 0.0069, instance_loss: 0.0452, weighted_loss: 0.0184, label: 1, bag_size: 6792\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9599085365853659: correct 12594/13120\n",
      "class 1 clustering acc 0.7900914634146341: correct 5183/6560\n",
      "Epoch: 13, train_loss: 0.3131, train_clustering_loss:  0.3863, train_error: 0.1293\n",
      "class 0: acc 0.8891454965357968, correct 385/433\n",
      "class 1: acc 0.8501291989664083, correct 329/387\n",
      "\n",
      "Val Set, val_loss: 0.2364, val_error: 0.1182, auc: 0.9731\n",
      "class 0 clustering acc 0.9954545454545455: correct 1752/1760\n",
      "class 1 clustering acc 0.07272727272727272: correct 64/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8103448275862069, correct 47/58\n",
      "Validation loss decreased (0.242626 --> 0.236387).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0224, instance_loss: 0.0291, weighted_loss: 0.0244, label: 0, bag_size: 9866\n",
      "batch 39, loss: 0.0249, instance_loss: 0.0221, weighted_loss: 0.0241, label: 0, bag_size: 2424\n",
      "batch 59, loss: 0.6263, instance_loss: 1.3101, weighted_loss: 0.8314, label: 0, bag_size: 1592\n",
      "batch 79, loss: 0.6858, instance_loss: 1.2581, weighted_loss: 0.8575, label: 1, bag_size: 12180\n",
      "batch 99, loss: 0.3127, instance_loss: 0.3087, weighted_loss: 0.3115, label: 1, bag_size: 19972\n",
      "batch 119, loss: 0.0589, instance_loss: 1.2306, weighted_loss: 0.4104, label: 0, bag_size: 2844\n",
      "batch 139, loss: 0.2844, instance_loss: 0.4327, weighted_loss: 0.3288, label: 0, bag_size: 16690\n",
      "batch 159, loss: 0.0130, instance_loss: 0.0175, weighted_loss: 0.0143, label: 1, bag_size: 7613\n",
      "batch 179, loss: 0.0129, instance_loss: 0.0682, weighted_loss: 0.0295, label: 0, bag_size: 16052\n",
      "batch 199, loss: 0.0016, instance_loss: 0.3890, weighted_loss: 0.1179, label: 1, bag_size: 5612\n",
      "batch 219, loss: 0.0040, instance_loss: 0.4195, weighted_loss: 0.1286, label: 1, bag_size: 11387\n",
      "batch 239, loss: 0.0043, instance_loss: 0.1797, weighted_loss: 0.0569, label: 0, bag_size: 2424\n",
      "batch 259, loss: 0.1917, instance_loss: 0.1391, weighted_loss: 0.1760, label: 1, bag_size: 8680\n",
      "batch 279, loss: 0.0061, instance_loss: 0.0193, weighted_loss: 0.0101, label: 0, bag_size: 10995\n",
      "batch 299, loss: 0.0683, instance_loss: 0.1776, weighted_loss: 0.1011, label: 1, bag_size: 10072\n",
      "batch 319, loss: 0.0624, instance_loss: 0.1433, weighted_loss: 0.0867, label: 0, bag_size: 32227\n",
      "batch 339, loss: 0.0122, instance_loss: 0.0007, weighted_loss: 0.0088, label: 1, bag_size: 1015\n",
      "batch 359, loss: 0.4228, instance_loss: 0.5287, weighted_loss: 0.4546, label: 0, bag_size: 23618\n",
      "batch 379, loss: 0.0003, instance_loss: 0.1747, weighted_loss: 0.0526, label: 0, bag_size: 16992\n",
      "batch 399, loss: 0.0415, instance_loss: 0.0593, weighted_loss: 0.0468, label: 1, bag_size: 1525\n",
      "batch 419, loss: 0.4772, instance_loss: 0.2552, weighted_loss: 0.4106, label: 1, bag_size: 12946\n",
      "batch 439, loss: 0.0275, instance_loss: 0.0313, weighted_loss: 0.0286, label: 1, bag_size: 9078\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0325, weighted_loss: 0.0098, label: 0, bag_size: 32227\n",
      "batch 479, loss: 0.0037, instance_loss: 0.0098, weighted_loss: 0.0055, label: 1, bag_size: 20161\n",
      "batch 499, loss: 0.9302, instance_loss: 0.9176, weighted_loss: 0.9264, label: 1, bag_size: 16034\n",
      "batch 519, loss: 0.0994, instance_loss: 0.5066, weighted_loss: 0.2215, label: 0, bag_size: 30751\n",
      "batch 539, loss: 0.0121, instance_loss: 0.0647, weighted_loss: 0.0279, label: 0, bag_size: 9471\n",
      "batch 559, loss: 0.0156, instance_loss: 0.0086, weighted_loss: 0.0135, label: 1, bag_size: 5991\n",
      "batch 579, loss: 0.0241, instance_loss: 0.0224, weighted_loss: 0.0236, label: 0, bag_size: 30751\n",
      "batch 599, loss: 0.0037, instance_loss: 0.0047, weighted_loss: 0.0040, label: 0, bag_size: 22800\n",
      "batch 619, loss: 0.0084, instance_loss: 0.0122, weighted_loss: 0.0095, label: 1, bag_size: 29832\n",
      "batch 639, loss: 0.0013, instance_loss: 0.0024, weighted_loss: 0.0016, label: 1, bag_size: 3453\n",
      "batch 659, loss: 1.1059, instance_loss: 1.9361, weighted_loss: 1.3549, label: 1, bag_size: 1703\n",
      "batch 679, loss: 0.0189, instance_loss: 0.0473, weighted_loss: 0.0274, label: 0, bag_size: 2511\n",
      "batch 699, loss: 0.4757, instance_loss: 0.4155, weighted_loss: 0.4576, label: 1, bag_size: 2681\n",
      "batch 719, loss: 0.0026, instance_loss: 0.0040, weighted_loss: 0.0030, label: 1, bag_size: 6317\n",
      "batch 739, loss: 0.3100, instance_loss: 0.4107, weighted_loss: 0.3402, label: 1, bag_size: 2179\n",
      "batch 759, loss: 0.7421, instance_loss: 0.3298, weighted_loss: 0.6184, label: 1, bag_size: 7989\n",
      "batch 779, loss: 0.0001, instance_loss: 0.0014, weighted_loss: 0.0005, label: 0, bag_size: 11654\n",
      "batch 799, loss: 0.2108, instance_loss: 0.4740, weighted_loss: 0.2897, label: 0, bag_size: 11151\n",
      "batch 819, loss: 0.1586, instance_loss: 0.1046, weighted_loss: 0.1424, label: 1, bag_size: 9062\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9491615853658537: correct 12453/13120\n",
      "class 1 clustering acc 0.7890243902439025: correct 5176/6560\n",
      "Epoch: 14, train_loss: 0.3215, train_clustering_loss:  0.4485, train_error: 0.1220\n",
      "class 0: acc 0.8730964467005076, correct 344/394\n",
      "class 1: acc 0.8826291079812206, correct 376/426\n",
      "\n",
      "Val Set, val_loss: 0.4197, val_error: 0.2000, auc: 0.9579\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.05: correct 44/880\n",
      "class 0: acc 0.5961538461538461, correct 31/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0052, instance_loss: 0.0130, weighted_loss: 0.0076, label: 0, bag_size: 18045\n",
      "batch 39, loss: 0.7740, instance_loss: 0.7812, weighted_loss: 0.7761, label: 0, bag_size: 9387\n",
      "batch 59, loss: 0.0221, instance_loss: 0.0190, weighted_loss: 0.0211, label: 1, bag_size: 1022\n",
      "batch 79, loss: 0.0081, instance_loss: 0.0096, weighted_loss: 0.0086, label: 1, bag_size: 10112\n",
      "batch 99, loss: 0.0111, instance_loss: 0.0165, weighted_loss: 0.0127, label: 1, bag_size: 8685\n",
      "batch 119, loss: 0.0629, instance_loss: 0.0404, weighted_loss: 0.0562, label: 1, bag_size: 10432\n",
      "batch 139, loss: 0.2109, instance_loss: 0.2023, weighted_loss: 0.2083, label: 0, bag_size: 1370\n",
      "batch 159, loss: 0.0293, instance_loss: 0.0206, weighted_loss: 0.0267, label: 0, bag_size: 705\n",
      "batch 179, loss: 0.0239, instance_loss: 0.0684, weighted_loss: 0.0372, label: 0, bag_size: 10263\n",
      "batch 199, loss: 0.0069, instance_loss: 0.0075, weighted_loss: 0.0071, label: 0, bag_size: 12593\n",
      "batch 219, loss: 0.2106, instance_loss: 0.2368, weighted_loss: 0.2184, label: 1, bag_size: 9561\n",
      "batch 239, loss: 0.2167, instance_loss: 0.0764, weighted_loss: 0.1746, label: 0, bag_size: 4997\n",
      "batch 259, loss: 0.0271, instance_loss: 0.0263, weighted_loss: 0.0269, label: 0, bag_size: 16211\n",
      "batch 279, loss: 0.0024, instance_loss: 0.0027, weighted_loss: 0.0025, label: 1, bag_size: 7217\n",
      "batch 299, loss: 0.0019, instance_loss: 0.0052, weighted_loss: 0.0029, label: 0, bag_size: 21682\n",
      "batch 319, loss: 0.0298, instance_loss: 0.0241, weighted_loss: 0.0281, label: 1, bag_size: 4789\n",
      "batch 339, loss: 0.2689, instance_loss: 0.3203, weighted_loss: 0.2843, label: 1, bag_size: 4939\n",
      "batch 359, loss: 0.0388, instance_loss: 0.0179, weighted_loss: 0.0325, label: 1, bag_size: 9065\n",
      "batch 379, loss: 0.0514, instance_loss: 0.0265, weighted_loss: 0.0439, label: 1, bag_size: 16034\n",
      "batch 399, loss: 0.0462, instance_loss: 0.1055, weighted_loss: 0.0640, label: 0, bag_size: 2036\n",
      "batch 419, loss: 0.0199, instance_loss: 0.0242, weighted_loss: 0.0212, label: 0, bag_size: 13205\n",
      "batch 439, loss: 0.0530, instance_loss: 0.0308, weighted_loss: 0.0463, label: 1, bag_size: 12626\n",
      "batch 459, loss: 1.2201, instance_loss: 1.5516, weighted_loss: 1.3195, label: 1, bag_size: 1683\n",
      "batch 479, loss: 0.0100, instance_loss: 0.0118, weighted_loss: 0.0105, label: 1, bag_size: 11387\n",
      "batch 499, loss: 0.0194, instance_loss: 0.0151, weighted_loss: 0.0181, label: 1, bag_size: 2695\n",
      "batch 519, loss: 4.2889, instance_loss: 4.6849, weighted_loss: 4.4077, label: 0, bag_size: 4692\n",
      "batch 539, loss: 0.0460, instance_loss: 0.0301, weighted_loss: 0.0412, label: 0, bag_size: 3502\n",
      "batch 559, loss: 0.0397, instance_loss: 0.0202, weighted_loss: 0.0338, label: 0, bag_size: 10791\n",
      "batch 579, loss: 0.0048, instance_loss: 0.0068, weighted_loss: 0.0054, label: 0, bag_size: 3970\n",
      "batch 599, loss: 0.0035, instance_loss: 0.0024, weighted_loss: 0.0032, label: 1, bag_size: 11875\n",
      "batch 619, loss: 0.0029, instance_loss: 0.0033, weighted_loss: 0.0030, label: 0, bag_size: 11900\n",
      "batch 639, loss: 0.2026, instance_loss: 0.2405, weighted_loss: 0.2140, label: 0, bag_size: 2998\n",
      "batch 659, loss: 0.0020, instance_loss: 0.0075, weighted_loss: 0.0036, label: 0, bag_size: 9885\n",
      "batch 679, loss: 0.0606, instance_loss: 0.0252, weighted_loss: 0.0500, label: 1, bag_size: 8602\n",
      "batch 699, loss: 0.0531, instance_loss: 0.0539, weighted_loss: 0.0533, label: 1, bag_size: 11220\n",
      "batch 719, loss: 0.0128, instance_loss: 0.0040, weighted_loss: 0.0102, label: 1, bag_size: 10912\n",
      "batch 739, loss: 0.3861, instance_loss: 0.4899, weighted_loss: 0.4173, label: 1, bag_size: 3368\n",
      "batch 759, loss: 0.0212, instance_loss: 0.0176, weighted_loss: 0.0201, label: 1, bag_size: 5345\n",
      "batch 779, loss: 3.0744, instance_loss: 3.7937, weighted_loss: 3.2902, label: 1, bag_size: 15563\n",
      "batch 799, loss: 0.0780, instance_loss: 0.0475, weighted_loss: 0.0688, label: 0, bag_size: 6281\n",
      "batch 819, loss: 0.0004, instance_loss: 0.0033, weighted_loss: 0.0013, label: 0, bag_size: 3190\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9720274390243903: correct 12753/13120\n",
      "class 1 clustering acc 0.8652439024390244: correct 5676/6560\n",
      "Epoch: 15, train_loss: 0.2361, train_clustering_loss:  0.2793, train_error: 0.0817\n",
      "class 0: acc 0.930952380952381, correct 391/420\n",
      "class 1: acc 0.905, correct 362/400\n",
      "\n",
      "Val Set, val_loss: 0.5629, val_error: 0.1818, auc: 0.9698\n",
      "class 0 clustering acc 0.990909090909091: correct 1744/1760\n",
      "class 1 clustering acc 0.08181818181818182: correct 72/880\n",
      "class 0: acc 1.0, correct 52/52\n",
      "class 1: acc 0.6551724137931034, correct 38/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0487, instance_loss: 0.0349, weighted_loss: 0.0446, label: 0, bag_size: 21082\n",
      "batch 39, loss: 0.0092, instance_loss: 0.0129, weighted_loss: 0.0103, label: 0, bag_size: 19067\n",
      "batch 59, loss: 0.0592, instance_loss: 0.0389, weighted_loss: 0.0531, label: 0, bag_size: 11187\n",
      "batch 79, loss: 0.1149, instance_loss: 0.0972, weighted_loss: 0.1096, label: 0, bag_size: 7557\n",
      "batch 99, loss: 0.0694, instance_loss: 0.0518, weighted_loss: 0.0641, label: 1, bag_size: 18649\n",
      "batch 119, loss: 0.0014, instance_loss: 0.0137, weighted_loss: 0.0051, label: 0, bag_size: 4465\n",
      "batch 139, loss: 0.0944, instance_loss: 0.0886, weighted_loss: 0.0927, label: 1, bag_size: 7989\n",
      "batch 159, loss: 0.0020, instance_loss: 0.0022, weighted_loss: 0.0020, label: 1, bag_size: 8868\n",
      "batch 179, loss: 0.0002, instance_loss: 0.0002, weighted_loss: 0.0002, label: 0, bag_size: 20150\n",
      "batch 199, loss: 0.0379, instance_loss: 0.0579, weighted_loss: 0.0439, label: 1, bag_size: 1015\n",
      "batch 219, loss: 0.2335, instance_loss: 0.2045, weighted_loss: 0.2248, label: 1, bag_size: 16154\n",
      "batch 239, loss: 0.1921, instance_loss: 0.2966, weighted_loss: 0.2235, label: 0, bag_size: 8549\n",
      "batch 259, loss: 0.0131, instance_loss: 0.0239, weighted_loss: 0.0164, label: 0, bag_size: 19470\n",
      "batch 279, loss: 0.0011, instance_loss: 0.0005, weighted_loss: 0.0009, label: 0, bag_size: 17791\n",
      "batch 299, loss: 0.0459, instance_loss: 0.0529, weighted_loss: 0.0480, label: 0, bag_size: 11917\n",
      "batch 319, loss: 0.3583, instance_loss: 0.5187, weighted_loss: 0.4064, label: 1, bag_size: 1123\n",
      "batch 339, loss: 0.0212, instance_loss: 0.0443, weighted_loss: 0.0281, label: 0, bag_size: 2998\n",
      "batch 359, loss: 0.0239, instance_loss: 0.0130, weighted_loss: 0.0206, label: 0, bag_size: 9930\n",
      "batch 379, loss: 0.0014, instance_loss: 0.0011, weighted_loss: 0.0013, label: 1, bag_size: 7515\n",
      "batch 399, loss: 0.1597, instance_loss: 0.2185, weighted_loss: 0.1773, label: 1, bag_size: 1609\n",
      "batch 419, loss: 0.0111, instance_loss: 0.0076, weighted_loss: 0.0100, label: 1, bag_size: 18468\n",
      "batch 439, loss: 0.2196, instance_loss: 0.1812, weighted_loss: 0.2081, label: 1, bag_size: 2314\n",
      "batch 459, loss: 0.0111, instance_loss: 0.0099, weighted_loss: 0.0107, label: 0, bag_size: 15464\n",
      "batch 479, loss: 0.6840, instance_loss: 0.8771, weighted_loss: 0.7419, label: 0, bag_size: 2918\n",
      "batch 499, loss: 0.0120, instance_loss: 0.0067, weighted_loss: 0.0104, label: 0, bag_size: 10146\n",
      "batch 519, loss: 0.0225, instance_loss: 0.0157, weighted_loss: 0.0204, label: 1, bag_size: 13015\n",
      "batch 539, loss: 0.0020, instance_loss: 0.0086, weighted_loss: 0.0040, label: 1, bag_size: 16512\n",
      "batch 559, loss: 0.0099, instance_loss: 0.0377, weighted_loss: 0.0182, label: 1, bag_size: 10969\n",
      "batch 579, loss: 0.0126, instance_loss: 0.0138, weighted_loss: 0.0130, label: 1, bag_size: 13692\n",
      "batch 599, loss: 0.3670, instance_loss: 0.5278, weighted_loss: 0.4153, label: 0, bag_size: 931\n",
      "batch 619, loss: 0.0991, instance_loss: 0.1480, weighted_loss: 0.1138, label: 0, bag_size: 9930\n",
      "batch 639, loss: 0.0804, instance_loss: 0.0380, weighted_loss: 0.0677, label: 1, bag_size: 1919\n",
      "batch 659, loss: 0.3295, instance_loss: 0.2663, weighted_loss: 0.3105, label: 0, bag_size: 8744\n",
      "batch 679, loss: 0.6643, instance_loss: 1.2052, weighted_loss: 0.8266, label: 1, bag_size: 14515\n",
      "batch 699, loss: 0.7938, instance_loss: 1.8897, weighted_loss: 1.1226, label: 0, bag_size: 2091\n",
      "batch 719, loss: 0.0159, instance_loss: 0.0446, weighted_loss: 0.0245, label: 0, bag_size: 705\n",
      "batch 739, loss: 0.7211, instance_loss: 0.4764, weighted_loss: 0.6477, label: 0, bag_size: 1370\n",
      "batch 759, loss: 0.2119, instance_loss: 0.2425, weighted_loss: 0.2211, label: 1, bag_size: 8026\n",
      "batch 779, loss: 0.0056, instance_loss: 0.0077, weighted_loss: 0.0063, label: 1, bag_size: 15008\n",
      "batch 799, loss: 0.0018, instance_loss: 0.0110, weighted_loss: 0.0046, label: 1, bag_size: 3968\n",
      "batch 819, loss: 0.0027, instance_loss: 0.0032, weighted_loss: 0.0029, label: 1, bag_size: 5894\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9648628048780488: correct 12659/13120\n",
      "class 1 clustering acc 0.8202743902439025: correct 5381/6560\n",
      "Epoch: 16, train_loss: 0.2756, train_clustering_loss:  0.3522, train_error: 0.1085\n",
      "class 0: acc 0.8816120906801007, correct 350/397\n",
      "class 1: acc 0.900709219858156, correct 381/423\n",
      "\n",
      "Val Set, val_loss: 0.2666, val_error: 0.1000, auc: 0.9639\n",
      "class 0 clustering acc 0.9897727272727272: correct 1742/1760\n",
      "class 1 clustering acc 0.3534090909090909: correct 311/880\n",
      "class 0: acc 0.8653846153846154, correct 45/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0003, instance_loss: 0.0060, weighted_loss: 0.0020, label: 0, bag_size: 11759\n",
      "batch 39, loss: 0.1177, instance_loss: 0.0345, weighted_loss: 0.0927, label: 0, bag_size: 6898\n",
      "batch 59, loss: 0.0970, instance_loss: 0.1574, weighted_loss: 0.1151, label: 0, bag_size: 3089\n",
      "batch 79, loss: 0.0674, instance_loss: 0.0559, weighted_loss: 0.0639, label: 0, bag_size: 8744\n",
      "batch 99, loss: 0.0043, instance_loss: 0.0078, weighted_loss: 0.0053, label: 0, bag_size: 17630\n",
      "batch 119, loss: 1.0411, instance_loss: 1.1941, weighted_loss: 1.0870, label: 0, bag_size: 2242\n",
      "batch 139, loss: 0.0035, instance_loss: 0.0022, weighted_loss: 0.0031, label: 1, bag_size: 1294\n",
      "batch 159, loss: 0.0012, instance_loss: 0.0015, weighted_loss: 0.0013, label: 1, bag_size: 10920\n",
      "batch 179, loss: 0.0817, instance_loss: 0.0696, weighted_loss: 0.0781, label: 0, bag_size: 13339\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0029, weighted_loss: 0.0009, label: 0, bag_size: 2748\n",
      "batch 219, loss: 0.0025, instance_loss: 0.0026, weighted_loss: 0.0025, label: 0, bag_size: 9866\n",
      "batch 239, loss: 0.1060, instance_loss: 0.0931, weighted_loss: 0.1022, label: 1, bag_size: 5921\n",
      "batch 259, loss: 0.0265, instance_loss: 0.0365, weighted_loss: 0.0295, label: 1, bag_size: 30675\n",
      "batch 279, loss: 0.0062, instance_loss: 0.0193, weighted_loss: 0.0102, label: 1, bag_size: 10033\n",
      "batch 299, loss: 0.7037, instance_loss: 0.9637, weighted_loss: 0.7817, label: 0, bag_size: 9387\n",
      "batch 319, loss: 0.0005, instance_loss: 0.0001, weighted_loss: 0.0004, label: 0, bag_size: 4465\n",
      "batch 339, loss: 1.5994, instance_loss: 3.0914, weighted_loss: 2.0470, label: 0, bag_size: 7428\n",
      "batch 359, loss: 0.0345, instance_loss: 0.0661, weighted_loss: 0.0440, label: 1, bag_size: 9321\n",
      "batch 379, loss: 0.0790, instance_loss: 0.0854, weighted_loss: 0.0809, label: 1, bag_size: 11223\n",
      "batch 399, loss: 0.0124, instance_loss: 0.0339, weighted_loss: 0.0188, label: 0, bag_size: 1458\n",
      "batch 419, loss: 0.0419, instance_loss: 0.0814, weighted_loss: 0.0538, label: 1, bag_size: 9321\n",
      "batch 439, loss: 0.1690, instance_loss: 0.0870, weighted_loss: 0.1444, label: 1, bag_size: 10432\n",
      "batch 459, loss: 0.0747, instance_loss: 0.0430, weighted_loss: 0.0652, label: 1, bag_size: 5921\n",
      "batch 479, loss: 0.0745, instance_loss: 0.1245, weighted_loss: 0.0895, label: 1, bag_size: 4239\n",
      "batch 499, loss: 0.0042, instance_loss: 0.0250, weighted_loss: 0.0104, label: 1, bag_size: 5864\n",
      "batch 519, loss: 2.1832, instance_loss: 2.6448, weighted_loss: 2.3217, label: 0, bag_size: 2815\n",
      "batch 539, loss: 0.0019, instance_loss: 0.0075, weighted_loss: 0.0036, label: 1, bag_size: 3437\n",
      "batch 559, loss: 0.2188, instance_loss: 0.2815, weighted_loss: 0.2376, label: 1, bag_size: 30675\n",
      "batch 579, loss: 3.5042, instance_loss: 4.5956, weighted_loss: 3.8316, label: 0, bag_size: 4692\n",
      "batch 599, loss: 0.1076, instance_loss: 0.0732, weighted_loss: 0.0973, label: 0, bag_size: 9069\n",
      "batch 619, loss: 1.7552, instance_loss: 2.1676, weighted_loss: 1.8789, label: 1, bag_size: 13089\n",
      "batch 639, loss: 0.0136, instance_loss: 0.0291, weighted_loss: 0.0182, label: 1, bag_size: 2140\n",
      "batch 659, loss: 2.4526, instance_loss: 2.5887, weighted_loss: 2.4934, label: 0, bag_size: 7239\n",
      "batch 679, loss: 0.1166, instance_loss: 0.1038, weighted_loss: 0.1128, label: 1, bag_size: 3409\n",
      "batch 699, loss: 0.0094, instance_loss: 0.0082, weighted_loss: 0.0090, label: 1, bag_size: 12349\n",
      "batch 719, loss: 1.5706, instance_loss: 1.7676, weighted_loss: 1.6297, label: 1, bag_size: 1095\n",
      "batch 739, loss: 0.0227, instance_loss: 0.0259, weighted_loss: 0.0237, label: 0, bag_size: 30751\n",
      "batch 759, loss: 0.0012, instance_loss: 0.0010, weighted_loss: 0.0012, label: 0, bag_size: 18240\n",
      "batch 779, loss: 0.0309, instance_loss: 0.0220, weighted_loss: 0.0282, label: 0, bag_size: 15464\n",
      "batch 799, loss: 0.0531, instance_loss: 0.0452, weighted_loss: 0.0507, label: 0, bag_size: 8330\n",
      "batch 819, loss: 0.1533, instance_loss: 0.1126, weighted_loss: 0.1411, label: 1, bag_size: 2140\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9678353658536586: correct 12698/13120\n",
      "class 1 clustering acc 0.844359756097561: correct 5539/6560\n",
      "Epoch: 17, train_loss: 0.2793, train_clustering_loss:  0.3182, train_error: 0.1073\n",
      "class 0: acc 0.9, correct 378/420\n",
      "class 1: acc 0.885, correct 354/400\n",
      "\n",
      "Val Set, val_loss: 0.3470, val_error: 0.1455, auc: 0.9675\n",
      "class 0 clustering acc 0.9954545454545455: correct 1752/1760\n",
      "class 1 clustering acc 0.2034090909090909: correct 179/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.7586206896551724, correct 44/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0951, instance_loss: 0.0266, weighted_loss: 0.0745, label: 1, bag_size: 13255\n",
      "batch 39, loss: 0.0159, instance_loss: 0.0124, weighted_loss: 0.0149, label: 0, bag_size: 15313\n",
      "batch 59, loss: 1.0945, instance_loss: 1.6789, weighted_loss: 1.2699, label: 1, bag_size: 9404\n",
      "batch 79, loss: 0.1654, instance_loss: 0.1643, weighted_loss: 0.1651, label: 0, bag_size: 14828\n",
      "batch 99, loss: 0.0466, instance_loss: 0.0413, weighted_loss: 0.0450, label: 1, bag_size: 9004\n",
      "batch 119, loss: 0.0004, instance_loss: 0.0013, weighted_loss: 0.0007, label: 0, bag_size: 2748\n",
      "batch 139, loss: 0.3462, instance_loss: 0.5132, weighted_loss: 0.3963, label: 0, bag_size: 10410\n",
      "batch 159, loss: 0.0671, instance_loss: 0.0630, weighted_loss: 0.0658, label: 1, bag_size: 29832\n",
      "batch 179, loss: 0.0017, instance_loss: 0.0007, weighted_loss: 0.0014, label: 1, bag_size: 9408\n",
      "batch 199, loss: 0.1305, instance_loss: 0.1333, weighted_loss: 0.1313, label: 0, bag_size: 11281\n",
      "batch 219, loss: 0.0051, instance_loss: 0.0036, weighted_loss: 0.0047, label: 0, bag_size: 19466\n",
      "batch 239, loss: 0.0988, instance_loss: 0.0927, weighted_loss: 0.0969, label: 0, bag_size: 2351\n",
      "batch 259, loss: 0.0158, instance_loss: 0.0113, weighted_loss: 0.0145, label: 1, bag_size: 10432\n",
      "batch 279, loss: 0.2167, instance_loss: 0.3583, weighted_loss: 0.2592, label: 0, bag_size: 2104\n",
      "batch 299, loss: 0.6362, instance_loss: 0.6993, weighted_loss: 0.6551, label: 1, bag_size: 2179\n",
      "batch 319, loss: 0.1128, instance_loss: 0.1829, weighted_loss: 0.1338, label: 0, bag_size: 5999\n",
      "batch 339, loss: 0.1477, instance_loss: 0.1189, weighted_loss: 0.1390, label: 1, bag_size: 2785\n",
      "batch 359, loss: 0.3471, instance_loss: 0.5504, weighted_loss: 0.4081, label: 1, bag_size: 1339\n",
      "batch 379, loss: 0.0184, instance_loss: 0.0437, weighted_loss: 0.0260, label: 1, bag_size: 9446\n",
      "batch 399, loss: 0.0187, instance_loss: 0.0088, weighted_loss: 0.0157, label: 1, bag_size: 2140\n",
      "batch 419, loss: 0.0083, instance_loss: 0.0169, weighted_loss: 0.0109, label: 0, bag_size: 10146\n",
      "batch 439, loss: 0.0311, instance_loss: 0.0324, weighted_loss: 0.0315, label: 0, bag_size: 25814\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0452, weighted_loss: 0.0136, label: 0, bag_size: 18225\n",
      "batch 479, loss: 0.0455, instance_loss: 0.0362, weighted_loss: 0.0427, label: 0, bag_size: 11922\n",
      "batch 499, loss: 0.0003, instance_loss: 0.0023, weighted_loss: 0.0009, label: 1, bag_size: 3437\n",
      "batch 519, loss: 0.0148, instance_loss: 0.0060, weighted_loss: 0.0122, label: 1, bag_size: 6606\n",
      "batch 539, loss: 0.4561, instance_loss: 0.6813, weighted_loss: 0.5237, label: 0, bag_size: 2998\n",
      "batch 559, loss: 0.0479, instance_loss: 0.0488, weighted_loss: 0.0482, label: 0, bag_size: 6898\n",
      "batch 579, loss: 0.0025, instance_loss: 0.0009, weighted_loss: 0.0020, label: 0, bag_size: 14266\n",
      "batch 599, loss: 0.0015, instance_loss: 0.0006, weighted_loss: 0.0012, label: 1, bag_size: 13255\n",
      "batch 619, loss: 0.0428, instance_loss: 0.0371, weighted_loss: 0.0411, label: 0, bag_size: 9542\n",
      "batch 639, loss: 0.0667, instance_loss: 0.0602, weighted_loss: 0.0648, label: 0, bag_size: 9616\n",
      "batch 659, loss: 0.1222, instance_loss: 1.3445, weighted_loss: 0.4889, label: 0, bag_size: 2179\n",
      "batch 679, loss: 0.0559, instance_loss: 0.3992, weighted_loss: 0.1589, label: 0, bag_size: 14956\n",
      "batch 699, loss: 0.3095, instance_loss: 0.5353, weighted_loss: 0.3773, label: 1, bag_size: 7424\n",
      "batch 719, loss: 0.0642, instance_loss: 0.1475, weighted_loss: 0.0892, label: 0, bag_size: 7011\n",
      "batch 739, loss: 0.0089, instance_loss: 0.0119, weighted_loss: 0.0098, label: 1, bag_size: 2344\n",
      "batch 759, loss: 0.0510, instance_loss: 0.0500, weighted_loss: 0.0507, label: 0, bag_size: 8582\n",
      "batch 779, loss: 0.0409, instance_loss: 0.0419, weighted_loss: 0.0412, label: 0, bag_size: 10995\n",
      "batch 799, loss: 0.0057, instance_loss: 0.0083, weighted_loss: 0.0065, label: 0, bag_size: 2044\n",
      "batch 819, loss: 0.0029, instance_loss: 0.0018, weighted_loss: 0.0026, label: 1, bag_size: 12178\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9682926829268292: correct 12704/13120\n",
      "class 1 clustering acc 0.8452743902439024: correct 5545/6560\n",
      "Epoch: 18, train_loss: 0.2413, train_clustering_loss:  0.3112, train_error: 0.0866\n",
      "class 0: acc 0.9296116504854369, correct 383/412\n",
      "class 1: acc 0.8970588235294118, correct 366/408\n",
      "\n",
      "Val Set, val_loss: 0.4333, val_error: 0.1909, auc: 0.9705\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.07045454545454545: correct 62/880\n",
      "class 0: acc 0.6153846153846154, correct 32/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0972, instance_loss: 0.0668, weighted_loss: 0.0881, label: 0, bag_size: 10029\n",
      "batch 39, loss: 0.0027, instance_loss: 0.0028, weighted_loss: 0.0027, label: 1, bag_size: 7371\n",
      "batch 59, loss: 0.0085, instance_loss: 0.0056, weighted_loss: 0.0076, label: 1, bag_size: 13026\n",
      "batch 79, loss: 0.0048, instance_loss: 0.0013, weighted_loss: 0.0038, label: 1, bag_size: 6343\n",
      "batch 99, loss: 0.2085, instance_loss: 0.1496, weighted_loss: 0.1908, label: 1, bag_size: 5723\n",
      "batch 119, loss: 0.3434, instance_loss: 0.3379, weighted_loss: 0.3417, label: 1, bag_size: 1822\n",
      "batch 139, loss: 0.0322, instance_loss: 0.0316, weighted_loss: 0.0320, label: 1, bag_size: 3652\n",
      "batch 159, loss: 0.0003, instance_loss: 0.0006, weighted_loss: 0.0004, label: 1, bag_size: 4862\n",
      "batch 179, loss: 0.0006, instance_loss: 0.0027, weighted_loss: 0.0012, label: 0, bag_size: 8866\n",
      "batch 199, loss: 0.0730, instance_loss: 0.0591, weighted_loss: 0.0688, label: 1, bag_size: 1759\n",
      "batch 219, loss: 0.0006, instance_loss: 0.0035, weighted_loss: 0.0015, label: 0, bag_size: 13892\n",
      "batch 239, loss: 0.0083, instance_loss: 0.0014, weighted_loss: 0.0062, label: 1, bag_size: 7935\n",
      "batch 259, loss: 0.0618, instance_loss: 0.0260, weighted_loss: 0.0511, label: 0, bag_size: 1234\n",
      "batch 279, loss: 0.0175, instance_loss: 0.0294, weighted_loss: 0.0211, label: 1, bag_size: 1638\n",
      "batch 299, loss: 0.0034, instance_loss: 0.0009, weighted_loss: 0.0026, label: 0, bag_size: 21076\n",
      "batch 319, loss: 0.0384, instance_loss: 0.0181, weighted_loss: 0.0323, label: 0, bag_size: 2351\n",
      "batch 339, loss: 1.0252, instance_loss: 1.3725, weighted_loss: 1.1294, label: 1, bag_size: 7468\n",
      "batch 359, loss: 0.1576, instance_loss: 0.2478, weighted_loss: 0.1847, label: 0, bag_size: 2242\n",
      "batch 379, loss: 2.7910, instance_loss: 3.2198, weighted_loss: 2.9197, label: 0, bag_size: 5211\n",
      "batch 399, loss: 0.0051, instance_loss: 0.0054, weighted_loss: 0.0052, label: 0, bag_size: 17268\n",
      "batch 419, loss: 0.0038, instance_loss: 0.0025, weighted_loss: 0.0034, label: 0, bag_size: 11654\n",
      "batch 439, loss: 0.0015, instance_loss: 0.0003, weighted_loss: 0.0011, label: 0, bag_size: 12524\n",
      "batch 459, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 19518\n",
      "batch 479, loss: 0.0089, instance_loss: 0.0051, weighted_loss: 0.0077, label: 0, bag_size: 9542\n",
      "batch 499, loss: 0.0841, instance_loss: 0.0687, weighted_loss: 0.0795, label: 1, bag_size: 5345\n",
      "batch 519, loss: 4.3171, instance_loss: 4.4353, weighted_loss: 4.3526, label: 0, bag_size: 3468\n",
      "batch 539, loss: 0.0140, instance_loss: 0.0232, weighted_loss: 0.0168, label: 1, bag_size: 8660\n",
      "batch 559, loss: 0.0014, instance_loss: 0.0021, weighted_loss: 0.0016, label: 0, bag_size: 11727\n",
      "batch 579, loss: 0.0043, instance_loss: 0.0074, weighted_loss: 0.0052, label: 0, bag_size: 13880\n",
      "batch 599, loss: 1.9652, instance_loss: 2.2090, weighted_loss: 2.0384, label: 1, bag_size: 1638\n",
      "batch 619, loss: 0.0839, instance_loss: 0.0570, weighted_loss: 0.0759, label: 0, bag_size: 2266\n",
      "batch 639, loss: 1.0239, instance_loss: 1.1486, weighted_loss: 1.0613, label: 1, bag_size: 9215\n",
      "batch 659, loss: 0.0139, instance_loss: 0.0109, weighted_loss: 0.0130, label: 1, bag_size: 3640\n",
      "batch 679, loss: 0.5744, instance_loss: 0.6012, weighted_loss: 0.5824, label: 1, bag_size: 8982\n",
      "batch 699, loss: 0.1120, instance_loss: 0.2099, weighted_loss: 0.1414, label: 1, bag_size: 2678\n",
      "batch 719, loss: 0.0162, instance_loss: 0.0044, weighted_loss: 0.0126, label: 1, bag_size: 30675\n",
      "batch 739, loss: 0.0015, instance_loss: 0.0044, weighted_loss: 0.0024, label: 0, bag_size: 8898\n",
      "batch 759, loss: 0.0556, instance_loss: 0.0155, weighted_loss: 0.0436, label: 1, bag_size: 12575\n",
      "batch 779, loss: 0.0087, instance_loss: 0.0144, weighted_loss: 0.0104, label: 1, bag_size: 16051\n",
      "batch 799, loss: 0.0026, instance_loss: 0.0025, weighted_loss: 0.0026, label: 1, bag_size: 8448\n",
      "batch 819, loss: 0.0104, instance_loss: 0.0105, weighted_loss: 0.0105, label: 0, bag_size: 13992\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9754573170731707: correct 12798/13120\n",
      "class 1 clustering acc 0.8815548780487805: correct 5783/6560\n",
      "Epoch: 19, train_loss: 0.2204, train_clustering_loss:  0.2592, train_error: 0.0732\n",
      "class 0: acc 0.9225181598062954, correct 381/413\n",
      "class 1: acc 0.9312039312039312, correct 379/407\n",
      "\n",
      "Val Set, val_loss: 0.2278, val_error: 0.1000, auc: 0.9708\n",
      "class 0 clustering acc 0.9926136363636363: correct 1747/1760\n",
      "class 1 clustering acc 0.1: correct 88/880\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 0.896551724137931, correct 52/58\n",
      "Validation loss decreased (0.236387 --> 0.227805).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0226, instance_loss: 0.0079, weighted_loss: 0.0182, label: 1, bag_size: 10622\n",
      "batch 39, loss: 0.0027, instance_loss: 0.0075, weighted_loss: 0.0042, label: 0, bag_size: 1370\n",
      "batch 59, loss: 0.0116, instance_loss: 0.0122, weighted_loss: 0.0118, label: 0, bag_size: 9930\n",
      "batch 79, loss: 0.0008, instance_loss: 0.0011, weighted_loss: 0.0009, label: 0, bag_size: 15313\n",
      "batch 99, loss: 0.0093, instance_loss: 0.0064, weighted_loss: 0.0084, label: 1, bag_size: 9732\n",
      "batch 119, loss: 0.0010, instance_loss: 0.0009, weighted_loss: 0.0010, label: 1, bag_size: 7513\n",
      "batch 139, loss: 0.0075, instance_loss: 0.0009, weighted_loss: 0.0055, label: 1, bag_size: 9561\n",
      "batch 159, loss: 0.0005, instance_loss: 0.0031, weighted_loss: 0.0013, label: 1, bag_size: 6453\n",
      "batch 179, loss: 0.0217, instance_loss: 0.0109, weighted_loss: 0.0185, label: 1, bag_size: 10460\n",
      "batch 199, loss: 0.0215, instance_loss: 0.0199, weighted_loss: 0.0210, label: 0, bag_size: 12083\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0001, weighted_loss: 0.0002, label: 0, bag_size: 4497\n",
      "batch 239, loss: 0.0017, instance_loss: 0.0009, weighted_loss: 0.0014, label: 0, bag_size: 2270\n",
      "batch 259, loss: 0.0100, instance_loss: 0.0073, weighted_loss: 0.0092, label: 0, bag_size: 3502\n",
      "batch 279, loss: 0.0529, instance_loss: 0.0403, weighted_loss: 0.0491, label: 1, bag_size: 2480\n",
      "batch 299, loss: 0.3579, instance_loss: 0.7543, weighted_loss: 0.4768, label: 0, bag_size: 1800\n",
      "batch 319, loss: 0.1066, instance_loss: 0.1034, weighted_loss: 0.1057, label: 0, bag_size: 11212\n",
      "batch 339, loss: 0.0137, instance_loss: 0.0036, weighted_loss: 0.0107, label: 1, bag_size: 10867\n",
      "batch 359, loss: 0.0051, instance_loss: 0.0142, weighted_loss: 0.0078, label: 1, bag_size: 11981\n",
      "batch 379, loss: 0.0014, instance_loss: 0.0025, weighted_loss: 0.0017, label: 0, bag_size: 1797\n",
      "batch 399, loss: 0.0009, instance_loss: 0.0027, weighted_loss: 0.0015, label: 0, bag_size: 3908\n",
      "batch 419, loss: 0.0258, instance_loss: 0.0218, weighted_loss: 0.0246, label: 1, bag_size: 18603\n",
      "batch 439, loss: 0.0016, instance_loss: 0.0140, weighted_loss: 0.0053, label: 1, bag_size: 1022\n",
      "batch 459, loss: 3.2627, instance_loss: 3.3617, weighted_loss: 3.2924, label: 0, bag_size: 7835\n",
      "batch 479, loss: 0.0040, instance_loss: 0.0018, weighted_loss: 0.0033, label: 0, bag_size: 11727\n",
      "batch 499, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0011, label: 1, bag_size: 6875\n",
      "batch 519, loss: 1.7788, instance_loss: 2.0605, weighted_loss: 1.8633, label: 1, bag_size: 3121\n",
      "batch 539, loss: 0.0011, instance_loss: 0.0011, weighted_loss: 0.0011, label: 0, bag_size: 18954\n",
      "batch 559, loss: 0.3809, instance_loss: 0.3591, weighted_loss: 0.3744, label: 0, bag_size: 11607\n",
      "batch 579, loss: 2.5149, instance_loss: 2.6150, weighted_loss: 2.5450, label: 1, bag_size: 2681\n",
      "batch 599, loss: 0.0081, instance_loss: 0.0079, weighted_loss: 0.0081, label: 1, bag_size: 10105\n",
      "batch 619, loss: 0.0037, instance_loss: 0.0052, weighted_loss: 0.0042, label: 1, bag_size: 11600\n",
      "batch 639, loss: 0.0302, instance_loss: 0.0270, weighted_loss: 0.0292, label: 1, bag_size: 1294\n",
      "batch 659, loss: 0.0009, instance_loss: 0.0012, weighted_loss: 0.0010, label: 0, bag_size: 1213\n",
      "batch 679, loss: 0.2297, instance_loss: 0.3340, weighted_loss: 0.2610, label: 0, bag_size: 2303\n",
      "batch 699, loss: 0.0519, instance_loss: 0.0350, weighted_loss: 0.0468, label: 0, bag_size: 17630\n",
      "batch 719, loss: 0.1561, instance_loss: 0.2616, weighted_loss: 0.1877, label: 0, bag_size: 1370\n",
      "batch 739, loss: 0.0158, instance_loss: 0.0290, weighted_loss: 0.0198, label: 1, bag_size: 9689\n",
      "batch 759, loss: 0.0024, instance_loss: 0.0104, weighted_loss: 0.0048, label: 1, bag_size: 9971\n",
      "batch 779, loss: 0.0029, instance_loss: 0.0068, weighted_loss: 0.0041, label: 0, bag_size: 1452\n",
      "batch 799, loss: 1.5299, instance_loss: 2.0648, weighted_loss: 1.6903, label: 0, bag_size: 7428\n",
      "batch 819, loss: 0.0967, instance_loss: 0.1020, weighted_loss: 0.0983, label: 1, bag_size: 7389\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9723323170731707: correct 12757/13120\n",
      "class 1 clustering acc 0.8754573170731708: correct 5743/6560\n",
      "Epoch: 20, train_loss: 0.2119, train_clustering_loss:  0.2561, train_error: 0.0805\n",
      "class 0: acc 0.9254807692307693, correct 385/416\n",
      "class 1: acc 0.9133663366336634, correct 369/404\n",
      "\n",
      "Val Set, val_loss: 0.2213, val_error: 0.1000, auc: 0.9712\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8448275862068966, correct 49/58\n",
      "Validation loss decreased (0.227805 --> 0.221276).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0022, instance_loss: 0.0029, weighted_loss: 0.0024, label: 1, bag_size: 11389\n",
      "batch 39, loss: 0.0006, instance_loss: 0.0019, weighted_loss: 0.0010, label: 0, bag_size: 15077\n",
      "batch 59, loss: 0.0459, instance_loss: 0.0146, weighted_loss: 0.0365, label: 1, bag_size: 18603\n",
      "batch 79, loss: 4.6311, instance_loss: 4.8708, weighted_loss: 4.7030, label: 0, bag_size: 3802\n",
      "batch 99, loss: 0.3854, instance_loss: 0.5270, weighted_loss: 0.4279, label: 1, bag_size: 13440\n",
      "batch 119, loss: 0.0601, instance_loss: 0.0508, weighted_loss: 0.0573, label: 1, bag_size: 8040\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 11390\n",
      "batch 159, loss: 0.0790, instance_loss: 0.0321, weighted_loss: 0.0649, label: 1, bag_size: 9408\n",
      "batch 179, loss: 0.0079, instance_loss: 0.0095, weighted_loss: 0.0084, label: 0, bag_size: 14319\n",
      "batch 199, loss: 0.0540, instance_loss: 0.0530, weighted_loss: 0.0537, label: 0, bag_size: 763\n",
      "batch 219, loss: 0.0289, instance_loss: 0.1861, weighted_loss: 0.0761, label: 1, bag_size: 10920\n",
      "batch 239, loss: 0.0032, instance_loss: 0.0049, weighted_loss: 0.0037, label: 0, bag_size: 2844\n",
      "batch 259, loss: 0.0009, instance_loss: 0.0007, weighted_loss: 0.0008, label: 0, bag_size: 15003\n",
      "batch 279, loss: 0.8882, instance_loss: 1.4887, weighted_loss: 1.0683, label: 1, bag_size: 9942\n",
      "batch 299, loss: 0.0033, instance_loss: 0.0228, weighted_loss: 0.0092, label: 0, bag_size: 2534\n",
      "batch 319, loss: 0.1189, instance_loss: 0.1680, weighted_loss: 0.1336, label: 0, bag_size: 1072\n",
      "batch 339, loss: 0.0038, instance_loss: 0.0000, weighted_loss: 0.0027, label: 0, bag_size: 9786\n",
      "batch 359, loss: 0.6208, instance_loss: 1.0051, weighted_loss: 0.7361, label: 0, bag_size: 8420\n",
      "batch 379, loss: 0.0033, instance_loss: 0.0000, weighted_loss: 0.0023, label: 0, bag_size: 8755\n",
      "batch 399, loss: 0.1462, instance_loss: 1.2209, weighted_loss: 0.4687, label: 0, bag_size: 4598\n",
      "batch 419, loss: 0.0049, instance_loss: 0.0028, weighted_loss: 0.0043, label: 0, bag_size: 803\n",
      "batch 439, loss: 0.0030, instance_loss: 0.0058, weighted_loss: 0.0039, label: 0, bag_size: 931\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 15093\n",
      "batch 479, loss: 0.2020, instance_loss: 0.0965, weighted_loss: 0.1703, label: 1, bag_size: 1888\n",
      "batch 499, loss: 0.0039, instance_loss: 0.0704, weighted_loss: 0.0238, label: 1, bag_size: 16051\n",
      "batch 519, loss: 0.5352, instance_loss: 0.5687, weighted_loss: 0.5452, label: 1, bag_size: 2681\n",
      "batch 539, loss: 0.0274, instance_loss: 0.0155, weighted_loss: 0.0238, label: 1, bag_size: 6842\n",
      "batch 559, loss: 0.0373, instance_loss: 0.0376, weighted_loss: 0.0374, label: 0, bag_size: 9930\n",
      "batch 579, loss: 0.9022, instance_loss: 1.3861, weighted_loss: 1.0473, label: 1, bag_size: 13440\n",
      "batch 599, loss: 0.0387, instance_loss: 0.0674, weighted_loss: 0.0473, label: 0, bag_size: 1684\n",
      "batch 619, loss: 0.0217, instance_loss: 0.0152, weighted_loss: 0.0197, label: 1, bag_size: 9533\n",
      "batch 639, loss: 0.0073, instance_loss: 0.0073, weighted_loss: 0.0073, label: 0, bag_size: 3810\n",
      "batch 659, loss: 0.0591, instance_loss: 0.0531, weighted_loss: 0.0573, label: 1, bag_size: 12758\n",
      "batch 679, loss: 0.0702, instance_loss: 0.0719, weighted_loss: 0.0707, label: 0, bag_size: 23791\n",
      "batch 699, loss: 0.0073, instance_loss: 0.0044, weighted_loss: 0.0065, label: 0, bag_size: 15003\n",
      "batch 719, loss: 0.0021, instance_loss: 0.0003, weighted_loss: 0.0016, label: 1, bag_size: 9732\n",
      "batch 739, loss: 0.7490, instance_loss: 0.8007, weighted_loss: 0.7645, label: 1, bag_size: 16514\n",
      "batch 759, loss: 0.0262, instance_loss: 0.0253, weighted_loss: 0.0259, label: 0, bag_size: 1416\n",
      "batch 779, loss: 0.0063, instance_loss: 0.0074, weighted_loss: 0.0066, label: 0, bag_size: 11546\n",
      "batch 799, loss: 0.0196, instance_loss: 0.0068, weighted_loss: 0.0157, label: 1, bag_size: 18095\n",
      "batch 819, loss: 0.4700, instance_loss: 0.8105, weighted_loss: 0.5721, label: 0, bag_size: 2653\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9696646341463414: correct 12722/13120\n",
      "class 1 clustering acc 0.864329268292683: correct 5670/6560\n",
      "Epoch: 21, train_loss: 0.2413, train_clustering_loss:  0.2956, train_error: 0.0805\n",
      "class 0: acc 0.9204545454545454, correct 405/440\n",
      "class 1: acc 0.9184210526315789, correct 349/380\n",
      "\n",
      "Val Set, val_loss: 0.2756, val_error: 0.1091, auc: 0.9735\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8275862068965517, correct 48/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0045, instance_loss: 0.0280, weighted_loss: 0.0115, label: 1, bag_size: 10592\n",
      "batch 39, loss: 0.0037, instance_loss: 0.0017, weighted_loss: 0.0031, label: 1, bag_size: 14887\n",
      "batch 59, loss: 2.3592, instance_loss: 2.9634, weighted_loss: 2.5405, label: 0, bag_size: 15898\n",
      "batch 79, loss: 0.8020, instance_loss: 0.9058, weighted_loss: 0.8331, label: 1, bag_size: 1819\n",
      "batch 99, loss: 0.0184, instance_loss: 0.0087, weighted_loss: 0.0155, label: 0, bag_size: 10490\n",
      "batch 119, loss: 0.0064, instance_loss: 0.0081, weighted_loss: 0.0069, label: 1, bag_size: 1051\n",
      "batch 139, loss: 0.0002, instance_loss: 0.0035, weighted_loss: 0.0012, label: 0, bag_size: 1984\n",
      "batch 159, loss: 0.6223, instance_loss: 0.9951, weighted_loss: 0.7341, label: 1, bag_size: 1497\n",
      "batch 179, loss: 0.0070, instance_loss: 0.0242, weighted_loss: 0.0121, label: 0, bag_size: 1560\n",
      "batch 199, loss: 0.0041, instance_loss: 0.0097, weighted_loss: 0.0057, label: 0, bag_size: 18954\n",
      "batch 219, loss: 0.0019, instance_loss: 0.0008, weighted_loss: 0.0015, label: 1, bag_size: 20161\n",
      "batch 239, loss: 0.0011, instance_loss: 0.0048, weighted_loss: 0.0022, label: 0, bag_size: 5409\n",
      "batch 259, loss: 0.0024, instance_loss: 0.0022, weighted_loss: 0.0023, label: 1, bag_size: 9408\n",
      "batch 279, loss: 0.0200, instance_loss: 0.0117, weighted_loss: 0.0175, label: 1, bag_size: 6731\n",
      "batch 299, loss: 0.0066, instance_loss: 0.0058, weighted_loss: 0.0064, label: 1, bag_size: 9878\n",
      "batch 319, loss: 0.0381, instance_loss: 0.0345, weighted_loss: 0.0370, label: 0, bag_size: 9171\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0029, weighted_loss: 0.0010, label: 0, bag_size: 5965\n",
      "batch 359, loss: 0.0007, instance_loss: 0.0007, weighted_loss: 0.0007, label: 1, bag_size: 11964\n",
      "batch 379, loss: 0.0002, instance_loss: 0.0006, weighted_loss: 0.0003, label: 1, bag_size: 4877\n",
      "batch 399, loss: 0.2234, instance_loss: 0.3973, weighted_loss: 0.2755, label: 1, bag_size: 1755\n",
      "batch 419, loss: 0.2977, instance_loss: 0.2609, weighted_loss: 0.2867, label: 0, bag_size: 21864\n",
      "batch 439, loss: 0.0887, instance_loss: 0.0861, weighted_loss: 0.0879, label: 0, bag_size: 2360\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 15077\n",
      "batch 479, loss: 3.0085, instance_loss: 3.7180, weighted_loss: 3.2213, label: 0, bag_size: 3468\n",
      "batch 499, loss: 0.0014, instance_loss: 0.0010, weighted_loss: 0.0013, label: 0, bag_size: 5551\n",
      "batch 519, loss: 0.0028, instance_loss: 0.0010, weighted_loss: 0.0022, label: 0, bag_size: 18240\n",
      "batch 539, loss: 0.0168, instance_loss: 0.0210, weighted_loss: 0.0180, label: 0, bag_size: 2044\n",
      "batch 559, loss: 0.2775, instance_loss: 0.7672, weighted_loss: 0.4244, label: 1, bag_size: 1437\n",
      "batch 579, loss: 0.0243, instance_loss: 0.0106, weighted_loss: 0.0202, label: 1, bag_size: 5494\n",
      "batch 599, loss: 0.0002, instance_loss: 0.0007, weighted_loss: 0.0004, label: 0, bag_size: 11735\n",
      "batch 619, loss: 0.0094, instance_loss: 0.0070, weighted_loss: 0.0087, label: 0, bag_size: 8145\n",
      "batch 639, loss: 0.0412, instance_loss: 0.0170, weighted_loss: 0.0340, label: 0, bag_size: 1349\n",
      "batch 659, loss: 0.0008, instance_loss: 0.0006, weighted_loss: 0.0007, label: 0, bag_size: 15077\n",
      "batch 679, loss: 0.0298, instance_loss: 0.0264, weighted_loss: 0.0288, label: 1, bag_size: 1051\n",
      "batch 699, loss: 0.2910, instance_loss: 0.3457, weighted_loss: 0.3074, label: 1, bag_size: 5723\n",
      "batch 719, loss: 0.0401, instance_loss: 0.0262, weighted_loss: 0.0360, label: 0, bag_size: 3444\n",
      "batch 739, loss: 0.0015, instance_loss: 0.0022, weighted_loss: 0.0017, label: 0, bag_size: 13795\n",
      "batch 759, loss: 0.2544, instance_loss: 0.2583, weighted_loss: 0.2556, label: 0, bag_size: 1149\n",
      "batch 779, loss: 0.0861, instance_loss: 0.0095, weighted_loss: 0.0631, label: 0, bag_size: 9069\n",
      "batch 799, loss: 0.5145, instance_loss: 0.2024, weighted_loss: 0.4208, label: 1, bag_size: 20161\n",
      "batch 819, loss: 0.0022, instance_loss: 0.0537, weighted_loss: 0.0177, label: 1, bag_size: 17486\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9720274390243903: correct 12753/13120\n",
      "class 1 clustering acc 0.8618902439024391: correct 5654/6560\n",
      "Epoch: 22, train_loss: 0.2333, train_clustering_loss:  0.2757, train_error: 0.0915\n",
      "class 0: acc 0.916256157635468, correct 372/406\n",
      "class 1: acc 0.9009661835748792, correct 373/414\n",
      "\n",
      "Val Set, val_loss: 0.2203, val_error: 0.0818, auc: 0.9738\n",
      "class 0 clustering acc 0.9363636363636364: correct 1648/1760\n",
      "class 1 clustering acc 0.35568181818181815: correct 313/880\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "Validation loss decreased (0.221276 --> 0.220261).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0019, instance_loss: 0.0007, weighted_loss: 0.0016, label: 0, bag_size: 13964\n",
      "batch 39, loss: 0.0171, instance_loss: 0.0118, weighted_loss: 0.0155, label: 1, bag_size: 5454\n",
      "batch 59, loss: 0.0003, instance_loss: 0.0312, weighted_loss: 0.0095, label: 1, bag_size: 4442\n",
      "batch 79, loss: 2.5706, instance_loss: 2.6830, weighted_loss: 2.6043, label: 0, bag_size: 2959\n",
      "batch 99, loss: 0.0206, instance_loss: 0.0368, weighted_loss: 0.0255, label: 1, bag_size: 12425\n",
      "batch 119, loss: 1.9367, instance_loss: 2.4521, weighted_loss: 2.0913, label: 0, bag_size: 23618\n",
      "batch 139, loss: 0.0188, instance_loss: 0.0249, weighted_loss: 0.0206, label: 0, bag_size: 2873\n",
      "batch 159, loss: 0.0057, instance_loss: 0.0044, weighted_loss: 0.0053, label: 0, bag_size: 22870\n",
      "batch 179, loss: 0.0323, instance_loss: 0.0186, weighted_loss: 0.0282, label: 1, bag_size: 21701\n",
      "batch 199, loss: 0.0369, instance_loss: 0.0422, weighted_loss: 0.0385, label: 0, bag_size: 9455\n",
      "batch 219, loss: 1.5077, instance_loss: 2.0012, weighted_loss: 1.6557, label: 0, bag_size: 1732\n",
      "batch 239, loss: 0.0010, instance_loss: 0.0017, weighted_loss: 0.0012, label: 0, bag_size: 10995\n",
      "batch 259, loss: 0.2426, instance_loss: 0.3914, weighted_loss: 0.2872, label: 0, bag_size: 3375\n",
      "batch 279, loss: 0.0003, instance_loss: 0.0003, weighted_loss: 0.0003, label: 0, bag_size: 2654\n",
      "batch 299, loss: 0.0144, instance_loss: 0.0211, weighted_loss: 0.0165, label: 0, bag_size: 3970\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0042, weighted_loss: 0.0013, label: 1, bag_size: 6792\n",
      "batch 339, loss: 0.0569, instance_loss: 0.0790, weighted_loss: 0.0635, label: 0, bag_size: 8582\n",
      "batch 359, loss: 0.0580, instance_loss: 0.0266, weighted_loss: 0.0485, label: 1, bag_size: 9078\n",
      "batch 379, loss: 0.0045, instance_loss: 0.0072, weighted_loss: 0.0053, label: 0, bag_size: 16341\n",
      "batch 399, loss: 0.0129, instance_loss: 0.0148, weighted_loss: 0.0135, label: 1, bag_size: 3409\n",
      "batch 419, loss: 0.0685, instance_loss: 0.1061, weighted_loss: 0.0798, label: 0, bag_size: 2873\n",
      "batch 439, loss: 0.0017, instance_loss: 0.0017, weighted_loss: 0.0017, label: 1, bag_size: 9971\n",
      "batch 459, loss: 0.0081, instance_loss: 0.0050, weighted_loss: 0.0072, label: 0, bag_size: 5999\n",
      "batch 479, loss: 0.0016, instance_loss: 0.0002, weighted_loss: 0.0012, label: 0, bag_size: 18154\n",
      "batch 499, loss: 0.0085, instance_loss: 0.0070, weighted_loss: 0.0081, label: 1, bag_size: 2356\n",
      "batch 519, loss: 0.0152, instance_loss: 0.0094, weighted_loss: 0.0135, label: 1, bag_size: 4394\n",
      "batch 539, loss: 1.1117, instance_loss: 1.3365, weighted_loss: 1.1791, label: 1, bag_size: 21252\n",
      "batch 559, loss: 0.0256, instance_loss: 0.0217, weighted_loss: 0.0244, label: 0, bag_size: 1881\n",
      "batch 579, loss: 0.4843, instance_loss: 0.6875, weighted_loss: 0.5452, label: 0, bag_size: 2266\n",
      "batch 599, loss: 0.0171, instance_loss: 0.0145, weighted_loss: 0.0163, label: 0, bag_size: 5009\n",
      "batch 619, loss: 0.0275, instance_loss: 0.0209, weighted_loss: 0.0255, label: 0, bag_size: 2367\n",
      "batch 639, loss: 0.0667, instance_loss: 0.1313, weighted_loss: 0.0861, label: 0, bag_size: 10535\n",
      "batch 659, loss: 0.7377, instance_loss: 1.3172, weighted_loss: 0.9115, label: 0, bag_size: 15898\n",
      "batch 679, loss: 0.7026, instance_loss: 0.6576, weighted_loss: 0.6891, label: 0, bag_size: 19043\n",
      "batch 699, loss: 0.3695, instance_loss: 1.2075, weighted_loss: 0.6209, label: 1, bag_size: 13174\n",
      "batch 719, loss: 0.0979, instance_loss: 0.2549, weighted_loss: 0.1450, label: 1, bag_size: 13947\n",
      "batch 739, loss: 0.0203, instance_loss: 0.0558, weighted_loss: 0.0309, label: 0, bag_size: 31780\n",
      "batch 759, loss: 0.3324, instance_loss: 0.4273, weighted_loss: 0.3609, label: 0, bag_size: 3541\n",
      "batch 779, loss: 0.1622, instance_loss: 0.0695, weighted_loss: 0.1344, label: 0, bag_size: 2104\n",
      "batch 799, loss: 0.0067, instance_loss: 0.1329, weighted_loss: 0.0446, label: 1, bag_size: 13015\n",
      "batch 819, loss: 0.0004, instance_loss: 0.0003, weighted_loss: 0.0003, label: 0, bag_size: 16211\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9657774390243903: correct 12671/13120\n",
      "class 1 clustering acc 0.8342987804878049: correct 5473/6560\n",
      "Epoch: 23, train_loss: 0.2305, train_clustering_loss:  0.3073, train_error: 0.0927\n",
      "class 0: acc 0.9133489461358314, correct 390/427\n",
      "class 1: acc 0.9007633587786259, correct 354/393\n",
      "\n",
      "Val Set, val_loss: 0.6595, val_error: 0.1818, auc: 0.9675\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 1.0, correct 52/52\n",
      "class 1: acc 0.6551724137931034, correct 38/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0184, instance_loss: 0.0365, weighted_loss: 0.0239, label: 1, bag_size: 5155\n",
      "batch 39, loss: 0.4016, instance_loss: 0.3548, weighted_loss: 0.3875, label: 0, bag_size: 10381\n",
      "batch 59, loss: 0.0019, instance_loss: 0.0156, weighted_loss: 0.0060, label: 1, bag_size: 4862\n",
      "batch 79, loss: 0.0018, instance_loss: 0.0076, weighted_loss: 0.0035, label: 1, bag_size: 11964\n",
      "batch 99, loss: 0.4728, instance_loss: 0.4747, weighted_loss: 0.4734, label: 1, bag_size: 12340\n",
      "batch 119, loss: 0.0217, instance_loss: 0.0148, weighted_loss: 0.0196, label: 1, bag_size: 1823\n",
      "batch 139, loss: 0.0002, instance_loss: 0.0007, weighted_loss: 0.0003, label: 0, bag_size: 11865\n",
      "batch 159, loss: 0.0542, instance_loss: 0.1496, weighted_loss: 0.0828, label: 1, bag_size: 9478\n",
      "batch 179, loss: 0.0079, instance_loss: 0.0067, weighted_loss: 0.0076, label: 0, bag_size: 16087\n",
      "batch 199, loss: 0.2502, instance_loss: 0.3291, weighted_loss: 0.2738, label: 0, bag_size: 2815\n",
      "batch 219, loss: 0.2672, instance_loss: 0.2793, weighted_loss: 0.2708, label: 0, bag_size: 2006\n",
      "batch 239, loss: 0.0100, instance_loss: 0.0095, weighted_loss: 0.0099, label: 1, bag_size: 13026\n",
      "batch 259, loss: 2.2386, instance_loss: 3.0569, weighted_loss: 2.4841, label: 0, bag_size: 17279\n",
      "batch 279, loss: 0.0155, instance_loss: 0.0324, weighted_loss: 0.0206, label: 1, bag_size: 7246\n",
      "batch 299, loss: 0.0910, instance_loss: 0.2529, weighted_loss: 0.1396, label: 0, bag_size: 3101\n",
      "batch 319, loss: 0.5425, instance_loss: 0.6632, weighted_loss: 0.5787, label: 1, bag_size: 2937\n",
      "batch 339, loss: 0.0067, instance_loss: 0.0016, weighted_loss: 0.0052, label: 1, bag_size: 12349\n",
      "batch 359, loss: 0.0055, instance_loss: 0.0053, weighted_loss: 0.0055, label: 0, bag_size: 3190\n",
      "batch 379, loss: 0.2191, instance_loss: 0.2050, weighted_loss: 0.2149, label: 1, bag_size: 16890\n",
      "batch 399, loss: 0.6133, instance_loss: 0.8556, weighted_loss: 0.6860, label: 1, bag_size: 16548\n",
      "batch 419, loss: 0.0235, instance_loss: 0.0174, weighted_loss: 0.0217, label: 1, bag_size: 9147\n",
      "batch 439, loss: 0.3107, instance_loss: 0.1765, weighted_loss: 0.2705, label: 1, bag_size: 1512\n",
      "batch 459, loss: 0.0350, instance_loss: 0.0522, weighted_loss: 0.0402, label: 1, bag_size: 7981\n",
      "batch 479, loss: 0.0035, instance_loss: 0.0040, weighted_loss: 0.0037, label: 0, bag_size: 2063\n",
      "batch 499, loss: 0.2047, instance_loss: 0.1409, weighted_loss: 0.1855, label: 1, bag_size: 2455\n",
      "batch 519, loss: 4.6978, instance_loss: 4.3348, weighted_loss: 4.5889, label: 0, bag_size: 4692\n",
      "batch 539, loss: 0.0062, instance_loss: 0.0031, weighted_loss: 0.0052, label: 0, bag_size: 15313\n",
      "batch 559, loss: 0.0051, instance_loss: 0.0066, weighted_loss: 0.0056, label: 1, bag_size: 8522\n",
      "batch 579, loss: 0.0147, instance_loss: 0.0150, weighted_loss: 0.0148, label: 0, bag_size: 12593\n",
      "batch 599, loss: 0.0442, instance_loss: 0.0337, weighted_loss: 0.0411, label: 0, bag_size: 21093\n",
      "batch 619, loss: 0.1279, instance_loss: 0.1369, weighted_loss: 0.1306, label: 1, bag_size: 20767\n",
      "batch 639, loss: 0.0340, instance_loss: 0.0321, weighted_loss: 0.0335, label: 1, bag_size: 15689\n",
      "batch 659, loss: 0.1065, instance_loss: 0.0939, weighted_loss: 0.1027, label: 1, bag_size: 7351\n",
      "batch 679, loss: 0.3360, instance_loss: 0.3415, weighted_loss: 0.3376, label: 1, bag_size: 1525\n",
      "batch 699, loss: 6.1405, instance_loss: 4.8991, weighted_loss: 5.7681, label: 1, bag_size: 2565\n",
      "batch 719, loss: 0.0202, instance_loss: 0.0187, weighted_loss: 0.0198, label: 1, bag_size: 6927\n",
      "batch 739, loss: 0.0753, instance_loss: 0.0730, weighted_loss: 0.0746, label: 1, bag_size: 5921\n",
      "batch 759, loss: 1.4485, instance_loss: 1.8565, weighted_loss: 1.5709, label: 1, bag_size: 6360\n",
      "batch 779, loss: 0.0314, instance_loss: 0.0184, weighted_loss: 0.0275, label: 1, bag_size: 9230\n",
      "batch 799, loss: 0.0076, instance_loss: 0.0027, weighted_loss: 0.0061, label: 1, bag_size: 12795\n",
      "batch 819, loss: 0.0644, instance_loss: 0.0443, weighted_loss: 0.0583, label: 1, bag_size: 2682\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9712652439024391: correct 12743/13120\n",
      "class 1 clustering acc 0.8753048780487804: correct 5742/6560\n",
      "Epoch: 24, train_loss: 0.2243, train_clustering_loss:  0.2601, train_error: 0.0829\n",
      "class 0: acc 0.9108910891089109, correct 368/404\n",
      "class 1: acc 0.9230769230769231, correct 384/416\n",
      "\n",
      "Val Set, val_loss: 0.2477, val_error: 0.1182, auc: 0.9748\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.7884615384615384, correct 41/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0013, instance_loss: 0.0037, weighted_loss: 0.0020, label: 1, bag_size: 13051\n",
      "batch 39, loss: 0.0464, instance_loss: 0.0580, weighted_loss: 0.0499, label: 1, bag_size: 6734\n",
      "batch 59, loss: 0.0107, instance_loss: 0.0069, weighted_loss: 0.0095, label: 1, bag_size: 10501\n",
      "batch 79, loss: 0.0018, instance_loss: 0.0029, weighted_loss: 0.0021, label: 0, bag_size: 3787\n",
      "batch 99, loss: 0.0218, instance_loss: 0.0125, weighted_loss: 0.0190, label: 1, bag_size: 2136\n",
      "batch 119, loss: 0.0038, instance_loss: 0.0047, weighted_loss: 0.0041, label: 0, bag_size: 5551\n",
      "batch 139, loss: 1.6095, instance_loss: 1.9279, weighted_loss: 1.7050, label: 1, bag_size: 2678\n",
      "batch 159, loss: 0.0243, instance_loss: 0.0079, weighted_loss: 0.0193, label: 0, bag_size: 15313\n",
      "batch 179, loss: 0.0426, instance_loss: 0.0355, weighted_loss: 0.0405, label: 0, bag_size: 5409\n",
      "batch 199, loss: 0.0044, instance_loss: 0.0043, weighted_loss: 0.0044, label: 0, bag_size: 1984\n",
      "batch 219, loss: 0.0119, instance_loss: 0.0040, weighted_loss: 0.0095, label: 0, bag_size: 10995\n",
      "batch 239, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 25420\n",
      "batch 259, loss: 0.0077, instance_loss: 0.0234, weighted_loss: 0.0124, label: 1, bag_size: 5516\n",
      "batch 279, loss: 0.0148, instance_loss: 0.0100, weighted_loss: 0.0133, label: 0, bag_size: 12910\n",
      "batch 299, loss: 0.0027, instance_loss: 0.0027, weighted_loss: 0.0027, label: 0, bag_size: 15747\n",
      "batch 319, loss: 0.0617, instance_loss: 0.0568, weighted_loss: 0.0602, label: 0, bag_size: 21361\n",
      "batch 339, loss: 0.0140, instance_loss: 0.0171, weighted_loss: 0.0149, label: 1, bag_size: 9533\n",
      "batch 359, loss: 0.0323, instance_loss: 0.0205, weighted_loss: 0.0288, label: 0, bag_size: 6652\n",
      "batch 379, loss: 0.0023, instance_loss: 0.0015, weighted_loss: 0.0020, label: 0, bag_size: 18415\n",
      "batch 399, loss: 1.6165, instance_loss: 2.0697, weighted_loss: 1.7525, label: 0, bag_size: 2653\n",
      "batch 419, loss: 0.4068, instance_loss: 0.3773, weighted_loss: 0.3979, label: 0, bag_size: 9596\n",
      "batch 439, loss: 0.1486, instance_loss: 0.1375, weighted_loss: 0.1452, label: 1, bag_size: 2842\n",
      "batch 459, loss: 0.3409, instance_loss: 0.3856, weighted_loss: 0.3543, label: 1, bag_size: 3368\n",
      "batch 479, loss: 0.1079, instance_loss: 0.0771, weighted_loss: 0.0987, label: 0, bag_size: 2004\n",
      "batch 499, loss: 0.1541, instance_loss: 0.2151, weighted_loss: 0.1724, label: 1, bag_size: 2678\n",
      "batch 519, loss: 0.4185, instance_loss: 0.4404, weighted_loss: 0.4250, label: 0, bag_size: 2219\n",
      "batch 539, loss: 0.0105, instance_loss: 0.0074, weighted_loss: 0.0096, label: 1, bag_size: 6606\n",
      "batch 559, loss: 0.0109, instance_loss: 0.0056, weighted_loss: 0.0093, label: 1, bag_size: 12931\n",
      "batch 579, loss: 0.0709, instance_loss: 0.0705, weighted_loss: 0.0708, label: 1, bag_size: 14779\n",
      "batch 599, loss: 0.1289, instance_loss: 0.1530, weighted_loss: 0.1361, label: 1, bag_size: 6726\n",
      "batch 619, loss: 0.0838, instance_loss: 0.1076, weighted_loss: 0.0909, label: 0, bag_size: 10304\n",
      "batch 639, loss: 0.0043, instance_loss: 0.0086, weighted_loss: 0.0056, label: 0, bag_size: 19518\n",
      "batch 659, loss: 1.2641, instance_loss: 1.3355, weighted_loss: 1.2855, label: 0, bag_size: 21361\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0036, weighted_loss: 0.0011, label: 0, bag_size: 8948\n",
      "batch 699, loss: 0.1488, instance_loss: 0.1808, weighted_loss: 0.1584, label: 0, bag_size: 16052\n",
      "batch 719, loss: 0.0022, instance_loss: 0.0021, weighted_loss: 0.0022, label: 0, bag_size: 2548\n",
      "batch 739, loss: 0.0054, instance_loss: 0.0029, weighted_loss: 0.0047, label: 1, bag_size: 6606\n",
      "batch 759, loss: 0.2486, instance_loss: 0.3132, weighted_loss: 0.2680, label: 1, bag_size: 7768\n",
      "batch 779, loss: 0.0024, instance_loss: 0.0002, weighted_loss: 0.0017, label: 1, bag_size: 5025\n",
      "batch 799, loss: 0.0038, instance_loss: 0.0024, weighted_loss: 0.0034, label: 1, bag_size: 15008\n",
      "batch 819, loss: 1.3732, instance_loss: 1.6438, weighted_loss: 1.4544, label: 0, bag_size: 1498\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.968140243902439: correct 12702/13120\n",
      "class 1 clustering acc 0.8605182926829268: correct 5645/6560\n",
      "Epoch: 25, train_loss: 0.2358, train_clustering_loss:  0.2769, train_error: 0.0939\n",
      "class 0: acc 0.9113300492610837, correct 370/406\n",
      "class 1: acc 0.9009661835748792, correct 373/414\n",
      "\n",
      "Val Set, val_loss: 0.4143, val_error: 0.2000, auc: 0.9715\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.5961538461538461, correct 31/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6360, instance_loss: 0.7239, weighted_loss: 0.6624, label: 0, bag_size: 21361\n",
      "batch 39, loss: 0.0785, instance_loss: 0.1420, weighted_loss: 0.0975, label: 0, bag_size: 1891\n",
      "batch 59, loss: 0.0019, instance_loss: 0.0139, weighted_loss: 0.0055, label: 1, bag_size: 5221\n",
      "batch 79, loss: 0.1495, instance_loss: 0.2437, weighted_loss: 0.1778, label: 1, bag_size: 5366\n",
      "batch 99, loss: 0.0099, instance_loss: 0.0079, weighted_loss: 0.0093, label: 0, bag_size: 9851\n",
      "batch 119, loss: 0.1166, instance_loss: 0.1429, weighted_loss: 0.1245, label: 1, bag_size: 2137\n",
      "batch 139, loss: 0.0644, instance_loss: 0.0712, weighted_loss: 0.0664, label: 0, bag_size: 1614\n",
      "batch 159, loss: 0.4820, instance_loss: 0.5946, weighted_loss: 0.5157, label: 1, bag_size: 1051\n",
      "batch 179, loss: 0.2486, instance_loss: 0.2840, weighted_loss: 0.2592, label: 1, bag_size: 1609\n",
      "batch 199, loss: 0.0252, instance_loss: 0.0215, weighted_loss: 0.0241, label: 0, bag_size: 17268\n",
      "batch 219, loss: 0.0284, instance_loss: 0.0241, weighted_loss: 0.0271, label: 0, bag_size: 10898\n",
      "batch 239, loss: 0.0151, instance_loss: 0.0146, weighted_loss: 0.0150, label: 0, bag_size: 10791\n",
      "batch 259, loss: 0.0786, instance_loss: 0.1001, weighted_loss: 0.0850, label: 1, bag_size: 4786\n",
      "batch 279, loss: 0.0074, instance_loss: 0.0032, weighted_loss: 0.0061, label: 0, bag_size: 890\n",
      "batch 299, loss: 0.0005, instance_loss: 0.0006, weighted_loss: 0.0005, label: 0, bag_size: 9069\n",
      "batch 319, loss: 0.0146, instance_loss: 0.0109, weighted_loss: 0.0135, label: 0, bag_size: 20796\n",
      "batch 339, loss: 0.0042, instance_loss: 0.0060, weighted_loss: 0.0048, label: 1, bag_size: 10969\n",
      "batch 359, loss: 0.0242, instance_loss: 0.0114, weighted_loss: 0.0204, label: 1, bag_size: 13732\n",
      "batch 379, loss: 0.0004, instance_loss: 0.0006, weighted_loss: 0.0005, label: 1, bag_size: 6966\n",
      "batch 399, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 15003\n",
      "batch 419, loss: 0.0006, instance_loss: 0.0025, weighted_loss: 0.0011, label: 1, bag_size: 9673\n",
      "batch 439, loss: 0.0075, instance_loss: 0.0060, weighted_loss: 0.0070, label: 0, bag_size: 10263\n",
      "batch 459, loss: 0.0831, instance_loss: 0.0963, weighted_loss: 0.0870, label: 1, bag_size: 5137\n",
      "batch 479, loss: 0.0127, instance_loss: 0.0097, weighted_loss: 0.0118, label: 0, bag_size: 11146\n",
      "batch 499, loss: 0.0221, instance_loss: 0.0158, weighted_loss: 0.0202, label: 1, bag_size: 1014\n",
      "batch 519, loss: 1.2575, instance_loss: 1.7269, weighted_loss: 1.3983, label: 1, bag_size: 8103\n",
      "batch 539, loss: 0.0259, instance_loss: 0.0161, weighted_loss: 0.0230, label: 1, bag_size: 7389\n",
      "batch 559, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 19472\n",
      "batch 579, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 13964\n",
      "batch 599, loss: 0.0018, instance_loss: 0.0001, weighted_loss: 0.0013, label: 0, bag_size: 19067\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0049, weighted_loss: 0.0015, label: 1, bag_size: 5612\n",
      "batch 639, loss: 0.0018, instance_loss: 0.0026, weighted_loss: 0.0021, label: 1, bag_size: 3640\n",
      "batch 659, loss: 0.0062, instance_loss: 0.0046, weighted_loss: 0.0057, label: 1, bag_size: 14887\n",
      "batch 679, loss: 0.6879, instance_loss: 0.7375, weighted_loss: 0.7028, label: 0, bag_size: 11390\n",
      "batch 699, loss: 0.0037, instance_loss: 0.0030, weighted_loss: 0.0035, label: 1, bag_size: 1512\n",
      "batch 719, loss: 0.0002, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 3552\n",
      "batch 739, loss: 0.0111, instance_loss: 0.0043, weighted_loss: 0.0090, label: 0, bag_size: 10751\n",
      "batch 759, loss: 0.0052, instance_loss: 0.0040, weighted_loss: 0.0049, label: 1, bag_size: 11600\n",
      "batch 779, loss: 0.1584, instance_loss: 0.1848, weighted_loss: 0.1663, label: 0, bag_size: 2006\n",
      "batch 799, loss: 0.0212, instance_loss: 0.0213, weighted_loss: 0.0213, label: 0, bag_size: 1588\n",
      "batch 819, loss: 0.1791, instance_loss: 0.2536, weighted_loss: 0.2015, label: 0, bag_size: 2548\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9738567073170732: correct 12777/13120\n",
      "class 1 clustering acc 0.875: correct 5740/6560\n",
      "Epoch: 26, train_loss: 0.2093, train_clustering_loss:  0.2427, train_error: 0.0780\n",
      "class 0: acc 0.9325301204819277, correct 387/415\n",
      "class 1: acc 0.9111111111111111, correct 369/405\n",
      "\n",
      "Val Set, val_loss: 0.4125, val_error: 0.1636, auc: 0.9741\n",
      "class 0 clustering acc 0.9880681818181818: correct 1739/1760\n",
      "class 1 clustering acc 0.11136363636363636: correct 98/880\n",
      "class 0: acc 0.6923076923076923, correct 36/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3173, instance_loss: 0.5043, weighted_loss: 0.3734, label: 0, bag_size: 9387\n",
      "batch 39, loss: 0.0189, instance_loss: 0.0100, weighted_loss: 0.0162, label: 0, bag_size: 705\n",
      "batch 59, loss: 0.3770, instance_loss: 0.3778, weighted_loss: 0.3772, label: 1, bag_size: 5903\n",
      "batch 79, loss: 0.0913, instance_loss: 0.0698, weighted_loss: 0.0849, label: 0, bag_size: 6652\n",
      "batch 99, loss: 0.0019, instance_loss: 0.0002, weighted_loss: 0.0014, label: 0, bag_size: 9888\n",
      "batch 119, loss: 0.0115, instance_loss: 0.0092, weighted_loss: 0.0108, label: 1, bag_size: 19972\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0057, weighted_loss: 0.0018, label: 1, bag_size: 3224\n",
      "batch 159, loss: 0.0066, instance_loss: 0.0039, weighted_loss: 0.0058, label: 0, bag_size: 3774\n",
      "batch 179, loss: 0.0607, instance_loss: 0.0384, weighted_loss: 0.0540, label: 1, bag_size: 2140\n",
      "batch 199, loss: 0.0015, instance_loss: 0.0006, weighted_loss: 0.0012, label: 0, bag_size: 7612\n",
      "batch 219, loss: 0.0006, instance_loss: 0.0005, weighted_loss: 0.0006, label: 1, bag_size: 10396\n",
      "batch 239, loss: 1.9732, instance_loss: 2.4516, weighted_loss: 2.1167, label: 0, bag_size: 2070\n",
      "batch 259, loss: 0.0008, instance_loss: 0.0015, weighted_loss: 0.0010, label: 0, bag_size: 15313\n",
      "batch 279, loss: 0.0700, instance_loss: 0.0590, weighted_loss: 0.0667, label: 0, bag_size: 1458\n",
      "batch 299, loss: 0.0055, instance_loss: 0.0028, weighted_loss: 0.0047, label: 0, bag_size: 3810\n",
      "batch 319, loss: 0.0077, instance_loss: 0.0068, weighted_loss: 0.0075, label: 1, bag_size: 2140\n",
      "batch 339, loss: 0.1910, instance_loss: 0.1961, weighted_loss: 0.1925, label: 0, bag_size: 2006\n",
      "batch 359, loss: 0.0273, instance_loss: 0.0298, weighted_loss: 0.0281, label: 0, bag_size: 1745\n",
      "batch 379, loss: 0.0016, instance_loss: 0.0008, weighted_loss: 0.0013, label: 0, bag_size: 11759\n",
      "batch 399, loss: 0.3956, instance_loss: 0.4586, weighted_loss: 0.4145, label: 1, bag_size: 5155\n",
      "batch 419, loss: 0.0364, instance_loss: 0.0298, weighted_loss: 0.0344, label: 1, bag_size: 9533\n",
      "batch 439, loss: 0.0026, instance_loss: 0.0016, weighted_loss: 0.0023, label: 0, bag_size: 10146\n",
      "batch 459, loss: 0.0549, instance_loss: 0.0391, weighted_loss: 0.0502, label: 1, bag_size: 13365\n",
      "batch 479, loss: 0.1300, instance_loss: 0.1173, weighted_loss: 0.1262, label: 1, bag_size: 1014\n",
      "batch 499, loss: 0.0908, instance_loss: 0.0612, weighted_loss: 0.0819, label: 0, bag_size: 1438\n",
      "batch 519, loss: 1.9377, instance_loss: 2.8929, weighted_loss: 2.2242, label: 0, bag_size: 5211\n",
      "batch 539, loss: 0.0012, instance_loss: 0.0002, weighted_loss: 0.0009, label: 0, bag_size: 16992\n",
      "batch 559, loss: 0.3882, instance_loss: 0.6255, weighted_loss: 0.4594, label: 1, bag_size: 13440\n",
      "batch 579, loss: 0.0051, instance_loss: 0.0033, weighted_loss: 0.0045, label: 1, bag_size: 10492\n",
      "batch 599, loss: 0.0023, instance_loss: 0.0025, weighted_loss: 0.0024, label: 1, bag_size: 9470\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 0, bag_size: 6652\n",
      "batch 639, loss: 1.6391, instance_loss: 2.3319, weighted_loss: 1.8469, label: 1, bag_size: 13089\n",
      "batch 659, loss: 0.3392, instance_loss: 0.3895, weighted_loss: 0.3543, label: 1, bag_size: 8103\n",
      "batch 679, loss: 0.0156, instance_loss: 0.0080, weighted_loss: 0.0134, label: 1, bag_size: 6343\n",
      "batch 699, loss: 0.0582, instance_loss: 0.0289, weighted_loss: 0.0494, label: 1, bag_size: 3674\n",
      "batch 719, loss: 0.1334, instance_loss: 0.1288, weighted_loss: 0.1320, label: 0, bag_size: 13992\n",
      "batch 739, loss: 1.0474, instance_loss: 1.0762, weighted_loss: 1.0560, label: 1, bag_size: 13440\n",
      "batch 759, loss: 0.0009, instance_loss: 0.0003, weighted_loss: 0.0008, label: 0, bag_size: 3552\n",
      "batch 779, loss: 0.1329, instance_loss: 0.1090, weighted_loss: 0.1257, label: 1, bag_size: 10072\n",
      "batch 799, loss: 0.1364, instance_loss: 0.1065, weighted_loss: 0.1275, label: 0, bag_size: 2091\n",
      "batch 819, loss: 0.0150, instance_loss: 0.0172, weighted_loss: 0.0157, label: 1, bag_size: 13194\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9708079268292683: correct 12737/13120\n",
      "class 1 clustering acc 0.8620426829268293: correct 5655/6560\n",
      "Epoch: 27, train_loss: 0.2342, train_clustering_loss:  0.2756, train_error: 0.0890\n",
      "class 0: acc 0.9093137254901961, correct 371/408\n",
      "class 1: acc 0.912621359223301, correct 376/412\n",
      "\n",
      "Val Set, val_loss: 0.1996, val_error: 0.0909, auc: 0.9778\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.01818181818181818: correct 16/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "Validation loss decreased (0.220261 --> 0.199585).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0141, instance_loss: 0.0290, weighted_loss: 0.0186, label: 0, bag_size: 12593\n",
      "batch 39, loss: 0.0295, instance_loss: 0.0317, weighted_loss: 0.0302, label: 0, bag_size: 3198\n",
      "batch 59, loss: 0.0202, instance_loss: 0.0091, weighted_loss: 0.0169, label: 1, bag_size: 16267\n",
      "batch 79, loss: 0.0009, instance_loss: 0.0021, weighted_loss: 0.0013, label: 1, bag_size: 15093\n",
      "batch 99, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 1, bag_size: 4259\n",
      "batch 119, loss: 0.0041, instance_loss: 0.0027, weighted_loss: 0.0037, label: 0, bag_size: 2063\n",
      "batch 139, loss: 0.0242, instance_loss: 0.4637, weighted_loss: 0.1560, label: 1, bag_size: 2662\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0766, weighted_loss: 0.0231, label: 0, bag_size: 518\n",
      "batch 179, loss: 0.0070, instance_loss: 0.0018, weighted_loss: 0.0054, label: 0, bag_size: 11199\n",
      "batch 199, loss: 0.0043, instance_loss: 0.0037, weighted_loss: 0.0041, label: 1, bag_size: 1638\n",
      "batch 219, loss: 0.1283, instance_loss: 0.1374, weighted_loss: 0.1310, label: 1, bag_size: 4786\n",
      "batch 239, loss: 0.0503, instance_loss: 0.0276, weighted_loss: 0.0435, label: 0, bag_size: 1506\n",
      "batch 259, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 5317\n",
      "batch 279, loss: 0.0026, instance_loss: 0.0017, weighted_loss: 0.0023, label: 1, bag_size: 12795\n",
      "batch 299, loss: 0.0024, instance_loss: 0.0015, weighted_loss: 0.0022, label: 1, bag_size: 11394\n",
      "batch 319, loss: 0.0021, instance_loss: 0.0007, weighted_loss: 0.0017, label: 0, bag_size: 30751\n",
      "batch 339, loss: 0.0985, instance_loss: 0.0656, weighted_loss: 0.0886, label: 1, bag_size: 22264\n",
      "batch 359, loss: 0.1053, instance_loss: 0.0609, weighted_loss: 0.0920, label: 0, bag_size: 9930\n",
      "batch 379, loss: 0.0110, instance_loss: 0.0077, weighted_loss: 0.0100, label: 1, bag_size: 2495\n",
      "batch 399, loss: 0.0004, instance_loss: 0.0054, weighted_loss: 0.0019, label: 1, bag_size: 4880\n",
      "batch 419, loss: 0.0045, instance_loss: 0.0079, weighted_loss: 0.0055, label: 1, bag_size: 1051\n",
      "batch 439, loss: 0.0166, instance_loss: 0.0103, weighted_loss: 0.0147, label: 0, bag_size: 1483\n",
      "batch 459, loss: 0.0016, instance_loss: 0.0022, weighted_loss: 0.0017, label: 1, bag_size: 1255\n",
      "batch 479, loss: 0.3056, instance_loss: 0.4002, weighted_loss: 0.3340, label: 0, bag_size: 15898\n",
      "batch 499, loss: 0.0015, instance_loss: 0.0028, weighted_loss: 0.0019, label: 1, bag_size: 15233\n",
      "batch 519, loss: 0.0286, instance_loss: 0.0211, weighted_loss: 0.0264, label: 0, bag_size: 8582\n",
      "batch 539, loss: 0.0045, instance_loss: 0.0034, weighted_loss: 0.0042, label: 0, bag_size: 8959\n",
      "batch 559, loss: 0.0205, instance_loss: 0.0226, weighted_loss: 0.0211, label: 1, bag_size: 689\n",
      "batch 579, loss: 0.0171, instance_loss: 0.0132, weighted_loss: 0.0159, label: 0, bag_size: 3190\n",
      "batch 599, loss: 0.1225, instance_loss: 0.1301, weighted_loss: 0.1248, label: 0, bag_size: 21864\n",
      "batch 619, loss: 2.0731, instance_loss: 2.5164, weighted_loss: 2.2061, label: 0, bag_size: 9616\n",
      "batch 639, loss: 0.0309, instance_loss: 0.0181, weighted_loss: 0.0271, label: 0, bag_size: 11259\n",
      "batch 659, loss: 0.0123, instance_loss: 0.0100, weighted_loss: 0.0116, label: 0, bag_size: 16211\n",
      "batch 679, loss: 0.0201, instance_loss: 0.0222, weighted_loss: 0.0207, label: 1, bag_size: 1255\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0074, weighted_loss: 0.0023, label: 1, bag_size: 4877\n",
      "batch 719, loss: 0.0124, instance_loss: 0.0182, weighted_loss: 0.0142, label: 1, bag_size: 2904\n",
      "batch 739, loss: 0.0172, instance_loss: 0.0092, weighted_loss: 0.0148, label: 0, bag_size: 13880\n",
      "batch 759, loss: 0.9891, instance_loss: 1.4107, weighted_loss: 1.1156, label: 0, bag_size: 7428\n",
      "batch 779, loss: 0.0389, instance_loss: 0.0388, weighted_loss: 0.0389, label: 1, bag_size: 11316\n",
      "batch 799, loss: 0.6646, instance_loss: 0.9581, weighted_loss: 0.7526, label: 0, bag_size: 2959\n",
      "batch 819, loss: 0.0097, instance_loss: 0.0054, weighted_loss: 0.0084, label: 1, bag_size: 8754\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9772865853658537: correct 12822/13120\n",
      "class 1 clustering acc 0.879420731707317: correct 5769/6560\n",
      "Epoch: 28, train_loss: 0.1992, train_clustering_loss:  0.2343, train_error: 0.0732\n",
      "class 0: acc 0.9292929292929293, correct 368/396\n",
      "class 1: acc 0.9245283018867925, correct 392/424\n",
      "\n",
      "Val Set, val_loss: 0.2319, val_error: 0.1000, auc: 0.9695\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0035, instance_loss: 0.0156, weighted_loss: 0.0071, label: 1, bag_size: 5864\n",
      "batch 39, loss: 0.0023, instance_loss: 0.0008, weighted_loss: 0.0018, label: 0, bag_size: 8981\n",
      "batch 59, loss: 0.0002, instance_loss: 0.0007, weighted_loss: 0.0003, label: 1, bag_size: 14202\n",
      "batch 79, loss: 0.0300, instance_loss: 0.0214, weighted_loss: 0.0274, label: 0, bag_size: 1814\n",
      "batch 99, loss: 0.0957, instance_loss: 0.1061, weighted_loss: 0.0988, label: 0, bag_size: 2351\n",
      "batch 119, loss: 0.0196, instance_loss: 0.0200, weighted_loss: 0.0198, label: 0, bag_size: 12793\n",
      "batch 139, loss: 0.0028, instance_loss: 0.0037, weighted_loss: 0.0031, label: 0, bag_size: 14266\n",
      "batch 159, loss: 0.0068, instance_loss: 0.0081, weighted_loss: 0.0072, label: 0, bag_size: 8025\n",
      "batch 179, loss: 0.0967, instance_loss: 0.0893, weighted_loss: 0.0944, label: 0, bag_size: 1508\n",
      "batch 199, loss: 0.0072, instance_loss: 0.0066, weighted_loss: 0.0070, label: 1, bag_size: 2495\n",
      "batch 219, loss: 0.0040, instance_loss: 0.0014, weighted_loss: 0.0032, label: 0, bag_size: 47866\n",
      "batch 239, loss: 0.0451, instance_loss: 0.0414, weighted_loss: 0.0440, label: 0, bag_size: 1772\n",
      "batch 259, loss: 0.8438, instance_loss: 1.1852, weighted_loss: 0.9462, label: 0, bag_size: 6281\n",
      "batch 279, loss: 0.0005, instance_loss: 0.0007, weighted_loss: 0.0006, label: 0, bag_size: 10751\n",
      "batch 299, loss: 0.0035, instance_loss: 0.0035, weighted_loss: 0.0035, label: 1, bag_size: 5340\n",
      "batch 319, loss: 0.0000, instance_loss: 0.1383, weighted_loss: 0.0415, label: 1, bag_size: 2385\n",
      "batch 339, loss: 0.0058, instance_loss: 0.0596, weighted_loss: 0.0219, label: 1, bag_size: 621\n",
      "batch 359, loss: 0.0144, instance_loss: 0.0063, weighted_loss: 0.0120, label: 0, bag_size: 3265\n",
      "batch 379, loss: 0.0022, instance_loss: 0.0005, weighted_loss: 0.0017, label: 1, bag_size: 7389\n",
      "batch 399, loss: 0.1834, instance_loss: 0.3292, weighted_loss: 0.2272, label: 0, bag_size: 11390\n",
      "batch 419, loss: 0.0066, instance_loss: 0.0021, weighted_loss: 0.0052, label: 0, bag_size: 13777\n",
      "batch 439, loss: 0.1080, instance_loss: 0.0883, weighted_loss: 0.1021, label: 1, bag_size: 9162\n",
      "batch 459, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 1881\n",
      "batch 479, loss: 0.0011, instance_loss: 0.0028, weighted_loss: 0.0016, label: 1, bag_size: 2136\n",
      "batch 499, loss: 0.5862, instance_loss: 0.6322, weighted_loss: 0.6000, label: 0, bag_size: 9387\n",
      "batch 519, loss: 3.2733, instance_loss: 4.5014, weighted_loss: 3.6417, label: 0, bag_size: 2815\n",
      "batch 539, loss: 0.0016, instance_loss: 0.0123, weighted_loss: 0.0048, label: 0, bag_size: 65728\n",
      "batch 559, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 11512\n",
      "batch 579, loss: 0.0294, instance_loss: 0.0241, weighted_loss: 0.0278, label: 1, bag_size: 11729\n",
      "batch 599, loss: 0.0131, instance_loss: 0.0076, weighted_loss: 0.0115, label: 1, bag_size: 5690\n",
      "batch 619, loss: 0.0780, instance_loss: 0.0915, weighted_loss: 0.0820, label: 0, bag_size: 2004\n",
      "batch 639, loss: 0.0041, instance_loss: 3.1449, weighted_loss: 0.9463, label: 0, bag_size: 3725\n",
      "batch 659, loss: 0.0027, instance_loss: 0.0004, weighted_loss: 0.0020, label: 1, bag_size: 13947\n",
      "batch 679, loss: 0.0035, instance_loss: 0.0051, weighted_loss: 0.0040, label: 1, bag_size: 17769\n",
      "batch 699, loss: 0.0257, instance_loss: 0.0624, weighted_loss: 0.0367, label: 0, bag_size: 24439\n",
      "batch 719, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 1, bag_size: 11642\n",
      "batch 739, loss: 0.0080, instance_loss: 0.0202, weighted_loss: 0.0116, label: 0, bag_size: 10490\n",
      "batch 759, loss: 0.0008, instance_loss: 0.0022, weighted_loss: 0.0012, label: 0, bag_size: 21864\n",
      "batch 779, loss: 0.0041, instance_loss: 0.0082, weighted_loss: 0.0053, label: 1, bag_size: 1064\n",
      "batch 799, loss: 0.0068, instance_loss: 0.0029, weighted_loss: 0.0056, label: 1, bag_size: 15689\n",
      "batch 819, loss: 0.0789, instance_loss: 0.0716, weighted_loss: 0.0767, label: 1, bag_size: 16565\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.977515243902439: correct 12825/13120\n",
      "class 1 clustering acc 0.875609756097561: correct 5744/6560\n",
      "Epoch: 29, train_loss: 0.1707, train_clustering_loss:  0.2470, train_error: 0.0720\n",
      "class 0: acc 0.9267676767676768, correct 367/396\n",
      "class 1: acc 0.9292452830188679, correct 394/424\n",
      "\n",
      "Val Set, val_loss: 0.2310, val_error: 0.1000, auc: 0.9771\n",
      "class 0 clustering acc 0.9863636363636363: correct 1736/1760\n",
      "class 1 clustering acc 0.08977272727272727: correct 79/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8448275862068966, correct 49/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0245, instance_loss: 0.0162, weighted_loss: 0.0220, label: 0, bag_size: 1651\n",
      "batch 39, loss: 0.0296, instance_loss: 0.0316, weighted_loss: 0.0302, label: 0, bag_size: 1920\n",
      "batch 59, loss: 0.0029, instance_loss: 0.0044, weighted_loss: 0.0034, label: 0, bag_size: 12731\n",
      "batch 79, loss: 0.0092, instance_loss: 0.0038, weighted_loss: 0.0076, label: 1, bag_size: 11981\n",
      "batch 99, loss: 0.0004, instance_loss: 0.0039, weighted_loss: 0.0014, label: 0, bag_size: 10898\n",
      "batch 119, loss: 0.0015, instance_loss: 0.0019, weighted_loss: 0.0017, label: 0, bag_size: 11527\n",
      "batch 139, loss: 4.4517, instance_loss: 4.4459, weighted_loss: 4.4499, label: 0, bag_size: 5120\n",
      "batch 159, loss: 0.0294, instance_loss: 0.0193, weighted_loss: 0.0264, label: 1, bag_size: 7110\n",
      "batch 179, loss: 0.0340, instance_loss: 0.0241, weighted_loss: 0.0310, label: 1, bag_size: 4939\n",
      "batch 199, loss: 0.0089, instance_loss: 0.0080, weighted_loss: 0.0086, label: 1, bag_size: 9561\n",
      "batch 219, loss: 0.0501, instance_loss: 0.0574, weighted_loss: 0.0523, label: 0, bag_size: 1416\n",
      "batch 239, loss: 3.0581, instance_loss: 3.3605, weighted_loss: 3.1488, label: 1, bag_size: 2565\n",
      "batch 259, loss: 0.0043, instance_loss: 0.0020, weighted_loss: 0.0036, label: 1, bag_size: 15233\n",
      "batch 279, loss: 0.0037, instance_loss: 0.0069, weighted_loss: 0.0047, label: 0, bag_size: 9069\n",
      "batch 299, loss: 0.0002, instance_loss: 0.0016, weighted_loss: 0.0006, label: 0, bag_size: 9470\n",
      "batch 319, loss: 0.0012, instance_loss: 0.0017, weighted_loss: 0.0013, label: 0, bag_size: 2091\n",
      "batch 339, loss: 0.0008, instance_loss: 0.0004, weighted_loss: 0.0007, label: 1, bag_size: 11256\n",
      "batch 359, loss: 1.2725, instance_loss: 1.7833, weighted_loss: 1.4257, label: 0, bag_size: 11128\n",
      "batch 379, loss: 0.0048, instance_loss: 0.0885, weighted_loss: 0.0299, label: 1, bag_size: 2412\n",
      "batch 399, loss: 0.2124, instance_loss: 0.2259, weighted_loss: 0.2165, label: 0, bag_size: 11259\n",
      "batch 419, loss: 0.0014, instance_loss: 0.0011, weighted_loss: 0.0013, label: 0, bag_size: 22828\n",
      "batch 439, loss: 0.0013, instance_loss: 0.0014, weighted_loss: 0.0013, label: 1, bag_size: 19039\n",
      "batch 459, loss: 0.3376, instance_loss: 0.3652, weighted_loss: 0.3459, label: 0, bag_size: 14664\n",
      "batch 479, loss: 0.0465, instance_loss: 0.0405, weighted_loss: 0.0447, label: 0, bag_size: 8549\n",
      "batch 499, loss: 0.1007, instance_loss: 0.0998, weighted_loss: 0.1004, label: 1, bag_size: 1038\n",
      "batch 519, loss: 4.2312, instance_loss: 4.5817, weighted_loss: 4.3364, label: 1, bag_size: 9162\n",
      "batch 539, loss: 0.0036, instance_loss: 0.0016, weighted_loss: 0.0030, label: 1, bag_size: 12895\n",
      "batch 559, loss: 0.0063, instance_loss: 0.0042, weighted_loss: 0.0057, label: 1, bag_size: 14681\n",
      "batch 579, loss: 0.0050, instance_loss: 0.0040, weighted_loss: 0.0047, label: 1, bag_size: 6317\n",
      "batch 599, loss: 0.0305, instance_loss: 0.0266, weighted_loss: 0.0293, label: 1, bag_size: 5256\n",
      "batch 619, loss: 3.4731, instance_loss: 3.7441, weighted_loss: 3.5544, label: 1, bag_size: 15185\n",
      "batch 639, loss: 0.3169, instance_loss: 0.3257, weighted_loss: 0.3195, label: 0, bag_size: 6624\n",
      "batch 659, loss: 0.0400, instance_loss: 0.0274, weighted_loss: 0.0362, label: 0, bag_size: 2367\n",
      "batch 679, loss: 0.0214, instance_loss: 0.0200, weighted_loss: 0.0210, label: 0, bag_size: 23791\n",
      "batch 699, loss: 0.7199, instance_loss: 1.0536, weighted_loss: 0.8200, label: 1, bag_size: 1095\n",
      "batch 719, loss: 0.0459, instance_loss: 0.0316, weighted_loss: 0.0416, label: 1, bag_size: 771\n",
      "batch 739, loss: 0.3493, instance_loss: 0.4284, weighted_loss: 0.3730, label: 1, bag_size: 12340\n",
      "batch 759, loss: 0.0042, instance_loss: 0.0014, weighted_loss: 0.0034, label: 1, bag_size: 12603\n",
      "batch 779, loss: 0.0109, instance_loss: 0.0129, weighted_loss: 0.0115, label: 1, bag_size: 2137\n",
      "batch 799, loss: 0.0010, instance_loss: 0.0003, weighted_loss: 0.0008, label: 0, bag_size: 13892\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0185, weighted_loss: 0.0056, label: 1, bag_size: 19039\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9788871951219512: correct 12843/13120\n",
      "class 1 clustering acc 0.9: correct 5904/6560\n",
      "Epoch: 30, train_loss: 0.1834, train_clustering_loss:  0.2078, train_error: 0.0610\n",
      "class 0: acc 0.946078431372549, correct 386/408\n",
      "class 1: acc 0.9320388349514563, correct 384/412\n",
      "\n",
      "Val Set, val_loss: 0.4381, val_error: 0.1727, auc: 0.9738\n",
      "class 0 clustering acc 0.9636363636363636: correct 1696/1760\n",
      "class 1 clustering acc 0.10909090909090909: correct 96/880\n",
      "class 0: acc 0.6730769230769231, correct 35/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0280, instance_loss: 0.0232, weighted_loss: 0.0266, label: 1, bag_size: 1746\n",
      "batch 39, loss: 0.0010, instance_loss: 0.0013, weighted_loss: 0.0011, label: 1, bag_size: 6731\n",
      "batch 59, loss: 0.4061, instance_loss: 0.3890, weighted_loss: 0.4009, label: 0, bag_size: 1508\n",
      "batch 79, loss: 0.0151, instance_loss: 0.0108, weighted_loss: 0.0138, label: 1, bag_size: 3968\n",
      "batch 99, loss: 0.0006, instance_loss: 0.0005, weighted_loss: 0.0006, label: 0, bag_size: 15967\n",
      "batch 119, loss: 0.0006, instance_loss: 0.0006, weighted_loss: 0.0006, label: 1, bag_size: 4259\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 518\n",
      "batch 159, loss: 0.0226, instance_loss: 0.0166, weighted_loss: 0.0208, label: 0, bag_size: 16211\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 10481\n",
      "batch 199, loss: 0.0002, instance_loss: 0.0037, weighted_loss: 0.0012, label: 1, bag_size: 4250\n",
      "batch 219, loss: 0.0339, instance_loss: 0.0257, weighted_loss: 0.0314, label: 0, bag_size: 1483\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 5965\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0009, weighted_loss: 0.0003, label: 0, bag_size: 16782\n",
      "batch 279, loss: 0.8843, instance_loss: 1.0952, weighted_loss: 0.9476, label: 0, bag_size: 4523\n",
      "batch 299, loss: 0.0066, instance_loss: 0.0076, weighted_loss: 0.0069, label: 0, bag_size: 11259\n",
      "batch 319, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 15093\n",
      "batch 339, loss: 0.0010, instance_loss: 0.0009, weighted_loss: 0.0010, label: 0, bag_size: 5009\n",
      "batch 359, loss: 0.0004, instance_loss: 0.0002, weighted_loss: 0.0004, label: 1, bag_size: 3968\n",
      "batch 379, loss: 0.0014, instance_loss: 0.0008, weighted_loss: 0.0012, label: 1, bag_size: 12795\n",
      "batch 399, loss: 0.0002, instance_loss: 0.0006, weighted_loss: 0.0003, label: 0, bag_size: 4465\n",
      "batch 419, loss: 0.0641, instance_loss: 0.0355, weighted_loss: 0.0555, label: 1, bag_size: 18649\n",
      "batch 439, loss: 0.0036, instance_loss: 0.0189, weighted_loss: 0.0082, label: 0, bag_size: 8252\n",
      "batch 459, loss: 0.0047, instance_loss: 0.0051, weighted_loss: 0.0048, label: 1, bag_size: 8438\n",
      "batch 479, loss: 0.0077, instance_loss: 0.0103, weighted_loss: 0.0085, label: 0, bag_size: 9069\n",
      "batch 499, loss: 0.0002, instance_loss: 0.0013, weighted_loss: 0.0005, label: 0, bag_size: 3444\n",
      "batch 519, loss: 0.0039, instance_loss: 0.0017, weighted_loss: 0.0032, label: 1, bag_size: 11160\n",
      "batch 539, loss: 0.0055, instance_loss: 0.0073, weighted_loss: 0.0060, label: 0, bag_size: 11759\n",
      "batch 559, loss: 0.0994, instance_loss: 0.0832, weighted_loss: 0.0945, label: 1, bag_size: 11316\n",
      "batch 579, loss: 0.0247, instance_loss: 0.0204, weighted_loss: 0.0234, label: 1, bag_size: 7669\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0005, label: 0, bag_size: 10481\n",
      "batch 619, loss: 0.0008, instance_loss: 0.0021, weighted_loss: 0.0012, label: 0, bag_size: 9234\n",
      "batch 639, loss: 0.3995, instance_loss: 0.4236, weighted_loss: 0.4068, label: 1, bag_size: 20537\n",
      "batch 659, loss: 0.4441, instance_loss: 0.5330, weighted_loss: 0.4708, label: 0, bag_size: 1714\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0064, weighted_loss: 0.0019, label: 0, bag_size: 8661\n",
      "batch 699, loss: 0.4970, instance_loss: 0.8633, weighted_loss: 0.6069, label: 1, bag_size: 771\n",
      "batch 719, loss: 0.0124, instance_loss: 0.0154, weighted_loss: 0.0133, label: 0, bag_size: 18777\n",
      "batch 739, loss: 0.0345, instance_loss: 0.0795, weighted_loss: 0.0480, label: 1, bag_size: 10033\n",
      "batch 759, loss: 0.5663, instance_loss: 0.7601, weighted_loss: 0.6244, label: 1, bag_size: 6360\n",
      "batch 779, loss: 0.0003, instance_loss: 0.0018, weighted_loss: 0.0008, label: 1, bag_size: 6792\n",
      "batch 799, loss: 0.0068, instance_loss: 0.0062, weighted_loss: 0.0066, label: 1, bag_size: 1483\n",
      "batch 819, loss: 0.0468, instance_loss: 0.1811, weighted_loss: 0.0871, label: 1, bag_size: 10725\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9766006097560975: correct 12813/13120\n",
      "class 1 clustering acc 0.8810975609756098: correct 5780/6560\n",
      "Epoch: 31, train_loss: 0.2064, train_clustering_loss:  0.2380, train_error: 0.0756\n",
      "class 0: acc 0.9236453201970444, correct 375/406\n",
      "class 1: acc 0.9251207729468599, correct 383/414\n",
      "\n",
      "Val Set, val_loss: 0.2617, val_error: 0.1091, auc: 0.9655\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5439, instance_loss: 0.9669, weighted_loss: 0.6708, label: 0, bag_size: 4418\n",
      "batch 39, loss: 0.0206, instance_loss: 0.0223, weighted_loss: 0.0211, label: 1, bag_size: 3856\n",
      "batch 59, loss: 0.0038, instance_loss: 0.0029, weighted_loss: 0.0035, label: 1, bag_size: 4259\n",
      "batch 79, loss: 0.1863, instance_loss: 0.2857, weighted_loss: 0.2161, label: 1, bag_size: 6682\n",
      "batch 99, loss: 2.9232, instance_loss: 3.3735, weighted_loss: 3.0583, label: 0, bag_size: 1701\n",
      "batch 119, loss: 0.0050, instance_loss: 0.0038, weighted_loss: 0.0046, label: 0, bag_size: 2732\n",
      "batch 139, loss: 0.0825, instance_loss: 0.0934, weighted_loss: 0.0858, label: 1, bag_size: 549\n",
      "batch 159, loss: 0.0005, instance_loss: 0.0004, weighted_loss: 0.0005, label: 1, bag_size: 3082\n",
      "batch 179, loss: 0.0258, instance_loss: 0.0230, weighted_loss: 0.0250, label: 0, bag_size: 25814\n",
      "batch 199, loss: 0.0005, instance_loss: 0.0018, weighted_loss: 0.0008, label: 1, bag_size: 8466\n",
      "batch 219, loss: 0.0101, instance_loss: 0.0172, weighted_loss: 0.0122, label: 0, bag_size: 13591\n",
      "batch 239, loss: 0.0004, instance_loss: 0.0005, weighted_loss: 0.0005, label: 0, bag_size: 17630\n",
      "batch 259, loss: 0.5862, instance_loss: 0.6350, weighted_loss: 0.6009, label: 0, bag_size: 1814\n",
      "batch 279, loss: 0.0005, instance_loss: 0.0001, weighted_loss: 0.0004, label: 0, bag_size: 2360\n",
      "batch 299, loss: 0.0834, instance_loss: 0.0484, weighted_loss: 0.0729, label: 1, bag_size: 9610\n",
      "batch 319, loss: 0.5803, instance_loss: 0.7448, weighted_loss: 0.6297, label: 1, bag_size: 10072\n",
      "batch 339, loss: 1.1436, instance_loss: 1.2514, weighted_loss: 1.1759, label: 1, bag_size: 9942\n",
      "batch 359, loss: 0.0063, instance_loss: 0.0059, weighted_loss: 0.0061, label: 1, bag_size: 11981\n",
      "batch 379, loss: 0.6445, instance_loss: 0.9226, weighted_loss: 0.7280, label: 0, bag_size: 1953\n",
      "batch 399, loss: 0.1650, instance_loss: 0.1457, weighted_loss: 0.1593, label: 1, bag_size: 1683\n",
      "batch 419, loss: 0.0016, instance_loss: 0.0019, weighted_loss: 0.0017, label: 1, bag_size: 7798\n",
      "batch 439, loss: 0.1013, instance_loss: 0.0937, weighted_loss: 0.0990, label: 0, bag_size: 11281\n",
      "batch 459, loss: 1.4960, instance_loss: 1.6452, weighted_loss: 1.5408, label: 0, bag_size: 18516\n",
      "batch 479, loss: 0.0010, instance_loss: 0.0012, weighted_loss: 0.0011, label: 0, bag_size: 11125\n",
      "batch 499, loss: 0.0518, instance_loss: 0.0485, weighted_loss: 0.0508, label: 1, bag_size: 2678\n",
      "batch 519, loss: 0.0748, instance_loss: 0.0629, weighted_loss: 0.0712, label: 0, bag_size: 3774\n",
      "batch 539, loss: 0.0354, instance_loss: 0.0275, weighted_loss: 0.0330, label: 1, bag_size: 14887\n",
      "batch 559, loss: 0.0033, instance_loss: 0.0004, weighted_loss: 0.0025, label: 1, bag_size: 7246\n",
      "batch 579, loss: 0.0947, instance_loss: 0.0601, weighted_loss: 0.0843, label: 0, bag_size: 24439\n",
      "batch 599, loss: 0.6476, instance_loss: 0.9048, weighted_loss: 0.7248, label: 0, bag_size: 1437\n",
      "batch 619, loss: 0.7887, instance_loss: 1.2574, weighted_loss: 0.9293, label: 0, bag_size: 18516\n",
      "batch 639, loss: 0.3154, instance_loss: 0.3844, weighted_loss: 0.3361, label: 0, bag_size: 11151\n",
      "batch 659, loss: 0.0237, instance_loss: 0.0125, weighted_loss: 0.0203, label: 0, bag_size: 3198\n",
      "batch 679, loss: 0.0011, instance_loss: 0.0003, weighted_loss: 0.0009, label: 0, bag_size: 11654\n",
      "batch 699, loss: 0.0672, instance_loss: 0.0648, weighted_loss: 0.0665, label: 1, bag_size: 21450\n",
      "batch 719, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 12593\n",
      "batch 739, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 2367\n",
      "batch 759, loss: 0.0267, instance_loss: 0.0168, weighted_loss: 0.0237, label: 1, bag_size: 2678\n",
      "batch 779, loss: 0.0357, instance_loss: 0.0739, weighted_loss: 0.0472, label: 0, bag_size: 2070\n",
      "batch 799, loss: 0.0001, instance_loss: 0.0897, weighted_loss: 0.0270, label: 0, bag_size: 19390\n",
      "batch 819, loss: 0.0188, instance_loss: 0.0085, weighted_loss: 0.0157, label: 0, bag_size: 8252\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9780487804878049: correct 12832/13120\n",
      "class 1 clustering acc 0.9016768292682927: correct 5915/6560\n",
      "Epoch: 32, train_loss: 0.1623, train_clustering_loss:  0.2141, train_error: 0.0634\n",
      "class 0: acc 0.9403341288782816, correct 394/419\n",
      "class 1: acc 0.9326683291770573, correct 374/401\n",
      "\n",
      "Val Set, val_loss: 0.4379, val_error: 0.1909, auc: 0.9692\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.6346153846153846, correct 33/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0105, instance_loss: 0.0386, weighted_loss: 0.0189, label: 0, bag_size: 26271\n",
      "batch 39, loss: 0.0019, instance_loss: 0.0248, weighted_loss: 0.0088, label: 1, bag_size: 14618\n",
      "batch 59, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 23714\n",
      "batch 79, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9949\n",
      "batch 99, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 22870\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0009, weighted_loss: 0.0003, label: 1, bag_size: 7217\n",
      "batch 139, loss: 0.0006, instance_loss: 0.0020, weighted_loss: 0.0010, label: 0, bag_size: 47866\n",
      "batch 159, loss: 0.0069, instance_loss: 0.0050, weighted_loss: 0.0063, label: 1, bag_size: 2136\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 0, bag_size: 2036\n",
      "batch 199, loss: 0.3898, instance_loss: 0.5762, weighted_loss: 0.4457, label: 0, bag_size: 15071\n",
      "batch 219, loss: 0.0348, instance_loss: 0.0041, weighted_loss: 0.0256, label: 1, bag_size: 6734\n",
      "batch 239, loss: 0.0448, instance_loss: 0.1183, weighted_loss: 0.0668, label: 1, bag_size: 10848\n",
      "batch 259, loss: 0.0082, instance_loss: 0.0079, weighted_loss: 0.0081, label: 0, bag_size: 14266\n",
      "batch 279, loss: 0.0011, instance_loss: 0.0001, weighted_loss: 0.0008, label: 0, bag_size: 3908\n",
      "batch 299, loss: 0.4556, instance_loss: 0.6203, weighted_loss: 0.5050, label: 1, bag_size: 3121\n",
      "batch 319, loss: 0.6327, instance_loss: 0.7314, weighted_loss: 0.6623, label: 1, bag_size: 1683\n",
      "batch 339, loss: 0.0096, instance_loss: 0.0089, weighted_loss: 0.0094, label: 1, bag_size: 11363\n",
      "batch 359, loss: 0.2688, instance_loss: 0.4436, weighted_loss: 0.3213, label: 0, bag_size: 21361\n",
      "batch 379, loss: 0.0297, instance_loss: 0.0269, weighted_loss: 0.0289, label: 0, bag_size: 16720\n",
      "batch 399, loss: 0.0877, instance_loss: 0.0720, weighted_loss: 0.0830, label: 0, bag_size: 26208\n",
      "batch 419, loss: 0.0004, instance_loss: 0.0164, weighted_loss: 0.0052, label: 1, bag_size: 12349\n",
      "batch 439, loss: 0.0018, instance_loss: 0.0015, weighted_loss: 0.0017, label: 0, bag_size: 9930\n",
      "batch 459, loss: 0.0002, instance_loss: 0.0135, weighted_loss: 0.0042, label: 0, bag_size: 2624\n",
      "batch 479, loss: 0.0010, instance_loss: 0.0029, weighted_loss: 0.0015, label: 1, bag_size: 29832\n",
      "batch 499, loss: 0.0006, instance_loss: 0.0005, weighted_loss: 0.0006, label: 1, bag_size: 18468\n",
      "batch 519, loss: 0.0005, instance_loss: 0.0014, weighted_loss: 0.0008, label: 1, bag_size: 13051\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 19039\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0006, weighted_loss: 0.0003, label: 0, bag_size: 14206\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0025, weighted_loss: 0.0008, label: 1, bag_size: 11195\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0003, weighted_loss: 0.0002, label: 0, bag_size: 3557\n",
      "batch 619, loss: 0.0009, instance_loss: 0.0007, weighted_loss: 0.0008, label: 0, bag_size: 18954\n",
      "batch 639, loss: 0.0861, instance_loss: 0.0890, weighted_loss: 0.0870, label: 0, bag_size: 14625\n",
      "batch 659, loss: 0.0115, instance_loss: 0.0035, weighted_loss: 0.0091, label: 0, bag_size: 3321\n",
      "batch 679, loss: 0.0011, instance_loss: 0.0008, weighted_loss: 0.0010, label: 1, bag_size: 16565\n",
      "batch 699, loss: 0.0131, instance_loss: 0.0120, weighted_loss: 0.0128, label: 0, bag_size: 14264\n",
      "batch 719, loss: 0.4824, instance_loss: 0.5934, weighted_loss: 0.5157, label: 0, bag_size: 13992\n",
      "batch 739, loss: 0.0001, instance_loss: 0.0024, weighted_loss: 0.0008, label: 0, bag_size: 9234\n",
      "batch 759, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 10481\n",
      "batch 779, loss: 0.0436, instance_loss: 0.0281, weighted_loss: 0.0390, label: 1, bag_size: 5864\n",
      "batch 799, loss: 0.0320, instance_loss: 0.0138, weighted_loss: 0.0266, label: 0, bag_size: 11527\n",
      "batch 819, loss: 0.0065, instance_loss: 0.0055, weighted_loss: 0.0062, label: 0, bag_size: 9234\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9786585365853658: correct 12840/13120\n",
      "class 1 clustering acc 0.8891768292682927: correct 5833/6560\n",
      "Epoch: 33, train_loss: 0.1750, train_clustering_loss:  0.2022, train_error: 0.0646\n",
      "class 0: acc 0.9363207547169812, correct 397/424\n",
      "class 1: acc 0.9343434343434344, correct 370/396\n",
      "\n",
      "Val Set, val_loss: 0.2160, val_error: 0.1182, auc: 0.9745\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8103448275862069, correct 47/58\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0013, instance_loss: 0.0022, weighted_loss: 0.0015, label: 1, bag_size: 645\n",
      "batch 39, loss: 0.0351, instance_loss: 0.0230, weighted_loss: 0.0315, label: 0, bag_size: 6367\n",
      "batch 59, loss: 0.0463, instance_loss: 0.0407, weighted_loss: 0.0446, label: 0, bag_size: 2382\n",
      "batch 79, loss: 0.1525, instance_loss: 0.1282, weighted_loss: 0.1452, label: 0, bag_size: 10381\n",
      "batch 99, loss: 0.3746, instance_loss: 0.4237, weighted_loss: 0.3893, label: 0, bag_size: 3375\n",
      "batch 119, loss: 0.0012, instance_loss: 0.0013, weighted_loss: 0.0012, label: 0, bag_size: 30751\n",
      "batch 139, loss: 0.0046, instance_loss: 0.0060, weighted_loss: 0.0050, label: 0, bag_size: 15841\n",
      "batch 159, loss: 0.0107, instance_loss: 0.0138, weighted_loss: 0.0117, label: 1, bag_size: 8003\n",
      "batch 179, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 11607\n",
      "batch 199, loss: 0.0075, instance_loss: 0.0171, weighted_loss: 0.0104, label: 1, bag_size: 2137\n",
      "batch 219, loss: 0.0004, instance_loss: 0.0003, weighted_loss: 0.0004, label: 0, bag_size: 8661\n",
      "batch 239, loss: 0.0023, instance_loss: 0.0111, weighted_loss: 0.0049, label: 1, bag_size: 15332\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0017, weighted_loss: 0.0005, label: 0, bag_size: 5551\n",
      "batch 279, loss: 0.8294, instance_loss: 1.5219, weighted_loss: 1.0371, label: 1, bag_size: 898\n",
      "batch 299, loss: 0.0007, instance_loss: 0.0004, weighted_loss: 0.0006, label: 1, bag_size: 6745\n",
      "batch 319, loss: 0.0011, instance_loss: 0.0069, weighted_loss: 0.0028, label: 1, bag_size: 8982\n",
      "batch 339, loss: 0.0171, instance_loss: 0.0079, weighted_loss: 0.0143, label: 1, bag_size: 8019\n",
      "batch 359, loss: 0.0065, instance_loss: 0.0087, weighted_loss: 0.0071, label: 1, bag_size: 9689\n",
      "batch 379, loss: 0.0206, instance_loss: 0.0785, weighted_loss: 0.0380, label: 1, bag_size: 2344\n",
      "batch 399, loss: 0.1485, instance_loss: 0.1120, weighted_loss: 0.1376, label: 0, bag_size: 2004\n",
      "batch 419, loss: 0.1627, instance_loss: 0.2627, weighted_loss: 0.1927, label: 1, bag_size: 1038\n",
      "batch 439, loss: 0.0017, instance_loss: 0.0030, weighted_loss: 0.0021, label: 1, bag_size: 2356\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 19518\n",
      "batch 479, loss: 0.0032, instance_loss: 0.0019, weighted_loss: 0.0028, label: 1, bag_size: 8660\n",
      "batch 499, loss: 0.0045, instance_loss: 0.0118, weighted_loss: 0.0067, label: 0, bag_size: 705\n",
      "batch 519, loss: 0.0035, instance_loss: 0.0068, weighted_loss: 0.0045, label: 0, bag_size: 12731\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0144, weighted_loss: 0.0044, label: 1, bag_size: 10920\n",
      "batch 559, loss: 0.1569, instance_loss: 0.5052, weighted_loss: 0.2614, label: 1, bag_size: 1244\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 19043\n",
      "batch 599, loss: 0.0002, instance_loss: 0.1794, weighted_loss: 0.0540, label: 1, bag_size: 14515\n",
      "batch 619, loss: 0.0730, instance_loss: 0.0333, weighted_loss: 0.0611, label: 1, bag_size: 11256\n",
      "batch 639, loss: 0.0023, instance_loss: 0.0009, weighted_loss: 0.0019, label: 0, bag_size: 10365\n",
      "batch 659, loss: 0.1993, instance_loss: 0.3409, weighted_loss: 0.2418, label: 1, bag_size: 10432\n",
      "batch 679, loss: 0.6284, instance_loss: 0.1857, weighted_loss: 0.4956, label: 1, bag_size: 12712\n",
      "batch 699, loss: 0.0001, instance_loss: 0.0018, weighted_loss: 0.0006, label: 0, bag_size: 21404\n",
      "batch 719, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 0, bag_size: 2457\n",
      "batch 739, loss: 0.1182, instance_loss: 0.2333, weighted_loss: 0.1527, label: 1, bag_size: 6090\n",
      "batch 759, loss: 0.0077, instance_loss: 0.0209, weighted_loss: 0.0117, label: 1, bag_size: 9321\n",
      "batch 779, loss: 0.0092, instance_loss: 0.0092, weighted_loss: 0.0092, label: 0, bag_size: 3908\n",
      "batch 799, loss: 1.0440, instance_loss: 2.9212, weighted_loss: 1.6072, label: 0, bag_size: 3802\n",
      "batch 819, loss: 0.0672, instance_loss: 0.0716, weighted_loss: 0.0685, label: 1, bag_size: 6736\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9734756097560976: correct 12772/13120\n",
      "class 1 clustering acc 0.8788109756097561: correct 5765/6560\n",
      "Epoch: 34, train_loss: 0.1941, train_clustering_loss:  0.2453, train_error: 0.0744\n",
      "class 0: acc 0.9195121951219513, correct 377/410\n",
      "class 1: acc 0.9317073170731708, correct 382/410\n",
      "\n",
      "Val Set, val_loss: 0.5959, val_error: 0.2000, auc: 0.9549\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.00909090909090909: correct 8/880\n",
      "class 0: acc 0.5769230769230769, correct 30/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0675, instance_loss: 0.0650, weighted_loss: 0.0668, label: 0, bag_size: 2242\n",
      "batch 39, loss: 0.0123, instance_loss: 0.0060, weighted_loss: 0.0104, label: 1, bag_size: 9446\n",
      "batch 59, loss: 0.0123, instance_loss: 0.0118, weighted_loss: 0.0122, label: 1, bag_size: 7798\n",
      "batch 79, loss: 0.1995, instance_loss: 0.2198, weighted_loss: 0.2056, label: 0, bag_size: 21361\n",
      "batch 99, loss: 0.0081, instance_loss: 0.0078, weighted_loss: 0.0080, label: 1, bag_size: 2785\n",
      "batch 119, loss: 0.0375, instance_loss: 0.0233, weighted_loss: 0.0332, label: 1, bag_size: 3651\n",
      "batch 139, loss: 0.0097, instance_loss: 0.0078, weighted_loss: 0.0091, label: 1, bag_size: 14433\n",
      "batch 159, loss: 0.2883, instance_loss: 0.3050, weighted_loss: 0.2933, label: 1, bag_size: 1294\n",
      "batch 179, loss: 2.0248, instance_loss: 2.5217, weighted_loss: 2.1739, label: 1, bag_size: 21252\n",
      "batch 199, loss: 0.0077, instance_loss: 0.0200, weighted_loss: 0.0114, label: 1, bag_size: 1101\n",
      "batch 219, loss: 0.0094, instance_loss: 0.0050, weighted_loss: 0.0080, label: 0, bag_size: 23714\n",
      "batch 239, loss: 0.4005, instance_loss: 0.4499, weighted_loss: 0.4154, label: 1, bag_size: 7669\n",
      "batch 259, loss: 0.0019, instance_loss: 0.0020, weighted_loss: 0.0019, label: 1, bag_size: 1919\n",
      "batch 279, loss: 0.0011, instance_loss: 0.0007, weighted_loss: 0.0010, label: 1, bag_size: 10392\n",
      "batch 299, loss: 0.0005, instance_loss: 0.0070, weighted_loss: 0.0025, label: 0, bag_size: 16211\n",
      "batch 319, loss: 0.0034, instance_loss: 0.0092, weighted_loss: 0.0051, label: 0, bag_size: 16690\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0035, weighted_loss: 0.0011, label: 0, bag_size: 3787\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 10898\n",
      "batch 379, loss: 0.0244, instance_loss: 0.0106, weighted_loss: 0.0202, label: 1, bag_size: 1244\n",
      "batch 399, loss: 2.1617, instance_loss: 2.7126, weighted_loss: 2.3270, label: 0, bag_size: 24382\n",
      "batch 419, loss: 0.0011, instance_loss: 0.0008, weighted_loss: 0.0010, label: 0, bag_size: 1881\n",
      "batch 439, loss: 0.0021, instance_loss: 0.0018, weighted_loss: 0.0020, label: 0, bag_size: 2091\n",
      "batch 459, loss: 0.0725, instance_loss: 0.0646, weighted_loss: 0.0702, label: 0, bag_size: 21361\n",
      "batch 479, loss: 0.0007, instance_loss: 0.0235, weighted_loss: 0.0076, label: 1, bag_size: 11032\n",
      "batch 499, loss: 0.0175, instance_loss: 0.0137, weighted_loss: 0.0164, label: 1, bag_size: 7424\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0109, weighted_loss: 0.0033, label: 1, bag_size: 4250\n",
      "batch 539, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 1213\n",
      "batch 559, loss: 0.5180, instance_loss: 0.6498, weighted_loss: 0.5575, label: 1, bag_size: 5723\n",
      "batch 579, loss: 0.0317, instance_loss: 0.0093, weighted_loss: 0.0250, label: 1, bag_size: 7371\n",
      "batch 599, loss: 0.0006, instance_loss: 0.0001, weighted_loss: 0.0005, label: 1, bag_size: 1244\n",
      "batch 619, loss: 0.0003, instance_loss: 0.0015, weighted_loss: 0.0007, label: 0, bag_size: 7011\n",
      "batch 639, loss: 0.2085, instance_loss: 0.2363, weighted_loss: 0.2169, label: 0, bag_size: 2628\n",
      "batch 659, loss: 0.0055, instance_loss: 0.0036, weighted_loss: 0.0049, label: 1, bag_size: 8685\n",
      "batch 679, loss: 0.3633, instance_loss: 0.4905, weighted_loss: 0.4015, label: 1, bag_size: 1867\n",
      "batch 699, loss: 0.0003, instance_loss: 0.0004, weighted_loss: 0.0003, label: 1, bag_size: 10501\n",
      "batch 719, loss: 3.1187, instance_loss: 3.6959, weighted_loss: 3.2919, label: 1, bag_size: 684\n",
      "batch 739, loss: 0.0018, instance_loss: 0.0017, weighted_loss: 0.0018, label: 0, bag_size: 3787\n",
      "batch 759, loss: 0.0045, instance_loss: 0.0034, weighted_loss: 0.0042, label: 0, bag_size: 23796\n",
      "batch 779, loss: 0.0058, instance_loss: 0.0068, weighted_loss: 0.0061, label: 1, bag_size: 1622\n",
      "batch 799, loss: 0.0053, instance_loss: 0.0061, weighted_loss: 0.0055, label: 0, bag_size: 10068\n",
      "batch 819, loss: 0.0002, instance_loss: 0.0005, weighted_loss: 0.0003, label: 1, bag_size: 7119\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.979420731707317: correct 12850/13120\n",
      "class 1 clustering acc 0.8824695121951219: correct 5789/6560\n",
      "Epoch: 35, train_loss: 0.1684, train_clustering_loss:  0.2002, train_error: 0.0780\n",
      "class 0: acc 0.9119804400977995, correct 373/409\n",
      "class 1: acc 0.9318734793187348, correct 383/411\n",
      "\n",
      "Val Set, val_loss: 0.2085, val_error: 0.0727, auc: 0.9735\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0048, instance_loss: 0.0039, weighted_loss: 0.0045, label: 0, bag_size: 8812\n",
      "batch 39, loss: 0.0089, instance_loss: 0.0039, weighted_loss: 0.0074, label: 1, bag_size: 11394\n",
      "batch 59, loss: 0.0001, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 16051\n",
      "batch 79, loss: 0.0336, instance_loss: 0.0161, weighted_loss: 0.0283, label: 1, bag_size: 9322\n",
      "batch 99, loss: 0.2333, instance_loss: 0.3461, weighted_loss: 0.2671, label: 1, bag_size: 6478\n",
      "batch 119, loss: 0.2580, instance_loss: 0.2763, weighted_loss: 0.2635, label: 1, bag_size: 983\n",
      "batch 139, loss: 0.3647, instance_loss: 0.5238, weighted_loss: 0.4125, label: 0, bag_size: 9132\n",
      "batch 159, loss: 0.1344, instance_loss: 0.1416, weighted_loss: 0.1366, label: 0, bag_size: 8420\n",
      "batch 179, loss: 0.0049, instance_loss: 0.0031, weighted_loss: 0.0043, label: 0, bag_size: 7381\n",
      "batch 199, loss: 0.0075, instance_loss: 0.0045, weighted_loss: 0.0066, label: 0, bag_size: 16607\n",
      "batch 219, loss: 0.0084, instance_loss: 0.0084, weighted_loss: 0.0084, label: 0, bag_size: 16052\n",
      "batch 239, loss: 0.2629, instance_loss: 0.3524, weighted_loss: 0.2898, label: 1, bag_size: 12494\n",
      "batch 259, loss: 0.0057, instance_loss: 0.0040, weighted_loss: 0.0052, label: 0, bag_size: 19043\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 65728\n",
      "batch 299, loss: 0.0088, instance_loss: 0.0123, weighted_loss: 0.0099, label: 1, bag_size: 3368\n",
      "batch 319, loss: 0.1033, instance_loss: 0.0932, weighted_loss: 0.1003, label: 0, bag_size: 12840\n",
      "batch 339, loss: 0.1539, instance_loss: 0.1267, weighted_loss: 0.1457, label: 0, bag_size: 15672\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0005, weighted_loss: 0.0002, label: 1, bag_size: 10920\n",
      "batch 379, loss: 0.0085, instance_loss: 0.0177, weighted_loss: 0.0113, label: 1, bag_size: 7110\n",
      "batch 399, loss: 0.0405, instance_loss: 0.0399, weighted_loss: 0.0403, label: 0, bag_size: 9433\n",
      "batch 419, loss: 0.0058, instance_loss: 0.0035, weighted_loss: 0.0051, label: 1, bag_size: 7798\n",
      "batch 439, loss: 0.0008, instance_loss: 0.0003, weighted_loss: 0.0007, label: 1, bag_size: 10281\n",
      "batch 459, loss: 0.2633, instance_loss: 0.2781, weighted_loss: 0.2678, label: 0, bag_size: 11607\n",
      "batch 479, loss: 0.0505, instance_loss: 0.0231, weighted_loss: 0.0422, label: 0, bag_size: 1149\n",
      "batch 499, loss: 0.0019, instance_loss: 0.0046, weighted_loss: 0.0027, label: 0, bag_size: 31106\n",
      "batch 519, loss: 0.0080, instance_loss: 0.0082, weighted_loss: 0.0081, label: 0, bag_size: 1920\n",
      "batch 539, loss: 0.4189, instance_loss: 0.3240, weighted_loss: 0.3904, label: 0, bag_size: 12910\n",
      "batch 559, loss: 0.0017, instance_loss: 0.0007, weighted_loss: 0.0014, label: 1, bag_size: 12931\n",
      "batch 579, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 9060\n",
      "batch 599, loss: 0.0861, instance_loss: 0.0798, weighted_loss: 0.0842, label: 0, bag_size: 9387\n",
      "batch 619, loss: 0.0020, instance_loss: 0.0056, weighted_loss: 0.0031, label: 0, bag_size: 1588\n",
      "batch 639, loss: 0.1981, instance_loss: 0.2520, weighted_loss: 0.2143, label: 0, bag_size: 4418\n",
      "batch 659, loss: 0.0135, instance_loss: 0.0095, weighted_loss: 0.0123, label: 1, bag_size: 11701\n",
      "batch 679, loss: 0.3397, instance_loss: 0.4314, weighted_loss: 0.3672, label: 1, bag_size: 5903\n",
      "batch 699, loss: 0.5942, instance_loss: 0.7745, weighted_loss: 0.6483, label: 1, bag_size: 1493\n",
      "batch 719, loss: 0.0029, instance_loss: 0.0017, weighted_loss: 0.0026, label: 0, bag_size: 9470\n",
      "batch 739, loss: 0.0075, instance_loss: 0.0087, weighted_loss: 0.0078, label: 0, bag_size: 2732\n",
      "batch 759, loss: 0.0082, instance_loss: 0.0024, weighted_loss: 0.0065, label: 0, bag_size: 14377\n",
      "batch 779, loss: 0.0003, instance_loss: 0.0008, weighted_loss: 0.0004, label: 1, bag_size: 5221\n",
      "batch 799, loss: 0.4389, instance_loss: 0.5454, weighted_loss: 0.4708, label: 0, bag_size: 1498\n",
      "batch 819, loss: 0.0003, instance_loss: 0.0028, weighted_loss: 0.0010, label: 1, bag_size: 4877\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9770579268292683: correct 12819/13120\n",
      "class 1 clustering acc 0.8748475609756098: correct 5739/6560\n",
      "Epoch: 36, train_loss: 0.2007, train_clustering_loss:  0.2294, train_error: 0.0805\n",
      "class 0: acc 0.9219512195121952, correct 378/410\n",
      "class 1: acc 0.9170731707317074, correct 376/410\n",
      "\n",
      "Val Set, val_loss: 0.2246, val_error: 0.1000, auc: 0.9708\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1208, instance_loss: 0.1009, weighted_loss: 0.1148, label: 1, bag_size: 14887\n",
      "batch 39, loss: 0.0018, instance_loss: 0.0003, weighted_loss: 0.0014, label: 1, bag_size: 7650\n",
      "batch 59, loss: 0.6114, instance_loss: 0.8166, weighted_loss: 0.6729, label: 1, bag_size: 2937\n",
      "batch 79, loss: 0.0749, instance_loss: 0.0746, weighted_loss: 0.0748, label: 1, bag_size: 1015\n",
      "batch 99, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 8372\n",
      "batch 119, loss: 0.0002, instance_loss: 0.0104, weighted_loss: 0.0032, label: 0, bag_size: 14266\n",
      "batch 139, loss: 0.0006, instance_loss: 0.0014, weighted_loss: 0.0008, label: 1, bag_size: 11884\n",
      "batch 159, loss: 0.0296, instance_loss: 0.0314, weighted_loss: 0.0302, label: 0, bag_size: 8582\n",
      "batch 179, loss: 0.0005, instance_loss: 0.0005, weighted_loss: 0.0005, label: 0, bag_size: 10128\n",
      "batch 199, loss: 0.1655, instance_loss: 0.1880, weighted_loss: 0.1722, label: 0, bag_size: 2296\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0003, label: 0, bag_size: 11383\n",
      "batch 239, loss: 0.2719, instance_loss: 0.2383, weighted_loss: 0.2618, label: 1, bag_size: 4929\n",
      "batch 259, loss: 0.3382, instance_loss: 0.4442, weighted_loss: 0.3700, label: 0, bag_size: 1483\n",
      "batch 279, loss: 0.0022, instance_loss: 0.0016, weighted_loss: 0.0020, label: 0, bag_size: 2654\n",
      "batch 299, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 6898\n",
      "batch 319, loss: 0.0022, instance_loss: 0.0009, weighted_loss: 0.0018, label: 1, bag_size: 13786\n",
      "batch 339, loss: 0.3733, instance_loss: 0.3860, weighted_loss: 0.3771, label: 0, bag_size: 7428\n",
      "batch 359, loss: 0.4657, instance_loss: 0.5529, weighted_loss: 0.4918, label: 0, bag_size: 2367\n",
      "batch 379, loss: 3.0714, instance_loss: 3.5955, weighted_loss: 3.2286, label: 0, bag_size: 2815\n",
      "batch 399, loss: 0.0032, instance_loss: 0.0045, weighted_loss: 0.0036, label: 0, bag_size: 3190\n",
      "batch 419, loss: 0.0646, instance_loss: 0.0397, weighted_loss: 0.0571, label: 1, bag_size: 4039\n",
      "batch 439, loss: 0.3305, instance_loss: 0.3645, weighted_loss: 0.3407, label: 0, bag_size: 12840\n",
      "batch 459, loss: 1.9774, instance_loss: 2.4849, weighted_loss: 2.1297, label: 0, bag_size: 7428\n",
      "batch 479, loss: 0.0002, instance_loss: 0.0008, weighted_loss: 0.0004, label: 0, bag_size: 15313\n",
      "batch 499, loss: 0.0008, instance_loss: 0.0003, weighted_loss: 0.0006, label: 0, bag_size: 18045\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 18738\n",
      "batch 539, loss: 0.1723, instance_loss: 0.1681, weighted_loss: 0.1711, label: 0, bag_size: 17791\n",
      "batch 559, loss: 0.0031, instance_loss: 0.0027, weighted_loss: 0.0030, label: 1, bag_size: 15213\n",
      "batch 579, loss: 0.0052, instance_loss: 0.0029, weighted_loss: 0.0045, label: 1, bag_size: 928\n",
      "batch 599, loss: 0.0169, instance_loss: 0.0089, weighted_loss: 0.0145, label: 0, bag_size: 16720\n",
      "batch 619, loss: 1.5172, instance_loss: 2.0281, weighted_loss: 1.6704, label: 1, bag_size: 2395\n",
      "batch 639, loss: 0.2791, instance_loss: 0.3736, weighted_loss: 0.3075, label: 0, bag_size: 1690\n",
      "batch 659, loss: 0.0063, instance_loss: 0.0070, weighted_loss: 0.0065, label: 1, bag_size: 5025\n",
      "batch 679, loss: 0.0224, instance_loss: 0.0120, weighted_loss: 0.0193, label: 0, bag_size: 12148\n",
      "batch 699, loss: 0.5710, instance_loss: 0.7728, weighted_loss: 0.6315, label: 0, bag_size: 1760\n",
      "batch 719, loss: 0.0114, instance_loss: 0.0077, weighted_loss: 0.0103, label: 1, bag_size: 8191\n",
      "batch 739, loss: 0.0062, instance_loss: 0.0070, weighted_loss: 0.0064, label: 1, bag_size: 19606\n",
      "batch 759, loss: 0.0006, instance_loss: 0.0011, weighted_loss: 0.0007, label: 0, bag_size: 1962\n",
      "batch 779, loss: 0.0040, instance_loss: 0.0128, weighted_loss: 0.0066, label: 1, bag_size: 7513\n",
      "batch 799, loss: 0.1239, instance_loss: 0.1298, weighted_loss: 0.1257, label: 0, bag_size: 2382\n",
      "batch 819, loss: 0.0031, instance_loss: 0.0037, weighted_loss: 0.0032, label: 0, bag_size: 3970\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9772865853658537: correct 12822/13120\n",
      "class 1 clustering acc 0.873780487804878: correct 5732/6560\n",
      "Epoch: 37, train_loss: 0.1880, train_clustering_loss:  0.2187, train_error: 0.0805\n",
      "class 0: acc 0.9213759213759214, correct 375/407\n",
      "class 1: acc 0.9176755447941889, correct 379/413\n",
      "\n",
      "Val Set, val_loss: 0.2085, val_error: 0.1000, auc: 0.9771\n",
      "class 0 clustering acc 0.990909090909091: correct 1744/1760\n",
      "class 1 clustering acc 0.03636363636363636: correct 32/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8448275862068966, correct 49/58\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6107, instance_loss: 0.6885, weighted_loss: 0.6340, label: 0, bag_size: 2070\n",
      "batch 39, loss: 0.0511, instance_loss: 0.0525, weighted_loss: 0.0515, label: 0, bag_size: 9132\n",
      "batch 59, loss: 0.0012, instance_loss: 0.0018, weighted_loss: 0.0014, label: 1, bag_size: 5340\n",
      "batch 79, loss: 0.0229, instance_loss: 0.0186, weighted_loss: 0.0216, label: 1, bag_size: 8680\n",
      "batch 99, loss: 0.0175, instance_loss: 0.0130, weighted_loss: 0.0161, label: 0, bag_size: 13205\n",
      "batch 119, loss: 0.0006, instance_loss: 0.0009, weighted_loss: 0.0007, label: 0, bag_size: 4465\n",
      "batch 139, loss: 0.0091, instance_loss: 0.0073, weighted_loss: 0.0085, label: 0, bag_size: 1415\n",
      "batch 159, loss: 0.3601, instance_loss: 0.3346, weighted_loss: 0.3525, label: 1, bag_size: 983\n",
      "batch 179, loss: 1.1152, instance_loss: 1.1117, weighted_loss: 1.1142, label: 1, bag_size: 3652\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 5317\n",
      "batch 219, loss: 0.0005, instance_loss: 0.0009, weighted_loss: 0.0006, label: 0, bag_size: 2303\n",
      "batch 239, loss: 0.0346, instance_loss: 0.0254, weighted_loss: 0.0318, label: 0, bag_size: 9542\n",
      "batch 259, loss: 0.0055, instance_loss: 0.0017, weighted_loss: 0.0044, label: 0, bag_size: 2195\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0057, weighted_loss: 0.0018, label: 1, bag_size: 15233\n",
      "batch 299, loss: 0.0261, instance_loss: 0.0814, weighted_loss: 0.0427, label: 1, bag_size: 8982\n",
      "batch 319, loss: 0.0096, instance_loss: 0.0034, weighted_loss: 0.0077, label: 1, bag_size: 14515\n",
      "batch 339, loss: 0.0724, instance_loss: 0.0561, weighted_loss: 0.0675, label: 0, bag_size: 2004\n",
      "batch 359, loss: 0.3774, instance_loss: 0.3715, weighted_loss: 0.3756, label: 1, bag_size: 5903\n",
      "batch 379, loss: 0.0434, instance_loss: 0.0404, weighted_loss: 0.0425, label: 0, bag_size: 10942\n",
      "batch 399, loss: 0.0026, instance_loss: 0.0021, weighted_loss: 0.0024, label: 1, bag_size: 9878\n",
      "batch 419, loss: 0.0841, instance_loss: 0.0883, weighted_loss: 0.0853, label: 1, bag_size: 1759\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 6792\n",
      "batch 459, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 1838\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11125\n",
      "batch 499, loss: 0.0009, instance_loss: 0.0004, weighted_loss: 0.0008, label: 1, bag_size: 12931\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 0, bag_size: 15747\n",
      "batch 539, loss: 0.0004, instance_loss: 0.0003, weighted_loss: 0.0004, label: 0, bag_size: 15967\n",
      "batch 559, loss: 0.0562, instance_loss: 0.0377, weighted_loss: 0.0506, label: 1, bag_size: 19832\n",
      "batch 579, loss: 0.0224, instance_loss: 0.0117, weighted_loss: 0.0192, label: 0, bag_size: 22498\n",
      "batch 599, loss: 0.0164, instance_loss: 0.0244, weighted_loss: 0.0188, label: 1, bag_size: 10396\n",
      "batch 619, loss: 0.0008, instance_loss: 0.0013, weighted_loss: 0.0010, label: 1, bag_size: 11875\n",
      "batch 639, loss: 0.0080, instance_loss: 0.0113, weighted_loss: 0.0090, label: 1, bag_size: 1638\n",
      "batch 659, loss: 0.0930, instance_loss: 0.0935, weighted_loss: 0.0932, label: 0, bag_size: 12083\n",
      "batch 679, loss: 1.6171, instance_loss: 1.8343, weighted_loss: 1.6823, label: 0, bag_size: 13332\n",
      "batch 699, loss: 1.7563, instance_loss: 1.9556, weighted_loss: 1.8161, label: 1, bag_size: 2731\n",
      "batch 719, loss: 0.0820, instance_loss: 0.0802, weighted_loss: 0.0815, label: 1, bag_size: 2678\n",
      "batch 739, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 8252\n",
      "batch 759, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 13225\n",
      "batch 779, loss: 0.0214, instance_loss: 0.0210, weighted_loss: 0.0213, label: 0, bag_size: 12793\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0043, weighted_loss: 0.0013, label: 1, bag_size: 10392\n",
      "batch 819, loss: 3.7198, instance_loss: 4.1700, weighted_loss: 3.8548, label: 1, bag_size: 4786\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9811737804878049: correct 12873/13120\n",
      "class 1 clustering acc 0.9051829268292683: correct 5938/6560\n",
      "Epoch: 38, train_loss: 0.1554, train_clustering_loss:  0.1870, train_error: 0.0573\n",
      "class 0: acc 0.9384236453201971, correct 381/406\n",
      "class 1: acc 0.9468599033816425, correct 392/414\n",
      "\n",
      "Val Set, val_loss: 0.7651, val_error: 0.2000, auc: 0.9738\n",
      "class 0 clustering acc 0.9880681818181818: correct 1739/1760\n",
      "class 1 clustering acc 0.03636363636363636: correct 32/880\n",
      "class 0: acc 1.0, correct 52/52\n",
      "class 1: acc 0.6206896551724138, correct 36/58\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1402, instance_loss: 0.0995, weighted_loss: 0.1280, label: 0, bag_size: 19470\n",
      "batch 39, loss: 0.0059, instance_loss: 0.0030, weighted_loss: 0.0051, label: 1, bag_size: 1255\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0071, weighted_loss: 0.0022, label: 0, bag_size: 3725\n",
      "batch 79, loss: 0.0018, instance_loss: 0.0010, weighted_loss: 0.0016, label: 0, bag_size: 19043\n",
      "batch 99, loss: 2.5979, instance_loss: 3.1001, weighted_loss: 2.7486, label: 1, bag_size: 1533\n",
      "batch 119, loss: 0.0295, instance_loss: 0.0314, weighted_loss: 0.0301, label: 0, bag_size: 19808\n",
      "batch 139, loss: 0.0043, instance_loss: 0.0034, weighted_loss: 0.0040, label: 0, bag_size: 7637\n",
      "batch 159, loss: 0.0123, instance_loss: 0.0094, weighted_loss: 0.0114, label: 0, bag_size: 19470\n",
      "batch 179, loss: 0.0016, instance_loss: 0.0006, weighted_loss: 0.0013, label: 1, bag_size: 4394\n",
      "batch 199, loss: 0.0089, instance_loss: 0.0105, weighted_loss: 0.0094, label: 0, bag_size: 2195\n",
      "batch 219, loss: 0.0158, instance_loss: 0.0134, weighted_loss: 0.0150, label: 0, bag_size: 22681\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0266, weighted_loss: 0.0080, label: 1, bag_size: 2385\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 1, bag_size: 8868\n",
      "batch 279, loss: 0.0018, instance_loss: 0.0008, weighted_loss: 0.0015, label: 0, bag_size: 2367\n",
      "batch 299, loss: 0.0027, instance_loss: 0.0010, weighted_loss: 0.0022, label: 0, bag_size: 8025\n",
      "batch 319, loss: 0.0002, instance_loss: 0.0128, weighted_loss: 0.0040, label: 0, bag_size: 7011\n",
      "batch 339, loss: 0.0002, instance_loss: 0.0002, weighted_loss: 0.0002, label: 1, bag_size: 10028\n",
      "batch 359, loss: 0.0014, instance_loss: 0.0008, weighted_loss: 0.0012, label: 0, bag_size: 3502\n",
      "batch 379, loss: 0.0026, instance_loss: 0.0025, weighted_loss: 0.0026, label: 1, bag_size: 10867\n",
      "batch 399, loss: 0.0004, instance_loss: 0.0016, weighted_loss: 0.0008, label: 0, bag_size: 19472\n",
      "batch 419, loss: 0.0012, instance_loss: 0.0004, weighted_loss: 0.0009, label: 1, bag_size: 11518\n",
      "batch 439, loss: 0.0091, instance_loss: 0.0037, weighted_loss: 0.0075, label: 0, bag_size: 12793\n",
      "batch 459, loss: 0.1224, instance_loss: 0.1404, weighted_loss: 0.1278, label: 0, bag_size: 3502\n",
      "batch 479, loss: 0.0002, instance_loss: 0.0014, weighted_loss: 0.0005, label: 0, bag_size: 25814\n",
      "batch 499, loss: 0.0009, instance_loss: 0.0007, weighted_loss: 0.0008, label: 1, bag_size: 10112\n",
      "batch 519, loss: 0.1703, instance_loss: 0.1534, weighted_loss: 0.1653, label: 0, bag_size: 10113\n",
      "batch 539, loss: 0.2074, instance_loss: 0.2555, weighted_loss: 0.2218, label: 1, bag_size: 1746\n",
      "batch 559, loss: 2.1203, instance_loss: 2.5938, weighted_loss: 2.2623, label: 0, bag_size: 2160\n",
      "batch 579, loss: 0.0024, instance_loss: 0.0001, weighted_loss: 0.0017, label: 0, bag_size: 23796\n",
      "batch 599, loss: 0.6957, instance_loss: 0.8659, weighted_loss: 0.7468, label: 1, bag_size: 2937\n",
      "batch 619, loss: 0.0067, instance_loss: 0.0032, weighted_loss: 0.0056, label: 1, bag_size: 10492\n",
      "batch 639, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 19472\n",
      "batch 659, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 7637\n",
      "batch 679, loss: 0.0004, instance_loss: 0.0005, weighted_loss: 0.0004, label: 1, bag_size: 3224\n",
      "batch 699, loss: 0.0069, instance_loss: 0.0053, weighted_loss: 0.0065, label: 1, bag_size: 4821\n",
      "batch 719, loss: 0.1535, instance_loss: 0.1336, weighted_loss: 0.1475, label: 0, bag_size: 2242\n",
      "batch 739, loss: 0.4196, instance_loss: 0.4796, weighted_loss: 0.4376, label: 1, bag_size: 7351\n",
      "batch 759, loss: 0.0037, instance_loss: 0.0024, weighted_loss: 0.0033, label: 1, bag_size: 8448\n",
      "batch 779, loss: 0.0046, instance_loss: 0.0021, weighted_loss: 0.0039, label: 1, bag_size: 11518\n",
      "batch 799, loss: 0.0909, instance_loss: 0.0552, weighted_loss: 0.0802, label: 0, bag_size: 6281\n",
      "batch 819, loss: 0.0003, instance_loss: 0.0002, weighted_loss: 0.0002, label: 0, bag_size: 11735\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9814024390243903: correct 12876/13120\n",
      "class 1 clustering acc 0.8989329268292683: correct 5897/6560\n",
      "Epoch: 39, train_loss: 0.1644, train_clustering_loss:  0.1925, train_error: 0.0646\n",
      "class 0: acc 0.9411764705882353, correct 384/408\n",
      "class 1: acc 0.9296116504854369, correct 383/412\n",
      "\n",
      "Val Set, val_loss: 0.1860, val_error: 0.0818, auc: 0.9781\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "Validation loss decreased (0.199585 --> 0.186012).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5199, instance_loss: 0.6654, weighted_loss: 0.5635, label: 1, bag_size: 9162\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 11032\n",
      "batch 59, loss: 0.0031, instance_loss: 0.0021, weighted_loss: 0.0028, label: 0, bag_size: 23714\n",
      "batch 79, loss: 0.0699, instance_loss: 0.0413, weighted_loss: 0.0613, label: 1, bag_size: 2137\n",
      "batch 99, loss: 0.0030, instance_loss: 0.0060, weighted_loss: 0.0039, label: 0, bag_size: 8252\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 0, bag_size: 2844\n",
      "batch 139, loss: 0.4582, instance_loss: 0.4548, weighted_loss: 0.4572, label: 1, bag_size: 3652\n",
      "batch 159, loss: 3.3832, instance_loss: 4.2763, weighted_loss: 3.6511, label: 0, bag_size: 2290\n",
      "batch 179, loss: 0.0023, instance_loss: 0.0040, weighted_loss: 0.0028, label: 1, bag_size: 8019\n",
      "batch 199, loss: 0.2983, instance_loss: 0.2681, weighted_loss: 0.2892, label: 1, bag_size: 2682\n",
      "batch 219, loss: 0.0003, instance_loss: 0.0013, weighted_loss: 0.0006, label: 0, bag_size: 10898\n",
      "batch 239, loss: 0.0839, instance_loss: 0.0813, weighted_loss: 0.0831, label: 0, bag_size: 3557\n",
      "batch 259, loss: 0.0122, instance_loss: 0.0098, weighted_loss: 0.0115, label: 1, bag_size: 15689\n",
      "batch 279, loss: 0.0533, instance_loss: 0.1553, weighted_loss: 0.0839, label: 1, bag_size: 2137\n",
      "batch 299, loss: 0.1197, instance_loss: 0.1231, weighted_loss: 0.1207, label: 0, bag_size: 1508\n",
      "batch 319, loss: 0.0626, instance_loss: 0.0384, weighted_loss: 0.0553, label: 1, bag_size: 2137\n",
      "batch 339, loss: 0.0471, instance_loss: 0.0367, weighted_loss: 0.0440, label: 0, bag_size: 4598\n",
      "batch 359, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 12460\n",
      "batch 379, loss: 0.1976, instance_loss: 0.2233, weighted_loss: 0.2053, label: 1, bag_size: 1819\n",
      "batch 399, loss: 0.0165, instance_loss: 0.0087, weighted_loss: 0.0142, label: 0, bag_size: 9171\n",
      "batch 419, loss: 0.8033, instance_loss: 1.1388, weighted_loss: 0.9039, label: 0, bag_size: 2918\n",
      "batch 439, loss: 0.0004, instance_loss: 0.0054, weighted_loss: 0.0019, label: 1, bag_size: 11389\n",
      "batch 459, loss: 0.0333, instance_loss: 0.0256, weighted_loss: 0.0310, label: 0, bag_size: 1831\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 11759\n",
      "batch 499, loss: 0.0002, instance_loss: 0.0011, weighted_loss: 0.0005, label: 1, bag_size: 6781\n",
      "batch 519, loss: 0.0009, instance_loss: 0.0006, weighted_loss: 0.0008, label: 0, bag_size: 2652\n",
      "batch 539, loss: 0.0154, instance_loss: 0.0134, weighted_loss: 0.0148, label: 1, bag_size: 16565\n",
      "batch 559, loss: 0.0007, instance_loss: 0.0043, weighted_loss: 0.0018, label: 0, bag_size: 18045\n",
      "batch 579, loss: 0.0106, instance_loss: 0.0016, weighted_loss: 0.0079, label: 1, bag_size: 10432\n",
      "batch 599, loss: 0.0077, instance_loss: 0.0160, weighted_loss: 0.0102, label: 0, bag_size: 2367\n",
      "batch 619, loss: 0.0027, instance_loss: 0.0159, weighted_loss: 0.0067, label: 0, bag_size: 11512\n",
      "batch 639, loss: 0.0802, instance_loss: 0.2508, weighted_loss: 0.1314, label: 0, bag_size: 2918\n",
      "batch 659, loss: 0.1171, instance_loss: 0.2231, weighted_loss: 0.1489, label: 0, bag_size: 8582\n",
      "batch 679, loss: 0.0078, instance_loss: 0.0058, weighted_loss: 0.0072, label: 0, bag_size: 6356\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 15665\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0042, weighted_loss: 0.0013, label: 1, bag_size: 11642\n",
      "batch 739, loss: 1.2657, instance_loss: 1.5017, weighted_loss: 1.3365, label: 0, bag_size: 1592\n",
      "batch 759, loss: 0.0001, instance_loss: 0.0005, weighted_loss: 0.0002, label: 1, bag_size: 12931\n",
      "batch 779, loss: 0.0010, instance_loss: 0.0003, weighted_loss: 0.0008, label: 1, bag_size: 8680\n",
      "batch 799, loss: 0.0045, instance_loss: 0.0004, weighted_loss: 0.0033, label: 0, bag_size: 1651\n",
      "batch 819, loss: 0.0496, instance_loss: 0.0354, weighted_loss: 0.0453, label: 0, bag_size: 1684\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9789634146341464: correct 12844/13120\n",
      "class 1 clustering acc 0.8896341463414634: correct 5836/6560\n",
      "Epoch: 40, train_loss: 0.1746, train_clustering_loss:  0.2081, train_error: 0.0622\n",
      "class 0: acc 0.9393939393939394, correct 372/396\n",
      "class 1: acc 0.9363207547169812, correct 397/424\n",
      "\n",
      "Val Set, val_loss: 0.2359, val_error: 0.1091, auc: 0.9778\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.8076923076923077, correct 42/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.0012, instance_loss: 0.9761, weighted_loss: 0.9937, label: 1, bag_size: 2935\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 1, bag_size: 6966\n",
      "batch 59, loss: 0.0150, instance_loss: 0.0243, weighted_loss: 0.0178, label: 1, bag_size: 5561\n",
      "batch 79, loss: 0.0040, instance_loss: 0.0012, weighted_loss: 0.0032, label: 0, bag_size: 18954\n",
      "batch 99, loss: 0.1631, instance_loss: 0.2537, weighted_loss: 0.1903, label: 0, bag_size: 65728\n",
      "batch 119, loss: 0.0107, instance_loss: 0.0062, weighted_loss: 0.0094, label: 0, bag_size: 2336\n",
      "batch 139, loss: 0.0053, instance_loss: 0.0042, weighted_loss: 0.0050, label: 0, bag_size: 24439\n",
      "batch 159, loss: 0.0240, instance_loss: 0.0189, weighted_loss: 0.0224, label: 0, bag_size: 10068\n",
      "batch 179, loss: 0.1775, instance_loss: 0.1755, weighted_loss: 0.1769, label: 1, bag_size: 5903\n",
      "batch 199, loss: 0.0046, instance_loss: 0.0024, weighted_loss: 0.0040, label: 0, bag_size: 23996\n",
      "batch 219, loss: 0.0016, instance_loss: 0.0001, weighted_loss: 0.0012, label: 0, bag_size: 22828\n",
      "batch 239, loss: 0.0009, instance_loss: 0.0002, weighted_loss: 0.0007, label: 1, bag_size: 9065\n",
      "batch 259, loss: 0.0003, instance_loss: 0.0003, weighted_loss: 0.0003, label: 0, bag_size: 14828\n",
      "batch 279, loss: 0.1328, instance_loss: 0.0957, weighted_loss: 0.1217, label: 0, bag_size: 2266\n",
      "batch 299, loss: 0.0121, instance_loss: 0.0098, weighted_loss: 0.0114, label: 1, bag_size: 15665\n",
      "batch 319, loss: 0.0079, instance_loss: 0.0051, weighted_loss: 0.0070, label: 1, bag_size: 12946\n",
      "batch 339, loss: 0.6089, instance_loss: 0.7266, weighted_loss: 0.6442, label: 0, bag_size: 1714\n",
      "batch 359, loss: 0.0323, instance_loss: 0.0605, weighted_loss: 0.0408, label: 1, bag_size: 6736\n",
      "batch 379, loss: 0.0002, instance_loss: 0.2067, weighted_loss: 0.0621, label: 1, bag_size: 645\n",
      "batch 399, loss: 0.0270, instance_loss: 0.0134, weighted_loss: 0.0229, label: 0, bag_size: 12840\n",
      "batch 419, loss: 0.0061, instance_loss: 0.0035, weighted_loss: 0.0053, label: 0, bag_size: 12201\n",
      "batch 439, loss: 0.0125, instance_loss: 0.0097, weighted_loss: 0.0117, label: 0, bag_size: 1213\n",
      "batch 459, loss: 0.0096, instance_loss: 0.0095, weighted_loss: 0.0095, label: 0, bag_size: 13205\n",
      "batch 479, loss: 0.0552, instance_loss: 0.0643, weighted_loss: 0.0579, label: 0, bag_size: 13205\n",
      "batch 499, loss: 0.0131, instance_loss: 0.0093, weighted_loss: 0.0120, label: 0, bag_size: 1891\n",
      "batch 519, loss: 0.0891, instance_loss: 0.1026, weighted_loss: 0.0931, label: 1, bag_size: 2314\n",
      "batch 539, loss: 0.3152, instance_loss: 0.3955, weighted_loss: 0.3393, label: 1, bag_size: 1759\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 7798\n",
      "batch 579, loss: 0.3075, instance_loss: 0.3918, weighted_loss: 0.3328, label: 0, bag_size: 1651\n",
      "batch 599, loss: 0.0006, instance_loss: 0.0003, weighted_loss: 0.0005, label: 0, bag_size: 9252\n",
      "batch 619, loss: 0.0009, instance_loss: 0.0007, weighted_loss: 0.0008, label: 1, bag_size: 1255\n",
      "batch 639, loss: 0.0141, instance_loss: 0.0164, weighted_loss: 0.0148, label: 1, bag_size: 9519\n",
      "batch 659, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 18154\n",
      "batch 679, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 8145\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 1, bag_size: 5833\n",
      "batch 719, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 8959\n",
      "batch 739, loss: 0.0146, instance_loss: 0.0111, weighted_loss: 0.0135, label: 0, bag_size: 13339\n",
      "batch 759, loss: 0.1090, instance_loss: 0.1161, weighted_loss: 0.1111, label: 0, bag_size: 21319\n",
      "batch 779, loss: 0.0073, instance_loss: 0.0036, weighted_loss: 0.0062, label: 1, bag_size: 2522\n",
      "batch 799, loss: 0.2267, instance_loss: 0.1884, weighted_loss: 0.2152, label: 0, bag_size: 12083\n",
      "batch 819, loss: 0.0023, instance_loss: 0.0015, weighted_loss: 0.0020, label: 0, bag_size: 2322\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9823932926829269: correct 12889/13120\n",
      "class 1 clustering acc 0.9103658536585366: correct 5972/6560\n",
      "Epoch: 41, train_loss: 0.1492, train_clustering_loss:  0.1706, train_error: 0.0537\n",
      "class 0: acc 0.948019801980198, correct 383/404\n",
      "class 1: acc 0.9447115384615384, correct 393/416\n",
      "\n",
      "Val Set, val_loss: 0.3029, val_error: 0.1455, auc: 0.9741\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.7307692307692307, correct 38/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0013, instance_loss: 0.0069, weighted_loss: 0.0030, label: 1, bag_size: 689\n",
      "batch 39, loss: 0.0025, instance_loss: 0.0029, weighted_loss: 0.0026, label: 1, bag_size: 8003\n",
      "batch 59, loss: 0.0003, instance_loss: 0.0015, weighted_loss: 0.0007, label: 1, bag_size: 9732\n",
      "batch 79, loss: 0.0004, instance_loss: 0.0003, weighted_loss: 0.0004, label: 1, bag_size: 19932\n",
      "batch 99, loss: 0.0005, instance_loss: 0.0037, weighted_loss: 0.0015, label: 1, bag_size: 8019\n",
      "batch 119, loss: 0.0022, instance_loss: 0.0015, weighted_loss: 0.0020, label: 0, bag_size: 19808\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0407, weighted_loss: 0.0122, label: 1, bag_size: 11600\n",
      "batch 159, loss: 0.0600, instance_loss: 0.0290, weighted_loss: 0.0507, label: 1, bag_size: 12611\n",
      "batch 179, loss: 0.0015, instance_loss: 0.0014, weighted_loss: 0.0015, label: 1, bag_size: 12575\n",
      "batch 199, loss: 3.5980, instance_loss: 3.2163, weighted_loss: 3.4835, label: 0, bag_size: 3802\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 13225\n",
      "batch 239, loss: 0.1125, instance_loss: 0.0768, weighted_loss: 0.1018, label: 0, bag_size: 13205\n",
      "batch 259, loss: 0.0905, instance_loss: 0.0663, weighted_loss: 0.0833, label: 1, bag_size: 8982\n",
      "batch 279, loss: 0.4081, instance_loss: 0.5305, weighted_loss: 0.4448, label: 0, bag_size: 4418\n",
      "batch 299, loss: 0.0011, instance_loss: 0.0024, weighted_loss: 0.0015, label: 1, bag_size: 16512\n",
      "batch 319, loss: 0.0494, instance_loss: 0.0375, weighted_loss: 0.0458, label: 1, bag_size: 9147\n",
      "batch 339, loss: 0.0604, instance_loss: 0.0450, weighted_loss: 0.0558, label: 0, bag_size: 22681\n",
      "batch 359, loss: 0.0369, instance_loss: 0.0100, weighted_loss: 0.0288, label: 1, bag_size: 13786\n",
      "batch 379, loss: 0.0345, instance_loss: 0.0225, weighted_loss: 0.0309, label: 1, bag_size: 20161\n",
      "batch 399, loss: 0.0345, instance_loss: 0.0214, weighted_loss: 0.0305, label: 1, bag_size: 3295\n",
      "batch 419, loss: 0.0040, instance_loss: 0.0012, weighted_loss: 0.0031, label: 1, bag_size: 14030\n",
      "batch 439, loss: 0.0017, instance_loss: 0.0026, weighted_loss: 0.0020, label: 0, bag_size: 3160\n",
      "batch 459, loss: 0.0491, instance_loss: 0.0399, weighted_loss: 0.0464, label: 0, bag_size: 10113\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11735\n",
      "batch 499, loss: 0.1767, instance_loss: 0.1602, weighted_loss: 0.1717, label: 1, bag_size: 2308\n",
      "batch 519, loss: 0.0160, instance_loss: 0.0063, weighted_loss: 0.0131, label: 0, bag_size: 19067\n",
      "batch 539, loss: 0.0866, instance_loss: 0.0356, weighted_loss: 0.0713, label: 1, bag_size: 3651\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 11383\n",
      "batch 579, loss: 0.0719, instance_loss: 0.0724, weighted_loss: 0.0720, label: 0, bag_size: 11151\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11113\n",
      "batch 619, loss: 0.0020, instance_loss: 0.0044, weighted_loss: 0.0027, label: 0, bag_size: 1639\n",
      "batch 639, loss: 0.0004, instance_loss: 0.0002, weighted_loss: 0.0003, label: 1, bag_size: 9230\n",
      "batch 659, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 3101\n",
      "batch 679, loss: 0.0087, instance_loss: 0.0010, weighted_loss: 0.0064, label: 0, bag_size: 11194\n",
      "batch 699, loss: 0.0017, instance_loss: 0.0007, weighted_loss: 0.0014, label: 1, bag_size: 5833\n",
      "batch 719, loss: 0.0082, instance_loss: 0.0093, weighted_loss: 0.0085, label: 1, bag_size: 11394\n",
      "batch 739, loss: 0.3009, instance_loss: 0.4020, weighted_loss: 0.3312, label: 0, bag_size: 9616\n",
      "batch 759, loss: 0.0148, instance_loss: 0.0055, weighted_loss: 0.0120, label: 0, bag_size: 18240\n",
      "batch 779, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 1213\n",
      "batch 799, loss: 0.0394, instance_loss: 0.0110, weighted_loss: 0.0309, label: 0, bag_size: 10942\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0033, weighted_loss: 0.0010, label: 1, bag_size: 4877\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9830792682926829: correct 12898/13120\n",
      "class 1 clustering acc 0.9214939024390244: correct 6045/6560\n",
      "Epoch: 42, train_loss: 0.1425, train_clustering_loss:  0.1599, train_error: 0.0488\n",
      "class 0: acc 0.948780487804878, correct 389/410\n",
      "class 1: acc 0.9536585365853658, correct 391/410\n",
      "\n",
      "Val Set, val_loss: 0.2418, val_error: 0.1000, auc: 0.9768\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0065, instance_loss: 0.0079, weighted_loss: 0.0070, label: 1, bag_size: 13477\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 11642\n",
      "batch 59, loss: 0.0086, instance_loss: 0.0091, weighted_loss: 0.0088, label: 1, bag_size: 16154\n",
      "batch 79, loss: 0.0003, instance_loss: 0.0006, weighted_loss: 0.0004, label: 1, bag_size: 12408\n",
      "batch 99, loss: 0.0240, instance_loss: 0.0247, weighted_loss: 0.0242, label: 1, bag_size: 1759\n",
      "batch 119, loss: 0.0022, instance_loss: 0.0017, weighted_loss: 0.0021, label: 0, bag_size: 6281\n",
      "batch 139, loss: 0.0156, instance_loss: 0.0151, weighted_loss: 0.0154, label: 1, bag_size: 865\n",
      "batch 159, loss: 0.0019, instance_loss: 0.0012, weighted_loss: 0.0017, label: 0, bag_size: 1745\n",
      "batch 179, loss: 0.0015, instance_loss: 0.0002, weighted_loss: 0.0011, label: 1, bag_size: 9478\n",
      "batch 199, loss: 0.0005, instance_loss: 0.0310, weighted_loss: 0.0097, label: 1, bag_size: 10112\n",
      "batch 219, loss: 0.0034, instance_loss: 0.0050, weighted_loss: 0.0039, label: 0, bag_size: 17630\n",
      "batch 239, loss: 0.0011, instance_loss: 0.0011, weighted_loss: 0.0011, label: 1, bag_size: 1638\n",
      "batch 259, loss: 0.0047, instance_loss: 0.0038, weighted_loss: 0.0044, label: 1, bag_size: 9230\n",
      "batch 279, loss: 0.3717, instance_loss: 0.2761, weighted_loss: 0.3430, label: 0, bag_size: 3541\n",
      "batch 299, loss: 0.5484, instance_loss: 0.6608, weighted_loss: 0.5821, label: 0, bag_size: 21361\n",
      "batch 319, loss: 0.0286, instance_loss: 0.0156, weighted_loss: 0.0247, label: 0, bag_size: 2654\n",
      "batch 339, loss: 0.0041, instance_loss: 0.0018, weighted_loss: 0.0034, label: 0, bag_size: 3552\n",
      "batch 359, loss: 0.0213, instance_loss: 0.0144, weighted_loss: 0.0192, label: 1, bag_size: 5516\n",
      "batch 379, loss: 0.0062, instance_loss: 0.0058, weighted_loss: 0.0061, label: 1, bag_size: 5894\n",
      "batch 399, loss: 0.0006, instance_loss: 0.0007, weighted_loss: 0.0006, label: 1, bag_size: 16267\n",
      "batch 419, loss: 0.0010, instance_loss: 0.0069, weighted_loss: 0.0028, label: 1, bag_size: 3640\n",
      "batch 439, loss: 0.0118, instance_loss: 0.0140, weighted_loss: 0.0125, label: 0, bag_size: 4345\n",
      "batch 459, loss: 0.0019, instance_loss: 0.0041, weighted_loss: 0.0026, label: 1, bag_size: 2278\n",
      "batch 479, loss: 0.0241, instance_loss: 0.0264, weighted_loss: 0.0248, label: 0, bag_size: 8959\n",
      "batch 499, loss: 0.0143, instance_loss: 0.0115, weighted_loss: 0.0134, label: 1, bag_size: 5256\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0003, weighted_loss: 0.0002, label: 1, bag_size: 5991\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0991, weighted_loss: 0.0297, label: 1, bag_size: 9971\n",
      "batch 559, loss: 0.0035, instance_loss: 0.0007, weighted_loss: 0.0027, label: 0, bag_size: 11194\n",
      "batch 579, loss: 0.0657, instance_loss: 0.0499, weighted_loss: 0.0610, label: 0, bag_size: 1745\n",
      "batch 599, loss: 0.0036, instance_loss: 0.0023, weighted_loss: 0.0032, label: 0, bag_size: 13795\n",
      "batch 619, loss: 0.0084, instance_loss: 0.0078, weighted_loss: 0.0082, label: 0, bag_size: 2511\n",
      "batch 639, loss: 0.6171, instance_loss: 0.7495, weighted_loss: 0.6568, label: 0, bag_size: 2213\n",
      "batch 659, loss: 0.0011, instance_loss: 0.0026, weighted_loss: 0.0016, label: 1, bag_size: 7613\n",
      "batch 679, loss: 0.3700, instance_loss: 0.4708, weighted_loss: 0.4003, label: 1, bag_size: 8103\n",
      "batch 699, loss: 2.3450, instance_loss: 2.8065, weighted_loss: 2.4835, label: 1, bag_size: 13367\n",
      "batch 719, loss: 0.1292, instance_loss: 0.1191, weighted_loss: 0.1262, label: 0, bag_size: 15071\n",
      "batch 739, loss: 0.0009, instance_loss: 0.0002, weighted_loss: 0.0007, label: 0, bag_size: 931\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 17633\n",
      "batch 779, loss: 0.1832, instance_loss: 0.1922, weighted_loss: 0.1859, label: 1, bag_size: 1339\n",
      "batch 799, loss: 0.0035, instance_loss: 0.0047, weighted_loss: 0.0039, label: 0, bag_size: 1760\n",
      "batch 819, loss: 0.0009, instance_loss: 0.0013, weighted_loss: 0.0010, label: 1, bag_size: 9533\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9842987804878048: correct 12914/13120\n",
      "class 1 clustering acc 0.9195121951219513: correct 6032/6560\n",
      "Epoch: 43, train_loss: 0.1244, train_clustering_loss:  0.1470, train_error: 0.0524\n",
      "class 0: acc 0.9507389162561576, correct 386/406\n",
      "class 1: acc 0.9444444444444444, correct 391/414\n",
      "\n",
      "Val Set, val_loss: 0.2366, val_error: 0.1000, auc: 0.9765\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0144, instance_loss: 0.0141, weighted_loss: 0.0143, label: 0, bag_size: 1416\n",
      "batch 39, loss: 0.0751, instance_loss: 0.0605, weighted_loss: 0.0707, label: 0, bag_size: 5639\n",
      "batch 59, loss: 0.4613, instance_loss: 0.5259, weighted_loss: 0.4807, label: 1, bag_size: 1095\n",
      "batch 79, loss: 0.0068, instance_loss: 0.0020, weighted_loss: 0.0054, label: 0, bag_size: 11187\n",
      "batch 99, loss: 0.0020, instance_loss: 0.0015, weighted_loss: 0.0018, label: 1, bag_size: 629\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0002, weighted_loss: 0.0002, label: 1, bag_size: 15716\n",
      "batch 139, loss: 0.0039, instance_loss: 0.0026, weighted_loss: 0.0035, label: 0, bag_size: 10365\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 19472\n",
      "batch 179, loss: 0.0182, instance_loss: 0.0184, weighted_loss: 0.0182, label: 0, bag_size: 14264\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 1, bag_size: 10920\n",
      "batch 219, loss: 0.0006, instance_loss: 0.0016, weighted_loss: 0.0009, label: 1, bag_size: 6875\n",
      "batch 239, loss: 0.0945, instance_loss: 0.0776, weighted_loss: 0.0895, label: 0, bag_size: 11146\n",
      "batch 259, loss: 0.0308, instance_loss: 0.0160, weighted_loss: 0.0263, label: 0, bag_size: 2043\n",
      "batch 279, loss: 0.2259, instance_loss: 0.2166, weighted_loss: 0.2231, label: 1, bag_size: 2308\n",
      "batch 299, loss: 0.0023, instance_loss: 0.0006, weighted_loss: 0.0018, label: 1, bag_size: 16379\n",
      "batch 319, loss: 0.0005, instance_loss: 0.0003, weighted_loss: 0.0004, label: 0, bag_size: 18045\n",
      "batch 339, loss: 0.0168, instance_loss: 0.0170, weighted_loss: 0.0169, label: 0, bag_size: 8420\n",
      "batch 359, loss: 0.0955, instance_loss: 0.2156, weighted_loss: 0.1315, label: 1, bag_size: 684\n",
      "batch 379, loss: 0.0734, instance_loss: 0.0466, weighted_loss: 0.0654, label: 0, bag_size: 12149\n",
      "batch 399, loss: 0.0103, instance_loss: 0.0206, weighted_loss: 0.0134, label: 1, bag_size: 3082\n",
      "batch 419, loss: 0.0003, instance_loss: 0.0285, weighted_loss: 0.0087, label: 0, bag_size: 13225\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0005, weighted_loss: 0.0002, label: 1, bag_size: 8448\n",
      "batch 459, loss: 0.0320, instance_loss: 0.0103, weighted_loss: 0.0254, label: 0, bag_size: 18415\n",
      "batch 479, loss: 0.0112, instance_loss: 0.0016, weighted_loss: 0.0083, label: 1, bag_size: 2136\n",
      "batch 499, loss: 0.0026, instance_loss: 0.0002, weighted_loss: 0.0018, label: 1, bag_size: 16267\n",
      "batch 519, loss: 0.0006, instance_loss: 0.0002, weighted_loss: 0.0005, label: 0, bag_size: 13225\n",
      "batch 539, loss: 0.0020, instance_loss: 0.0105, weighted_loss: 0.0046, label: 0, bag_size: 14319\n",
      "batch 559, loss: 0.0254, instance_loss: 0.0339, weighted_loss: 0.0279, label: 1, bag_size: 5256\n",
      "batch 579, loss: 0.0009, instance_loss: 0.0012, weighted_loss: 0.0010, label: 1, bag_size: 9078\n",
      "batch 599, loss: 0.0002, instance_loss: 0.0002, weighted_loss: 0.0002, label: 1, bag_size: 6317\n",
      "batch 619, loss: 0.0070, instance_loss: 0.0113, weighted_loss: 0.0083, label: 0, bag_size: 14377\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 9471\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 2424\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0016, weighted_loss: 0.0005, label: 1, bag_size: 14223\n",
      "batch 699, loss: 0.0003, instance_loss: 0.0022, weighted_loss: 0.0008, label: 0, bag_size: 24439\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 11512\n",
      "batch 739, loss: 0.0115, instance_loss: 0.0456, weighted_loss: 0.0217, label: 1, bag_size: 13026\n",
      "batch 759, loss: 0.0072, instance_loss: 0.0045, weighted_loss: 0.0064, label: 1, bag_size: 19972\n",
      "batch 779, loss: 4.2290, instance_loss: 4.3549, weighted_loss: 4.2667, label: 0, bag_size: 3802\n",
      "batch 799, loss: 0.0256, instance_loss: 0.0176, weighted_loss: 0.0232, label: 0, bag_size: 2814\n",
      "batch 819, loss: 0.0001, instance_loss: 0.0722, weighted_loss: 0.0217, label: 1, bag_size: 7119\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9839939024390244: correct 12910/13120\n",
      "class 1 clustering acc 0.9149390243902439: correct 6002/6560\n",
      "Epoch: 44, train_loss: 0.1667, train_clustering_loss:  0.1819, train_error: 0.0561\n",
      "class 0: acc 0.9432098765432099, correct 382/405\n",
      "class 1: acc 0.944578313253012, correct 392/415\n",
      "\n",
      "Val Set, val_loss: 0.2377, val_error: 0.0909, auc: 0.9765\n",
      "class 0 clustering acc 0.9460227272727273: correct 1665/1760\n",
      "class 1 clustering acc 0.17272727272727273: correct 152/880\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0058, instance_loss: 0.0041, weighted_loss: 0.0053, label: 1, bag_size: 4821\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0017, weighted_loss: 0.0006, label: 0, bag_size: 9885\n",
      "batch 59, loss: 0.0031, instance_loss: 0.0070, weighted_loss: 0.0042, label: 0, bag_size: 10365\n",
      "batch 79, loss: 0.0015, instance_loss: 0.0019, weighted_loss: 0.0016, label: 1, bag_size: 11387\n",
      "batch 99, loss: 0.7639, instance_loss: 1.0024, weighted_loss: 0.8354, label: 1, bag_size: 898\n",
      "batch 119, loss: 0.0033, instance_loss: 0.0019, weighted_loss: 0.0029, label: 1, bag_size: 5894\n",
      "batch 139, loss: 0.0018, instance_loss: 0.0007, weighted_loss: 0.0015, label: 0, bag_size: 1349\n",
      "batch 159, loss: 0.0005, instance_loss: 0.0003, weighted_loss: 0.0004, label: 1, bag_size: 1838\n",
      "batch 179, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 4250\n",
      "batch 199, loss: 0.0175, instance_loss: 0.0122, weighted_loss: 0.0159, label: 1, bag_size: 13365\n",
      "batch 219, loss: 2.5647, instance_loss: 3.1285, weighted_loss: 2.7338, label: 0, bag_size: 7428\n",
      "batch 239, loss: 0.8019, instance_loss: 1.0370, weighted_loss: 0.8724, label: 0, bag_size: 2653\n",
      "batch 259, loss: 0.0116, instance_loss: 0.0134, weighted_loss: 0.0121, label: 1, bag_size: 11394\n",
      "batch 279, loss: 0.0021, instance_loss: 0.0006, weighted_loss: 0.0017, label: 0, bag_size: 2282\n",
      "batch 299, loss: 0.1105, instance_loss: 0.1237, weighted_loss: 0.1145, label: 1, bag_size: 1525\n",
      "batch 319, loss: 0.0003, instance_loss: 0.0026, weighted_loss: 0.0009, label: 1, bag_size: 11642\n",
      "batch 339, loss: 0.0038, instance_loss: 0.0023, weighted_loss: 0.0034, label: 1, bag_size: 13255\n",
      "batch 359, loss: 1.6135, instance_loss: 1.8781, weighted_loss: 1.6929, label: 0, bag_size: 9597\n",
      "batch 379, loss: 1.2905, instance_loss: 1.1787, weighted_loss: 1.2569, label: 0, bag_size: 3802\n",
      "batch 399, loss: 0.0079, instance_loss: 0.0031, weighted_loss: 0.0065, label: 1, bag_size: 6731\n",
      "batch 419, loss: 0.0151, instance_loss: 0.0076, weighted_loss: 0.0129, label: 0, bag_size: 2511\n",
      "batch 439, loss: 0.0363, instance_loss: 0.0212, weighted_loss: 0.0318, label: 1, bag_size: 1015\n",
      "batch 459, loss: 0.0850, instance_loss: 0.1654, weighted_loss: 0.1091, label: 0, bag_size: 3654\n",
      "batch 479, loss: 0.0071, instance_loss: 0.0015, weighted_loss: 0.0054, label: 1, bag_size: 11600\n",
      "batch 499, loss: 0.0225, instance_loss: 0.0222, weighted_loss: 0.0225, label: 1, bag_size: 1819\n",
      "batch 519, loss: 0.0004, instance_loss: 0.0001, weighted_loss: 0.0003, label: 0, bag_size: 4465\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 17633\n",
      "batch 559, loss: 0.0006, instance_loss: 0.0005, weighted_loss: 0.0005, label: 0, bag_size: 19466\n",
      "batch 579, loss: 0.0820, instance_loss: 0.0832, weighted_loss: 0.0824, label: 0, bag_size: 2628\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 7513\n",
      "batch 619, loss: 0.0114, instance_loss: 0.0081, weighted_loss: 0.0104, label: 0, bag_size: 19518\n",
      "batch 639, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 5409\n",
      "batch 659, loss: 0.0097, instance_loss: 0.0068, weighted_loss: 0.0088, label: 1, bag_size: 5340\n",
      "batch 679, loss: 0.3584, instance_loss: 0.4531, weighted_loss: 0.3868, label: 0, bag_size: 1760\n",
      "batch 699, loss: 0.0045, instance_loss: 0.0018, weighted_loss: 0.0037, label: 1, bag_size: 15233\n",
      "batch 719, loss: 0.3869, instance_loss: 0.5092, weighted_loss: 0.4236, label: 1, bag_size: 1703\n",
      "batch 739, loss: 0.0001, instance_loss: 0.0104, weighted_loss: 0.0032, label: 1, bag_size: 9955\n",
      "batch 759, loss: 0.0009, instance_loss: 0.0001, weighted_loss: 0.0006, label: 0, bag_size: 20796\n",
      "batch 779, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 7381\n",
      "batch 799, loss: 0.0060, instance_loss: 0.0029, weighted_loss: 0.0051, label: 1, bag_size: 7613\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 7823\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9830030487804878: correct 12897/13120\n",
      "class 1 clustering acc 0.910670731707317: correct 5974/6560\n",
      "Epoch: 45, train_loss: 0.1438, train_clustering_loss:  0.1677, train_error: 0.0561\n",
      "class 0: acc 0.9358288770053476, correct 350/374\n",
      "class 1: acc 0.9506726457399103, correct 424/446\n",
      "\n",
      "Val Set, val_loss: 0.3002, val_error: 0.1273, auc: 0.9765\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.7931034482758621, correct 46/58\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0049, instance_loss: 0.0044, weighted_loss: 0.0047, label: 1, bag_size: 34356\n",
      "batch 39, loss: 0.0119, instance_loss: 0.0068, weighted_loss: 0.0104, label: 1, bag_size: 13440\n",
      "batch 59, loss: 0.0786, instance_loss: 0.0656, weighted_loss: 0.0747, label: 0, bag_size: 9616\n",
      "batch 79, loss: 0.1649, instance_loss: 0.1894, weighted_loss: 0.1723, label: 1, bag_size: 5903\n",
      "batch 99, loss: 0.0185, instance_loss: 0.0105, weighted_loss: 0.0161, label: 0, bag_size: 1560\n",
      "batch 119, loss: 0.0159, instance_loss: 0.0212, weighted_loss: 0.0175, label: 1, bag_size: 4039\n",
      "batch 139, loss: 0.9908, instance_loss: 1.1132, weighted_loss: 1.0275, label: 1, bag_size: 6682\n",
      "batch 159, loss: 0.0193, instance_loss: 0.0197, weighted_loss: 0.0194, label: 1, bag_size: 2681\n",
      "batch 179, loss: 0.0028, instance_loss: 0.0040, weighted_loss: 0.0032, label: 0, bag_size: 1614\n",
      "batch 199, loss: 0.0759, instance_loss: 0.0498, weighted_loss: 0.0680, label: 1, bag_size: 7669\n",
      "batch 219, loss: 0.0049, instance_loss: 0.0017, weighted_loss: 0.0039, label: 0, bag_size: 3101\n",
      "batch 239, loss: 0.0068, instance_loss: 0.0043, weighted_loss: 0.0061, label: 0, bag_size: 6652\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 7011\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 23398\n",
      "batch 299, loss: 0.0352, instance_loss: 0.0310, weighted_loss: 0.0340, label: 0, bag_size: 2624\n",
      "batch 319, loss: 0.0003, instance_loss: 0.0077, weighted_loss: 0.0025, label: 1, bag_size: 617\n",
      "batch 339, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 10396\n",
      "batch 359, loss: 0.0000, instance_loss: 0.3488, weighted_loss: 0.1046, label: 1, bag_size: 1781\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9470\n",
      "batch 399, loss: 0.0062, instance_loss: 0.0071, weighted_loss: 0.0065, label: 1, bag_size: 3409\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 14202\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 12575\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15077\n",
      "batch 479, loss: 0.0214, instance_loss: 0.0090, weighted_loss: 0.0177, label: 1, bag_size: 5340\n",
      "batch 499, loss: 1.1091, instance_loss: 1.5379, weighted_loss: 1.2377, label: 0, bag_size: 5120\n",
      "batch 519, loss: 0.0200, instance_loss: 0.0194, weighted_loss: 0.0198, label: 1, bag_size: 15609\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0004, weighted_loss: 0.0004, label: 1, bag_size: 8438\n",
      "batch 559, loss: 0.0341, instance_loss: 0.0312, weighted_loss: 0.0333, label: 0, bag_size: 18954\n",
      "batch 579, loss: 0.0021, instance_loss: 0.0012, weighted_loss: 0.0018, label: 1, bag_size: 19972\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 10867\n",
      "batch 619, loss: 0.0106, instance_loss: 0.0120, weighted_loss: 0.0110, label: 1, bag_size: 7351\n",
      "batch 639, loss: 0.0008, instance_loss: 0.0016, weighted_loss: 0.0010, label: 1, bag_size: 5317\n",
      "batch 659, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 16341\n",
      "batch 679, loss: 0.0032, instance_loss: 0.0025, weighted_loss: 0.0030, label: 1, bag_size: 9078\n",
      "batch 699, loss: 0.0008, instance_loss: 0.0011, weighted_loss: 0.0009, label: 0, bag_size: 13992\n",
      "batch 719, loss: 0.2655, instance_loss: 0.3349, weighted_loss: 0.2863, label: 0, bag_size: 1614\n",
      "batch 739, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 0, bag_size: 4271\n",
      "batch 759, loss: 0.0014, instance_loss: 0.0022, weighted_loss: 0.0017, label: 0, bag_size: 22681\n",
      "batch 779, loss: 5.9844, instance_loss: 4.7661, weighted_loss: 5.6189, label: 1, bag_size: 1497\n",
      "batch 799, loss: 0.2822, instance_loss: 0.1675, weighted_loss: 0.2478, label: 0, bag_size: 1772\n",
      "batch 819, loss: 0.0026, instance_loss: 0.0005, weighted_loss: 0.0020, label: 1, bag_size: 10622\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9836128048780488: correct 12905/13120\n",
      "class 1 clustering acc 0.9280487804878049: correct 6088/6560\n",
      "Epoch: 46, train_loss: 0.1255, train_clustering_loss:  0.1448, train_error: 0.0512\n",
      "class 0: acc 0.9385026737967914, correct 351/374\n",
      "class 1: acc 0.9573991031390134, correct 427/446\n",
      "\n",
      "Val Set, val_loss: 0.2977, val_error: 0.1455, auc: 0.9784\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.011363636363636364: correct 10/880\n",
      "class 0: acc 0.7307692307692307, correct 38/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0048, instance_loss: 0.0046, weighted_loss: 0.0047, label: 0, bag_size: 18415\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 9069\n",
      "batch 59, loss: 0.3134, instance_loss: 0.3552, weighted_loss: 0.3259, label: 1, bag_size: 2314\n",
      "batch 79, loss: 0.0101, instance_loss: 0.0605, weighted_loss: 0.0252, label: 1, bag_size: 8019\n",
      "batch 99, loss: 0.0025, instance_loss: 0.0047, weighted_loss: 0.0032, label: 0, bag_size: 21093\n",
      "batch 119, loss: 0.0034, instance_loss: 0.0046, weighted_loss: 0.0037, label: 0, bag_size: 2628\n",
      "batch 139, loss: 0.0032, instance_loss: 0.0004, weighted_loss: 0.0024, label: 1, bag_size: 7381\n",
      "batch 159, loss: 0.3138, instance_loss: 0.3996, weighted_loss: 0.3395, label: 1, bag_size: 898\n",
      "batch 179, loss: 0.0006, instance_loss: 0.0011, weighted_loss: 0.0008, label: 1, bag_size: 1823\n",
      "batch 199, loss: 0.0048, instance_loss: 0.0022, weighted_loss: 0.0040, label: 0, bag_size: 26271\n",
      "batch 219, loss: 0.0231, instance_loss: 0.0141, weighted_loss: 0.0204, label: 0, bag_size: 17268\n",
      "batch 239, loss: 0.1700, instance_loss: 0.1353, weighted_loss: 0.1596, label: 1, bag_size: 3652\n",
      "batch 259, loss: 0.0005, instance_loss: 0.0001, weighted_loss: 0.0004, label: 1, bag_size: 11642\n",
      "batch 279, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 10128\n",
      "batch 299, loss: 0.0017, instance_loss: 0.0011, weighted_loss: 0.0015, label: 1, bag_size: 3980\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 0, bag_size: 16052\n",
      "batch 339, loss: 0.0038, instance_loss: 0.0044, weighted_loss: 0.0040, label: 0, bag_size: 1884\n",
      "batch 359, loss: 0.0028, instance_loss: 0.0218, weighted_loss: 0.0085, label: 1, bag_size: 7110\n",
      "batch 379, loss: 0.0315, instance_loss: 0.0200, weighted_loss: 0.0281, label: 1, bag_size: 6478\n",
      "batch 399, loss: 0.0117, instance_loss: 0.0030, weighted_loss: 0.0091, label: 1, bag_size: 20161\n",
      "batch 419, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 19932\n",
      "batch 439, loss: 0.0031, instance_loss: 0.0022, weighted_loss: 0.0028, label: 1, bag_size: 4239\n",
      "batch 459, loss: 0.0027, instance_loss: 0.0032, weighted_loss: 0.0029, label: 1, bag_size: 4394\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 2303\n",
      "batch 499, loss: 6.8427, instance_loss: 5.4089, weighted_loss: 6.4126, label: 1, bag_size: 1095\n",
      "batch 519, loss: 0.0282, instance_loss: 0.0134, weighted_loss: 0.0238, label: 0, bag_size: 2043\n",
      "batch 539, loss: 0.2397, instance_loss: 0.3082, weighted_loss: 0.2603, label: 0, bag_size: 1637\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0040, weighted_loss: 0.0013, label: 0, bag_size: 19390\n",
      "batch 579, loss: 0.0786, instance_loss: 0.0592, weighted_loss: 0.0728, label: 0, bag_size: 1772\n",
      "batch 599, loss: 0.0008, instance_loss: 0.0021, weighted_loss: 0.0012, label: 1, bag_size: 6734\n",
      "batch 619, loss: 0.0768, instance_loss: 0.0726, weighted_loss: 0.0755, label: 0, bag_size: 2104\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 1, bag_size: 2495\n",
      "batch 659, loss: 0.0075, instance_loss: 0.0053, weighted_loss: 0.0069, label: 0, bag_size: 10444\n",
      "batch 679, loss: 0.3371, instance_loss: 0.4145, weighted_loss: 0.3603, label: 0, bag_size: 2815\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11389\n",
      "batch 719, loss: 0.0376, instance_loss: 0.0456, weighted_loss: 0.0400, label: 1, bag_size: 11220\n",
      "batch 739, loss: 0.0670, instance_loss: 0.0675, weighted_loss: 0.0671, label: 0, bag_size: 1789\n",
      "batch 759, loss: 0.0326, instance_loss: 0.0441, weighted_loss: 0.0361, label: 1, bag_size: 3211\n",
      "batch 779, loss: 0.0081, instance_loss: 0.0047, weighted_loss: 0.0071, label: 1, bag_size: 10848\n",
      "batch 799, loss: 0.0007, instance_loss: 0.0012, weighted_loss: 0.0008, label: 1, bag_size: 6317\n",
      "batch 819, loss: 0.0005, instance_loss: 0.0016, weighted_loss: 0.0008, label: 1, bag_size: 9533\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9813262195121951: correct 12875/13120\n",
      "class 1 clustering acc 0.9173780487804878: correct 6018/6560\n",
      "Epoch: 47, train_loss: 0.1619, train_clustering_loss:  0.1845, train_error: 0.0610\n",
      "class 0: acc 0.9370277078085643, correct 372/397\n",
      "class 1: acc 0.9408983451536643, correct 398/423\n",
      "\n",
      "Val Set, val_loss: 0.5336, val_error: 0.2000, auc: 0.9738\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.5769230769230769, correct 30/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0058, instance_loss: 0.0052, weighted_loss: 0.0056, label: 1, bag_size: 4250\n",
      "batch 39, loss: 0.0171, instance_loss: 0.0149, weighted_loss: 0.0164, label: 0, bag_size: 25814\n",
      "batch 59, loss: 1.4510, instance_loss: 2.2257, weighted_loss: 1.6834, label: 0, bag_size: 3375\n",
      "batch 79, loss: 0.0083, instance_loss: 0.0058, weighted_loss: 0.0075, label: 1, bag_size: 6736\n",
      "batch 99, loss: 0.0011, instance_loss: 0.0003, weighted_loss: 0.0009, label: 0, bag_size: 21076\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 0, bag_size: 11759\n",
      "batch 139, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 18415\n",
      "batch 159, loss: 0.0014, instance_loss: 0.0005, weighted_loss: 0.0012, label: 0, bag_size: 6356\n",
      "batch 179, loss: 0.0189, instance_loss: 0.0109, weighted_loss: 0.0165, label: 0, bag_size: 26271\n",
      "batch 199, loss: 0.0007, instance_loss: 0.0661, weighted_loss: 0.0203, label: 1, bag_size: 1101\n",
      "batch 219, loss: 0.0062, instance_loss: 0.0082, weighted_loss: 0.0068, label: 0, bag_size: 14828\n",
      "batch 239, loss: 0.0775, instance_loss: 0.0462, weighted_loss: 0.0681, label: 0, bag_size: 9387\n",
      "batch 259, loss: 0.0113, instance_loss: 0.0074, weighted_loss: 0.0102, label: 0, bag_size: 3459\n",
      "batch 279, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 19390\n",
      "batch 299, loss: 0.0139, instance_loss: 0.0103, weighted_loss: 0.0128, label: 1, bag_size: 5345\n",
      "batch 319, loss: 0.0321, instance_loss: 0.0221, weighted_loss: 0.0291, label: 1, bag_size: 20537\n",
      "batch 339, loss: 0.0184, instance_loss: 0.0158, weighted_loss: 0.0176, label: 0, bag_size: 10444\n",
      "batch 359, loss: 0.1597, instance_loss: 0.1903, weighted_loss: 0.1689, label: 0, bag_size: 1732\n",
      "batch 379, loss: 0.0679, instance_loss: 0.0522, weighted_loss: 0.0632, label: 0, bag_size: 12732\n",
      "batch 399, loss: 0.0178, instance_loss: 0.0120, weighted_loss: 0.0160, label: 1, bag_size: 2814\n",
      "batch 419, loss: 0.0018, instance_loss: 0.0021, weighted_loss: 0.0019, label: 0, bag_size: 11125\n",
      "batch 439, loss: 0.0142, instance_loss: 0.0142, weighted_loss: 0.0142, label: 1, bag_size: 2278\n",
      "batch 459, loss: 1.1766, instance_loss: 2.6521, weighted_loss: 1.6193, label: 0, bag_size: 2694\n",
      "batch 479, loss: 0.0227, instance_loss: 0.0174, weighted_loss: 0.0211, label: 0, bag_size: 14333\n",
      "batch 499, loss: 0.0246, instance_loss: 0.0772, weighted_loss: 0.0404, label: 1, bag_size: 689\n",
      "batch 519, loss: 0.0138, instance_loss: 0.0151, weighted_loss: 0.0142, label: 1, bag_size: 9561\n",
      "batch 539, loss: 0.0072, instance_loss: 0.0045, weighted_loss: 0.0064, label: 1, bag_size: 9408\n",
      "batch 559, loss: 0.1239, instance_loss: 0.1441, weighted_loss: 0.1300, label: 1, bag_size: 2179\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 2732\n",
      "batch 599, loss: 0.6702, instance_loss: 0.8392, weighted_loss: 0.7209, label: 0, bag_size: 15898\n",
      "batch 619, loss: 0.0038, instance_loss: 0.0033, weighted_loss: 0.0036, label: 0, bag_size: 3198\n",
      "batch 639, loss: 0.0348, instance_loss: 0.0423, weighted_loss: 0.0371, label: 1, bag_size: 16514\n",
      "batch 659, loss: 0.0123, instance_loss: 0.0069, weighted_loss: 0.0107, label: 0, bag_size: 25420\n",
      "batch 679, loss: 0.0153, instance_loss: 0.0145, weighted_loss: 0.0150, label: 0, bag_size: 3893\n",
      "batch 699, loss: 0.0210, instance_loss: 0.0371, weighted_loss: 0.0258, label: 1, bag_size: 21701\n",
      "batch 719, loss: 0.0004, instance_loss: 0.0003, weighted_loss: 0.0004, label: 0, bag_size: 16936\n",
      "batch 739, loss: 0.0016, instance_loss: 0.0087, weighted_loss: 0.0037, label: 1, bag_size: 6927\n",
      "batch 759, loss: 0.0001, instance_loss: 0.5025, weighted_loss: 0.1508, label: 1, bag_size: 5605\n",
      "batch 779, loss: 0.3889, instance_loss: 0.4078, weighted_loss: 0.3946, label: 0, bag_size: 2104\n",
      "batch 799, loss: 0.0006, instance_loss: 0.0004, weighted_loss: 0.0006, label: 0, bag_size: 6281\n",
      "batch 819, loss: 0.0009, instance_loss: 0.0003, weighted_loss: 0.0008, label: 1, bag_size: 621\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9782774390243902: correct 12835/13120\n",
      "class 1 clustering acc 0.881859756097561: correct 5785/6560\n",
      "Epoch: 48, train_loss: 0.1799, train_clustering_loss:  0.2204, train_error: 0.0744\n",
      "class 0: acc 0.9217171717171717, correct 365/396\n",
      "class 1: acc 0.9292452830188679, correct 394/424\n",
      "\n",
      "Val Set, val_loss: 0.2492, val_error: 0.1000, auc: 0.9771\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0076, instance_loss: 0.0055, weighted_loss: 0.0070, label: 0, bag_size: 3557\n",
      "batch 39, loss: 0.0005, instance_loss: 0.0007, weighted_loss: 0.0005, label: 0, bag_size: 21218\n",
      "batch 59, loss: 0.0004, instance_loss: 0.0001, weighted_loss: 0.0003, label: 1, bag_size: 15213\n",
      "batch 79, loss: 0.6262, instance_loss: 0.8987, weighted_loss: 0.7080, label: 0, bag_size: 2959\n",
      "batch 99, loss: 0.0097, instance_loss: 0.0108, weighted_loss: 0.0100, label: 0, bag_size: 1438\n",
      "batch 119, loss: 0.0086, instance_loss: 0.0080, weighted_loss: 0.0084, label: 0, bag_size: 16211\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0008, weighted_loss: 0.0003, label: 1, bag_size: 4423\n",
      "batch 159, loss: 0.0155, instance_loss: 0.0142, weighted_loss: 0.0151, label: 0, bag_size: 10113\n",
      "batch 179, loss: 1.2158, instance_loss: 1.8365, weighted_loss: 1.4020, label: 0, bag_size: 4418\n",
      "batch 199, loss: 0.0288, instance_loss: 0.0622, weighted_loss: 0.0388, label: 1, bag_size: 1822\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0001, weighted_loss: 0.0002, label: 1, bag_size: 7767\n",
      "batch 239, loss: 0.0212, instance_loss: 0.0123, weighted_loss: 0.0185, label: 1, bag_size: 5629\n",
      "batch 259, loss: 0.7527, instance_loss: 0.8148, weighted_loss: 0.7713, label: 1, bag_size: 2179\n",
      "batch 279, loss: 0.7501, instance_loss: 1.0020, weighted_loss: 0.8257, label: 0, bag_size: 11128\n",
      "batch 299, loss: 0.0039, instance_loss: 0.0028, weighted_loss: 0.0036, label: 1, bag_size: 14681\n",
      "batch 319, loss: 1.6331, instance_loss: 1.7437, weighted_loss: 1.6663, label: 1, bag_size: 1497\n",
      "batch 339, loss: 0.0034, instance_loss: 0.0016, weighted_loss: 0.0028, label: 1, bag_size: 5629\n",
      "batch 359, loss: 0.1137, instance_loss: 0.0628, weighted_loss: 0.0984, label: 1, bag_size: 2480\n",
      "batch 379, loss: 0.0386, instance_loss: 0.0375, weighted_loss: 0.0383, label: 0, bag_size: 9596\n",
      "batch 399, loss: 0.0063, instance_loss: 0.0054, weighted_loss: 0.0060, label: 1, bag_size: 8592\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0001, label: 0, bag_size: 1072\n",
      "batch 439, loss: 0.0006, instance_loss: 0.0003, weighted_loss: 0.0005, label: 0, bag_size: 3198\n",
      "batch 459, loss: 0.0017, instance_loss: 0.0020, weighted_loss: 0.0018, label: 1, bag_size: 8410\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0583, weighted_loss: 0.0175, label: 1, bag_size: 645\n",
      "batch 499, loss: 0.0018, instance_loss: 0.0019, weighted_loss: 0.0018, label: 1, bag_size: 7217\n",
      "batch 519, loss: 0.0638, instance_loss: 0.0377, weighted_loss: 0.0560, label: 1, bag_size: 3450\n",
      "batch 539, loss: 0.0036, instance_loss: 0.0018, weighted_loss: 0.0031, label: 1, bag_size: 4423\n",
      "batch 559, loss: 0.0002, instance_loss: 0.0003, weighted_loss: 0.0002, label: 0, bag_size: 1614\n",
      "batch 579, loss: 0.0086, instance_loss: 0.0030, weighted_loss: 0.0069, label: 1, bag_size: 19972\n",
      "batch 599, loss: 0.0131, instance_loss: 0.0068, weighted_loss: 0.0113, label: 0, bag_size: 10751\n",
      "batch 619, loss: 0.0024, instance_loss: 0.0010, weighted_loss: 0.0020, label: 1, bag_size: 11122\n",
      "batch 639, loss: 0.0015, instance_loss: 0.0001, weighted_loss: 0.0011, label: 1, bag_size: 16379\n",
      "batch 659, loss: 0.2668, instance_loss: 0.2839, weighted_loss: 0.2719, label: 0, bag_size: 6624\n",
      "batch 679, loss: 0.0388, instance_loss: 0.0265, weighted_loss: 0.0351, label: 0, bag_size: 1690\n",
      "batch 699, loss: 0.1039, instance_loss: 0.1209, weighted_loss: 0.1090, label: 1, bag_size: 1764\n",
      "batch 719, loss: 0.2955, instance_loss: 0.3229, weighted_loss: 0.3037, label: 0, bag_size: 2266\n",
      "batch 739, loss: 2.0142, instance_loss: 2.5664, weighted_loss: 2.1798, label: 1, bag_size: 2565\n",
      "batch 759, loss: 0.0328, instance_loss: 0.0174, weighted_loss: 0.0282, label: 0, bag_size: 26271\n",
      "batch 779, loss: 0.0032, instance_loss: 0.0010, weighted_loss: 0.0025, label: 0, bag_size: 1213\n",
      "batch 799, loss: 0.2340, instance_loss: 0.0708, weighted_loss: 0.1850, label: 1, bag_size: 9321\n",
      "batch 819, loss: 0.0117, instance_loss: 0.0117, weighted_loss: 0.0117, label: 1, bag_size: 1437\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9797256097560976: correct 12854/13120\n",
      "class 1 clustering acc 0.8702743902439024: correct 5709/6560\n",
      "Epoch: 49, train_loss: 0.1917, train_clustering_loss:  0.2248, train_error: 0.0878\n",
      "class 0: acc 0.9232613908872902, correct 385/417\n",
      "class 1: acc 0.9007444168734491, correct 363/403\n",
      "\n",
      "Val Set, val_loss: 0.1769, val_error: 0.0545, auc: 0.9828\n",
      "class 0 clustering acc 0.990909090909091: correct 1744/1760\n",
      "class 1 clustering acc 0.053409090909090906: correct 47/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "Validation loss decreased (0.186012 --> 0.176909).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0003, instance_loss: 0.0033, weighted_loss: 0.0012, label: 1, bag_size: 8868\n",
      "batch 39, loss: 0.0004, instance_loss: 0.0089, weighted_loss: 0.0029, label: 1, bag_size: 11266\n",
      "batch 59, loss: 0.0427, instance_loss: 0.0368, weighted_loss: 0.0409, label: 1, bag_size: 2278\n",
      "batch 79, loss: 0.0011, instance_loss: 0.0014, weighted_loss: 0.0012, label: 0, bag_size: 24911\n",
      "batch 99, loss: 0.1229, instance_loss: 0.1240, weighted_loss: 0.1232, label: 1, bag_size: 16514\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0007, weighted_loss: 0.0003, label: 0, bag_size: 2534\n",
      "batch 139, loss: 0.0011, instance_loss: 0.0004, weighted_loss: 0.0009, label: 1, bag_size: 9408\n",
      "batch 159, loss: 0.0064, instance_loss: 0.0076, weighted_loss: 0.0067, label: 1, bag_size: 1924\n",
      "batch 179, loss: 0.1218, instance_loss: 0.1503, weighted_loss: 0.1304, label: 0, bag_size: 1701\n",
      "batch 199, loss: 0.0207, instance_loss: 0.0192, weighted_loss: 0.0202, label: 0, bag_size: 705\n",
      "batch 219, loss: 0.0042, instance_loss: 0.0066, weighted_loss: 0.0049, label: 1, bag_size: 10112\n",
      "batch 239, loss: 0.0025, instance_loss: 0.0032, weighted_loss: 0.0027, label: 1, bag_size: 2785\n",
      "batch 259, loss: 0.2600, instance_loss: 0.2704, weighted_loss: 0.2631, label: 0, bag_size: 5297\n",
      "batch 279, loss: 0.0004, instance_loss: 0.0029, weighted_loss: 0.0011, label: 0, bag_size: 6356\n",
      "batch 299, loss: 0.0269, instance_loss: 0.0125, weighted_loss: 0.0226, label: 0, bag_size: 12593\n",
      "batch 319, loss: 0.4983, instance_loss: 0.4446, weighted_loss: 0.4822, label: 1, bag_size: 8103\n",
      "batch 339, loss: 0.0032, instance_loss: 0.0091, weighted_loss: 0.0050, label: 1, bag_size: 5894\n",
      "batch 359, loss: 0.0242, instance_loss: 0.0516, weighted_loss: 0.0324, label: 1, bag_size: 3211\n",
      "batch 379, loss: 0.0395, instance_loss: 0.0403, weighted_loss: 0.0397, label: 0, bag_size: 14681\n",
      "batch 399, loss: 0.0073, instance_loss: 0.0063, weighted_loss: 0.0070, label: 1, bag_size: 9078\n",
      "batch 419, loss: 0.0011, instance_loss: 0.0002, weighted_loss: 0.0009, label: 0, bag_size: 12593\n",
      "batch 439, loss: 0.3546, instance_loss: 0.4053, weighted_loss: 0.3698, label: 1, bag_size: 21450\n",
      "batch 459, loss: 0.0024, instance_loss: 0.0064, weighted_loss: 0.0036, label: 1, bag_size: 8012\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 12795\n",
      "batch 499, loss: 0.0092, instance_loss: 0.0085, weighted_loss: 0.0089, label: 0, bag_size: 14333\n",
      "batch 519, loss: 0.0163, instance_loss: 0.0028, weighted_loss: 0.0123, label: 0, bag_size: 2760\n",
      "batch 539, loss: 0.0002, instance_loss: 0.0002, weighted_loss: 0.0002, label: 0, bag_size: 8372\n",
      "batch 559, loss: 0.0055, instance_loss: 0.0052, weighted_loss: 0.0054, label: 0, bag_size: 14681\n",
      "batch 579, loss: 0.2008, instance_loss: 0.2845, weighted_loss: 0.2259, label: 1, bag_size: 1819\n",
      "batch 599, loss: 0.0256, instance_loss: 0.0152, weighted_loss: 0.0225, label: 0, bag_size: 10444\n",
      "batch 619, loss: 0.0005, instance_loss: 0.0001, weighted_loss: 0.0004, label: 0, bag_size: 18240\n",
      "batch 639, loss: 0.0141, instance_loss: 0.0078, weighted_loss: 0.0122, label: 0, bag_size: 2242\n",
      "batch 659, loss: 0.0154, instance_loss: 0.0131, weighted_loss: 0.0147, label: 0, bag_size: 1438\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 7191\n",
      "batch 699, loss: 0.2271, instance_loss: 0.3146, weighted_loss: 0.2533, label: 0, bag_size: 3321\n",
      "batch 719, loss: 0.0293, instance_loss: 0.0234, weighted_loss: 0.0275, label: 1, bag_size: 2314\n",
      "batch 739, loss: 0.0304, instance_loss: 0.0225, weighted_loss: 0.0280, label: 1, bag_size: 12719\n",
      "batch 759, loss: 0.0120, instance_loss: 0.0117, weighted_loss: 0.0119, label: 1, bag_size: 11266\n",
      "batch 779, loss: 0.0520, instance_loss: 0.0488, weighted_loss: 0.0510, label: 0, bag_size: 1920\n",
      "batch 799, loss: 0.4259, instance_loss: 0.6622, weighted_loss: 0.4968, label: 1, bag_size: 2937\n",
      "batch 819, loss: 0.1393, instance_loss: 0.1020, weighted_loss: 0.1281, label: 1, bag_size: 15609\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.980640243902439: correct 12866/13120\n",
      "class 1 clustering acc 0.9114329268292682: correct 5979/6560\n",
      "Epoch: 50, train_loss: 0.1559, train_clustering_loss:  0.1846, train_error: 0.0573\n",
      "class 0: acc 0.9362745098039216, correct 382/408\n",
      "class 1: acc 0.9490291262135923, correct 391/412\n",
      "\n",
      "Val Set, val_loss: 0.2942, val_error: 0.1273, auc: 0.9791\n",
      "class 0 clustering acc 0.9954545454545455: correct 1752/1760\n",
      "class 1 clustering acc 0.035227272727272725: correct 31/880\n",
      "class 0: acc 0.9807692307692307, correct 51/52\n",
      "class 1: acc 0.7758620689655172, correct 45/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0005, instance_loss: 0.0009, weighted_loss: 0.0006, label: 0, bag_size: 13777\n",
      "batch 39, loss: 0.0016, instance_loss: 0.0001, weighted_loss: 0.0012, label: 0, bag_size: 13225\n",
      "batch 59, loss: 0.0147, instance_loss: 0.0119, weighted_loss: 0.0138, label: 1, bag_size: 3651\n",
      "batch 79, loss: 0.0005, instance_loss: 0.0030, weighted_loss: 0.0013, label: 0, bag_size: 1213\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8330\n",
      "batch 119, loss: 0.0006, instance_loss: 0.0013, weighted_loss: 0.0008, label: 1, bag_size: 5991\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 13892\n",
      "batch 159, loss: 0.0398, instance_loss: 0.0283, weighted_loss: 0.0363, label: 1, bag_size: 5231\n",
      "batch 179, loss: 0.0135, instance_loss: 0.0133, weighted_loss: 0.0134, label: 0, bag_size: 803\n",
      "batch 199, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 5991\n",
      "batch 219, loss: 3.4005, instance_loss: 3.9714, weighted_loss: 3.5717, label: 0, bag_size: 5105\n",
      "batch 239, loss: 0.0391, instance_loss: 0.0243, weighted_loss: 0.0347, label: 1, bag_size: 10492\n",
      "batch 259, loss: 0.0026, instance_loss: 0.0033, weighted_loss: 0.0028, label: 0, bag_size: 23796\n",
      "batch 279, loss: 0.0003, instance_loss: 0.0007, weighted_loss: 0.0005, label: 0, bag_size: 21682\n",
      "batch 299, loss: 0.3868, instance_loss: 0.5028, weighted_loss: 0.4216, label: 1, bag_size: 5366\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 1, bag_size: 10725\n",
      "batch 339, loss: 0.0025, instance_loss: 0.0000, weighted_loss: 0.0018, label: 1, bag_size: 10501\n",
      "batch 359, loss: 0.0108, instance_loss: 0.0085, weighted_loss: 0.0101, label: 1, bag_size: 13692\n",
      "batch 379, loss: 0.0245, instance_loss: 0.0182, weighted_loss: 0.0226, label: 0, bag_size: 12910\n",
      "batch 399, loss: 0.0006, instance_loss: 0.0006, weighted_loss: 0.0006, label: 0, bag_size: 1213\n",
      "batch 419, loss: 0.0012, instance_loss: 0.0012, weighted_loss: 0.0012, label: 0, bag_size: 23368\n",
      "batch 439, loss: 0.0065, instance_loss: 0.0076, weighted_loss: 0.0068, label: 1, bag_size: 5454\n",
      "batch 459, loss: 0.0000, instance_loss: 0.3404, weighted_loss: 0.1021, label: 1, bag_size: 10867\n",
      "batch 479, loss: 0.0249, instance_loss: 0.0323, weighted_loss: 0.0271, label: 1, bag_size: 21827\n",
      "batch 499, loss: 0.0192, instance_loss: 0.0260, weighted_loss: 0.0213, label: 1, bag_size: 1919\n",
      "batch 519, loss: 0.0637, instance_loss: 0.0603, weighted_loss: 0.0626, label: 0, bag_size: 25814\n",
      "batch 539, loss: 0.0021, instance_loss: 0.0015, weighted_loss: 0.0019, label: 1, bag_size: 14030\n",
      "batch 559, loss: 0.0098, instance_loss: 0.0076, weighted_loss: 0.0092, label: 1, bag_size: 5605\n",
      "batch 579, loss: 0.0346, instance_loss: 0.0207, weighted_loss: 0.0304, label: 1, bag_size: 11223\n",
      "batch 599, loss: 0.5834, instance_loss: 0.6387, weighted_loss: 0.6000, label: 1, bag_size: 1831\n",
      "batch 619, loss: 0.0019, instance_loss: 0.0009, weighted_loss: 0.0016, label: 1, bag_size: 7246\n",
      "batch 639, loss: 0.0015, instance_loss: 0.0026, weighted_loss: 0.0018, label: 0, bag_size: 1202\n",
      "batch 659, loss: 0.0130, instance_loss: 0.0205, weighted_loss: 0.0152, label: 1, bag_size: 3082\n",
      "batch 679, loss: 0.0653, instance_loss: 0.0485, weighted_loss: 0.0602, label: 0, bag_size: 803\n",
      "batch 699, loss: 0.3521, instance_loss: 0.4063, weighted_loss: 0.3684, label: 1, bag_size: 2179\n",
      "batch 719, loss: 0.0175, instance_loss: 0.0104, weighted_loss: 0.0154, label: 1, bag_size: 10281\n",
      "batch 739, loss: 0.0051, instance_loss: 0.0046, weighted_loss: 0.0049, label: 0, bag_size: 14206\n",
      "batch 759, loss: 0.0104, instance_loss: 0.0072, weighted_loss: 0.0095, label: 0, bag_size: 22681\n",
      "batch 779, loss: 0.0032, instance_loss: 0.0018, weighted_loss: 0.0028, label: 0, bag_size: 2814\n",
      "batch 799, loss: 0.0039, instance_loss: 0.0005, weighted_loss: 0.0029, label: 0, bag_size: 10365\n",
      "batch 819, loss: 0.0045, instance_loss: 0.0030, weighted_loss: 0.0041, label: 1, bag_size: 12895\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9789634146341464: correct 12844/13120\n",
      "class 1 clustering acc 0.8807926829268292: correct 5778/6560\n",
      "Epoch: 51, train_loss: 0.1713, train_clustering_loss:  0.1989, train_error: 0.0768\n",
      "class 0: acc 0.9168765743073047, correct 364/397\n",
      "class 1: acc 0.9290780141843972, correct 393/423\n",
      "\n",
      "Val Set, val_loss: 0.3767, val_error: 0.1909, auc: 0.9188\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.01818181818181818: correct 16/880\n",
      "class 0: acc 0.6923076923076923, correct 36/52\n",
      "class 1: acc 0.9137931034482759, correct 53/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5845, instance_loss: 0.7318, weighted_loss: 0.6287, label: 1, bag_size: 8103\n",
      "batch 39, loss: 0.0020, instance_loss: 0.0012, weighted_loss: 0.0017, label: 1, bag_size: 13255\n",
      "batch 59, loss: 0.0068, instance_loss: 0.0040, weighted_loss: 0.0059, label: 0, bag_size: 3893\n",
      "batch 79, loss: 0.0010, instance_loss: 0.0005, weighted_loss: 0.0009, label: 1, bag_size: 5494\n",
      "batch 99, loss: 0.0084, instance_loss: 0.0027, weighted_loss: 0.0067, label: 1, bag_size: 4308\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 0, bag_size: 3552\n",
      "batch 139, loss: 0.0366, instance_loss: 0.0201, weighted_loss: 0.0317, label: 1, bag_size: 1831\n",
      "batch 159, loss: 0.5615, instance_loss: 0.6435, weighted_loss: 0.5861, label: 0, bag_size: 13332\n",
      "batch 179, loss: 0.0004, instance_loss: 0.0126, weighted_loss: 0.0040, label: 1, bag_size: 2140\n",
      "batch 199, loss: 0.0997, instance_loss: 0.1610, weighted_loss: 0.1181, label: 0, bag_size: 1800\n",
      "batch 219, loss: 0.0639, instance_loss: 0.0685, weighted_loss: 0.0653, label: 1, bag_size: 6682\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 15008\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 6356\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0018, weighted_loss: 0.0006, label: 1, bag_size: 14681\n",
      "batch 299, loss: 0.1765, instance_loss: 0.1198, weighted_loss: 0.1595, label: 0, bag_size: 4523\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 1, bag_size: 3082\n",
      "batch 339, loss: 0.3002, instance_loss: 0.2675, weighted_loss: 0.2904, label: 1, bag_size: 12712\n",
      "batch 359, loss: 0.0028, instance_loss: 0.0019, weighted_loss: 0.0026, label: 0, bag_size: 13205\n",
      "batch 379, loss: 0.1603, instance_loss: 0.2005, weighted_loss: 0.1724, label: 1, bag_size: 8103\n",
      "batch 399, loss: 0.0041, instance_loss: 0.0024, weighted_loss: 0.0036, label: 1, bag_size: 5516\n",
      "batch 419, loss: 0.0818, instance_loss: 0.0518, weighted_loss: 0.0728, label: 0, bag_size: 10995\n",
      "batch 439, loss: 0.0017, instance_loss: 0.0012, weighted_loss: 0.0016, label: 1, bag_size: 30675\n",
      "batch 459, loss: 1.3121, instance_loss: 1.9054, weighted_loss: 1.4901, label: 1, bag_size: 1703\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 4423\n",
      "batch 499, loss: 0.0438, instance_loss: 0.0321, weighted_loss: 0.0403, label: 0, bag_size: 16690\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 15001\n",
      "batch 539, loss: 3.2860, instance_loss: 3.6366, weighted_loss: 3.3911, label: 0, bag_size: 2815\n",
      "batch 559, loss: 0.0284, instance_loss: 0.0099, weighted_loss: 0.0229, label: 1, bag_size: 9571\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 15967\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 11778\n",
      "batch 619, loss: 0.0080, instance_loss: 0.0060, weighted_loss: 0.0074, label: 1, bag_size: 1255\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0003, weighted_loss: 0.0002, label: 1, bag_size: 9673\n",
      "batch 659, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 13174\n",
      "batch 679, loss: 0.0003, instance_loss: 0.0001, weighted_loss: 0.0002, label: 0, bag_size: 2548\n",
      "batch 699, loss: 0.0054, instance_loss: 0.0053, weighted_loss: 0.0053, label: 1, bag_size: 15125\n",
      "batch 719, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 1, bag_size: 6734\n",
      "batch 739, loss: 0.0090, instance_loss: 0.0113, weighted_loss: 0.0097, label: 0, bag_size: 1814\n",
      "batch 759, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 16052\n",
      "batch 779, loss: 0.2059, instance_loss: 0.1620, weighted_loss: 0.1928, label: 1, bag_size: 2935\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14319\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 21682\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9817835365853659: correct 12881/13120\n",
      "class 1 clustering acc 0.9102134146341463: correct 5971/6560\n",
      "Epoch: 52, train_loss: 0.1470, train_clustering_loss:  0.1740, train_error: 0.0573\n",
      "class 0: acc 0.9458128078817734, correct 384/406\n",
      "class 1: acc 0.9396135265700483, correct 389/414\n",
      "\n",
      "Val Set, val_loss: 0.2236, val_error: 0.0818, auc: 0.9735\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.8653846153846154, correct 45/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1157, instance_loss: 0.1221, weighted_loss: 0.1176, label: 1, bag_size: 5454\n",
      "batch 39, loss: 0.0004, instance_loss: 0.0002, weighted_loss: 0.0004, label: 1, bag_size: 13947\n",
      "batch 59, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 1701\n",
      "batch 79, loss: 0.0306, instance_loss: 0.0104, weighted_loss: 0.0245, label: 1, bag_size: 9561\n",
      "batch 99, loss: 0.0024, instance_loss: 0.0018, weighted_loss: 0.0022, label: 1, bag_size: 2936\n",
      "batch 119, loss: 0.0169, instance_loss: 0.0142, weighted_loss: 0.0161, label: 1, bag_size: 3980\n",
      "batch 139, loss: 0.0165, instance_loss: 0.0122, weighted_loss: 0.0152, label: 0, bag_size: 2336\n",
      "batch 159, loss: 0.0004, instance_loss: 0.0018, weighted_loss: 0.0008, label: 1, bag_size: 16051\n",
      "batch 179, loss: 0.0101, instance_loss: 0.0193, weighted_loss: 0.0129, label: 1, bag_size: 6736\n",
      "batch 199, loss: 0.0042, instance_loss: 0.0021, weighted_loss: 0.0035, label: 0, bag_size: 9949\n",
      "batch 219, loss: 0.4327, instance_loss: 0.5064, weighted_loss: 0.4548, label: 1, bag_size: 5903\n",
      "batch 239, loss: 0.2233, instance_loss: 0.2234, weighted_loss: 0.2233, label: 0, bag_size: 1506\n",
      "batch 259, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 12217\n",
      "batch 279, loss: 0.0020, instance_loss: 0.0008, weighted_loss: 0.0017, label: 1, bag_size: 12758\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 6966\n",
      "batch 319, loss: 0.9704, instance_loss: 1.2579, weighted_loss: 1.0566, label: 1, bag_size: 2731\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 6875\n",
      "batch 359, loss: 0.0006, instance_loss: 0.0001, weighted_loss: 0.0005, label: 0, bag_size: 2098\n",
      "batch 379, loss: 0.0115, instance_loss: 0.0150, weighted_loss: 0.0125, label: 0, bag_size: 12212\n",
      "batch 399, loss: 0.0012, instance_loss: 0.0008, weighted_loss: 0.0011, label: 0, bag_size: 10068\n",
      "batch 419, loss: 1.3395, instance_loss: 1.8247, weighted_loss: 1.4851, label: 1, bag_size: 898\n",
      "batch 439, loss: 0.0020, instance_loss: 0.0006, weighted_loss: 0.0016, label: 1, bag_size: 16267\n",
      "batch 459, loss: 0.9648, instance_loss: 1.0889, weighted_loss: 1.0020, label: 1, bag_size: 7424\n",
      "batch 479, loss: 0.0437, instance_loss: 0.0226, weighted_loss: 0.0373, label: 0, bag_size: 2814\n",
      "batch 499, loss: 0.3202, instance_loss: 0.3486, weighted_loss: 0.3287, label: 1, bag_size: 1609\n",
      "batch 519, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 16267\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 5221\n",
      "batch 559, loss: 0.0163, instance_loss: 0.0089, weighted_loss: 0.0141, label: 0, bag_size: 2998\n",
      "batch 579, loss: 0.0366, instance_loss: 0.0273, weighted_loss: 0.0338, label: 1, bag_size: 8475\n",
      "batch 599, loss: 0.0094, instance_loss: 0.0083, weighted_loss: 0.0091, label: 1, bag_size: 6343\n",
      "batch 619, loss: 0.0403, instance_loss: 0.0419, weighted_loss: 0.0408, label: 1, bag_size: 5340\n",
      "batch 639, loss: 0.0354, instance_loss: 0.0204, weighted_loss: 0.0309, label: 1, bag_size: 1294\n",
      "batch 659, loss: 0.1845, instance_loss: 0.1815, weighted_loss: 0.1836, label: 1, bag_size: 1755\n",
      "batch 679, loss: 0.0038, instance_loss: 0.0019, weighted_loss: 0.0032, label: 1, bag_size: 4250\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 18225\n",
      "batch 719, loss: 0.0008, instance_loss: 0.0004, weighted_loss: 0.0006, label: 0, bag_size: 14305\n",
      "batch 739, loss: 0.0100, instance_loss: 0.0055, weighted_loss: 0.0087, label: 0, bag_size: 1452\n",
      "batch 759, loss: 0.0245, instance_loss: 0.0187, weighted_loss: 0.0227, label: 0, bag_size: 47866\n",
      "batch 779, loss: 0.0099, instance_loss: 0.0115, weighted_loss: 0.0104, label: 1, bag_size: 2278\n",
      "batch 799, loss: 0.6002, instance_loss: 0.7180, weighted_loss: 0.6355, label: 0, bag_size: 1437\n",
      "batch 819, loss: 0.0011, instance_loss: 0.0007, weighted_loss: 0.0010, label: 1, bag_size: 2695\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.978125: correct 12833/13120\n",
      "class 1 clustering acc 0.8899390243902439: correct 5838/6560\n",
      "Epoch: 53, train_loss: 0.1733, train_clustering_loss:  0.2016, train_error: 0.0683\n",
      "class 0: acc 0.9364303178484108, correct 383/409\n",
      "class 1: acc 0.927007299270073, correct 381/411\n",
      "\n",
      "Val Set, val_loss: 0.2234, val_error: 0.1000, auc: 0.9765\n",
      "class 0 clustering acc 0.990909090909091: correct 1744/1760\n",
      "class 1 clustering acc 0.06136363636363636: correct 54/880\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0152, instance_loss: 0.0145, weighted_loss: 0.0149, label: 0, bag_size: 2063\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 1984\n",
      "batch 59, loss: 0.0076, instance_loss: 0.0071, weighted_loss: 0.0075, label: 1, bag_size: 8438\n",
      "batch 79, loss: 0.0031, instance_loss: 0.0019, weighted_loss: 0.0028, label: 1, bag_size: 1014\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9689\n",
      "batch 119, loss: 0.1100, instance_loss: 0.1580, weighted_loss: 0.1244, label: 1, bag_size: 2937\n",
      "batch 139, loss: 0.0003, instance_loss: 0.0007, weighted_loss: 0.0004, label: 1, bag_size: 16565\n",
      "batch 159, loss: 0.1396, instance_loss: 0.1163, weighted_loss: 0.1326, label: 0, bag_size: 705\n",
      "batch 179, loss: 0.0008, instance_loss: 0.0006, weighted_loss: 0.0007, label: 0, bag_size: 1202\n",
      "batch 199, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 1891\n",
      "batch 219, loss: 0.0150, instance_loss: 0.0144, weighted_loss: 0.0148, label: 0, bag_size: 1920\n",
      "batch 239, loss: 0.0989, instance_loss: 0.0827, weighted_loss: 0.0941, label: 1, bag_size: 8040\n",
      "batch 259, loss: 0.0004, instance_loss: 0.0014, weighted_loss: 0.0007, label: 1, bag_size: 2638\n",
      "batch 279, loss: 0.0511, instance_loss: 0.0529, weighted_loss: 0.0517, label: 1, bag_size: 699\n",
      "batch 299, loss: 0.0004, instance_loss: 0.0012, weighted_loss: 0.0006, label: 1, bag_size: 6792\n",
      "batch 319, loss: 0.0033, instance_loss: 0.0085, weighted_loss: 0.0048, label: 0, bag_size: 5297\n",
      "batch 339, loss: 0.2308, instance_loss: 0.2326, weighted_loss: 0.2314, label: 0, bag_size: 4959\n",
      "batch 359, loss: 0.0014, instance_loss: 0.0052, weighted_loss: 0.0025, label: 1, bag_size: 7515\n",
      "batch 379, loss: 0.0078, instance_loss: 0.0079, weighted_loss: 0.0078, label: 0, bag_size: 8788\n",
      "batch 399, loss: 0.0022, instance_loss: 0.0017, weighted_loss: 0.0020, label: 1, bag_size: 4821\n",
      "batch 419, loss: 0.0021, instance_loss: 0.0006, weighted_loss: 0.0016, label: 0, bag_size: 27158\n",
      "batch 439, loss: 0.7836, instance_loss: 0.9654, weighted_loss: 0.8382, label: 1, bag_size: 1683\n",
      "batch 459, loss: 0.3673, instance_loss: 0.3569, weighted_loss: 0.3642, label: 0, bag_size: 1814\n",
      "batch 479, loss: 0.0003, instance_loss: 0.0007, weighted_loss: 0.0004, label: 1, bag_size: 6745\n",
      "batch 499, loss: 0.0032, instance_loss: 0.0087, weighted_loss: 0.0048, label: 1, bag_size: 6090\n",
      "batch 519, loss: 0.0071, instance_loss: 0.0074, weighted_loss: 0.0072, label: 1, bag_size: 3980\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 1, bag_size: 10392\n",
      "batch 559, loss: 0.0115, instance_loss: 0.0175, weighted_loss: 0.0133, label: 1, bag_size: 1512\n",
      "batch 579, loss: 0.0089, instance_loss: 0.0172, weighted_loss: 0.0114, label: 1, bag_size: 8602\n",
      "batch 599, loss: 0.0223, instance_loss: 0.0131, weighted_loss: 0.0196, label: 1, bag_size: 12931\n",
      "batch 619, loss: 0.0578, instance_loss: 0.1330, weighted_loss: 0.0804, label: 1, bag_size: 5454\n",
      "batch 639, loss: 0.3030, instance_loss: 0.5449, weighted_loss: 0.3756, label: 0, bag_size: 2098\n",
      "batch 659, loss: 0.0056, instance_loss: 0.0066, weighted_loss: 0.0059, label: 1, bag_size: 11363\n",
      "batch 679, loss: 0.0056, instance_loss: 0.0067, weighted_loss: 0.0059, label: 1, bag_size: 1294\n",
      "batch 699, loss: 0.0926, instance_loss: 0.0405, weighted_loss: 0.0770, label: 0, bag_size: 2998\n",
      "batch 719, loss: 0.0004, instance_loss: 0.0047, weighted_loss: 0.0017, label: 1, bag_size: 5864\n",
      "batch 739, loss: 0.0047, instance_loss: 0.0035, weighted_loss: 0.0043, label: 1, bag_size: 13947\n",
      "batch 759, loss: 0.0031, instance_loss: 0.0002, weighted_loss: 0.0022, label: 0, bag_size: 11194\n",
      "batch 779, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 4465\n",
      "batch 799, loss: 0.0035, instance_loss: 0.0033, weighted_loss: 0.0034, label: 1, bag_size: 3640\n",
      "batch 819, loss: 0.0147, instance_loss: 0.0090, weighted_loss: 0.0130, label: 0, bag_size: 11390\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9805640243902439: correct 12865/13120\n",
      "class 1 clustering acc 0.8841463414634146: correct 5800/6560\n",
      "Epoch: 54, train_loss: 0.1755, train_clustering_loss:  0.2073, train_error: 0.0707\n",
      "class 0: acc 0.9342723004694836, correct 398/426\n",
      "class 1: acc 0.9238578680203046, correct 364/394\n",
      "\n",
      "Val Set, val_loss: 0.2019, val_error: 0.1091, auc: 0.9814\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.019318181818181818: correct 17/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8275862068965517, correct 48/58\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11113\n",
      "batch 39, loss: 0.0048, instance_loss: 0.0027, weighted_loss: 0.0041, label: 0, bag_size: 3970\n",
      "batch 59, loss: 0.0053, instance_loss: 0.0048, weighted_loss: 0.0051, label: 1, bag_size: 6734\n",
      "batch 79, loss: 0.0681, instance_loss: 0.0482, weighted_loss: 0.0621, label: 1, bag_size: 11316\n",
      "batch 99, loss: 2.9413, instance_loss: 3.6354, weighted_loss: 3.1495, label: 1, bag_size: 8103\n",
      "batch 119, loss: 0.0026, instance_loss: 0.0003, weighted_loss: 0.0019, label: 0, bag_size: 2654\n",
      "batch 139, loss: 0.0266, instance_loss: 0.0184, weighted_loss: 0.0242, label: 0, bag_size: 24439\n",
      "batch 159, loss: 0.0330, instance_loss: 0.0232, weighted_loss: 0.0301, label: 0, bag_size: 2628\n",
      "batch 179, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 13795\n",
      "batch 199, loss: 0.0003, instance_loss: 0.0003, weighted_loss: 0.0003, label: 1, bag_size: 8019\n",
      "batch 219, loss: 0.0017, instance_loss: 0.0020, weighted_loss: 0.0018, label: 1, bag_size: 5907\n",
      "batch 239, loss: 0.0001, instance_loss: 0.0011, weighted_loss: 0.0004, label: 1, bag_size: 8522\n",
      "batch 259, loss: 0.0114, instance_loss: 0.0089, weighted_loss: 0.0107, label: 0, bag_size: 3810\n",
      "batch 279, loss: 2.4122, instance_loss: 2.8441, weighted_loss: 2.5417, label: 0, bag_size: 13332\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15841\n",
      "batch 319, loss: 0.1818, instance_loss: 0.1957, weighted_loss: 0.1860, label: 0, bag_size: 2213\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 18738\n",
      "batch 359, loss: 0.6880, instance_loss: 0.7576, weighted_loss: 0.7089, label: 0, bag_size: 2918\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 22800\n",
      "batch 399, loss: 0.0034, instance_loss: 0.0014, weighted_loss: 0.0028, label: 0, bag_size: 15001\n",
      "batch 419, loss: 0.0068, instance_loss: 0.0150, weighted_loss: 0.0092, label: 1, bag_size: 18095\n",
      "batch 439, loss: 0.9687, instance_loss: 1.1534, weighted_loss: 1.0241, label: 0, bag_size: 2694\n",
      "batch 459, loss: 0.0012, instance_loss: 0.0006, weighted_loss: 0.0010, label: 1, bag_size: 10281\n",
      "batch 479, loss: 0.0439, instance_loss: 0.0628, weighted_loss: 0.0496, label: 1, bag_size: 1038\n",
      "batch 499, loss: 0.5183, instance_loss: 0.5552, weighted_loss: 0.5293, label: 1, bag_size: 7768\n",
      "batch 519, loss: 0.0033, instance_loss: 0.0087, weighted_loss: 0.0050, label: 1, bag_size: 1924\n",
      "batch 539, loss: 0.0028, instance_loss: 0.0012, weighted_loss: 0.0023, label: 1, bag_size: 16565\n",
      "batch 559, loss: 0.0016, instance_loss: 0.0016, weighted_loss: 0.0016, label: 1, bag_size: 10394\n",
      "batch 579, loss: 0.0003, instance_loss: 0.0004, weighted_loss: 0.0004, label: 1, bag_size: 8685\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 1, bag_size: 10392\n",
      "batch 619, loss: 0.0114, instance_loss: 0.0058, weighted_loss: 0.0098, label: 1, bag_size: 10912\n",
      "batch 639, loss: 0.0005, instance_loss: 0.0038, weighted_loss: 0.0015, label: 0, bag_size: 11759\n",
      "batch 659, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 20150\n",
      "batch 679, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 2140\n",
      "batch 699, loss: 1.6605, instance_loss: 2.1824, weighted_loss: 1.8171, label: 1, bag_size: 21450\n",
      "batch 719, loss: 0.0089, instance_loss: 0.0062, weighted_loss: 0.0081, label: 1, bag_size: 10622\n",
      "batch 739, loss: 0.0195, instance_loss: 0.0243, weighted_loss: 0.0210, label: 0, bag_size: 1483\n",
      "batch 759, loss: 0.0013, instance_loss: 0.0007, weighted_loss: 0.0011, label: 1, bag_size: 11964\n",
      "batch 779, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 22828\n",
      "batch 799, loss: 0.0954, instance_loss: 0.0229, weighted_loss: 0.0736, label: 1, bag_size: 2638\n",
      "batch 819, loss: 0.0042, instance_loss: 0.0029, weighted_loss: 0.0038, label: 1, bag_size: 19832\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.979344512195122: correct 12849/13120\n",
      "class 1 clustering acc 0.8972560975609756: correct 5886/6560\n",
      "Epoch: 55, train_loss: 0.1577, train_clustering_loss:  0.1888, train_error: 0.0659\n",
      "class 0: acc 0.9371980676328503, correct 388/414\n",
      "class 1: acc 0.9310344827586207, correct 378/406\n",
      "\n",
      "Val Set, val_loss: 0.1903, val_error: 0.0818, auc: 0.9778\n",
      "class 0 clustering acc 0.9954545454545455: correct 1752/1760\n",
      "class 1 clustering acc 0.08636363636363636: correct 76/880\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0032, instance_loss: 0.0000, weighted_loss: 0.0023, label: 0, bag_size: 9234\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0023, weighted_loss: 0.0007, label: 0, bag_size: 23037\n",
      "batch 59, loss: 0.0004, instance_loss: 0.0003, weighted_loss: 0.0003, label: 0, bag_size: 9888\n",
      "batch 79, loss: 2.1402, instance_loss: 3.0685, weighted_loss: 2.4187, label: 1, bag_size: 898\n",
      "batch 99, loss: 1.2925, instance_loss: 1.1561, weighted_loss: 1.2516, label: 0, bag_size: 2351\n",
      "batch 119, loss: 0.0113, instance_loss: 0.0062, weighted_loss: 0.0098, label: 0, bag_size: 1614\n",
      "batch 139, loss: 0.0332, instance_loss: 0.0328, weighted_loss: 0.0330, label: 1, bag_size: 15689\n",
      "batch 159, loss: 0.0004, instance_loss: 0.0009, weighted_loss: 0.0005, label: 1, bag_size: 2136\n",
      "batch 179, loss: 0.0118, instance_loss: 0.0031, weighted_loss: 0.0092, label: 0, bag_size: 10415\n",
      "batch 199, loss: 0.0499, instance_loss: 0.0469, weighted_loss: 0.0490, label: 0, bag_size: 2694\n",
      "batch 219, loss: 0.0004, instance_loss: 0.0002, weighted_loss: 0.0003, label: 0, bag_size: 12593\n",
      "batch 239, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 22870\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21076\n",
      "batch 279, loss: 0.0719, instance_loss: 0.0580, weighted_loss: 0.0677, label: 0, bag_size: 9132\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0066, weighted_loss: 0.0021, label: 1, bag_size: 865\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 1712\n",
      "batch 339, loss: 0.0004, instance_loss: 0.1294, weighted_loss: 0.0391, label: 1, bag_size: 2146\n",
      "batch 359, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 4271\n",
      "batch 379, loss: 0.6948, instance_loss: 0.8177, weighted_loss: 0.7317, label: 1, bag_size: 7351\n",
      "batch 399, loss: 0.0165, instance_loss: 0.0175, weighted_loss: 0.0168, label: 0, bag_size: 14305\n",
      "batch 419, loss: 0.0011, instance_loss: 0.0016, weighted_loss: 0.0013, label: 1, bag_size: 6736\n",
      "batch 439, loss: 0.0084, instance_loss: 0.0058, weighted_loss: 0.0076, label: 1, bag_size: 21827\n",
      "batch 459, loss: 0.0090, instance_loss: 0.0083, weighted_loss: 0.0088, label: 1, bag_size: 10498\n",
      "batch 479, loss: 1.6048, instance_loss: 2.0891, weighted_loss: 1.7501, label: 1, bag_size: 13089\n",
      "batch 499, loss: 0.1907, instance_loss: 0.2262, weighted_loss: 0.2013, label: 0, bag_size: 11607\n",
      "batch 519, loss: 0.0579, instance_loss: 0.0417, weighted_loss: 0.0530, label: 1, bag_size: 12626\n",
      "batch 539, loss: 0.0154, instance_loss: 0.0140, weighted_loss: 0.0150, label: 0, bag_size: 25420\n",
      "batch 559, loss: 0.0050, instance_loss: 0.0034, weighted_loss: 0.0045, label: 1, bag_size: 11363\n",
      "batch 579, loss: 0.0244, instance_loss: 0.0243, weighted_loss: 0.0243, label: 1, bag_size: 10622\n",
      "batch 599, loss: 0.0287, instance_loss: 0.0335, weighted_loss: 0.0302, label: 0, bag_size: 13992\n",
      "batch 619, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 11654\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 1, bag_size: 8191\n",
      "batch 659, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 3970\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 22828\n",
      "batch 699, loss: 0.0024, instance_loss: 0.0016, weighted_loss: 0.0021, label: 1, bag_size: 2146\n",
      "batch 719, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 18240\n",
      "batch 739, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 1, bag_size: 5864\n",
      "batch 759, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 2036\n",
      "batch 779, loss: 0.1258, instance_loss: 0.0568, weighted_loss: 0.1051, label: 1, bag_size: 19606\n",
      "batch 799, loss: 0.2614, instance_loss: 0.1658, weighted_loss: 0.2327, label: 1, bag_size: 9215\n",
      "batch 819, loss: 0.1643, instance_loss: 0.2160, weighted_loss: 0.1798, label: 1, bag_size: 2480\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9801067073170732: correct 12859/13120\n",
      "class 1 clustering acc 0.9001524390243902: correct 5905/6560\n",
      "Epoch: 56, train_loss: 0.1499, train_clustering_loss:  0.1866, train_error: 0.0610\n",
      "class 0: acc 0.9423963133640553, correct 409/434\n",
      "class 1: acc 0.9352331606217616, correct 361/386\n",
      "\n",
      "Val Set, val_loss: 0.2512, val_error: 0.1000, auc: 0.9768\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.020454545454545454: correct 18/880\n",
      "class 0: acc 0.9807692307692307, correct 51/52\n",
      "class 1: acc 0.8275862068965517, correct 48/58\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0046, instance_loss: 0.0033, weighted_loss: 0.0042, label: 1, bag_size: 3980\n",
      "batch 39, loss: 0.0007, instance_loss: 0.0020, weighted_loss: 0.0011, label: 0, bag_size: 8981\n",
      "batch 59, loss: 0.0204, instance_loss: 0.0163, weighted_loss: 0.0192, label: 0, bag_size: 8549\n",
      "batch 79, loss: 0.0807, instance_loss: 0.0661, weighted_loss: 0.0764, label: 0, bag_size: 9455\n",
      "batch 99, loss: 0.0033, instance_loss: 0.0062, weighted_loss: 0.0042, label: 1, bag_size: 7613\n",
      "batch 119, loss: 0.0144, instance_loss: 0.0112, weighted_loss: 0.0134, label: 0, bag_size: 1639\n",
      "batch 139, loss: 0.8787, instance_loss: 1.0531, weighted_loss: 0.9310, label: 1, bag_size: 3121\n",
      "batch 159, loss: 0.0634, instance_loss: 0.0512, weighted_loss: 0.0597, label: 0, bag_size: 2266\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0005, label: 1, bag_size: 10920\n",
      "batch 199, loss: 0.0892, instance_loss: 0.0889, weighted_loss: 0.0891, label: 1, bag_size: 1444\n",
      "batch 219, loss: 0.0012, instance_loss: 0.0006, weighted_loss: 0.0010, label: 0, bag_size: 19043\n",
      "batch 239, loss: 0.0013, instance_loss: 0.0005, weighted_loss: 0.0011, label: 0, bag_size: 1884\n",
      "batch 259, loss: 0.0994, instance_loss: 0.1063, weighted_loss: 0.1015, label: 1, bag_size: 2455\n",
      "batch 279, loss: 0.0003, instance_loss: 0.0003, weighted_loss: 0.0003, label: 1, bag_size: 11363\n",
      "batch 299, loss: 0.0008, instance_loss: 0.0004, weighted_loss: 0.0007, label: 1, bag_size: 10482\n",
      "batch 319, loss: 0.0164, instance_loss: 0.0137, weighted_loss: 0.0156, label: 0, bag_size: 3670\n",
      "batch 339, loss: 0.0011, instance_loss: 0.0005, weighted_loss: 0.0009, label: 0, bag_size: 4271\n",
      "batch 359, loss: 1.2244, instance_loss: 2.5530, weighted_loss: 1.6230, label: 0, bag_size: 3897\n",
      "batch 379, loss: 0.0057, instance_loss: 0.0092, weighted_loss: 0.0068, label: 1, bag_size: 10432\n",
      "batch 399, loss: 0.0016, instance_loss: 0.0024, weighted_loss: 0.0018, label: 1, bag_size: 1823\n",
      "batch 419, loss: 0.0514, instance_loss: 0.0355, weighted_loss: 0.0466, label: 0, bag_size: 1349\n",
      "batch 439, loss: 0.0106, instance_loss: 0.0066, weighted_loss: 0.0094, label: 1, bag_size: 3640\n",
      "batch 459, loss: 0.0022, instance_loss: 0.0014, weighted_loss: 0.0019, label: 0, bag_size: 10146\n",
      "batch 479, loss: 0.0442, instance_loss: 0.0288, weighted_loss: 0.0396, label: 0, bag_size: 2624\n",
      "batch 499, loss: 0.0024, instance_loss: 0.0006, weighted_loss: 0.0019, label: 0, bag_size: 6898\n",
      "batch 519, loss: 0.6062, instance_loss: 0.7898, weighted_loss: 0.6613, label: 0, bag_size: 1142\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 8868\n",
      "batch 559, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 15213\n",
      "batch 579, loss: 0.0884, instance_loss: 0.1063, weighted_loss: 0.0938, label: 1, bag_size: 13692\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 19932\n",
      "batch 619, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 12178\n",
      "batch 639, loss: 0.0526, instance_loss: 0.0510, weighted_loss: 0.0521, label: 1, bag_size: 1284\n",
      "batch 659, loss: 0.0004, instance_loss: 0.0016, weighted_loss: 0.0008, label: 1, bag_size: 9878\n",
      "batch 679, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 2654\n",
      "batch 699, loss: 0.0013, instance_loss: 0.0014, weighted_loss: 0.0013, label: 1, bag_size: 7371\n",
      "batch 719, loss: 0.0011, instance_loss: 0.0006, weighted_loss: 0.0010, label: 0, bag_size: 11546\n",
      "batch 739, loss: 0.0014, instance_loss: 0.0019, weighted_loss: 0.0016, label: 1, bag_size: 16565\n",
      "batch 759, loss: 0.0234, instance_loss: 0.2845, weighted_loss: 0.1018, label: 1, bag_size: 2904\n",
      "batch 779, loss: 0.0020, instance_loss: 0.0012, weighted_loss: 0.0018, label: 1, bag_size: 2785\n",
      "batch 799, loss: 0.2240, instance_loss: 0.2214, weighted_loss: 0.2232, label: 0, bag_size: 9597\n",
      "batch 819, loss: 0.0373, instance_loss: 0.0311, weighted_loss: 0.0354, label: 0, bag_size: 1127\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9804115853658537: correct 12863/13120\n",
      "class 1 clustering acc 0.9024390243902439: correct 5920/6560\n",
      "Epoch: 57, train_loss: 0.1544, train_clustering_loss:  0.1845, train_error: 0.0659\n",
      "class 0: acc 0.9306666666666666, correct 349/375\n",
      "class 1: acc 0.9370786516853933, correct 417/445\n",
      "\n",
      "Val Set, val_loss: 0.1931, val_error: 0.0818, auc: 0.9808\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9955\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 15636\n",
      "batch 59, loss: 0.0409, instance_loss: 0.0235, weighted_loss: 0.0357, label: 0, bag_size: 2873\n",
      "batch 79, loss: 0.0637, instance_loss: 0.0529, weighted_loss: 0.0604, label: 0, bag_size: 24382\n",
      "batch 99, loss: 0.0248, instance_loss: 0.0194, weighted_loss: 0.0232, label: 1, bag_size: 8754\n",
      "batch 119, loss: 0.0030, instance_loss: 0.0016, weighted_loss: 0.0026, label: 0, bag_size: 10263\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 1888\n",
      "batch 159, loss: 0.0073, instance_loss: 0.0023, weighted_loss: 0.0058, label: 0, bag_size: 31780\n",
      "batch 179, loss: 0.0052, instance_loss: 0.0025, weighted_loss: 0.0044, label: 0, bag_size: 3265\n",
      "batch 199, loss: 0.1596, instance_loss: 0.1649, weighted_loss: 0.1612, label: 1, bag_size: 4929\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 1, bag_size: 9955\n",
      "batch 239, loss: 0.0106, instance_loss: 0.0033, weighted_loss: 0.0084, label: 0, bag_size: 10365\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0003, weighted_loss: 0.0002, label: 1, bag_size: 14230\n",
      "batch 279, loss: 0.4760, instance_loss: 0.3557, weighted_loss: 0.4399, label: 1, bag_size: 699\n",
      "batch 299, loss: 0.0040, instance_loss: 0.0026, weighted_loss: 0.0036, label: 0, bag_size: 12212\n",
      "batch 319, loss: 0.0311, instance_loss: 0.0224, weighted_loss: 0.0285, label: 0, bag_size: 16690\n",
      "batch 339, loss: 2.4170, instance_loss: 3.1098, weighted_loss: 2.6248, label: 1, bag_size: 1095\n",
      "batch 359, loss: 0.1860, instance_loss: 0.2203, weighted_loss: 0.1963, label: 0, bag_size: 1953\n",
      "batch 379, loss: 0.0080, instance_loss: 0.0051, weighted_loss: 0.0071, label: 0, bag_size: 13892\n",
      "batch 399, loss: 0.1500, instance_loss: 0.1528, weighted_loss: 0.1508, label: 0, bag_size: 2006\n",
      "batch 419, loss: 0.1075, instance_loss: 0.1033, weighted_loss: 0.1062, label: 0, bag_size: 1498\n",
      "batch 439, loss: 1.0240, instance_loss: 1.2951, weighted_loss: 1.1054, label: 1, bag_size: 6360\n",
      "batch 459, loss: 0.0103, instance_loss: 0.0067, weighted_loss: 0.0092, label: 1, bag_size: 6665\n",
      "batch 479, loss: 0.2841, instance_loss: 0.4100, weighted_loss: 0.3219, label: 1, bag_size: 1525\n",
      "batch 499, loss: 0.0020, instance_loss: 0.0005, weighted_loss: 0.0016, label: 1, bag_size: 8026\n",
      "batch 519, loss: 0.0003, instance_loss: 0.0001, weighted_loss: 0.0002, label: 0, bag_size: 12793\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 11518\n",
      "batch 559, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 9004\n",
      "batch 579, loss: 0.4487, instance_loss: 0.5733, weighted_loss: 0.4861, label: 1, bag_size: 12180\n",
      "batch 599, loss: 0.0491, instance_loss: 0.0320, weighted_loss: 0.0440, label: 1, bag_size: 10671\n",
      "batch 619, loss: 0.0022, instance_loss: 0.0038, weighted_loss: 0.0027, label: 1, bag_size: 2759\n",
      "batch 639, loss: 0.0055, instance_loss: 0.0116, weighted_loss: 0.0073, label: 0, bag_size: 2091\n",
      "batch 659, loss: 0.0001, instance_loss: 0.0007, weighted_loss: 0.0003, label: 1, bag_size: 9877\n",
      "batch 679, loss: 2.5097, instance_loss: 2.7422, weighted_loss: 2.5795, label: 0, bag_size: 7428\n",
      "batch 699, loss: 0.0033, instance_loss: 0.0029, weighted_loss: 0.0032, label: 1, bag_size: 2146\n",
      "batch 719, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 15464\n",
      "batch 739, loss: 0.1216, instance_loss: 0.1098, weighted_loss: 0.1181, label: 0, bag_size: 1789\n",
      "batch 759, loss: 0.6919, instance_loss: 1.0049, weighted_loss: 0.7858, label: 0, bag_size: 2213\n",
      "batch 779, loss: 0.0010, instance_loss: 0.0009, weighted_loss: 0.0010, label: 1, bag_size: 1638\n",
      "batch 799, loss: 0.0302, instance_loss: 0.0309, weighted_loss: 0.0304, label: 0, bag_size: 9851\n",
      "batch 819, loss: 0.0349, instance_loss: 0.0227, weighted_loss: 0.0312, label: 1, bag_size: 10671\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9855182926829268: correct 12930/13120\n",
      "class 1 clustering acc 0.9152439024390244: correct 6004/6560\n",
      "Epoch: 58, train_loss: 0.1298, train_clustering_loss:  0.1527, train_error: 0.0537\n",
      "class 0: acc 0.9502369668246445, correct 401/422\n",
      "class 1: acc 0.9422110552763819, correct 375/398\n",
      "\n",
      "Val Set, val_loss: 0.1980, val_error: 0.0818, auc: 0.9804\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0036, instance_loss: 0.0026, weighted_loss: 0.0033, label: 1, bag_size: 10072\n",
      "batch 39, loss: 0.0018, instance_loss: 0.0019, weighted_loss: 0.0019, label: 1, bag_size: 4821\n",
      "batch 59, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 3228\n",
      "batch 79, loss: 0.0767, instance_loss: 0.0614, weighted_loss: 0.0721, label: 0, bag_size: 21319\n",
      "batch 99, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 14515\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 3640\n",
      "batch 139, loss: 1.3313, instance_loss: 1.6945, weighted_loss: 1.4403, label: 1, bag_size: 1963\n",
      "batch 159, loss: 1.7698, instance_loss: 2.0587, weighted_loss: 1.8565, label: 0, bag_size: 9597\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 13947\n",
      "batch 199, loss: 1.4339, instance_loss: 1.7313, weighted_loss: 1.5232, label: 1, bag_size: 1703\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11387\n",
      "batch 239, loss: 0.4028, instance_loss: 0.5074, weighted_loss: 0.4342, label: 1, bag_size: 2179\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 5605\n",
      "batch 279, loss: 0.0159, instance_loss: 0.0108, weighted_loss: 0.0144, label: 0, bag_size: 12148\n",
      "batch 299, loss: 0.0024, instance_loss: 0.0018, weighted_loss: 0.0022, label: 0, bag_size: 2511\n",
      "batch 319, loss: 0.0013, instance_loss: 0.0180, weighted_loss: 0.0063, label: 1, bag_size: 6090\n",
      "batch 339, loss: 0.0726, instance_loss: 0.0838, weighted_loss: 0.0760, label: 0, bag_size: 3810\n",
      "batch 359, loss: 0.0020, instance_loss: 0.0012, weighted_loss: 0.0017, label: 0, bag_size: 10304\n",
      "batch 379, loss: 0.0097, instance_loss: 0.0079, weighted_loss: 0.0091, label: 0, bag_size: 2036\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 18225\n",
      "batch 419, loss: 0.0015, instance_loss: 0.0002, weighted_loss: 0.0011, label: 1, bag_size: 11518\n",
      "batch 439, loss: 0.0042, instance_loss: 0.0024, weighted_loss: 0.0036, label: 1, bag_size: 10622\n",
      "batch 459, loss: 0.0023, instance_loss: 0.0080, weighted_loss: 0.0040, label: 0, bag_size: 9433\n",
      "batch 479, loss: 0.0144, instance_loss: 0.0167, weighted_loss: 0.0151, label: 1, bag_size: 29832\n",
      "batch 499, loss: 0.3799, instance_loss: 0.4740, weighted_loss: 0.4081, label: 0, bag_size: 5120\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 15008\n",
      "batch 539, loss: 0.1498, instance_loss: 0.1712, weighted_loss: 0.1562, label: 0, bag_size: 2336\n",
      "batch 559, loss: 0.0008, instance_loss: 0.0004, weighted_loss: 0.0007, label: 0, bag_size: 3725\n",
      "batch 579, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 13786\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 6453\n",
      "batch 619, loss: 0.0663, instance_loss: 0.0518, weighted_loss: 0.0619, label: 1, bag_size: 5231\n",
      "batch 639, loss: 1.5681, instance_loss: 1.5772, weighted_loss: 1.5709, label: 1, bag_size: 1963\n",
      "batch 659, loss: 0.0189, instance_loss: 0.0236, weighted_loss: 0.0203, label: 0, bag_size: 10444\n",
      "batch 679, loss: 0.0198, instance_loss: 0.0135, weighted_loss: 0.0179, label: 1, bag_size: 18603\n",
      "batch 699, loss: 0.4158, instance_loss: 0.3567, weighted_loss: 0.3980, label: 1, bag_size: 5605\n",
      "batch 719, loss: 0.0030, instance_loss: 0.0012, weighted_loss: 0.0025, label: 0, bag_size: 8788\n",
      "batch 739, loss: 0.0008, instance_loss: 0.0010, weighted_loss: 0.0008, label: 1, bag_size: 3640\n",
      "batch 759, loss: 0.0513, instance_loss: 0.0521, weighted_loss: 0.0515, label: 1, bag_size: 1920\n",
      "batch 779, loss: 0.0083, instance_loss: 0.0066, weighted_loss: 0.0078, label: 0, bag_size: 15071\n",
      "batch 799, loss: 0.0035, instance_loss: 0.0033, weighted_loss: 0.0035, label: 1, bag_size: 5561\n",
      "batch 819, loss: 0.0148, instance_loss: 0.0086, weighted_loss: 0.0129, label: 0, bag_size: 3783\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9814786585365853: correct 12877/13120\n",
      "class 1 clustering acc 0.9170731707317074: correct 6016/6560\n",
      "Epoch: 59, train_loss: 0.1361, train_clustering_loss:  0.1619, train_error: 0.0500\n",
      "class 0: acc 0.9568345323741008, correct 399/417\n",
      "class 1: acc 0.9429280397022333, correct 380/403\n",
      "\n",
      "Val Set, val_loss: 0.2205, val_error: 0.1000, auc: 0.9745\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8448275862068966, correct 49/58\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0003, instance_loss: 0.0009, weighted_loss: 0.0005, label: 1, bag_size: 6731\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 1, bag_size: 16051\n",
      "batch 59, loss: 0.0082, instance_loss: 0.0063, weighted_loss: 0.0077, label: 0, bag_size: 19466\n",
      "batch 79, loss: 0.0179, instance_loss: 0.0123, weighted_loss: 0.0162, label: 0, bag_size: 1909\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 1622\n",
      "batch 119, loss: 0.3110, instance_loss: 0.4597, weighted_loss: 0.3556, label: 1, bag_size: 6842\n",
      "batch 139, loss: 0.3695, instance_loss: 0.4376, weighted_loss: 0.3900, label: 0, bag_size: 2290\n",
      "batch 159, loss: 0.0875, instance_loss: 0.1747, weighted_loss: 0.1136, label: 1, bag_size: 10105\n",
      "batch 179, loss: 0.4679, instance_loss: 0.5857, weighted_loss: 0.5032, label: 0, bag_size: 7989\n",
      "batch 199, loss: 1.3367, instance_loss: 1.5315, weighted_loss: 1.3951, label: 1, bag_size: 10848\n",
      "batch 219, loss: 0.0070, instance_loss: 0.0031, weighted_loss: 0.0059, label: 1, bag_size: 12626\n",
      "batch 239, loss: 0.0151, instance_loss: 0.0114, weighted_loss: 0.0140, label: 0, bag_size: 8866\n",
      "batch 259, loss: 0.2820, instance_loss: 0.2995, weighted_loss: 0.2873, label: 0, bag_size: 15071\n",
      "batch 279, loss: 0.9989, instance_loss: 1.2880, weighted_loss: 1.0856, label: 1, bag_size: 12719\n",
      "batch 299, loss: 0.4193, instance_loss: 0.5203, weighted_loss: 0.4496, label: 1, bag_size: 13089\n",
      "batch 319, loss: 0.0006, instance_loss: 0.0001, weighted_loss: 0.0005, label: 0, bag_size: 15841\n",
      "batch 339, loss: 0.0059, instance_loss: 0.0075, weighted_loss: 0.0064, label: 0, bag_size: 1745\n",
      "batch 359, loss: 0.0021, instance_loss: 0.0018, weighted_loss: 0.0020, label: 0, bag_size: 9171\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0038, weighted_loss: 0.0011, label: 1, bag_size: 11701\n",
      "batch 399, loss: 0.0028, instance_loss: 0.0023, weighted_loss: 0.0026, label: 0, bag_size: 7605\n",
      "batch 419, loss: 0.0125, instance_loss: 0.0247, weighted_loss: 0.0162, label: 1, bag_size: 1339\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 1884\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0028, weighted_loss: 0.0008, label: 0, bag_size: 5965\n",
      "batch 479, loss: 0.0015, instance_loss: 0.0015, weighted_loss: 0.0015, label: 1, bag_size: 14230\n",
      "batch 499, loss: 0.0030, instance_loss: 0.0058, weighted_loss: 0.0038, label: 1, bag_size: 3968\n",
      "batch 519, loss: 0.0665, instance_loss: 0.0737, weighted_loss: 0.0687, label: 1, bag_size: 16890\n",
      "batch 539, loss: 0.1137, instance_loss: 0.1236, weighted_loss: 0.1167, label: 1, bag_size: 8040\n",
      "batch 559, loss: 0.0387, instance_loss: 0.0504, weighted_loss: 0.0422, label: 0, bag_size: 2918\n",
      "batch 579, loss: 0.2600, instance_loss: 0.2564, weighted_loss: 0.2590, label: 0, bag_size: 7989\n",
      "batch 599, loss: 0.0974, instance_loss: 0.0755, weighted_loss: 0.0908, label: 0, bag_size: 1789\n",
      "batch 619, loss: 0.0160, instance_loss: 0.0202, weighted_loss: 0.0173, label: 0, bag_size: 3897\n",
      "batch 639, loss: 0.0007, instance_loss: 0.0005, weighted_loss: 0.0006, label: 1, bag_size: 8438\n",
      "batch 659, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 9542\n",
      "batch 679, loss: 0.0045, instance_loss: 0.0035, weighted_loss: 0.0042, label: 0, bag_size: 10751\n",
      "batch 699, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 3541\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0057, weighted_loss: 0.0017, label: 1, bag_size: 4877\n",
      "batch 739, loss: 0.0008, instance_loss: 0.0003, weighted_loss: 0.0006, label: 1, bag_size: 6745\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 1, bag_size: 3437\n",
      "batch 779, loss: 0.0054, instance_loss: 0.0042, weighted_loss: 0.0051, label: 0, bag_size: 3444\n",
      "batch 799, loss: 0.0381, instance_loss: 0.0378, weighted_loss: 0.0381, label: 1, bag_size: 7989\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9971\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9827743902439025: correct 12894/13120\n",
      "class 1 clustering acc 0.9094512195121951: correct 5966/6560\n",
      "Epoch: 60, train_loss: 0.1435, train_clustering_loss:  0.1679, train_error: 0.0598\n",
      "class 0: acc 0.946078431372549, correct 386/408\n",
      "class 1: acc 0.9344660194174758, correct 385/412\n",
      "\n",
      "Val Set, val_loss: 0.3350, val_error: 0.1636, auc: 0.9784\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.0: correct 0/880\n",
      "class 0: acc 0.6923076923076923, correct 36/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 4271\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 13786\n",
      "batch 59, loss: 0.0108, instance_loss: 0.0077, weighted_loss: 0.0099, label: 0, bag_size: 4523\n",
      "batch 79, loss: 0.0005, instance_loss: 0.0006, weighted_loss: 0.0005, label: 1, bag_size: 4956\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 22828\n",
      "batch 119, loss: 0.0226, instance_loss: 0.0445, weighted_loss: 0.0292, label: 1, bag_size: 1051\n",
      "batch 139, loss: 0.0086, instance_loss: 0.0144, weighted_loss: 0.0103, label: 0, bag_size: 2006\n",
      "batch 159, loss: 0.0130, instance_loss: 0.1819, weighted_loss: 0.0636, label: 1, bag_size: 3368\n",
      "batch 179, loss: 4.2889, instance_loss: 4.1367, weighted_loss: 4.2432, label: 1, bag_size: 15563\n",
      "batch 199, loss: 0.2082, instance_loss: 0.2681, weighted_loss: 0.2262, label: 0, bag_size: 17630\n",
      "batch 219, loss: 0.0214, instance_loss: 0.0069, weighted_loss: 0.0171, label: 1, bag_size: 6731\n",
      "batch 239, loss: 0.7964, instance_loss: 0.6636, weighted_loss: 0.7566, label: 0, bag_size: 10113\n",
      "batch 259, loss: 0.0083, instance_loss: 0.0060, weighted_loss: 0.0076, label: 0, bag_size: 5409\n",
      "batch 279, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 8003\n",
      "batch 299, loss: 0.0098, instance_loss: 0.0167, weighted_loss: 0.0118, label: 1, bag_size: 1459\n",
      "batch 319, loss: 0.2372, instance_loss: 0.3426, weighted_loss: 0.2688, label: 1, bag_size: 1746\n",
      "batch 339, loss: 0.5813, instance_loss: 0.7742, weighted_loss: 0.6392, label: 1, bag_size: 21450\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9885\n",
      "batch 379, loss: 0.0054, instance_loss: 0.0021, weighted_loss: 0.0044, label: 1, bag_size: 2935\n",
      "batch 399, loss: 0.0007, instance_loss: 0.0227, weighted_loss: 0.0073, label: 0, bag_size: 18777\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 5629\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 8602\n",
      "batch 459, loss: 1.4449, instance_loss: 2.1082, weighted_loss: 1.6439, label: 1, bag_size: 7389\n",
      "batch 479, loss: 0.0849, instance_loss: 0.0790, weighted_loss: 0.0831, label: 0, bag_size: 22426\n",
      "batch 499, loss: 0.0253, instance_loss: 0.0343, weighted_loss: 0.0280, label: 1, bag_size: 9561\n",
      "batch 519, loss: 0.2986, instance_loss: 0.5889, weighted_loss: 0.3857, label: 1, bag_size: 2395\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0008, weighted_loss: 0.0003, label: 0, bag_size: 27158\n",
      "batch 559, loss: 0.0011, instance_loss: 0.0006, weighted_loss: 0.0010, label: 0, bag_size: 19518\n",
      "batch 579, loss: 0.1880, instance_loss: 0.1619, weighted_loss: 0.1802, label: 0, bag_size: 3089\n",
      "batch 599, loss: 0.6897, instance_loss: 0.8083, weighted_loss: 0.7253, label: 1, bag_size: 1845\n",
      "batch 619, loss: 0.0057, instance_loss: 0.0082, weighted_loss: 0.0065, label: 0, bag_size: 11199\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0020, weighted_loss: 0.0006, label: 0, bag_size: 4271\n",
      "batch 659, loss: 0.0306, instance_loss: 0.0359, weighted_loss: 0.0322, label: 1, bag_size: 2137\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0017, weighted_loss: 0.0005, label: 1, bag_size: 6343\n",
      "batch 699, loss: 0.0214, instance_loss: 0.0152, weighted_loss: 0.0195, label: 0, bag_size: 705\n",
      "batch 719, loss: 0.2580, instance_loss: 0.3044, weighted_loss: 0.2719, label: 0, bag_size: 3670\n",
      "batch 739, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 24911\n",
      "batch 759, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 5999\n",
      "batch 779, loss: 0.0025, instance_loss: 0.0014, weighted_loss: 0.0022, label: 0, bag_size: 4598\n",
      "batch 799, loss: 5.2481, instance_loss: 4.4957, weighted_loss: 5.0224, label: 0, bag_size: 6281\n",
      "batch 819, loss: 0.0006, instance_loss: 0.0106, weighted_loss: 0.0036, label: 0, bag_size: 19808\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9786585365853658: correct 12840/13120\n",
      "class 1 clustering acc 0.9065548780487804: correct 5947/6560\n",
      "Epoch: 61, train_loss: 0.1786, train_clustering_loss:  0.2041, train_error: 0.0634\n",
      "class 0: acc 0.9387755102040817, correct 414/441\n",
      "class 1: acc 0.9340369393139841, correct 354/379\n",
      "\n",
      "Val Set, val_loss: 0.2965, val_error: 0.1364, auc: 0.9761\n",
      "class 0 clustering acc 0.9818181818181818: correct 1728/1760\n",
      "class 1 clustering acc 0.17613636363636365: correct 155/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.7758620689655172, correct 45/58\n",
      "EarlyStopping counter: 12 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0069, instance_loss: 0.0047, weighted_loss: 0.0063, label: 1, bag_size: 7798\n",
      "batch 39, loss: 0.0111, instance_loss: 0.0064, weighted_loss: 0.0097, label: 1, bag_size: 1512\n",
      "batch 59, loss: 1.2629, instance_loss: 1.6108, weighted_loss: 1.3672, label: 1, bag_size: 898\n",
      "batch 79, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 4877\n",
      "batch 99, loss: 0.0076, instance_loss: 0.0032, weighted_loss: 0.0062, label: 0, bag_size: 11654\n",
      "batch 119, loss: 0.0628, instance_loss: 0.0632, weighted_loss: 0.0629, label: 1, bag_size: 5155\n",
      "batch 139, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0014, label: 1, bag_size: 1622\n",
      "batch 159, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 6652\n",
      "batch 179, loss: 0.0087, instance_loss: 0.0069, weighted_loss: 0.0082, label: 0, bag_size: 1639\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 14319\n",
      "batch 219, loss: 0.0207, instance_loss: 0.0195, weighted_loss: 0.0203, label: 0, bag_size: 5009\n",
      "batch 239, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 6606\n",
      "batch 259, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 10482\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 8003\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 19932\n",
      "batch 319, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 3459\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11701\n",
      "batch 359, loss: 0.0180, instance_loss: 0.0089, weighted_loss: 0.0152, label: 0, bag_size: 12910\n",
      "batch 379, loss: 0.0037, instance_loss: 0.0009, weighted_loss: 0.0028, label: 1, bag_size: 1064\n",
      "batch 399, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 12687\n",
      "batch 419, loss: 0.2797, instance_loss: 0.2983, weighted_loss: 0.2853, label: 0, bag_size: 1760\n",
      "batch 439, loss: 0.2639, instance_loss: 0.2960, weighted_loss: 0.2735, label: 1, bag_size: 12719\n",
      "batch 459, loss: 0.0004, instance_loss: 0.0001, weighted_loss: 0.0003, label: 0, bag_size: 11654\n",
      "batch 479, loss: 0.0439, instance_loss: 0.0223, weighted_loss: 0.0374, label: 1, bag_size: 1339\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11865\n",
      "batch 519, loss: 0.3149, instance_loss: 0.1479, weighted_loss: 0.2648, label: 1, bag_size: 2638\n",
      "batch 539, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 19039\n",
      "batch 559, loss: 1.9603, instance_loss: 2.4108, weighted_loss: 2.0954, label: 0, bag_size: 2290\n",
      "batch 579, loss: 0.0042, instance_loss: 0.0007, weighted_loss: 0.0032, label: 0, bag_size: 6281\n",
      "batch 599, loss: 1.1189, instance_loss: 1.3390, weighted_loss: 1.1849, label: 0, bag_size: 13332\n",
      "batch 619, loss: 0.0493, instance_loss: 0.0390, weighted_loss: 0.0462, label: 0, bag_size: 14828\n",
      "batch 639, loss: 0.0034, instance_loss: 0.0012, weighted_loss: 0.0027, label: 0, bag_size: 13225\n",
      "batch 659, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 9689\n",
      "batch 679, loss: 0.0156, instance_loss: 0.0107, weighted_loss: 0.0141, label: 0, bag_size: 8755\n",
      "batch 699, loss: 0.0593, instance_loss: 0.0510, weighted_loss: 0.0568, label: 0, bag_size: 803\n",
      "batch 719, loss: 0.0451, instance_loss: 0.0469, weighted_loss: 0.0457, label: 0, bag_size: 1483\n",
      "batch 739, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 12425\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 19472\n",
      "batch 779, loss: 0.0151, instance_loss: 0.0173, weighted_loss: 0.0158, label: 1, bag_size: 9470\n",
      "batch 799, loss: 0.0004, instance_loss: 0.0001, weighted_loss: 0.0003, label: 0, bag_size: 13225\n",
      "batch 819, loss: 0.0038, instance_loss: 0.0023, weighted_loss: 0.0033, label: 0, bag_size: 2044\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9842987804878048: correct 12914/13120\n",
      "class 1 clustering acc 0.9178353658536585: correct 6021/6560\n",
      "Epoch: 62, train_loss: 0.1455, train_clustering_loss:  0.1650, train_error: 0.0537\n",
      "class 0: acc 0.945, correct 378/400\n",
      "class 1: acc 0.9476190476190476, correct 398/420\n",
      "\n",
      "Val Set, val_loss: 0.1934, val_error: 0.0818, auc: 0.9781\n",
      "class 0 clustering acc 0.9926136363636363: correct 1747/1760\n",
      "class 1 clustering acc 0.029545454545454545: correct 26/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.896551724137931, correct 52/58\n",
      "EarlyStopping counter: 13 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0607, instance_loss: 0.0678, weighted_loss: 0.0629, label: 1, bag_size: 16890\n",
      "batch 39, loss: 0.0002, instance_loss: 0.0038, weighted_loss: 0.0013, label: 1, bag_size: 9062\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19472\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 5965\n",
      "batch 99, loss: 0.0025, instance_loss: 0.0001, weighted_loss: 0.0018, label: 1, bag_size: 3968\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 5494\n",
      "batch 139, loss: 0.2708, instance_loss: 0.3517, weighted_loss: 0.2951, label: 1, bag_size: 8012\n",
      "batch 159, loss: 0.5170, instance_loss: 0.6739, weighted_loss: 0.5641, label: 0, bag_size: 1953\n",
      "batch 179, loss: 0.0586, instance_loss: 0.0352, weighted_loss: 0.0516, label: 0, bag_size: 1831\n",
      "batch 199, loss: 0.0336, instance_loss: 0.0323, weighted_loss: 0.0332, label: 1, bag_size: 10460\n",
      "batch 219, loss: 0.7605, instance_loss: 1.1024, weighted_loss: 0.8631, label: 0, bag_size: 1637\n",
      "batch 239, loss: 0.0475, instance_loss: 0.0119, weighted_loss: 0.0368, label: 1, bag_size: 699\n",
      "batch 259, loss: 0.0092, instance_loss: 0.0028, weighted_loss: 0.0073, label: 1, bag_size: 9689\n",
      "batch 279, loss: 0.0094, instance_loss: 0.0099, weighted_loss: 0.0095, label: 1, bag_size: 10394\n",
      "batch 299, loss: 0.0066, instance_loss: 0.0039, weighted_loss: 0.0058, label: 0, bag_size: 21093\n",
      "batch 319, loss: 0.0023, instance_loss: 0.0101, weighted_loss: 0.0046, label: 1, bag_size: 9610\n",
      "batch 339, loss: 0.0086, instance_loss: 0.0079, weighted_loss: 0.0084, label: 0, bag_size: 3908\n",
      "batch 359, loss: 0.0239, instance_loss: 0.0293, weighted_loss: 0.0255, label: 1, bag_size: 10671\n",
      "batch 379, loss: 0.0002, instance_loss: 0.0001, weighted_loss: 0.0002, label: 0, bag_size: 2457\n",
      "batch 399, loss: 0.0000, instance_loss: 0.6896, weighted_loss: 0.2069, label: 1, bag_size: 1781\n",
      "batch 419, loss: 0.0572, instance_loss: 0.0746, weighted_loss: 0.0624, label: 1, bag_size: 10105\n",
      "batch 439, loss: 0.0092, instance_loss: 0.0053, weighted_loss: 0.0080, label: 0, bag_size: 1962\n",
      "batch 459, loss: 1.5105, instance_loss: 1.8839, weighted_loss: 1.6225, label: 1, bag_size: 1533\n",
      "batch 479, loss: 0.0041, instance_loss: 0.0023, weighted_loss: 0.0035, label: 1, bag_size: 29832\n",
      "batch 499, loss: 0.0004, instance_loss: 0.0011, weighted_loss: 0.0006, label: 1, bag_size: 1255\n",
      "batch 519, loss: 0.0057, instance_loss: 0.0031, weighted_loss: 0.0049, label: 1, bag_size: 5256\n",
      "batch 539, loss: 0.0019, instance_loss: 0.0004, weighted_loss: 0.0014, label: 0, bag_size: 2732\n",
      "batch 559, loss: 0.0020, instance_loss: 0.0007, weighted_loss: 0.0016, label: 0, bag_size: 9949\n",
      "batch 579, loss: 0.0182, instance_loss: 0.0147, weighted_loss: 0.0172, label: 1, bag_size: 20333\n",
      "batch 599, loss: 0.2127, instance_loss: 0.3077, weighted_loss: 0.2412, label: 1, bag_size: 2455\n",
      "batch 619, loss: 0.0004, instance_loss: 0.0004, weighted_loss: 0.0004, label: 1, bag_size: 4128\n",
      "batch 639, loss: 0.0241, instance_loss: 0.0224, weighted_loss: 0.0236, label: 0, bag_size: 13880\n",
      "batch 659, loss: 0.0490, instance_loss: 0.0423, weighted_loss: 0.0470, label: 0, bag_size: 19067\n",
      "batch 679, loss: 0.0021, instance_loss: 0.0012, weighted_loss: 0.0018, label: 0, bag_size: 21682\n",
      "batch 699, loss: 0.0046, instance_loss: 0.0033, weighted_loss: 0.0042, label: 0, bag_size: 9415\n",
      "batch 719, loss: 0.0258, instance_loss: 0.0266, weighted_loss: 0.0261, label: 0, bag_size: 19518\n",
      "batch 739, loss: 0.1982, instance_loss: 0.2755, weighted_loss: 0.2214, label: 1, bag_size: 8103\n",
      "batch 759, loss: 0.0340, instance_loss: 0.0345, weighted_loss: 0.0342, label: 1, bag_size: 4239\n",
      "batch 779, loss: 0.0329, instance_loss: 0.0385, weighted_loss: 0.0346, label: 1, bag_size: 6736\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 5221\n",
      "batch 819, loss: 0.0182, instance_loss: 0.0165, weighted_loss: 0.0177, label: 1, bag_size: 5454\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9801829268292683: correct 12860/13120\n",
      "class 1 clustering acc 0.9015243902439024: correct 5914/6560\n",
      "Epoch: 63, train_loss: 0.1568, train_clustering_loss:  0.1824, train_error: 0.0646\n",
      "class 0: acc 0.9408983451536643, correct 398/423\n",
      "class 1: acc 0.929471032745592, correct 369/397\n",
      "\n",
      "Val Set, val_loss: 0.2048, val_error: 0.1000, auc: 0.9794\n",
      "class 0 clustering acc 0.9852272727272727: correct 1734/1760\n",
      "class 1 clustering acc 0.125: correct 110/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8448275862068966, correct 49/58\n",
      "EarlyStopping counter: 14 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11195\n",
      "batch 39, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 16052\n",
      "batch 59, loss: 0.0023, instance_loss: 0.0021, weighted_loss: 0.0022, label: 0, bag_size: 9415\n",
      "batch 79, loss: 0.0275, instance_loss: 0.0200, weighted_loss: 0.0252, label: 1, bag_size: 1746\n",
      "batch 99, loss: 0.0381, instance_loss: 0.0329, weighted_loss: 0.0365, label: 0, bag_size: 24439\n",
      "batch 119, loss: 0.0066, instance_loss: 0.0055, weighted_loss: 0.0063, label: 0, bag_size: 11900\n",
      "batch 139, loss: 0.5974, instance_loss: 0.8046, weighted_loss: 0.6595, label: 1, bag_size: 13440\n",
      "batch 159, loss: 0.0087, instance_loss: 0.0091, weighted_loss: 0.0088, label: 0, bag_size: 2351\n",
      "batch 179, loss: 0.0937, instance_loss: 0.0658, weighted_loss: 0.0853, label: 1, bag_size: 11220\n",
      "batch 199, loss: 0.7407, instance_loss: 1.0235, weighted_loss: 0.8256, label: 0, bag_size: 5009\n",
      "batch 219, loss: 0.0287, instance_loss: 0.0156, weighted_loss: 0.0248, label: 0, bag_size: 14681\n",
      "batch 239, loss: 0.0273, instance_loss: 0.0246, weighted_loss: 0.0265, label: 1, bag_size: 9065\n",
      "batch 259, loss: 0.0090, instance_loss: 0.0041, weighted_loss: 0.0075, label: 1, bag_size: 2695\n",
      "batch 279, loss: 0.4018, instance_loss: 0.4018, weighted_loss: 0.4018, label: 0, bag_size: 2219\n",
      "batch 299, loss: 0.2764, instance_loss: 0.2880, weighted_loss: 0.2799, label: 0, bag_size: 20555\n",
      "batch 319, loss: 0.0046, instance_loss: 0.0017, weighted_loss: 0.0037, label: 0, bag_size: 1588\n",
      "batch 339, loss: 0.0043, instance_loss: 0.0012, weighted_loss: 0.0033, label: 1, bag_size: 6343\n",
      "batch 359, loss: 0.0075, instance_loss: 0.0045, weighted_loss: 0.0066, label: 0, bag_size: 1560\n",
      "batch 379, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 2457\n",
      "batch 399, loss: 0.2292, instance_loss: 0.2863, weighted_loss: 0.2463, label: 1, bag_size: 16548\n",
      "batch 419, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 11477\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 17633\n",
      "batch 459, loss: 0.0009, instance_loss: 0.0010, weighted_loss: 0.0010, label: 0, bag_size: 10535\n",
      "batch 479, loss: 0.0046, instance_loss: 0.0026, weighted_loss: 0.0040, label: 0, bag_size: 18777\n",
      "batch 499, loss: 0.0182, instance_loss: 0.0120, weighted_loss: 0.0163, label: 0, bag_size: 1614\n",
      "batch 519, loss: 0.0543, instance_loss: 0.0456, weighted_loss: 0.0517, label: 0, bag_size: 10942\n",
      "batch 539, loss: 0.0752, instance_loss: 0.0799, weighted_loss: 0.0766, label: 0, bag_size: 7612\n",
      "batch 559, loss: 0.0008, instance_loss: 0.0003, weighted_loss: 0.0006, label: 1, bag_size: 8003\n",
      "batch 579, loss: 0.0003, instance_loss: 0.0003, weighted_loss: 0.0003, label: 1, bag_size: 1919\n",
      "batch 599, loss: 0.0255, instance_loss: 0.0172, weighted_loss: 0.0230, label: 0, bag_size: 7989\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0111, weighted_loss: 0.0033, label: 1, bag_size: 9955\n",
      "batch 639, loss: 0.0006, instance_loss: 0.0001, weighted_loss: 0.0004, label: 1, bag_size: 2278\n",
      "batch 659, loss: 3.6951, instance_loss: 4.0968, weighted_loss: 3.8156, label: 0, bag_size: 2694\n",
      "batch 679, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 14206\n",
      "batch 699, loss: 1.1883, instance_loss: 1.4261, weighted_loss: 1.2596, label: 0, bag_size: 7428\n",
      "batch 719, loss: 2.3579, instance_loss: 3.0788, weighted_loss: 2.5742, label: 1, bag_size: 3879\n",
      "batch 739, loss: 0.0098, instance_loss: 0.0066, weighted_loss: 0.0089, label: 0, bag_size: 763\n",
      "batch 759, loss: 0.0304, instance_loss: 0.0164, weighted_loss: 0.0262, label: 1, bag_size: 9519\n",
      "batch 779, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 9644\n",
      "batch 799, loss: 0.2541, instance_loss: 0.3144, weighted_loss: 0.2722, label: 0, bag_size: 1052\n",
      "batch 819, loss: 0.0046, instance_loss: 0.0018, weighted_loss: 0.0037, label: 1, bag_size: 928\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9828506097560976: correct 12895/13120\n",
      "class 1 clustering acc 0.916920731707317: correct 6015/6560\n",
      "Epoch: 64, train_loss: 0.1503, train_clustering_loss:  0.1704, train_error: 0.0549\n",
      "class 0: acc 0.9477434679334917, correct 399/421\n",
      "class 1: acc 0.9423558897243107, correct 376/399\n",
      "\n",
      "Val Set, val_loss: 0.2405, val_error: 0.1091, auc: 0.9791\n",
      "class 0 clustering acc 0.9772727272727273: correct 1720/1760\n",
      "class 1 clustering acc 0.09318181818181819: correct 82/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8275862068965517, correct 48/58\n",
      "EarlyStopping counter: 15 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5198, instance_loss: 0.5833, weighted_loss: 0.5388, label: 0, bag_size: 2098\n",
      "batch 39, loss: 0.0019, instance_loss: 0.0016, weighted_loss: 0.0018, label: 0, bag_size: 22870\n",
      "batch 59, loss: 0.0004, instance_loss: 0.0010, weighted_loss: 0.0006, label: 1, bag_size: 3453\n",
      "batch 79, loss: 0.0018, instance_loss: 0.0015, weighted_loss: 0.0017, label: 1, bag_size: 5025\n",
      "batch 99, loss: 0.0052, instance_loss: 0.0040, weighted_loss: 0.0048, label: 0, bag_size: 13777\n",
      "batch 119, loss: 0.0040, instance_loss: 0.0016, weighted_loss: 0.0033, label: 0, bag_size: 14249\n",
      "batch 139, loss: 0.0135, instance_loss: 0.0085, weighted_loss: 0.0120, label: 0, bag_size: 1370\n",
      "batch 159, loss: 0.0014, instance_loss: 0.0005, weighted_loss: 0.0011, label: 1, bag_size: 16162\n",
      "batch 179, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 3787\n",
      "batch 199, loss: 0.0100, instance_loss: 0.0101, weighted_loss: 0.0100, label: 1, bag_size: 12425\n",
      "batch 219, loss: 0.0006, instance_loss: 0.0002, weighted_loss: 0.0005, label: 0, bag_size: 7191\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 4465\n",
      "batch 259, loss: 0.0155, instance_loss: 0.0106, weighted_loss: 0.0140, label: 0, bag_size: 21319\n",
      "batch 279, loss: 0.0041, instance_loss: 0.0024, weighted_loss: 0.0036, label: 1, bag_size: 11421\n",
      "batch 299, loss: 0.0851, instance_loss: 0.0579, weighted_loss: 0.0770, label: 0, bag_size: 2360\n",
      "batch 319, loss: 0.0186, instance_loss: 0.0201, weighted_loss: 0.0190, label: 0, bag_size: 9387\n",
      "batch 339, loss: 0.0050, instance_loss: 0.0065, weighted_loss: 0.0055, label: 1, bag_size: 20537\n",
      "batch 359, loss: 2.9038, instance_loss: 3.2367, weighted_loss: 3.0036, label: 1, bag_size: 15185\n",
      "batch 379, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 25970\n",
      "batch 399, loss: 0.0010, instance_loss: 0.0006, weighted_loss: 0.0009, label: 0, bag_size: 8661\n",
      "batch 419, loss: 0.0237, instance_loss: 0.0227, weighted_loss: 0.0234, label: 1, bag_size: 8040\n",
      "batch 439, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 928\n",
      "batch 459, loss: 0.1642, instance_loss: 0.1532, weighted_loss: 0.1609, label: 0, bag_size: 6624\n",
      "batch 479, loss: 0.0005, instance_loss: 0.0002, weighted_loss: 0.0004, label: 0, bag_size: 890\n",
      "batch 499, loss: 0.2734, instance_loss: 0.3185, weighted_loss: 0.2869, label: 0, bag_size: 8420\n",
      "batch 519, loss: 0.1244, instance_loss: 0.1028, weighted_loss: 0.1179, label: 0, bag_size: 1814\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 6205\n",
      "batch 559, loss: 0.0223, instance_loss: 0.0225, weighted_loss: 0.0224, label: 1, bag_size: 3651\n",
      "batch 579, loss: 0.0155, instance_loss: 0.0099, weighted_loss: 0.0138, label: 0, bag_size: 12148\n",
      "batch 599, loss: 0.1901, instance_loss: 0.2245, weighted_loss: 0.2004, label: 0, bag_size: 2959\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11778\n",
      "batch 639, loss: 0.0029, instance_loss: 0.0022, weighted_loss: 0.0027, label: 0, bag_size: 19808\n",
      "batch 659, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 20537\n",
      "batch 679, loss: 0.1080, instance_loss: 0.0833, weighted_loss: 0.1006, label: 0, bag_size: 1508\n",
      "batch 699, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 14202\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 4880\n",
      "batch 739, loss: 0.0003, instance_loss: 0.0001, weighted_loss: 0.0002, label: 1, bag_size: 9004\n",
      "batch 759, loss: 0.0029, instance_loss: 0.0016, weighted_loss: 0.0025, label: 1, bag_size: 12697\n",
      "batch 779, loss: 0.0029, instance_loss: 0.0045, weighted_loss: 0.0034, label: 1, bag_size: 689\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8948\n",
      "batch 819, loss: 0.1321, instance_loss: 0.1117, weighted_loss: 0.1260, label: 0, bag_size: 5211\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.984375: correct 12915/13120\n",
      "class 1 clustering acc 0.9155487804878049: correct 6006/6560\n",
      "Epoch: 65, train_loss: 0.1242, train_clustering_loss:  0.1508, train_error: 0.0500\n",
      "class 0: acc 0.9472361809045227, correct 377/398\n",
      "class 1: acc 0.95260663507109, correct 402/422\n",
      "\n",
      "Val Set, val_loss: 0.2323, val_error: 0.1091, auc: 0.9798\n",
      "class 0 clustering acc 0.9994318181818181: correct 1759/1760\n",
      "class 1 clustering acc 0.025: correct 22/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8275862068965517, correct 48/58\n",
      "EarlyStopping counter: 16 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.1814, instance_loss: 1.5018, weighted_loss: 1.2775, label: 0, bag_size: 2653\n",
      "batch 39, loss: 0.0914, instance_loss: 0.1841, weighted_loss: 0.1192, label: 1, bag_size: 2904\n",
      "batch 59, loss: 0.0007, instance_loss: 0.0003, weighted_loss: 0.0006, label: 1, bag_size: 6317\n",
      "batch 79, loss: 0.0189, instance_loss: 0.0100, weighted_loss: 0.0162, label: 1, bag_size: 12340\n",
      "batch 99, loss: 1.7925, instance_loss: 1.4234, weighted_loss: 1.6817, label: 1, bag_size: 2140\n",
      "batch 119, loss: 0.0358, instance_loss: 0.0141, weighted_loss: 0.0293, label: 0, bag_size: 10304\n",
      "batch 139, loss: 0.0454, instance_loss: 0.0546, weighted_loss: 0.0482, label: 0, bag_size: 4523\n",
      "batch 159, loss: 0.0815, instance_loss: 0.0925, weighted_loss: 0.0848, label: 0, bag_size: 14264\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 8866\n",
      "batch 199, loss: 0.0059, instance_loss: 0.0050, weighted_loss: 0.0057, label: 0, bag_size: 14249\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 1789\n",
      "batch 239, loss: 0.4952, instance_loss: 0.7097, weighted_loss: 0.5596, label: 0, bag_size: 2219\n",
      "batch 259, loss: 0.0047, instance_loss: 0.0024, weighted_loss: 0.0040, label: 0, bag_size: 3774\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 10128\n",
      "batch 299, loss: 0.0216, instance_loss: 0.0152, weighted_loss: 0.0197, label: 0, bag_size: 12840\n",
      "batch 319, loss: 0.0062, instance_loss: 0.0037, weighted_loss: 0.0055, label: 0, bag_size: 3198\n",
      "batch 339, loss: 0.0211, instance_loss: 0.0168, weighted_loss: 0.0198, label: 1, bag_size: 12425\n",
      "batch 359, loss: 0.0716, instance_loss: 0.0491, weighted_loss: 0.0648, label: 0, bag_size: 2873\n",
      "batch 379, loss: 1.7768, instance_loss: 2.0091, weighted_loss: 1.8465, label: 1, bag_size: 6360\n",
      "batch 399, loss: 2.8700, instance_loss: 3.4832, weighted_loss: 3.0539, label: 0, bag_size: 8744\n",
      "batch 419, loss: 0.0015, instance_loss: 0.0001, weighted_loss: 0.0011, label: 0, bag_size: 26271\n",
      "batch 439, loss: 0.0330, instance_loss: 0.0334, weighted_loss: 0.0331, label: 1, bag_size: 7768\n",
      "batch 459, loss: 0.0017, instance_loss: 0.0012, weighted_loss: 0.0015, label: 0, bag_size: 14206\n",
      "batch 479, loss: 0.0025, instance_loss: 0.0020, weighted_loss: 0.0023, label: 1, bag_size: 4308\n",
      "batch 499, loss: 0.0005, instance_loss: 0.0004, weighted_loss: 0.0005, label: 0, bag_size: 1202\n",
      "batch 519, loss: 0.5679, instance_loss: 0.7601, weighted_loss: 0.6256, label: 1, bag_size: 9942\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 2511\n",
      "batch 559, loss: 0.0071, instance_loss: 0.0048, weighted_loss: 0.0064, label: 1, bag_size: 20537\n",
      "batch 579, loss: 0.0150, instance_loss: 0.0122, weighted_loss: 0.0142, label: 1, bag_size: 5155\n",
      "batch 599, loss: 0.0000, instance_loss: 0.3125, weighted_loss: 0.0938, label: 1, bag_size: 13174\n",
      "batch 619, loss: 0.0079, instance_loss: 0.0052, weighted_loss: 0.0071, label: 1, bag_size: 689\n",
      "batch 639, loss: 0.8209, instance_loss: 1.0703, weighted_loss: 0.8957, label: 0, bag_size: 1637\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8812\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 3453\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0003, weighted_loss: 0.0001, label: 0, bag_size: 12524\n",
      "batch 719, loss: 0.0023, instance_loss: 0.0007, weighted_loss: 0.0018, label: 1, bag_size: 22286\n",
      "batch 739, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 5317\n",
      "batch 759, loss: 0.1749, instance_loss: 0.1911, weighted_loss: 0.1797, label: 1, bag_size: 1609\n",
      "batch 779, loss: 0.1676, instance_loss: 0.1682, weighted_loss: 0.1678, label: 1, bag_size: 11729\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 11113\n",
      "batch 819, loss: 0.1445, instance_loss: 0.1610, weighted_loss: 0.1495, label: 1, bag_size: 2842\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9814024390243903: correct 12876/13120\n",
      "class 1 clustering acc 0.9059451219512196: correct 5943/6560\n",
      "Epoch: 66, train_loss: 0.1526, train_clustering_loss:  0.1808, train_error: 0.0610\n",
      "class 0: acc 0.945823927765237, correct 419/443\n",
      "class 1: acc 0.9310344827586207, correct 351/377\n",
      "\n",
      "Val Set, val_loss: 0.1802, val_error: 0.0727, auc: 0.9808\n",
      "class 0 clustering acc 0.9954545454545455: correct 1752/1760\n",
      "class 1 clustering acc 0.013636363636363636: correct 12/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.896551724137931, correct 52/58\n",
      "EarlyStopping counter: 17 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 4.0986, instance_loss: 4.3510, weighted_loss: 4.1743, label: 0, bag_size: 3897\n",
      "batch 39, loss: 0.0048, instance_loss: 0.0012, weighted_loss: 0.0037, label: 0, bag_size: 4497\n",
      "batch 59, loss: 0.1489, instance_loss: 0.1348, weighted_loss: 0.1447, label: 0, bag_size: 15071\n",
      "batch 79, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 1, bag_size: 16565\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0052, weighted_loss: 0.0016, label: 1, bag_size: 10392\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 7011\n",
      "batch 139, loss: 0.0106, instance_loss: 0.0223, weighted_loss: 0.0141, label: 1, bag_size: 3368\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 518\n",
      "batch 179, loss: 0.0045, instance_loss: 0.0057, weighted_loss: 0.0048, label: 1, bag_size: 2344\n",
      "batch 199, loss: 0.0003, instance_loss: 0.0002, weighted_loss: 0.0002, label: 0, bag_size: 9234\n",
      "batch 219, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 7011\n",
      "batch 239, loss: 0.0353, instance_loss: 0.0220, weighted_loss: 0.0313, label: 0, bag_size: 15672\n",
      "batch 259, loss: 0.3436, instance_loss: 0.5502, weighted_loss: 0.4056, label: 0, bag_size: 2624\n",
      "batch 279, loss: 0.0046, instance_loss: 0.0011, weighted_loss: 0.0036, label: 0, bag_size: 2424\n",
      "batch 299, loss: 0.0075, instance_loss: 0.0010, weighted_loss: 0.0056, label: 1, bag_size: 3651\n",
      "batch 319, loss: 0.0100, instance_loss: 0.0054, weighted_loss: 0.0086, label: 1, bag_size: 21827\n",
      "batch 339, loss: 0.2628, instance_loss: 0.3238, weighted_loss: 0.2811, label: 1, bag_size: 5340\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 18225\n",
      "batch 379, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 5221\n",
      "batch 399, loss: 0.0156, instance_loss: 0.0137, weighted_loss: 0.0150, label: 0, bag_size: 3101\n",
      "batch 419, loss: 0.2496, instance_loss: 0.3034, weighted_loss: 0.2658, label: 0, bag_size: 12510\n",
      "batch 439, loss: 0.5177, instance_loss: 0.6852, weighted_loss: 0.5680, label: 1, bag_size: 21252\n",
      "batch 459, loss: 0.6151, instance_loss: 0.7148, weighted_loss: 0.6451, label: 0, bag_size: 1592\n",
      "batch 479, loss: 0.0010, instance_loss: 0.0012, weighted_loss: 0.0011, label: 0, bag_size: 8025\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 8410\n",
      "batch 519, loss: 0.0493, instance_loss: 0.0358, weighted_loss: 0.0452, label: 0, bag_size: 10415\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 10146\n",
      "batch 559, loss: 0.0649, instance_loss: 0.0510, weighted_loss: 0.0607, label: 1, bag_size: 2842\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 9234\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0011, weighted_loss: 0.0004, label: 1, bag_size: 15332\n",
      "batch 619, loss: 0.0310, instance_loss: 0.0222, weighted_loss: 0.0283, label: 0, bag_size: 1349\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 2195\n",
      "batch 659, loss: 0.0033, instance_loss: 0.0047, weighted_loss: 0.0037, label: 0, bag_size: 14266\n",
      "batch 679, loss: 0.0267, instance_loss: 0.0349, weighted_loss: 0.0292, label: 0, bag_size: 1732\n",
      "batch 699, loss: 0.0127, instance_loss: 0.0115, weighted_loss: 0.0123, label: 0, bag_size: 6884\n",
      "batch 719, loss: 0.0558, instance_loss: 0.0532, weighted_loss: 0.0550, label: 1, bag_size: 2842\n",
      "batch 739, loss: 0.0096, instance_loss: 0.0068, weighted_loss: 0.0087, label: 1, bag_size: 3980\n",
      "batch 759, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 2844\n",
      "batch 779, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15003\n",
      "batch 799, loss: 0.1433, instance_loss: 0.1354, weighted_loss: 0.1409, label: 0, bag_size: 1349\n",
      "batch 819, loss: 0.0307, instance_loss: 0.0211, weighted_loss: 0.0279, label: 0, bag_size: 1438\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.978810975609756: correct 12842/13120\n",
      "class 1 clustering acc 0.8946646341463415: correct 5869/6560\n",
      "Epoch: 67, train_loss: 0.1591, train_clustering_loss:  0.1839, train_error: 0.0671\n",
      "class 0: acc 0.927710843373494, correct 385/415\n",
      "class 1: acc 0.9382716049382716, correct 380/405\n",
      "\n",
      "Val Set, val_loss: 0.2579, val_error: 0.1182, auc: 0.9738\n",
      "class 0 clustering acc 1.0: correct 1760/1760\n",
      "class 1 clustering acc 0.08068181818181819: correct 71/880\n",
      "class 0: acc 0.7884615384615384, correct 41/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 18 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0156, instance_loss: 0.0089, weighted_loss: 0.0136, label: 1, bag_size: 1525\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 2654\n",
      "batch 59, loss: 0.0738, instance_loss: 0.0775, weighted_loss: 0.0749, label: 1, bag_size: 6478\n",
      "batch 79, loss: 0.0033, instance_loss: 0.0007, weighted_loss: 0.0025, label: 1, bag_size: 13365\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 12217\n",
      "batch 119, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 19039\n",
      "batch 139, loss: 0.0004, instance_loss: 0.0004, weighted_loss: 0.0004, label: 0, bag_size: 24439\n",
      "batch 159, loss: 0.1058, instance_loss: 0.0930, weighted_loss: 0.1019, label: 0, bag_size: 1909\n",
      "batch 179, loss: 1.0994, instance_loss: 1.3205, weighted_loss: 1.1657, label: 1, bag_size: 684\n",
      "batch 199, loss: 0.1106, instance_loss: 0.0954, weighted_loss: 0.1061, label: 0, bag_size: 12732\n",
      "batch 219, loss: 0.0006, instance_loss: 0.0002, weighted_loss: 0.0005, label: 0, bag_size: 9949\n",
      "batch 239, loss: 0.0012, instance_loss: 0.0008, weighted_loss: 0.0011, label: 0, bag_size: 16936\n",
      "batch 259, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 4465\n",
      "batch 279, loss: 0.0003, instance_loss: 0.0001, weighted_loss: 0.0003, label: 0, bag_size: 7381\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 2760\n",
      "batch 319, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 2920\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19390\n",
      "batch 359, loss: 0.0589, instance_loss: 0.1087, weighted_loss: 0.0738, label: 1, bag_size: 7110\n",
      "batch 379, loss: 0.0253, instance_loss: 0.0165, weighted_loss: 0.0227, label: 0, bag_size: 11146\n",
      "batch 399, loss: 0.0038, instance_loss: 0.0011, weighted_loss: 0.0030, label: 1, bag_size: 2785\n",
      "batch 419, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 12840\n",
      "batch 439, loss: 0.0202, instance_loss: 0.0177, weighted_loss: 0.0195, label: 0, bag_size: 8582\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9885\n",
      "batch 479, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 15464\n",
      "batch 499, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 8754\n",
      "batch 519, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 13880\n",
      "batch 539, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 2548\n",
      "batch 559, loss: 0.0026, instance_loss: 0.0021, weighted_loss: 0.0025, label: 0, bag_size: 13880\n",
      "batch 579, loss: 0.5062, instance_loss: 0.7443, weighted_loss: 0.5776, label: 0, bag_size: 1127\n",
      "batch 599, loss: 0.0458, instance_loss: 0.0389, weighted_loss: 0.0437, label: 0, bag_size: 4497\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 5894\n",
      "batch 639, loss: 0.0008, instance_loss: 0.0010, weighted_loss: 0.0009, label: 1, bag_size: 13365\n",
      "batch 659, loss: 0.0357, instance_loss: 0.0317, weighted_loss: 0.0345, label: 0, bag_size: 2336\n",
      "batch 679, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 10725\n",
      "batch 699, loss: 0.0025, instance_loss: 0.0055, weighted_loss: 0.0034, label: 1, bag_size: 7613\n",
      "batch 719, loss: 0.0260, instance_loss: 0.0329, weighted_loss: 0.0281, label: 0, bag_size: 17268\n",
      "batch 739, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 20796\n",
      "batch 759, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 1614\n",
      "batch 779, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 19972\n",
      "batch 799, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 931\n",
      "batch 819, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 8438\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9864329268292683: correct 12942/13120\n",
      "class 1 clustering acc 0.9164634146341464: correct 6012/6560\n",
      "Epoch: 68, train_loss: 0.1199, train_clustering_loss:  0.1485, train_error: 0.0476\n",
      "class 0: acc 0.9509345794392523, correct 407/428\n",
      "class 1: acc 0.9540816326530612, correct 374/392\n",
      "\n",
      "Val Set, val_loss: 0.3646, val_error: 0.1727, auc: 0.9751\n",
      "class 0 clustering acc 0.9971590909090909: correct 1755/1760\n",
      "class 1 clustering acc 0.3: correct 264/880\n",
      "class 0: acc 0.6538461538461539, correct 34/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 19 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0821, instance_loss: 0.0596, weighted_loss: 0.0754, label: 0, bag_size: 16690\n",
      "batch 39, loss: 0.0433, instance_loss: 0.0303, weighted_loss: 0.0394, label: 1, bag_size: 9561\n",
      "batch 59, loss: 0.0082, instance_loss: 0.0045, weighted_loss: 0.0071, label: 1, bag_size: 3368\n",
      "batch 79, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 617\n",
      "batch 99, loss: 0.0696, instance_loss: 0.0573, weighted_loss: 0.0659, label: 0, bag_size: 2242\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 7011\n",
      "batch 139, loss: 0.0227, instance_loss: 0.0172, weighted_loss: 0.0210, label: 0, bag_size: 2815\n",
      "batch 159, loss: 0.0031, instance_loss: 0.0038, weighted_loss: 0.0033, label: 1, bag_size: 5903\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 14319\n",
      "batch 199, loss: 2.2625, instance_loss: 2.7925, weighted_loss: 2.4215, label: 0, bag_size: 14664\n",
      "batch 219, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 18045\n",
      "batch 239, loss: 0.2188, instance_loss: 0.2488, weighted_loss: 0.2278, label: 0, bag_size: 2918\n",
      "batch 259, loss: 0.0004, instance_loss: 0.0001, weighted_loss: 0.0003, label: 0, bag_size: 9851\n",
      "batch 279, loss: 0.0031, instance_loss: 0.0015, weighted_loss: 0.0026, label: 1, bag_size: 1512\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0054, weighted_loss: 0.0016, label: 0, bag_size: 21218\n",
      "batch 319, loss: 0.0008, instance_loss: 0.0004, weighted_loss: 0.0007, label: 0, bag_size: 2322\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 7110\n",
      "batch 359, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 1, bag_size: 14604\n",
      "batch 379, loss: 0.0445, instance_loss: 0.0312, weighted_loss: 0.0405, label: 1, bag_size: 1483\n",
      "batch 399, loss: 0.0033, instance_loss: 0.0002, weighted_loss: 0.0024, label: 0, bag_size: 5551\n",
      "batch 419, loss: 0.1165, instance_loss: 0.1119, weighted_loss: 0.1151, label: 1, bag_size: 9470\n",
      "batch 439, loss: 0.0008, instance_loss: 0.0005, weighted_loss: 0.0008, label: 0, bag_size: 10410\n",
      "batch 459, loss: 0.0093, instance_loss: 0.0094, weighted_loss: 0.0093, label: 1, bag_size: 4789\n",
      "batch 479, loss: 0.7020, instance_loss: 0.5387, weighted_loss: 0.6530, label: 1, bag_size: 18161\n",
      "batch 499, loss: 0.0029, instance_loss: 0.0010, weighted_loss: 0.0023, label: 0, bag_size: 2760\n",
      "batch 519, loss: 1.8945, instance_loss: 1.9642, weighted_loss: 1.9154, label: 0, bag_size: 11306\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 12201\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11387\n",
      "batch 579, loss: 0.1471, instance_loss: 0.1429, weighted_loss: 0.1458, label: 0, bag_size: 705\n",
      "batch 599, loss: 0.0023, instance_loss: 0.0042, weighted_loss: 0.0029, label: 0, bag_size: 1891\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 18738\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 19832\n",
      "batch 659, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 1891\n",
      "batch 679, loss: 0.0238, instance_loss: 0.0286, weighted_loss: 0.0252, label: 0, bag_size: 7835\n",
      "batch 699, loss: 0.4817, instance_loss: 0.6639, weighted_loss: 0.5363, label: 1, bag_size: 21450\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 25970\n",
      "batch 739, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11113\n",
      "batch 759, loss: 0.0048, instance_loss: 0.0014, weighted_loss: 0.0038, label: 1, bag_size: 16890\n",
      "batch 779, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 6745\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 15332\n",
      "batch 819, loss: 0.0036, instance_loss: 0.0021, weighted_loss: 0.0032, label: 1, bag_size: 1242\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9855182926829268: correct 12930/13120\n",
      "class 1 clustering acc 0.9307926829268293: correct 6106/6560\n",
      "Epoch: 69, train_loss: 0.1181, train_clustering_loss:  0.1372, train_error: 0.0451\n",
      "class 0: acc 0.9560975609756097, correct 392/410\n",
      "class 1: acc 0.9536585365853658, correct 391/410\n",
      "\n",
      "Val Set, val_loss: 0.4640, val_error: 0.1636, auc: 0.9758\n",
      "class 0 clustering acc 0.9454545454545454: correct 1664/1760\n",
      "class 1 clustering acc 0.15681818181818183: correct 138/880\n",
      "class 0: acc 0.6923076923076923, correct 36/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 20 out of 20\n",
      "Early stopping\n",
      "Val error: 0.0545, ROC AUC: 0.9828\n",
      "Test error: 0.0928, ROC AUC: 0.9690\n",
      "class 0: acc 1.0, correct 49/49\n",
      "class 1: acc 0.8125, correct 39/48\n",
      "\n",
      "Training Fold 1!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 822 samples\n",
      "Validating on 109 samples\n",
      "Testing on 96 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Setting tau to 1.0\n",
      "Done!\n",
      "TransformerMIL_SB(\n",
      "  (instance_loss_fn): SmoothTop1SVM()\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (transformer): TransformerEncoder_PerformerAttention(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): SelfAttention(\n",
      "            (fast_attention): FastAttention(\n",
      "              (kernel_fn): ReLU()\n",
      "            )\n",
      "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
      "            (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attention): Attn_Net_Gated(\n",
      "    (attention_a): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_b): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (attention_V2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (attention_U2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (attention_weights2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Total number of parameters: 8406537\n",
      "Total number of trainable parameters: 8406537\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 13.8540, instance_loss: 2.0177, weighted_loss: 10.3031, label: 0, bag_size: 10490\n",
      "batch 39, loss: 2.9101, instance_loss: 3.5007, weighted_loss: 3.0873, label: 1, bag_size: 17769\n",
      "batch 59, loss: 0.5025, instance_loss: 2.2934, weighted_loss: 1.0397, label: 0, bag_size: 15898\n",
      "batch 79, loss: 0.8163, instance_loss: 1.2263, weighted_loss: 0.9393, label: 1, bag_size: 5763\n",
      "batch 99, loss: 1.6909, instance_loss: 1.0409, weighted_loss: 1.4959, label: 1, bag_size: 19932\n",
      "batch 119, loss: 1.7199, instance_loss: 0.7215, weighted_loss: 1.4204, label: 1, bag_size: 8754\n",
      "batch 139, loss: 0.2540, instance_loss: 0.7018, weighted_loss: 0.3883, label: 0, bag_size: 4598\n",
      "batch 159, loss: 0.5316, instance_loss: 0.7752, weighted_loss: 0.6047, label: 0, bag_size: 11199\n",
      "batch 179, loss: 4.4298, instance_loss: 1.4001, weighted_loss: 3.5209, label: 1, bag_size: 13367\n",
      "batch 199, loss: 0.6671, instance_loss: 0.9219, weighted_loss: 0.7435, label: 0, bag_size: 17630\n",
      "batch 219, loss: 7.4307, instance_loss: 2.1818, weighted_loss: 5.8560, label: 0, bag_size: 7235\n",
      "batch 239, loss: 0.4017, instance_loss: 1.1738, weighted_loss: 0.6333, label: 0, bag_size: 18574\n",
      "batch 259, loss: 0.0879, instance_loss: 0.9636, weighted_loss: 0.3506, label: 0, bag_size: 16211\n",
      "batch 279, loss: 0.1007, instance_loss: 1.8828, weighted_loss: 0.6353, label: 0, bag_size: 9387\n",
      "batch 299, loss: 2.5035, instance_loss: 2.5244, weighted_loss: 2.5098, label: 0, bag_size: 12131\n",
      "batch 319, loss: 0.4806, instance_loss: 1.8293, weighted_loss: 0.8852, label: 1, bag_size: 15332\n",
      "batch 339, loss: 0.2967, instance_loss: 1.5911, weighted_loss: 0.6850, label: 1, bag_size: 6599\n",
      "batch 359, loss: 0.1342, instance_loss: 1.5147, weighted_loss: 0.5483, label: 1, bag_size: 11389\n",
      "batch 379, loss: 1.5711, instance_loss: 1.4100, weighted_loss: 1.5228, label: 1, bag_size: 12712\n",
      "batch 399, loss: 0.2426, instance_loss: 0.8386, weighted_loss: 0.4214, label: 0, bag_size: 1438\n",
      "batch 419, loss: 0.3000, instance_loss: 0.7283, weighted_loss: 0.4285, label: 1, bag_size: 11884\n",
      "batch 439, loss: 0.1676, instance_loss: 0.7147, weighted_loss: 0.3317, label: 1, bag_size: 10112\n",
      "batch 459, loss: 0.5857, instance_loss: 0.7023, weighted_loss: 0.6207, label: 1, bag_size: 6752\n",
      "batch 479, loss: 0.2345, instance_loss: 1.2324, weighted_loss: 0.5339, label: 0, bag_size: 1437\n",
      "batch 499, loss: 0.1368, instance_loss: 1.0096, weighted_loss: 0.3986, label: 1, bag_size: 13440\n",
      "batch 519, loss: 0.2071, instance_loss: 0.7427, weighted_loss: 0.3678, label: 0, bag_size: 20150\n",
      "batch 539, loss: 0.0628, instance_loss: 0.7029, weighted_loss: 0.2548, label: 0, bag_size: 11259\n",
      "batch 559, loss: 1.9445, instance_loss: 1.2968, weighted_loss: 1.7502, label: 0, bag_size: 6367\n",
      "batch 579, loss: 0.5421, instance_loss: 1.0071, weighted_loss: 0.6816, label: 1, bag_size: 13089\n",
      "batch 599, loss: 0.0178, instance_loss: 0.7236, weighted_loss: 0.2295, label: 0, bag_size: 2044\n",
      "batch 619, loss: 0.0053, instance_loss: 0.6869, weighted_loss: 0.2098, label: 1, bag_size: 18603\n",
      "batch 639, loss: 0.0861, instance_loss: 0.9780, weighted_loss: 0.3537, label: 1, bag_size: 4480\n",
      "batch 659, loss: 0.0239, instance_loss: 0.6902, weighted_loss: 0.2238, label: 0, bag_size: 17155\n",
      "batch 679, loss: 1.5620, instance_loss: 1.0855, weighted_loss: 1.4191, label: 0, bag_size: 3089\n",
      "batch 699, loss: 0.0771, instance_loss: 0.7364, weighted_loss: 0.2749, label: 0, bag_size: 16782\n",
      "batch 719, loss: 3.3199, instance_loss: 0.9633, weighted_loss: 2.6129, label: 0, bag_size: 12593\n",
      "batch 739, loss: 1.0182, instance_loss: 0.8188, weighted_loss: 0.9584, label: 1, bag_size: 15332\n",
      "batch 759, loss: 0.0482, instance_loss: 0.7670, weighted_loss: 0.2638, label: 1, bag_size: 1786\n",
      "batch 779, loss: 0.0405, instance_loss: 0.7292, weighted_loss: 0.2471, label: 0, bag_size: 14664\n",
      "batch 799, loss: 3.6322, instance_loss: 1.6479, weighted_loss: 3.0369, label: 1, bag_size: 10105\n",
      "batch 819, loss: 0.2218, instance_loss: 0.6269, weighted_loss: 0.3434, label: 1, bag_size: 16267\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.8877737226277372: correct 11676/13152\n",
      "class 1 clustering acc 0.17989659367396593: correct 1183/6576\n",
      "Epoch: 0, train_loss: 1.0022, train_clustering_loss:  1.0962, train_error: 0.3613\n",
      "class 0: acc 0.6234718826405868, correct 255/409\n",
      "class 1: acc 0.6537530266343826, correct 270/413\n",
      "\n",
      "Val Set, val_loss: 0.4338, val_error: 0.1743, auc: 0.9300\n",
      "class 0 clustering acc 0.9868119266055045: correct 1721/1744\n",
      "class 1 clustering acc 0.04128440366972477: correct 36/872\n",
      "class 0: acc 0.9565217391304348, correct 44/46\n",
      "class 1: acc 0.7301587301587301, correct 46/63\n",
      "Validation loss decreased (inf --> 0.433782).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0132, instance_loss: 0.6863, weighted_loss: 0.2151, label: 1, bag_size: 15125\n",
      "batch 39, loss: 0.7294, instance_loss: 0.7534, weighted_loss: 0.7366, label: 0, bag_size: 1349\n",
      "batch 59, loss: 0.2611, instance_loss: 0.6626, weighted_loss: 0.3815, label: 1, bag_size: 7935\n",
      "batch 79, loss: 2.0663, instance_loss: 1.3731, weighted_loss: 1.8584, label: 0, bag_size: 8427\n",
      "batch 99, loss: 0.0010, instance_loss: 0.6201, weighted_loss: 0.1867, label: 1, bag_size: 19972\n",
      "batch 119, loss: 0.0140, instance_loss: 0.6902, weighted_loss: 0.2169, label: 0, bag_size: 20555\n",
      "batch 139, loss: 0.2609, instance_loss: 0.6859, weighted_loss: 0.3884, label: 1, bag_size: 2848\n",
      "batch 159, loss: 0.4077, instance_loss: 0.6941, weighted_loss: 0.4936, label: 1, bag_size: 12895\n",
      "batch 179, loss: 0.1092, instance_loss: 0.9059, weighted_loss: 0.3482, label: 0, bag_size: 9171\n",
      "batch 199, loss: 0.0121, instance_loss: 0.6398, weighted_loss: 0.2004, label: 1, bag_size: 8216\n",
      "batch 219, loss: 2.1261, instance_loss: 0.7719, weighted_loss: 1.7199, label: 1, bag_size: 19932\n",
      "batch 239, loss: 0.0591, instance_loss: 0.9284, weighted_loss: 0.3199, label: 1, bag_size: 2904\n",
      "batch 259, loss: 0.0045, instance_loss: 0.6697, weighted_loss: 0.2041, label: 0, bag_size: 12137\n",
      "batch 279, loss: 3.0822, instance_loss: 1.6758, weighted_loss: 2.6603, label: 0, bag_size: 5105\n",
      "batch 299, loss: 0.0125, instance_loss: 0.7515, weighted_loss: 0.2342, label: 1, bag_size: 2405\n",
      "batch 319, loss: 0.2685, instance_loss: 0.9193, weighted_loss: 0.4637, label: 0, bag_size: 2920\n",
      "batch 339, loss: 0.0143, instance_loss: 0.6502, weighted_loss: 0.2051, label: 1, bag_size: 11223\n",
      "batch 359, loss: 0.7271, instance_loss: 0.7033, weighted_loss: 0.7199, label: 1, bag_size: 3437\n",
      "batch 379, loss: 0.4661, instance_loss: 1.5061, weighted_loss: 0.7781, label: 0, bag_size: 10063\n",
      "batch 399, loss: 0.1688, instance_loss: 0.7442, weighted_loss: 0.3414, label: 1, bag_size: 2381\n",
      "batch 419, loss: 0.0001, instance_loss: 0.9734, weighted_loss: 0.2921, label: 1, bag_size: 13194\n",
      "batch 439, loss: 0.0184, instance_loss: 0.7262, weighted_loss: 0.2308, label: 1, bag_size: 5155\n",
      "batch 459, loss: 0.0009, instance_loss: 0.6846, weighted_loss: 0.2060, label: 0, bag_size: 15057\n",
      "batch 479, loss: 6.4928, instance_loss: 2.8327, weighted_loss: 5.3948, label: 0, bag_size: 4692\n",
      "batch 499, loss: 0.0167, instance_loss: 0.7732, weighted_loss: 0.2436, label: 1, bag_size: 8680\n",
      "batch 519, loss: 3.5403, instance_loss: 1.4229, weighted_loss: 2.9051, label: 1, bag_size: 8602\n",
      "batch 539, loss: 0.0107, instance_loss: 0.6639, weighted_loss: 0.2066, label: 0, bag_size: 26271\n",
      "batch 559, loss: 0.3476, instance_loss: 0.6913, weighted_loss: 0.4507, label: 1, bag_size: 1064\n",
      "batch 579, loss: 0.0003, instance_loss: 0.6359, weighted_loss: 0.1910, label: 0, bag_size: 11527\n",
      "batch 599, loss: 2.6735, instance_loss: 2.3829, weighted_loss: 2.5863, label: 1, bag_size: 13440\n",
      "batch 619, loss: 0.9944, instance_loss: 0.8964, weighted_loss: 0.9650, label: 0, bag_size: 11151\n",
      "batch 639, loss: 0.1677, instance_loss: 0.8495, weighted_loss: 0.3722, label: 0, bag_size: 10146\n",
      "batch 659, loss: 0.1737, instance_loss: 0.6964, weighted_loss: 0.3305, label: 1, bag_size: 7186\n",
      "batch 679, loss: 0.1012, instance_loss: 0.7552, weighted_loss: 0.2974, label: 1, bag_size: 10848\n",
      "batch 699, loss: 0.0971, instance_loss: 0.6993, weighted_loss: 0.2778, label: 1, bag_size: 6736\n",
      "batch 719, loss: 0.0642, instance_loss: 0.6048, weighted_loss: 0.2264, label: 0, bag_size: 1984\n",
      "batch 739, loss: 0.0033, instance_loss: 0.8417, weighted_loss: 0.2548, label: 1, bag_size: 2814\n",
      "batch 759, loss: 0.3829, instance_loss: 0.7651, weighted_loss: 0.4975, label: 1, bag_size: 10432\n",
      "batch 779, loss: 0.0013, instance_loss: 0.6426, weighted_loss: 0.1937, label: 1, bag_size: 2522\n",
      "batch 799, loss: 0.0048, instance_loss: 0.6577, weighted_loss: 0.2007, label: 0, bag_size: 14828\n",
      "batch 819, loss: 0.0234, instance_loss: 0.6646, weighted_loss: 0.2158, label: 1, bag_size: 13051\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.8595650851581509: correct 11305/13152\n",
      "class 1 clustering acc 0.30170316301703165: correct 1984/6576\n",
      "Epoch: 1, train_loss: 0.5798, train_clustering_loss:  0.8721, train_error: 0.1983\n",
      "class 0: acc 0.7924050632911392, correct 313/395\n",
      "class 1: acc 0.810304449648712, correct 346/427\n",
      "\n",
      "Val Set, val_loss: 0.3481, val_error: 0.1468, auc: 0.9344\n",
      "class 0 clustering acc 0.8893348623853211: correct 1551/1744\n",
      "class 1 clustering acc 0.26605504587155965: correct 232/872\n",
      "class 0: acc 0.9130434782608695, correct 42/46\n",
      "class 1: acc 0.8095238095238095, correct 51/63\n",
      "Validation loss decreased (0.433782 --> 0.348109).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0595, instance_loss: 0.6902, weighted_loss: 0.2487, label: 0, bag_size: 11390\n",
      "batch 39, loss: 2.8289, instance_loss: 2.2860, weighted_loss: 2.6660, label: 0, bag_size: 10113\n",
      "batch 59, loss: 0.1900, instance_loss: 0.6183, weighted_loss: 0.3185, label: 0, bag_size: 15672\n",
      "batch 79, loss: 0.5835, instance_loss: 0.7324, weighted_loss: 0.6282, label: 1, bag_size: 1015\n",
      "batch 99, loss: 2.9117, instance_loss: 2.0469, weighted_loss: 2.6523, label: 1, bag_size: 12494\n",
      "batch 119, loss: 0.0061, instance_loss: 0.7165, weighted_loss: 0.2192, label: 0, bag_size: 9485\n",
      "batch 139, loss: 0.1161, instance_loss: 1.3455, weighted_loss: 0.4849, label: 0, bag_size: 2609\n",
      "batch 159, loss: 0.0052, instance_loss: 0.6133, weighted_loss: 0.1876, label: 0, bag_size: 2179\n",
      "batch 179, loss: 0.0021, instance_loss: 0.6311, weighted_loss: 0.1908, label: 1, bag_size: 4715\n",
      "batch 199, loss: 1.2667, instance_loss: 1.7341, weighted_loss: 1.4069, label: 0, bag_size: 3783\n",
      "batch 219, loss: 0.0136, instance_loss: 0.6117, weighted_loss: 0.1930, label: 0, bag_size: 22498\n",
      "batch 239, loss: 1.4836, instance_loss: 0.8303, weighted_loss: 1.2876, label: 1, bag_size: 20333\n",
      "batch 259, loss: 0.3663, instance_loss: 1.1634, weighted_loss: 0.6054, label: 0, bag_size: 18215\n",
      "batch 279, loss: 0.3686, instance_loss: 1.0316, weighted_loss: 0.5675, label: 0, bag_size: 8959\n",
      "batch 299, loss: 0.0920, instance_loss: 0.6092, weighted_loss: 0.2472, label: 0, bag_size: 31085\n",
      "batch 319, loss: 0.0085, instance_loss: 0.6080, weighted_loss: 0.1884, label: 1, bag_size: 11701\n",
      "batch 339, loss: 0.2728, instance_loss: 0.6652, weighted_loss: 0.3905, label: 0, bag_size: 7823\n",
      "batch 359, loss: 2.8592, instance_loss: 0.8139, weighted_loss: 2.2456, label: 0, bag_size: 2036\n",
      "batch 379, loss: 1.8166, instance_loss: 2.3799, weighted_loss: 1.9856, label: 1, bag_size: 15192\n",
      "batch 399, loss: 0.0008, instance_loss: 0.6294, weighted_loss: 0.1894, label: 0, bag_size: 18225\n",
      "batch 419, loss: 0.0630, instance_loss: 0.6291, weighted_loss: 0.2329, label: 0, bag_size: 2044\n",
      "batch 439, loss: 0.0051, instance_loss: 0.6634, weighted_loss: 0.2026, label: 0, bag_size: 8252\n",
      "batch 459, loss: 0.8470, instance_loss: 1.0989, weighted_loss: 0.9226, label: 1, bag_size: 13440\n",
      "batch 479, loss: 0.0020, instance_loss: 0.5692, weighted_loss: 0.1721, label: 0, bag_size: 14681\n",
      "batch 499, loss: 0.0001, instance_loss: 0.4861, weighted_loss: 0.1459, label: 1, bag_size: 7078\n",
      "batch 519, loss: 0.1617, instance_loss: 0.8056, weighted_loss: 0.3549, label: 1, bag_size: 2759\n",
      "batch 539, loss: 0.1630, instance_loss: 0.6568, weighted_loss: 0.3111, label: 1, bag_size: 4259\n",
      "batch 559, loss: 0.0657, instance_loss: 0.6965, weighted_loss: 0.2549, label: 0, bag_size: 8420\n",
      "batch 579, loss: 0.2062, instance_loss: 0.6477, weighted_loss: 0.3386, label: 1, bag_size: 2935\n",
      "batch 599, loss: 0.0338, instance_loss: 0.5772, weighted_loss: 0.1968, label: 0, bag_size: 5161\n",
      "batch 619, loss: 1.7410, instance_loss: 0.8511, weighted_loss: 1.4741, label: 0, bag_size: 10029\n",
      "batch 639, loss: 0.0021, instance_loss: 0.3400, weighted_loss: 0.1035, label: 1, bag_size: 10867\n",
      "batch 659, loss: 0.0055, instance_loss: 0.5923, weighted_loss: 0.1815, label: 0, bag_size: 20666\n",
      "batch 679, loss: 0.0544, instance_loss: 0.5873, weighted_loss: 0.2143, label: 0, bag_size: 2534\n",
      "batch 699, loss: 0.0049, instance_loss: 0.4846, weighted_loss: 0.1488, label: 0, bag_size: 11383\n",
      "batch 719, loss: 0.0120, instance_loss: 0.4847, weighted_loss: 0.1538, label: 1, bag_size: 14433\n",
      "batch 739, loss: 0.7694, instance_loss: 0.7110, weighted_loss: 0.7519, label: 0, bag_size: 7917\n",
      "batch 759, loss: 0.0557, instance_loss: 0.5873, weighted_loss: 0.2152, label: 0, bag_size: 19808\n",
      "batch 779, loss: 8.3769, instance_loss: 3.3853, weighted_loss: 6.8794, label: 0, bag_size: 3468\n",
      "batch 799, loss: 0.0243, instance_loss: 0.5256, weighted_loss: 0.1747, label: 0, bag_size: 19466\n",
      "batch 819, loss: 0.0544, instance_loss: 0.6070, weighted_loss: 0.2201, label: 0, bag_size: 29270\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.8967457420924574: correct 11794/13152\n",
      "class 1 clustering acc 0.35933698296836986: correct 2363/6576\n",
      "Epoch: 2, train_loss: 0.4891, train_clustering_loss:  0.8125, train_error: 0.1861\n",
      "class 0: acc 0.8195488721804511, correct 327/399\n",
      "class 1: acc 0.8085106382978723, correct 342/423\n",
      "\n",
      "Val Set, val_loss: 0.3956, val_error: 0.1468, auc: 0.9289\n",
      "class 0 clustering acc 0.7219036697247706: correct 1259/1744\n",
      "class 1 clustering acc 0.6708715596330275: correct 585/872\n",
      "class 0: acc 0.9130434782608695, correct 42/46\n",
      "class 1: acc 0.8095238095238095, correct 51/63\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0027, instance_loss: 0.5514, weighted_loss: 0.1673, label: 1, bag_size: 10482\n",
      "batch 39, loss: 0.0837, instance_loss: 0.4572, weighted_loss: 0.1957, label: 0, bag_size: 10995\n",
      "batch 59, loss: 0.0174, instance_loss: 0.4453, weighted_loss: 0.1458, label: 0, bag_size: 9885\n",
      "batch 79, loss: 0.0126, instance_loss: 0.6660, weighted_loss: 0.2086, label: 1, bag_size: 7935\n",
      "batch 99, loss: 3.1074, instance_loss: 2.6520, weighted_loss: 2.9708, label: 0, bag_size: 14264\n",
      "batch 119, loss: 0.0324, instance_loss: 0.3933, weighted_loss: 0.1407, label: 0, bag_size: 8866\n",
      "batch 139, loss: 0.1264, instance_loss: 0.4143, weighted_loss: 0.2128, label: 1, bag_size: 8003\n",
      "batch 159, loss: 5.5150, instance_loss: 2.9953, weighted_loss: 4.7591, label: 1, bag_size: 2565\n",
      "batch 179, loss: 0.0627, instance_loss: 0.3875, weighted_loss: 0.1602, label: 1, bag_size: 19500\n",
      "batch 199, loss: 4.9035, instance_loss: 4.8080, weighted_loss: 4.8748, label: 0, bag_size: 5105\n",
      "batch 219, loss: 1.7559, instance_loss: 1.4250, weighted_loss: 1.6566, label: 0, bag_size: 6367\n",
      "batch 239, loss: 0.0978, instance_loss: 0.8845, weighted_loss: 0.3338, label: 1, bag_size: 3295\n",
      "batch 259, loss: 2.2792, instance_loss: 1.9238, weighted_loss: 2.1725, label: 1, bag_size: 21252\n",
      "batch 279, loss: 0.0025, instance_loss: 0.2948, weighted_loss: 0.0902, label: 0, bag_size: 1651\n",
      "batch 299, loss: 0.0637, instance_loss: 0.6439, weighted_loss: 0.2378, label: 1, bag_size: 5605\n",
      "batch 319, loss: 0.0657, instance_loss: 0.5156, weighted_loss: 0.2007, label: 1, bag_size: 1924\n",
      "batch 339, loss: 0.0063, instance_loss: 0.4400, weighted_loss: 0.1364, label: 0, bag_size: 12131\n",
      "batch 359, loss: 0.6204, instance_loss: 1.9609, weighted_loss: 1.0225, label: 0, bag_size: 1953\n",
      "batch 379, loss: 0.3766, instance_loss: 0.5690, weighted_loss: 0.4343, label: 0, bag_size: 20478\n",
      "batch 399, loss: 0.8155, instance_loss: 0.8075, weighted_loss: 0.8131, label: 1, bag_size: 1609\n",
      "batch 419, loss: 0.0659, instance_loss: 0.5088, weighted_loss: 0.1988, label: 1, bag_size: 9065\n",
      "batch 439, loss: 0.0044, instance_loss: 0.2996, weighted_loss: 0.0930, label: 0, bag_size: 9060\n",
      "batch 459, loss: 1.2562, instance_loss: 2.0311, weighted_loss: 1.4886, label: 0, bag_size: 2815\n",
      "batch 479, loss: 0.7805, instance_loss: 1.2655, weighted_loss: 0.9260, label: 1, bag_size: 1703\n",
      "batch 499, loss: 0.0882, instance_loss: 0.6455, weighted_loss: 0.2554, label: 0, bag_size: 9596\n",
      "batch 519, loss: 0.1666, instance_loss: 0.5785, weighted_loss: 0.2902, label: 0, bag_size: 10721\n",
      "batch 539, loss: 0.0252, instance_loss: 0.4185, weighted_loss: 0.1432, label: 1, bag_size: 12697\n",
      "batch 559, loss: 0.0596, instance_loss: 0.1846, weighted_loss: 0.0971, label: 1, bag_size: 7110\n",
      "batch 579, loss: 1.4256, instance_loss: 0.9866, weighted_loss: 1.2939, label: 0, bag_size: 9069\n",
      "batch 599, loss: 0.0002, instance_loss: 0.0448, weighted_loss: 0.0136, label: 0, bag_size: 13225\n",
      "batch 619, loss: 0.1112, instance_loss: 0.4086, weighted_loss: 0.2004, label: 1, bag_size: 12408\n",
      "batch 639, loss: 0.0087, instance_loss: 0.3239, weighted_loss: 0.1032, label: 0, bag_size: 3101\n",
      "batch 659, loss: 0.0018, instance_loss: 0.1534, weighted_loss: 0.0473, label: 0, bag_size: 10995\n",
      "batch 679, loss: 0.0159, instance_loss: 0.4121, weighted_loss: 0.1347, label: 1, bag_size: 8026\n",
      "batch 699, loss: 1.4558, instance_loss: 1.1771, weighted_loss: 1.3722, label: 1, bag_size: 771\n",
      "batch 719, loss: 0.7678, instance_loss: 0.8687, weighted_loss: 0.7981, label: 1, bag_size: 5723\n",
      "batch 739, loss: 0.0282, instance_loss: 0.0849, weighted_loss: 0.0452, label: 0, bag_size: 2628\n",
      "batch 759, loss: 0.6313, instance_loss: 0.7257, weighted_loss: 0.6596, label: 0, bag_size: 18738\n",
      "batch 779, loss: 0.0030, instance_loss: 0.0168, weighted_loss: 0.0071, label: 0, bag_size: 1984\n",
      "batch 799, loss: 0.0015, instance_loss: 0.0167, weighted_loss: 0.0060, label: 0, bag_size: 18574\n",
      "batch 819, loss: 0.0665, instance_loss: 0.6153, weighted_loss: 0.2311, label: 1, bag_size: 15093\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9355991484184915: correct 12305/13152\n",
      "class 1 clustering acc 0.5200729927007299: correct 3420/6576\n",
      "Epoch: 3, train_loss: 0.4328, train_clustering_loss:  0.6713, train_error: 0.1727\n",
      "class 0: acc 0.8426150121065376, correct 348/413\n",
      "class 1: acc 0.8117359413202934, correct 332/409\n",
      "\n",
      "Val Set, val_loss: 0.6494, val_error: 0.1743, auc: 0.9303\n",
      "class 0 clustering acc 1.0: correct 1744/1744\n",
      "class 1 clustering acc 0.0194954128440367: correct 17/872\n",
      "class 0: acc 1.0, correct 46/46\n",
      "class 1: acc 0.6984126984126984, correct 44/63\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2769, instance_loss: 0.9928, weighted_loss: 0.4916, label: 0, bag_size: 11212\n",
      "batch 39, loss: 0.0034, instance_loss: 0.2345, weighted_loss: 0.0727, label: 0, bag_size: 8898\n",
      "batch 59, loss: 0.0148, instance_loss: 0.0939, weighted_loss: 0.0386, label: 0, bag_size: 8981\n",
      "batch 79, loss: 0.0252, instance_loss: 0.1083, weighted_loss: 0.0501, label: 1, bag_size: 12895\n",
      "batch 99, loss: 0.0014, instance_loss: 0.0219, weighted_loss: 0.0076, label: 0, bag_size: 15736\n",
      "batch 119, loss: 0.0102, instance_loss: 0.4081, weighted_loss: 0.1296, label: 0, bag_size: 1825\n",
      "batch 139, loss: 0.0002, instance_loss: 0.0285, weighted_loss: 0.0087, label: 0, bag_size: 12793\n",
      "batch 159, loss: 0.0015, instance_loss: 0.0094, weighted_loss: 0.0038, label: 0, bag_size: 21082\n",
      "batch 179, loss: 0.5414, instance_loss: 2.1500, weighted_loss: 1.0240, label: 0, bag_size: 1800\n",
      "batch 199, loss: 1.0883, instance_loss: 1.1879, weighted_loss: 1.1182, label: 0, bag_size: 15636\n",
      "batch 219, loss: 0.6588, instance_loss: 1.6582, weighted_loss: 0.9586, label: 1, bag_size: 699\n",
      "batch 239, loss: 0.0376, instance_loss: 0.3001, weighted_loss: 0.1164, label: 0, bag_size: 3502\n",
      "batch 259, loss: 0.0540, instance_loss: 0.9480, weighted_loss: 0.3222, label: 0, bag_size: 1797\n",
      "batch 279, loss: 0.0093, instance_loss: 0.4743, weighted_loss: 0.1488, label: 1, bag_size: 5494\n",
      "batch 299, loss: 1.3205, instance_loss: 1.2349, weighted_loss: 1.2948, label: 1, bag_size: 1230\n",
      "batch 319, loss: 0.0223, instance_loss: 0.3672, weighted_loss: 0.1258, label: 1, bag_size: 9533\n",
      "batch 339, loss: 0.1530, instance_loss: 0.5903, weighted_loss: 0.2842, label: 1, bag_size: 15665\n",
      "batch 359, loss: 0.0008, instance_loss: 0.1736, weighted_loss: 0.0526, label: 0, bag_size: 21385\n",
      "batch 379, loss: 0.2173, instance_loss: 0.4049, weighted_loss: 0.2736, label: 1, bag_size: 8438\n",
      "batch 399, loss: 0.0975, instance_loss: 0.4976, weighted_loss: 0.2175, label: 0, bag_size: 2004\n",
      "batch 419, loss: 0.0028, instance_loss: 0.6703, weighted_loss: 0.2030, label: 1, bag_size: 3640\n",
      "batch 439, loss: 0.1294, instance_loss: 0.4605, weighted_loss: 0.2288, label: 1, bag_size: 12697\n",
      "batch 459, loss: 0.0709, instance_loss: 0.4565, weighted_loss: 0.1866, label: 1, bag_size: 8103\n",
      "batch 479, loss: 0.4007, instance_loss: 0.3554, weighted_loss: 0.3871, label: 0, bag_size: 2091\n",
      "batch 499, loss: 0.0125, instance_loss: 0.0328, weighted_loss: 0.0186, label: 0, bag_size: 18045\n",
      "batch 519, loss: 0.4000, instance_loss: 0.6047, weighted_loss: 0.4614, label: 0, bag_size: 18516\n",
      "batch 539, loss: 0.1795, instance_loss: 0.2672, weighted_loss: 0.2058, label: 0, bag_size: 2748\n",
      "batch 559, loss: 0.0086, instance_loss: 0.8353, weighted_loss: 0.2566, label: 1, bag_size: 7110\n",
      "batch 579, loss: 0.0005, instance_loss: 0.4720, weighted_loss: 0.1420, label: 1, bag_size: 9955\n",
      "batch 599, loss: 0.0012, instance_loss: 0.0080, weighted_loss: 0.0032, label: 0, bag_size: 10535\n",
      "batch 619, loss: 0.0836, instance_loss: 0.4314, weighted_loss: 0.1879, label: 0, bag_size: 9930\n",
      "batch 639, loss: 0.0440, instance_loss: 0.2825, weighted_loss: 0.1155, label: 1, bag_size: 4128\n",
      "batch 659, loss: 0.1002, instance_loss: 0.4799, weighted_loss: 0.2141, label: 1, bag_size: 629\n",
      "batch 679, loss: 0.0072, instance_loss: 0.0745, weighted_loss: 0.0274, label: 0, bag_size: 9060\n",
      "batch 699, loss: 0.0009, instance_loss: 0.0042, weighted_loss: 0.0019, label: 0, bag_size: 18132\n",
      "batch 719, loss: 0.0029, instance_loss: 0.2893, weighted_loss: 0.0889, label: 1, bag_size: 13015\n",
      "batch 739, loss: 0.1912, instance_loss: 0.5924, weighted_loss: 0.3116, label: 0, bag_size: 1891\n",
      "batch 759, loss: 0.0817, instance_loss: 0.2468, weighted_loss: 0.1312, label: 0, bag_size: 14739\n",
      "batch 779, loss: 0.0182, instance_loss: 0.2500, weighted_loss: 0.0878, label: 1, bag_size: 12425\n",
      "batch 799, loss: 0.0095, instance_loss: 0.1026, weighted_loss: 0.0374, label: 1, bag_size: 1249\n",
      "batch 819, loss: 0.0181, instance_loss: 0.1083, weighted_loss: 0.0452, label: 0, bag_size: 19466\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9421380778588808: correct 12391/13152\n",
      "class 1 clustering acc 0.572992700729927: correct 3768/6576\n",
      "Epoch: 4, train_loss: 0.4244, train_clustering_loss:  0.6228, train_error: 0.1545\n",
      "class 0: acc 0.8459715639810427, correct 357/422\n",
      "class 1: acc 0.845, correct 338/400\n",
      "\n",
      "Val Set, val_loss: 0.3851, val_error: 0.1651, auc: 0.9306\n",
      "class 0 clustering acc 0.7958715596330275: correct 1388/1744\n",
      "class 1 clustering acc 0.4334862385321101: correct 378/872\n",
      "class 0: acc 0.9130434782608695, correct 42/46\n",
      "class 1: acc 0.7777777777777778, correct 49/63\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0015, instance_loss: 0.0064, weighted_loss: 0.0030, label: 0, bag_size: 13795\n",
      "batch 39, loss: 0.0012, instance_loss: 0.0001, weighted_loss: 0.0009, label: 0, bag_size: 31085\n",
      "batch 59, loss: 4.8017, instance_loss: 3.8693, weighted_loss: 4.5220, label: 0, bag_size: 17279\n",
      "batch 79, loss: 0.1417, instance_loss: 0.7942, weighted_loss: 0.3374, label: 0, bag_size: 6281\n",
      "batch 99, loss: 0.0036, instance_loss: 0.2362, weighted_loss: 0.0733, label: 1, bag_size: 15716\n",
      "batch 119, loss: 0.2926, instance_loss: 0.6157, weighted_loss: 0.3895, label: 0, bag_size: 14893\n",
      "batch 139, loss: 0.0205, instance_loss: 0.8137, weighted_loss: 0.2585, label: 0, bag_size: 1639\n",
      "batch 159, loss: 0.0085, instance_loss: 0.0628, weighted_loss: 0.0248, label: 0, bag_size: 13225\n",
      "batch 179, loss: 0.0049, instance_loss: 0.2429, weighted_loss: 0.0763, label: 1, bag_size: 3549\n",
      "batch 199, loss: 1.1876, instance_loss: 1.0005, weighted_loss: 1.1315, label: 0, bag_size: 1701\n",
      "batch 219, loss: 0.0167, instance_loss: 0.3022, weighted_loss: 0.1023, label: 0, bag_size: 14377\n",
      "batch 239, loss: 0.7694, instance_loss: 0.9899, weighted_loss: 0.8356, label: 1, bag_size: 1038\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python main.py --drop_out --early_stopping --lr 2e-4 --k 5 --label_frac 1 \\\n",
    "--exp_code cptac_lung_100_level0_transformer_adam --weighted_sample --bag_loss ce --inst_loss svm \\\n",
    "--task task_2_tumor_subtyping --model_type transmil --log_data --data_root_dir /home/sci/Disk2/CPTAC-LUNG/FEATURES_level0 \\\n",
    "--split_dir /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/cptac_lung_100 --subtyping \\\n",
    "--csv_path dataset_csv/cptac_lung_subtyping.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCBAT Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load Dataset\n",
      "label column: label\n",
      "label dictionary: {'LUAD': 0, 'LUSC': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 0    507\n",
      "1    520\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 164\n",
      "Slide-LVL; Number of samples registered in class 0: 507\n",
      "Patient-LVL; Number of samples registered in class 1: 157\n",
      "Slide-LVL; Number of samples registered in class 1: 520\n",
      "split_dir:  /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/cptac_lung_100\n",
      "################# Settings ###################\n",
      "num_splits:  5\n",
      "k_start:  -1\n",
      "k_end:  -1\n",
      "task:  task_2_tumor_subtyping\n",
      "max_epochs:  200\n",
      "results_dir:  ./results\n",
      "lr:  0.0002\n",
      "experiment:  cptac_lung_100_level02_mcbat_sb_depth1_adam_FLASH\n",
      "reg:  1e-05\n",
      "label_frac:  1.0\n",
      "bag_loss:  ce\n",
      "seed:  1\n",
      "model_type:  mcbat_sb\n",
      "model_size:  small\n",
      "use_drop_out:  True\n",
      "weighted_sample:  True\n",
      "opt:  adam\n",
      "split_dir:  /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/cptac_lung_100\n",
      "\n",
      "Training Fold 0!\n",
      "\n",
      "Init train/val/test splits... \n",
      "Done!\n",
      "Training on 820 samples\n",
      "Validating on 110 samples\n",
      "Testing on 97 samples\n",
      "\n",
      "Init loss function... Done!\n",
      "\n",
      "Init Model... Setting tau to 1.0\n",
      "Done!\n",
      "MCBAT_SB(\n",
      "  (instance_loss_fn): SmoothTop1SVM()\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (transformer_low): TransformerEncoder_FLASH(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FLASH(\n",
      "            (attn_fn): LaplacianAttnFn()\n",
      "            (rel_pos_bias): T5RelativePositionBias(\n",
      "              (relative_attention_bias): Embedding(32, 1)\n",
      "            )\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (to_hidden): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (to_qk): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (qk_offset_scale): OffsetScale()\n",
      "            (to_out): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer_high): TransformerEncoder_FLASH(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FLASH(\n",
      "            (attn_fn): LaplacianAttnFn()\n",
      "            (rel_pos_bias): T5RelativePositionBias(\n",
      "              (relative_attention_bias): Embedding(32, 1)\n",
      "            )\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (to_hidden): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (to_qk): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (qk_offset_scale): OffsetScale()\n",
      "            (to_out): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (projector): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attention): Attn_Net_Gated(\n",
      "    (attention_a): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_b): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Sigmoid()\n",
      "      (2): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "    (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (fusion_encoder): FusionEncoder()\n",
      "  (attention_V2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      "  (attention_U2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (attention_weights2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "Total number of parameters: 24575947\n",
      "Total number of trainable parameters: 24575947\n",
      "\n",
      "Init optimizer ... Done!\n",
      "\n",
      "Init Loaders... 2\n",
      "Done!\n",
      "\n",
      "Setup EarlyStopping... Done!\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5655, instance_loss: 9.8178, weighted_loss: 3.3412, label: 0, bag_size: 18516\n",
      "batch 39, loss: 0.8416, instance_loss: 1.6485, weighted_loss: 1.0837, label: 0, bag_size: 8025\n",
      "batch 59, loss: 0.9255, instance_loss: 6.0913, weighted_loss: 2.4752, label: 1, bag_size: 9446\n",
      "batch 79, loss: 0.8316, instance_loss: 0.8396, weighted_loss: 0.8340, label: 1, bag_size: 645\n",
      "batch 99, loss: 0.7434, instance_loss: 0.8896, weighted_loss: 0.7873, label: 0, bag_size: 2091\n",
      "batch 119, loss: 0.5872, instance_loss: 0.8308, weighted_loss: 0.6603, label: 0, bag_size: 16992\n",
      "batch 139, loss: 0.5187, instance_loss: 2.5152, weighted_loss: 1.1176, label: 0, bag_size: 1789\n",
      "batch 159, loss: 0.5279, instance_loss: 0.8768, weighted_loss: 0.6326, label: 0, bag_size: 1213\n",
      "batch 179, loss: 0.4653, instance_loss: 1.1324, weighted_loss: 0.6654, label: 0, bag_size: 5409\n",
      "batch 199, loss: 0.5902, instance_loss: 0.4450, weighted_loss: 0.5466, label: 0, bag_size: 11654\n",
      "batch 219, loss: 0.6938, instance_loss: 1.8077, weighted_loss: 1.0280, label: 0, bag_size: 1772\n",
      "batch 239, loss: 0.6206, instance_loss: 1.2903, weighted_loss: 0.8215, label: 0, bag_size: 9060\n",
      "batch 259, loss: 0.7603, instance_loss: 0.5250, weighted_loss: 0.6897, label: 1, bag_size: 6164\n",
      "batch 279, loss: 0.6109, instance_loss: 0.6075, weighted_loss: 0.6099, label: 1, bag_size: 645\n",
      "batch 299, loss: 0.4914, instance_loss: 0.7522, weighted_loss: 0.5696, label: 1, bag_size: 4250\n",
      "batch 319, loss: 0.6564, instance_loss: 0.8352, weighted_loss: 0.7101, label: 0, bag_size: 2160\n",
      "batch 339, loss: 0.7895, instance_loss: 1.8189, weighted_loss: 1.0984, label: 1, bag_size: 8216\n",
      "batch 359, loss: 0.7490, instance_loss: 0.7472, weighted_loss: 0.7484, label: 1, bag_size: 9062\n",
      "batch 379, loss: 0.6504, instance_loss: 1.3368, weighted_loss: 0.8563, label: 1, bag_size: 13365\n",
      "batch 399, loss: 0.6941, instance_loss: 1.2388, weighted_loss: 0.8576, label: 1, bag_size: 549\n",
      "batch 419, loss: 0.8759, instance_loss: 0.7484, weighted_loss: 0.8376, label: 0, bag_size: 16690\n",
      "batch 439, loss: 0.6227, instance_loss: 2.1969, weighted_loss: 1.0950, label: 0, bag_size: 10146\n",
      "batch 459, loss: 0.9136, instance_loss: 1.1874, weighted_loss: 0.9958, label: 0, bag_size: 13205\n",
      "batch 479, loss: 0.6977, instance_loss: 0.4065, weighted_loss: 0.6103, label: 0, bag_size: 1639\n",
      "batch 499, loss: 0.6799, instance_loss: 0.6353, weighted_loss: 0.6665, label: 1, bag_size: 8012\n",
      "batch 519, loss: 0.6857, instance_loss: 0.5704, weighted_loss: 0.6511, label: 0, bag_size: 16341\n",
      "batch 539, loss: 0.6966, instance_loss: 0.9706, weighted_loss: 0.7788, label: 0, bag_size: 3670\n",
      "batch 559, loss: 0.7823, instance_loss: 0.6982, weighted_loss: 0.7571, label: 0, bag_size: 2609\n",
      "batch 579, loss: 0.7801, instance_loss: 1.2126, weighted_loss: 0.9099, label: 0, bag_size: 25420\n",
      "batch 599, loss: 0.6393, instance_loss: 2.0980, weighted_loss: 1.0769, label: 1, bag_size: 10725\n",
      "batch 619, loss: 1.0123, instance_loss: 1.1406, weighted_loss: 1.0508, label: 1, bag_size: 7468\n",
      "batch 639, loss: 1.0371, instance_loss: 0.7469, weighted_loss: 0.9500, label: 1, bag_size: 12931\n",
      "batch 659, loss: 0.5337, instance_loss: 1.5145, weighted_loss: 0.8279, label: 1, bag_size: 1497\n",
      "batch 679, loss: 0.3509, instance_loss: 0.9088, weighted_loss: 0.5182, label: 1, bag_size: 7246\n",
      "batch 699, loss: 0.9147, instance_loss: 1.5689, weighted_loss: 1.1110, label: 0, bag_size: 2160\n",
      "batch 719, loss: 0.8424, instance_loss: 1.6130, weighted_loss: 1.0736, label: 1, bag_size: 13194\n",
      "batch 739, loss: 0.5091, instance_loss: 0.3255, weighted_loss: 0.4540, label: 0, bag_size: 20796\n",
      "batch 759, loss: 0.6693, instance_loss: 2.2687, weighted_loss: 1.1491, label: 1, bag_size: 21450\n",
      "batch 779, loss: 0.8277, instance_loss: 2.0059, weighted_loss: 1.1812, label: 0, bag_size: 2219\n",
      "batch 799, loss: 0.5834, instance_loss: 0.4000, weighted_loss: 0.5284, label: 1, bag_size: 8685\n",
      "batch 819, loss: 0.9488, instance_loss: 1.0949, weighted_loss: 0.9927, label: 0, bag_size: 3101\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.8745426829268292: correct 11474/13120\n",
      "class 1 clustering acc 0.3408536585365854: correct 2236/6560\n",
      "Epoch: 0, train_loss: 0.7118, train_clustering_loss:  1.3087, train_error: 0.5122\n",
      "class 0: acc 0.5326876513317191, correct 220/413\n",
      "class 1: acc 0.44226044226044225, correct 180/407\n",
      "\n",
      "Val Set, val_loss: 0.6947, val_error: 0.4727, auc: 0.7590\n",
      "class 0 clustering acc 0.9431818181818182: correct 1660/1760\n",
      "class 1 clustering acc 0.7443181818181818: correct 655/880\n",
      "class 0: acc 0.0, correct 0/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "Validation loss decreased (inf --> 0.694745).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7530, instance_loss: 1.3226, weighted_loss: 0.9239, label: 0, bag_size: 2694\n",
      "batch 39, loss: 0.6404, instance_loss: 1.5782, weighted_loss: 0.9217, label: 0, bag_size: 1416\n",
      "batch 59, loss: 0.6268, instance_loss: 1.6339, weighted_loss: 0.9289, label: 0, bag_size: 3897\n",
      "batch 79, loss: 0.6693, instance_loss: 0.1078, weighted_loss: 0.5008, label: 0, bag_size: 17633\n",
      "batch 99, loss: 0.3712, instance_loss: 0.4864, weighted_loss: 0.4057, label: 0, bag_size: 10481\n",
      "batch 119, loss: 0.6838, instance_loss: 0.7597, weighted_loss: 0.7065, label: 1, bag_size: 9065\n",
      "batch 139, loss: 1.0268, instance_loss: 2.8397, weighted_loss: 1.5707, label: 0, bag_size: 10146\n",
      "batch 159, loss: 0.6882, instance_loss: 2.2984, weighted_loss: 1.1712, label: 1, bag_size: 865\n",
      "batch 179, loss: 0.7255, instance_loss: 0.4284, weighted_loss: 0.6363, label: 0, bag_size: 22800\n",
      "batch 199, loss: 0.7179, instance_loss: 0.5241, weighted_loss: 0.6597, label: 1, bag_size: 4039\n",
      "batch 219, loss: 0.6399, instance_loss: 0.7238, weighted_loss: 0.6651, label: 0, bag_size: 7612\n",
      "batch 239, loss: 0.8299, instance_loss: 0.6097, weighted_loss: 0.7639, label: 1, bag_size: 5903\n",
      "batch 259, loss: 0.5665, instance_loss: 0.0578, weighted_loss: 0.4139, label: 1, bag_size: 19972\n",
      "batch 279, loss: 0.9566, instance_loss: 0.2185, weighted_loss: 0.7352, label: 0, bag_size: 4345\n",
      "batch 299, loss: 0.6222, instance_loss: 0.0840, weighted_loss: 0.4607, label: 0, bag_size: 21218\n",
      "batch 319, loss: 0.7182, instance_loss: 1.5157, weighted_loss: 0.9574, label: 0, bag_size: 3474\n",
      "batch 339, loss: 0.7488, instance_loss: 1.4846, weighted_loss: 0.9695, label: 1, bag_size: 2179\n",
      "batch 359, loss: 0.7089, instance_loss: 0.9668, weighted_loss: 0.7862, label: 1, bag_size: 8680\n",
      "batch 379, loss: 0.5037, instance_loss: 0.5031, weighted_loss: 0.5035, label: 1, bag_size: 4128\n",
      "batch 399, loss: 0.5112, instance_loss: 0.9468, weighted_loss: 0.6419, label: 1, bag_size: 5292\n",
      "batch 419, loss: 0.8961, instance_loss: 4.0989, weighted_loss: 1.8569, label: 0, bag_size: 4692\n",
      "batch 439, loss: 1.1314, instance_loss: 0.7206, weighted_loss: 1.0082, label: 0, bag_size: 8788\n",
      "batch 459, loss: 0.3718, instance_loss: 0.1834, weighted_loss: 0.3153, label: 1, bag_size: 15332\n",
      "batch 479, loss: 0.7346, instance_loss: 0.6896, weighted_loss: 0.7211, label: 0, bag_size: 2266\n",
      "batch 499, loss: 0.9404, instance_loss: 0.3469, weighted_loss: 0.7624, label: 1, bag_size: 7246\n",
      "batch 519, loss: 0.6318, instance_loss: 0.4541, weighted_loss: 0.5785, label: 0, bag_size: 2628\n",
      "batch 539, loss: 0.8395, instance_loss: 0.4076, weighted_loss: 0.7099, label: 1, bag_size: 9877\n",
      "batch 559, loss: 0.5539, instance_loss: 0.4967, weighted_loss: 0.5368, label: 1, bag_size: 15332\n",
      "batch 579, loss: 0.7664, instance_loss: 0.8059, weighted_loss: 0.7782, label: 0, bag_size: 13619\n",
      "batch 599, loss: 0.5216, instance_loss: 1.0030, weighted_loss: 0.6660, label: 1, bag_size: 8680\n",
      "batch 619, loss: 0.7250, instance_loss: 1.2216, weighted_loss: 0.8740, label: 1, bag_size: 6665\n",
      "batch 639, loss: 0.7817, instance_loss: 2.8067, weighted_loss: 1.3892, label: 1, bag_size: 2937\n",
      "batch 659, loss: 0.7266, instance_loss: 0.0837, weighted_loss: 0.5337, label: 1, bag_size: 12575\n",
      "batch 679, loss: 0.7939, instance_loss: 0.1770, weighted_loss: 0.6088, label: 1, bag_size: 9408\n",
      "batch 699, loss: 0.6413, instance_loss: 0.8672, weighted_loss: 0.7091, label: 0, bag_size: 2920\n",
      "batch 719, loss: 0.7256, instance_loss: 1.1213, weighted_loss: 0.8443, label: 0, bag_size: 8755\n",
      "batch 739, loss: 0.6357, instance_loss: 0.2208, weighted_loss: 0.5112, label: 1, bag_size: 2522\n",
      "batch 759, loss: 0.9225, instance_loss: 0.0079, weighted_loss: 0.6481, label: 0, bag_size: 9060\n",
      "batch 779, loss: 0.5342, instance_loss: 1.0101, weighted_loss: 0.6770, label: 1, bag_size: 11421\n",
      "batch 799, loss: 0.5800, instance_loss: 0.0050, weighted_loss: 0.4075, label: 1, bag_size: 4789\n",
      "batch 819, loss: 0.8747, instance_loss: 0.7025, weighted_loss: 0.8230, label: 0, bag_size: 8948\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9055640243902439: correct 11881/13120\n",
      "class 1 clustering acc 0.5214939024390244: correct 3421/6560\n",
      "Epoch: 1, train_loss: 0.7098, train_clustering_loss:  0.8673, train_error: 0.5085\n",
      "class 0: acc 0.41414141414141414, correct 164/396\n",
      "class 1: acc 0.5636792452830188, correct 239/424\n",
      "\n",
      "Val Set, val_loss: 0.6950, val_error: 0.4727, auc: 0.8558\n",
      "class 0 clustering acc 0.9607954545454546: correct 1691/1760\n",
      "class 1 clustering acc 0.4125: correct 363/880\n",
      "class 0: acc 0.0, correct 0/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.8255, instance_loss: 0.5230, weighted_loss: 0.7347, label: 1, bag_size: 5160\n",
      "batch 39, loss: 0.8086, instance_loss: 0.3347, weighted_loss: 0.6664, label: 0, bag_size: 12217\n",
      "batch 59, loss: 0.7855, instance_loss: 0.5687, weighted_loss: 0.7204, label: 1, bag_size: 8466\n",
      "batch 79, loss: 0.7567, instance_loss: 0.6957, weighted_loss: 0.7384, label: 0, bag_size: 20796\n",
      "batch 99, loss: 0.5742, instance_loss: 0.6268, weighted_loss: 0.5900, label: 1, bag_size: 7613\n",
      "batch 119, loss: 0.4083, instance_loss: 0.0102, weighted_loss: 0.2889, label: 1, bag_size: 2936\n",
      "batch 139, loss: 0.6398, instance_loss: 2.7537, weighted_loss: 1.2740, label: 0, bag_size: 14264\n",
      "batch 159, loss: 0.5848, instance_loss: 0.2710, weighted_loss: 0.4907, label: 1, bag_size: 1525\n",
      "batch 179, loss: 1.1256, instance_loss: 5.6602, weighted_loss: 2.4860, label: 0, bag_size: 3802\n",
      "batch 199, loss: 0.8674, instance_loss: 0.2177, weighted_loss: 0.6725, label: 0, bag_size: 9851\n",
      "batch 219, loss: 0.4697, instance_loss: 0.3901, weighted_loss: 0.4458, label: 1, bag_size: 9215\n",
      "batch 239, loss: 0.5380, instance_loss: 0.0456, weighted_loss: 0.3903, label: 1, bag_size: 6842\n",
      "batch 259, loss: 0.7319, instance_loss: 3.5643, weighted_loss: 1.5816, label: 0, bag_size: 3802\n",
      "batch 279, loss: 0.5392, instance_loss: 0.0596, weighted_loss: 0.3953, label: 1, bag_size: 6950\n",
      "batch 299, loss: 0.4823, instance_loss: 0.2707, weighted_loss: 0.4189, label: 1, bag_size: 9673\n",
      "batch 319, loss: 0.7791, instance_loss: 0.0063, weighted_loss: 0.5472, label: 0, bag_size: 13619\n",
      "batch 339, loss: 0.5626, instance_loss: 0.0255, weighted_loss: 0.4015, label: 0, bag_size: 14305\n",
      "batch 359, loss: 0.8585, instance_loss: 0.2888, weighted_loss: 0.6876, label: 1, bag_size: 5454\n",
      "batch 379, loss: 0.7729, instance_loss: 0.2331, weighted_loss: 0.6110, label: 1, bag_size: 9078\n",
      "batch 399, loss: 0.7250, instance_loss: 3.7095, weighted_loss: 1.6204, label: 0, bag_size: 47866\n",
      "batch 419, loss: 0.6303, instance_loss: 0.0947, weighted_loss: 0.4696, label: 1, bag_size: 10105\n",
      "batch 439, loss: 0.6504, instance_loss: 3.1524, weighted_loss: 1.4010, label: 1, bag_size: 10591\n",
      "batch 459, loss: 0.9045, instance_loss: 0.7514, weighted_loss: 0.8586, label: 0, bag_size: 22828\n",
      "batch 479, loss: 0.6669, instance_loss: 0.6305, weighted_loss: 0.6560, label: 1, bag_size: 9065\n",
      "batch 499, loss: 0.6063, instance_loss: 0.1093, weighted_loss: 0.4572, label: 0, bag_size: 5297\n",
      "batch 519, loss: 0.8693, instance_loss: 0.0052, weighted_loss: 0.6101, label: 1, bag_size: 5317\n",
      "batch 539, loss: 0.4781, instance_loss: 0.1138, weighted_loss: 0.3688, label: 0, bag_size: 3970\n",
      "batch 559, loss: 0.6052, instance_loss: 0.3556, weighted_loss: 0.5303, label: 0, bag_size: 8755\n",
      "batch 579, loss: 0.8249, instance_loss: 0.0053, weighted_loss: 0.5790, label: 0, bag_size: 14681\n",
      "batch 599, loss: 0.5398, instance_loss: 0.4135, weighted_loss: 0.5019, label: 1, bag_size: 9230\n",
      "batch 619, loss: 0.7027, instance_loss: 0.0396, weighted_loss: 0.5038, label: 0, bag_size: 3552\n",
      "batch 639, loss: 0.6411, instance_loss: 0.2366, weighted_loss: 0.5197, label: 0, bag_size: 2036\n",
      "batch 659, loss: 0.7179, instance_loss: 0.6029, weighted_loss: 0.6834, label: 1, bag_size: 1759\n",
      "batch 679, loss: 0.5791, instance_loss: 0.8630, weighted_loss: 0.6643, label: 0, bag_size: 518\n",
      "batch 699, loss: 0.6740, instance_loss: 1.1867, weighted_loss: 0.8278, label: 0, bag_size: 1142\n",
      "batch 719, loss: 0.5819, instance_loss: 1.1833, weighted_loss: 0.7623, label: 0, bag_size: 2998\n",
      "batch 739, loss: 1.0885, instance_loss: 0.8557, weighted_loss: 1.0186, label: 0, bag_size: 13619\n",
      "batch 759, loss: 0.8529, instance_loss: 0.0254, weighted_loss: 0.6046, label: 0, bag_size: 10146\n",
      "batch 779, loss: 0.5639, instance_loss: 1.0190, weighted_loss: 0.7004, label: 0, bag_size: 1732\n",
      "batch 799, loss: 0.6450, instance_loss: 0.0542, weighted_loss: 0.4678, label: 0, bag_size: 8866\n",
      "batch 819, loss: 0.6909, instance_loss: 0.2377, weighted_loss: 0.5549, label: 1, bag_size: 20767\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.940625: correct 12341/13120\n",
      "class 1 clustering acc 0.6515243902439024: correct 4274/6560\n",
      "Epoch: 2, train_loss: 0.7048, train_clustering_loss:  0.5850, train_error: 0.4963\n",
      "class 0: acc 0.4198473282442748, correct 165/393\n",
      "class 1: acc 0.5807962529274004, correct 248/427\n",
      "\n",
      "Val Set, val_loss: 0.6908, val_error: 0.1818, auc: 0.9244\n",
      "class 0 clustering acc 0.9744318181818182: correct 1715/1760\n",
      "class 1 clustering acc 0.6602272727272728: correct 581/880\n",
      "class 0: acc 0.7692307692307693, correct 40/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "Validation loss decreased (0.694745 --> 0.690811).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6068, instance_loss: 0.2457, weighted_loss: 0.4985, label: 0, bag_size: 1814\n",
      "batch 39, loss: 0.5469, instance_loss: 0.0067, weighted_loss: 0.3848, label: 0, bag_size: 13205\n",
      "batch 59, loss: 0.6076, instance_loss: 0.0337, weighted_loss: 0.4354, label: 1, bag_size: 6478\n",
      "batch 79, loss: 0.7159, instance_loss: 1.1021, weighted_loss: 0.8318, label: 1, bag_size: 7513\n",
      "batch 99, loss: 0.5974, instance_loss: 0.6039, weighted_loss: 0.5993, label: 0, bag_size: 2004\n",
      "batch 119, loss: 0.8781, instance_loss: 0.0034, weighted_loss: 0.6157, label: 1, bag_size: 10033\n",
      "batch 139, loss: 0.6380, instance_loss: 0.9724, weighted_loss: 0.7384, label: 0, bag_size: 9132\n",
      "batch 159, loss: 0.4629, instance_loss: 0.9969, weighted_loss: 0.6231, label: 0, bag_size: 10113\n",
      "batch 179, loss: 0.6953, instance_loss: 0.1785, weighted_loss: 0.5403, label: 0, bag_size: 1962\n",
      "batch 199, loss: 0.6655, instance_loss: 0.7483, weighted_loss: 0.6903, label: 0, bag_size: 15071\n",
      "batch 219, loss: 0.7092, instance_loss: 0.1387, weighted_loss: 0.5380, label: 1, bag_size: 6343\n",
      "batch 239, loss: 0.8364, instance_loss: 0.4919, weighted_loss: 0.7330, label: 1, bag_size: 16379\n",
      "batch 259, loss: 0.6325, instance_loss: 1.1114, weighted_loss: 0.7762, label: 0, bag_size: 1560\n",
      "batch 279, loss: 0.6079, instance_loss: 0.0071, weighted_loss: 0.4277, label: 0, bag_size: 8744\n",
      "batch 299, loss: 0.6840, instance_loss: 0.0000, weighted_loss: 0.4788, label: 0, bag_size: 18045\n",
      "batch 319, loss: 0.4964, instance_loss: 0.0420, weighted_loss: 0.3601, label: 0, bag_size: 10995\n",
      "batch 339, loss: 0.4194, instance_loss: 0.0105, weighted_loss: 0.2967, label: 0, bag_size: 10942\n",
      "batch 359, loss: 1.1014, instance_loss: 0.3460, weighted_loss: 0.8748, label: 1, bag_size: 6950\n",
      "batch 379, loss: 0.5747, instance_loss: 0.0069, weighted_loss: 0.4044, label: 0, bag_size: 21138\n",
      "batch 399, loss: 0.5930, instance_loss: 0.9063, weighted_loss: 0.6870, label: 0, bag_size: 1370\n",
      "batch 419, loss: 0.7651, instance_loss: 0.1660, weighted_loss: 0.5854, label: 1, bag_size: 1924\n",
      "batch 439, loss: 0.7401, instance_loss: 1.4843, weighted_loss: 0.9633, label: 1, bag_size: 2904\n",
      "batch 459, loss: 0.9287, instance_loss: 1.0250, weighted_loss: 0.9576, label: 1, bag_size: 2480\n",
      "batch 479, loss: 0.8409, instance_loss: 0.0954, weighted_loss: 0.6173, label: 1, bag_size: 5605\n",
      "batch 499, loss: 0.7157, instance_loss: 0.2099, weighted_loss: 0.5640, label: 0, bag_size: 2044\n",
      "batch 519, loss: 0.7361, instance_loss: 0.3619, weighted_loss: 0.6238, label: 0, bag_size: 2244\n",
      "batch 539, loss: 0.7851, instance_loss: 0.3668, weighted_loss: 0.6596, label: 0, bag_size: 7557\n",
      "batch 559, loss: 0.6327, instance_loss: 0.0004, weighted_loss: 0.4430, label: 1, bag_size: 6164\n",
      "batch 579, loss: 0.8585, instance_loss: 0.8396, weighted_loss: 0.8528, label: 0, bag_size: 4497\n",
      "batch 599, loss: 0.5385, instance_loss: 1.1532, weighted_loss: 0.7229, label: 1, bag_size: 10072\n",
      "batch 619, loss: 0.4631, instance_loss: 0.1574, weighted_loss: 0.3714, label: 1, bag_size: 2179\n",
      "batch 639, loss: 0.9820, instance_loss: 0.4310, weighted_loss: 0.8167, label: 0, bag_size: 14249\n",
      "batch 659, loss: 0.6240, instance_loss: 1.1340, weighted_loss: 0.7770, label: 0, bag_size: 3783\n",
      "batch 679, loss: 0.7099, instance_loss: 3.6336, weighted_loss: 1.5870, label: 0, bag_size: 21076\n",
      "batch 699, loss: 0.7027, instance_loss: 0.0559, weighted_loss: 0.5087, label: 0, bag_size: 8755\n",
      "batch 719, loss: 0.6419, instance_loss: 0.0234, weighted_loss: 0.4563, label: 0, bag_size: 10490\n",
      "batch 739, loss: 0.4782, instance_loss: 0.0277, weighted_loss: 0.3430, label: 0, bag_size: 13205\n",
      "batch 759, loss: 0.5271, instance_loss: 0.1337, weighted_loss: 0.4091, label: 0, bag_size: 19043\n",
      "batch 779, loss: 0.7053, instance_loss: 0.1911, weighted_loss: 0.5510, label: 0, bag_size: 2006\n",
      "batch 799, loss: 0.6272, instance_loss: 1.4308, weighted_loss: 0.8683, label: 0, bag_size: 7557\n",
      "batch 819, loss: 0.4105, instance_loss: 0.0001, weighted_loss: 0.2874, label: 0, bag_size: 16341\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9429115853658536: correct 12371/13120\n",
      "class 1 clustering acc 0.6708841463414634: correct 4401/6560\n",
      "Epoch: 3, train_loss: 0.6909, train_clustering_loss:  0.5558, train_error: 0.4585\n",
      "class 0: acc 0.6944444444444444, correct 300/432\n",
      "class 1: acc 0.3711340206185567, correct 144/388\n",
      "\n",
      "Val Set, val_loss: 0.7157, val_error: 0.5273, auc: 0.9406\n",
      "class 0 clustering acc 0.9403409090909091: correct 1655/1760\n",
      "class 1 clustering acc 0.7920454545454545: correct 697/880\n",
      "class 0: acc 1.0, correct 52/52\n",
      "class 1: acc 0.0, correct 0/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6642, instance_loss: 0.0219, weighted_loss: 0.4715, label: 1, bag_size: 10920\n",
      "batch 39, loss: 0.6164, instance_loss: 0.1778, weighted_loss: 0.4848, label: 1, bag_size: 10622\n",
      "batch 59, loss: 0.8853, instance_loss: 0.0760, weighted_loss: 0.6425, label: 0, bag_size: 21361\n",
      "batch 79, loss: 0.7755, instance_loss: 0.2514, weighted_loss: 0.6182, label: 1, bag_size: 22264\n",
      "batch 99, loss: 0.5479, instance_loss: 0.6138, weighted_loss: 0.5676, label: 1, bag_size: 16154\n",
      "batch 119, loss: 0.4352, instance_loss: 0.0006, weighted_loss: 0.3048, label: 0, bag_size: 12217\n",
      "batch 139, loss: 0.9456, instance_loss: 0.1903, weighted_loss: 0.7190, label: 1, bag_size: 21450\n",
      "batch 159, loss: 0.5723, instance_loss: 0.0448, weighted_loss: 0.4141, label: 1, bag_size: 6343\n",
      "batch 179, loss: 0.7893, instance_loss: 0.2818, weighted_loss: 0.6371, label: 0, bag_size: 2920\n",
      "batch 199, loss: 0.6135, instance_loss: 1.3581, weighted_loss: 0.8369, label: 0, bag_size: 1506\n",
      "batch 219, loss: 0.9932, instance_loss: 1.4365, weighted_loss: 1.1262, label: 1, bag_size: 1255\n",
      "batch 239, loss: 0.5397, instance_loss: 0.0939, weighted_loss: 0.4060, label: 0, bag_size: 1814\n",
      "batch 259, loss: 1.0423, instance_loss: 0.4359, weighted_loss: 0.8604, label: 1, bag_size: 2936\n",
      "batch 279, loss: 0.8221, instance_loss: 0.9704, weighted_loss: 0.8666, label: 1, bag_size: 865\n",
      "batch 299, loss: 0.5381, instance_loss: 0.0560, weighted_loss: 0.3935, label: 1, bag_size: 14681\n",
      "batch 319, loss: 0.7729, instance_loss: 0.7793, weighted_loss: 0.7748, label: 0, bag_size: 4997\n",
      "batch 339, loss: 0.7169, instance_loss: 0.0008, weighted_loss: 0.5021, label: 0, bag_size: 18225\n",
      "batch 359, loss: 0.5982, instance_loss: 0.1476, weighted_loss: 0.4630, label: 0, bag_size: 1415\n",
      "batch 379, loss: 0.7532, instance_loss: 0.2446, weighted_loss: 0.6006, label: 0, bag_size: 6652\n",
      "batch 399, loss: 0.5942, instance_loss: 0.2790, weighted_loss: 0.4997, label: 1, bag_size: 10396\n",
      "batch 419, loss: 0.7095, instance_loss: 0.1372, weighted_loss: 0.5378, label: 0, bag_size: 11212\n",
      "batch 439, loss: 0.7026, instance_loss: 0.0081, weighted_loss: 0.4943, label: 1, bag_size: 30675\n",
      "batch 459, loss: 0.7079, instance_loss: 0.1353, weighted_loss: 0.5361, label: 0, bag_size: 18738\n",
      "batch 479, loss: 0.6325, instance_loss: 0.2477, weighted_loss: 0.5170, label: 0, bag_size: 1824\n",
      "batch 499, loss: 0.8838, instance_loss: 0.4957, weighted_loss: 0.7674, label: 1, bag_size: 9004\n",
      "batch 519, loss: 0.8597, instance_loss: 0.4005, weighted_loss: 0.7220, label: 0, bag_size: 7605\n",
      "batch 539, loss: 0.4721, instance_loss: 0.0063, weighted_loss: 0.3323, label: 1, bag_size: 12603\n",
      "batch 559, loss: 0.6184, instance_loss: 0.0007, weighted_loss: 0.4331, label: 0, bag_size: 18415\n",
      "batch 579, loss: 0.5458, instance_loss: 0.0282, weighted_loss: 0.3905, label: 0, bag_size: 16720\n",
      "batch 599, loss: 0.8215, instance_loss: 0.0026, weighted_loss: 0.5758, label: 0, bag_size: 15001\n",
      "batch 619, loss: 0.9688, instance_loss: 0.2600, weighted_loss: 0.7562, label: 0, bag_size: 10490\n",
      "batch 639, loss: 1.0068, instance_loss: 1.2552, weighted_loss: 1.0813, label: 0, bag_size: 10490\n",
      "batch 659, loss: 0.8312, instance_loss: 0.7523, weighted_loss: 0.8075, label: 0, bag_size: 9132\n",
      "batch 679, loss: 0.5733, instance_loss: 0.0054, weighted_loss: 0.4029, label: 0, bag_size: 14333\n",
      "batch 699, loss: 0.4965, instance_loss: 0.0054, weighted_loss: 0.3492, label: 0, bag_size: 19390\n",
      "batch 719, loss: 0.7368, instance_loss: 0.8981, weighted_loss: 0.7852, label: 1, bag_size: 9215\n",
      "batch 739, loss: 0.4028, instance_loss: 0.1324, weighted_loss: 0.3217, label: 1, bag_size: 14887\n",
      "batch 759, loss: 0.4147, instance_loss: 0.3584, weighted_loss: 0.3978, label: 1, bag_size: 9942\n",
      "batch 779, loss: 0.7158, instance_loss: 0.1116, weighted_loss: 0.5346, label: 0, bag_size: 9415\n",
      "batch 799, loss: 0.5383, instance_loss: 0.0119, weighted_loss: 0.3804, label: 1, bag_size: 5903\n",
      "batch 819, loss: 0.7756, instance_loss: 0.1333, weighted_loss: 0.5829, label: 1, bag_size: 4862\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9571646341463415: correct 12558/13120\n",
      "class 1 clustering acc 0.7469512195121951: correct 4900/6560\n",
      "Epoch: 4, train_loss: 0.6945, train_clustering_loss:  0.4462, train_error: 0.4841\n",
      "class 0: acc 0.48284313725490197, correct 197/408\n",
      "class 1: acc 0.5485436893203883, correct 226/412\n",
      "\n",
      "Val Set, val_loss: 0.6769, val_error: 0.5273, auc: 0.9390\n",
      "class 0 clustering acc 0.9255681818181818: correct 1629/1760\n",
      "class 1 clustering acc 0.7863636363636364: correct 692/880\n",
      "class 0: acc 1.0, correct 52/52\n",
      "class 1: acc 0.0, correct 0/58\n",
      "Validation loss decreased (0.690811 --> 0.676872).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4857, instance_loss: 0.0005, weighted_loss: 0.3401, label: 0, bag_size: 13892\n",
      "batch 39, loss: 0.3962, instance_loss: 0.0066, weighted_loss: 0.2793, label: 0, bag_size: 12201\n",
      "batch 59, loss: 0.8052, instance_loss: 0.1719, weighted_loss: 0.6152, label: 1, bag_size: 10028\n",
      "batch 79, loss: 0.6076, instance_loss: 0.2750, weighted_loss: 0.5078, label: 0, bag_size: 16720\n",
      "batch 99, loss: 0.5057, instance_loss: 0.5585, weighted_loss: 0.5215, label: 0, bag_size: 1831\n",
      "batch 119, loss: 0.5156, instance_loss: 0.0043, weighted_loss: 0.3622, label: 0, bag_size: 14681\n",
      "batch 139, loss: 0.7315, instance_loss: 0.1046, weighted_loss: 0.5434, label: 1, bag_size: 9877\n",
      "batch 159, loss: 0.5476, instance_loss: 0.0131, weighted_loss: 0.3872, label: 0, bag_size: 11865\n",
      "batch 179, loss: 0.6849, instance_loss: 0.3145, weighted_loss: 0.5738, label: 1, bag_size: 5561\n",
      "batch 199, loss: 0.9855, instance_loss: 0.8195, weighted_loss: 0.9357, label: 0, bag_size: 2609\n",
      "batch 219, loss: 0.5834, instance_loss: 1.2528, weighted_loss: 0.7842, label: 1, bag_size: 2179\n",
      "batch 239, loss: 0.6166, instance_loss: 1.1688, weighted_loss: 0.7823, label: 1, bag_size: 2904\n",
      "batch 259, loss: 0.5781, instance_loss: 0.0047, weighted_loss: 0.4061, label: 0, bag_size: 8948\n",
      "batch 279, loss: 0.4913, instance_loss: 0.5476, weighted_loss: 0.5082, label: 1, bag_size: 3968\n",
      "batch 299, loss: 0.4578, instance_loss: 0.4115, weighted_loss: 0.4439, label: 1, bag_size: 6453\n",
      "batch 319, loss: 0.7320, instance_loss: 0.2717, weighted_loss: 0.5939, label: 1, bag_size: 11266\n",
      "batch 339, loss: 1.2683, instance_loss: 3.1769, weighted_loss: 1.8408, label: 1, bag_size: 2565\n",
      "batch 359, loss: 0.7063, instance_loss: 0.1708, weighted_loss: 0.5457, label: 1, bag_size: 2385\n",
      "batch 379, loss: 0.6810, instance_loss: 0.9687, weighted_loss: 0.7673, label: 0, bag_size: 9597\n",
      "batch 399, loss: 0.7280, instance_loss: 0.2889, weighted_loss: 0.5963, label: 1, bag_size: 8191\n",
      "batch 419, loss: 0.7874, instance_loss: 1.0227, weighted_loss: 0.8580, label: 1, bag_size: 3409\n",
      "batch 439, loss: 0.7765, instance_loss: 0.0188, weighted_loss: 0.5492, label: 1, bag_size: 15213\n",
      "batch 459, loss: 0.7677, instance_loss: 0.1100, weighted_loss: 0.5704, label: 1, bag_size: 11266\n",
      "batch 479, loss: 0.9577, instance_loss: 0.0023, weighted_loss: 0.6711, label: 1, bag_size: 8592\n",
      "batch 499, loss: 0.3086, instance_loss: 0.0057, weighted_loss: 0.2178, label: 0, bag_size: 15001\n",
      "batch 519, loss: 0.4219, instance_loss: 0.0815, weighted_loss: 0.3198, label: 0, bag_size: 2534\n",
      "batch 539, loss: 0.8557, instance_loss: 0.0437, weighted_loss: 0.6121, label: 0, bag_size: 2628\n",
      "batch 559, loss: 0.9723, instance_loss: 4.3087, weighted_loss: 1.9732, label: 0, bag_size: 5105\n",
      "batch 579, loss: 0.6341, instance_loss: 1.3126, weighted_loss: 0.8377, label: 1, bag_size: 865\n",
      "batch 599, loss: 0.6433, instance_loss: 0.0290, weighted_loss: 0.4590, label: 1, bag_size: 7110\n",
      "batch 619, loss: 0.3726, instance_loss: 0.0046, weighted_loss: 0.2622, label: 1, bag_size: 18794\n",
      "batch 639, loss: 1.3571, instance_loss: 0.2904, weighted_loss: 1.0371, label: 0, bag_size: 4345\n",
      "batch 659, loss: 0.9581, instance_loss: 0.2777, weighted_loss: 0.7540, label: 0, bag_size: 12731\n",
      "batch 679, loss: 0.5888, instance_loss: 0.8380, weighted_loss: 0.6636, label: 1, bag_size: 9470\n",
      "batch 699, loss: 0.6603, instance_loss: 0.0812, weighted_loss: 0.4866, label: 1, bag_size: 14618\n",
      "batch 719, loss: 0.4855, instance_loss: 0.3260, weighted_loss: 0.4376, label: 0, bag_size: 13619\n",
      "batch 739, loss: 0.7347, instance_loss: 0.8940, weighted_loss: 0.7825, label: 0, bag_size: 2104\n",
      "batch 759, loss: 0.4250, instance_loss: 0.0911, weighted_loss: 0.3248, label: 1, bag_size: 617\n",
      "batch 779, loss: 0.6454, instance_loss: 0.0047, weighted_loss: 0.4532, label: 0, bag_size: 23714\n",
      "batch 799, loss: 0.4462, instance_loss: 0.3077, weighted_loss: 0.4047, label: 0, bag_size: 4497\n",
      "batch 819, loss: 0.7266, instance_loss: 1.2808, weighted_loss: 0.8929, label: 1, bag_size: 12494\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9585365853658536: correct 12576/13120\n",
      "class 1 clustering acc 0.7359756097560975: correct 4828/6560\n",
      "Epoch: 5, train_loss: 0.6455, train_clustering_loss:  0.4410, train_error: 0.3646\n",
      "class 0: acc 0.7344497607655502, correct 307/418\n",
      "class 1: acc 0.5323383084577115, correct 214/402\n",
      "\n",
      "Val Set, val_loss: 0.5994, val_error: 0.1636, auc: 0.9410\n",
      "class 0 clustering acc 0.9590909090909091: correct 1688/1760\n",
      "class 1 clustering acc 0.6920454545454545: correct 609/880\n",
      "class 0: acc 0.75, correct 39/52\n",
      "class 1: acc 0.9137931034482759, correct 53/58\n",
      "Validation loss decreased (0.676872 --> 0.599352).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.7252, instance_loss: 0.0808, weighted_loss: 0.5319, label: 0, bag_size: 10995\n",
      "batch 39, loss: 0.7214, instance_loss: 0.0356, weighted_loss: 0.5157, label: 0, bag_size: 4497\n",
      "batch 59, loss: 0.7110, instance_loss: 0.7506, weighted_loss: 0.7229, label: 0, bag_size: 4523\n",
      "batch 79, loss: 0.4199, instance_loss: 0.2911, weighted_loss: 0.3812, label: 1, bag_size: 3651\n",
      "batch 99, loss: 0.6296, instance_loss: 0.0106, weighted_loss: 0.4439, label: 0, bag_size: 10113\n",
      "batch 119, loss: 0.8948, instance_loss: 0.3884, weighted_loss: 0.7429, label: 1, bag_size: 11160\n",
      "batch 139, loss: 0.8481, instance_loss: 2.6429, weighted_loss: 1.3866, label: 1, bag_size: 2935\n",
      "batch 159, loss: 0.8666, instance_loss: 0.4357, weighted_loss: 0.7373, label: 0, bag_size: 9542\n",
      "batch 179, loss: 0.7458, instance_loss: 0.6790, weighted_loss: 0.7257, label: 0, bag_size: 4997\n",
      "batch 199, loss: 0.3423, instance_loss: 0.0378, weighted_loss: 0.2510, label: 1, bag_size: 4786\n",
      "batch 219, loss: 0.4514, instance_loss: 1.4113, weighted_loss: 0.7394, label: 1, bag_size: 6950\n",
      "batch 239, loss: 0.5960, instance_loss: 0.6224, weighted_loss: 0.6039, label: 1, bag_size: 8191\n",
      "batch 259, loss: 0.7701, instance_loss: 1.4373, weighted_loss: 0.9703, label: 1, bag_size: 9162\n",
      "batch 279, loss: 0.5910, instance_loss: 0.0977, weighted_loss: 0.4430, label: 1, bag_size: 11195\n",
      "batch 299, loss: 1.0780, instance_loss: 2.0548, weighted_loss: 1.3710, label: 1, bag_size: 13365\n",
      "batch 319, loss: 0.6210, instance_loss: 0.2321, weighted_loss: 0.5044, label: 0, bag_size: 12840\n",
      "batch 339, loss: 0.7814, instance_loss: 3.0977, weighted_loss: 1.4763, label: 0, bag_size: 2815\n",
      "batch 359, loss: 0.5309, instance_loss: 0.4370, weighted_loss: 0.5028, label: 1, bag_size: 2495\n",
      "batch 379, loss: 0.6608, instance_loss: 0.8788, weighted_loss: 0.7262, label: 1, bag_size: 13089\n",
      "batch 399, loss: 0.5934, instance_loss: 0.0210, weighted_loss: 0.4217, label: 1, bag_size: 29832\n",
      "batch 419, loss: 0.5173, instance_loss: 0.0004, weighted_loss: 0.3622, label: 1, bag_size: 20161\n",
      "batch 439, loss: 0.5736, instance_loss: 0.0003, weighted_loss: 0.4016, label: 1, bag_size: 18095\n",
      "batch 459, loss: 0.4499, instance_loss: 0.5070, weighted_loss: 0.4670, label: 0, bag_size: 17268\n",
      "batch 479, loss: 0.4892, instance_loss: 0.5718, weighted_loss: 0.5140, label: 0, bag_size: 1452\n",
      "batch 499, loss: 0.9068, instance_loss: 0.8854, weighted_loss: 0.9004, label: 0, bag_size: 1800\n",
      "batch 519, loss: 0.5132, instance_loss: 0.3694, weighted_loss: 0.4701, label: 0, bag_size: 13591\n",
      "batch 539, loss: 0.4578, instance_loss: 0.1205, weighted_loss: 0.3566, label: 0, bag_size: 12083\n",
      "batch 559, loss: 0.3601, instance_loss: 0.0004, weighted_loss: 0.2522, label: 0, bag_size: 21864\n",
      "batch 579, loss: 0.5476, instance_loss: 0.0296, weighted_loss: 0.3922, label: 0, bag_size: 18045\n",
      "batch 599, loss: 0.5081, instance_loss: 0.3336, weighted_loss: 0.4557, label: 1, bag_size: 1101\n",
      "batch 619, loss: 0.6846, instance_loss: 0.5228, weighted_loss: 0.6361, label: 1, bag_size: 2695\n",
      "batch 639, loss: 0.8537, instance_loss: 0.5123, weighted_loss: 0.7513, label: 0, bag_size: 2296\n",
      "batch 659, loss: 0.5674, instance_loss: 0.0105, weighted_loss: 0.4003, label: 0, bag_size: 65728\n",
      "batch 679, loss: 0.3477, instance_loss: 0.0085, weighted_loss: 0.2459, label: 0, bag_size: 12796\n",
      "batch 699, loss: 0.5374, instance_loss: 0.1028, weighted_loss: 0.4070, label: 1, bag_size: 7217\n",
      "batch 719, loss: 0.5644, instance_loss: 0.1726, weighted_loss: 0.4468, label: 0, bag_size: 5551\n",
      "batch 739, loss: 0.5768, instance_loss: 0.3816, weighted_loss: 0.5183, label: 1, bag_size: 1622\n",
      "batch 759, loss: 0.7169, instance_loss: 0.6739, weighted_loss: 0.7040, label: 1, bag_size: 1294\n",
      "batch 779, loss: 0.4186, instance_loss: 0.0869, weighted_loss: 0.3190, label: 0, bag_size: 5225\n",
      "batch 799, loss: 0.4136, instance_loss: 0.3860, weighted_loss: 0.4053, label: 0, bag_size: 1760\n",
      "batch 819, loss: 0.2968, instance_loss: 0.0046, weighted_loss: 0.2091, label: 1, bag_size: 6164\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9401676829268293: correct 12335/13120\n",
      "class 1 clustering acc 0.6807926829268293: correct 4466/6560\n",
      "Epoch: 6, train_loss: 0.5808, train_clustering_loss:  0.5663, train_error: 0.2439\n",
      "class 0: acc 0.7202970297029703, correct 291/404\n",
      "class 1: acc 0.7908653846153846, correct 329/416\n",
      "\n",
      "Val Set, val_loss: 0.5120, val_error: 0.2273, auc: 0.9446\n",
      "class 0 clustering acc 0.9323863636363636: correct 1641/1760\n",
      "class 1 clustering acc 0.8011363636363636: correct 705/880\n",
      "class 0: acc 0.5769230769230769, correct 30/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "Validation loss decreased (0.599352 --> 0.511970).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.6373, instance_loss: 0.1629, weighted_loss: 0.4950, label: 0, bag_size: 9485\n",
      "batch 39, loss: 0.4015, instance_loss: 0.0068, weighted_loss: 0.2831, label: 1, bag_size: 11389\n",
      "batch 59, loss: 1.0514, instance_loss: 1.2014, weighted_loss: 1.0964, label: 0, bag_size: 5409\n",
      "batch 79, loss: 0.6213, instance_loss: 0.1960, weighted_loss: 0.4937, label: 1, bag_size: 14681\n",
      "batch 99, loss: 0.5360, instance_loss: 0.0033, weighted_loss: 0.3762, label: 0, bag_size: 23368\n",
      "batch 119, loss: 0.4278, instance_loss: 0.1008, weighted_loss: 0.3297, label: 0, bag_size: 15841\n",
      "batch 139, loss: 0.4121, instance_loss: 0.0113, weighted_loss: 0.2919, label: 1, bag_size: 13732\n",
      "batch 159, loss: 0.4639, instance_loss: 0.0711, weighted_loss: 0.3461, label: 1, bag_size: 16890\n",
      "batch 179, loss: 0.2423, instance_loss: 0.0131, weighted_loss: 0.1735, label: 0, bag_size: 16936\n",
      "batch 199, loss: 0.2804, instance_loss: 0.0276, weighted_loss: 0.2046, label: 0, bag_size: 9885\n",
      "batch 219, loss: 0.5480, instance_loss: 0.0463, weighted_loss: 0.3975, label: 0, bag_size: 3810\n",
      "batch 239, loss: 0.2757, instance_loss: 0.0160, weighted_loss: 0.1978, label: 0, bag_size: 8898\n",
      "batch 259, loss: 0.3384, instance_loss: 0.0008, weighted_loss: 0.2371, label: 0, bag_size: 27158\n",
      "batch 279, loss: 0.2591, instance_loss: 0.0050, weighted_loss: 0.1828, label: 0, bag_size: 9060\n",
      "batch 299, loss: 0.7114, instance_loss: 1.4326, weighted_loss: 0.9277, label: 0, bag_size: 1714\n",
      "batch 319, loss: 0.3481, instance_loss: 0.0260, weighted_loss: 0.2515, label: 0, bag_size: 10481\n",
      "batch 339, loss: 0.5265, instance_loss: 0.1469, weighted_loss: 0.4126, label: 1, bag_size: 4394\n",
      "batch 359, loss: 0.2886, instance_loss: 0.1317, weighted_loss: 0.2415, label: 0, bag_size: 19808\n",
      "batch 379, loss: 0.1223, instance_loss: 0.1931, weighted_loss: 0.1436, label: 0, bag_size: 23796\n",
      "batch 399, loss: 0.5996, instance_loss: 1.8609, weighted_loss: 0.9780, label: 0, bag_size: 2367\n",
      "batch 419, loss: 0.1379, instance_loss: 0.0207, weighted_loss: 0.1027, label: 1, bag_size: 5221\n",
      "batch 439, loss: 0.6693, instance_loss: 0.8981, weighted_loss: 0.7379, label: 0, bag_size: 11607\n",
      "batch 459, loss: 0.7463, instance_loss: 0.1868, weighted_loss: 0.5784, label: 0, bag_size: 2998\n",
      "batch 479, loss: 0.3786, instance_loss: 0.0039, weighted_loss: 0.2662, label: 1, bag_size: 13194\n",
      "batch 499, loss: 0.6722, instance_loss: 0.3883, weighted_loss: 0.5871, label: 1, bag_size: 1015\n",
      "batch 519, loss: 0.1892, instance_loss: 0.0182, weighted_loss: 0.1379, label: 1, bag_size: 7217\n",
      "batch 539, loss: 0.7520, instance_loss: 0.4938, weighted_loss: 0.6745, label: 0, bag_size: 7011\n",
      "batch 559, loss: 0.2997, instance_loss: 0.2531, weighted_loss: 0.2857, label: 0, bag_size: 3787\n",
      "batch 579, loss: 0.6097, instance_loss: 0.1633, weighted_loss: 0.4758, label: 1, bag_size: 5256\n",
      "batch 599, loss: 0.4710, instance_loss: 0.0494, weighted_loss: 0.3445, label: 1, bag_size: 6606\n",
      "batch 619, loss: 1.0337, instance_loss: 0.0207, weighted_loss: 0.7298, label: 1, bag_size: 10492\n",
      "batch 639, loss: 0.2670, instance_loss: 0.1160, weighted_loss: 0.2217, label: 0, bag_size: 13880\n",
      "batch 659, loss: 0.1585, instance_loss: 0.1042, weighted_loss: 0.1422, label: 1, bag_size: 5454\n",
      "batch 679, loss: 0.3116, instance_loss: 0.0845, weighted_loss: 0.2435, label: 1, bag_size: 2522\n",
      "batch 699, loss: 0.4937, instance_loss: 0.0416, weighted_loss: 0.3580, label: 0, bag_size: 6850\n",
      "batch 719, loss: 0.9657, instance_loss: 0.0605, weighted_loss: 0.6942, label: 1, bag_size: 14681\n",
      "batch 739, loss: 0.6987, instance_loss: 0.4725, weighted_loss: 0.6309, label: 0, bag_size: 1127\n",
      "batch 759, loss: 0.1469, instance_loss: 0.0112, weighted_loss: 0.1062, label: 1, bag_size: 18603\n",
      "batch 779, loss: 0.4313, instance_loss: 0.5074, weighted_loss: 0.4541, label: 0, bag_size: 47866\n",
      "batch 799, loss: 0.4620, instance_loss: 0.3420, weighted_loss: 0.4260, label: 0, bag_size: 10146\n",
      "batch 819, loss: 0.2278, instance_loss: 0.2027, weighted_loss: 0.2202, label: 1, bag_size: 8191\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9596798780487805: correct 12591/13120\n",
      "class 1 clustering acc 0.7731707317073171: correct 5072/6560\n",
      "Epoch: 7, train_loss: 0.4899, train_clustering_loss:  0.4183, train_error: 0.1841\n",
      "class 0: acc 0.8109452736318408, correct 326/402\n",
      "class 1: acc 0.8205741626794258, correct 343/418\n",
      "\n",
      "Val Set, val_loss: 0.4281, val_error: 0.1727, auc: 0.9463\n",
      "class 0 clustering acc 0.9482954545454545: correct 1669/1760\n",
      "class 1 clustering acc 0.8136363636363636: correct 716/880\n",
      "class 0: acc 0.6923076923076923, correct 36/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "Validation loss decreased (0.511970 --> 0.428072).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5326, instance_loss: 0.3205, weighted_loss: 0.4689, label: 0, bag_size: 13591\n",
      "batch 39, loss: 0.3529, instance_loss: 0.1812, weighted_loss: 0.3014, label: 0, bag_size: 8959\n",
      "batch 59, loss: 0.3964, instance_loss: 0.0386, weighted_loss: 0.2890, label: 0, bag_size: 19470\n",
      "batch 79, loss: 0.5012, instance_loss: 0.1527, weighted_loss: 0.3967, label: 0, bag_size: 4523\n",
      "batch 99, loss: 0.1182, instance_loss: 0.0012, weighted_loss: 0.0831, label: 0, bag_size: 17633\n",
      "batch 119, loss: 0.2553, instance_loss: 0.0000, weighted_loss: 0.1787, label: 0, bag_size: 19390\n",
      "batch 139, loss: 0.3447, instance_loss: 0.2474, weighted_loss: 0.3155, label: 0, bag_size: 10381\n",
      "batch 159, loss: 0.1550, instance_loss: 0.0581, weighted_loss: 0.1259, label: 1, bag_size: 11122\n",
      "batch 179, loss: 0.1279, instance_loss: 0.0740, weighted_loss: 0.1117, label: 1, bag_size: 9519\n",
      "batch 199, loss: 0.6926, instance_loss: 0.4351, weighted_loss: 0.6154, label: 0, bag_size: 3657\n",
      "batch 219, loss: 0.1356, instance_loss: 0.0074, weighted_loss: 0.0971, label: 1, bag_size: 25970\n",
      "batch 239, loss: 0.1629, instance_loss: 0.1312, weighted_loss: 0.1534, label: 0, bag_size: 3908\n",
      "batch 259, loss: 0.3993, instance_loss: 0.0089, weighted_loss: 0.2822, label: 1, bag_size: 8003\n",
      "batch 279, loss: 0.5164, instance_loss: 1.3763, weighted_loss: 0.7744, label: 1, bag_size: 3368\n",
      "batch 299, loss: 0.7747, instance_loss: 0.4919, weighted_loss: 0.6899, label: 1, bag_size: 1703\n",
      "batch 319, loss: 0.9115, instance_loss: 0.7232, weighted_loss: 0.8550, label: 0, bag_size: 2242\n",
      "batch 339, loss: 0.3090, instance_loss: 0.2235, weighted_loss: 0.2833, label: 0, bag_size: 3970\n",
      "batch 359, loss: 0.1479, instance_loss: 0.0000, weighted_loss: 0.1035, label: 0, bag_size: 16992\n",
      "batch 379, loss: 0.3591, instance_loss: 1.5651, weighted_loss: 0.7209, label: 0, bag_size: 9171\n",
      "batch 399, loss: 0.5073, instance_loss: 1.1437, weighted_loss: 0.6982, label: 1, bag_size: 1294\n",
      "batch 419, loss: 0.8205, instance_loss: 0.0610, weighted_loss: 0.5926, label: 0, bag_size: 8420\n",
      "batch 439, loss: 0.2378, instance_loss: 0.0564, weighted_loss: 0.1834, label: 1, bag_size: 6606\n",
      "batch 459, loss: 0.5242, instance_loss: 1.0865, weighted_loss: 0.6929, label: 0, bag_size: 1052\n",
      "batch 479, loss: 0.3384, instance_loss: 0.1970, weighted_loss: 0.2960, label: 0, bag_size: 2609\n",
      "batch 499, loss: 0.7530, instance_loss: 0.3378, weighted_loss: 0.6284, label: 0, bag_size: 4997\n",
      "batch 519, loss: 0.4175, instance_loss: 0.0033, weighted_loss: 0.2932, label: 1, bag_size: 15332\n",
      "batch 539, loss: 0.1102, instance_loss: 0.0088, weighted_loss: 0.0798, label: 1, bag_size: 25970\n",
      "batch 559, loss: 0.6078, instance_loss: 1.0844, weighted_loss: 0.7508, label: 1, bag_size: 1255\n",
      "batch 579, loss: 0.0832, instance_loss: 0.0006, weighted_loss: 0.0584, label: 0, bag_size: 23037\n",
      "batch 599, loss: 0.6181, instance_loss: 0.7251, weighted_loss: 0.6502, label: 0, bag_size: 1831\n",
      "batch 619, loss: 1.1058, instance_loss: 0.8423, weighted_loss: 1.0268, label: 1, bag_size: 12946\n",
      "batch 639, loss: 0.5649, instance_loss: 0.4036, weighted_loss: 0.5165, label: 1, bag_size: 1230\n",
      "batch 659, loss: 0.1177, instance_loss: 0.0006, weighted_loss: 0.0826, label: 1, bag_size: 8003\n",
      "batch 679, loss: 0.3451, instance_loss: 0.1282, weighted_loss: 0.2801, label: 0, bag_size: 4845\n",
      "batch 699, loss: 0.0580, instance_loss: 0.0241, weighted_loss: 0.0478, label: 1, bag_size: 645\n",
      "batch 719, loss: 0.9780, instance_loss: 0.1603, weighted_loss: 0.7327, label: 0, bag_size: 16690\n",
      "batch 739, loss: 0.1779, instance_loss: 0.0026, weighted_loss: 0.1253, label: 1, bag_size: 11387\n",
      "batch 759, loss: 0.4226, instance_loss: 0.2931, weighted_loss: 0.3838, label: 0, bag_size: 12731\n",
      "batch 779, loss: 0.7200, instance_loss: 1.2716, weighted_loss: 0.8855, label: 1, bag_size: 10105\n",
      "batch 799, loss: 0.1179, instance_loss: 0.0242, weighted_loss: 0.0898, label: 0, bag_size: 7823\n",
      "batch 819, loss: 0.6533, instance_loss: 1.8388, weighted_loss: 1.0090, label: 1, bag_size: 1242\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9584603658536586: correct 12575/13120\n",
      "class 1 clustering acc 0.744359756097561: correct 4883/6560\n",
      "Epoch: 8, train_loss: 0.4574, train_clustering_loss:  0.4324, train_error: 0.1841\n",
      "class 0: acc 0.8341232227488151, correct 352/422\n",
      "class 1: acc 0.7964824120603015, correct 317/398\n",
      "\n",
      "Val Set, val_loss: 0.3706, val_error: 0.1727, auc: 0.9519\n",
      "class 0 clustering acc 0.9556818181818182: correct 1682/1760\n",
      "class 1 clustering acc 0.8806818181818182: correct 775/880\n",
      "class 0: acc 0.7115384615384616, correct 37/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "Validation loss decreased (0.428072 --> 0.370598).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3465, instance_loss: 0.0204, weighted_loss: 0.2487, label: 1, bag_size: 1412\n",
      "batch 39, loss: 0.9905, instance_loss: 0.1554, weighted_loss: 0.7400, label: 0, bag_size: 5211\n",
      "batch 59, loss: 0.5873, instance_loss: 0.5127, weighted_loss: 0.5649, label: 0, bag_size: 7011\n",
      "batch 79, loss: 0.1048, instance_loss: 0.1409, weighted_loss: 0.1156, label: 0, bag_size: 2424\n",
      "batch 99, loss: 0.1843, instance_loss: 0.0122, weighted_loss: 0.1326, label: 1, bag_size: 9446\n",
      "batch 119, loss: 0.7010, instance_loss: 0.1763, weighted_loss: 0.5436, label: 0, bag_size: 2303\n",
      "batch 139, loss: 0.6416, instance_loss: 0.0057, weighted_loss: 0.4508, label: 1, bag_size: 10460\n",
      "batch 159, loss: 0.1126, instance_loss: 0.0123, weighted_loss: 0.0825, label: 1, bag_size: 17769\n",
      "batch 179, loss: 0.2198, instance_loss: 0.1870, weighted_loss: 0.2100, label: 1, bag_size: 9519\n",
      "batch 199, loss: 0.1502, instance_loss: 0.0020, weighted_loss: 0.1058, label: 1, bag_size: 11032\n",
      "batch 219, loss: 0.3663, instance_loss: 0.3214, weighted_loss: 0.3528, label: 1, bag_size: 12712\n",
      "batch 239, loss: 0.4539, instance_loss: 0.2111, weighted_loss: 0.3811, label: 1, bag_size: 10912\n",
      "batch 259, loss: 0.4504, instance_loss: 0.0044, weighted_loss: 0.3166, label: 1, bag_size: 1015\n",
      "batch 279, loss: 0.4642, instance_loss: 0.0331, weighted_loss: 0.3349, label: 0, bag_size: 2534\n",
      "batch 299, loss: 0.0700, instance_loss: 0.0072, weighted_loss: 0.0511, label: 0, bag_size: 17791\n",
      "batch 319, loss: 0.2013, instance_loss: 0.4704, weighted_loss: 0.2821, label: 0, bag_size: 2270\n",
      "batch 339, loss: 0.5182, instance_loss: 0.1668, weighted_loss: 0.4128, label: 1, bag_size: 1483\n",
      "batch 359, loss: 0.1522, instance_loss: 0.0455, weighted_loss: 0.1202, label: 1, bag_size: 16379\n",
      "batch 379, loss: 0.1191, instance_loss: 0.0220, weighted_loss: 0.0899, label: 0, bag_size: 18154\n",
      "batch 399, loss: 0.1869, instance_loss: 0.0146, weighted_loss: 0.1352, label: 0, bag_size: 5225\n",
      "batch 419, loss: 0.3483, instance_loss: 0.0750, weighted_loss: 0.2663, label: 1, bag_size: 1483\n",
      "batch 439, loss: 0.1467, instance_loss: 1.0108, weighted_loss: 0.4059, label: 1, bag_size: 3409\n",
      "batch 459, loss: 0.0709, instance_loss: 0.0000, weighted_loss: 0.0497, label: 1, bag_size: 25695\n",
      "batch 479, loss: 0.2735, instance_loss: 0.2306, weighted_loss: 0.2606, label: 0, bag_size: 2918\n",
      "batch 499, loss: 0.0327, instance_loss: 0.0332, weighted_loss: 0.0329, label: 1, bag_size: 9955\n",
      "batch 519, loss: 0.0939, instance_loss: 0.0300, weighted_loss: 0.0747, label: 1, bag_size: 10028\n",
      "batch 539, loss: 0.1637, instance_loss: 0.0104, weighted_loss: 0.1177, label: 1, bag_size: 10501\n",
      "batch 559, loss: 1.5587, instance_loss: 5.2278, weighted_loss: 2.6595, label: 0, bag_size: 14264\n",
      "batch 579, loss: 0.4491, instance_loss: 0.1113, weighted_loss: 0.3478, label: 0, bag_size: 3375\n",
      "batch 599, loss: 0.3697, instance_loss: 0.3252, weighted_loss: 0.3563, label: 1, bag_size: 6842\n",
      "batch 619, loss: 0.0460, instance_loss: 0.0022, weighted_loss: 0.0329, label: 1, bag_size: 13174\n",
      "batch 639, loss: 0.0454, instance_loss: 0.0931, weighted_loss: 0.0597, label: 1, bag_size: 6453\n",
      "batch 659, loss: 0.2825, instance_loss: 0.5785, weighted_loss: 0.3713, label: 1, bag_size: 11964\n",
      "batch 679, loss: 0.4271, instance_loss: 0.0019, weighted_loss: 0.2995, label: 1, bag_size: 19606\n",
      "batch 699, loss: 0.0450, instance_loss: 0.0069, weighted_loss: 0.0335, label: 1, bag_size: 10969\n",
      "batch 719, loss: 1.8334, instance_loss: 2.8368, weighted_loss: 2.1344, label: 1, bag_size: 898\n",
      "batch 739, loss: 0.2946, instance_loss: 0.5900, weighted_loss: 0.3833, label: 1, bag_size: 11981\n",
      "batch 759, loss: 0.3170, instance_loss: 0.9443, weighted_loss: 0.5052, label: 1, bag_size: 9404\n",
      "batch 779, loss: 0.4397, instance_loss: 0.2376, weighted_loss: 0.3790, label: 0, bag_size: 5551\n",
      "batch 799, loss: 0.0533, instance_loss: 0.1997, weighted_loss: 0.0973, label: 1, bag_size: 3453\n",
      "batch 819, loss: 0.2219, instance_loss: 0.3843, weighted_loss: 0.2706, label: 0, bag_size: 1909\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9677591463414634: correct 12697/13120\n",
      "class 1 clustering acc 0.7829268292682927: correct 5136/6560\n",
      "Epoch: 9, train_loss: 0.3833, train_clustering_loss:  0.3707, train_error: 0.1366\n",
      "class 0: acc 0.8645320197044335, correct 351/406\n",
      "class 1: acc 0.8623188405797102, correct 357/414\n",
      "\n",
      "Val Set, val_loss: 0.3501, val_error: 0.1636, auc: 0.9529\n",
      "class 0 clustering acc 0.96875: correct 1705/1760\n",
      "class 1 clustering acc 0.7443181818181818: correct 655/880\n",
      "class 0: acc 0.7115384615384616, correct 37/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "Validation loss decreased (0.370598 --> 0.350121).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5291, instance_loss: 0.7532, weighted_loss: 0.5963, label: 0, bag_size: 65728\n",
      "batch 39, loss: 0.0879, instance_loss: 0.4827, weighted_loss: 0.2063, label: 0, bag_size: 9470\n",
      "batch 59, loss: 0.6586, instance_loss: 0.0072, weighted_loss: 0.4631, label: 1, bag_size: 13365\n",
      "batch 79, loss: 0.1081, instance_loss: 0.2975, weighted_loss: 0.1650, label: 0, bag_size: 15636\n",
      "batch 99, loss: 0.1561, instance_loss: 0.0011, weighted_loss: 0.1096, label: 1, bag_size: 7110\n",
      "batch 119, loss: 0.8236, instance_loss: 0.2056, weighted_loss: 0.6382, label: 0, bag_size: 1508\n",
      "batch 139, loss: 4.4745, instance_loss: 7.6220, weighted_loss: 5.4187, label: 1, bag_size: 2565\n",
      "batch 159, loss: 0.0572, instance_loss: 0.1273, weighted_loss: 0.0782, label: 1, bag_size: 9322\n",
      "batch 179, loss: 0.0391, instance_loss: 0.0029, weighted_loss: 0.0283, label: 1, bag_size: 3640\n",
      "batch 199, loss: 0.0387, instance_loss: 0.0128, weighted_loss: 0.0310, label: 1, bag_size: 5221\n",
      "batch 219, loss: 0.2793, instance_loss: 0.1238, weighted_loss: 0.2326, label: 1, bag_size: 4956\n",
      "batch 239, loss: 0.2052, instance_loss: 0.0000, weighted_loss: 0.1436, label: 1, bag_size: 10482\n",
      "batch 259, loss: 0.0785, instance_loss: 0.0191, weighted_loss: 0.0607, label: 0, bag_size: 11477\n",
      "batch 279, loss: 0.8659, instance_loss: 0.4256, weighted_loss: 0.7338, label: 0, bag_size: 9069\n",
      "batch 299, loss: 0.1028, instance_loss: 0.4373, weighted_loss: 0.2031, label: 0, bag_size: 8549\n",
      "batch 319, loss: 0.3356, instance_loss: 0.2180, weighted_loss: 0.3003, label: 1, bag_size: 16034\n",
      "batch 339, loss: 0.0517, instance_loss: 0.1649, weighted_loss: 0.0856, label: 1, bag_size: 5894\n",
      "batch 359, loss: 0.0685, instance_loss: 0.0050, weighted_loss: 0.0495, label: 1, bag_size: 9533\n",
      "batch 379, loss: 0.1840, instance_loss: 0.4997, weighted_loss: 0.2787, label: 1, bag_size: 6781\n",
      "batch 399, loss: 0.0280, instance_loss: 0.3788, weighted_loss: 0.1333, label: 0, bag_size: 10263\n",
      "batch 419, loss: 0.0538, instance_loss: 0.1603, weighted_loss: 0.0858, label: 0, bag_size: 11125\n",
      "batch 439, loss: 0.1838, instance_loss: 0.6813, weighted_loss: 0.3330, label: 1, bag_size: 3651\n",
      "batch 459, loss: 0.3348, instance_loss: 0.3151, weighted_loss: 0.3289, label: 0, bag_size: 1690\n",
      "batch 479, loss: 0.0465, instance_loss: 0.0088, weighted_loss: 0.0352, label: 0, bag_size: 9234\n",
      "batch 499, loss: 0.1713, instance_loss: 0.0000, weighted_loss: 0.1199, label: 1, bag_size: 19932\n",
      "batch 519, loss: 0.1825, instance_loss: 0.0075, weighted_loss: 0.1300, label: 1, bag_size: 8103\n",
      "batch 539, loss: 0.0912, instance_loss: 0.0265, weighted_loss: 0.0718, label: 1, bag_size: 16051\n",
      "batch 559, loss: 0.0304, instance_loss: 0.2438, weighted_loss: 0.0944, label: 0, bag_size: 11900\n",
      "batch 579, loss: 0.0604, instance_loss: 0.0882, weighted_loss: 0.0688, label: 0, bag_size: 22681\n",
      "batch 599, loss: 1.7248, instance_loss: 3.4300, weighted_loss: 2.2363, label: 1, bag_size: 9162\n",
      "batch 619, loss: 0.7874, instance_loss: 1.7222, weighted_loss: 1.0678, label: 0, bag_size: 1920\n",
      "batch 639, loss: 0.1460, instance_loss: 1.7775, weighted_loss: 0.6354, label: 1, bag_size: 1512\n",
      "batch 659, loss: 2.7771, instance_loss: 1.6502, weighted_loss: 2.4390, label: 1, bag_size: 13477\n",
      "batch 679, loss: 1.8468, instance_loss: 2.8886, weighted_loss: 2.1594, label: 1, bag_size: 898\n",
      "batch 699, loss: 0.0451, instance_loss: 0.5861, weighted_loss: 0.2074, label: 1, bag_size: 4054\n",
      "batch 719, loss: 0.2318, instance_loss: 0.5373, weighted_loss: 0.3234, label: 0, bag_size: 3474\n",
      "batch 739, loss: 0.2253, instance_loss: 0.2712, weighted_loss: 0.2391, label: 0, bag_size: 1415\n",
      "batch 759, loss: 0.7702, instance_loss: 0.1996, weighted_loss: 0.5990, label: 0, bag_size: 3783\n",
      "batch 779, loss: 0.0755, instance_loss: 0.2851, weighted_loss: 0.1384, label: 0, bag_size: 1370\n",
      "batch 799, loss: 0.0314, instance_loss: 0.0213, weighted_loss: 0.0283, label: 0, bag_size: 20150\n",
      "batch 819, loss: 0.2899, instance_loss: 0.2492, weighted_loss: 0.2777, label: 0, bag_size: 1797\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9423018292682926: correct 12363/13120\n",
      "class 1 clustering acc 0.7228658536585366: correct 4742/6560\n",
      "Epoch: 10, train_loss: 0.3517, train_clustering_loss:  0.5176, train_error: 0.1427\n",
      "class 0: acc 0.8450363196125908, correct 349/413\n",
      "class 1: acc 0.8697788697788698, correct 354/407\n",
      "\n",
      "Val Set, val_loss: 0.2976, val_error: 0.1182, auc: 0.9562\n",
      "class 0 clustering acc 0.9238636363636363: correct 1626/1760\n",
      "class 1 clustering acc 0.8068181818181818: correct 710/880\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "Validation loss decreased (0.350121 --> 0.297565).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0606, instance_loss: 0.0624, weighted_loss: 0.0611, label: 0, bag_size: 11146\n",
      "batch 39, loss: 0.0786, instance_loss: 0.0499, weighted_loss: 0.0700, label: 0, bag_size: 9866\n",
      "batch 59, loss: 0.6541, instance_loss: 1.0109, weighted_loss: 0.7612, label: 0, bag_size: 21361\n",
      "batch 79, loss: 0.0936, instance_loss: 0.0672, weighted_loss: 0.0856, label: 1, bag_size: 5731\n",
      "batch 99, loss: 0.2864, instance_loss: 0.1218, weighted_loss: 0.2370, label: 1, bag_size: 4394\n",
      "batch 119, loss: 0.2840, instance_loss: 0.0608, weighted_loss: 0.2170, label: 1, bag_size: 15665\n",
      "batch 139, loss: 0.0413, instance_loss: 0.0007, weighted_loss: 0.0291, label: 1, bag_size: 20767\n",
      "batch 159, loss: 2.7285, instance_loss: 2.6998, weighted_loss: 2.7199, label: 0, bag_size: 4692\n",
      "batch 179, loss: 0.0624, instance_loss: 0.0023, weighted_loss: 0.0443, label: 0, bag_size: 19470\n",
      "batch 199, loss: 0.5856, instance_loss: 0.1854, weighted_loss: 0.4655, label: 0, bag_size: 1962\n",
      "batch 219, loss: 0.5699, instance_loss: 0.0316, weighted_loss: 0.4084, label: 0, bag_size: 4271\n",
      "batch 239, loss: 1.2112, instance_loss: 0.3458, weighted_loss: 0.9515, label: 1, bag_size: 9942\n",
      "batch 259, loss: 0.1432, instance_loss: 0.0582, weighted_loss: 0.1177, label: 0, bag_size: 21093\n",
      "batch 279, loss: 0.0510, instance_loss: 0.0048, weighted_loss: 0.0372, label: 1, bag_size: 5605\n",
      "batch 299, loss: 0.2948, instance_loss: 0.1013, weighted_loss: 0.2368, label: 1, bag_size: 12626\n",
      "batch 319, loss: 0.2515, instance_loss: 0.1373, weighted_loss: 0.2172, label: 0, bag_size: 4845\n",
      "batch 339, loss: 0.1557, instance_loss: 0.7464, weighted_loss: 0.3329, label: 1, bag_size: 2136\n",
      "batch 359, loss: 0.6057, instance_loss: 0.9690, weighted_loss: 0.7147, label: 0, bag_size: 7381\n",
      "batch 379, loss: 0.2074, instance_loss: 0.8730, weighted_loss: 0.4070, label: 1, bag_size: 14604\n",
      "batch 399, loss: 0.0379, instance_loss: 0.1506, weighted_loss: 0.0717, label: 0, bag_size: 11383\n",
      "batch 419, loss: 0.2169, instance_loss: 0.1058, weighted_loss: 0.1836, label: 0, bag_size: 2732\n",
      "batch 439, loss: 0.0886, instance_loss: 0.0560, weighted_loss: 0.0788, label: 0, bag_size: 13880\n",
      "batch 459, loss: 0.0329, instance_loss: 0.0251, weighted_loss: 0.0305, label: 0, bag_size: 17633\n",
      "batch 479, loss: 0.2308, instance_loss: 0.3399, weighted_loss: 0.2635, label: 1, bag_size: 10396\n",
      "batch 499, loss: 0.2305, instance_loss: 0.4149, weighted_loss: 0.2858, label: 0, bag_size: 3657\n",
      "batch 519, loss: 0.4411, instance_loss: 0.1936, weighted_loss: 0.3668, label: 0, bag_size: 1142\n",
      "batch 539, loss: 0.2377, instance_loss: 0.1408, weighted_loss: 0.2086, label: 0, bag_size: 2290\n",
      "batch 559, loss: 0.6928, instance_loss: 1.0531, weighted_loss: 0.8009, label: 1, bag_size: 4929\n",
      "batch 579, loss: 0.0249, instance_loss: 0.0250, weighted_loss: 0.0249, label: 1, bag_size: 11195\n",
      "batch 599, loss: 0.0286, instance_loss: 0.0000, weighted_loss: 0.0200, label: 0, bag_size: 26271\n",
      "batch 619, loss: 0.0254, instance_loss: 0.0313, weighted_loss: 0.0272, label: 0, bag_size: 10068\n",
      "batch 639, loss: 1.1711, instance_loss: 0.3721, weighted_loss: 0.9314, label: 1, bag_size: 2937\n",
      "batch 659, loss: 0.1532, instance_loss: 0.0278, weighted_loss: 0.1156, label: 0, bag_size: 14625\n",
      "batch 679, loss: 0.0143, instance_loss: 0.1725, weighted_loss: 0.0617, label: 0, bag_size: 3787\n",
      "batch 699, loss: 0.3464, instance_loss: 0.8824, weighted_loss: 0.5072, label: 1, bag_size: 8026\n",
      "batch 719, loss: 0.3550, instance_loss: 2.4290, weighted_loss: 0.9772, label: 1, bag_size: 7246\n",
      "batch 739, loss: 0.0999, instance_loss: 0.1720, weighted_loss: 0.1215, label: 1, bag_size: 6205\n",
      "batch 759, loss: 0.5626, instance_loss: 0.5683, weighted_loss: 0.5643, label: 0, bag_size: 1213\n",
      "batch 779, loss: 0.6224, instance_loss: 0.3203, weighted_loss: 0.5318, label: 0, bag_size: 7612\n",
      "batch 799, loss: 0.3237, instance_loss: 1.3531, weighted_loss: 0.6325, label: 1, bag_size: 1294\n",
      "batch 819, loss: 0.0199, instance_loss: 0.2296, weighted_loss: 0.0828, label: 1, bag_size: 6090\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9563262195121951: correct 12547/13120\n",
      "class 1 clustering acc 0.7841463414634147: correct 5144/6560\n",
      "Epoch: 11, train_loss: 0.3584, train_clustering_loss:  0.4129, train_error: 0.1146\n",
      "class 0: acc 0.9035294117647059, correct 384/425\n",
      "class 1: acc 0.8658227848101265, correct 342/395\n",
      "\n",
      "Val Set, val_loss: 0.2891, val_error: 0.1273, auc: 0.9549\n",
      "class 0 clustering acc 0.9642045454545455: correct 1697/1760\n",
      "class 1 clustering acc 0.7897727272727273: correct 695/880\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "Validation loss decreased (0.297565 --> 0.289068).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0394, instance_loss: 0.0110, weighted_loss: 0.0309, label: 1, bag_size: 30675\n",
      "batch 39, loss: 0.2259, instance_loss: 0.6582, weighted_loss: 0.3555, label: 1, bag_size: 2146\n",
      "batch 59, loss: 0.0631, instance_loss: 0.7631, weighted_loss: 0.2731, label: 1, bag_size: 12611\n",
      "batch 79, loss: 0.3159, instance_loss: 0.6228, weighted_loss: 0.4079, label: 0, bag_size: 1149\n",
      "batch 99, loss: 0.1867, instance_loss: 0.4174, weighted_loss: 0.2559, label: 0, bag_size: 2609\n",
      "batch 119, loss: 0.6022, instance_loss: 0.0217, weighted_loss: 0.4280, label: 1, bag_size: 12340\n",
      "batch 139, loss: 0.1430, instance_loss: 0.0284, weighted_loss: 0.1086, label: 0, bag_size: 1202\n",
      "batch 159, loss: 0.0599, instance_loss: 0.0615, weighted_loss: 0.0604, label: 1, bag_size: 8522\n",
      "batch 179, loss: 0.9273, instance_loss: 0.4513, weighted_loss: 0.7845, label: 1, bag_size: 12340\n",
      "batch 199, loss: 0.7376, instance_loss: 2.5476, weighted_loss: 1.2806, label: 0, bag_size: 12510\n",
      "batch 219, loss: 0.0264, instance_loss: 0.0989, weighted_loss: 0.0482, label: 0, bag_size: 8252\n",
      "batch 239, loss: 0.3839, instance_loss: 0.0169, weighted_loss: 0.2738, label: 0, bag_size: 8755\n",
      "batch 259, loss: 0.3955, instance_loss: 0.2413, weighted_loss: 0.3492, label: 1, bag_size: 3211\n",
      "batch 279, loss: 0.0476, instance_loss: 0.0056, weighted_loss: 0.0350, label: 1, bag_size: 14779\n",
      "batch 299, loss: 0.0359, instance_loss: 0.0166, weighted_loss: 0.0301, label: 0, bag_size: 19518\n",
      "batch 319, loss: 0.8526, instance_loss: 1.4798, weighted_loss: 1.0407, label: 1, bag_size: 1051\n",
      "batch 339, loss: 0.0236, instance_loss: 0.0530, weighted_loss: 0.0324, label: 1, bag_size: 9877\n",
      "batch 359, loss: 0.0332, instance_loss: 0.0976, weighted_loss: 0.0526, label: 1, bag_size: 7798\n",
      "batch 379, loss: 0.0483, instance_loss: 0.1044, weighted_loss: 0.0652, label: 1, bag_size: 2140\n",
      "batch 399, loss: 0.2026, instance_loss: 0.0291, weighted_loss: 0.1506, label: 0, bag_size: 11113\n",
      "batch 419, loss: 0.3042, instance_loss: 0.5583, weighted_loss: 0.3804, label: 1, bag_size: 2356\n",
      "batch 439, loss: 0.0327, instance_loss: 0.0048, weighted_loss: 0.0243, label: 1, bag_size: 11387\n",
      "batch 459, loss: 0.0278, instance_loss: 0.0766, weighted_loss: 0.0425, label: 1, bag_size: 1919\n",
      "batch 479, loss: 0.3060, instance_loss: 0.0020, weighted_loss: 0.2148, label: 1, bag_size: 5561\n",
      "batch 499, loss: 0.5596, instance_loss: 0.1959, weighted_loss: 0.4505, label: 0, bag_size: 3810\n",
      "batch 519, loss: 1.5971, instance_loss: 1.0137, weighted_loss: 1.4221, label: 1, bag_size: 2842\n",
      "batch 539, loss: 0.0825, instance_loss: 0.0320, weighted_loss: 0.0674, label: 0, bag_size: 12149\n",
      "batch 559, loss: 0.0462, instance_loss: 0.1326, weighted_loss: 0.0721, label: 1, bag_size: 2385\n",
      "batch 579, loss: 0.0949, instance_loss: 0.2872, weighted_loss: 0.1526, label: 0, bag_size: 2534\n",
      "batch 599, loss: 0.1083, instance_loss: 0.0580, weighted_loss: 0.0932, label: 0, bag_size: 2091\n",
      "batch 619, loss: 0.6944, instance_loss: 0.4693, weighted_loss: 0.6269, label: 0, bag_size: 5409\n",
      "batch 639, loss: 0.1447, instance_loss: 0.0234, weighted_loss: 0.1083, label: 0, bag_size: 18215\n",
      "batch 659, loss: 0.0977, instance_loss: 0.0354, weighted_loss: 0.0790, label: 1, bag_size: 7650\n",
      "batch 679, loss: 0.2994, instance_loss: 0.2178, weighted_loss: 0.2749, label: 1, bag_size: 11386\n",
      "batch 699, loss: 1.2050, instance_loss: 2.5263, weighted_loss: 1.6014, label: 0, bag_size: 2653\n",
      "batch 719, loss: 0.8534, instance_loss: 0.2924, weighted_loss: 0.6851, label: 0, bag_size: 1800\n",
      "batch 739, loss: 0.0761, instance_loss: 0.0238, weighted_loss: 0.0604, label: 1, bag_size: 10501\n",
      "batch 759, loss: 0.0219, instance_loss: 0.0053, weighted_loss: 0.0169, label: 0, bag_size: 9949\n",
      "batch 779, loss: 0.2441, instance_loss: 0.3305, weighted_loss: 0.2700, label: 0, bag_size: 1690\n",
      "batch 799, loss: 0.1442, instance_loss: 0.0449, weighted_loss: 0.1144, label: 1, bag_size: 8003\n",
      "batch 819, loss: 0.4612, instance_loss: 0.0316, weighted_loss: 0.3324, label: 1, bag_size: 2480\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9560975609756097: correct 12544/13120\n",
      "class 1 clustering acc 0.7867378048780488: correct 5161/6560\n",
      "Epoch: 12, train_loss: 0.3349, train_clustering_loss:  0.4067, train_error: 0.1378\n",
      "class 0: acc 0.8728179551122195, correct 350/401\n",
      "class 1: acc 0.8520286396181385, correct 357/419\n",
      "\n",
      "Val Set, val_loss: 0.3205, val_error: 0.1545, auc: 0.9572\n",
      "class 0 clustering acc 0.946590909090909: correct 1666/1760\n",
      "class 1 clustering acc 0.7431818181818182: correct 654/880\n",
      "class 0: acc 0.7307692307692307, correct 38/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 4.1364, instance_loss: 2.1881, weighted_loss: 3.5519, label: 1, bag_size: 2565\n",
      "batch 39, loss: 0.0655, instance_loss: 0.0113, weighted_loss: 0.0492, label: 1, bag_size: 10622\n",
      "batch 59, loss: 1.1897, instance_loss: 0.6856, weighted_loss: 1.0385, label: 0, bag_size: 1701\n",
      "batch 79, loss: 0.2587, instance_loss: 0.4806, weighted_loss: 0.3253, label: 0, bag_size: 1651\n",
      "batch 99, loss: 1.5117, instance_loss: 2.1435, weighted_loss: 1.7012, label: 1, bag_size: 1444\n",
      "batch 119, loss: 0.0086, instance_loss: 0.0001, weighted_loss: 0.0061, label: 1, bag_size: 14223\n",
      "batch 139, loss: 0.2244, instance_loss: 0.8015, weighted_loss: 0.3976, label: 1, bag_size: 928\n",
      "batch 159, loss: 0.1083, instance_loss: 0.3198, weighted_loss: 0.1717, label: 1, bag_size: 5292\n",
      "batch 179, loss: 1.7442, instance_loss: 0.1077, weighted_loss: 1.2533, label: 0, bag_size: 5120\n",
      "batch 199, loss: 0.0127, instance_loss: 0.0680, weighted_loss: 0.0293, label: 0, bag_size: 17633\n",
      "batch 219, loss: 0.0141, instance_loss: 0.0075, weighted_loss: 0.0121, label: 1, bag_size: 5221\n",
      "batch 239, loss: 0.2268, instance_loss: 0.0386, weighted_loss: 0.1703, label: 0, bag_size: 6367\n",
      "batch 259, loss: 0.0587, instance_loss: 0.1431, weighted_loss: 0.0840, label: 0, bag_size: 12201\n",
      "batch 279, loss: 0.2378, instance_loss: 0.0554, weighted_loss: 0.1831, label: 1, bag_size: 5292\n",
      "batch 299, loss: 0.0227, instance_loss: 0.1234, weighted_loss: 0.0529, label: 0, bag_size: 16782\n",
      "batch 319, loss: 0.0410, instance_loss: 0.4983, weighted_loss: 0.1782, label: 0, bag_size: 3190\n",
      "batch 339, loss: 0.2477, instance_loss: 0.2286, weighted_loss: 0.2420, label: 0, bag_size: 12910\n",
      "batch 359, loss: 0.0030, instance_loss: 0.0000, weighted_loss: 0.0021, label: 1, bag_size: 10392\n",
      "batch 379, loss: 0.1058, instance_loss: 0.0012, weighted_loss: 0.0744, label: 1, bag_size: 9561\n",
      "batch 399, loss: 0.0334, instance_loss: 0.0031, weighted_loss: 0.0243, label: 1, bag_size: 11884\n",
      "batch 419, loss: 0.2612, instance_loss: 0.1139, weighted_loss: 0.2170, label: 1, bag_size: 4786\n",
      "batch 439, loss: 0.0138, instance_loss: 0.0003, weighted_loss: 0.0097, label: 0, bag_size: 23037\n",
      "batch 459, loss: 0.1627, instance_loss: 0.2309, weighted_loss: 0.1832, label: 0, bag_size: 12732\n",
      "batch 479, loss: 0.4567, instance_loss: 0.0336, weighted_loss: 0.3298, label: 0, bag_size: 25814\n",
      "batch 499, loss: 0.0934, instance_loss: 0.1011, weighted_loss: 0.0957, label: 0, bag_size: 2844\n",
      "batch 519, loss: 0.1279, instance_loss: 0.0069, weighted_loss: 0.0916, label: 1, bag_size: 11160\n",
      "batch 539, loss: 0.0198, instance_loss: 0.0001, weighted_loss: 0.0139, label: 0, bag_size: 26271\n",
      "batch 559, loss: 0.0248, instance_loss: 0.8309, weighted_loss: 0.2666, label: 0, bag_size: 13964\n",
      "batch 579, loss: 0.0547, instance_loss: 0.0093, weighted_loss: 0.0411, label: 0, bag_size: 13964\n",
      "batch 599, loss: 1.1187, instance_loss: 0.0787, weighted_loss: 0.8067, label: 0, bag_size: 20555\n",
      "batch 619, loss: 0.6223, instance_loss: 0.5944, weighted_loss: 0.6139, label: 0, bag_size: 7428\n",
      "batch 639, loss: 0.0546, instance_loss: 0.6101, weighted_loss: 0.2213, label: 1, bag_size: 1412\n",
      "batch 659, loss: 0.4051, instance_loss: 0.1991, weighted_loss: 0.3433, label: 0, bag_size: 2760\n",
      "batch 679, loss: 0.5414, instance_loss: 1.2025, weighted_loss: 0.7398, label: 0, bag_size: 15071\n",
      "batch 699, loss: 0.1850, instance_loss: 0.1548, weighted_loss: 0.1759, label: 1, bag_size: 13026\n",
      "batch 719, loss: 0.7946, instance_loss: 0.2890, weighted_loss: 0.6429, label: 1, bag_size: 11386\n",
      "batch 739, loss: 0.3529, instance_loss: 0.0188, weighted_loss: 0.2526, label: 0, bag_size: 6850\n",
      "batch 759, loss: 0.1140, instance_loss: 0.0078, weighted_loss: 0.0821, label: 1, bag_size: 7110\n",
      "batch 779, loss: 1.7642, instance_loss: 3.1633, weighted_loss: 2.1839, label: 1, bag_size: 684\n",
      "batch 799, loss: 0.6051, instance_loss: 0.0569, weighted_loss: 0.4407, label: 0, bag_size: 518\n",
      "batch 819, loss: 0.0527, instance_loss: 0.0889, weighted_loss: 0.0635, label: 1, bag_size: 4862\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9609756097560975: correct 12608/13120\n",
      "class 1 clustering acc 0.7798780487804878: correct 5116/6560\n",
      "Epoch: 13, train_loss: 0.3374, train_clustering_loss:  0.3962, train_error: 0.1183\n",
      "class 0: acc 0.8808290155440415, correct 340/386\n",
      "class 1: acc 0.8824884792626728, correct 383/434\n",
      "\n",
      "Val Set, val_loss: 0.2879, val_error: 0.1182, auc: 0.9579\n",
      "class 0 clustering acc 0.959659090909091: correct 1689/1760\n",
      "class 1 clustering acc 0.8159090909090909: correct 718/880\n",
      "class 0: acc 0.8076923076923077, correct 42/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "Validation loss decreased (0.289068 --> 0.287883).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2502, instance_loss: 2.6144, weighted_loss: 0.9595, label: 1, bag_size: 10072\n",
      "batch 39, loss: 0.3479, instance_loss: 0.0001, weighted_loss: 0.2436, label: 1, bag_size: 10396\n",
      "batch 59, loss: 0.1490, instance_loss: 0.1070, weighted_loss: 0.1364, label: 1, bag_size: 1051\n",
      "batch 79, loss: 0.4841, instance_loss: 0.5777, weighted_loss: 0.5122, label: 1, bag_size: 10848\n",
      "batch 99, loss: 0.6027, instance_loss: 1.0416, weighted_loss: 0.7344, label: 0, bag_size: 1953\n",
      "batch 119, loss: 0.1123, instance_loss: 0.4764, weighted_loss: 0.2215, label: 1, bag_size: 2356\n",
      "batch 139, loss: 0.0445, instance_loss: 0.2289, weighted_loss: 0.0998, label: 1, bag_size: 1412\n",
      "batch 159, loss: 0.0327, instance_loss: 0.1364, weighted_loss: 0.0638, label: 0, bag_size: 11727\n",
      "batch 179, loss: 0.0494, instance_loss: 0.0041, weighted_loss: 0.0358, label: 1, bag_size: 11642\n",
      "batch 199, loss: 0.0518, instance_loss: 0.0401, weighted_loss: 0.0483, label: 0, bag_size: 22426\n",
      "batch 219, loss: 0.0977, instance_loss: 0.0074, weighted_loss: 0.0706, label: 1, bag_size: 16154\n",
      "batch 239, loss: 0.8507, instance_loss: 0.5022, weighted_loss: 0.7461, label: 1, bag_size: 7989\n",
      "batch 259, loss: 0.3537, instance_loss: 1.7045, weighted_loss: 0.7590, label: 1, bag_size: 9404\n",
      "batch 279, loss: 0.0585, instance_loss: 0.0036, weighted_loss: 0.0420, label: 0, bag_size: 32227\n",
      "batch 299, loss: 0.0829, instance_loss: 0.0013, weighted_loss: 0.0584, label: 0, bag_size: 17630\n",
      "batch 319, loss: 0.5412, instance_loss: 0.0230, weighted_loss: 0.3858, label: 0, bag_size: 9069\n",
      "batch 339, loss: 0.0130, instance_loss: 0.0666, weighted_loss: 0.0291, label: 1, bag_size: 13015\n",
      "batch 359, loss: 0.8384, instance_loss: 1.8159, weighted_loss: 1.1317, label: 1, bag_size: 2842\n",
      "batch 379, loss: 0.0215, instance_loss: 0.0022, weighted_loss: 0.0157, label: 1, bag_size: 15008\n",
      "batch 399, loss: 0.0588, instance_loss: 0.3680, weighted_loss: 0.1516, label: 0, bag_size: 14681\n",
      "batch 419, loss: 0.0109, instance_loss: 0.0051, weighted_loss: 0.0092, label: 0, bag_size: 9786\n",
      "batch 439, loss: 0.1072, instance_loss: 0.3905, weighted_loss: 0.1922, label: 0, bag_size: 705\n",
      "batch 459, loss: 0.0851, instance_loss: 0.0005, weighted_loss: 0.0597, label: 0, bag_size: 8788\n",
      "batch 479, loss: 0.0208, instance_loss: 0.0037, weighted_loss: 0.0157, label: 1, bag_size: 8019\n",
      "batch 499, loss: 0.0433, instance_loss: 0.0003, weighted_loss: 0.0304, label: 1, bag_size: 4394\n",
      "batch 519, loss: 0.1162, instance_loss: 0.0127, weighted_loss: 0.0852, label: 0, bag_size: 12083\n",
      "batch 539, loss: 0.4766, instance_loss: 0.0070, weighted_loss: 0.3358, label: 0, bag_size: 15898\n",
      "batch 559, loss: 0.4564, instance_loss: 0.2148, weighted_loss: 0.3839, label: 0, bag_size: 26208\n",
      "batch 579, loss: 0.0414, instance_loss: 0.0145, weighted_loss: 0.0333, label: 1, bag_size: 2412\n",
      "batch 599, loss: 0.4029, instance_loss: 0.2266, weighted_loss: 0.3500, label: 0, bag_size: 1415\n",
      "batch 619, loss: 0.3924, instance_loss: 1.2929, weighted_loss: 0.6625, label: 1, bag_size: 2314\n",
      "batch 639, loss: 0.5571, instance_loss: 0.6829, weighted_loss: 0.5948, label: 1, bag_size: 11386\n",
      "batch 659, loss: 0.0476, instance_loss: 0.0003, weighted_loss: 0.0334, label: 1, bag_size: 10592\n",
      "batch 679, loss: 1.3869, instance_loss: 0.1025, weighted_loss: 1.0016, label: 1, bag_size: 9942\n",
      "batch 699, loss: 0.0951, instance_loss: 0.8589, weighted_loss: 0.3242, label: 1, bag_size: 1622\n",
      "batch 719, loss: 0.0389, instance_loss: 0.1892, weighted_loss: 0.0840, label: 1, bag_size: 11875\n",
      "batch 739, loss: 0.2106, instance_loss: 0.4302, weighted_loss: 0.2765, label: 1, bag_size: 6682\n",
      "batch 759, loss: 0.0899, instance_loss: 0.0032, weighted_loss: 0.0639, label: 0, bag_size: 11113\n",
      "batch 779, loss: 0.0367, instance_loss: 0.1088, weighted_loss: 0.0583, label: 0, bag_size: 2534\n",
      "batch 799, loss: 0.4574, instance_loss: 0.3764, weighted_loss: 0.4331, label: 0, bag_size: 1127\n",
      "batch 819, loss: 0.5949, instance_loss: 0.6610, weighted_loss: 0.6147, label: 1, bag_size: 6360\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9617378048780488: correct 12618/13120\n",
      "class 1 clustering acc 0.8268292682926829: correct 5424/6560\n",
      "Epoch: 14, train_loss: 0.2723, train_clustering_loss:  0.3311, train_error: 0.1085\n",
      "class 0: acc 0.8868421052631579, correct 337/380\n",
      "class 1: acc 0.8954545454545455, correct 394/440\n",
      "\n",
      "Val Set, val_loss: 0.3135, val_error: 0.1545, auc: 0.9592\n",
      "class 0 clustering acc 0.9204545454545454: correct 1620/1760\n",
      "class 1 clustering acc 0.7863636363636364: correct 692/880\n",
      "class 0: acc 0.7307692307692307, correct 38/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.2404, instance_loss: 2.7312, weighted_loss: 0.9877, label: 0, bag_size: 931\n",
      "batch 39, loss: 0.4198, instance_loss: 0.0074, weighted_loss: 0.2961, label: 0, bag_size: 1506\n",
      "batch 59, loss: 0.1050, instance_loss: 0.2938, weighted_loss: 0.1616, label: 0, bag_size: 2760\n",
      "batch 79, loss: 0.0657, instance_loss: 0.1957, weighted_loss: 0.1047, label: 0, bag_size: 1891\n",
      "batch 99, loss: 0.3495, instance_loss: 0.2264, weighted_loss: 0.3126, label: 0, bag_size: 518\n",
      "batch 119, loss: 0.0688, instance_loss: 0.2025, weighted_loss: 0.1089, label: 0, bag_size: 9252\n",
      "batch 139, loss: 0.3737, instance_loss: 0.1572, weighted_loss: 0.3087, label: 0, bag_size: 4598\n",
      "batch 159, loss: 0.0348, instance_loss: 0.0030, weighted_loss: 0.0253, label: 1, bag_size: 6731\n",
      "batch 179, loss: 0.5518, instance_loss: 0.0114, weighted_loss: 0.3897, label: 1, bag_size: 6682\n",
      "batch 199, loss: 0.0670, instance_loss: 0.0832, weighted_loss: 0.0719, label: 0, bag_size: 4902\n",
      "batch 219, loss: 0.0376, instance_loss: 0.0367, weighted_loss: 0.0373, label: 1, bag_size: 15689\n",
      "batch 239, loss: 0.0398, instance_loss: 0.0013, weighted_loss: 0.0283, label: 0, bag_size: 24439\n",
      "batch 259, loss: 0.0122, instance_loss: 0.0328, weighted_loss: 0.0184, label: 0, bag_size: 8898\n",
      "batch 279, loss: 0.2519, instance_loss: 0.0405, weighted_loss: 0.1885, label: 0, bag_size: 4997\n",
      "batch 299, loss: 0.9018, instance_loss: 0.8753, weighted_loss: 0.8939, label: 0, bag_size: 2815\n",
      "batch 319, loss: 0.2918, instance_loss: 1.0720, weighted_loss: 0.5259, label: 1, bag_size: 2785\n",
      "batch 339, loss: 1.8329, instance_loss: 0.8090, weighted_loss: 1.5257, label: 0, bag_size: 2242\n",
      "batch 359, loss: 0.0440, instance_loss: 0.0038, weighted_loss: 0.0320, label: 0, bag_size: 11527\n",
      "batch 379, loss: 0.0402, instance_loss: 0.0181, weighted_loss: 0.0336, label: 1, bag_size: 2140\n",
      "batch 399, loss: 1.3398, instance_loss: 3.0963, weighted_loss: 1.8668, label: 0, bag_size: 23618\n",
      "batch 419, loss: 0.1327, instance_loss: 0.0156, weighted_loss: 0.0976, label: 0, bag_size: 21093\n",
      "batch 439, loss: 0.0260, instance_loss: 0.1772, weighted_loss: 0.0714, label: 1, bag_size: 3295\n",
      "batch 459, loss: 0.0451, instance_loss: 0.0037, weighted_loss: 0.0327, label: 1, bag_size: 15609\n",
      "batch 479, loss: 0.0925, instance_loss: 0.0405, weighted_loss: 0.0769, label: 1, bag_size: 3674\n",
      "batch 499, loss: 0.0220, instance_loss: 0.0003, weighted_loss: 0.0155, label: 0, bag_size: 11917\n",
      "batch 519, loss: 0.2771, instance_loss: 0.0003, weighted_loss: 0.1941, label: 0, bag_size: 11113\n",
      "batch 539, loss: 0.2481, instance_loss: 0.0959, weighted_loss: 0.2025, label: 1, bag_size: 2678\n",
      "batch 559, loss: 0.0606, instance_loss: 0.0285, weighted_loss: 0.0510, label: 1, bag_size: 3651\n",
      "batch 579, loss: 0.4724, instance_loss: 0.1457, weighted_loss: 0.3744, label: 0, bag_size: 7612\n",
      "batch 599, loss: 0.1866, instance_loss: 0.4727, weighted_loss: 0.2725, label: 1, bag_size: 5723\n",
      "batch 619, loss: 1.0195, instance_loss: 1.1698, weighted_loss: 1.0646, label: 0, bag_size: 5009\n",
      "batch 639, loss: 0.0407, instance_loss: 0.3791, weighted_loss: 0.1422, label: 1, bag_size: 11387\n",
      "batch 659, loss: 1.2321, instance_loss: 2.5353, weighted_loss: 1.6231, label: 1, bag_size: 1609\n",
      "batch 679, loss: 0.2998, instance_loss: 0.3515, weighted_loss: 0.3153, label: 0, bag_size: 10029\n",
      "batch 699, loss: 0.0497, instance_loss: 0.0000, weighted_loss: 0.0348, label: 1, bag_size: 30675\n",
      "batch 719, loss: 0.0179, instance_loss: 0.0421, weighted_loss: 0.0252, label: 0, bag_size: 13795\n",
      "batch 739, loss: 0.2197, instance_loss: 0.0706, weighted_loss: 0.1750, label: 0, bag_size: 7612\n",
      "batch 759, loss: 0.0039, instance_loss: 0.3308, weighted_loss: 0.1020, label: 0, bag_size: 17791\n",
      "batch 779, loss: 0.1821, instance_loss: 0.9966, weighted_loss: 0.4265, label: 1, bag_size: 9983\n",
      "batch 799, loss: 0.0281, instance_loss: 0.7054, weighted_loss: 0.2313, label: 0, bag_size: 9433\n",
      "batch 819, loss: 0.2884, instance_loss: 0.5418, weighted_loss: 0.3645, label: 0, bag_size: 3541\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9602134146341463: correct 12598/13120\n",
      "class 1 clustering acc 0.7967987804878048: correct 5227/6560\n",
      "Epoch: 15, train_loss: 0.2514, train_clustering_loss:  0.3671, train_error: 0.0951\n",
      "class 0: acc 0.8987654320987655, correct 364/405\n",
      "class 1: acc 0.9108433734939759, correct 378/415\n",
      "\n",
      "Val Set, val_loss: 0.2756, val_error: 0.1364, auc: 0.9619\n",
      "class 0 clustering acc 0.9522727272727273: correct 1676/1760\n",
      "class 1 clustering acc 0.6488636363636363: correct 571/880\n",
      "class 0: acc 0.7692307692307693, correct 40/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "Validation loss decreased (0.287883 --> 0.275599).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0833, instance_loss: 0.1722, weighted_loss: 0.1100, label: 1, bag_size: 25695\n",
      "batch 39, loss: 0.8741, instance_loss: 0.8254, weighted_loss: 0.8595, label: 1, bag_size: 1095\n",
      "batch 59, loss: 0.7292, instance_loss: 1.1387, weighted_loss: 0.8520, label: 0, bag_size: 3893\n",
      "batch 79, loss: 0.0176, instance_loss: 0.3799, weighted_loss: 0.1263, label: 1, bag_size: 8522\n",
      "batch 99, loss: 0.0147, instance_loss: 0.0412, weighted_loss: 0.0227, label: 0, bag_size: 21076\n",
      "batch 119, loss: 0.8298, instance_loss: 0.4175, weighted_loss: 0.7061, label: 0, bag_size: 14664\n",
      "batch 139, loss: 0.0117, instance_loss: 0.0168, weighted_loss: 0.0132, label: 0, bag_size: 9866\n",
      "batch 159, loss: 0.0044, instance_loss: 0.3147, weighted_loss: 0.0975, label: 0, bag_size: 1984\n",
      "batch 179, loss: 0.0017, instance_loss: 0.2479, weighted_loss: 0.0756, label: 1, bag_size: 10033\n",
      "batch 199, loss: 0.0320, instance_loss: 0.2604, weighted_loss: 0.1005, label: 0, bag_size: 11727\n",
      "batch 219, loss: 0.2193, instance_loss: 0.4761, weighted_loss: 0.2963, label: 0, bag_size: 9616\n",
      "batch 239, loss: 0.0258, instance_loss: 0.3200, weighted_loss: 0.1140, label: 0, bag_size: 2424\n",
      "batch 259, loss: 0.0652, instance_loss: 0.1513, weighted_loss: 0.0911, label: 1, bag_size: 5561\n",
      "batch 279, loss: 0.0251, instance_loss: 0.0042, weighted_loss: 0.0188, label: 1, bag_size: 11363\n",
      "batch 299, loss: 0.0064, instance_loss: 0.1132, weighted_loss: 0.0384, label: 1, bag_size: 7513\n",
      "batch 319, loss: 0.1695, instance_loss: 0.5480, weighted_loss: 0.2830, label: 0, bag_size: 2296\n",
      "batch 339, loss: 0.2272, instance_loss: 0.1684, weighted_loss: 0.2096, label: 1, bag_size: 8040\n",
      "batch 359, loss: 0.0688, instance_loss: 0.4886, weighted_loss: 0.1947, label: 1, bag_size: 3674\n",
      "batch 379, loss: 0.0657, instance_loss: 0.2249, weighted_loss: 0.1134, label: 0, bag_size: 12731\n",
      "batch 399, loss: 0.0487, instance_loss: 0.1658, weighted_loss: 0.0839, label: 1, bag_size: 12895\n",
      "batch 419, loss: 0.6120, instance_loss: 0.1314, weighted_loss: 0.4678, label: 1, bag_size: 15563\n",
      "batch 439, loss: 0.2096, instance_loss: 0.2364, weighted_loss: 0.2176, label: 0, bag_size: 2036\n",
      "batch 459, loss: 0.4196, instance_loss: 0.7931, weighted_loss: 0.5316, label: 1, bag_size: 2662\n",
      "batch 479, loss: 0.7118, instance_loss: 0.6123, weighted_loss: 0.6819, label: 1, bag_size: 1683\n",
      "batch 499, loss: 0.0578, instance_loss: 0.8169, weighted_loss: 0.2855, label: 1, bag_size: 549\n",
      "batch 519, loss: 0.2489, instance_loss: 0.2070, weighted_loss: 0.2364, label: 0, bag_size: 6624\n",
      "batch 539, loss: 0.1156, instance_loss: 0.0389, weighted_loss: 0.0926, label: 0, bag_size: 8582\n",
      "batch 559, loss: 0.0295, instance_loss: 0.0574, weighted_loss: 0.0379, label: 0, bag_size: 11125\n",
      "batch 579, loss: 0.0150, instance_loss: 0.0508, weighted_loss: 0.0258, label: 0, bag_size: 23368\n",
      "batch 599, loss: 1.2355, instance_loss: 1.3950, weighted_loss: 1.2833, label: 1, bag_size: 1284\n",
      "batch 619, loss: 0.1999, instance_loss: 1.2912, weighted_loss: 0.5273, label: 0, bag_size: 2844\n",
      "batch 639, loss: 0.0176, instance_loss: 0.0271, weighted_loss: 0.0204, label: 1, bag_size: 15609\n",
      "batch 659, loss: 0.0253, instance_loss: 0.0101, weighted_loss: 0.0208, label: 1, bag_size: 15213\n",
      "batch 679, loss: 0.1734, instance_loss: 1.8932, weighted_loss: 0.6893, label: 1, bag_size: 15125\n",
      "batch 699, loss: 0.0135, instance_loss: 0.0157, weighted_loss: 0.0142, label: 1, bag_size: 11266\n",
      "batch 719, loss: 0.0637, instance_loss: 0.4772, weighted_loss: 0.1877, label: 1, bag_size: 5894\n",
      "batch 739, loss: 0.1484, instance_loss: 0.3155, weighted_loss: 0.1985, label: 0, bag_size: 16087\n",
      "batch 759, loss: 0.0126, instance_loss: 0.2518, weighted_loss: 0.0843, label: 1, bag_size: 5137\n",
      "batch 779, loss: 0.0092, instance_loss: 0.0656, weighted_loss: 0.0261, label: 0, bag_size: 11477\n",
      "batch 799, loss: 0.9075, instance_loss: 0.7468, weighted_loss: 0.8593, label: 1, bag_size: 6682\n",
      "batch 819, loss: 0.0072, instance_loss: 0.0994, weighted_loss: 0.0349, label: 0, bag_size: 2179\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9394817073170731: correct 12326/13120\n",
      "class 1 clustering acc 0.7346036585365854: correct 4819/6560\n",
      "Epoch: 16, train_loss: 0.2902, train_clustering_loss:  0.5420, train_error: 0.1024\n",
      "class 0: acc 0.9025, correct 361/400\n",
      "class 1: acc 0.8928571428571429, correct 375/420\n",
      "\n",
      "Val Set, val_loss: 0.2822, val_error: 0.1273, auc: 0.9632\n",
      "class 0 clustering acc 0.9392045454545455: correct 1653/1760\n",
      "class 1 clustering acc 0.7625: correct 671/880\n",
      "class 0: acc 0.7884615384615384, correct 41/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1077, instance_loss: 0.0753, weighted_loss: 0.0980, label: 1, bag_size: 6665\n",
      "batch 39, loss: 0.0683, instance_loss: 0.3703, weighted_loss: 0.1589, label: 0, bag_size: 1416\n",
      "batch 59, loss: 0.0290, instance_loss: 0.8218, weighted_loss: 0.2669, label: 1, bag_size: 621\n",
      "batch 79, loss: 0.0100, instance_loss: 0.1666, weighted_loss: 0.0570, label: 1, bag_size: 9644\n",
      "batch 99, loss: 0.0436, instance_loss: 0.0054, weighted_loss: 0.0321, label: 0, bag_size: 16211\n",
      "batch 119, loss: 0.0736, instance_loss: 0.0071, weighted_loss: 0.0537, label: 1, bag_size: 17486\n",
      "batch 139, loss: 0.0309, instance_loss: 0.0076, weighted_loss: 0.0239, label: 1, bag_size: 12697\n",
      "batch 159, loss: 0.4599, instance_loss: 0.2159, weighted_loss: 0.3867, label: 1, bag_size: 2814\n",
      "batch 179, loss: 0.0744, instance_loss: 0.6593, weighted_loss: 0.2499, label: 0, bag_size: 1614\n",
      "batch 199, loss: 0.5949, instance_loss: 0.8584, weighted_loss: 0.6740, label: 0, bag_size: 26208\n",
      "batch 219, loss: 0.5543, instance_loss: 0.1098, weighted_loss: 0.4210, label: 0, bag_size: 5639\n",
      "batch 239, loss: 0.0285, instance_loss: 0.3640, weighted_loss: 0.1291, label: 1, bag_size: 11389\n",
      "batch 259, loss: 0.0473, instance_loss: 0.0451, weighted_loss: 0.0467, label: 0, bag_size: 11727\n",
      "batch 279, loss: 0.0036, instance_loss: 0.1624, weighted_loss: 0.0513, label: 1, bag_size: 6950\n",
      "batch 299, loss: 2.0627, instance_loss: 1.7289, weighted_loss: 1.9626, label: 1, bag_size: 7981\n",
      "batch 319, loss: 0.0166, instance_loss: 0.1718, weighted_loss: 0.0631, label: 0, bag_size: 3190\n",
      "batch 339, loss: 2.1694, instance_loss: 2.4994, weighted_loss: 2.2684, label: 0, bag_size: 7428\n",
      "batch 359, loss: 1.0417, instance_loss: 0.6764, weighted_loss: 0.9321, label: 1, bag_size: 1123\n",
      "batch 379, loss: 0.4452, instance_loss: 0.0732, weighted_loss: 0.3336, label: 1, bag_size: 12946\n",
      "batch 399, loss: 0.0720, instance_loss: 0.0828, weighted_loss: 0.0752, label: 0, bag_size: 8145\n",
      "batch 419, loss: 0.0523, instance_loss: 0.0730, weighted_loss: 0.0585, label: 0, bag_size: 9234\n",
      "batch 439, loss: 0.1449, instance_loss: 0.2581, weighted_loss: 0.1789, label: 1, bag_size: 7935\n",
      "batch 459, loss: 0.0106, instance_loss: 0.0153, weighted_loss: 0.0120, label: 1, bag_size: 7078\n",
      "batch 479, loss: 2.3218, instance_loss: 3.4643, weighted_loss: 2.6645, label: 1, bag_size: 12180\n",
      "batch 499, loss: 0.1698, instance_loss: 0.4779, weighted_loss: 0.2622, label: 0, bag_size: 13619\n",
      "batch 519, loss: 0.0694, instance_loss: 0.1902, weighted_loss: 0.1056, label: 0, bag_size: 6624\n",
      "batch 539, loss: 0.6601, instance_loss: 0.7049, weighted_loss: 0.6735, label: 1, bag_size: 1609\n",
      "batch 559, loss: 0.0436, instance_loss: 0.2315, weighted_loss: 0.1000, label: 1, bag_size: 3683\n",
      "batch 579, loss: 0.0490, instance_loss: 0.0321, weighted_loss: 0.0439, label: 0, bag_size: 9415\n",
      "batch 599, loss: 0.2217, instance_loss: 0.5542, weighted_loss: 0.3214, label: 0, bag_size: 2043\n",
      "batch 619, loss: 0.1316, instance_loss: 0.2531, weighted_loss: 0.1680, label: 0, bag_size: 2920\n",
      "batch 639, loss: 1.8415, instance_loss: 1.5899, weighted_loss: 1.7660, label: 0, bag_size: 2179\n",
      "batch 659, loss: 0.0789, instance_loss: 0.2731, weighted_loss: 0.1372, label: 0, bag_size: 3444\n",
      "batch 679, loss: 0.4606, instance_loss: 0.4375, weighted_loss: 0.4537, label: 1, bag_size: 8475\n",
      "batch 699, loss: 0.0316, instance_loss: 0.0021, weighted_loss: 0.0227, label: 1, bag_size: 11964\n",
      "batch 719, loss: 0.0023, instance_loss: 0.0256, weighted_loss: 0.0093, label: 0, bag_size: 9470\n",
      "batch 739, loss: 0.0181, instance_loss: 0.0000, weighted_loss: 0.0127, label: 0, bag_size: 21864\n",
      "batch 759, loss: 0.0244, instance_loss: 0.2568, weighted_loss: 0.0941, label: 1, bag_size: 15332\n",
      "batch 779, loss: 0.0628, instance_loss: 0.5006, weighted_loss: 0.1941, label: 1, bag_size: 1412\n",
      "batch 799, loss: 0.0108, instance_loss: 0.0115, weighted_loss: 0.0110, label: 0, bag_size: 22426\n",
      "batch 819, loss: 0.0594, instance_loss: 0.0413, weighted_loss: 0.0540, label: 0, bag_size: 2920\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.958155487804878: correct 12571/13120\n",
      "class 1 clustering acc 0.7835365853658537: correct 5140/6560\n",
      "Epoch: 17, train_loss: 0.2859, train_clustering_loss:  0.4186, train_error: 0.1073\n",
      "class 0: acc 0.8969555035128806, correct 383/427\n",
      "class 1: acc 0.8880407124681934, correct 349/393\n",
      "\n",
      "Val Set, val_loss: 0.2745, val_error: 0.1273, auc: 0.9615\n",
      "class 0 clustering acc 0.9306818181818182: correct 1638/1760\n",
      "class 1 clustering acc 0.8113636363636364: correct 714/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.7931034482758621, correct 46/58\n",
      "Validation loss decreased (0.275599 --> 0.274520).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0568, instance_loss: 0.3723, weighted_loss: 0.1515, label: 0, bag_size: 2336\n",
      "batch 39, loss: 0.0012, instance_loss: 0.3443, weighted_loss: 0.1041, label: 1, bag_size: 7513\n",
      "batch 59, loss: 0.7067, instance_loss: 1.5966, weighted_loss: 0.9736, label: 1, bag_size: 1038\n",
      "batch 79, loss: 0.2677, instance_loss: 0.1897, weighted_loss: 0.2443, label: 1, bag_size: 9321\n",
      "batch 99, loss: 0.3497, instance_loss: 0.1127, weighted_loss: 0.2786, label: 1, bag_size: 7351\n",
      "batch 119, loss: 0.0936, instance_loss: 0.0390, weighted_loss: 0.0772, label: 1, bag_size: 4394\n",
      "batch 139, loss: 0.3269, instance_loss: 0.2349, weighted_loss: 0.2993, label: 0, bag_size: 2918\n",
      "batch 159, loss: 1.3941, instance_loss: 2.8407, weighted_loss: 1.8281, label: 1, bag_size: 1867\n",
      "batch 179, loss: 0.2038, instance_loss: 4.1284, weighted_loss: 1.3812, label: 1, bag_size: 9404\n",
      "batch 199, loss: 0.0074, instance_loss: 0.0108, weighted_loss: 0.0084, label: 1, bag_size: 7078\n",
      "batch 219, loss: 0.0492, instance_loss: 0.1440, weighted_loss: 0.0776, label: 1, bag_size: 8685\n",
      "batch 239, loss: 0.0033, instance_loss: 0.3131, weighted_loss: 0.0962, label: 1, bag_size: 3640\n",
      "batch 259, loss: 0.0022, instance_loss: 0.0103, weighted_loss: 0.0046, label: 1, bag_size: 7371\n",
      "batch 279, loss: 0.1223, instance_loss: 0.0632, weighted_loss: 0.1045, label: 0, bag_size: 10490\n",
      "batch 299, loss: 0.0419, instance_loss: 0.0000, weighted_loss: 0.0293, label: 0, bag_size: 5965\n",
      "batch 319, loss: 0.0385, instance_loss: 0.1324, weighted_loss: 0.0667, label: 1, bag_size: 7246\n",
      "batch 339, loss: 0.0008, instance_loss: 0.0909, weighted_loss: 0.0278, label: 1, bag_size: 1781\n",
      "batch 359, loss: 0.0044, instance_loss: 0.0003, weighted_loss: 0.0032, label: 1, bag_size: 19832\n",
      "batch 379, loss: 0.1925, instance_loss: 0.3564, weighted_loss: 0.2417, label: 1, bag_size: 1459\n",
      "batch 399, loss: 0.0347, instance_loss: 0.0262, weighted_loss: 0.0322, label: 1, bag_size: 6731\n",
      "batch 419, loss: 0.0394, instance_loss: 0.0091, weighted_loss: 0.0303, label: 0, bag_size: 8898\n",
      "batch 439, loss: 0.0636, instance_loss: 0.0068, weighted_loss: 0.0465, label: 1, bag_size: 14202\n",
      "batch 459, loss: 0.0400, instance_loss: 0.1160, weighted_loss: 0.0628, label: 0, bag_size: 12796\n",
      "batch 479, loss: 0.4511, instance_loss: 0.2085, weighted_loss: 0.3784, label: 0, bag_size: 2998\n",
      "batch 499, loss: 0.2041, instance_loss: 0.3220, weighted_loss: 0.2395, label: 0, bag_size: 1760\n",
      "batch 519, loss: 0.9440, instance_loss: 1.3602, weighted_loss: 1.0689, label: 1, bag_size: 1920\n",
      "batch 539, loss: 0.1501, instance_loss: 0.3022, weighted_loss: 0.1957, label: 0, bag_size: 2998\n",
      "batch 559, loss: 0.2137, instance_loss: 0.2723, weighted_loss: 0.2313, label: 0, bag_size: 1797\n",
      "batch 579, loss: 0.0026, instance_loss: 0.0532, weighted_loss: 0.0178, label: 1, bag_size: 5317\n",
      "batch 599, loss: 0.0342, instance_loss: 0.7085, weighted_loss: 0.2365, label: 0, bag_size: 2628\n",
      "batch 619, loss: 0.0477, instance_loss: 0.7424, weighted_loss: 0.2561, label: 1, bag_size: 12895\n",
      "batch 639, loss: 0.3387, instance_loss: 0.6277, weighted_loss: 0.4254, label: 1, bag_size: 8475\n",
      "batch 659, loss: 0.0114, instance_loss: 0.5144, weighted_loss: 0.1623, label: 0, bag_size: 10304\n",
      "batch 679, loss: 0.0536, instance_loss: 0.0132, weighted_loss: 0.0415, label: 1, bag_size: 19832\n",
      "batch 699, loss: 0.0766, instance_loss: 1.1096, weighted_loss: 0.3865, label: 0, bag_size: 10146\n",
      "batch 719, loss: 0.2951, instance_loss: 0.6046, weighted_loss: 0.3879, label: 0, bag_size: 2043\n",
      "batch 739, loss: 0.0248, instance_loss: 0.0860, weighted_loss: 0.0431, label: 1, bag_size: 5833\n",
      "batch 759, loss: 0.0035, instance_loss: 0.1600, weighted_loss: 0.0504, label: 1, bag_size: 9971\n",
      "batch 779, loss: 0.1401, instance_loss: 0.1102, weighted_loss: 0.1311, label: 0, bag_size: 3444\n",
      "batch 799, loss: 0.0361, instance_loss: 0.0002, weighted_loss: 0.0253, label: 0, bag_size: 19518\n",
      "batch 819, loss: 0.2347, instance_loss: 0.1150, weighted_loss: 0.1988, label: 1, bag_size: 15125\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.958079268292683: correct 12570/13120\n",
      "class 1 clustering acc 0.7900914634146341: correct 5183/6560\n",
      "Epoch: 18, train_loss: 0.2893, train_clustering_loss:  0.4018, train_error: 0.1122\n",
      "class 0: acc 0.883419689119171, correct 341/386\n",
      "class 1: acc 0.8917050691244239, correct 387/434\n",
      "\n",
      "Val Set, val_loss: 0.2553, val_error: 0.1182, auc: 0.9632\n",
      "class 0 clustering acc 0.9477272727272728: correct 1668/1760\n",
      "class 1 clustering acc 0.8852272727272728: correct 779/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8103448275862069, correct 47/58\n",
      "Validation loss decreased (0.274520 --> 0.255295).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0415, instance_loss: 2.7810, weighted_loss: 0.8633, label: 0, bag_size: 763\n",
      "batch 39, loss: 0.0060, instance_loss: 0.4888, weighted_loss: 0.1509, label: 0, bag_size: 13795\n",
      "batch 59, loss: 0.1650, instance_loss: 1.5696, weighted_loss: 0.5864, label: 0, bag_size: 3893\n",
      "batch 79, loss: 0.5473, instance_loss: 0.9661, weighted_loss: 0.6729, label: 1, bag_size: 8026\n",
      "batch 99, loss: 1.6275, instance_loss: 0.1722, weighted_loss: 1.1909, label: 1, bag_size: 9942\n",
      "batch 119, loss: 0.7788, instance_loss: 0.5704, weighted_loss: 0.7163, label: 1, bag_size: 10492\n",
      "batch 139, loss: 1.8382, instance_loss: 2.4105, weighted_loss: 2.0099, label: 0, bag_size: 1506\n",
      "batch 159, loss: 0.0767, instance_loss: 0.2951, weighted_loss: 0.1422, label: 1, bag_size: 4259\n",
      "batch 179, loss: 1.0540, instance_loss: 0.8522, weighted_loss: 0.9935, label: 0, bag_size: 3654\n",
      "batch 199, loss: 1.0099, instance_loss: 1.3570, weighted_loss: 1.1140, label: 0, bag_size: 7239\n",
      "batch 219, loss: 0.1220, instance_loss: 0.3531, weighted_loss: 0.1913, label: 1, bag_size: 7246\n",
      "batch 239, loss: 0.0008, instance_loss: 0.0641, weighted_loss: 0.0198, label: 1, bag_size: 4877\n",
      "batch 259, loss: 0.0556, instance_loss: 0.4878, weighted_loss: 0.1853, label: 1, bag_size: 16890\n",
      "batch 279, loss: 0.0097, instance_loss: 0.6309, weighted_loss: 0.1961, label: 1, bag_size: 7767\n",
      "batch 299, loss: 4.5779, instance_loss: 2.3113, weighted_loss: 3.8979, label: 1, bag_size: 2565\n",
      "batch 319, loss: 0.1449, instance_loss: 0.6050, weighted_loss: 0.2829, label: 1, bag_size: 18161\n",
      "batch 339, loss: 0.0429, instance_loss: 0.2467, weighted_loss: 0.1041, label: 1, bag_size: 5731\n",
      "batch 359, loss: 0.5787, instance_loss: 0.5700, weighted_loss: 0.5761, label: 0, bag_size: 24382\n",
      "batch 379, loss: 0.0229, instance_loss: 0.0575, weighted_loss: 0.0332, label: 1, bag_size: 13015\n",
      "batch 399, loss: 0.0486, instance_loss: 0.1114, weighted_loss: 0.0674, label: 0, bag_size: 1891\n",
      "batch 419, loss: 0.0221, instance_loss: 0.0044, weighted_loss: 0.0168, label: 0, bag_size: 12796\n",
      "batch 439, loss: 0.0311, instance_loss: 0.0882, weighted_loss: 0.0482, label: 0, bag_size: 17155\n",
      "batch 459, loss: 0.0995, instance_loss: 0.9483, weighted_loss: 0.3541, label: 0, bag_size: 2820\n",
      "batch 479, loss: 0.0016, instance_loss: 0.0953, weighted_loss: 0.0297, label: 1, bag_size: 7513\n",
      "batch 499, loss: 0.3578, instance_loss: 2.1892, weighted_loss: 0.9073, label: 1, bag_size: 3450\n",
      "batch 519, loss: 0.0372, instance_loss: 0.4642, weighted_loss: 0.1653, label: 0, bag_size: 6652\n",
      "batch 539, loss: 0.1675, instance_loss: 0.7585, weighted_loss: 0.3448, label: 0, bag_size: 10381\n",
      "batch 559, loss: 0.0693, instance_loss: 0.4985, weighted_loss: 0.1981, label: 0, bag_size: 2036\n",
      "batch 579, loss: 0.0400, instance_loss: 0.2084, weighted_loss: 0.0905, label: 0, bag_size: 3970\n",
      "batch 599, loss: 0.0125, instance_loss: 0.1312, weighted_loss: 0.0481, label: 0, bag_size: 11512\n",
      "batch 619, loss: 0.0167, instance_loss: 0.0350, weighted_loss: 0.0222, label: 1, bag_size: 11266\n",
      "batch 639, loss: 0.0149, instance_loss: 0.1244, weighted_loss: 0.0477, label: 1, bag_size: 9644\n",
      "batch 659, loss: 1.7311, instance_loss: 0.0301, weighted_loss: 1.2208, label: 1, bag_size: 9942\n",
      "batch 679, loss: 0.0026, instance_loss: 0.0009, weighted_loss: 0.0021, label: 1, bag_size: 15233\n",
      "batch 699, loss: 0.0045, instance_loss: 0.0002, weighted_loss: 0.0032, label: 1, bag_size: 15008\n",
      "batch 719, loss: 0.0672, instance_loss: 0.0118, weighted_loss: 0.0506, label: 0, bag_size: 10128\n",
      "batch 739, loss: 0.3791, instance_loss: 0.4553, weighted_loss: 0.4019, label: 1, bag_size: 3980\n",
      "batch 759, loss: 0.0032, instance_loss: 0.0311, weighted_loss: 0.0116, label: 1, bag_size: 6164\n",
      "batch 779, loss: 0.0569, instance_loss: 0.1011, weighted_loss: 0.0701, label: 0, bag_size: 14333\n",
      "batch 799, loss: 0.0115, instance_loss: 0.0038, weighted_loss: 0.0091, label: 1, bag_size: 11642\n",
      "batch 819, loss: 0.0115, instance_loss: 0.5270, weighted_loss: 0.1662, label: 0, bag_size: 18240\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9446646341463415: correct 12394/13120\n",
      "class 1 clustering acc 0.68125: correct 4469/6560\n",
      "Epoch: 19, train_loss: 0.3082, train_clustering_loss:  0.5428, train_error: 0.1244\n",
      "class 0: acc 0.878345498783455, correct 361/411\n",
      "class 1: acc 0.8728606356968215, correct 357/409\n",
      "\n",
      "Val Set, val_loss: 0.2505, val_error: 0.1182, auc: 0.9645\n",
      "class 0 clustering acc 0.9130681818181818: correct 1607/1760\n",
      "class 1 clustering acc 0.6363636363636364: correct 560/880\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 0.9137931034482759, correct 53/58\n",
      "Validation loss decreased (0.255295 --> 0.250451).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0659, instance_loss: 1.1542, weighted_loss: 0.3924, label: 0, bag_size: 4959\n",
      "batch 39, loss: 0.0017, instance_loss: 0.0016, weighted_loss: 0.0017, label: 1, bag_size: 6966\n",
      "batch 59, loss: 0.0811, instance_loss: 0.5735, weighted_loss: 0.2288, label: 1, bag_size: 9561\n",
      "batch 79, loss: 0.0325, instance_loss: 0.0235, weighted_loss: 0.0298, label: 1, bag_size: 1412\n",
      "batch 99, loss: 0.0055, instance_loss: 0.1073, weighted_loss: 0.0360, label: 0, bag_size: 9433\n",
      "batch 119, loss: 0.0061, instance_loss: 0.0086, weighted_loss: 0.0068, label: 1, bag_size: 6966\n",
      "batch 139, loss: 0.7508, instance_loss: 1.1805, weighted_loss: 0.8798, label: 0, bag_size: 3375\n",
      "batch 159, loss: 0.0982, instance_loss: 0.1855, weighted_loss: 0.1244, label: 0, bag_size: 763\n",
      "batch 179, loss: 0.0248, instance_loss: 0.0991, weighted_loss: 0.0471, label: 0, bag_size: 10415\n",
      "batch 199, loss: 0.0487, instance_loss: 0.0762, weighted_loss: 0.0570, label: 0, bag_size: 22828\n",
      "batch 219, loss: 0.2024, instance_loss: 0.2297, weighted_loss: 0.2106, label: 0, bag_size: 5409\n",
      "batch 239, loss: 0.5610, instance_loss: 0.6861, weighted_loss: 0.5985, label: 1, bag_size: 5160\n",
      "batch 259, loss: 0.0468, instance_loss: 0.1072, weighted_loss: 0.0649, label: 1, bag_size: 11122\n",
      "batch 279, loss: 0.0089, instance_loss: 0.0035, weighted_loss: 0.0073, label: 0, bag_size: 16211\n",
      "batch 299, loss: 0.0502, instance_loss: 0.4941, weighted_loss: 0.1833, label: 0, bag_size: 8959\n",
      "batch 319, loss: 1.2164, instance_loss: 0.8773, weighted_loss: 1.1147, label: 0, bag_size: 2959\n",
      "batch 339, loss: 0.1244, instance_loss: 0.0025, weighted_loss: 0.0878, label: 1, bag_size: 18649\n",
      "batch 359, loss: 0.0339, instance_loss: 0.0042, weighted_loss: 0.0250, label: 0, bag_size: 22870\n",
      "batch 379, loss: 0.0600, instance_loss: 0.0146, weighted_loss: 0.0464, label: 1, bag_size: 12895\n",
      "batch 399, loss: 0.0053, instance_loss: 0.0000, weighted_loss: 0.0037, label: 1, bag_size: 19039\n",
      "batch 419, loss: 0.0082, instance_loss: 0.1713, weighted_loss: 0.0571, label: 0, bag_size: 8948\n",
      "batch 439, loss: 0.0540, instance_loss: 0.4203, weighted_loss: 0.1639, label: 0, bag_size: 1588\n",
      "batch 459, loss: 0.0157, instance_loss: 0.0192, weighted_loss: 0.0167, label: 1, bag_size: 14433\n",
      "batch 479, loss: 0.2082, instance_loss: 0.1122, weighted_loss: 0.1794, label: 0, bag_size: 2195\n",
      "batch 499, loss: 1.0121, instance_loss: 0.9526, weighted_loss: 0.9942, label: 0, bag_size: 2179\n",
      "batch 519, loss: 0.1099, instance_loss: 0.0998, weighted_loss: 0.1069, label: 1, bag_size: 8754\n",
      "batch 539, loss: 0.1010, instance_loss: 0.6010, weighted_loss: 0.2510, label: 1, bag_size: 2356\n",
      "batch 559, loss: 0.1010, instance_loss: 0.3558, weighted_loss: 0.1774, label: 1, bag_size: 9446\n",
      "batch 579, loss: 0.5638, instance_loss: 0.1855, weighted_loss: 0.4503, label: 1, bag_size: 10848\n",
      "batch 599, loss: 0.0301, instance_loss: 0.0908, weighted_loss: 0.0484, label: 0, bag_size: 3459\n",
      "batch 619, loss: 1.0686, instance_loss: 2.2376, weighted_loss: 1.4193, label: 1, bag_size: 1609\n",
      "batch 639, loss: 0.1208, instance_loss: 0.1222, weighted_loss: 0.1212, label: 0, bag_size: 1349\n",
      "batch 659, loss: 0.0341, instance_loss: 0.0014, weighted_loss: 0.0243, label: 1, bag_size: 19606\n",
      "batch 679, loss: 0.0453, instance_loss: 0.0000, weighted_loss: 0.0317, label: 1, bag_size: 21827\n",
      "batch 699, loss: 0.0052, instance_loss: 0.0141, weighted_loss: 0.0079, label: 0, bag_size: 10304\n",
      "batch 719, loss: 0.2658, instance_loss: 0.5859, weighted_loss: 0.3619, label: 1, bag_size: 1823\n",
      "batch 739, loss: 0.0792, instance_loss: 0.0022, weighted_loss: 0.0561, label: 1, bag_size: 6752\n",
      "batch 759, loss: 0.1106, instance_loss: 0.0578, weighted_loss: 0.0948, label: 1, bag_size: 4789\n",
      "batch 779, loss: 0.0226, instance_loss: 0.0284, weighted_loss: 0.0243, label: 0, bag_size: 1881\n",
      "batch 799, loss: 0.0040, instance_loss: 0.0000, weighted_loss: 0.0028, label: 0, bag_size: 14266\n",
      "batch 819, loss: 0.0020, instance_loss: 0.0872, weighted_loss: 0.0275, label: 1, bag_size: 6950\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9659298780487805: correct 12673/13120\n",
      "class 1 clustering acc 0.8048780487804879: correct 5280/6560\n",
      "Epoch: 20, train_loss: 0.2652, train_clustering_loss:  0.3508, train_error: 0.1024\n",
      "class 0: acc 0.903755868544601, correct 385/426\n",
      "class 1: acc 0.8908629441624365, correct 351/394\n",
      "\n",
      "Val Set, val_loss: 0.2598, val_error: 0.1091, auc: 0.9678\n",
      "class 0 clustering acc 0.9517045454545454: correct 1675/1760\n",
      "class 1 clustering acc 0.8522727272727273: correct 750/880\n",
      "class 0: acc 0.8076923076923077, correct 42/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4015, instance_loss: 0.0783, weighted_loss: 0.3046, label: 1, bag_size: 7381\n",
      "batch 39, loss: 0.0295, instance_loss: 0.0310, weighted_loss: 0.0300, label: 1, bag_size: 9478\n",
      "batch 59, loss: 0.0456, instance_loss: 0.0013, weighted_loss: 0.0323, label: 1, bag_size: 10498\n",
      "batch 79, loss: 0.0729, instance_loss: 0.0802, weighted_loss: 0.0751, label: 1, bag_size: 5690\n",
      "batch 99, loss: 0.1370, instance_loss: 0.4506, weighted_loss: 0.2311, label: 1, bag_size: 9983\n",
      "batch 119, loss: 0.0120, instance_loss: 0.3317, weighted_loss: 0.1079, label: 0, bag_size: 1588\n",
      "batch 139, loss: 0.1913, instance_loss: 0.0006, weighted_loss: 0.1341, label: 1, bag_size: 10492\n",
      "batch 159, loss: 0.0164, instance_loss: 0.0200, weighted_loss: 0.0175, label: 1, bag_size: 8216\n",
      "batch 179, loss: 0.0377, instance_loss: 0.0005, weighted_loss: 0.0265, label: 0, bag_size: 2044\n",
      "batch 199, loss: 0.1914, instance_loss: 0.0003, weighted_loss: 0.1341, label: 0, bag_size: 14625\n",
      "batch 219, loss: 0.0272, instance_loss: 0.0000, weighted_loss: 0.0191, label: 1, bag_size: 14618\n",
      "batch 239, loss: 0.0042, instance_loss: 0.0000, weighted_loss: 0.0029, label: 0, bag_size: 11900\n",
      "batch 259, loss: 0.1123, instance_loss: 0.0003, weighted_loss: 0.0787, label: 0, bag_size: 2004\n",
      "batch 279, loss: 0.0310, instance_loss: 0.0038, weighted_loss: 0.0229, label: 0, bag_size: 12524\n",
      "batch 299, loss: 0.0829, instance_loss: 0.1839, weighted_loss: 0.1132, label: 0, bag_size: 2044\n",
      "batch 319, loss: 0.3115, instance_loss: 0.3948, weighted_loss: 0.3365, label: 1, bag_size: 10848\n",
      "batch 339, loss: 0.4188, instance_loss: 0.8389, weighted_loss: 0.5448, label: 1, bag_size: 6726\n",
      "batch 359, loss: 0.0248, instance_loss: 0.0007, weighted_loss: 0.0176, label: 1, bag_size: 12603\n",
      "batch 379, loss: 0.0006, instance_loss: 0.0259, weighted_loss: 0.0082, label: 1, bag_size: 10392\n",
      "batch 399, loss: 0.1005, instance_loss: 0.3842, weighted_loss: 0.1856, label: 1, bag_size: 6734\n",
      "batch 419, loss: 0.1109, instance_loss: 0.0111, weighted_loss: 0.0810, label: 1, bag_size: 13786\n",
      "batch 439, loss: 0.0270, instance_loss: 0.4963, weighted_loss: 0.1678, label: 0, bag_size: 3265\n",
      "batch 459, loss: 0.0436, instance_loss: 0.0028, weighted_loss: 0.0313, label: 0, bag_size: 3725\n",
      "batch 479, loss: 0.7474, instance_loss: 0.5030, weighted_loss: 0.6741, label: 1, bag_size: 8040\n",
      "batch 499, loss: 0.0076, instance_loss: 0.0039, weighted_loss: 0.0065, label: 1, bag_size: 9644\n",
      "batch 519, loss: 0.0160, instance_loss: 0.0024, weighted_loss: 0.0119, label: 1, bag_size: 16051\n",
      "batch 539, loss: 0.0026, instance_loss: 1.1736, weighted_loss: 0.3539, label: 1, bag_size: 19039\n",
      "batch 559, loss: 2.1973, instance_loss: 2.7209, weighted_loss: 2.3544, label: 0, bag_size: 7239\n",
      "batch 579, loss: 0.6679, instance_loss: 0.5425, weighted_loss: 0.6303, label: 0, bag_size: 7239\n",
      "batch 599, loss: 0.0760, instance_loss: 0.2557, weighted_loss: 0.1299, label: 0, bag_size: 21093\n",
      "batch 619, loss: 0.0709, instance_loss: 0.7623, weighted_loss: 0.2783, label: 0, bag_size: 11212\n",
      "batch 639, loss: 0.0174, instance_loss: 0.0569, weighted_loss: 0.0292, label: 0, bag_size: 1234\n",
      "batch 659, loss: 0.0328, instance_loss: 0.3682, weighted_loss: 0.1334, label: 1, bag_size: 5629\n",
      "batch 679, loss: 0.0081, instance_loss: 0.0003, weighted_loss: 0.0058, label: 0, bag_size: 22828\n",
      "batch 699, loss: 0.0370, instance_loss: 0.0101, weighted_loss: 0.0290, label: 1, bag_size: 12697\n",
      "batch 719, loss: 0.1172, instance_loss: 0.5215, weighted_loss: 0.2385, label: 0, bag_size: 6850\n",
      "batch 739, loss: 0.0125, instance_loss: 0.0319, weighted_loss: 0.0183, label: 1, bag_size: 6752\n",
      "batch 759, loss: 0.0235, instance_loss: 0.9822, weighted_loss: 0.3111, label: 0, bag_size: 1614\n",
      "batch 779, loss: 0.0087, instance_loss: 0.0239, weighted_loss: 0.0133, label: 0, bag_size: 12201\n",
      "batch 799, loss: 0.1474, instance_loss: 0.0172, weighted_loss: 0.1084, label: 0, bag_size: 1884\n",
      "batch 819, loss: 6.7838, instance_loss: 4.0444, weighted_loss: 5.9620, label: 0, bag_size: 3897\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9730182926829268: correct 12766/13120\n",
      "class 1 clustering acc 0.8596036585365854: correct 5639/6560\n",
      "Epoch: 21, train_loss: 0.2320, train_clustering_loss:  0.2666, train_error: 0.0829\n",
      "class 0: acc 0.916256157635468, correct 372/406\n",
      "class 1: acc 0.9178743961352657, correct 380/414\n",
      "\n",
      "Val Set, val_loss: 0.2678, val_error: 0.1091, auc: 0.9692\n",
      "class 0 clustering acc 0.9477272727272728: correct 1668/1760\n",
      "class 1 clustering acc 0.8534090909090909: correct 751/880\n",
      "class 0: acc 0.8076923076923077, correct 42/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0426, instance_loss: 0.0000, weighted_loss: 0.0298, label: 0, bag_size: 21682\n",
      "batch 39, loss: 0.0539, instance_loss: 0.0105, weighted_loss: 0.0409, label: 1, bag_size: 11387\n",
      "batch 59, loss: 0.7682, instance_loss: 0.0435, weighted_loss: 0.5508, label: 1, bag_size: 7381\n",
      "batch 79, loss: 0.0041, instance_loss: 0.0026, weighted_loss: 0.0036, label: 1, bag_size: 9689\n",
      "batch 99, loss: 0.1135, instance_loss: 0.1098, weighted_loss: 0.1124, label: 0, bag_size: 8959\n",
      "batch 119, loss: 0.0129, instance_loss: 0.0031, weighted_loss: 0.0100, label: 1, bag_size: 8602\n",
      "batch 139, loss: 0.0034, instance_loss: 0.0142, weighted_loss: 0.0066, label: 0, bag_size: 7191\n",
      "batch 159, loss: 0.0682, instance_loss: 0.0016, weighted_loss: 0.0482, label: 1, bag_size: 13365\n",
      "batch 179, loss: 0.0044, instance_loss: 0.0014, weighted_loss: 0.0035, label: 0, bag_size: 21218\n",
      "batch 199, loss: 0.0603, instance_loss: 0.0014, weighted_loss: 0.0426, label: 0, bag_size: 11113\n",
      "batch 219, loss: 0.9381, instance_loss: 0.4566, weighted_loss: 0.7936, label: 1, bag_size: 1284\n",
      "batch 239, loss: 1.0241, instance_loss: 0.0085, weighted_loss: 0.7194, label: 0, bag_size: 5297\n",
      "batch 259, loss: 0.1825, instance_loss: 0.0660, weighted_loss: 0.1476, label: 0, bag_size: 10029\n",
      "batch 279, loss: 0.0712, instance_loss: 0.1955, weighted_loss: 0.1085, label: 0, bag_size: 2534\n",
      "batch 299, loss: 0.3767, instance_loss: 0.4186, weighted_loss: 0.3893, label: 0, bag_size: 7835\n",
      "batch 319, loss: 0.0261, instance_loss: 0.0095, weighted_loss: 0.0211, label: 1, bag_size: 16162\n",
      "batch 339, loss: 0.2527, instance_loss: 0.0094, weighted_loss: 0.1797, label: 1, bag_size: 12895\n",
      "batch 359, loss: 0.0932, instance_loss: 0.1469, weighted_loss: 0.1093, label: 1, bag_size: 689\n",
      "batch 379, loss: 0.0039, instance_loss: 0.0007, weighted_loss: 0.0029, label: 1, bag_size: 1838\n",
      "batch 399, loss: 0.1025, instance_loss: 0.4245, weighted_loss: 0.1991, label: 0, bag_size: 763\n",
      "batch 419, loss: 0.1001, instance_loss: 0.0443, weighted_loss: 0.0834, label: 1, bag_size: 9533\n",
      "batch 439, loss: 0.0025, instance_loss: 0.0125, weighted_loss: 0.0055, label: 1, bag_size: 14223\n",
      "batch 459, loss: 0.1418, instance_loss: 0.1378, weighted_loss: 0.1406, label: 1, bag_size: 12340\n",
      "batch 479, loss: 1.3864, instance_loss: 0.7207, weighted_loss: 1.1867, label: 0, bag_size: 1592\n",
      "batch 499, loss: 0.7364, instance_loss: 0.4402, weighted_loss: 0.6475, label: 0, bag_size: 1506\n",
      "batch 519, loss: 1.7472, instance_loss: 1.2610, weighted_loss: 1.6013, label: 1, bag_size: 2455\n",
      "batch 539, loss: 0.0172, instance_loss: 0.0019, weighted_loss: 0.0126, label: 0, bag_size: 26271\n",
      "batch 559, loss: 0.1197, instance_loss: 0.0186, weighted_loss: 0.0893, label: 1, bag_size: 22264\n",
      "batch 579, loss: 0.6356, instance_loss: 0.7580, weighted_loss: 0.6723, label: 1, bag_size: 8103\n",
      "batch 599, loss: 1.1275, instance_loss: 0.0278, weighted_loss: 0.7976, label: 1, bag_size: 9942\n",
      "batch 619, loss: 0.5821, instance_loss: 1.5290, weighted_loss: 0.8661, label: 0, bag_size: 4418\n",
      "batch 639, loss: 0.5003, instance_loss: 2.9066, weighted_loss: 1.2222, label: 1, bag_size: 8026\n",
      "batch 659, loss: 0.0006, instance_loss: 0.0022, weighted_loss: 0.0011, label: 1, bag_size: 5612\n",
      "batch 679, loss: 0.1497, instance_loss: 0.0289, weighted_loss: 0.1135, label: 0, bag_size: 8744\n",
      "batch 699, loss: 0.1049, instance_loss: 0.0178, weighted_loss: 0.0787, label: 1, bag_size: 7613\n",
      "batch 719, loss: 0.4110, instance_loss: 1.7002, weighted_loss: 0.7978, label: 1, bag_size: 1242\n",
      "batch 739, loss: 0.9481, instance_loss: 0.3262, weighted_loss: 0.7615, label: 0, bag_size: 12510\n",
      "batch 759, loss: 0.0004, instance_loss: 0.0026, weighted_loss: 0.0011, label: 1, bag_size: 5612\n",
      "batch 779, loss: 0.0727, instance_loss: 0.0037, weighted_loss: 0.0520, label: 0, bag_size: 12732\n",
      "batch 799, loss: 0.1119, instance_loss: 0.0336, weighted_loss: 0.0885, label: 0, bag_size: 3444\n",
      "batch 819, loss: 0.0130, instance_loss: 0.0000, weighted_loss: 0.0091, label: 0, bag_size: 19466\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9753810975609756: correct 12797/13120\n",
      "class 1 clustering acc 0.8698170731707318: correct 5706/6560\n",
      "Epoch: 22, train_loss: 0.2515, train_clustering_loss:  0.2489, train_error: 0.1024\n",
      "class 0: acc 0.8921052631578947, correct 339/380\n",
      "class 1: acc 0.9022727272727272, correct 397/440\n",
      "\n",
      "Val Set, val_loss: 0.2730, val_error: 0.1091, auc: 0.9698\n",
      "class 0 clustering acc 0.9267045454545455: correct 1631/1760\n",
      "class 1 clustering acc 0.8363636363636363: correct 736/880\n",
      "class 0: acc 0.8076923076923077, correct 42/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0073, instance_loss: 0.0000, weighted_loss: 0.0051, label: 1, bag_size: 12408\n",
      "batch 39, loss: 0.0100, instance_loss: 0.0079, weighted_loss: 0.0094, label: 0, bag_size: 12217\n",
      "batch 59, loss: 0.0887, instance_loss: 0.0122, weighted_loss: 0.0658, label: 1, bag_size: 3450\n",
      "batch 79, loss: 0.0216, instance_loss: 0.0000, weighted_loss: 0.0151, label: 1, bag_size: 13947\n",
      "batch 99, loss: 0.2463, instance_loss: 0.3038, weighted_loss: 0.2636, label: 0, bag_size: 10381\n",
      "batch 119, loss: 0.0208, instance_loss: 0.0000, weighted_loss: 0.0146, label: 1, bag_size: 19932\n",
      "batch 139, loss: 0.0869, instance_loss: 0.5514, weighted_loss: 0.2263, label: 0, bag_size: 1458\n",
      "batch 159, loss: 0.5341, instance_loss: 0.0050, weighted_loss: 0.3754, label: 0, bag_size: 15071\n",
      "batch 179, loss: 0.7182, instance_loss: 0.0069, weighted_loss: 0.5048, label: 0, bag_size: 14264\n",
      "batch 199, loss: 0.0057, instance_loss: 0.0116, weighted_loss: 0.0075, label: 0, bag_size: 13992\n",
      "batch 219, loss: 0.0079, instance_loss: 0.0000, weighted_loss: 0.0055, label: 1, bag_size: 2638\n",
      "batch 239, loss: 0.0825, instance_loss: 0.0765, weighted_loss: 0.0807, label: 1, bag_size: 4789\n",
      "batch 259, loss: 0.2623, instance_loss: 0.5672, weighted_loss: 0.3537, label: 0, bag_size: 5639\n",
      "batch 279, loss: 0.0032, instance_loss: 0.0092, weighted_loss: 0.0050, label: 0, bag_size: 18240\n",
      "batch 299, loss: 0.0472, instance_loss: 0.0638, weighted_loss: 0.0522, label: 0, bag_size: 2654\n",
      "batch 319, loss: 0.0360, instance_loss: 0.0163, weighted_loss: 0.0301, label: 0, bag_size: 11122\n",
      "batch 339, loss: 0.0127, instance_loss: 0.0090, weighted_loss: 0.0116, label: 1, bag_size: 699\n",
      "batch 359, loss: 0.0035, instance_loss: 0.0258, weighted_loss: 0.0102, label: 0, bag_size: 18415\n",
      "batch 379, loss: 0.2069, instance_loss: 0.6545, weighted_loss: 0.3412, label: 1, bag_size: 12719\n",
      "batch 399, loss: 0.0112, instance_loss: 0.0559, weighted_loss: 0.0246, label: 0, bag_size: 11122\n",
      "batch 419, loss: 0.0840, instance_loss: 0.0017, weighted_loss: 0.0593, label: 1, bag_size: 7873\n",
      "batch 439, loss: 0.2745, instance_loss: 0.3545, weighted_loss: 0.2985, label: 0, bag_size: 1052\n",
      "batch 459, loss: 2.3350, instance_loss: 0.3400, weighted_loss: 1.7365, label: 1, bag_size: 2179\n",
      "batch 479, loss: 0.0005, instance_loss: 0.1178, weighted_loss: 0.0357, label: 0, bag_size: 11900\n",
      "batch 499, loss: 0.1769, instance_loss: 0.5916, weighted_loss: 0.3013, label: 0, bag_size: 10410\n",
      "batch 519, loss: 0.0455, instance_loss: 0.0729, weighted_loss: 0.0537, label: 0, bag_size: 23714\n",
      "batch 539, loss: 0.0033, instance_loss: 0.0134, weighted_loss: 0.0063, label: 0, bag_size: 15001\n",
      "batch 559, loss: 0.0144, instance_loss: 0.0029, weighted_loss: 0.0109, label: 1, bag_size: 11421\n",
      "batch 579, loss: 0.0011, instance_loss: 0.0501, weighted_loss: 0.0158, label: 0, bag_size: 18240\n",
      "batch 599, loss: 0.0687, instance_loss: 0.0116, weighted_loss: 0.0516, label: 1, bag_size: 8602\n",
      "batch 619, loss: 0.6630, instance_loss: 0.0914, weighted_loss: 0.4915, label: 0, bag_size: 12510\n",
      "batch 639, loss: 0.0491, instance_loss: 0.0217, weighted_loss: 0.0409, label: 0, bag_size: 3321\n",
      "batch 659, loss: 0.7174, instance_loss: 3.4219, weighted_loss: 1.5288, label: 1, bag_size: 9404\n",
      "batch 679, loss: 0.0186, instance_loss: 0.1555, weighted_loss: 0.0597, label: 0, bag_size: 5999\n",
      "batch 699, loss: 0.3509, instance_loss: 0.6363, weighted_loss: 0.4366, label: 0, bag_size: 10381\n",
      "batch 719, loss: 0.5437, instance_loss: 1.1769, weighted_loss: 0.7337, label: 1, bag_size: 1038\n",
      "batch 739, loss: 0.1316, instance_loss: 0.4254, weighted_loss: 0.2198, label: 1, bag_size: 10848\n",
      "batch 759, loss: 0.0935, instance_loss: 0.0352, weighted_loss: 0.0760, label: 0, bag_size: 7381\n",
      "batch 779, loss: 3.6266, instance_loss: 0.9828, weighted_loss: 2.8335, label: 1, bag_size: 898\n",
      "batch 799, loss: 0.0336, instance_loss: 0.0118, weighted_loss: 0.0270, label: 0, bag_size: 11917\n",
      "batch 819, loss: 0.0072, instance_loss: 0.0788, weighted_loss: 0.0287, label: 0, bag_size: 1149\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.973094512195122: correct 12767/13120\n",
      "class 1 clustering acc 0.8830792682926829: correct 5793/6560\n",
      "Epoch: 23, train_loss: 0.2125, train_clustering_loss:  0.2421, train_error: 0.0780\n",
      "class 0: acc 0.9172413793103448, correct 399/435\n",
      "class 1: acc 0.9272727272727272, correct 357/385\n",
      "\n",
      "Val Set, val_loss: 0.3348, val_error: 0.1364, auc: 0.9688\n",
      "class 0 clustering acc 0.9375: correct 1650/1760\n",
      "class 1 clustering acc 0.8306818181818182: correct 731/880\n",
      "class 0: acc 1.0, correct 52/52\n",
      "class 1: acc 0.7413793103448276, correct 43/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0069, instance_loss: 0.0222, weighted_loss: 0.0115, label: 0, bag_size: 11199\n",
      "batch 39, loss: 1.0395, instance_loss: 1.1518, weighted_loss: 1.0732, label: 1, bag_size: 1867\n",
      "batch 59, loss: 0.6327, instance_loss: 0.1155, weighted_loss: 0.4776, label: 0, bag_size: 3654\n",
      "batch 79, loss: 0.1552, instance_loss: 0.1458, weighted_loss: 0.1523, label: 1, bag_size: 2695\n",
      "batch 99, loss: 0.0110, instance_loss: 0.0037, weighted_loss: 0.0088, label: 0, bag_size: 9252\n",
      "batch 119, loss: 0.5436, instance_loss: 0.0324, weighted_loss: 0.3902, label: 1, bag_size: 12714\n",
      "batch 139, loss: 0.0020, instance_loss: 0.0154, weighted_loss: 0.0060, label: 0, bag_size: 8948\n",
      "batch 159, loss: 0.2062, instance_loss: 0.5720, weighted_loss: 0.3159, label: 1, bag_size: 3674\n",
      "batch 179, loss: 0.3439, instance_loss: 0.6179, weighted_loss: 0.4261, label: 0, bag_size: 6884\n",
      "batch 199, loss: 0.0021, instance_loss: 0.5127, weighted_loss: 0.1552, label: 1, bag_size: 6950\n",
      "batch 219, loss: 0.0032, instance_loss: 0.0000, weighted_loss: 0.0022, label: 1, bag_size: 10867\n",
      "batch 239, loss: 0.0080, instance_loss: 0.0712, weighted_loss: 0.0270, label: 1, bag_size: 10028\n",
      "batch 259, loss: 0.1641, instance_loss: 0.3365, weighted_loss: 0.2158, label: 0, bag_size: 4959\n",
      "batch 279, loss: 0.3968, instance_loss: 0.0295, weighted_loss: 0.2866, label: 1, bag_size: 13089\n",
      "batch 299, loss: 0.4112, instance_loss: 0.6547, weighted_loss: 0.4842, label: 0, bag_size: 5639\n",
      "batch 319, loss: 0.0200, instance_loss: 0.0012, weighted_loss: 0.0143, label: 1, bag_size: 6731\n",
      "batch 339, loss: 0.0944, instance_loss: 0.0157, weighted_loss: 0.0708, label: 0, bag_size: 11390\n",
      "batch 359, loss: 0.4653, instance_loss: 0.3625, weighted_loss: 0.4344, label: 0, bag_size: 11151\n",
      "batch 379, loss: 0.0100, instance_loss: 0.0210, weighted_loss: 0.0133, label: 0, bag_size: 12687\n",
      "batch 399, loss: 0.0337, instance_loss: 0.0362, weighted_loss: 0.0344, label: 0, bag_size: 9930\n",
      "batch 419, loss: 0.0185, instance_loss: 0.0371, weighted_loss: 0.0241, label: 0, bag_size: 11759\n",
      "batch 439, loss: 0.0210, instance_loss: 0.0094, weighted_loss: 0.0175, label: 0, bag_size: 4465\n",
      "batch 459, loss: 0.0551, instance_loss: 0.0166, weighted_loss: 0.0435, label: 1, bag_size: 3652\n",
      "batch 479, loss: 0.5817, instance_loss: 0.4269, weighted_loss: 0.5353, label: 1, bag_size: 9162\n",
      "batch 499, loss: 0.7409, instance_loss: 0.1656, weighted_loss: 0.5683, label: 0, bag_size: 18738\n",
      "batch 519, loss: 0.0041, instance_loss: 0.0111, weighted_loss: 0.0062, label: 0, bag_size: 13892\n",
      "batch 539, loss: 0.0196, instance_loss: 0.0004, weighted_loss: 0.0138, label: 1, bag_size: 5561\n",
      "batch 559, loss: 0.0018, instance_loss: 0.2505, weighted_loss: 0.0764, label: 1, bag_size: 9955\n",
      "batch 579, loss: 0.1906, instance_loss: 0.2434, weighted_loss: 0.2064, label: 0, bag_size: 21319\n",
      "batch 599, loss: 1.0129, instance_loss: 0.3814, weighted_loss: 0.8235, label: 0, bag_size: 11306\n",
      "batch 619, loss: 0.3380, instance_loss: 0.0604, weighted_loss: 0.2547, label: 1, bag_size: 1759\n",
      "batch 639, loss: 0.0211, instance_loss: 0.0259, weighted_loss: 0.0225, label: 1, bag_size: 6606\n",
      "batch 659, loss: 0.0027, instance_loss: 0.0022, weighted_loss: 0.0025, label: 1, bag_size: 8448\n",
      "batch 679, loss: 0.0511, instance_loss: 0.0121, weighted_loss: 0.0394, label: 0, bag_size: 4902\n",
      "batch 699, loss: 4.2030, instance_loss: 2.7343, weighted_loss: 3.7623, label: 1, bag_size: 3879\n",
      "batch 719, loss: 0.2441, instance_loss: 1.1051, weighted_loss: 0.5024, label: 0, bag_size: 2814\n",
      "batch 739, loss: 2.1947, instance_loss: 5.2197, weighted_loss: 3.1022, label: 0, bag_size: 17279\n",
      "batch 759, loss: 0.0028, instance_loss: 0.0134, weighted_loss: 0.0059, label: 1, bag_size: 9610\n",
      "batch 779, loss: 0.0303, instance_loss: 0.0676, weighted_loss: 0.0415, label: 0, bag_size: 12201\n",
      "batch 799, loss: 0.0127, instance_loss: 0.0096, weighted_loss: 0.0117, label: 1, bag_size: 10920\n",
      "batch 819, loss: 0.2047, instance_loss: 0.0389, weighted_loss: 0.1549, label: 1, bag_size: 1924\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9701981707317073: correct 12729/13120\n",
      "class 1 clustering acc 0.8448170731707317: correct 5542/6560\n",
      "Epoch: 24, train_loss: 0.2733, train_clustering_loss:  0.3014, train_error: 0.1012\n",
      "class 0: acc 0.9113300492610837, correct 370/406\n",
      "class 1: acc 0.8864734299516909, correct 367/414\n",
      "\n",
      "Val Set, val_loss: 0.2230, val_error: 0.0909, auc: 0.9705\n",
      "class 0 clustering acc 0.9494318181818182: correct 1671/1760\n",
      "class 1 clustering acc 0.8545454545454545: correct 752/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "Validation loss decreased (0.250451 --> 0.223022).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0089, instance_loss: 0.0084, weighted_loss: 0.0087, label: 1, bag_size: 12795\n",
      "batch 39, loss: 0.1046, instance_loss: 0.0295, weighted_loss: 0.0821, label: 1, bag_size: 10498\n",
      "batch 59, loss: 0.0029, instance_loss: 0.0048, weighted_loss: 0.0035, label: 0, bag_size: 18415\n",
      "batch 79, loss: 0.0057, instance_loss: 0.0000, weighted_loss: 0.0040, label: 1, bag_size: 5731\n",
      "batch 99, loss: 0.4963, instance_loss: 1.6467, weighted_loss: 0.8414, label: 1, bag_size: 1242\n",
      "batch 119, loss: 0.0233, instance_loss: 0.0208, weighted_loss: 0.0226, label: 0, bag_size: 5999\n",
      "batch 139, loss: 0.4848, instance_loss: 0.0470, weighted_loss: 0.3535, label: 0, bag_size: 12510\n",
      "batch 159, loss: 0.1477, instance_loss: 0.0533, weighted_loss: 0.1194, label: 0, bag_size: 2760\n",
      "batch 179, loss: 0.0254, instance_loss: 0.0008, weighted_loss: 0.0180, label: 1, bag_size: 10432\n",
      "batch 199, loss: 0.5551, instance_loss: 0.8446, weighted_loss: 0.6419, label: 0, bag_size: 13332\n",
      "batch 219, loss: 0.0348, instance_loss: 0.0096, weighted_loss: 0.0272, label: 0, bag_size: 2195\n",
      "batch 239, loss: 0.0936, instance_loss: 0.4546, weighted_loss: 0.2019, label: 0, bag_size: 1438\n",
      "batch 259, loss: 0.0013, instance_loss: 0.0190, weighted_loss: 0.0067, label: 1, bag_size: 8410\n",
      "batch 279, loss: 1.1430, instance_loss: 0.8344, weighted_loss: 1.0504, label: 0, bag_size: 1701\n",
      "batch 299, loss: 0.0115, instance_loss: 0.0000, weighted_loss: 0.0081, label: 0, bag_size: 15967\n",
      "batch 319, loss: 0.0219, instance_loss: 0.0101, weighted_loss: 0.0184, label: 1, bag_size: 6745\n",
      "batch 339, loss: 0.2522, instance_loss: 0.2816, weighted_loss: 0.2610, label: 1, bag_size: 1823\n",
      "batch 359, loss: 0.0335, instance_loss: 0.1988, weighted_loss: 0.0831, label: 1, bag_size: 4054\n",
      "batch 379, loss: 0.0229, instance_loss: 0.0001, weighted_loss: 0.0160, label: 1, bag_size: 7217\n",
      "batch 399, loss: 0.0313, instance_loss: 0.0022, weighted_loss: 0.0226, label: 0, bag_size: 2043\n",
      "batch 419, loss: 0.0154, instance_loss: 0.4274, weighted_loss: 0.1390, label: 0, bag_size: 2534\n",
      "batch 439, loss: 0.0128, instance_loss: 0.0022, weighted_loss: 0.0096, label: 1, bag_size: 22286\n",
      "batch 459, loss: 0.0449, instance_loss: 0.0026, weighted_loss: 0.0322, label: 0, bag_size: 22800\n",
      "batch 479, loss: 1.3966, instance_loss: 0.8935, weighted_loss: 1.2457, label: 0, bag_size: 2242\n",
      "batch 499, loss: 0.0061, instance_loss: 0.0036, weighted_loss: 0.0053, label: 0, bag_size: 1415\n",
      "batch 519, loss: 0.0268, instance_loss: 0.1317, weighted_loss: 0.0583, label: 1, bag_size: 2412\n",
      "batch 539, loss: 0.1676, instance_loss: 0.2433, weighted_loss: 0.1903, label: 0, bag_size: 9069\n",
      "batch 559, loss: 1.1243, instance_loss: 0.0043, weighted_loss: 0.7883, label: 1, bag_size: 9162\n",
      "batch 579, loss: 0.0254, instance_loss: 0.0030, weighted_loss: 0.0187, label: 0, bag_size: 2322\n",
      "batch 599, loss: 0.9205, instance_loss: 0.0016, weighted_loss: 0.6448, label: 1, bag_size: 8982\n",
      "batch 619, loss: 0.0015, instance_loss: 0.0418, weighted_loss: 0.0136, label: 0, bag_size: 890\n",
      "batch 639, loss: 0.1080, instance_loss: 0.0031, weighted_loss: 0.0765, label: 1, bag_size: 10501\n",
      "batch 659, loss: 0.0270, instance_loss: 0.0158, weighted_loss: 0.0236, label: 0, bag_size: 2920\n",
      "batch 679, loss: 0.0755, instance_loss: 0.0553, weighted_loss: 0.0695, label: 1, bag_size: 8012\n",
      "batch 699, loss: 0.2004, instance_loss: 0.0427, weighted_loss: 0.1531, label: 0, bag_size: 11390\n",
      "batch 719, loss: 3.9392, instance_loss: 1.3661, weighted_loss: 3.1673, label: 1, bag_size: 2565\n",
      "batch 739, loss: 0.0174, instance_loss: 0.0454, weighted_loss: 0.0258, label: 1, bag_size: 16267\n",
      "batch 759, loss: 0.2223, instance_loss: 0.4206, weighted_loss: 0.2818, label: 0, bag_size: 2063\n",
      "batch 779, loss: 0.0952, instance_loss: 0.0000, weighted_loss: 0.0666, label: 1, bag_size: 18161\n",
      "batch 799, loss: 0.0243, instance_loss: 0.0050, weighted_loss: 0.0185, label: 0, bag_size: 23368\n",
      "batch 819, loss: 0.0319, instance_loss: 0.2772, weighted_loss: 0.1055, label: 0, bag_size: 1458\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9724085365853659: correct 12758/13120\n",
      "class 1 clustering acc 0.8713414634146341: correct 5716/6560\n",
      "Epoch: 25, train_loss: 0.2630, train_clustering_loss:  0.2676, train_error: 0.0939\n",
      "class 0: acc 0.9155251141552512, correct 401/438\n",
      "class 1: acc 0.8952879581151832, correct 342/382\n",
      "\n",
      "Val Set, val_loss: 0.2355, val_error: 0.1000, auc: 0.9738\n",
      "class 0 clustering acc 0.977840909090909: correct 1721/1760\n",
      "class 1 clustering acc 0.8306818181818182: correct 731/880\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0239, instance_loss: 0.0089, weighted_loss: 0.0194, label: 0, bag_size: 12687\n",
      "batch 39, loss: 0.1092, instance_loss: 0.0329, weighted_loss: 0.0863, label: 1, bag_size: 2495\n",
      "batch 59, loss: 0.0604, instance_loss: 0.3002, weighted_loss: 0.1323, label: 0, bag_size: 1483\n",
      "batch 79, loss: 0.0324, instance_loss: 0.2857, weighted_loss: 0.1084, label: 1, bag_size: 621\n",
      "batch 99, loss: 0.0038, instance_loss: 0.1149, weighted_loss: 0.0371, label: 1, bag_size: 5864\n",
      "batch 119, loss: 0.0090, instance_loss: 0.1084, weighted_loss: 0.0388, label: 0, bag_size: 1560\n",
      "batch 139, loss: 0.2720, instance_loss: 0.4903, weighted_loss: 0.3375, label: 1, bag_size: 5256\n",
      "batch 159, loss: 0.0022, instance_loss: 0.0034, weighted_loss: 0.0025, label: 0, bag_size: 22800\n",
      "batch 179, loss: 0.0182, instance_loss: 0.1092, weighted_loss: 0.0455, label: 0, bag_size: 4345\n",
      "batch 199, loss: 0.3211, instance_loss: 0.3074, weighted_loss: 0.3170, label: 0, bag_size: 6281\n",
      "batch 219, loss: 0.0191, instance_loss: 0.0003, weighted_loss: 0.0134, label: 1, bag_size: 13732\n",
      "batch 239, loss: 0.0365, instance_loss: 0.0032, weighted_loss: 0.0265, label: 1, bag_size: 4239\n",
      "batch 259, loss: 0.1786, instance_loss: 0.1934, weighted_loss: 0.1830, label: 1, bag_size: 2278\n",
      "batch 279, loss: 0.0397, instance_loss: 0.0087, weighted_loss: 0.0304, label: 0, bag_size: 11194\n",
      "batch 299, loss: 0.1373, instance_loss: 0.3921, weighted_loss: 0.2137, label: 1, bag_size: 4786\n",
      "batch 319, loss: 0.0019, instance_loss: 0.0000, weighted_loss: 0.0013, label: 1, bag_size: 15332\n",
      "batch 339, loss: 0.0010, instance_loss: 0.0030, weighted_loss: 0.0016, label: 0, bag_size: 23037\n",
      "batch 359, loss: 0.0806, instance_loss: 2.5038, weighted_loss: 0.8075, label: 0, bag_size: 1149\n",
      "batch 379, loss: 0.5070, instance_loss: 0.5839, weighted_loss: 0.5301, label: 0, bag_size: 3670\n",
      "batch 399, loss: 0.1368, instance_loss: 0.3613, weighted_loss: 0.2041, label: 0, bag_size: 3321\n",
      "batch 419, loss: 0.0016, instance_loss: 0.0020, weighted_loss: 0.0017, label: 0, bag_size: 23996\n",
      "batch 439, loss: 0.5289, instance_loss: 0.2393, weighted_loss: 0.4420, label: 1, bag_size: 10492\n",
      "batch 459, loss: 0.0461, instance_loss: 0.5734, weighted_loss: 0.2043, label: 0, bag_size: 7823\n",
      "batch 479, loss: 0.0758, instance_loss: 0.2389, weighted_loss: 0.1248, label: 1, bag_size: 22264\n",
      "batch 499, loss: 0.0416, instance_loss: 0.6442, weighted_loss: 0.2224, label: 0, bag_size: 1483\n",
      "batch 519, loss: 0.0021, instance_loss: 0.6110, weighted_loss: 0.1848, label: 1, bag_size: 8019\n",
      "batch 539, loss: 0.0068, instance_loss: 0.0031, weighted_loss: 0.0057, label: 0, bag_size: 15636\n",
      "batch 559, loss: 0.4297, instance_loss: 1.1094, weighted_loss: 0.6336, label: 1, bag_size: 1920\n",
      "batch 579, loss: 2.9013, instance_loss: 0.2739, weighted_loss: 2.1131, label: 0, bag_size: 2815\n",
      "batch 599, loss: 0.0688, instance_loss: 0.6010, weighted_loss: 0.2285, label: 0, bag_size: 18777\n",
      "batch 619, loss: 0.0744, instance_loss: 0.0653, weighted_loss: 0.0717, label: 0, bag_size: 2534\n",
      "batch 639, loss: 0.0032, instance_loss: 0.0013, weighted_loss: 0.0027, label: 0, bag_size: 11512\n",
      "batch 659, loss: 0.0203, instance_loss: 0.0256, weighted_loss: 0.0219, label: 1, bag_size: 9230\n",
      "batch 679, loss: 0.0459, instance_loss: 0.0514, weighted_loss: 0.0476, label: 0, bag_size: 1452\n",
      "batch 699, loss: 0.0038, instance_loss: 0.0403, weighted_loss: 0.0147, label: 0, bag_size: 12217\n",
      "batch 719, loss: 0.0109, instance_loss: 0.0509, weighted_loss: 0.0229, label: 1, bag_size: 25970\n",
      "batch 739, loss: 0.0104, instance_loss: 0.8293, weighted_loss: 0.2561, label: 1, bag_size: 1888\n",
      "batch 759, loss: 0.0479, instance_loss: 1.0977, weighted_loss: 0.3628, label: 1, bag_size: 7351\n",
      "batch 779, loss: 0.0809, instance_loss: 0.6066, weighted_loss: 0.2386, label: 0, bag_size: 763\n",
      "batch 799, loss: 0.0364, instance_loss: 0.1675, weighted_loss: 0.0757, label: 1, bag_size: 9877\n",
      "batch 819, loss: 0.0045, instance_loss: 0.0819, weighted_loss: 0.0277, label: 0, bag_size: 22426\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9569359756097561: correct 12555/13120\n",
      "class 1 clustering acc 0.7797256097560976: correct 5115/6560\n",
      "Epoch: 26, train_loss: 0.2050, train_clustering_loss:  0.4229, train_error: 0.0720\n",
      "class 0: acc 0.9402298850574713, correct 409/435\n",
      "class 1: acc 0.9142857142857143, correct 352/385\n",
      "\n",
      "Val Set, val_loss: 0.2092, val_error: 0.0909, auc: 0.9748\n",
      "class 0 clustering acc 0.9073863636363636: correct 1597/1760\n",
      "class 1 clustering acc 0.7477272727272727: correct 658/880\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 0.896551724137931, correct 52/58\n",
      "Validation loss decreased (0.223022 --> 0.209245).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0530, instance_loss: 0.0044, weighted_loss: 0.0384, label: 0, bag_size: 17155\n",
      "batch 39, loss: 0.0124, instance_loss: 0.2171, weighted_loss: 0.0738, label: 1, bag_size: 2136\n",
      "batch 59, loss: 0.0099, instance_loss: 0.2542, weighted_loss: 0.0832, label: 0, bag_size: 2748\n",
      "batch 79, loss: 0.0176, instance_loss: 0.1561, weighted_loss: 0.0591, label: 1, bag_size: 12408\n",
      "batch 99, loss: 0.0082, instance_loss: 0.2329, weighted_loss: 0.0756, label: 0, bag_size: 3970\n",
      "batch 119, loss: 0.0007, instance_loss: 0.0821, weighted_loss: 0.0251, label: 0, bag_size: 19390\n",
      "batch 139, loss: 0.0386, instance_loss: 0.0467, weighted_loss: 0.0410, label: 0, bag_size: 10128\n",
      "batch 159, loss: 0.0527, instance_loss: 0.1545, weighted_loss: 0.0832, label: 0, bag_size: 2367\n",
      "batch 179, loss: 0.0028, instance_loss: 0.1355, weighted_loss: 0.0426, label: 0, bag_size: 2628\n",
      "batch 199, loss: 0.0284, instance_loss: 0.5876, weighted_loss: 0.1962, label: 0, bag_size: 8744\n",
      "batch 219, loss: 0.6118, instance_loss: 0.8610, weighted_loss: 0.6865, label: 1, bag_size: 5160\n",
      "batch 239, loss: 0.1431, instance_loss: 0.0575, weighted_loss: 0.1174, label: 1, bag_size: 6736\n",
      "batch 259, loss: 0.0051, instance_loss: 0.0261, weighted_loss: 0.0114, label: 1, bag_size: 4862\n",
      "batch 279, loss: 0.4166, instance_loss: 0.0867, weighted_loss: 0.3176, label: 1, bag_size: 1493\n",
      "batch 299, loss: 0.2654, instance_loss: 0.6990, weighted_loss: 0.3955, label: 1, bag_size: 7989\n",
      "batch 319, loss: 0.0004, instance_loss: 0.0081, weighted_loss: 0.0027, label: 1, bag_size: 11195\n",
      "batch 339, loss: 0.1631, instance_loss: 0.0473, weighted_loss: 0.1284, label: 1, bag_size: 1638\n",
      "batch 359, loss: 0.0110, instance_loss: 0.0943, weighted_loss: 0.0360, label: 1, bag_size: 5991\n",
      "batch 379, loss: 0.2366, instance_loss: 0.6558, weighted_loss: 0.3623, label: 1, bag_size: 10848\n",
      "batch 399, loss: 0.8686, instance_loss: 1.7097, weighted_loss: 1.1209, label: 1, bag_size: 5366\n",
      "batch 419, loss: 0.8972, instance_loss: 0.6870, weighted_loss: 0.8341, label: 1, bag_size: 1493\n",
      "batch 439, loss: 1.0620, instance_loss: 1.1188, weighted_loss: 1.0790, label: 1, bag_size: 6360\n",
      "batch 459, loss: 0.0172, instance_loss: 0.1021, weighted_loss: 0.0427, label: 0, bag_size: 18045\n",
      "batch 479, loss: 3.3868, instance_loss: 3.4186, weighted_loss: 3.3963, label: 0, bag_size: 4692\n",
      "batch 499, loss: 0.0346, instance_loss: 0.1186, weighted_loss: 0.0598, label: 1, bag_size: 10105\n",
      "batch 519, loss: 0.0188, instance_loss: 0.0385, weighted_loss: 0.0247, label: 0, bag_size: 2360\n",
      "batch 539, loss: 0.1182, instance_loss: 0.2274, weighted_loss: 0.1510, label: 0, bag_size: 1370\n",
      "batch 559, loss: 0.3026, instance_loss: 1.1370, weighted_loss: 0.5529, label: 0, bag_size: 3893\n",
      "batch 579, loss: 0.0112, instance_loss: 0.0150, weighted_loss: 0.0124, label: 0, bag_size: 2360\n",
      "batch 599, loss: 1.5020, instance_loss: 0.8471, weighted_loss: 1.3055, label: 1, bag_size: 9162\n",
      "batch 619, loss: 0.0189, instance_loss: 0.0565, weighted_loss: 0.0302, label: 0, bag_size: 19472\n",
      "batch 639, loss: 0.0437, instance_loss: 0.1563, weighted_loss: 0.0775, label: 1, bag_size: 14887\n",
      "batch 659, loss: 0.1383, instance_loss: 0.0070, weighted_loss: 0.0989, label: 1, bag_size: 16154\n",
      "batch 679, loss: 0.0813, instance_loss: 0.1263, weighted_loss: 0.0948, label: 1, bag_size: 11964\n",
      "batch 699, loss: 0.0041, instance_loss: 0.0439, weighted_loss: 0.0160, label: 0, bag_size: 22828\n",
      "batch 719, loss: 0.0353, instance_loss: 0.0119, weighted_loss: 0.0283, label: 1, bag_size: 7873\n",
      "batch 739, loss: 1.5337, instance_loss: 0.4695, weighted_loss: 1.2144, label: 0, bag_size: 3375\n",
      "batch 759, loss: 0.6235, instance_loss: 1.5630, weighted_loss: 0.9054, label: 1, bag_size: 1230\n",
      "batch 779, loss: 3.2359, instance_loss: 2.3116, weighted_loss: 2.9586, label: 1, bag_size: 684\n",
      "batch 799, loss: 0.0214, instance_loss: 0.1651, weighted_loss: 0.0645, label: 0, bag_size: 22426\n",
      "batch 819, loss: 1.7325, instance_loss: 1.6907, weighted_loss: 1.7200, label: 1, bag_size: 2842\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.958155487804878: correct 12571/13120\n",
      "class 1 clustering acc 0.8094512195121951: correct 5310/6560\n",
      "Epoch: 27, train_loss: 0.2679, train_clustering_loss:  0.3794, train_error: 0.1000\n",
      "class 0: acc 0.9073170731707317, correct 372/410\n",
      "class 1: acc 0.8926829268292683, correct 366/410\n",
      "\n",
      "Val Set, val_loss: 0.2316, val_error: 0.1000, auc: 0.9748\n",
      "class 0 clustering acc 0.9323863636363636: correct 1641/1760\n",
      "class 1 clustering acc 0.8363636363636363: correct 736/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8448275862068966, correct 49/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.4778, instance_loss: 0.9410, weighted_loss: 0.6168, label: 0, bag_size: 3654\n",
      "batch 39, loss: 0.0221, instance_loss: 0.0843, weighted_loss: 0.0408, label: 0, bag_size: 11194\n",
      "batch 59, loss: 0.0045, instance_loss: 0.0053, weighted_loss: 0.0047, label: 1, bag_size: 5731\n",
      "batch 79, loss: 0.0236, instance_loss: 0.1094, weighted_loss: 0.0494, label: 1, bag_size: 699\n",
      "batch 99, loss: 0.1506, instance_loss: 0.0211, weighted_loss: 0.1118, label: 0, bag_size: 2548\n",
      "batch 119, loss: 0.1519, instance_loss: 0.0104, weighted_loss: 0.1095, label: 1, bag_size: 1483\n",
      "batch 139, loss: 0.1547, instance_loss: 0.2935, weighted_loss: 0.1963, label: 0, bag_size: 11390\n",
      "batch 159, loss: 0.6064, instance_loss: 0.3841, weighted_loss: 0.5397, label: 1, bag_size: 2314\n",
      "batch 179, loss: 0.1000, instance_loss: 0.3073, weighted_loss: 0.1622, label: 1, bag_size: 12626\n",
      "batch 199, loss: 0.1562, instance_loss: 0.6977, weighted_loss: 0.3186, label: 1, bag_size: 5561\n",
      "batch 219, loss: 0.0235, instance_loss: 0.0206, weighted_loss: 0.0227, label: 0, bag_size: 2044\n",
      "batch 239, loss: 0.0058, instance_loss: 0.0032, weighted_loss: 0.0050, label: 0, bag_size: 21682\n",
      "batch 259, loss: 0.1452, instance_loss: 0.0641, weighted_loss: 0.1209, label: 0, bag_size: 21093\n",
      "batch 279, loss: 0.0718, instance_loss: 0.5405, weighted_loss: 0.2124, label: 1, bag_size: 20767\n",
      "batch 299, loss: 0.0339, instance_loss: 0.2695, weighted_loss: 0.1046, label: 0, bag_size: 3657\n",
      "batch 319, loss: 0.0014, instance_loss: 0.0221, weighted_loss: 0.0076, label: 0, bag_size: 14266\n",
      "batch 339, loss: 0.0601, instance_loss: 0.0176, weighted_loss: 0.0474, label: 0, bag_size: 8145\n",
      "batch 359, loss: 0.0375, instance_loss: 0.1364, weighted_loss: 0.0672, label: 1, bag_size: 5025\n",
      "batch 379, loss: 0.0697, instance_loss: 0.1422, weighted_loss: 0.0914, label: 0, bag_size: 2654\n",
      "batch 399, loss: 0.1328, instance_loss: 0.0018, weighted_loss: 0.0935, label: 0, bag_size: 6898\n",
      "batch 419, loss: 0.0210, instance_loss: 0.0010, weighted_loss: 0.0150, label: 1, bag_size: 13786\n",
      "batch 439, loss: 0.0127, instance_loss: 0.0096, weighted_loss: 0.0118, label: 1, bag_size: 9478\n",
      "batch 459, loss: 0.1933, instance_loss: 0.0173, weighted_loss: 0.1405, label: 0, bag_size: 20555\n",
      "batch 479, loss: 0.1654, instance_loss: 0.2014, weighted_loss: 0.1762, label: 1, bag_size: 3368\n",
      "batch 499, loss: 0.0439, instance_loss: 0.4513, weighted_loss: 0.1662, label: 1, bag_size: 5231\n",
      "batch 519, loss: 0.0067, instance_loss: 0.0000, weighted_loss: 0.0047, label: 0, bag_size: 5225\n",
      "batch 539, loss: 0.0549, instance_loss: 0.0136, weighted_loss: 0.0425, label: 1, bag_size: 16267\n",
      "batch 559, loss: 0.1099, instance_loss: 0.0503, weighted_loss: 0.0920, label: 0, bag_size: 3089\n",
      "batch 579, loss: 0.1196, instance_loss: 0.4431, weighted_loss: 0.2166, label: 0, bag_size: 4418\n",
      "batch 599, loss: 0.0105, instance_loss: 0.0027, weighted_loss: 0.0081, label: 1, bag_size: 10396\n",
      "batch 619, loss: 0.3359, instance_loss: 1.4161, weighted_loss: 0.6600, label: 1, bag_size: 2314\n",
      "batch 639, loss: 0.0413, instance_loss: 0.1057, weighted_loss: 0.0606, label: 0, bag_size: 12732\n",
      "batch 659, loss: 0.7456, instance_loss: 1.6893, weighted_loss: 1.0287, label: 1, bag_size: 21450\n",
      "batch 679, loss: 0.0614, instance_loss: 0.2331, weighted_loss: 0.1129, label: 0, bag_size: 11281\n",
      "batch 699, loss: 0.0082, instance_loss: 0.0010, weighted_loss: 0.0060, label: 1, bag_size: 9078\n",
      "batch 719, loss: 0.0098, instance_loss: 0.0209, weighted_loss: 0.0131, label: 1, bag_size: 6875\n",
      "batch 739, loss: 0.0023, instance_loss: 0.0355, weighted_loss: 0.0123, label: 0, bag_size: 2179\n",
      "batch 759, loss: 0.0626, instance_loss: 0.0795, weighted_loss: 0.0676, label: 1, bag_size: 2495\n",
      "batch 779, loss: 0.0012, instance_loss: 0.0129, weighted_loss: 0.0047, label: 0, bag_size: 23398\n",
      "batch 799, loss: 0.8797, instance_loss: 0.0234, weighted_loss: 0.6228, label: 1, bag_size: 8680\n",
      "batch 819, loss: 0.0131, instance_loss: 0.0009, weighted_loss: 0.0094, label: 0, bag_size: 19472\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9668445121951219: correct 12685/13120\n",
      "class 1 clustering acc 0.8644817073170732: correct 5671/6560\n",
      "Epoch: 28, train_loss: 0.2384, train_clustering_loss:  0.2814, train_error: 0.0939\n",
      "class 0: acc 0.90625, correct 377/416\n",
      "class 1: acc 0.905940594059406, correct 366/404\n",
      "\n",
      "Val Set, val_loss: 0.3141, val_error: 0.1545, auc: 0.9751\n",
      "class 0 clustering acc 0.9403409090909091: correct 1655/1760\n",
      "class 1 clustering acc 0.8340909090909091: correct 734/880\n",
      "class 0: acc 0.7115384615384616, correct 37/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0631, instance_loss: 0.0000, weighted_loss: 0.0442, label: 0, bag_size: 21138\n",
      "batch 39, loss: 0.0372, instance_loss: 0.1069, weighted_loss: 0.0581, label: 1, bag_size: 1512\n",
      "batch 59, loss: 0.0411, instance_loss: 0.0056, weighted_loss: 0.0305, label: 0, bag_size: 10791\n",
      "batch 79, loss: 0.0433, instance_loss: 0.0032, weighted_loss: 0.0313, label: 0, bag_size: 2367\n",
      "batch 99, loss: 0.0372, instance_loss: 0.0013, weighted_loss: 0.0265, label: 0, bag_size: 1072\n",
      "batch 119, loss: 0.0874, instance_loss: 0.7671, weighted_loss: 0.2913, label: 0, bag_size: 763\n",
      "batch 139, loss: 0.7081, instance_loss: 3.8203, weighted_loss: 1.6417, label: 1, bag_size: 13440\n",
      "batch 159, loss: 0.0023, instance_loss: 0.1690, weighted_loss: 0.0523, label: 1, bag_size: 7767\n",
      "batch 179, loss: 1.1379, instance_loss: 2.7528, weighted_loss: 1.6224, label: 0, bag_size: 23618\n",
      "batch 199, loss: 0.4270, instance_loss: 0.1147, weighted_loss: 0.3333, label: 0, bag_size: 1732\n",
      "batch 219, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 7191\n",
      "batch 239, loss: 0.0020, instance_loss: 0.0003, weighted_loss: 0.0015, label: 0, bag_size: 9471\n",
      "batch 259, loss: 0.0778, instance_loss: 0.1106, weighted_loss: 0.0876, label: 0, bag_size: 13619\n",
      "batch 279, loss: 0.0048, instance_loss: 0.5611, weighted_loss: 0.1717, label: 1, bag_size: 10112\n",
      "batch 299, loss: 0.0154, instance_loss: 0.0006, weighted_loss: 0.0110, label: 0, bag_size: 6851\n",
      "batch 319, loss: 0.0210, instance_loss: 0.0141, weighted_loss: 0.0189, label: 0, bag_size: 16211\n",
      "batch 339, loss: 0.2635, instance_loss: 0.1417, weighted_loss: 0.2269, label: 1, bag_size: 2678\n",
      "batch 359, loss: 0.0137, instance_loss: 0.0000, weighted_loss: 0.0096, label: 0, bag_size: 9485\n",
      "batch 379, loss: 0.0843, instance_loss: 0.2276, weighted_loss: 0.1273, label: 1, bag_size: 928\n",
      "batch 399, loss: 0.0032, instance_loss: 0.0036, weighted_loss: 0.0033, label: 1, bag_size: 6090\n",
      "batch 419, loss: 0.5620, instance_loss: 0.0617, weighted_loss: 0.4119, label: 1, bag_size: 1493\n",
      "batch 439, loss: 0.0425, instance_loss: 0.0392, weighted_loss: 0.0415, label: 1, bag_size: 5605\n",
      "batch 459, loss: 1.2400, instance_loss: 0.2967, weighted_loss: 0.9570, label: 1, bag_size: 12180\n",
      "batch 479, loss: 0.1114, instance_loss: 0.0217, weighted_loss: 0.0845, label: 1, bag_size: 9321\n",
      "batch 499, loss: 0.0366, instance_loss: 0.0010, weighted_loss: 0.0259, label: 0, bag_size: 1072\n",
      "batch 519, loss: 0.6521, instance_loss: 0.0615, weighted_loss: 0.4749, label: 0, bag_size: 11212\n",
      "batch 539, loss: 0.0228, instance_loss: 0.0010, weighted_loss: 0.0162, label: 0, bag_size: 2920\n",
      "batch 559, loss: 3.7231, instance_loss: 3.6026, weighted_loss: 3.6870, label: 0, bag_size: 4692\n",
      "batch 579, loss: 0.1156, instance_loss: 0.5062, weighted_loss: 0.2328, label: 0, bag_size: 8744\n",
      "batch 599, loss: 0.3732, instance_loss: 0.1206, weighted_loss: 0.2974, label: 0, bag_size: 15672\n",
      "batch 619, loss: 0.0169, instance_loss: 0.0024, weighted_loss: 0.0125, label: 1, bag_size: 3968\n",
      "batch 639, loss: 2.1639, instance_loss: 3.8269, weighted_loss: 2.6628, label: 0, bag_size: 2815\n",
      "batch 659, loss: 0.0251, instance_loss: 0.0026, weighted_loss: 0.0184, label: 0, bag_size: 9234\n",
      "batch 679, loss: 0.0191, instance_loss: 0.0000, weighted_loss: 0.0134, label: 0, bag_size: 15967\n",
      "batch 699, loss: 0.0882, instance_loss: 0.0011, weighted_loss: 0.0621, label: 0, bag_size: 12793\n",
      "batch 719, loss: 0.0306, instance_loss: 0.7779, weighted_loss: 0.2548, label: 0, bag_size: 9171\n",
      "batch 739, loss: 0.0030, instance_loss: 0.0015, weighted_loss: 0.0025, label: 1, bag_size: 6453\n",
      "batch 759, loss: 0.7516, instance_loss: 1.7691, weighted_loss: 1.0569, label: 0, bag_size: 24382\n",
      "batch 779, loss: 0.0067, instance_loss: 0.0000, weighted_loss: 0.0047, label: 0, bag_size: 12524\n",
      "batch 799, loss: 0.0114, instance_loss: 0.0131, weighted_loss: 0.0119, label: 1, bag_size: 13174\n",
      "batch 819, loss: 0.1210, instance_loss: 0.1121, weighted_loss: 0.1183, label: 1, bag_size: 1015\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9689786585365854: correct 12713/13120\n",
      "class 1 clustering acc 0.8339939024390244: correct 5471/6560\n",
      "Epoch: 29, train_loss: 0.2459, train_clustering_loss:  0.2876, train_error: 0.1000\n",
      "class 0: acc 0.900709219858156, correct 381/423\n",
      "class 1: acc 0.8992443324937027, correct 357/397\n",
      "\n",
      "Val Set, val_loss: 0.2286, val_error: 0.1000, auc: 0.9771\n",
      "class 0 clustering acc 0.9130681818181818: correct 1607/1760\n",
      "class 1 clustering acc 0.7284090909090909: correct 641/880\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0036, instance_loss: 0.0027, weighted_loss: 0.0033, label: 1, bag_size: 14779\n",
      "batch 39, loss: 0.0033, instance_loss: 0.0193, weighted_loss: 0.0081, label: 0, bag_size: 8252\n",
      "batch 59, loss: 0.0457, instance_loss: 0.0339, weighted_loss: 0.0422, label: 1, bag_size: 6205\n",
      "batch 79, loss: 0.0103, instance_loss: 0.0830, weighted_loss: 0.0321, label: 1, bag_size: 7513\n",
      "batch 99, loss: 0.0284, instance_loss: 0.0341, weighted_loss: 0.0301, label: 0, bag_size: 1202\n",
      "batch 119, loss: 0.0028, instance_loss: 0.0519, weighted_loss: 0.0176, label: 0, bag_size: 2424\n",
      "batch 139, loss: 0.0007, instance_loss: 0.0106, weighted_loss: 0.0037, label: 1, bag_size: 6792\n",
      "batch 159, loss: 0.1070, instance_loss: 0.0179, weighted_loss: 0.0803, label: 0, bag_size: 2548\n",
      "batch 179, loss: 0.1919, instance_loss: 0.0326, weighted_loss: 0.1441, label: 1, bag_size: 5366\n",
      "batch 199, loss: 0.2259, instance_loss: 0.1647, weighted_loss: 0.2075, label: 1, bag_size: 11220\n",
      "batch 219, loss: 0.1460, instance_loss: 0.2362, weighted_loss: 0.1731, label: 1, bag_size: 12719\n",
      "batch 239, loss: 0.5634, instance_loss: 0.1010, weighted_loss: 0.4247, label: 0, bag_size: 1814\n",
      "batch 259, loss: 0.0188, instance_loss: 0.0010, weighted_loss: 0.0135, label: 1, bag_size: 8602\n",
      "batch 279, loss: 0.0085, instance_loss: 0.0000, weighted_loss: 0.0059, label: 0, bag_size: 18045\n",
      "batch 299, loss: 0.2553, instance_loss: 0.0357, weighted_loss: 0.1894, label: 0, bag_size: 1814\n",
      "batch 319, loss: 0.3435, instance_loss: 0.3960, weighted_loss: 0.3593, label: 1, bag_size: 16548\n",
      "batch 339, loss: 0.1567, instance_loss: 0.0140, weighted_loss: 0.1139, label: 0, bag_size: 7381\n",
      "batch 359, loss: 0.2900, instance_loss: 0.0589, weighted_loss: 0.2206, label: 0, bag_size: 2918\n",
      "batch 379, loss: 0.0031, instance_loss: 0.0025, weighted_loss: 0.0029, label: 1, bag_size: 6164\n",
      "batch 399, loss: 0.0099, instance_loss: 0.0000, weighted_loss: 0.0070, label: 0, bag_size: 19472\n",
      "batch 419, loss: 0.0082, instance_loss: 0.0187, weighted_loss: 0.0114, label: 1, bag_size: 12931\n",
      "batch 439, loss: 0.0204, instance_loss: 0.0599, weighted_loss: 0.0323, label: 1, bag_size: 9321\n",
      "batch 459, loss: 0.2283, instance_loss: 0.1265, weighted_loss: 0.1978, label: 0, bag_size: 4497\n",
      "batch 479, loss: 0.1980, instance_loss: 0.7602, weighted_loss: 0.3666, label: 1, bag_size: 1015\n",
      "batch 499, loss: 0.0716, instance_loss: 0.0068, weighted_loss: 0.0522, label: 1, bag_size: 10622\n",
      "batch 519, loss: 0.0013, instance_loss: 0.5223, weighted_loss: 0.1576, label: 1, bag_size: 9732\n",
      "batch 539, loss: 0.0065, instance_loss: 0.0316, weighted_loss: 0.0140, label: 1, bag_size: 5833\n",
      "batch 559, loss: 0.2398, instance_loss: 1.3286, weighted_loss: 0.5665, label: 0, bag_size: 1714\n",
      "batch 579, loss: 0.1013, instance_loss: 0.0004, weighted_loss: 0.0710, label: 0, bag_size: 15672\n",
      "batch 599, loss: 0.6752, instance_loss: 2.1610, weighted_loss: 1.1210, label: 0, bag_size: 9616\n",
      "batch 619, loss: 0.7865, instance_loss: 0.7287, weighted_loss: 0.7691, label: 1, bag_size: 1444\n",
      "batch 639, loss: 0.8845, instance_loss: 0.1733, weighted_loss: 0.6711, label: 1, bag_size: 3121\n",
      "batch 659, loss: 0.0490, instance_loss: 0.0000, weighted_loss: 0.0343, label: 0, bag_size: 19518\n",
      "batch 679, loss: 0.0084, instance_loss: 0.0001, weighted_loss: 0.0059, label: 1, bag_size: 19932\n",
      "batch 699, loss: 1.3170, instance_loss: 0.1006, weighted_loss: 0.9521, label: 0, bag_size: 18516\n",
      "batch 719, loss: 0.0012, instance_loss: 0.0013, weighted_loss: 0.0012, label: 0, bag_size: 3459\n",
      "batch 739, loss: 0.2824, instance_loss: 0.2246, weighted_loss: 0.2651, label: 1, bag_size: 1512\n",
      "batch 759, loss: 4.4183, instance_loss: 1.7953, weighted_loss: 3.6314, label: 0, bag_size: 3802\n",
      "batch 779, loss: 0.0349, instance_loss: 0.0013, weighted_loss: 0.0248, label: 1, bag_size: 11684\n",
      "batch 799, loss: 0.0132, instance_loss: 0.0000, weighted_loss: 0.0093, label: 0, bag_size: 4271\n",
      "batch 819, loss: 0.0408, instance_loss: 0.0000, weighted_loss: 0.0286, label: 0, bag_size: 11922\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9686737804878048: correct 12709/13120\n",
      "class 1 clustering acc 0.8420731707317073: correct 5524/6560\n",
      "Epoch: 30, train_loss: 0.2661, train_clustering_loss:  0.2882, train_error: 0.0963\n",
      "class 0: acc 0.9064748201438849, correct 378/417\n",
      "class 1: acc 0.9007444168734491, correct 363/403\n",
      "\n",
      "Val Set, val_loss: 0.3010, val_error: 0.1182, auc: 0.9778\n",
      "class 0 clustering acc 0.9210227272727273: correct 1621/1760\n",
      "class 1 clustering acc 0.8079545454545455: correct 711/880\n",
      "class 0: acc 1.0, correct 52/52\n",
      "class 1: acc 0.7758620689655172, correct 45/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0054, instance_loss: 0.0009, weighted_loss: 0.0041, label: 1, bag_size: 10392\n",
      "batch 39, loss: 0.0038, instance_loss: 0.1622, weighted_loss: 0.0513, label: 1, bag_size: 8410\n",
      "batch 59, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 11900\n",
      "batch 79, loss: 0.9883, instance_loss: 0.3529, weighted_loss: 0.7977, label: 1, bag_size: 8026\n",
      "batch 99, loss: 0.1726, instance_loss: 0.3226, weighted_loss: 0.2176, label: 1, bag_size: 5605\n",
      "batch 119, loss: 0.4343, instance_loss: 0.1183, weighted_loss: 0.3395, label: 1, bag_size: 2682\n",
      "batch 139, loss: 0.0299, instance_loss: 0.0000, weighted_loss: 0.0209, label: 0, bag_size: 19470\n",
      "batch 159, loss: 0.1230, instance_loss: 0.0013, weighted_loss: 0.0865, label: 0, bag_size: 14249\n",
      "batch 179, loss: 0.0022, instance_loss: 0.0000, weighted_loss: 0.0015, label: 0, bag_size: 11512\n",
      "batch 199, loss: 0.0531, instance_loss: 0.0046, weighted_loss: 0.0386, label: 0, bag_size: 1962\n",
      "batch 219, loss: 0.6498, instance_loss: 0.0426, weighted_loss: 0.4676, label: 1, bag_size: 5903\n",
      "batch 239, loss: 0.5436, instance_loss: 0.1793, weighted_loss: 0.4343, label: 0, bag_size: 9597\n",
      "batch 259, loss: 1.6223, instance_loss: 1.1601, weighted_loss: 1.4836, label: 1, bag_size: 684\n",
      "batch 279, loss: 4.6349, instance_loss: 2.6274, weighted_loss: 4.0327, label: 0, bag_size: 5105\n",
      "batch 299, loss: 2.7063, instance_loss: 1.3255, weighted_loss: 2.2921, label: 1, bag_size: 684\n",
      "batch 319, loss: 0.3991, instance_loss: 0.0000, weighted_loss: 0.2794, label: 0, bag_size: 5297\n",
      "batch 339, loss: 0.5363, instance_loss: 0.2470, weighted_loss: 0.4495, label: 0, bag_size: 2266\n",
      "batch 359, loss: 0.0764, instance_loss: 0.2173, weighted_loss: 0.1187, label: 1, bag_size: 3409\n",
      "batch 379, loss: 0.5066, instance_loss: 0.2514, weighted_loss: 0.4301, label: 1, bag_size: 1095\n",
      "batch 399, loss: 0.0288, instance_loss: 0.0029, weighted_loss: 0.0211, label: 0, bag_size: 12148\n",
      "batch 419, loss: 0.6042, instance_loss: 0.8981, weighted_loss: 0.6924, label: 0, bag_size: 1831\n",
      "batch 439, loss: 0.0460, instance_loss: 0.0266, weighted_loss: 0.0402, label: 0, bag_size: 12793\n",
      "batch 459, loss: 0.0233, instance_loss: 0.1664, weighted_loss: 0.0662, label: 0, bag_size: 1234\n",
      "batch 479, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 9971\n",
      "batch 499, loss: 0.0581, instance_loss: 0.0425, weighted_loss: 0.0534, label: 1, bag_size: 14681\n",
      "batch 519, loss: 0.2823, instance_loss: 0.0490, weighted_loss: 0.2123, label: 1, bag_size: 8191\n",
      "batch 539, loss: 4.7559, instance_loss: 2.5400, weighted_loss: 4.0912, label: 0, bag_size: 3897\n",
      "batch 559, loss: 0.0259, instance_loss: 0.0229, weighted_loss: 0.0250, label: 1, bag_size: 16267\n",
      "batch 579, loss: 0.0107, instance_loss: 0.1032, weighted_loss: 0.0385, label: 1, bag_size: 4054\n",
      "batch 599, loss: 0.0005, instance_loss: 0.0402, weighted_loss: 0.0124, label: 1, bag_size: 3437\n",
      "batch 619, loss: 0.0223, instance_loss: 0.0002, weighted_loss: 0.0157, label: 0, bag_size: 4271\n",
      "batch 639, loss: 2.5333, instance_loss: 2.4630, weighted_loss: 2.5122, label: 1, bag_size: 15563\n",
      "batch 659, loss: 0.0141, instance_loss: 0.0202, weighted_loss: 0.0159, label: 1, bag_size: 2278\n",
      "batch 679, loss: 0.0122, instance_loss: 0.0005, weighted_loss: 0.0087, label: 0, bag_size: 15001\n",
      "batch 699, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0015, label: 0, bag_size: 19472\n",
      "batch 719, loss: 0.0002, instance_loss: 0.0006, weighted_loss: 0.0003, label: 0, bag_size: 10481\n",
      "batch 739, loss: 0.3216, instance_loss: 0.0808, weighted_loss: 0.2494, label: 0, bag_size: 20555\n",
      "batch 759, loss: 0.1303, instance_loss: 0.1357, weighted_loss: 0.1319, label: 0, bag_size: 7612\n",
      "batch 779, loss: 0.0042, instance_loss: 0.0000, weighted_loss: 0.0029, label: 0, bag_size: 18045\n",
      "batch 799, loss: 0.1871, instance_loss: 0.1968, weighted_loss: 0.1900, label: 1, bag_size: 3980\n",
      "batch 819, loss: 0.0865, instance_loss: 1.8610, weighted_loss: 0.6189, label: 0, bag_size: 2360\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9733993902439024: correct 12771/13120\n",
      "class 1 clustering acc 0.8650914634146342: correct 5675/6560\n",
      "Epoch: 31, train_loss: 0.2377, train_clustering_loss:  0.2556, train_error: 0.0890\n",
      "class 0: acc 0.9093078758949881, correct 381/419\n",
      "class 1: acc 0.912718204488778, correct 366/401\n",
      "\n",
      "Val Set, val_loss: 0.3095, val_error: 0.1364, auc: 0.9814\n",
      "class 0 clustering acc 0.9244318181818182: correct 1627/1760\n",
      "class 1 clustering acc 0.8102272727272727: correct 713/880\n",
      "class 0: acc 0.7115384615384616, correct 37/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0808, instance_loss: 0.0081, weighted_loss: 0.0590, label: 0, bag_size: 2998\n",
      "batch 39, loss: 0.0310, instance_loss: 0.0553, weighted_loss: 0.0383, label: 0, bag_size: 3557\n",
      "batch 59, loss: 0.0215, instance_loss: 0.0047, weighted_loss: 0.0165, label: 1, bag_size: 2662\n",
      "batch 79, loss: 0.5962, instance_loss: 0.1862, weighted_loss: 0.4732, label: 1, bag_size: 8103\n",
      "batch 99, loss: 0.0481, instance_loss: 0.5294, weighted_loss: 0.1925, label: 0, bag_size: 9888\n",
      "batch 119, loss: 0.0660, instance_loss: 0.1284, weighted_loss: 0.0847, label: 1, bag_size: 1888\n",
      "batch 139, loss: 0.4487, instance_loss: 1.2221, weighted_loss: 0.6807, label: 1, bag_size: 2937\n",
      "batch 159, loss: 0.0261, instance_loss: 0.1710, weighted_loss: 0.0696, label: 0, bag_size: 763\n",
      "batch 179, loss: 0.0153, instance_loss: 0.0028, weighted_loss: 0.0116, label: 1, bag_size: 1101\n",
      "batch 199, loss: 0.0395, instance_loss: 0.1534, weighted_loss: 0.0737, label: 0, bag_size: 2628\n",
      "batch 219, loss: 0.0170, instance_loss: 0.0746, weighted_loss: 0.0342, label: 0, bag_size: 11199\n",
      "batch 239, loss: 0.0283, instance_loss: 0.0230, weighted_loss: 0.0267, label: 1, bag_size: 7935\n",
      "batch 259, loss: 1.4380, instance_loss: 0.2289, weighted_loss: 1.0753, label: 1, bag_size: 13440\n",
      "batch 279, loss: 0.0068, instance_loss: 0.0042, weighted_loss: 0.0060, label: 0, bag_size: 14305\n",
      "batch 299, loss: 0.0497, instance_loss: 0.0000, weighted_loss: 0.0348, label: 0, bag_size: 4271\n",
      "batch 319, loss: 0.0263, instance_loss: 0.0001, weighted_loss: 0.0184, label: 1, bag_size: 15093\n",
      "batch 339, loss: 0.0076, instance_loss: 0.0000, weighted_loss: 0.0054, label: 0, bag_size: 14319\n",
      "batch 359, loss: 0.1138, instance_loss: 0.0881, weighted_loss: 0.1061, label: 1, bag_size: 19606\n",
      "batch 379, loss: 0.0026, instance_loss: 0.0011, weighted_loss: 0.0021, label: 0, bag_size: 15967\n",
      "batch 399, loss: 0.0173, instance_loss: 0.0747, weighted_loss: 0.0345, label: 0, bag_size: 9252\n",
      "batch 419, loss: 0.1362, instance_loss: 0.1635, weighted_loss: 0.1444, label: 1, bag_size: 3651\n",
      "batch 439, loss: 0.1060, instance_loss: 0.0003, weighted_loss: 0.0743, label: 1, bag_size: 19606\n",
      "batch 459, loss: 0.0209, instance_loss: 0.2647, weighted_loss: 0.0940, label: 1, bag_size: 549\n",
      "batch 479, loss: 0.0063, instance_loss: 1.1676, weighted_loss: 0.3547, label: 1, bag_size: 6875\n",
      "batch 499, loss: 0.0567, instance_loss: 0.0000, weighted_loss: 0.0397, label: 0, bag_size: 6898\n",
      "batch 519, loss: 0.0043, instance_loss: 0.0544, weighted_loss: 0.0193, label: 1, bag_size: 17486\n",
      "batch 539, loss: 1.3347, instance_loss: 0.0022, weighted_loss: 0.9350, label: 0, bag_size: 5211\n",
      "batch 559, loss: 0.2013, instance_loss: 0.0125, weighted_loss: 0.1447, label: 1, bag_size: 10622\n",
      "batch 579, loss: 0.0028, instance_loss: 0.0002, weighted_loss: 0.0020, label: 0, bag_size: 8812\n",
      "batch 599, loss: 0.0662, instance_loss: 0.0430, weighted_loss: 0.0592, label: 0, bag_size: 2652\n",
      "batch 619, loss: 1.7314, instance_loss: 0.5172, weighted_loss: 1.3672, label: 0, bag_size: 23618\n",
      "batch 639, loss: 0.0072, instance_loss: 0.0036, weighted_loss: 0.0062, label: 0, bag_size: 8661\n",
      "batch 659, loss: 0.1451, instance_loss: 0.8174, weighted_loss: 0.3468, label: 1, bag_size: 9519\n",
      "batch 679, loss: 0.0033, instance_loss: 0.0357, weighted_loss: 0.0130, label: 1, bag_size: 9065\n",
      "batch 699, loss: 0.0132, instance_loss: 0.1746, weighted_loss: 0.0616, label: 0, bag_size: 1458\n",
      "batch 719, loss: 1.2522, instance_loss: 0.5274, weighted_loss: 1.0348, label: 0, bag_size: 23618\n",
      "batch 739, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 19390\n",
      "batch 759, loss: 0.3332, instance_loss: 0.0431, weighted_loss: 0.2462, label: 1, bag_size: 8475\n",
      "batch 779, loss: 0.0722, instance_loss: 0.0190, weighted_loss: 0.0562, label: 0, bag_size: 7612\n",
      "batch 799, loss: 0.0349, instance_loss: 0.0006, weighted_loss: 0.0246, label: 1, bag_size: 30675\n",
      "batch 819, loss: 0.0222, instance_loss: 0.0000, weighted_loss: 0.0156, label: 0, bag_size: 12593\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9717987804878049: correct 12750/13120\n",
      "class 1 clustering acc 0.8757621951219512: correct 5745/6560\n",
      "Epoch: 32, train_loss: 0.2380, train_clustering_loss:  0.2508, train_error: 0.0963\n",
      "class 0: acc 0.8925233644859814, correct 382/428\n",
      "class 1: acc 0.9158163265306123, correct 359/392\n",
      "\n",
      "Val Set, val_loss: 0.1987, val_error: 0.0727, auc: 0.9818\n",
      "class 0 clustering acc 0.9255681818181818: correct 1629/1760\n",
      "class 1 clustering acc 0.8056818181818182: correct 709/880\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "Validation loss decreased (0.209245 --> 0.198740).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0092, instance_loss: 0.1630, weighted_loss: 0.0553, label: 0, bag_size: 2424\n",
      "batch 39, loss: 0.0096, instance_loss: 0.0094, weighted_loss: 0.0095, label: 0, bag_size: 16341\n",
      "batch 59, loss: 0.1175, instance_loss: 0.0039, weighted_loss: 0.0834, label: 0, bag_size: 1884\n",
      "batch 79, loss: 0.0131, instance_loss: 0.0148, weighted_loss: 0.0136, label: 0, bag_size: 22800\n",
      "batch 99, loss: 0.1915, instance_loss: 0.0000, weighted_loss: 0.1341, label: 0, bag_size: 10113\n",
      "batch 119, loss: 0.0027, instance_loss: 0.0000, weighted_loss: 0.0019, label: 0, bag_size: 11900\n",
      "batch 139, loss: 0.0076, instance_loss: 0.0011, weighted_loss: 0.0057, label: 1, bag_size: 11389\n",
      "batch 159, loss: 0.1359, instance_loss: 0.0326, weighted_loss: 0.1049, label: 0, bag_size: 7989\n",
      "batch 179, loss: 0.0437, instance_loss: 0.0007, weighted_loss: 0.0308, label: 0, bag_size: 15313\n",
      "batch 199, loss: 0.0735, instance_loss: 0.0816, weighted_loss: 0.0759, label: 1, bag_size: 5256\n",
      "batch 219, loss: 0.7028, instance_loss: 0.4236, weighted_loss: 0.6190, label: 1, bag_size: 2395\n",
      "batch 239, loss: 0.0134, instance_loss: 0.0838, weighted_loss: 0.0345, label: 0, bag_size: 3101\n",
      "batch 259, loss: 0.0544, instance_loss: 0.1850, weighted_loss: 0.0936, label: 0, bag_size: 931\n",
      "batch 279, loss: 0.0030, instance_loss: 0.0000, weighted_loss: 0.0021, label: 0, bag_size: 26271\n",
      "batch 299, loss: 0.1549, instance_loss: 0.0199, weighted_loss: 0.1144, label: 1, bag_size: 25695\n",
      "batch 319, loss: 0.3727, instance_loss: 0.0099, weighted_loss: 0.2639, label: 1, bag_size: 8592\n",
      "batch 339, loss: 0.0258, instance_loss: 0.0834, weighted_loss: 0.0431, label: 1, bag_size: 9004\n",
      "batch 359, loss: 0.0215, instance_loss: 0.0039, weighted_loss: 0.0162, label: 1, bag_size: 18468\n",
      "batch 379, loss: 3.8188, instance_loss: 0.3682, weighted_loss: 2.7837, label: 0, bag_size: 3802\n",
      "batch 399, loss: 0.0090, instance_loss: 0.0000, weighted_loss: 0.0063, label: 0, bag_size: 2844\n",
      "batch 419, loss: 0.0379, instance_loss: 0.0002, weighted_loss: 0.0266, label: 1, bag_size: 12575\n",
      "batch 439, loss: 0.0105, instance_loss: 0.0065, weighted_loss: 0.0093, label: 0, bag_size: 3228\n",
      "batch 459, loss: 0.1590, instance_loss: 0.0993, weighted_loss: 0.1411, label: 1, bag_size: 1609\n",
      "batch 479, loss: 0.0451, instance_loss: 0.0000, weighted_loss: 0.0316, label: 0, bag_size: 8788\n",
      "batch 499, loss: 0.0042, instance_loss: 0.0710, weighted_loss: 0.0242, label: 1, bag_size: 7767\n",
      "batch 519, loss: 0.2812, instance_loss: 0.0237, weighted_loss: 0.2040, label: 0, bag_size: 12840\n",
      "batch 539, loss: 0.0304, instance_loss: 0.5463, weighted_loss: 0.1852, label: 1, bag_size: 2785\n",
      "batch 559, loss: 0.0251, instance_loss: 0.0461, weighted_loss: 0.0314, label: 0, bag_size: 3774\n",
      "batch 579, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 11865\n",
      "batch 599, loss: 0.0360, instance_loss: 0.0109, weighted_loss: 0.0285, label: 1, bag_size: 4789\n",
      "batch 619, loss: 0.0147, instance_loss: 0.0002, weighted_loss: 0.0103, label: 0, bag_size: 2282\n",
      "batch 639, loss: 0.2576, instance_loss: 0.0084, weighted_loss: 0.1828, label: 1, bag_size: 13692\n",
      "batch 659, loss: 0.0444, instance_loss: 0.0027, weighted_loss: 0.0319, label: 0, bag_size: 9252\n",
      "batch 679, loss: 0.1080, instance_loss: 0.0020, weighted_loss: 0.0762, label: 0, bag_size: 14625\n",
      "batch 699, loss: 0.0054, instance_loss: 0.0055, weighted_loss: 0.0054, label: 1, bag_size: 11387\n",
      "batch 719, loss: 0.0175, instance_loss: 0.0121, weighted_loss: 0.0159, label: 1, bag_size: 1838\n",
      "batch 739, loss: 0.6429, instance_loss: 0.1756, weighted_loss: 0.5027, label: 1, bag_size: 3652\n",
      "batch 759, loss: 0.0319, instance_loss: 0.0037, weighted_loss: 0.0234, label: 1, bag_size: 16034\n",
      "batch 779, loss: 0.6722, instance_loss: 0.8905, weighted_loss: 0.7377, label: 0, bag_size: 2351\n",
      "batch 799, loss: 0.0186, instance_loss: 0.0028, weighted_loss: 0.0139, label: 0, bag_size: 17268\n",
      "batch 819, loss: 0.0964, instance_loss: 0.0147, weighted_loss: 0.0719, label: 1, bag_size: 5690\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9746189024390244: correct 12787/13120\n",
      "class 1 clustering acc 0.8652439024390244: correct 5676/6560\n",
      "Epoch: 33, train_loss: 0.2126, train_clustering_loss:  0.2668, train_error: 0.0780\n",
      "class 0: acc 0.919431279620853, correct 388/422\n",
      "class 1: acc 0.9246231155778895, correct 368/398\n",
      "\n",
      "Val Set, val_loss: 0.2027, val_error: 0.0909, auc: 0.9831\n",
      "class 0 clustering acc 0.9545454545454546: correct 1680/1760\n",
      "class 1 clustering acc 0.8511363636363637: correct 749/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 7078\n",
      "batch 39, loss: 1.0171, instance_loss: 0.0004, weighted_loss: 0.7121, label: 0, bag_size: 15672\n",
      "batch 59, loss: 0.0071, instance_loss: 0.0000, weighted_loss: 0.0050, label: 0, bag_size: 8898\n",
      "batch 79, loss: 0.3674, instance_loss: 0.9047, weighted_loss: 0.5286, label: 1, bag_size: 10072\n",
      "batch 99, loss: 0.0410, instance_loss: 0.2485, weighted_loss: 0.1032, label: 0, bag_size: 9596\n",
      "batch 119, loss: 0.1591, instance_loss: 0.0384, weighted_loss: 0.1229, label: 0, bag_size: 19808\n",
      "batch 139, loss: 0.0222, instance_loss: 0.0351, weighted_loss: 0.0261, label: 0, bag_size: 9888\n",
      "batch 159, loss: 0.0053, instance_loss: 0.0000, weighted_loss: 0.0037, label: 0, bag_size: 16341\n",
      "batch 179, loss: 0.0113, instance_loss: 0.0390, weighted_loss: 0.0196, label: 1, bag_size: 8660\n",
      "batch 199, loss: 0.0056, instance_loss: 0.0002, weighted_loss: 0.0040, label: 1, bag_size: 14433\n",
      "batch 219, loss: 0.0696, instance_loss: 0.0030, weighted_loss: 0.0496, label: 1, bag_size: 8438\n",
      "batch 239, loss: 0.0116, instance_loss: 0.0000, weighted_loss: 0.0081, label: 0, bag_size: 23796\n",
      "batch 259, loss: 0.0257, instance_loss: 0.5996, weighted_loss: 0.1978, label: 0, bag_size: 3893\n",
      "batch 279, loss: 0.0209, instance_loss: 0.0019, weighted_loss: 0.0152, label: 1, bag_size: 3082\n",
      "batch 299, loss: 0.0353, instance_loss: 0.3058, weighted_loss: 0.1165, label: 0, bag_size: 3893\n",
      "batch 319, loss: 0.0116, instance_loss: 0.0028, weighted_loss: 0.0089, label: 1, bag_size: 6343\n",
      "batch 339, loss: 0.1210, instance_loss: 0.0019, weighted_loss: 0.0853, label: 0, bag_size: 24439\n",
      "batch 359, loss: 0.0016, instance_loss: 0.0002, weighted_loss: 0.0012, label: 1, bag_size: 9673\n",
      "batch 379, loss: 0.3701, instance_loss: 0.0812, weighted_loss: 0.2835, label: 1, bag_size: 7768\n",
      "batch 399, loss: 0.0452, instance_loss: 0.8411, weighted_loss: 0.2840, label: 0, bag_size: 1614\n",
      "batch 419, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 16936\n",
      "batch 439, loss: 0.0053, instance_loss: 0.0001, weighted_loss: 0.0037, label: 1, bag_size: 3640\n",
      "batch 459, loss: 0.0037, instance_loss: 0.0009, weighted_loss: 0.0029, label: 1, bag_size: 10392\n",
      "batch 479, loss: 0.0884, instance_loss: 0.0291, weighted_loss: 0.0706, label: 1, bag_size: 13692\n",
      "batch 499, loss: 0.0024, instance_loss: 0.1454, weighted_loss: 0.0453, label: 1, bag_size: 10281\n",
      "batch 519, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 1, bag_size: 5864\n",
      "batch 539, loss: 0.1465, instance_loss: 0.1568, weighted_loss: 0.1496, label: 0, bag_size: 3502\n",
      "batch 559, loss: 0.0982, instance_loss: 0.0170, weighted_loss: 0.0738, label: 0, bag_size: 1202\n",
      "batch 579, loss: 0.0013, instance_loss: 0.2419, weighted_loss: 0.0735, label: 1, bag_size: 617\n",
      "batch 599, loss: 0.7484, instance_loss: 0.3032, weighted_loss: 0.6148, label: 0, bag_size: 1800\n",
      "batch 619, loss: 0.0485, instance_loss: 0.0029, weighted_loss: 0.0348, label: 0, bag_size: 11122\n",
      "batch 639, loss: 0.0286, instance_loss: 0.0057, weighted_loss: 0.0217, label: 1, bag_size: 5454\n",
      "batch 659, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 11778\n",
      "batch 679, loss: 0.0012, instance_loss: 0.0044, weighted_loss: 0.0021, label: 0, bag_size: 17791\n",
      "batch 699, loss: 0.4892, instance_loss: 0.8785, weighted_loss: 0.6060, label: 0, bag_size: 2242\n",
      "batch 719, loss: 0.0697, instance_loss: 0.1163, weighted_loss: 0.0837, label: 1, bag_size: 3683\n",
      "batch 739, loss: 0.0031, instance_loss: 0.1346, weighted_loss: 0.0425, label: 1, bag_size: 2140\n",
      "batch 759, loss: 0.0534, instance_loss: 0.1364, weighted_loss: 0.0783, label: 1, bag_size: 10671\n",
      "batch 779, loss: 0.2987, instance_loss: 0.1031, weighted_loss: 0.2400, label: 1, bag_size: 771\n",
      "batch 799, loss: 0.0155, instance_loss: 0.1358, weighted_loss: 0.0516, label: 0, bag_size: 1452\n",
      "batch 819, loss: 0.0066, instance_loss: 0.1478, weighted_loss: 0.0490, label: 0, bag_size: 518\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9782774390243902: correct 12835/13120\n",
      "class 1 clustering acc 0.8847560975609756: correct 5804/6560\n",
      "Epoch: 34, train_loss: 0.1833, train_clustering_loss:  0.2036, train_error: 0.0622\n",
      "class 0: acc 0.9369158878504673, correct 401/428\n",
      "class 1: acc 0.9387755102040817, correct 368/392\n",
      "\n",
      "Val Set, val_loss: 0.1799, val_error: 0.0818, auc: 0.9838\n",
      "class 0 clustering acc 0.9426136363636364: correct 1659/1760\n",
      "class 1 clustering acc 0.8272727272727273: correct 728/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.896551724137931, correct 52/58\n",
      "Validation loss decreased (0.198740 --> 0.179949).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.5319, instance_loss: 1.8928, weighted_loss: 0.9402, label: 1, bag_size: 1831\n",
      "batch 39, loss: 0.0070, instance_loss: 0.1755, weighted_loss: 0.0576, label: 0, bag_size: 17155\n",
      "batch 59, loss: 0.0044, instance_loss: 0.0038, weighted_loss: 0.0043, label: 1, bag_size: 16417\n",
      "batch 79, loss: 0.0176, instance_loss: 0.0263, weighted_loss: 0.0202, label: 1, bag_size: 3651\n",
      "batch 99, loss: 0.0028, instance_loss: 0.0018, weighted_loss: 0.0025, label: 1, bag_size: 5833\n",
      "batch 119, loss: 0.0779, instance_loss: 0.1034, weighted_loss: 0.0856, label: 0, bag_size: 2266\n",
      "batch 139, loss: 0.0087, instance_loss: 0.0029, weighted_loss: 0.0069, label: 1, bag_size: 12931\n",
      "batch 159, loss: 0.0423, instance_loss: 0.0025, weighted_loss: 0.0304, label: 0, bag_size: 12732\n",
      "batch 179, loss: 0.0007, instance_loss: 0.0075, weighted_loss: 0.0028, label: 0, bag_size: 27158\n",
      "batch 199, loss: 0.0939, instance_loss: 0.1004, weighted_loss: 0.0958, label: 1, bag_size: 4786\n",
      "batch 219, loss: 0.0378, instance_loss: 0.0228, weighted_loss: 0.0333, label: 0, bag_size: 4497\n",
      "batch 239, loss: 0.0009, instance_loss: 0.0008, weighted_loss: 0.0009, label: 1, bag_size: 15332\n",
      "batch 259, loss: 0.0202, instance_loss: 0.5197, weighted_loss: 0.1701, label: 1, bag_size: 2759\n",
      "batch 279, loss: 0.3832, instance_loss: 0.1241, weighted_loss: 0.3055, label: 0, bag_size: 15071\n",
      "batch 299, loss: 0.0800, instance_loss: 0.0096, weighted_loss: 0.0589, label: 0, bag_size: 7989\n",
      "batch 319, loss: 0.4538, instance_loss: 0.0180, weighted_loss: 0.3231, label: 0, bag_size: 10029\n",
      "batch 339, loss: 0.0111, instance_loss: 0.0035, weighted_loss: 0.0088, label: 1, bag_size: 5340\n",
      "batch 359, loss: 0.0874, instance_loss: 1.0939, weighted_loss: 0.3893, label: 0, bag_size: 931\n",
      "batch 379, loss: 0.2506, instance_loss: 0.0993, weighted_loss: 0.2052, label: 0, bag_size: 7612\n",
      "batch 399, loss: 0.0010, instance_loss: 0.0013, weighted_loss: 0.0011, label: 0, bag_size: 11512\n",
      "batch 419, loss: 0.0028, instance_loss: 0.0579, weighted_loss: 0.0193, label: 0, bag_size: 3228\n",
      "batch 439, loss: 0.0778, instance_loss: 0.0008, weighted_loss: 0.0547, label: 1, bag_size: 2904\n",
      "batch 459, loss: 0.0560, instance_loss: 0.0441, weighted_loss: 0.0524, label: 0, bag_size: 24439\n",
      "batch 479, loss: 0.0229, instance_loss: 0.4242, weighted_loss: 0.1433, label: 1, bag_size: 8660\n",
      "batch 499, loss: 0.1447, instance_loss: 0.1006, weighted_loss: 0.1314, label: 1, bag_size: 5907\n",
      "batch 519, loss: 0.0265, instance_loss: 0.7007, weighted_loss: 0.2287, label: 0, bag_size: 10146\n",
      "batch 539, loss: 0.0090, instance_loss: 0.0000, weighted_loss: 0.0063, label: 0, bag_size: 13892\n",
      "batch 559, loss: 0.0289, instance_loss: 0.2320, weighted_loss: 0.0898, label: 0, bag_size: 6367\n",
      "batch 579, loss: 0.0080, instance_loss: 0.1223, weighted_loss: 0.0423, label: 0, bag_size: 13777\n",
      "batch 599, loss: 0.0207, instance_loss: 0.0040, weighted_loss: 0.0157, label: 1, bag_size: 20161\n",
      "batch 619, loss: 0.0658, instance_loss: 0.0313, weighted_loss: 0.0555, label: 0, bag_size: 2873\n",
      "batch 639, loss: 0.5101, instance_loss: 0.0031, weighted_loss: 0.3580, label: 0, bag_size: 5297\n",
      "batch 659, loss: 0.0657, instance_loss: 0.0268, weighted_loss: 0.0540, label: 0, bag_size: 3725\n",
      "batch 679, loss: 0.0104, instance_loss: 0.0000, weighted_loss: 0.0073, label: 1, bag_size: 9322\n",
      "batch 699, loss: 0.0090, instance_loss: 0.0845, weighted_loss: 0.0316, label: 0, bag_size: 1234\n",
      "batch 719, loss: 0.5067, instance_loss: 0.0040, weighted_loss: 0.3559, label: 0, bag_size: 15672\n",
      "batch 739, loss: 0.0083, instance_loss: 0.0000, weighted_loss: 0.0058, label: 0, bag_size: 10898\n",
      "batch 759, loss: 0.2136, instance_loss: 0.0131, weighted_loss: 0.1534, label: 1, bag_size: 13089\n",
      "batch 779, loss: 0.0071, instance_loss: 0.0177, weighted_loss: 0.0103, label: 0, bag_size: 12201\n",
      "batch 799, loss: 0.0799, instance_loss: 0.2986, weighted_loss: 0.1455, label: 0, bag_size: 8025\n",
      "batch 819, loss: 0.4386, instance_loss: 0.6957, weighted_loss: 0.5157, label: 0, bag_size: 9597\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9753048780487805: correct 12796/13120\n",
      "class 1 clustering acc 0.8771341463414634: correct 5754/6560\n",
      "Epoch: 35, train_loss: 0.2394, train_clustering_loss:  0.2337, train_error: 0.0866\n",
      "class 0: acc 0.9137529137529138, correct 392/429\n",
      "class 1: acc 0.9130434782608695, correct 357/391\n",
      "\n",
      "Val Set, val_loss: 0.1926, val_error: 0.0727, auc: 0.9838\n",
      "class 0 clustering acc 0.9460227272727273: correct 1665/1760\n",
      "class 1 clustering acc 0.7965909090909091: correct 701/880\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0056, instance_loss: 0.0922, weighted_loss: 0.0315, label: 0, bag_size: 13892\n",
      "batch 39, loss: 0.0184, instance_loss: 0.2032, weighted_loss: 0.0738, label: 1, bag_size: 7119\n",
      "batch 59, loss: 0.0553, instance_loss: 0.0935, weighted_loss: 0.0668, label: 0, bag_size: 2382\n",
      "batch 79, loss: 0.0050, instance_loss: 0.0001, weighted_loss: 0.0035, label: 1, bag_size: 617\n",
      "batch 99, loss: 0.3538, instance_loss: 0.1259, weighted_loss: 0.2854, label: 0, bag_size: 9597\n",
      "batch 119, loss: 0.0822, instance_loss: 0.0141, weighted_loss: 0.0617, label: 0, bag_size: 2732\n",
      "batch 139, loss: 0.0272, instance_loss: 0.4870, weighted_loss: 0.1652, label: 0, bag_size: 11607\n",
      "batch 159, loss: 0.0085, instance_loss: 0.0628, weighted_loss: 0.0248, label: 0, bag_size: 2296\n",
      "batch 179, loss: 0.0171, instance_loss: 0.0801, weighted_loss: 0.0360, label: 0, bag_size: 1452\n",
      "batch 199, loss: 0.1186, instance_loss: 0.0191, weighted_loss: 0.0888, label: 0, bag_size: 1508\n",
      "batch 219, loss: 0.0186, instance_loss: 0.0099, weighted_loss: 0.0160, label: 0, bag_size: 10128\n",
      "batch 239, loss: 0.0357, instance_loss: 0.0000, weighted_loss: 0.0250, label: 1, bag_size: 10969\n",
      "batch 259, loss: 0.0673, instance_loss: 0.0851, weighted_loss: 0.0726, label: 1, bag_size: 7381\n",
      "batch 279, loss: 0.0199, instance_loss: 0.1924, weighted_loss: 0.0717, label: 1, bag_size: 4862\n",
      "batch 299, loss: 0.0159, instance_loss: 0.0205, weighted_loss: 0.0173, label: 0, bag_size: 16052\n",
      "batch 319, loss: 0.0777, instance_loss: 0.0367, weighted_loss: 0.0654, label: 1, bag_size: 4821\n",
      "batch 339, loss: 0.4150, instance_loss: 0.8088, weighted_loss: 0.5332, label: 1, bag_size: 1609\n",
      "batch 359, loss: 0.0103, instance_loss: 0.1220, weighted_loss: 0.0439, label: 0, bag_size: 1745\n",
      "batch 379, loss: 0.2740, instance_loss: 0.0699, weighted_loss: 0.2127, label: 0, bag_size: 13619\n",
      "batch 399, loss: 0.0016, instance_loss: 0.3552, weighted_loss: 0.1077, label: 1, bag_size: 10281\n",
      "batch 419, loss: 0.0335, instance_loss: 0.0000, weighted_loss: 0.0234, label: 0, bag_size: 8959\n",
      "batch 439, loss: 0.0104, instance_loss: 0.0339, weighted_loss: 0.0175, label: 1, bag_size: 10281\n",
      "batch 459, loss: 0.7897, instance_loss: 0.0902, weighted_loss: 0.5798, label: 0, bag_size: 14664\n",
      "batch 479, loss: 0.0235, instance_loss: 0.0010, weighted_loss: 0.0168, label: 0, bag_size: 2920\n",
      "batch 499, loss: 0.0005, instance_loss: 0.0008, weighted_loss: 0.0006, label: 0, bag_size: 16936\n",
      "batch 519, loss: 0.0055, instance_loss: 0.0000, weighted_loss: 0.0038, label: 0, bag_size: 12524\n",
      "batch 539, loss: 0.0214, instance_loss: 0.0031, weighted_loss: 0.0159, label: 0, bag_size: 14249\n",
      "batch 559, loss: 1.7823, instance_loss: 1.6651, weighted_loss: 1.7471, label: 1, bag_size: 1444\n",
      "batch 579, loss: 2.7246, instance_loss: 0.1658, weighted_loss: 1.9569, label: 1, bag_size: 2565\n",
      "batch 599, loss: 0.0023, instance_loss: 0.0849, weighted_loss: 0.0271, label: 0, bag_size: 8948\n",
      "batch 619, loss: 0.0796, instance_loss: 0.0000, weighted_loss: 0.0557, label: 0, bag_size: 21319\n",
      "batch 639, loss: 0.0297, instance_loss: 0.0000, weighted_loss: 0.0208, label: 0, bag_size: 2548\n",
      "batch 659, loss: 0.0927, instance_loss: 0.4150, weighted_loss: 0.1894, label: 0, bag_size: 1831\n",
      "batch 679, loss: 0.0018, instance_loss: 0.0000, weighted_loss: 0.0013, label: 1, bag_size: 6606\n",
      "batch 699, loss: 0.0095, instance_loss: 0.0019, weighted_loss: 0.0072, label: 1, bag_size: 10482\n",
      "batch 719, loss: 0.0029, instance_loss: 0.0004, weighted_loss: 0.0021, label: 0, bag_size: 10995\n",
      "batch 739, loss: 0.0694, instance_loss: 0.0731, weighted_loss: 0.0705, label: 1, bag_size: 1924\n",
      "batch 759, loss: 0.0346, instance_loss: 1.0654, weighted_loss: 0.3439, label: 1, bag_size: 1525\n",
      "batch 779, loss: 0.0035, instance_loss: 0.0425, weighted_loss: 0.0152, label: 1, bag_size: 8019\n",
      "batch 799, loss: 0.0240, instance_loss: 0.0821, weighted_loss: 0.0415, label: 0, bag_size: 10444\n",
      "batch 819, loss: 0.0104, instance_loss: 0.0002, weighted_loss: 0.0073, label: 0, bag_size: 13795\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9762957317073171: correct 12809/13120\n",
      "class 1 clustering acc 0.8765243902439024: correct 5750/6560\n",
      "Epoch: 36, train_loss: 0.1996, train_clustering_loss:  0.2289, train_error: 0.0780\n",
      "class 0: acc 0.9152542372881356, correct 378/413\n",
      "class 1: acc 0.9287469287469288, correct 378/407\n",
      "\n",
      "Val Set, val_loss: 0.1688, val_error: 0.0545, auc: 0.9861\n",
      "class 0 clustering acc 0.9619318181818182: correct 1693/1760\n",
      "class 1 clustering acc 0.8636363636363636: correct 760/880\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "Validation loss decreased (0.179949 --> 0.168807).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0087, instance_loss: 0.0013, weighted_loss: 0.0065, label: 1, bag_size: 6090\n",
      "batch 39, loss: 0.8795, instance_loss: 0.2447, weighted_loss: 0.6891, label: 0, bag_size: 2959\n",
      "batch 59, loss: 0.0755, instance_loss: 0.0034, weighted_loss: 0.0539, label: 1, bag_size: 3683\n",
      "batch 79, loss: 2.8005, instance_loss: 4.2617, weighted_loss: 3.2389, label: 0, bag_size: 7428\n",
      "batch 99, loss: 0.0114, instance_loss: 0.0000, weighted_loss: 0.0080, label: 0, bag_size: 22681\n",
      "batch 119, loss: 0.0250, instance_loss: 0.7384, weighted_loss: 0.2390, label: 0, bag_size: 8898\n",
      "batch 139, loss: 0.1075, instance_loss: 0.0098, weighted_loss: 0.0782, label: 0, bag_size: 15071\n",
      "batch 159, loss: 0.0041, instance_loss: 0.0145, weighted_loss: 0.0072, label: 0, bag_size: 1881\n",
      "batch 179, loss: 0.4426, instance_loss: 0.1524, weighted_loss: 0.3556, label: 0, bag_size: 3654\n",
      "batch 199, loss: 0.1116, instance_loss: 0.0042, weighted_loss: 0.0794, label: 1, bag_size: 20537\n",
      "batch 219, loss: 0.0613, instance_loss: 0.0168, weighted_loss: 0.0480, label: 1, bag_size: 2278\n",
      "batch 239, loss: 0.0471, instance_loss: 0.0019, weighted_loss: 0.0336, label: 0, bag_size: 1962\n",
      "batch 259, loss: 0.0054, instance_loss: 0.0000, weighted_loss: 0.0038, label: 0, bag_size: 2244\n",
      "batch 279, loss: 0.0148, instance_loss: 0.0243, weighted_loss: 0.0176, label: 0, bag_size: 9415\n",
      "batch 299, loss: 0.0032, instance_loss: 0.0046, weighted_loss: 0.0037, label: 0, bag_size: 18225\n",
      "batch 319, loss: 0.0022, instance_loss: 0.0483, weighted_loss: 0.0160, label: 0, bag_size: 16720\n",
      "batch 339, loss: 0.0293, instance_loss: 0.0687, weighted_loss: 0.0411, label: 0, bag_size: 8959\n",
      "batch 359, loss: 0.2305, instance_loss: 0.0562, weighted_loss: 0.1782, label: 0, bag_size: 3541\n",
      "batch 379, loss: 0.0147, instance_loss: 0.0009, weighted_loss: 0.0106, label: 1, bag_size: 5256\n",
      "batch 399, loss: 0.0904, instance_loss: 0.0074, weighted_loss: 0.0655, label: 1, bag_size: 5907\n",
      "batch 419, loss: 0.0153, instance_loss: 0.0000, weighted_loss: 0.0107, label: 0, bag_size: 16052\n",
      "batch 439, loss: 0.2470, instance_loss: 0.0031, weighted_loss: 0.1738, label: 0, bag_size: 3810\n",
      "batch 459, loss: 0.0111, instance_loss: 0.0105, weighted_loss: 0.0109, label: 1, bag_size: 8003\n",
      "batch 479, loss: 0.0363, instance_loss: 0.0018, weighted_loss: 0.0260, label: 1, bag_size: 9062\n",
      "batch 499, loss: 0.0025, instance_loss: 0.0397, weighted_loss: 0.0137, label: 0, bag_size: 10535\n",
      "batch 519, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 23996\n",
      "batch 539, loss: 4.7157, instance_loss: 5.0194, weighted_loss: 4.8068, label: 0, bag_size: 3468\n",
      "batch 559, loss: 0.0028, instance_loss: 0.0058, weighted_loss: 0.0037, label: 0, bag_size: 10751\n",
      "batch 579, loss: 0.0073, instance_loss: 0.0000, weighted_loss: 0.0051, label: 0, bag_size: 10146\n",
      "batch 599, loss: 0.0024, instance_loss: 0.0002, weighted_loss: 0.0017, label: 1, bag_size: 20333\n",
      "batch 619, loss: 0.0273, instance_loss: 0.0251, weighted_loss: 0.0266, label: 0, bag_size: 1349\n",
      "batch 639, loss: 0.0712, instance_loss: 0.0013, weighted_loss: 0.0502, label: 1, bag_size: 3856\n",
      "batch 659, loss: 1.0465, instance_loss: 0.1383, weighted_loss: 0.7740, label: 0, bag_size: 1437\n",
      "batch 679, loss: 0.7824, instance_loss: 0.0011, weighted_loss: 0.5480, label: 1, bag_size: 12180\n",
      "batch 699, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 6164\n",
      "batch 719, loss: 0.0457, instance_loss: 0.0183, weighted_loss: 0.0375, label: 1, bag_size: 6478\n",
      "batch 739, loss: 0.1608, instance_loss: 0.1593, weighted_loss: 0.1603, label: 0, bag_size: 1760\n",
      "batch 759, loss: 0.0233, instance_loss: 0.0000, weighted_loss: 0.0163, label: 1, bag_size: 7798\n",
      "batch 779, loss: 0.0026, instance_loss: 0.0364, weighted_loss: 0.0127, label: 0, bag_size: 10535\n",
      "batch 799, loss: 0.0111, instance_loss: 0.0101, weighted_loss: 0.0108, label: 0, bag_size: 12731\n",
      "batch 819, loss: 0.0156, instance_loss: 0.1653, weighted_loss: 0.0605, label: 0, bag_size: 3474\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9810213414634147: correct 12871/13120\n",
      "class 1 clustering acc 0.9038109756097561: correct 5929/6560\n",
      "Epoch: 37, train_loss: 0.1834, train_clustering_loss:  0.1797, train_error: 0.0683\n",
      "class 0: acc 0.921760391198044, correct 377/409\n",
      "class 1: acc 0.9416058394160584, correct 387/411\n",
      "\n",
      "Val Set, val_loss: 0.1746, val_error: 0.0909, auc: 0.9861\n",
      "class 0 clustering acc 0.9386363636363636: correct 1652/1760\n",
      "class 1 clustering acc 0.8431818181818181: correct 742/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0382, instance_loss: 0.0015, weighted_loss: 0.0272, label: 0, bag_size: 7381\n",
      "batch 39, loss: 0.0202, instance_loss: 0.0262, weighted_loss: 0.0220, label: 0, bag_size: 10942\n",
      "batch 59, loss: 0.0074, instance_loss: 0.0234, weighted_loss: 0.0122, label: 1, bag_size: 10033\n",
      "batch 79, loss: 0.0370, instance_loss: 0.1037, weighted_loss: 0.0570, label: 1, bag_size: 6927\n",
      "batch 99, loss: 0.0114, instance_loss: 0.0007, weighted_loss: 0.0082, label: 1, bag_size: 11684\n",
      "batch 119, loss: 0.0061, instance_loss: 0.0019, weighted_loss: 0.0049, label: 1, bag_size: 5991\n",
      "batch 139, loss: 0.0650, instance_loss: 0.0727, weighted_loss: 0.0673, label: 1, bag_size: 13026\n",
      "batch 159, loss: 0.0262, instance_loss: 0.0000, weighted_loss: 0.0183, label: 0, bag_size: 6898\n",
      "batch 179, loss: 0.4255, instance_loss: 0.1930, weighted_loss: 0.3557, label: 1, bag_size: 16514\n",
      "batch 199, loss: 1.1101, instance_loss: 2.3749, weighted_loss: 1.4895, label: 1, bag_size: 1444\n",
      "batch 219, loss: 0.0008, instance_loss: 0.0086, weighted_loss: 0.0032, label: 0, bag_size: 11735\n",
      "batch 239, loss: 0.0350, instance_loss: 0.7475, weighted_loss: 0.2488, label: 1, bag_size: 865\n",
      "batch 259, loss: 0.0132, instance_loss: 0.0130, weighted_loss: 0.0132, label: 0, bag_size: 12201\n",
      "batch 279, loss: 0.0008, instance_loss: 0.0333, weighted_loss: 0.0105, label: 0, bag_size: 1639\n",
      "batch 299, loss: 0.5556, instance_loss: 0.0098, weighted_loss: 0.3919, label: 1, bag_size: 7351\n",
      "batch 319, loss: 0.0045, instance_loss: 0.0000, weighted_loss: 0.0031, label: 1, bag_size: 10396\n",
      "batch 339, loss: 0.0022, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 11512\n",
      "batch 359, loss: 0.0042, instance_loss: 0.0014, weighted_loss: 0.0033, label: 1, bag_size: 11032\n",
      "batch 379, loss: 0.0039, instance_loss: 0.0000, weighted_loss: 0.0027, label: 0, bag_size: 11900\n",
      "batch 399, loss: 0.0025, instance_loss: 0.0000, weighted_loss: 0.0018, label: 0, bag_size: 11199\n",
      "batch 419, loss: 2.7241, instance_loss: 2.2881, weighted_loss: 2.5933, label: 0, bag_size: 4692\n",
      "batch 439, loss: 0.0072, instance_loss: 0.0000, weighted_loss: 0.0050, label: 1, bag_size: 11363\n",
      "batch 459, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 8410\n",
      "batch 479, loss: 0.2864, instance_loss: 0.0776, weighted_loss: 0.2238, label: 1, bag_size: 2179\n",
      "batch 499, loss: 0.0024, instance_loss: 0.0005, weighted_loss: 0.0019, label: 1, bag_size: 9732\n",
      "batch 519, loss: 0.2126, instance_loss: 0.3908, weighted_loss: 0.2660, label: 1, bag_size: 5160\n",
      "batch 539, loss: 0.0040, instance_loss: 0.0021, weighted_loss: 0.0034, label: 0, bag_size: 8145\n",
      "batch 559, loss: 0.0058, instance_loss: 0.0000, weighted_loss: 0.0041, label: 0, bag_size: 11199\n",
      "batch 579, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 14319\n",
      "batch 599, loss: 0.0051, instance_loss: 0.0082, weighted_loss: 0.0061, label: 0, bag_size: 1202\n",
      "batch 619, loss: 0.0059, instance_loss: 0.1294, weighted_loss: 0.0429, label: 0, bag_size: 2628\n",
      "batch 639, loss: 0.0033, instance_loss: 0.0050, weighted_loss: 0.0038, label: 0, bag_size: 3459\n",
      "batch 659, loss: 0.0076, instance_loss: 0.0104, weighted_loss: 0.0084, label: 0, bag_size: 2282\n",
      "batch 679, loss: 0.0219, instance_loss: 0.2544, weighted_loss: 0.0916, label: 0, bag_size: 2360\n",
      "batch 699, loss: 0.2115, instance_loss: 0.0030, weighted_loss: 0.1489, label: 1, bag_size: 7468\n",
      "batch 719, loss: 0.0495, instance_loss: 0.0043, weighted_loss: 0.0359, label: 0, bag_size: 7612\n",
      "batch 739, loss: 0.0018, instance_loss: 0.0004, weighted_loss: 0.0014, label: 1, bag_size: 7110\n",
      "batch 759, loss: 0.0023, instance_loss: 0.1514, weighted_loss: 0.0470, label: 0, bag_size: 2424\n",
      "batch 779, loss: 0.0029, instance_loss: 0.0029, weighted_loss: 0.0029, label: 0, bag_size: 5225\n",
      "batch 799, loss: 0.0428, instance_loss: 0.5102, weighted_loss: 0.1830, label: 1, bag_size: 4039\n",
      "batch 819, loss: 0.0517, instance_loss: 0.0437, weighted_loss: 0.0493, label: 1, bag_size: 1014\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9782774390243902: correct 12835/13120\n",
      "class 1 clustering acc 0.8908536585365854: correct 5844/6560\n",
      "Epoch: 38, train_loss: 0.1950, train_clustering_loss:  0.2070, train_error: 0.0793\n",
      "class 0: acc 0.9060240963855422, correct 376/415\n",
      "class 1: acc 0.9358024691358025, correct 379/405\n",
      "\n",
      "Val Set, val_loss: 0.1641, val_error: 0.0727, auc: 0.9867\n",
      "class 0 clustering acc 0.9573863636363636: correct 1685/1760\n",
      "class 1 clustering acc 0.8818181818181818: correct 776/880\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "Validation loss decreased (0.168807 --> 0.164123).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.9253, instance_loss: 1.8521, weighted_loss: 1.2033, label: 0, bag_size: 11128\n",
      "batch 39, loss: 0.0010, instance_loss: 0.0006, weighted_loss: 0.0009, label: 0, bag_size: 11383\n",
      "batch 59, loss: 0.0490, instance_loss: 0.0000, weighted_loss: 0.0343, label: 1, bag_size: 13026\n",
      "batch 79, loss: 0.0552, instance_loss: 0.1537, weighted_loss: 0.0848, label: 0, bag_size: 2296\n",
      "batch 99, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 18240\n",
      "batch 119, loss: 0.0075, instance_loss: 0.0000, weighted_loss: 0.0052, label: 1, bag_size: 18794\n",
      "batch 139, loss: 0.0292, instance_loss: 0.0000, weighted_loss: 0.0205, label: 0, bag_size: 10365\n",
      "batch 159, loss: 0.0370, instance_loss: 1.4813, weighted_loss: 0.4703, label: 1, bag_size: 8438\n",
      "batch 179, loss: 0.2786, instance_loss: 0.1451, weighted_loss: 0.2386, label: 1, bag_size: 16548\n",
      "batch 199, loss: 0.0257, instance_loss: 0.0035, weighted_loss: 0.0190, label: 0, bag_size: 4523\n",
      "batch 219, loss: 0.0077, instance_loss: 0.0000, weighted_loss: 0.0054, label: 0, bag_size: 15003\n",
      "batch 239, loss: 0.0068, instance_loss: 0.7929, weighted_loss: 0.2426, label: 1, bag_size: 3295\n",
      "batch 259, loss: 0.2788, instance_loss: 0.0203, weighted_loss: 0.2013, label: 1, bag_size: 8592\n",
      "batch 279, loss: 0.1484, instance_loss: 0.1274, weighted_loss: 0.1421, label: 1, bag_size: 1609\n",
      "batch 299, loss: 0.0159, instance_loss: 0.0083, weighted_loss: 0.0136, label: 1, bag_size: 7119\n",
      "batch 319, loss: 0.0093, instance_loss: 0.0060, weighted_loss: 0.0083, label: 0, bag_size: 7011\n",
      "batch 339, loss: 0.0495, instance_loss: 0.0082, weighted_loss: 0.0371, label: 0, bag_size: 1891\n",
      "batch 359, loss: 0.0562, instance_loss: 0.0114, weighted_loss: 0.0428, label: 0, bag_size: 2336\n",
      "batch 379, loss: 0.2970, instance_loss: 0.6017, weighted_loss: 0.3884, label: 0, bag_size: 1142\n",
      "batch 399, loss: 0.1781, instance_loss: 0.1825, weighted_loss: 0.1794, label: 1, bag_size: 11386\n",
      "batch 419, loss: 0.1401, instance_loss: 0.4764, weighted_loss: 0.2410, label: 1, bag_size: 2682\n",
      "batch 439, loss: 0.0119, instance_loss: 0.0000, weighted_loss: 0.0083, label: 1, bag_size: 11701\n",
      "batch 459, loss: 0.0028, instance_loss: 0.0021, weighted_loss: 0.0026, label: 1, bag_size: 5317\n",
      "batch 479, loss: 0.0013, instance_loss: 0.0060, weighted_loss: 0.0027, label: 1, bag_size: 9147\n",
      "batch 499, loss: 0.0208, instance_loss: 0.0247, weighted_loss: 0.0219, label: 1, bag_size: 2140\n",
      "batch 519, loss: 0.0406, instance_loss: 0.0466, weighted_loss: 0.0424, label: 1, bag_size: 7381\n",
      "batch 539, loss: 0.2732, instance_loss: 0.2658, weighted_loss: 0.2710, label: 0, bag_size: 2160\n",
      "batch 559, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 21082\n",
      "batch 579, loss: 0.0023, instance_loss: 0.0002, weighted_loss: 0.0017, label: 0, bag_size: 11199\n",
      "batch 599, loss: 0.0039, instance_loss: 0.0027, weighted_loss: 0.0035, label: 0, bag_size: 1452\n",
      "batch 619, loss: 0.0072, instance_loss: 0.0001, weighted_loss: 0.0051, label: 1, bag_size: 18603\n",
      "batch 639, loss: 0.1955, instance_loss: 0.3152, weighted_loss: 0.2314, label: 0, bag_size: 2270\n",
      "batch 659, loss: 0.0087, instance_loss: 0.0162, weighted_loss: 0.0109, label: 1, bag_size: 10105\n",
      "batch 679, loss: 0.0190, instance_loss: 0.7592, weighted_loss: 0.2411, label: 0, bag_size: 14956\n",
      "batch 699, loss: 0.1027, instance_loss: 0.1246, weighted_loss: 0.1093, label: 0, bag_size: 4959\n",
      "batch 719, loss: 4.9917, instance_loss: 0.7558, weighted_loss: 3.7210, label: 0, bag_size: 5105\n",
      "batch 739, loss: 0.0115, instance_loss: 0.0000, weighted_loss: 0.0081, label: 1, bag_size: 20537\n",
      "batch 759, loss: 0.1466, instance_loss: 0.0336, weighted_loss: 0.1127, label: 1, bag_size: 9561\n",
      "batch 779, loss: 1.1733, instance_loss: 0.0041, weighted_loss: 0.8225, label: 0, bag_size: 5211\n",
      "batch 799, loss: 0.2218, instance_loss: 0.1103, weighted_loss: 0.1883, label: 0, bag_size: 1814\n",
      "batch 819, loss: 0.3278, instance_loss: 0.4017, weighted_loss: 0.3499, label: 1, bag_size: 1919\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.979420731707317: correct 12850/13120\n",
      "class 1 clustering acc 0.9013719512195122: correct 5913/6560\n",
      "Epoch: 39, train_loss: 0.1963, train_clustering_loss:  0.1962, train_error: 0.0732\n",
      "class 0: acc 0.9209876543209876, correct 373/405\n",
      "class 1: acc 0.9325301204819277, correct 387/415\n",
      "\n",
      "Val Set, val_loss: 0.1977, val_error: 0.0909, auc: 0.9861\n",
      "class 0 clustering acc 0.9494318181818182: correct 1671/1760\n",
      "class 1 clustering acc 0.8397727272727272: correct 739/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8620689655172413, correct 50/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0203, instance_loss: 0.0005, weighted_loss: 0.0144, label: 0, bag_size: 1202\n",
      "batch 39, loss: 0.0050, instance_loss: 0.0000, weighted_loss: 0.0035, label: 1, bag_size: 11032\n",
      "batch 59, loss: 0.0236, instance_loss: 0.0000, weighted_loss: 0.0165, label: 1, bag_size: 7217\n",
      "batch 79, loss: 0.0240, instance_loss: 0.3299, weighted_loss: 0.1158, label: 1, bag_size: 1051\n",
      "batch 99, loss: 1.1124, instance_loss: 1.3020, weighted_loss: 1.1693, label: 0, bag_size: 1637\n",
      "batch 119, loss: 0.0156, instance_loss: 0.0052, weighted_loss: 0.0125, label: 1, bag_size: 12611\n",
      "batch 139, loss: 1.1406, instance_loss: 1.6270, weighted_loss: 1.2865, label: 1, bag_size: 2314\n",
      "batch 159, loss: 0.0134, instance_loss: 0.0035, weighted_loss: 0.0104, label: 1, bag_size: 20161\n",
      "batch 179, loss: 0.0516, instance_loss: 0.0046, weighted_loss: 0.0375, label: 0, bag_size: 12731\n",
      "batch 199, loss: 0.0059, instance_loss: 0.0000, weighted_loss: 0.0041, label: 0, bag_size: 4271\n",
      "batch 219, loss: 0.0029, instance_loss: 0.0001, weighted_loss: 0.0021, label: 1, bag_size: 19932\n",
      "batch 239, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 15747\n",
      "batch 259, loss: 0.1738, instance_loss: 0.0013, weighted_loss: 0.1221, label: 0, bag_size: 11390\n",
      "batch 279, loss: 0.8211, instance_loss: 0.0001, weighted_loss: 0.5748, label: 0, bag_size: 5211\n",
      "batch 299, loss: 0.0020, instance_loss: 0.0002, weighted_loss: 0.0014, label: 1, bag_size: 11032\n",
      "batch 319, loss: 0.0930, instance_loss: 0.5978, weighted_loss: 0.2445, label: 0, bag_size: 1483\n",
      "batch 339, loss: 0.0032, instance_loss: 0.0000, weighted_loss: 0.0022, label: 0, bag_size: 6652\n",
      "batch 359, loss: 0.3342, instance_loss: 0.2376, weighted_loss: 0.3052, label: 1, bag_size: 1242\n",
      "batch 379, loss: 0.4694, instance_loss: 0.0050, weighted_loss: 0.3301, label: 0, bag_size: 10029\n",
      "batch 399, loss: 0.1180, instance_loss: 0.1422, weighted_loss: 0.1253, label: 0, bag_size: 3654\n",
      "batch 419, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 10068\n",
      "batch 439, loss: 0.0716, instance_loss: 0.1230, weighted_loss: 0.0871, label: 0, bag_size: 1760\n",
      "batch 459, loss: 0.0333, instance_loss: 0.0210, weighted_loss: 0.0296, label: 0, bag_size: 2270\n",
      "batch 479, loss: 0.1421, instance_loss: 0.2915, weighted_loss: 0.1869, label: 1, bag_size: 2682\n",
      "batch 499, loss: 0.0246, instance_loss: 0.0907, weighted_loss: 0.0444, label: 0, bag_size: 12732\n",
      "batch 519, loss: 0.0461, instance_loss: 0.1285, weighted_loss: 0.0708, label: 1, bag_size: 21701\n",
      "batch 539, loss: 0.0070, instance_loss: 0.0003, weighted_loss: 0.0050, label: 1, bag_size: 4862\n",
      "batch 559, loss: 0.0201, instance_loss: 0.0000, weighted_loss: 0.0141, label: 0, bag_size: 8025\n",
      "batch 579, loss: 0.9036, instance_loss: 1.5830, weighted_loss: 1.1075, label: 1, bag_size: 1867\n",
      "batch 599, loss: 0.0833, instance_loss: 1.8444, weighted_loss: 0.6116, label: 0, bag_size: 20796\n",
      "batch 619, loss: 0.0002, instance_loss: 0.0014, weighted_loss: 0.0006, label: 1, bag_size: 9673\n",
      "batch 639, loss: 0.1234, instance_loss: 0.1517, weighted_loss: 0.1319, label: 0, bag_size: 2732\n",
      "batch 659, loss: 0.0915, instance_loss: 0.0736, weighted_loss: 0.0861, label: 0, bag_size: 2004\n",
      "batch 679, loss: 0.1029, instance_loss: 0.0002, weighted_loss: 0.0721, label: 1, bag_size: 6927\n",
      "batch 699, loss: 0.5943, instance_loss: 0.7693, weighted_loss: 0.6468, label: 1, bag_size: 1609\n",
      "batch 719, loss: 0.1455, instance_loss: 0.1475, weighted_loss: 0.1461, label: 1, bag_size: 1867\n",
      "batch 739, loss: 0.9199, instance_loss: 0.6469, weighted_loss: 0.8380, label: 0, bag_size: 1701\n",
      "batch 759, loss: 0.0231, instance_loss: 0.0268, weighted_loss: 0.0242, label: 0, bag_size: 3657\n",
      "batch 779, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 7191\n",
      "batch 799, loss: 0.0191, instance_loss: 0.0021, weighted_loss: 0.0140, label: 1, bag_size: 18603\n",
      "batch 819, loss: 0.0742, instance_loss: 0.0028, weighted_loss: 0.0528, label: 0, bag_size: 13992\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9753048780487805: correct 12796/13120\n",
      "class 1 clustering acc 0.8785060975609756: correct 5763/6560\n",
      "Epoch: 40, train_loss: 0.2135, train_clustering_loss:  0.2245, train_error: 0.0878\n",
      "class 0: acc 0.9116279069767442, correct 392/430\n",
      "class 1: acc 0.9128205128205128, correct 356/390\n",
      "\n",
      "Val Set, val_loss: 0.1615, val_error: 0.0636, auc: 0.9874\n",
      "class 0 clustering acc 0.9460227272727273: correct 1665/1760\n",
      "class 1 clustering acc 0.7909090909090909: correct 696/880\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "Validation loss decreased (0.164123 --> 0.161503).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0113, instance_loss: 0.0065, weighted_loss: 0.0099, label: 1, bag_size: 9571\n",
      "batch 39, loss: 0.0017, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 18225\n",
      "batch 59, loss: 0.0162, instance_loss: 0.0005, weighted_loss: 0.0115, label: 0, bag_size: 2732\n",
      "batch 79, loss: 0.0060, instance_loss: 0.1225, weighted_loss: 0.0409, label: 0, bag_size: 1881\n",
      "batch 99, loss: 0.1590, instance_loss: 0.0000, weighted_loss: 0.1113, label: 0, bag_size: 24439\n",
      "batch 119, loss: 0.0404, instance_loss: 0.0176, weighted_loss: 0.0336, label: 0, bag_size: 10415\n",
      "batch 139, loss: 0.0521, instance_loss: 0.0128, weighted_loss: 0.0403, label: 1, bag_size: 10671\n",
      "batch 159, loss: 0.1761, instance_loss: 1.0825, weighted_loss: 0.4480, label: 1, bag_size: 1294\n",
      "batch 179, loss: 0.0124, instance_loss: 0.0278, weighted_loss: 0.0170, label: 0, bag_size: 2654\n",
      "batch 199, loss: 3.3485, instance_loss: 0.0809, weighted_loss: 2.3682, label: 0, bag_size: 2694\n",
      "batch 219, loss: 0.3327, instance_loss: 0.2963, weighted_loss: 0.3218, label: 0, bag_size: 9616\n",
      "batch 239, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 23398\n",
      "batch 259, loss: 0.0234, instance_loss: 0.0207, weighted_loss: 0.0226, label: 1, bag_size: 11421\n",
      "batch 279, loss: 0.0137, instance_loss: 0.0000, weighted_loss: 0.0096, label: 0, bag_size: 22681\n",
      "batch 299, loss: 0.2325, instance_loss: 0.0539, weighted_loss: 0.1790, label: 1, bag_size: 4956\n",
      "batch 319, loss: 0.1006, instance_loss: 0.0000, weighted_loss: 0.0704, label: 1, bag_size: 15609\n",
      "batch 339, loss: 0.0180, instance_loss: 0.0168, weighted_loss: 0.0176, label: 0, bag_size: 2652\n",
      "batch 359, loss: 0.0206, instance_loss: 0.0075, weighted_loss: 0.0167, label: 1, bag_size: 7246\n",
      "batch 379, loss: 0.0081, instance_loss: 0.0017, weighted_loss: 0.0062, label: 0, bag_size: 9471\n",
      "batch 399, loss: 1.1515, instance_loss: 0.9302, weighted_loss: 1.0851, label: 0, bag_size: 7239\n",
      "batch 419, loss: 0.0534, instance_loss: 0.0002, weighted_loss: 0.0375, label: 1, bag_size: 5025\n",
      "batch 439, loss: 0.0040, instance_loss: 0.1509, weighted_loss: 0.0481, label: 0, bag_size: 890\n",
      "batch 459, loss: 0.0934, instance_loss: 0.0490, weighted_loss: 0.0800, label: 0, bag_size: 2351\n",
      "batch 479, loss: 0.0027, instance_loss: 0.0133, weighted_loss: 0.0059, label: 1, bag_size: 13051\n",
      "batch 499, loss: 0.0482, instance_loss: 0.0000, weighted_loss: 0.0338, label: 0, bag_size: 5297\n",
      "batch 519, loss: 0.0013, instance_loss: 0.0076, weighted_loss: 0.0032, label: 1, bag_size: 8448\n",
      "batch 539, loss: 0.0722, instance_loss: 0.0000, weighted_loss: 0.0505, label: 0, bag_size: 65728\n",
      "batch 559, loss: 0.0533, instance_loss: 0.0062, weighted_loss: 0.0392, label: 1, bag_size: 7119\n",
      "batch 579, loss: 0.0071, instance_loss: 0.0072, weighted_loss: 0.0071, label: 1, bag_size: 14604\n",
      "batch 599, loss: 0.0165, instance_loss: 0.0062, weighted_loss: 0.0134, label: 1, bag_size: 3224\n",
      "batch 619, loss: 0.0620, instance_loss: 0.0009, weighted_loss: 0.0437, label: 1, bag_size: 11220\n",
      "batch 639, loss: 0.0020, instance_loss: 0.0177, weighted_loss: 0.0067, label: 0, bag_size: 1824\n",
      "batch 659, loss: 0.0145, instance_loss: 0.0025, weighted_loss: 0.0109, label: 1, bag_size: 2638\n",
      "batch 679, loss: 0.8583, instance_loss: 0.1887, weighted_loss: 0.6574, label: 0, bag_size: 18516\n",
      "batch 699, loss: 0.1721, instance_loss: 0.0037, weighted_loss: 0.1216, label: 1, bag_size: 5516\n",
      "batch 719, loss: 0.0058, instance_loss: 0.1148, weighted_loss: 0.0385, label: 0, bag_size: 2336\n",
      "batch 739, loss: 0.3313, instance_loss: 0.0382, weighted_loss: 0.2434, label: 1, bag_size: 1123\n",
      "batch 759, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 8372\n",
      "batch 779, loss: 1.8438, instance_loss: 0.3597, weighted_loss: 1.3986, label: 1, bag_size: 1764\n",
      "batch 799, loss: 0.0412, instance_loss: 0.0338, weighted_loss: 0.0390, label: 1, bag_size: 6745\n",
      "batch 819, loss: 0.1992, instance_loss: 0.1551, weighted_loss: 0.1859, label: 1, bag_size: 11223\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9777439024390244: correct 12828/13120\n",
      "class 1 clustering acc 0.8939024390243903: correct 5864/6560\n",
      "Epoch: 41, train_loss: 0.1909, train_clustering_loss:  0.2264, train_error: 0.0756\n",
      "class 0: acc 0.91725768321513, correct 388/423\n",
      "class 1: acc 0.9319899244332494, correct 370/397\n",
      "\n",
      "Val Set, val_loss: 0.1625, val_error: 0.0727, auc: 0.9881\n",
      "class 0 clustering acc 0.95625: correct 1683/1760\n",
      "class 1 clustering acc 0.8977272727272727: correct 790/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.896551724137931, correct 52/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0582, instance_loss: 0.0040, weighted_loss: 0.0419, label: 1, bag_size: 6731\n",
      "batch 39, loss: 0.0311, instance_loss: 0.0561, weighted_loss: 0.0386, label: 1, bag_size: 689\n",
      "batch 59, loss: 0.0231, instance_loss: 0.0000, weighted_loss: 0.0162, label: 0, bag_size: 13205\n",
      "batch 79, loss: 0.0688, instance_loss: 0.0000, weighted_loss: 0.0482, label: 0, bag_size: 10113\n",
      "batch 99, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 11884\n",
      "batch 119, loss: 0.5839, instance_loss: 0.8661, weighted_loss: 0.6686, label: 0, bag_size: 1637\n",
      "batch 139, loss: 0.0026, instance_loss: 0.0001, weighted_loss: 0.0018, label: 0, bag_size: 19466\n",
      "batch 159, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 17633\n",
      "batch 179, loss: 0.0281, instance_loss: 0.0113, weighted_loss: 0.0230, label: 1, bag_size: 10492\n",
      "batch 199, loss: 0.0066, instance_loss: 0.0000, weighted_loss: 0.0046, label: 0, bag_size: 13777\n",
      "batch 219, loss: 0.0038, instance_loss: 0.0007, weighted_loss: 0.0028, label: 0, bag_size: 15636\n",
      "batch 239, loss: 0.0137, instance_loss: 0.0099, weighted_loss: 0.0126, label: 1, bag_size: 2344\n",
      "batch 259, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 11981\n",
      "batch 279, loss: 0.0364, instance_loss: 0.0006, weighted_loss: 0.0257, label: 1, bag_size: 4239\n",
      "batch 299, loss: 0.0285, instance_loss: 0.0000, weighted_loss: 0.0200, label: 0, bag_size: 14828\n",
      "batch 319, loss: 0.0400, instance_loss: 0.0392, weighted_loss: 0.0398, label: 1, bag_size: 3674\n",
      "batch 339, loss: 0.0062, instance_loss: 0.0000, weighted_loss: 0.0043, label: 0, bag_size: 13880\n",
      "batch 359, loss: 0.2776, instance_loss: 0.0015, weighted_loss: 0.1948, label: 1, bag_size: 4786\n",
      "batch 379, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 11512\n",
      "batch 399, loss: 0.0298, instance_loss: 0.0016, weighted_loss: 0.0213, label: 1, bag_size: 9322\n",
      "batch 419, loss: 0.6097, instance_loss: 0.0217, weighted_loss: 0.4333, label: 0, bag_size: 10410\n",
      "batch 439, loss: 0.1658, instance_loss: 0.5525, weighted_loss: 0.2818, label: 1, bag_size: 1014\n",
      "batch 459, loss: 0.0019, instance_loss: 0.0000, weighted_loss: 0.0013, label: 0, bag_size: 11512\n",
      "batch 479, loss: 0.3892, instance_loss: 0.0577, weighted_loss: 0.2898, label: 0, bag_size: 9597\n",
      "batch 499, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 11512\n",
      "batch 519, loss: 0.0112, instance_loss: 0.0056, weighted_loss: 0.0095, label: 1, bag_size: 2495\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0729, weighted_loss: 0.0219, label: 1, bag_size: 4442\n",
      "batch 559, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 26271\n",
      "batch 579, loss: 0.0072, instance_loss: 0.0313, weighted_loss: 0.0144, label: 1, bag_size: 621\n",
      "batch 599, loss: 0.0869, instance_loss: 0.0019, weighted_loss: 0.0614, label: 1, bag_size: 21252\n",
      "batch 619, loss: 0.0157, instance_loss: 0.0041, weighted_loss: 0.0123, label: 1, bag_size: 16565\n",
      "batch 639, loss: 0.0521, instance_loss: 0.0019, weighted_loss: 0.0371, label: 1, bag_size: 10492\n",
      "batch 659, loss: 0.3892, instance_loss: 0.1617, weighted_loss: 0.3210, label: 1, bag_size: 7468\n",
      "batch 679, loss: 0.0043, instance_loss: 0.0342, weighted_loss: 0.0133, label: 0, bag_size: 1560\n",
      "batch 699, loss: 0.0219, instance_loss: 0.0000, weighted_loss: 0.0153, label: 1, bag_size: 6090\n",
      "batch 719, loss: 0.0067, instance_loss: 0.0003, weighted_loss: 0.0048, label: 0, bag_size: 7989\n",
      "batch 739, loss: 0.2781, instance_loss: 0.0734, weighted_loss: 0.2167, label: 0, bag_size: 24382\n",
      "batch 759, loss: 2.7753, instance_loss: 2.2540, weighted_loss: 2.6189, label: 1, bag_size: 1533\n",
      "batch 779, loss: 0.0042, instance_loss: 0.0079, weighted_loss: 0.0053, label: 0, bag_size: 3459\n",
      "batch 799, loss: 0.0031, instance_loss: 0.0000, weighted_loss: 0.0022, label: 0, bag_size: 2036\n",
      "batch 819, loss: 0.0097, instance_loss: 0.0312, weighted_loss: 0.0161, label: 0, bag_size: 16690\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9803353658536585: correct 12862/13120\n",
      "class 1 clustering acc 0.911890243902439: correct 5982/6560\n",
      "Epoch: 42, train_loss: 0.1748, train_clustering_loss:  0.1889, train_error: 0.0646\n",
      "class 0: acc 0.9272300469483568, correct 395/426\n",
      "class 1: acc 0.9441624365482234, correct 372/394\n",
      "\n",
      "Val Set, val_loss: 0.2432, val_error: 0.1000, auc: 0.9884\n",
      "class 0 clustering acc 0.9352272727272727: correct 1646/1760\n",
      "class 1 clustering acc 0.85: correct 748/880\n",
      "class 0: acc 1.0, correct 52/52\n",
      "class 1: acc 0.8103448275862069, correct 47/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1125, instance_loss: 0.0721, weighted_loss: 0.1004, label: 0, bag_size: 20555\n",
      "batch 39, loss: 0.5561, instance_loss: 0.0921, weighted_loss: 0.4169, label: 1, bag_size: 1015\n",
      "batch 59, loss: 0.0061, instance_loss: 0.0110, weighted_loss: 0.0076, label: 0, bag_size: 3557\n",
      "batch 79, loss: 0.0910, instance_loss: 0.0105, weighted_loss: 0.0668, label: 1, bag_size: 2480\n",
      "batch 99, loss: 0.0173, instance_loss: 0.0001, weighted_loss: 0.0122, label: 0, bag_size: 2036\n",
      "batch 119, loss: 0.0053, instance_loss: 0.0048, weighted_loss: 0.0051, label: 1, bag_size: 10033\n",
      "batch 139, loss: 0.0002, instance_loss: 0.0016, weighted_loss: 0.0007, label: 1, bag_size: 9644\n",
      "batch 159, loss: 0.0006, instance_loss: 0.0011, weighted_loss: 0.0007, label: 1, bag_size: 10725\n",
      "batch 179, loss: 0.0807, instance_loss: 0.0088, weighted_loss: 0.0591, label: 0, bag_size: 7381\n",
      "batch 199, loss: 0.0110, instance_loss: 0.0093, weighted_loss: 0.0105, label: 1, bag_size: 13015\n",
      "batch 219, loss: 0.0176, instance_loss: 0.0134, weighted_loss: 0.0163, label: 0, bag_size: 8788\n",
      "batch 239, loss: 0.0034, instance_loss: 0.0000, weighted_loss: 0.0024, label: 0, bag_size: 2322\n",
      "batch 259, loss: 0.0136, instance_loss: 0.0113, weighted_loss: 0.0129, label: 1, bag_size: 3224\n",
      "batch 279, loss: 0.0105, instance_loss: 0.0009, weighted_loss: 0.0076, label: 0, bag_size: 11194\n",
      "batch 299, loss: 0.0203, instance_loss: 0.0004, weighted_loss: 0.0143, label: 0, bag_size: 3774\n",
      "batch 319, loss: 0.0010, instance_loss: 0.0001, weighted_loss: 0.0007, label: 0, bag_size: 2748\n",
      "batch 339, loss: 0.1559, instance_loss: 0.0667, weighted_loss: 0.1291, label: 1, bag_size: 1015\n",
      "batch 359, loss: 0.0072, instance_loss: 0.3680, weighted_loss: 0.1155, label: 0, bag_size: 1560\n",
      "batch 379, loss: 0.0298, instance_loss: 0.0004, weighted_loss: 0.0210, label: 1, bag_size: 5605\n",
      "batch 399, loss: 0.0077, instance_loss: 0.0310, weighted_loss: 0.0147, label: 0, bag_size: 9171\n",
      "batch 419, loss: 0.0015, instance_loss: 0.0078, weighted_loss: 0.0034, label: 0, bag_size: 12217\n",
      "batch 439, loss: 0.0110, instance_loss: 0.0335, weighted_loss: 0.0177, label: 0, bag_size: 2360\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 23037\n",
      "batch 479, loss: 0.0192, instance_loss: 0.9397, weighted_loss: 0.2954, label: 1, bag_size: 865\n",
      "batch 499, loss: 0.0307, instance_loss: 1.3327, weighted_loss: 0.4213, label: 1, bag_size: 2785\n",
      "batch 519, loss: 0.0071, instance_loss: 0.0000, weighted_loss: 0.0050, label: 1, bag_size: 11032\n",
      "batch 539, loss: 0.0013, instance_loss: 0.0051, weighted_loss: 0.0024, label: 0, bag_size: 2322\n",
      "batch 559, loss: 0.0127, instance_loss: 0.1143, weighted_loss: 0.0432, label: 0, bag_size: 1920\n",
      "batch 579, loss: 0.0136, instance_loss: 0.0006, weighted_loss: 0.0097, label: 0, bag_size: 13205\n",
      "batch 599, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 14515\n",
      "batch 619, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 14618\n",
      "batch 639, loss: 0.0615, instance_loss: 0.0469, weighted_loss: 0.0571, label: 1, bag_size: 11701\n",
      "batch 659, loss: 0.1944, instance_loss: 0.1318, weighted_loss: 0.1757, label: 1, bag_size: 1609\n",
      "batch 679, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 12524\n",
      "batch 699, loss: 0.0328, instance_loss: 0.0727, weighted_loss: 0.0447, label: 1, bag_size: 7381\n",
      "batch 719, loss: 3.6195, instance_loss: 0.3436, weighted_loss: 2.6368, label: 1, bag_size: 2565\n",
      "batch 739, loss: 0.0051, instance_loss: 0.0166, weighted_loss: 0.0086, label: 1, bag_size: 10498\n",
      "batch 759, loss: 0.1914, instance_loss: 1.2629, weighted_loss: 0.5128, label: 0, bag_size: 3089\n",
      "batch 779, loss: 0.4770, instance_loss: 0.0575, weighted_loss: 0.3512, label: 1, bag_size: 1755\n",
      "batch 799, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 9885\n",
      "batch 819, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 11195\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9789634146341464: correct 12844/13120\n",
      "class 1 clustering acc 0.9089939024390243: correct 5963/6560\n",
      "Epoch: 43, train_loss: 0.1955, train_clustering_loss:  0.1860, train_error: 0.0707\n",
      "class 0: acc 0.9184652278177458, correct 383/417\n",
      "class 1: acc 0.9404466501240695, correct 379/403\n",
      "\n",
      "Val Set, val_loss: 0.2867, val_error: 0.1182, auc: 0.9881\n",
      "class 0 clustering acc 0.9221590909090909: correct 1623/1760\n",
      "class 1 clustering acc 0.8125: correct 715/880\n",
      "class 0: acc 0.75, correct 39/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0168, instance_loss: 0.0047, weighted_loss: 0.0131, label: 1, bag_size: 2308\n",
      "batch 39, loss: 0.2593, instance_loss: 0.0760, weighted_loss: 0.2043, label: 1, bag_size: 16548\n",
      "batch 59, loss: 0.0132, instance_loss: 0.0434, weighted_loss: 0.0223, label: 1, bag_size: 12697\n",
      "batch 79, loss: 0.0125, instance_loss: 0.0012, weighted_loss: 0.0091, label: 1, bag_size: 9533\n",
      "batch 99, loss: 0.0844, instance_loss: 0.0000, weighted_loss: 0.0591, label: 0, bag_size: 47866\n",
      "batch 119, loss: 0.0228, instance_loss: 0.3552, weighted_loss: 0.1225, label: 0, bag_size: 4497\n",
      "batch 139, loss: 0.0110, instance_loss: 0.0589, weighted_loss: 0.0253, label: 0, bag_size: 9596\n",
      "batch 159, loss: 0.0158, instance_loss: 0.3433, weighted_loss: 0.1141, label: 0, bag_size: 5639\n",
      "batch 179, loss: 0.0029, instance_loss: 0.0002, weighted_loss: 0.0021, label: 1, bag_size: 10105\n",
      "batch 199, loss: 0.0022, instance_loss: 0.0005, weighted_loss: 0.0017, label: 1, bag_size: 13365\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0006, weighted_loss: 0.0003, label: 0, bag_size: 18154\n",
      "batch 239, loss: 0.3226, instance_loss: 1.8387, weighted_loss: 0.7774, label: 0, bag_size: 2290\n",
      "batch 259, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 15077\n",
      "batch 279, loss: 1.0320, instance_loss: 0.3105, weighted_loss: 0.8156, label: 1, bag_size: 9942\n",
      "batch 299, loss: 0.0216, instance_loss: 0.0032, weighted_loss: 0.0161, label: 1, bag_size: 3409\n",
      "batch 319, loss: 0.0095, instance_loss: 0.1902, weighted_loss: 0.0637, label: 0, bag_size: 2609\n",
      "batch 339, loss: 0.0188, instance_loss: 0.5381, weighted_loss: 0.1746, label: 0, bag_size: 3908\n",
      "batch 359, loss: 0.0046, instance_loss: 0.0000, weighted_loss: 0.0032, label: 1, bag_size: 6317\n",
      "batch 379, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 10592\n",
      "batch 399, loss: 0.9915, instance_loss: 1.1147, weighted_loss: 1.0285, label: 1, bag_size: 2731\n",
      "batch 419, loss: 0.0404, instance_loss: 0.0980, weighted_loss: 0.0577, label: 1, bag_size: 1294\n",
      "batch 439, loss: 0.0145, instance_loss: 0.0000, weighted_loss: 0.0102, label: 1, bag_size: 14681\n",
      "batch 459, loss: 0.0083, instance_loss: 0.0249, weighted_loss: 0.0132, label: 0, bag_size: 12793\n",
      "batch 479, loss: 0.2677, instance_loss: 0.1218, weighted_loss: 0.2239, label: 1, bag_size: 1015\n",
      "batch 499, loss: 0.0150, instance_loss: 0.0000, weighted_loss: 0.0105, label: 1, bag_size: 22286\n",
      "batch 519, loss: 0.0003, instance_loss: 0.0001, weighted_loss: 0.0002, label: 1, bag_size: 5833\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 19472\n",
      "batch 559, loss: 0.0043, instance_loss: 0.0026, weighted_loss: 0.0038, label: 0, bag_size: 3198\n",
      "batch 579, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 6966\n",
      "batch 599, loss: 0.0064, instance_loss: 0.0228, weighted_loss: 0.0113, label: 0, bag_size: 9786\n",
      "batch 619, loss: 0.0052, instance_loss: 0.0000, weighted_loss: 0.0037, label: 1, bag_size: 17769\n",
      "batch 639, loss: 0.7926, instance_loss: 0.6843, weighted_loss: 0.7601, label: 0, bag_size: 2959\n",
      "batch 659, loss: 0.0619, instance_loss: 1.6580, weighted_loss: 0.5407, label: 1, bag_size: 549\n",
      "batch 679, loss: 0.0001, instance_loss: 0.1142, weighted_loss: 0.0343, label: 0, bag_size: 3787\n",
      "batch 699, loss: 0.2081, instance_loss: 0.4384, weighted_loss: 0.2772, label: 0, bag_size: 2104\n",
      "batch 719, loss: 0.0158, instance_loss: 0.3232, weighted_loss: 0.1080, label: 1, bag_size: 2455\n",
      "batch 739, loss: 0.0768, instance_loss: 0.0141, weighted_loss: 0.0580, label: 0, bag_size: 2998\n",
      "batch 759, loss: 0.0464, instance_loss: 0.0316, weighted_loss: 0.0420, label: 1, bag_size: 2480\n",
      "batch 779, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 6875\n",
      "batch 799, loss: 0.0016, instance_loss: 0.0001, weighted_loss: 0.0011, label: 0, bag_size: 15914\n",
      "batch 819, loss: 0.0163, instance_loss: 0.0000, weighted_loss: 0.0114, label: 0, bag_size: 13205\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9809451219512195: correct 12870/13120\n",
      "class 1 clustering acc 0.9155487804878049: correct 6006/6560\n",
      "Epoch: 44, train_loss: 0.1689, train_clustering_loss:  0.1757, train_error: 0.0671\n",
      "class 0: acc 0.9304556354916067, correct 388/417\n",
      "class 1: acc 0.9354838709677419, correct 377/403\n",
      "\n",
      "Val Set, val_loss: 0.1460, val_error: 0.0545, auc: 0.9904\n",
      "class 0 clustering acc 0.95: correct 1672/1760\n",
      "class 1 clustering acc 0.8727272727272727: correct 768/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "Validation loss decreased (0.161503 --> 0.145951).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0070, instance_loss: 0.0092, weighted_loss: 0.0077, label: 0, bag_size: 11654\n",
      "batch 39, loss: 0.0198, instance_loss: 0.0847, weighted_loss: 0.0393, label: 0, bag_size: 12731\n",
      "batch 59, loss: 0.0053, instance_loss: 0.0000, weighted_loss: 0.0037, label: 0, bag_size: 14305\n",
      "batch 79, loss: 0.0198, instance_loss: 0.0031, weighted_loss: 0.0148, label: 1, bag_size: 2356\n",
      "batch 99, loss: 0.0079, instance_loss: 0.0045, weighted_loss: 0.0069, label: 1, bag_size: 2678\n",
      "batch 119, loss: 0.0182, instance_loss: 0.2321, weighted_loss: 0.0824, label: 0, bag_size: 1149\n",
      "batch 139, loss: 2.5930, instance_loss: 3.6028, weighted_loss: 2.8959, label: 0, bag_size: 4692\n",
      "batch 159, loss: 0.0120, instance_loss: 0.5794, weighted_loss: 0.1822, label: 0, bag_size: 2091\n",
      "batch 179, loss: 4.1373, instance_loss: 1.3535, weighted_loss: 3.3022, label: 1, bag_size: 898\n",
      "batch 199, loss: 0.0022, instance_loss: 0.0090, weighted_loss: 0.0042, label: 0, bag_size: 1234\n",
      "batch 219, loss: 0.0081, instance_loss: 0.0000, weighted_loss: 0.0057, label: 0, bag_size: 10942\n",
      "batch 239, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 6752\n",
      "batch 259, loss: 0.0262, instance_loss: 0.0192, weighted_loss: 0.0241, label: 0, bag_size: 1415\n",
      "batch 279, loss: 0.2406, instance_loss: 0.1505, weighted_loss: 0.2136, label: 1, bag_size: 8103\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11735\n",
      "batch 319, loss: 0.0036, instance_loss: 0.0030, weighted_loss: 0.0034, label: 0, bag_size: 10751\n",
      "batch 339, loss: 0.0098, instance_loss: 0.0000, weighted_loss: 0.0069, label: 1, bag_size: 5025\n",
      "batch 359, loss: 0.0282, instance_loss: 0.0000, weighted_loss: 0.0198, label: 0, bag_size: 18215\n",
      "batch 379, loss: 0.0081, instance_loss: 0.0027, weighted_loss: 0.0065, label: 0, bag_size: 1962\n",
      "batch 399, loss: 0.0318, instance_loss: 0.0222, weighted_loss: 0.0289, label: 1, bag_size: 5155\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 10481\n",
      "batch 439, loss: 0.0254, instance_loss: 0.0040, weighted_loss: 0.0190, label: 1, bag_size: 1622\n",
      "batch 459, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 3437\n",
      "batch 479, loss: 0.0010, instance_loss: 0.0028, weighted_loss: 0.0015, label: 1, bag_size: 617\n",
      "batch 499, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 12149\n",
      "batch 519, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 10725\n",
      "batch 539, loss: 0.0074, instance_loss: 0.0005, weighted_loss: 0.0053, label: 1, bag_size: 12719\n",
      "batch 559, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 3640\n",
      "batch 579, loss: 0.3542, instance_loss: 0.0001, weighted_loss: 0.2480, label: 1, bag_size: 5516\n",
      "batch 599, loss: 0.0003, instance_loss: 0.0003, weighted_loss: 0.0003, label: 1, bag_size: 14202\n",
      "batch 619, loss: 0.0066, instance_loss: 0.0000, weighted_loss: 0.0046, label: 0, bag_size: 4271\n",
      "batch 639, loss: 0.8542, instance_loss: 0.3851, weighted_loss: 0.7135, label: 1, bag_size: 1284\n",
      "batch 659, loss: 0.0014, instance_loss: 0.0007, weighted_loss: 0.0012, label: 1, bag_size: 4862\n",
      "batch 679, loss: 0.0899, instance_loss: 0.0000, weighted_loss: 0.0630, label: 0, bag_size: 3552\n",
      "batch 699, loss: 0.0070, instance_loss: 0.0007, weighted_loss: 0.0051, label: 0, bag_size: 4271\n",
      "batch 719, loss: 0.8878, instance_loss: 0.8593, weighted_loss: 0.8793, label: 1, bag_size: 13089\n",
      "batch 739, loss: 0.0019, instance_loss: 0.0233, weighted_loss: 0.0083, label: 1, bag_size: 16267\n",
      "batch 759, loss: 0.2402, instance_loss: 0.0177, weighted_loss: 0.1734, label: 1, bag_size: 13440\n",
      "batch 779, loss: 0.1135, instance_loss: 0.0000, weighted_loss: 0.0795, label: 1, bag_size: 16548\n",
      "batch 799, loss: 0.0025, instance_loss: 0.0000, weighted_loss: 0.0017, label: 0, bag_size: 20796\n",
      "batch 819, loss: 0.0080, instance_loss: 0.0665, weighted_loss: 0.0256, label: 1, bag_size: 2278\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.978734756097561: correct 12841/13120\n",
      "class 1 clustering acc 0.9015243902439024: correct 5914/6560\n",
      "Epoch: 45, train_loss: 0.2022, train_clustering_loss:  0.1886, train_error: 0.0817\n",
      "class 0: acc 0.9139240506329114, correct 361/395\n",
      "class 1: acc 0.9223529411764706, correct 392/425\n",
      "\n",
      "Val Set, val_loss: 0.1705, val_error: 0.0909, auc: 0.9907\n",
      "class 0 clustering acc 0.9659090909090909: correct 1700/1760\n",
      "class 1 clustering acc 0.7875: correct 693/880\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1417, instance_loss: 0.1723, weighted_loss: 0.1509, label: 1, bag_size: 2681\n",
      "batch 39, loss: 0.0308, instance_loss: 0.0000, weighted_loss: 0.0216, label: 0, bag_size: 9069\n",
      "batch 59, loss: 1.1356, instance_loss: 1.1633, weighted_loss: 1.1439, label: 0, bag_size: 7428\n",
      "batch 79, loss: 0.0266, instance_loss: 0.3333, weighted_loss: 0.1186, label: 1, bag_size: 14604\n",
      "batch 99, loss: 0.4474, instance_loss: 0.0285, weighted_loss: 0.3217, label: 1, bag_size: 15125\n",
      "batch 119, loss: 0.0288, instance_loss: 0.2493, weighted_loss: 0.0950, label: 0, bag_size: 5009\n",
      "batch 139, loss: 0.0017, instance_loss: 0.0015, weighted_loss: 0.0017, label: 1, bag_size: 6606\n",
      "batch 159, loss: 0.0043, instance_loss: 0.0000, weighted_loss: 0.0030, label: 1, bag_size: 6453\n",
      "batch 179, loss: 0.0733, instance_loss: 0.0000, weighted_loss: 0.0513, label: 1, bag_size: 25695\n",
      "batch 199, loss: 0.1908, instance_loss: 0.0004, weighted_loss: 0.1337, label: 1, bag_size: 8982\n",
      "batch 219, loss: 0.0009, instance_loss: 0.2259, weighted_loss: 0.0684, label: 0, bag_size: 3190\n",
      "batch 239, loss: 0.0768, instance_loss: 0.5624, weighted_loss: 0.2225, label: 1, bag_size: 7669\n",
      "batch 259, loss: 0.0062, instance_loss: 0.0001, weighted_loss: 0.0044, label: 1, bag_size: 8019\n",
      "batch 279, loss: 0.0049, instance_loss: 0.0000, weighted_loss: 0.0034, label: 0, bag_size: 3228\n",
      "batch 299, loss: 0.1811, instance_loss: 1.4900, weighted_loss: 0.5738, label: 1, bag_size: 1444\n",
      "batch 319, loss: 0.6886, instance_loss: 1.8280, weighted_loss: 1.0304, label: 1, bag_size: 1845\n",
      "batch 339, loss: 0.0373, instance_loss: 0.2168, weighted_loss: 0.0911, label: 1, bag_size: 1294\n",
      "batch 359, loss: 0.0058, instance_loss: 0.0026, weighted_loss: 0.0049, label: 1, bag_size: 5561\n",
      "batch 379, loss: 0.0068, instance_loss: 0.0006, weighted_loss: 0.0049, label: 1, bag_size: 18794\n",
      "batch 399, loss: 0.1529, instance_loss: 0.0006, weighted_loss: 0.1072, label: 1, bag_size: 8012\n",
      "batch 419, loss: 0.0318, instance_loss: 0.0744, weighted_loss: 0.0446, label: 0, bag_size: 1831\n",
      "batch 439, loss: 0.0358, instance_loss: 0.0000, weighted_loss: 0.0251, label: 0, bag_size: 11922\n",
      "batch 459, loss: 0.1059, instance_loss: 0.0013, weighted_loss: 0.0745, label: 1, bag_size: 11394\n",
      "batch 479, loss: 0.0013, instance_loss: 0.0060, weighted_loss: 0.0027, label: 0, bag_size: 32227\n",
      "batch 499, loss: 0.0687, instance_loss: 0.0091, weighted_loss: 0.0508, label: 0, bag_size: 7605\n",
      "batch 519, loss: 0.0087, instance_loss: 0.2021, weighted_loss: 0.0668, label: 0, bag_size: 1560\n",
      "batch 539, loss: 0.0155, instance_loss: 0.0002, weighted_loss: 0.0109, label: 0, bag_size: 9415\n",
      "batch 559, loss: 0.1173, instance_loss: 0.0014, weighted_loss: 0.0826, label: 0, bag_size: 16690\n",
      "batch 579, loss: 0.0623, instance_loss: 0.0008, weighted_loss: 0.0439, label: 1, bag_size: 10848\n",
      "batch 599, loss: 0.0267, instance_loss: 0.2600, weighted_loss: 0.0967, label: 0, bag_size: 1370\n",
      "batch 619, loss: 0.0065, instance_loss: 0.1248, weighted_loss: 0.0420, label: 0, bag_size: 2282\n",
      "batch 639, loss: 0.0169, instance_loss: 0.2700, weighted_loss: 0.0928, label: 1, bag_size: 2695\n",
      "batch 659, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 17633\n",
      "batch 679, loss: 0.1339, instance_loss: 0.1551, weighted_loss: 0.1403, label: 0, bag_size: 8549\n",
      "batch 699, loss: 0.0003, instance_loss: 0.0008, weighted_loss: 0.0004, label: 1, bag_size: 15233\n",
      "batch 719, loss: 0.6852, instance_loss: 0.1652, weighted_loss: 0.5292, label: 1, bag_size: 1845\n",
      "batch 739, loss: 0.0007, instance_loss: 0.0102, weighted_loss: 0.0036, label: 1, bag_size: 19932\n",
      "batch 759, loss: 0.0681, instance_loss: 0.0000, weighted_loss: 0.0477, label: 1, bag_size: 16565\n",
      "batch 779, loss: 0.0609, instance_loss: 0.0000, weighted_loss: 0.0426, label: 0, bag_size: 14333\n",
      "batch 799, loss: 0.0972, instance_loss: 0.5377, weighted_loss: 0.2293, label: 1, bag_size: 6478\n",
      "batch 819, loss: 0.0031, instance_loss: 0.0642, weighted_loss: 0.0214, label: 1, bag_size: 689\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9823170731707317: correct 12888/13120\n",
      "class 1 clustering acc 0.9045731707317073: correct 5934/6560\n",
      "Epoch: 46, train_loss: 0.1326, train_clustering_loss:  0.1727, train_error: 0.0476\n",
      "class 0: acc 0.9516908212560387, correct 394/414\n",
      "class 1: acc 0.9532019704433498, correct 387/406\n",
      "\n",
      "Val Set, val_loss: 0.2033, val_error: 0.1091, auc: 0.9901\n",
      "class 0 clustering acc 0.9539772727272727: correct 1679/1760\n",
      "class 1 clustering acc 0.8204545454545454: correct 722/880\n",
      "class 0: acc 0.7884615384615384, correct 41/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 2244\n",
      "batch 39, loss: 0.1343, instance_loss: 0.1192, weighted_loss: 0.1298, label: 1, bag_size: 5256\n",
      "batch 59, loss: 0.0515, instance_loss: 0.0005, weighted_loss: 0.0362, label: 1, bag_size: 7371\n",
      "batch 79, loss: 0.0042, instance_loss: 0.0057, weighted_loss: 0.0047, label: 1, bag_size: 12758\n",
      "batch 99, loss: 0.0387, instance_loss: 0.0462, weighted_loss: 0.0410, label: 1, bag_size: 6927\n",
      "batch 119, loss: 0.5784, instance_loss: 0.4294, weighted_loss: 0.5337, label: 1, bag_size: 1845\n",
      "batch 139, loss: 0.0009, instance_loss: 0.2623, weighted_loss: 0.0793, label: 1, bag_size: 2455\n",
      "batch 159, loss: 0.0099, instance_loss: 0.0151, weighted_loss: 0.0115, label: 0, bag_size: 1213\n",
      "batch 179, loss: 0.0062, instance_loss: 0.0105, weighted_loss: 0.0075, label: 0, bag_size: 9888\n",
      "batch 199, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 10920\n",
      "batch 219, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 4465\n",
      "batch 239, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 1101\n",
      "batch 259, loss: 0.2562, instance_loss: 0.0234, weighted_loss: 0.1864, label: 1, bag_size: 1919\n",
      "batch 279, loss: 0.6590, instance_loss: 1.1859, weighted_loss: 0.8171, label: 1, bag_size: 11729\n",
      "batch 299, loss: 0.0066, instance_loss: 0.0000, weighted_loss: 0.0046, label: 0, bag_size: 13339\n",
      "batch 319, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 18225\n",
      "batch 339, loss: 0.0400, instance_loss: 0.3676, weighted_loss: 0.1382, label: 0, bag_size: 1588\n",
      "batch 359, loss: 0.2363, instance_loss: 0.0000, weighted_loss: 0.1654, label: 0, bag_size: 10029\n",
      "batch 379, loss: 0.1829, instance_loss: 0.0007, weighted_loss: 0.1283, label: 1, bag_size: 21252\n",
      "batch 399, loss: 0.0666, instance_loss: 0.0564, weighted_loss: 0.0635, label: 0, bag_size: 11922\n",
      "batch 419, loss: 0.0250, instance_loss: 0.1303, weighted_loss: 0.0566, label: 1, bag_size: 7110\n",
      "batch 439, loss: 0.0020, instance_loss: 0.0000, weighted_loss: 0.0014, label: 0, bag_size: 23796\n",
      "batch 459, loss: 0.0119, instance_loss: 0.0050, weighted_loss: 0.0099, label: 0, bag_size: 1438\n",
      "batch 479, loss: 0.0030, instance_loss: 0.0690, weighted_loss: 0.0228, label: 0, bag_size: 10444\n",
      "batch 499, loss: 0.0282, instance_loss: 0.0005, weighted_loss: 0.0199, label: 0, bag_size: 18954\n",
      "batch 519, loss: 0.0698, instance_loss: 0.0002, weighted_loss: 0.0489, label: 0, bag_size: 11922\n",
      "batch 539, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 4259\n",
      "batch 559, loss: 0.1234, instance_loss: 0.0000, weighted_loss: 0.0864, label: 1, bag_size: 13692\n",
      "batch 579, loss: 0.0034, instance_loss: 0.0001, weighted_loss: 0.0024, label: 1, bag_size: 1244\n",
      "batch 599, loss: 0.0019, instance_loss: 0.1393, weighted_loss: 0.0431, label: 0, bag_size: 2179\n",
      "batch 619, loss: 0.0794, instance_loss: 0.0009, weighted_loss: 0.0558, label: 1, bag_size: 5137\n",
      "batch 639, loss: 0.3334, instance_loss: 0.0014, weighted_loss: 0.2338, label: 1, bag_size: 5903\n",
      "batch 659, loss: 0.3171, instance_loss: 0.0059, weighted_loss: 0.2238, label: 0, bag_size: 1732\n",
      "batch 679, loss: 0.0046, instance_loss: 0.2781, weighted_loss: 0.0866, label: 0, bag_size: 7381\n",
      "batch 699, loss: 0.0008, instance_loss: 0.0001, weighted_loss: 0.0006, label: 1, bag_size: 6453\n",
      "batch 719, loss: 0.0380, instance_loss: 0.3946, weighted_loss: 0.1450, label: 1, bag_size: 4939\n",
      "batch 739, loss: 0.0011, instance_loss: 0.0042, weighted_loss: 0.0020, label: 0, bag_size: 8812\n",
      "batch 759, loss: 0.1084, instance_loss: 1.2606, weighted_loss: 0.4540, label: 1, bag_size: 1963\n",
      "batch 779, loss: 0.0010, instance_loss: 1.4302, weighted_loss: 0.4297, label: 0, bag_size: 11759\n",
      "batch 799, loss: 0.4778, instance_loss: 1.6866, weighted_loss: 0.8404, label: 1, bag_size: 16514\n",
      "batch 819, loss: 0.1342, instance_loss: 1.2723, weighted_loss: 0.4756, label: 1, bag_size: 1242\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9808689024390244: correct 12869/13120\n",
      "class 1 clustering acc 0.8983231707317073: correct 5893/6560\n",
      "Epoch: 47, train_loss: 0.1961, train_clustering_loss:  0.1949, train_error: 0.0756\n",
      "class 0: acc 0.9214285714285714, correct 387/420\n",
      "class 1: acc 0.9275, correct 371/400\n",
      "\n",
      "Val Set, val_loss: 0.1657, val_error: 0.0818, auc: 0.9901\n",
      "class 0 clustering acc 0.9448863636363637: correct 1663/1760\n",
      "class 1 clustering acc 0.8534090909090909: correct 751/880\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1391, instance_loss: 0.4666, weighted_loss: 0.2373, label: 1, bag_size: 7989\n",
      "batch 39, loss: 0.0149, instance_loss: 0.1334, weighted_loss: 0.0505, label: 0, bag_size: 15003\n",
      "batch 59, loss: 0.0206, instance_loss: 0.0000, weighted_loss: 0.0144, label: 1, bag_size: 13015\n",
      "batch 79, loss: 0.0048, instance_loss: 0.0000, weighted_loss: 0.0033, label: 1, bag_size: 13368\n",
      "batch 99, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 5612\n",
      "batch 119, loss: 0.0195, instance_loss: 0.0000, weighted_loss: 0.0136, label: 1, bag_size: 15464\n",
      "batch 139, loss: 0.0019, instance_loss: 0.0000, weighted_loss: 0.0013, label: 1, bag_size: 7110\n",
      "batch 159, loss: 0.0110, instance_loss: 0.1461, weighted_loss: 0.0515, label: 1, bag_size: 1459\n",
      "batch 179, loss: 0.5670, instance_loss: 0.1045, weighted_loss: 0.4282, label: 0, bag_size: 24382\n",
      "batch 199, loss: 0.0124, instance_loss: 0.0000, weighted_loss: 0.0087, label: 1, bag_size: 13026\n",
      "batch 219, loss: 0.0138, instance_loss: 0.0114, weighted_loss: 0.0131, label: 1, bag_size: 7110\n",
      "batch 239, loss: 0.0726, instance_loss: 0.4464, weighted_loss: 0.1847, label: 1, bag_size: 11386\n",
      "batch 259, loss: 0.3604, instance_loss: 0.3443, weighted_loss: 0.3556, label: 0, bag_size: 1814\n",
      "batch 279, loss: 0.0030, instance_loss: 0.0000, weighted_loss: 0.0021, label: 0, bag_size: 9387\n",
      "batch 299, loss: 2.3313, instance_loss: 0.2063, weighted_loss: 1.6938, label: 0, bag_size: 5105\n",
      "batch 319, loss: 0.3573, instance_loss: 0.0751, weighted_loss: 0.2726, label: 1, bag_size: 8103\n",
      "batch 339, loss: 0.0008, instance_loss: 0.0044, weighted_loss: 0.0019, label: 1, bag_size: 3453\n",
      "batch 359, loss: 0.0037, instance_loss: 0.0633, weighted_loss: 0.0216, label: 0, bag_size: 705\n",
      "batch 379, loss: 0.0063, instance_loss: 0.0098, weighted_loss: 0.0073, label: 0, bag_size: 4959\n",
      "batch 399, loss: 0.0042, instance_loss: 0.0000, weighted_loss: 0.0029, label: 0, bag_size: 7989\n",
      "batch 419, loss: 0.0195, instance_loss: 0.0000, weighted_loss: 0.0137, label: 1, bag_size: 21827\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9610\n",
      "batch 459, loss: 0.0106, instance_loss: 0.0731, weighted_loss: 0.0293, label: 0, bag_size: 2296\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11477\n",
      "batch 499, loss: 1.9235, instance_loss: 0.3229, weighted_loss: 1.4433, label: 1, bag_size: 2565\n",
      "batch 519, loss: 1.1907, instance_loss: 0.2232, weighted_loss: 0.9004, label: 0, bag_size: 2219\n",
      "batch 539, loss: 0.1148, instance_loss: 0.0068, weighted_loss: 0.0824, label: 1, bag_size: 8012\n",
      "batch 559, loss: 0.0086, instance_loss: 0.0000, weighted_loss: 0.0060, label: 1, bag_size: 16417\n",
      "batch 579, loss: 0.2346, instance_loss: 0.0181, weighted_loss: 0.1697, label: 1, bag_size: 11729\n",
      "batch 599, loss: 0.0133, instance_loss: 0.0937, weighted_loss: 0.0374, label: 0, bag_size: 1920\n",
      "batch 619, loss: 0.0208, instance_loss: 0.0007, weighted_loss: 0.0148, label: 1, bag_size: 2356\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 7078\n",
      "batch 659, loss: 0.0252, instance_loss: 0.0720, weighted_loss: 0.0393, label: 1, bag_size: 10501\n",
      "batch 679, loss: 0.0002, instance_loss: 0.0723, weighted_loss: 0.0218, label: 0, bag_size: 17791\n",
      "batch 699, loss: 0.0059, instance_loss: 0.0007, weighted_loss: 0.0044, label: 1, bag_size: 6745\n",
      "batch 719, loss: 0.2961, instance_loss: 0.0411, weighted_loss: 0.2196, label: 1, bag_size: 1764\n",
      "batch 739, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 22828\n",
      "batch 759, loss: 0.0200, instance_loss: 0.0062, weighted_loss: 0.0158, label: 0, bag_size: 2195\n",
      "batch 779, loss: 0.0058, instance_loss: 0.8773, weighted_loss: 0.2672, label: 0, bag_size: 12687\n",
      "batch 799, loss: 0.0040, instance_loss: 0.0316, weighted_loss: 0.0123, label: 1, bag_size: 12611\n",
      "batch 819, loss: 0.0676, instance_loss: 0.0000, weighted_loss: 0.0473, label: 1, bag_size: 11256\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9841463414634146: correct 12912/13120\n",
      "class 1 clustering acc 0.9207317073170732: correct 6040/6560\n",
      "Epoch: 48, train_loss: 0.1609, train_clustering_loss:  0.1588, train_error: 0.0573\n",
      "class 0: acc 0.9391727493917275, correct 386/411\n",
      "class 1: acc 0.9462102689486552, correct 387/409\n",
      "\n",
      "Val Set, val_loss: 0.2128, val_error: 0.0909, auc: 0.9914\n",
      "class 0 clustering acc 0.9545454545454546: correct 1680/1760\n",
      "class 1 clustering acc 0.8306818181818182: correct 731/880\n",
      "class 0: acc 0.8076923076923077, correct 42/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0037, instance_loss: 0.0070, weighted_loss: 0.0047, label: 1, bag_size: 10482\n",
      "batch 39, loss: 0.0012, instance_loss: 0.0327, weighted_loss: 0.0106, label: 1, bag_size: 9321\n",
      "batch 59, loss: 0.8850, instance_loss: 0.2365, weighted_loss: 0.6905, label: 1, bag_size: 6360\n",
      "batch 79, loss: 0.0206, instance_loss: 0.0064, weighted_loss: 0.0164, label: 1, bag_size: 7798\n",
      "batch 99, loss: 0.1107, instance_loss: 0.0150, weighted_loss: 0.0820, label: 1, bag_size: 1493\n",
      "batch 119, loss: 0.0020, instance_loss: 0.0000, weighted_loss: 0.0014, label: 1, bag_size: 6966\n",
      "batch 139, loss: 0.0182, instance_loss: 0.0000, weighted_loss: 0.0127, label: 0, bag_size: 9415\n",
      "batch 159, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 3459\n",
      "batch 179, loss: 0.8979, instance_loss: 0.0000, weighted_loss: 0.6285, label: 0, bag_size: 15672\n",
      "batch 199, loss: 0.1270, instance_loss: 0.1313, weighted_loss: 0.1283, label: 1, bag_size: 8191\n",
      "batch 219, loss: 0.0122, instance_loss: 0.0004, weighted_loss: 0.0086, label: 0, bag_size: 13880\n",
      "batch 239, loss: 0.0028, instance_loss: 0.0000, weighted_loss: 0.0020, label: 0, bag_size: 19470\n",
      "batch 259, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 19808\n",
      "batch 279, loss: 0.0129, instance_loss: 0.0011, weighted_loss: 0.0093, label: 1, bag_size: 1014\n",
      "batch 299, loss: 0.0040, instance_loss: 0.0007, weighted_loss: 0.0030, label: 1, bag_size: 2662\n",
      "batch 319, loss: 0.0579, instance_loss: 0.0000, weighted_loss: 0.0406, label: 1, bag_size: 3683\n",
      "batch 339, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 18154\n",
      "batch 359, loss: 0.0101, instance_loss: 0.5004, weighted_loss: 0.1572, label: 1, bag_size: 928\n",
      "batch 379, loss: 0.0988, instance_loss: 0.0538, weighted_loss: 0.0853, label: 1, bag_size: 2682\n",
      "batch 399, loss: 0.1270, instance_loss: 0.3845, weighted_loss: 0.2043, label: 0, bag_size: 6884\n",
      "batch 419, loss: 0.0731, instance_loss: 0.1389, weighted_loss: 0.0928, label: 1, bag_size: 1015\n",
      "batch 439, loss: 0.0337, instance_loss: 0.0009, weighted_loss: 0.0239, label: 1, bag_size: 2638\n",
      "batch 459, loss: 0.0517, instance_loss: 0.0000, weighted_loss: 0.0362, label: 1, bag_size: 8466\n",
      "batch 479, loss: 0.1587, instance_loss: 0.6103, weighted_loss: 0.2942, label: 0, bag_size: 1149\n",
      "batch 499, loss: 0.0933, instance_loss: 0.3033, weighted_loss: 0.1563, label: 0, bag_size: 3444\n",
      "batch 519, loss: 0.0521, instance_loss: 0.2369, weighted_loss: 0.1075, label: 1, bag_size: 3450\n",
      "batch 539, loss: 0.0035, instance_loss: 0.0182, weighted_loss: 0.0079, label: 1, bag_size: 10392\n",
      "batch 559, loss: 0.0092, instance_loss: 0.4652, weighted_loss: 0.1460, label: 1, bag_size: 2785\n",
      "batch 579, loss: 0.0069, instance_loss: 0.0291, weighted_loss: 0.0136, label: 1, bag_size: 2495\n",
      "batch 599, loss: 0.0173, instance_loss: 0.7434, weighted_loss: 0.2351, label: 0, bag_size: 2091\n",
      "batch 619, loss: 0.0225, instance_loss: 0.0312, weighted_loss: 0.0251, label: 0, bag_size: 17630\n",
      "batch 639, loss: 0.0021, instance_loss: 0.0597, weighted_loss: 0.0194, label: 0, bag_size: 1588\n",
      "batch 659, loss: 0.1219, instance_loss: 0.1386, weighted_loss: 0.1269, label: 1, bag_size: 9004\n",
      "batch 679, loss: 0.0020, instance_loss: 0.0010, weighted_loss: 0.0017, label: 1, bag_size: 5864\n",
      "batch 699, loss: 0.4061, instance_loss: 0.2942, weighted_loss: 0.3725, label: 0, bag_size: 1789\n",
      "batch 719, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 9234\n",
      "batch 739, loss: 0.0019, instance_loss: 0.0037, weighted_loss: 0.0024, label: 0, bag_size: 6898\n",
      "batch 759, loss: 0.0024, instance_loss: 0.0907, weighted_loss: 0.0289, label: 1, bag_size: 3980\n",
      "batch 779, loss: 0.0147, instance_loss: 2.6867, weighted_loss: 0.8163, label: 1, bag_size: 34356\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 10592\n",
      "batch 819, loss: 0.0066, instance_loss: 0.1529, weighted_loss: 0.0505, label: 0, bag_size: 10444\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9785823170731708: correct 12839/13120\n",
      "class 1 clustering acc 0.9077743902439024: correct 5955/6560\n",
      "Epoch: 49, train_loss: 0.1683, train_clustering_loss:  0.1992, train_error: 0.0537\n",
      "class 0: acc 0.9354005167958657, correct 362/387\n",
      "class 1: acc 0.9561200923787528, correct 414/433\n",
      "\n",
      "Val Set, val_loss: 0.1821, val_error: 0.0727, auc: 0.9914\n",
      "class 0 clustering acc 0.9534090909090909: correct 1678/1760\n",
      "class 1 clustering acc 0.8943181818181818: correct 787/880\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.3521, instance_loss: 1.5689, weighted_loss: 0.7172, label: 0, bag_size: 1637\n",
      "batch 39, loss: 0.2656, instance_loss: 0.0059, weighted_loss: 0.1877, label: 1, bag_size: 5921\n",
      "batch 59, loss: 2.2310, instance_loss: 2.9647, weighted_loss: 2.4511, label: 1, bag_size: 15563\n",
      "batch 79, loss: 2.7478, instance_loss: 0.0813, weighted_loss: 1.9479, label: 0, bag_size: 5120\n",
      "batch 99, loss: 0.0886, instance_loss: 0.0059, weighted_loss: 0.0638, label: 1, bag_size: 10501\n",
      "batch 119, loss: 0.0169, instance_loss: 0.0360, weighted_loss: 0.0227, label: 0, bag_size: 9415\n",
      "batch 139, loss: 0.0386, instance_loss: 0.0091, weighted_loss: 0.0298, label: 1, bag_size: 5256\n",
      "batch 159, loss: 0.0006, instance_loss: 0.0147, weighted_loss: 0.0048, label: 0, bag_size: 8252\n",
      "batch 179, loss: 0.0014, instance_loss: 0.1917, weighted_loss: 0.0585, label: 0, bag_size: 1712\n",
      "batch 199, loss: 0.0004, instance_loss: 0.0093, weighted_loss: 0.0031, label: 0, bag_size: 10791\n",
      "batch 219, loss: 0.0088, instance_loss: 0.0046, weighted_loss: 0.0075, label: 0, bag_size: 12201\n",
      "batch 239, loss: 0.0745, instance_loss: 0.0738, weighted_loss: 0.0743, label: 0, bag_size: 3502\n",
      "batch 259, loss: 0.2747, instance_loss: 0.1080, weighted_loss: 0.2247, label: 1, bag_size: 1822\n",
      "batch 279, loss: 0.0024, instance_loss: 0.0000, weighted_loss: 0.0017, label: 1, bag_size: 19039\n",
      "batch 299, loss: 0.0321, instance_loss: 0.0070, weighted_loss: 0.0246, label: 1, bag_size: 1759\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 10920\n",
      "batch 339, loss: 0.3697, instance_loss: 0.4366, weighted_loss: 0.3897, label: 1, bag_size: 1831\n",
      "batch 359, loss: 0.0029, instance_loss: 0.1089, weighted_loss: 0.0347, label: 0, bag_size: 1560\n",
      "batch 379, loss: 0.0070, instance_loss: 0.4509, weighted_loss: 0.1402, label: 0, bag_size: 2814\n",
      "batch 399, loss: 0.0045, instance_loss: 0.0000, weighted_loss: 0.0032, label: 1, bag_size: 10105\n",
      "batch 419, loss: 0.0064, instance_loss: 0.0000, weighted_loss: 0.0045, label: 1, bag_size: 4821\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0007, weighted_loss: 0.0002, label: 1, bag_size: 1412\n",
      "batch 459, loss: 0.0635, instance_loss: 0.0000, weighted_loss: 0.0445, label: 1, bag_size: 34356\n",
      "batch 479, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 14604\n",
      "batch 499, loss: 0.0301, instance_loss: 0.0295, weighted_loss: 0.0299, label: 1, bag_size: 10072\n",
      "batch 519, loss: 0.0072, instance_loss: 0.0000, weighted_loss: 0.0050, label: 0, bag_size: 2036\n",
      "batch 539, loss: 0.0054, instance_loss: 0.0006, weighted_loss: 0.0039, label: 0, bag_size: 9471\n",
      "batch 559, loss: 1.4909, instance_loss: 0.6382, weighted_loss: 1.2351, label: 1, bag_size: 13367\n",
      "batch 579, loss: 0.0002, instance_loss: 0.0013, weighted_loss: 0.0005, label: 0, bag_size: 11735\n",
      "batch 599, loss: 0.0003, instance_loss: 0.0022, weighted_loss: 0.0009, label: 1, bag_size: 14515\n",
      "batch 619, loss: 0.0172, instance_loss: 1.4830, weighted_loss: 0.4569, label: 1, bag_size: 2455\n",
      "batch 639, loss: 0.0093, instance_loss: 0.0862, weighted_loss: 0.0323, label: 0, bag_size: 3893\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 9885\n",
      "batch 679, loss: 0.6223, instance_loss: 0.0071, weighted_loss: 0.4378, label: 1, bag_size: 12714\n",
      "batch 699, loss: 0.0592, instance_loss: 0.3017, weighted_loss: 0.1319, label: 1, bag_size: 1015\n",
      "batch 719, loss: 0.6330, instance_loss: 0.1970, weighted_loss: 0.5022, label: 0, bag_size: 1127\n",
      "batch 739, loss: 0.0017, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 21093\n",
      "batch 759, loss: 0.0041, instance_loss: 0.1081, weighted_loss: 0.0353, label: 1, bag_size: 4308\n",
      "batch 779, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 17486\n",
      "batch 799, loss: 0.0098, instance_loss: 0.0000, weighted_loss: 0.0068, label: 1, bag_size: 10033\n",
      "batch 819, loss: 0.8599, instance_loss: 0.0363, weighted_loss: 0.6128, label: 1, bag_size: 9162\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.984375: correct 12915/13120\n",
      "class 1 clustering acc 0.9288109756097561: correct 6093/6560\n",
      "Epoch: 50, train_loss: 0.1512, train_clustering_loss:  0.1570, train_error: 0.0549\n",
      "class 0: acc 0.9410377358490566, correct 399/424\n",
      "class 1: acc 0.9494949494949495, correct 376/396\n",
      "\n",
      "Val Set, val_loss: 0.2935, val_error: 0.1455, auc: 0.9927\n",
      "class 0 clustering acc 0.9477272727272728: correct 1668/1760\n",
      "class 1 clustering acc 0.8715909090909091: correct 767/880\n",
      "class 0: acc 0.6923076923076923, correct 36/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0021, instance_loss: 0.0015, weighted_loss: 0.0019, label: 1, bag_size: 6752\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 30751\n",
      "batch 59, loss: 0.2540, instance_loss: 0.1200, weighted_loss: 0.2138, label: 0, bag_size: 2270\n",
      "batch 79, loss: 0.0001, instance_loss: 0.0531, weighted_loss: 0.0160, label: 0, bag_size: 3787\n",
      "batch 99, loss: 0.0045, instance_loss: 1.1178, weighted_loss: 0.3385, label: 0, bag_size: 3101\n",
      "batch 119, loss: 2.2708, instance_loss: 1.9113, weighted_loss: 2.1629, label: 1, bag_size: 9162\n",
      "batch 139, loss: 0.0741, instance_loss: 0.0607, weighted_loss: 0.0701, label: 1, bag_size: 1014\n",
      "batch 159, loss: 0.0255, instance_loss: 0.0352, weighted_loss: 0.0284, label: 1, bag_size: 7669\n",
      "batch 179, loss: 0.0128, instance_loss: 0.0005, weighted_loss: 0.0091, label: 1, bag_size: 3224\n",
      "batch 199, loss: 0.0013, instance_loss: 0.0004, weighted_loss: 0.0010, label: 0, bag_size: 21093\n",
      "batch 219, loss: 1.3074, instance_loss: 0.2290, weighted_loss: 0.9838, label: 0, bag_size: 8420\n",
      "batch 239, loss: 0.1295, instance_loss: 0.3403, weighted_loss: 0.1927, label: 1, bag_size: 1919\n",
      "batch 259, loss: 0.0032, instance_loss: 0.0460, weighted_loss: 0.0160, label: 1, bag_size: 12611\n",
      "batch 279, loss: 0.0076, instance_loss: 0.0167, weighted_loss: 0.0103, label: 1, bag_size: 2495\n",
      "batch 299, loss: 0.1970, instance_loss: 0.1958, weighted_loss: 0.1967, label: 1, bag_size: 4956\n",
      "batch 319, loss: 0.0494, instance_loss: 0.0000, weighted_loss: 0.0346, label: 1, bag_size: 25970\n",
      "batch 339, loss: 0.0036, instance_loss: 0.0019, weighted_loss: 0.0031, label: 0, bag_size: 12796\n",
      "batch 359, loss: 0.0170, instance_loss: 0.0678, weighted_loss: 0.0323, label: 0, bag_size: 1614\n",
      "batch 379, loss: 0.4881, instance_loss: 0.2337, weighted_loss: 0.4117, label: 1, bag_size: 6360\n",
      "batch 399, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 12217\n",
      "batch 419, loss: 0.0028, instance_loss: 0.9526, weighted_loss: 0.2878, label: 0, bag_size: 8330\n",
      "batch 439, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 1962\n",
      "batch 459, loss: 0.0129, instance_loss: 0.0044, weighted_loss: 0.0104, label: 1, bag_size: 7424\n",
      "batch 479, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 11546\n",
      "batch 499, loss: 0.0028, instance_loss: 0.0000, weighted_loss: 0.0020, label: 1, bag_size: 13051\n",
      "batch 519, loss: 0.0001, instance_loss: 0.1101, weighted_loss: 0.0331, label: 0, bag_size: 3787\n",
      "batch 539, loss: 0.0091, instance_loss: 0.0000, weighted_loss: 0.0064, label: 0, bag_size: 5965\n",
      "batch 559, loss: 0.0958, instance_loss: 0.0003, weighted_loss: 0.0671, label: 1, bag_size: 12180\n",
      "batch 579, loss: 0.0056, instance_loss: 0.0000, weighted_loss: 0.0039, label: 0, bag_size: 10146\n",
      "batch 599, loss: 0.0050, instance_loss: 0.0000, weighted_loss: 0.0035, label: 0, bag_size: 18045\n",
      "batch 619, loss: 0.1213, instance_loss: 0.1216, weighted_loss: 0.1214, label: 0, bag_size: 1142\n",
      "batch 639, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 9644\n",
      "batch 659, loss: 0.0815, instance_loss: 1.4283, weighted_loss: 0.4856, label: 1, bag_size: 6478\n",
      "batch 679, loss: 0.0195, instance_loss: 0.0000, weighted_loss: 0.0136, label: 0, bag_size: 5551\n",
      "batch 699, loss: 0.0525, instance_loss: 0.0078, weighted_loss: 0.0391, label: 1, bag_size: 2179\n",
      "batch 719, loss: 0.0191, instance_loss: 0.0235, weighted_loss: 0.0204, label: 0, bag_size: 1800\n",
      "batch 739, loss: 0.0150, instance_loss: 0.0000, weighted_loss: 0.0105, label: 1, bag_size: 5340\n",
      "batch 759, loss: 0.5416, instance_loss: 0.1504, weighted_loss: 0.4243, label: 0, bag_size: 1127\n",
      "batch 779, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 18649\n",
      "batch 799, loss: 0.0172, instance_loss: 0.0312, weighted_loss: 0.0214, label: 1, bag_size: 3651\n",
      "batch 819, loss: 0.1655, instance_loss: 0.1133, weighted_loss: 0.1498, label: 1, bag_size: 7468\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9817073170731707: correct 12880/13120\n",
      "class 1 clustering acc 0.9221036585365854: correct 6049/6560\n",
      "Epoch: 51, train_loss: 0.1709, train_clustering_loss:  0.1612, train_error: 0.0622\n",
      "class 0: acc 0.9282178217821783, correct 375/404\n",
      "class 1: acc 0.9471153846153846, correct 394/416\n",
      "\n",
      "Val Set, val_loss: 0.1444, val_error: 0.0455, auc: 0.9914\n",
      "class 0 clustering acc 0.9664772727272727: correct 1701/1760\n",
      "class 1 clustering acc 0.8784090909090909: correct 773/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "Validation loss decreased (0.145951 --> 0.144369).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0161, instance_loss: 0.0019, weighted_loss: 0.0119, label: 0, bag_size: 10304\n",
      "batch 39, loss: 0.0112, instance_loss: 0.0200, weighted_loss: 0.0139, label: 0, bag_size: 3198\n",
      "batch 59, loss: 0.0588, instance_loss: 0.0007, weighted_loss: 0.0413, label: 1, bag_size: 9322\n",
      "batch 79, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 5225\n",
      "batch 99, loss: 0.0340, instance_loss: 0.3830, weighted_loss: 0.1387, label: 1, bag_size: 6478\n",
      "batch 119, loss: 0.0166, instance_loss: 0.0166, weighted_loss: 0.0166, label: 0, bag_size: 3444\n",
      "batch 139, loss: 0.0009, instance_loss: 0.0014, weighted_loss: 0.0011, label: 1, bag_size: 11421\n",
      "batch 159, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 6343\n",
      "batch 179, loss: 0.0468, instance_loss: 0.0058, weighted_loss: 0.0345, label: 1, bag_size: 13692\n",
      "batch 199, loss: 0.0160, instance_loss: 0.0126, weighted_loss: 0.0150, label: 0, bag_size: 11146\n",
      "batch 219, loss: 0.3920, instance_loss: 0.2484, weighted_loss: 0.3489, label: 0, bag_size: 2213\n",
      "batch 239, loss: 0.0141, instance_loss: 0.0000, weighted_loss: 0.0099, label: 1, bag_size: 11032\n",
      "batch 259, loss: 0.1257, instance_loss: 0.3823, weighted_loss: 0.2027, label: 1, bag_size: 9983\n",
      "batch 279, loss: 0.0008, instance_loss: 0.0394, weighted_loss: 0.0124, label: 0, bag_size: 9252\n",
      "batch 299, loss: 0.3045, instance_loss: 0.0060, weighted_loss: 0.2149, label: 1, bag_size: 9561\n",
      "batch 319, loss: 0.1124, instance_loss: 0.0418, weighted_loss: 0.0912, label: 0, bag_size: 3783\n",
      "batch 339, loss: 0.0337, instance_loss: 2.1317, weighted_loss: 0.6631, label: 1, bag_size: 2412\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 10920\n",
      "batch 379, loss: 0.1932, instance_loss: 0.1131, weighted_loss: 0.1692, label: 0, bag_size: 6884\n",
      "batch 399, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 30751\n",
      "batch 419, loss: 0.0018, instance_loss: 0.0000, weighted_loss: 0.0013, label: 0, bag_size: 2873\n",
      "batch 439, loss: 0.0253, instance_loss: 0.0310, weighted_loss: 0.0270, label: 0, bag_size: 5409\n",
      "batch 459, loss: 0.0682, instance_loss: 1.0899, weighted_loss: 0.3747, label: 0, bag_size: 931\n",
      "batch 479, loss: 0.3100, instance_loss: 0.0217, weighted_loss: 0.2235, label: 1, bag_size: 7768\n",
      "batch 499, loss: 0.0731, instance_loss: 0.0005, weighted_loss: 0.0513, label: 1, bag_size: 29832\n",
      "batch 519, loss: 0.0100, instance_loss: 0.0000, weighted_loss: 0.0070, label: 1, bag_size: 11363\n",
      "batch 539, loss: 0.0152, instance_loss: 0.0619, weighted_loss: 0.0292, label: 0, bag_size: 5009\n",
      "batch 559, loss: 0.0052, instance_loss: 0.0019, weighted_loss: 0.0042, label: 1, bag_size: 10501\n",
      "batch 579, loss: 0.5172, instance_loss: 0.9155, weighted_loss: 0.6367, label: 1, bag_size: 1831\n",
      "batch 599, loss: 0.0456, instance_loss: 0.0004, weighted_loss: 0.0321, label: 1, bag_size: 16565\n",
      "batch 619, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 12865\n",
      "batch 639, loss: 0.0013, instance_loss: 0.0001, weighted_loss: 0.0009, label: 0, bag_size: 11527\n",
      "batch 659, loss: 0.0022, instance_loss: 0.0000, weighted_loss: 0.0016, label: 0, bag_size: 11512\n",
      "batch 679, loss: 0.0126, instance_loss: 0.0001, weighted_loss: 0.0089, label: 1, bag_size: 11964\n",
      "batch 699, loss: 0.0230, instance_loss: 0.0000, weighted_loss: 0.0161, label: 0, bag_size: 24439\n",
      "batch 719, loss: 0.0059, instance_loss: 0.0008, weighted_loss: 0.0044, label: 1, bag_size: 3640\n",
      "batch 739, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 5551\n",
      "batch 759, loss: 1.9468, instance_loss: 0.0921, weighted_loss: 1.3904, label: 0, bag_size: 1437\n",
      "batch 779, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 6966\n",
      "batch 799, loss: 0.0039, instance_loss: 0.0732, weighted_loss: 0.0247, label: 0, bag_size: 8252\n",
      "batch 819, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 11187\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9841463414634146: correct 12912/13120\n",
      "class 1 clustering acc 0.9333841463414634: correct 6123/6560\n",
      "Epoch: 52, train_loss: 0.1371, train_clustering_loss:  0.1439, train_error: 0.0524\n",
      "class 0: acc 0.9504716981132075, correct 403/424\n",
      "class 1: acc 0.9444444444444444, correct 374/396\n",
      "\n",
      "Val Set, val_loss: 0.1364, val_error: 0.0545, auc: 0.9937\n",
      "class 0 clustering acc 0.9590909090909091: correct 1688/1760\n",
      "class 1 clustering acc 0.8465909090909091: correct 745/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9310344827586207, correct 54/58\n",
      "Validation loss decreased (0.144369 --> 0.136353).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.9751, instance_loss: 0.5367, weighted_loss: 0.8436, label: 1, bag_size: 771\n",
      "batch 39, loss: 0.0512, instance_loss: 0.5699, weighted_loss: 0.2068, label: 1, bag_size: 1867\n",
      "batch 59, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 11113\n",
      "batch 79, loss: 0.0143, instance_loss: 0.0002, weighted_loss: 0.0100, label: 1, bag_size: 8019\n",
      "batch 99, loss: 0.4863, instance_loss: 0.8574, weighted_loss: 0.5977, label: 1, bag_size: 1051\n",
      "batch 119, loss: 0.0345, instance_loss: 0.3337, weighted_loss: 0.1242, label: 0, bag_size: 1560\n",
      "batch 139, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 21218\n",
      "batch 159, loss: 0.0094, instance_loss: 0.0482, weighted_loss: 0.0211, label: 0, bag_size: 10898\n",
      "batch 179, loss: 0.4537, instance_loss: 0.0373, weighted_loss: 0.3288, label: 1, bag_size: 11316\n",
      "batch 199, loss: 0.0057, instance_loss: 0.0031, weighted_loss: 0.0049, label: 1, bag_size: 9062\n",
      "batch 219, loss: 0.0052, instance_loss: 0.0000, weighted_loss: 0.0036, label: 0, bag_size: 2044\n",
      "batch 239, loss: 0.0160, instance_loss: 0.0000, weighted_loss: 0.0112, label: 0, bag_size: 6898\n",
      "batch 259, loss: 0.0503, instance_loss: 0.0157, weighted_loss: 0.0399, label: 1, bag_size: 11386\n",
      "batch 279, loss: 0.0465, instance_loss: 0.0000, weighted_loss: 0.0326, label: 1, bag_size: 5690\n",
      "batch 299, loss: 0.0182, instance_loss: 0.0199, weighted_loss: 0.0187, label: 1, bag_size: 13692\n",
      "batch 319, loss: 0.1000, instance_loss: 0.4153, weighted_loss: 0.1946, label: 0, bag_size: 6884\n",
      "batch 339, loss: 0.1395, instance_loss: 0.2740, weighted_loss: 0.1799, label: 1, bag_size: 2842\n",
      "batch 359, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 11546\n",
      "batch 379, loss: 0.7286, instance_loss: 1.1635, weighted_loss: 0.8591, label: 1, bag_size: 2681\n",
      "batch 399, loss: 0.2094, instance_loss: 0.0003, weighted_loss: 0.1467, label: 1, bag_size: 10622\n",
      "batch 419, loss: 0.0489, instance_loss: 0.0209, weighted_loss: 0.0405, label: 1, bag_size: 3674\n",
      "batch 439, loss: 0.0046, instance_loss: 0.0001, weighted_loss: 0.0033, label: 1, bag_size: 12460\n",
      "batch 459, loss: 0.2273, instance_loss: 0.4356, weighted_loss: 0.2898, label: 1, bag_size: 1123\n",
      "batch 479, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 21404\n",
      "batch 499, loss: 0.0434, instance_loss: 0.0003, weighted_loss: 0.0305, label: 0, bag_size: 3725\n",
      "batch 519, loss: 0.0093, instance_loss: 0.0010, weighted_loss: 0.0068, label: 1, bag_size: 2146\n",
      "batch 539, loss: 0.0032, instance_loss: 0.0000, weighted_loss: 0.0022, label: 0, bag_size: 14305\n",
      "batch 559, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 10751\n",
      "batch 579, loss: 0.1280, instance_loss: 0.0000, weighted_loss: 0.0896, label: 1, bag_size: 7613\n",
      "batch 599, loss: 0.1844, instance_loss: 0.2703, weighted_loss: 0.2102, label: 1, bag_size: 1831\n",
      "batch 619, loss: 0.0495, instance_loss: 0.1846, weighted_loss: 0.0901, label: 0, bag_size: 1052\n",
      "batch 639, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 10146\n",
      "batch 659, loss: 0.0682, instance_loss: 1.3860, weighted_loss: 0.4636, label: 0, bag_size: 2814\n",
      "batch 679, loss: 0.0006, instance_loss: 0.0090, weighted_loss: 0.0031, label: 1, bag_size: 8602\n",
      "batch 699, loss: 0.0039, instance_loss: 0.0002, weighted_loss: 0.0028, label: 1, bag_size: 11032\n",
      "batch 719, loss: 0.2276, instance_loss: 0.0667, weighted_loss: 0.1793, label: 1, bag_size: 2681\n",
      "batch 739, loss: 0.0135, instance_loss: 0.0000, weighted_loss: 0.0094, label: 1, bag_size: 5629\n",
      "batch 759, loss: 0.0263, instance_loss: 0.0291, weighted_loss: 0.0271, label: 0, bag_size: 763\n",
      "batch 779, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 11727\n",
      "batch 799, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 14305\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11195\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9842987804878048: correct 12914/13120\n",
      "class 1 clustering acc 0.9320121951219512: correct 6114/6560\n",
      "Epoch: 53, train_loss: 0.1427, train_clustering_loss:  0.1432, train_error: 0.0573\n",
      "class 0: acc 0.9323308270676691, correct 372/399\n",
      "class 1: acc 0.9524940617577197, correct 401/421\n",
      "\n",
      "Val Set, val_loss: 0.3687, val_error: 0.1636, auc: 0.9947\n",
      "class 0 clustering acc 0.9244318181818182: correct 1627/1760\n",
      "class 1 clustering acc 0.8170454545454545: correct 719/880\n",
      "class 0: acc 0.6538461538461539, correct 34/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0649, instance_loss: 0.0243, weighted_loss: 0.0527, label: 0, bag_size: 3444\n",
      "batch 39, loss: 0.0090, instance_loss: 0.1362, weighted_loss: 0.0472, label: 0, bag_size: 7605\n",
      "batch 59, loss: 0.1544, instance_loss: 0.0000, weighted_loss: 0.1081, label: 0, bag_size: 20555\n",
      "batch 79, loss: 0.0253, instance_loss: 0.2965, weighted_loss: 0.1066, label: 1, bag_size: 549\n",
      "batch 99, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 11389\n",
      "batch 119, loss: 0.0019, instance_loss: 0.0000, weighted_loss: 0.0013, label: 1, bag_size: 9878\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 11981\n",
      "batch 159, loss: 0.0121, instance_loss: 0.0498, weighted_loss: 0.0234, label: 0, bag_size: 2091\n",
      "batch 179, loss: 0.0935, instance_loss: 0.0116, weighted_loss: 0.0689, label: 0, bag_size: 2732\n",
      "batch 199, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 15747\n",
      "batch 219, loss: 0.0064, instance_loss: 0.0015, weighted_loss: 0.0049, label: 0, bag_size: 16607\n",
      "batch 239, loss: 0.0060, instance_loss: 0.0000, weighted_loss: 0.0042, label: 1, bag_size: 4039\n",
      "batch 259, loss: 0.0074, instance_loss: 0.0225, weighted_loss: 0.0119, label: 0, bag_size: 1416\n",
      "batch 279, loss: 0.0671, instance_loss: 0.1209, weighted_loss: 0.0833, label: 1, bag_size: 5907\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 11195\n",
      "batch 319, loss: 0.0140, instance_loss: 0.0903, weighted_loss: 0.0369, label: 0, bag_size: 5009\n",
      "batch 339, loss: 0.0019, instance_loss: 0.0302, weighted_loss: 0.0104, label: 0, bag_size: 2382\n",
      "batch 359, loss: 0.0043, instance_loss: 0.1122, weighted_loss: 0.0367, label: 0, bag_size: 9930\n",
      "batch 379, loss: 0.0050, instance_loss: 0.0000, weighted_loss: 0.0035, label: 0, bag_size: 9949\n",
      "batch 399, loss: 0.0374, instance_loss: 0.0044, weighted_loss: 0.0275, label: 1, bag_size: 1924\n",
      "batch 419, loss: 0.0922, instance_loss: 0.0004, weighted_loss: 0.0646, label: 1, bag_size: 5137\n",
      "batch 439, loss: 0.0487, instance_loss: 0.0367, weighted_loss: 0.0451, label: 0, bag_size: 15464\n",
      "batch 459, loss: 0.0021, instance_loss: 0.0266, weighted_loss: 0.0094, label: 1, bag_size: 4423\n",
      "batch 479, loss: 0.0568, instance_loss: 0.0064, weighted_loss: 0.0417, label: 1, bag_size: 3674\n",
      "batch 499, loss: 0.0096, instance_loss: 0.0121, weighted_loss: 0.0103, label: 0, bag_size: 2534\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 22800\n",
      "batch 539, loss: 0.0074, instance_loss: 0.0858, weighted_loss: 0.0309, label: 1, bag_size: 7110\n",
      "batch 559, loss: 0.0402, instance_loss: 0.2957, weighted_loss: 0.1169, label: 1, bag_size: 1525\n",
      "batch 579, loss: 2.3120, instance_loss: 0.6739, weighted_loss: 1.8206, label: 1, bag_size: 1533\n",
      "batch 599, loss: 0.0008, instance_loss: 0.0010, weighted_loss: 0.0009, label: 1, bag_size: 10725\n",
      "batch 619, loss: 2.3833, instance_loss: 0.1770, weighted_loss: 1.7214, label: 0, bag_size: 14664\n",
      "batch 639, loss: 0.0598, instance_loss: 0.2648, weighted_loss: 0.1213, label: 0, bag_size: 1772\n",
      "batch 659, loss: 0.0058, instance_loss: 0.0000, weighted_loss: 0.0041, label: 0, bag_size: 16211\n",
      "batch 679, loss: 2.1239, instance_loss: 1.4115, weighted_loss: 1.9102, label: 1, bag_size: 13367\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 12349\n",
      "batch 719, loss: 0.2993, instance_loss: 0.4236, weighted_loss: 0.3366, label: 0, bag_size: 1814\n",
      "batch 739, loss: 0.0229, instance_loss: 0.0000, weighted_loss: 0.0160, label: 1, bag_size: 16379\n",
      "batch 759, loss: 0.0016, instance_loss: 0.3178, weighted_loss: 0.0964, label: 0, bag_size: 10263\n",
      "batch 779, loss: 0.0711, instance_loss: 0.0000, weighted_loss: 0.0497, label: 1, bag_size: 30675\n",
      "batch 799, loss: 0.2266, instance_loss: 0.0297, weighted_loss: 0.1675, label: 1, bag_size: 4786\n",
      "batch 819, loss: 0.0693, instance_loss: 0.0023, weighted_loss: 0.0492, label: 1, bag_size: 16379\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9805640243902439: correct 12865/13120\n",
      "class 1 clustering acc 0.9094512195121951: correct 5966/6560\n",
      "Epoch: 54, train_loss: 0.1559, train_clustering_loss:  0.1843, train_error: 0.0585\n",
      "class 0: acc 0.9345794392523364, correct 400/428\n",
      "class 1: acc 0.9489795918367347, correct 372/392\n",
      "\n",
      "Val Set, val_loss: 0.1235, val_error: 0.0364, auc: 0.9940\n",
      "class 0 clustering acc 0.9568181818181818: correct 1684/1760\n",
      "class 1 clustering acc 0.8772727272727273: correct 772/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "Validation loss decreased (0.136353 --> 0.123507).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0017, instance_loss: 0.0140, weighted_loss: 0.0054, label: 0, bag_size: 14305\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 5731\n",
      "batch 59, loss: 0.0649, instance_loss: 0.0448, weighted_loss: 0.0589, label: 0, bag_size: 9387\n",
      "batch 79, loss: 0.8686, instance_loss: 0.0250, weighted_loss: 0.6155, label: 0, bag_size: 2160\n",
      "batch 99, loss: 0.0376, instance_loss: 0.0305, weighted_loss: 0.0354, label: 1, bag_size: 16379\n",
      "batch 119, loss: 0.0130, instance_loss: 0.0740, weighted_loss: 0.0313, label: 0, bag_size: 12731\n",
      "batch 139, loss: 0.0021, instance_loss: 0.0106, weighted_loss: 0.0047, label: 0, bag_size: 1072\n",
      "batch 159, loss: 0.0187, instance_loss: 0.0000, weighted_loss: 0.0131, label: 1, bag_size: 11160\n",
      "batch 179, loss: 0.9394, instance_loss: 2.2240, weighted_loss: 1.3248, label: 1, bag_size: 1703\n",
      "batch 199, loss: 0.0408, instance_loss: 0.0000, weighted_loss: 0.0286, label: 0, bag_size: 20555\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 8898\n",
      "batch 239, loss: 0.0047, instance_loss: 0.0000, weighted_loss: 0.0033, label: 0, bag_size: 3541\n",
      "batch 259, loss: 0.0003, instance_loss: 0.3368, weighted_loss: 0.1013, label: 1, bag_size: 10112\n",
      "batch 279, loss: 2.2772, instance_loss: 0.4673, weighted_loss: 1.7343, label: 0, bag_size: 3897\n",
      "batch 299, loss: 0.0139, instance_loss: 0.0041, weighted_loss: 0.0110, label: 1, bag_size: 10028\n",
      "batch 319, loss: 0.0012, instance_loss: 0.0743, weighted_loss: 0.0231, label: 0, bag_size: 14956\n",
      "batch 339, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 11383\n",
      "batch 359, loss: 0.0034, instance_loss: 0.6359, weighted_loss: 0.1932, label: 0, bag_size: 8755\n",
      "batch 379, loss: 0.1885, instance_loss: 0.0203, weighted_loss: 0.1381, label: 0, bag_size: 10410\n",
      "batch 399, loss: 0.1761, instance_loss: 0.0023, weighted_loss: 0.1239, label: 1, bag_size: 6927\n",
      "batch 419, loss: 0.0015, instance_loss: 0.1891, weighted_loss: 0.0578, label: 0, bag_size: 14377\n",
      "batch 439, loss: 0.0084, instance_loss: 1.2127, weighted_loss: 0.3697, label: 0, bag_size: 17268\n",
      "batch 459, loss: 0.0150, instance_loss: 0.0271, weighted_loss: 0.0186, label: 0, bag_size: 7823\n",
      "batch 479, loss: 0.2405, instance_loss: 0.9639, weighted_loss: 0.4576, label: 1, bag_size: 1683\n",
      "batch 499, loss: 0.1761, instance_loss: 0.0268, weighted_loss: 0.1313, label: 1, bag_size: 4786\n",
      "batch 519, loss: 0.0101, instance_loss: 0.0481, weighted_loss: 0.0215, label: 0, bag_size: 10444\n",
      "batch 539, loss: 0.0054, instance_loss: 0.0741, weighted_loss: 0.0260, label: 0, bag_size: 9455\n",
      "batch 559, loss: 0.0452, instance_loss: 0.1178, weighted_loss: 0.0670, label: 1, bag_size: 1888\n",
      "batch 579, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 9252\n",
      "batch 599, loss: 0.6749, instance_loss: 0.0752, weighted_loss: 0.4950, label: 1, bag_size: 6927\n",
      "batch 619, loss: 0.0109, instance_loss: 0.6958, weighted_loss: 0.2164, label: 1, bag_size: 4039\n",
      "batch 639, loss: 0.2008, instance_loss: 0.1634, weighted_loss: 0.1896, label: 1, bag_size: 6682\n",
      "batch 659, loss: 0.6655, instance_loss: 0.0428, weighted_loss: 0.4786, label: 1, bag_size: 3652\n",
      "batch 679, loss: 0.0077, instance_loss: 0.0000, weighted_loss: 0.0054, label: 0, bag_size: 21138\n",
      "batch 699, loss: 0.0078, instance_loss: 0.0059, weighted_loss: 0.0072, label: 1, bag_size: 4239\n",
      "batch 719, loss: 0.0162, instance_loss: 0.0013, weighted_loss: 0.0118, label: 1, bag_size: 5256\n",
      "batch 739, loss: 0.0232, instance_loss: 0.0000, weighted_loss: 0.0162, label: 0, bag_size: 11194\n",
      "batch 759, loss: 0.0254, instance_loss: 0.1025, weighted_loss: 0.0485, label: 0, bag_size: 1508\n",
      "batch 779, loss: 0.0003, instance_loss: 0.1311, weighted_loss: 0.0396, label: 1, bag_size: 10394\n",
      "batch 799, loss: 0.0571, instance_loss: 0.4022, weighted_loss: 0.1606, label: 0, bag_size: 3670\n",
      "batch 819, loss: 0.0066, instance_loss: 0.0000, weighted_loss: 0.0046, label: 0, bag_size: 9866\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.974390243902439: correct 12784/13120\n",
      "class 1 clustering acc 0.8975609756097561: correct 5888/6560\n",
      "Epoch: 55, train_loss: 0.1587, train_clustering_loss:  0.2250, train_error: 0.0659\n",
      "class 0: acc 0.9349397590361446, correct 388/415\n",
      "class 1: acc 0.9333333333333333, correct 378/405\n",
      "\n",
      "Val Set, val_loss: 0.1311, val_error: 0.0455, auc: 0.9927\n",
      "class 0 clustering acc 0.9710227272727273: correct 1709/1760\n",
      "class 1 clustering acc 0.8715909090909091: correct 767/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0169, instance_loss: 0.3387, weighted_loss: 0.1135, label: 0, bag_size: 2063\n",
      "batch 39, loss: 0.0303, instance_loss: 0.0499, weighted_loss: 0.0361, label: 0, bag_size: 1891\n",
      "batch 59, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 6851\n",
      "batch 79, loss: 0.1340, instance_loss: 0.0002, weighted_loss: 0.0938, label: 1, bag_size: 1919\n",
      "batch 99, loss: 0.0050, instance_loss: 0.1569, weighted_loss: 0.0506, label: 0, bag_size: 9252\n",
      "batch 119, loss: 0.4601, instance_loss: 0.6399, weighted_loss: 0.5140, label: 1, bag_size: 1230\n",
      "batch 139, loss: 0.0368, instance_loss: 0.0000, weighted_loss: 0.0258, label: 1, bag_size: 6665\n",
      "batch 159, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 19466\n",
      "batch 179, loss: 0.0418, instance_loss: 0.0066, weighted_loss: 0.0312, label: 1, bag_size: 7424\n",
      "batch 199, loss: 0.0330, instance_loss: 1.0025, weighted_loss: 0.3238, label: 1, bag_size: 5907\n",
      "batch 219, loss: 0.1601, instance_loss: 0.0519, weighted_loss: 0.1277, label: 1, bag_size: 8438\n",
      "batch 239, loss: 0.0045, instance_loss: 0.0373, weighted_loss: 0.0143, label: 0, bag_size: 1639\n",
      "batch 259, loss: 0.0923, instance_loss: 0.0455, weighted_loss: 0.0783, label: 0, bag_size: 7823\n",
      "batch 279, loss: 0.0137, instance_loss: 0.8857, weighted_loss: 0.2753, label: 1, bag_size: 865\n",
      "batch 299, loss: 0.0546, instance_loss: 0.1620, weighted_loss: 0.0868, label: 1, bag_size: 8680\n",
      "batch 319, loss: 0.0106, instance_loss: 0.0000, weighted_loss: 0.0074, label: 0, bag_size: 13777\n",
      "batch 339, loss: 0.0071, instance_loss: 0.0000, weighted_loss: 0.0049, label: 0, bag_size: 2920\n",
      "batch 359, loss: 2.4895, instance_loss: 0.0958, weighted_loss: 1.7714, label: 0, bag_size: 3897\n",
      "batch 379, loss: 0.0020, instance_loss: 0.0155, weighted_loss: 0.0061, label: 1, bag_size: 7935\n",
      "batch 399, loss: 0.0130, instance_loss: 0.0000, weighted_loss: 0.0091, label: 0, bag_size: 8866\n",
      "batch 419, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 16341\n",
      "batch 439, loss: 0.0078, instance_loss: 0.0001, weighted_loss: 0.0055, label: 1, bag_size: 2936\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21218\n",
      "batch 479, loss: 0.1208, instance_loss: 0.0094, weighted_loss: 0.0874, label: 0, bag_size: 1732\n",
      "batch 499, loss: 0.0027, instance_loss: 0.0044, weighted_loss: 0.0032, label: 0, bag_size: 2063\n",
      "batch 519, loss: 0.0002, instance_loss: 0.0013, weighted_loss: 0.0005, label: 1, bag_size: 4259\n",
      "batch 539, loss: 0.0017, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 17633\n",
      "batch 559, loss: 0.9670, instance_loss: 0.1854, weighted_loss: 0.7325, label: 0, bag_size: 2070\n",
      "batch 579, loss: 0.0092, instance_loss: 0.0000, weighted_loss: 0.0065, label: 1, bag_size: 25970\n",
      "batch 599, loss: 0.1213, instance_loss: 0.0135, weighted_loss: 0.0889, label: 1, bag_size: 7351\n",
      "batch 619, loss: 0.0028, instance_loss: 0.0025, weighted_loss: 0.0027, label: 1, bag_size: 2662\n",
      "batch 639, loss: 0.0013, instance_loss: 0.0040, weighted_loss: 0.0021, label: 0, bag_size: 7823\n",
      "batch 659, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 13964\n",
      "batch 679, loss: 0.0029, instance_loss: 0.0449, weighted_loss: 0.0155, label: 0, bag_size: 1234\n",
      "batch 699, loss: 0.0095, instance_loss: 0.0000, weighted_loss: 0.0067, label: 1, bag_size: 16034\n",
      "batch 719, loss: 0.3992, instance_loss: 0.0229, weighted_loss: 0.2863, label: 1, bag_size: 5605\n",
      "batch 739, loss: 0.0091, instance_loss: 0.0604, weighted_loss: 0.0245, label: 0, bag_size: 9930\n",
      "batch 759, loss: 0.0683, instance_loss: 0.3210, weighted_loss: 0.1441, label: 0, bag_size: 1614\n",
      "batch 779, loss: 0.1133, instance_loss: 0.0150, weighted_loss: 0.0838, label: 1, bag_size: 4789\n",
      "batch 799, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 17633\n",
      "batch 819, loss: 0.0153, instance_loss: 0.0000, weighted_loss: 0.0107, label: 0, bag_size: 2534\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9839176829268292: correct 12909/13120\n",
      "class 1 clustering acc 0.9314024390243902: correct 6110/6560\n",
      "Epoch: 56, train_loss: 0.1538, train_clustering_loss:  0.1339, train_error: 0.0573\n",
      "class 0: acc 0.937046004842615, correct 387/413\n",
      "class 1: acc 0.9484029484029484, correct 386/407\n",
      "\n",
      "Val Set, val_loss: 0.2008, val_error: 0.0818, auc: 0.9930\n",
      "class 0 clustering acc 0.9647727272727272: correct 1698/1760\n",
      "class 1 clustering acc 0.8909090909090909: correct 784/880\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0017, instance_loss: 0.0275, weighted_loss: 0.0094, label: 1, bag_size: 14433\n",
      "batch 39, loss: 1.6896, instance_loss: 0.6466, weighted_loss: 1.3767, label: 0, bag_size: 9616\n",
      "batch 59, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0011, label: 1, bag_size: 5561\n",
      "batch 79, loss: 0.0018, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 26271\n",
      "batch 99, loss: 0.0044, instance_loss: 0.0000, weighted_loss: 0.0031, label: 0, bag_size: 2036\n",
      "batch 119, loss: 0.0424, instance_loss: 0.1481, weighted_loss: 0.0741, label: 1, bag_size: 10848\n",
      "batch 139, loss: 2.6658, instance_loss: 0.0000, weighted_loss: 1.8660, label: 0, bag_size: 3897\n",
      "batch 159, loss: 0.2118, instance_loss: 0.2192, weighted_loss: 0.2140, label: 1, bag_size: 1038\n",
      "batch 179, loss: 0.0052, instance_loss: 0.0721, weighted_loss: 0.0253, label: 1, bag_size: 10912\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0109, weighted_loss: 0.0033, label: 1, bag_size: 1781\n",
      "batch 219, loss: 0.0424, instance_loss: 0.0793, weighted_loss: 0.0535, label: 1, bag_size: 7389\n",
      "batch 239, loss: 0.1354, instance_loss: 0.0825, weighted_loss: 0.1195, label: 1, bag_size: 4789\n",
      "batch 259, loss: 0.0050, instance_loss: 0.0453, weighted_loss: 0.0171, label: 1, bag_size: 2179\n",
      "batch 279, loss: 0.0734, instance_loss: 0.0243, weighted_loss: 0.0587, label: 0, bag_size: 2367\n",
      "batch 299, loss: 0.9914, instance_loss: 0.0651, weighted_loss: 0.7135, label: 0, bag_size: 2653\n",
      "batch 319, loss: 0.2839, instance_loss: 0.0500, weighted_loss: 0.2137, label: 0, bag_size: 2653\n",
      "batch 339, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 16052\n",
      "batch 359, loss: 0.0347, instance_loss: 0.0903, weighted_loss: 0.0514, label: 1, bag_size: 5292\n",
      "batch 379, loss: 0.1870, instance_loss: 0.0000, weighted_loss: 0.1309, label: 0, bag_size: 18738\n",
      "batch 399, loss: 0.0651, instance_loss: 0.0000, weighted_loss: 0.0456, label: 0, bag_size: 10029\n",
      "batch 419, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 16087\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 23996\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 11195\n",
      "batch 479, loss: 0.0109, instance_loss: 0.0751, weighted_loss: 0.0302, label: 0, bag_size: 9455\n",
      "batch 499, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 19390\n",
      "batch 519, loss: 0.0563, instance_loss: 0.0452, weighted_loss: 0.0530, label: 0, bag_size: 1831\n",
      "batch 539, loss: 0.0574, instance_loss: 0.0002, weighted_loss: 0.0403, label: 1, bag_size: 9877\n",
      "batch 559, loss: 0.0138, instance_loss: 0.0004, weighted_loss: 0.0098, label: 1, bag_size: 10028\n",
      "batch 579, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 8372\n",
      "batch 599, loss: 0.0417, instance_loss: 0.0228, weighted_loss: 0.0360, label: 1, bag_size: 4939\n",
      "batch 619, loss: 0.0633, instance_loss: 0.1493, weighted_loss: 0.0891, label: 0, bag_size: 18738\n",
      "batch 639, loss: 0.0147, instance_loss: 0.0014, weighted_loss: 0.0107, label: 0, bag_size: 3444\n",
      "batch 659, loss: 0.0002, instance_loss: 0.0002, weighted_loss: 0.0002, label: 0, bag_size: 21093\n",
      "batch 679, loss: 0.0299, instance_loss: 0.0000, weighted_loss: 0.0210, label: 1, bag_size: 25970\n",
      "batch 699, loss: 0.2008, instance_loss: 0.0366, weighted_loss: 0.1516, label: 1, bag_size: 12340\n",
      "batch 719, loss: 0.0020, instance_loss: 0.0000, weighted_loss: 0.0014, label: 0, bag_size: 22870\n",
      "batch 739, loss: 0.0033, instance_loss: 0.0000, weighted_loss: 0.0023, label: 1, bag_size: 11600\n",
      "batch 759, loss: 0.0644, instance_loss: 0.0067, weighted_loss: 0.0471, label: 1, bag_size: 10460\n",
      "batch 779, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 16992\n",
      "batch 799, loss: 0.2269, instance_loss: 0.0358, weighted_loss: 0.1696, label: 0, bag_size: 1953\n",
      "batch 819, loss: 0.0354, instance_loss: 0.0147, weighted_loss: 0.0291, label: 1, bag_size: 3211\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9848323170731708: correct 12921/13120\n",
      "class 1 clustering acc 0.9448170731707317: correct 6198/6560\n",
      "Epoch: 57, train_loss: 0.1484, train_clustering_loss:  0.1324, train_error: 0.0598\n",
      "class 0: acc 0.9402985074626866, correct 378/402\n",
      "class 1: acc 0.9401913875598086, correct 393/418\n",
      "\n",
      "Val Set, val_loss: 0.1290, val_error: 0.0182, auc: 0.9927\n",
      "class 0 clustering acc 0.9346590909090909: correct 1645/1760\n",
      "class 1 clustering acc 0.8602272727272727: correct 757/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.0064, weighted_loss: 0.0020, label: 1, bag_size: 9732\n",
      "batch 39, loss: 0.0832, instance_loss: 0.0028, weighted_loss: 0.0591, label: 1, bag_size: 1919\n",
      "batch 59, loss: 0.0773, instance_loss: 0.0078, weighted_loss: 0.0564, label: 1, bag_size: 3652\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 15716\n",
      "batch 99, loss: 0.0195, instance_loss: 0.0003, weighted_loss: 0.0137, label: 1, bag_size: 6745\n",
      "batch 119, loss: 0.0998, instance_loss: 0.2248, weighted_loss: 0.1373, label: 0, bag_size: 3160\n",
      "batch 139, loss: 0.0042, instance_loss: 0.0000, weighted_loss: 0.0029, label: 1, bag_size: 7217\n",
      "batch 159, loss: 0.0003, instance_loss: 0.0040, weighted_loss: 0.0014, label: 0, bag_size: 518\n",
      "batch 179, loss: 0.3632, instance_loss: 0.1850, weighted_loss: 0.3097, label: 1, bag_size: 1963\n",
      "batch 199, loss: 0.0010, instance_loss: 0.0032, weighted_loss: 0.0017, label: 1, bag_size: 14618\n",
      "batch 219, loss: 0.0014, instance_loss: 0.0556, weighted_loss: 0.0176, label: 1, bag_size: 1746\n",
      "batch 239, loss: 0.0008, instance_loss: 0.0283, weighted_loss: 0.0091, label: 1, bag_size: 2140\n",
      "batch 259, loss: 0.1298, instance_loss: 2.8435, weighted_loss: 0.9439, label: 0, bag_size: 13332\n",
      "batch 279, loss: 0.0019, instance_loss: 0.2990, weighted_loss: 0.0911, label: 1, bag_size: 2662\n",
      "batch 299, loss: 0.0024, instance_loss: 0.0002, weighted_loss: 0.0017, label: 0, bag_size: 10263\n",
      "batch 319, loss: 0.7361, instance_loss: 0.0114, weighted_loss: 0.5187, label: 1, bag_size: 12714\n",
      "batch 339, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 24911\n",
      "batch 359, loss: 0.0290, instance_loss: 0.1879, weighted_loss: 0.0766, label: 0, bag_size: 7637\n",
      "batch 379, loss: 0.0078, instance_loss: 0.0003, weighted_loss: 0.0055, label: 1, bag_size: 14030\n",
      "batch 399, loss: 0.0005, instance_loss: 0.0263, weighted_loss: 0.0083, label: 0, bag_size: 2179\n",
      "batch 419, loss: 0.0504, instance_loss: 0.3435, weighted_loss: 0.1383, label: 1, bag_size: 4308\n",
      "batch 439, loss: 0.0015, instance_loss: 0.0097, weighted_loss: 0.0040, label: 1, bag_size: 7381\n",
      "batch 459, loss: 0.0134, instance_loss: 0.2190, weighted_loss: 0.0751, label: 1, bag_size: 10671\n",
      "batch 479, loss: 0.2095, instance_loss: 0.2552, weighted_loss: 0.2232, label: 1, bag_size: 1963\n",
      "batch 499, loss: 0.8425, instance_loss: 0.0052, weighted_loss: 0.5913, label: 0, bag_size: 1732\n",
      "batch 519, loss: 0.1043, instance_loss: 0.0729, weighted_loss: 0.0949, label: 1, bag_size: 18603\n",
      "batch 539, loss: 0.0137, instance_loss: 0.0065, weighted_loss: 0.0116, label: 1, bag_size: 5690\n",
      "batch 559, loss: 0.0081, instance_loss: 0.0000, weighted_loss: 0.0057, label: 1, bag_size: 2936\n",
      "batch 579, loss: 0.9636, instance_loss: 0.5781, weighted_loss: 0.8480, label: 0, bag_size: 1127\n",
      "batch 599, loss: 0.0068, instance_loss: 0.0013, weighted_loss: 0.0052, label: 1, bag_size: 1493\n",
      "batch 619, loss: 0.0790, instance_loss: 0.0000, weighted_loss: 0.0553, label: 1, bag_size: 30675\n",
      "batch 639, loss: 0.5678, instance_loss: 0.0006, weighted_loss: 0.3976, label: 1, bag_size: 11256\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 31780\n",
      "batch 679, loss: 0.1230, instance_loss: 0.0005, weighted_loss: 0.0863, label: 1, bag_size: 13732\n",
      "batch 699, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 1, bag_size: 14223\n",
      "batch 719, loss: 0.0231, instance_loss: 0.0000, weighted_loss: 0.0162, label: 1, bag_size: 29832\n",
      "batch 739, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 18225\n",
      "batch 759, loss: 0.0015, instance_loss: 0.0001, weighted_loss: 0.0011, label: 0, bag_size: 9234\n",
      "batch 779, loss: 0.0059, instance_loss: 0.0076, weighted_loss: 0.0064, label: 0, bag_size: 1415\n",
      "batch 799, loss: 2.3367, instance_loss: 3.0766, weighted_loss: 2.5587, label: 1, bag_size: 2731\n",
      "batch 819, loss: 0.9335, instance_loss: 0.3099, weighted_loss: 0.7465, label: 0, bag_size: 8420\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9817073170731707: correct 12880/13120\n",
      "class 1 clustering acc 0.9246951219512195: correct 6066/6560\n",
      "Epoch: 58, train_loss: 0.1773, train_clustering_loss:  0.1648, train_error: 0.0744\n",
      "class 0: acc 0.9164556962025316, correct 362/395\n",
      "class 1: acc 0.9341176470588235, correct 397/425\n",
      "\n",
      "Val Set, val_loss: 0.1901, val_error: 0.0818, auc: 0.9924\n",
      "class 0 clustering acc 0.9375: correct 1650/1760\n",
      "class 1 clustering acc 0.8193181818181818: correct 721/880\n",
      "class 0: acc 0.8269230769230769, correct 43/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0036, instance_loss: 0.1383, weighted_loss: 0.0440, label: 0, bag_size: 4523\n",
      "batch 39, loss: 0.2035, instance_loss: 0.0853, weighted_loss: 0.1680, label: 1, bag_size: 11729\n",
      "batch 59, loss: 0.1241, instance_loss: 0.1275, weighted_loss: 0.1251, label: 0, bag_size: 1438\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 7191\n",
      "batch 99, loss: 0.0320, instance_loss: 0.0004, weighted_loss: 0.0225, label: 1, bag_size: 11394\n",
      "batch 119, loss: 0.0009, instance_loss: 0.0009, weighted_loss: 0.0009, label: 1, bag_size: 5221\n",
      "batch 139, loss: 0.0341, instance_loss: 0.0143, weighted_loss: 0.0282, label: 0, bag_size: 14625\n",
      "batch 159, loss: 0.0031, instance_loss: 0.0009, weighted_loss: 0.0025, label: 1, bag_size: 10392\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0254, weighted_loss: 0.0076, label: 1, bag_size: 629\n",
      "batch 199, loss: 0.1559, instance_loss: 0.2497, weighted_loss: 0.1841, label: 1, bag_size: 2935\n",
      "batch 219, loss: 0.0093, instance_loss: 0.0000, weighted_loss: 0.0065, label: 0, bag_size: 14333\n",
      "batch 239, loss: 0.0001, instance_loss: 0.0008, weighted_loss: 0.0003, label: 1, bag_size: 5612\n",
      "batch 259, loss: 0.0175, instance_loss: 0.0000, weighted_loss: 0.0123, label: 0, bag_size: 13880\n",
      "batch 279, loss: 3.2986, instance_loss: 2.3541, weighted_loss: 3.0152, label: 1, bag_size: 13367\n",
      "batch 299, loss: 0.0821, instance_loss: 0.0000, weighted_loss: 0.0575, label: 0, bag_size: 13205\n",
      "batch 319, loss: 0.0031, instance_loss: 0.0000, weighted_loss: 0.0022, label: 0, bag_size: 15313\n",
      "batch 339, loss: 0.0477, instance_loss: 0.0078, weighted_loss: 0.0358, label: 1, bag_size: 12719\n",
      "batch 359, loss: 0.2456, instance_loss: 0.0004, weighted_loss: 0.1721, label: 1, bag_size: 12340\n",
      "batch 379, loss: 0.0030, instance_loss: 0.0161, weighted_loss: 0.0069, label: 1, bag_size: 2662\n",
      "batch 399, loss: 0.0182, instance_loss: 0.0006, weighted_loss: 0.0129, label: 1, bag_size: 11363\n",
      "batch 419, loss: 0.0684, instance_loss: 0.1027, weighted_loss: 0.0787, label: 1, bag_size: 16154\n",
      "batch 439, loss: 0.0226, instance_loss: 0.0366, weighted_loss: 0.0268, label: 0, bag_size: 2336\n",
      "batch 459, loss: 0.0353, instance_loss: 0.0000, weighted_loss: 0.0247, label: 1, bag_size: 13732\n",
      "batch 479, loss: 0.2434, instance_loss: 0.0062, weighted_loss: 0.1723, label: 0, bag_size: 14264\n",
      "batch 499, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 23791\n",
      "batch 519, loss: 0.0032, instance_loss: 0.0000, weighted_loss: 0.0023, label: 0, bag_size: 9455\n",
      "batch 539, loss: 0.2372, instance_loss: 0.0884, weighted_loss: 0.1926, label: 0, bag_size: 13332\n",
      "batch 559, loss: 0.0574, instance_loss: 0.0104, weighted_loss: 0.0433, label: 0, bag_size: 12083\n",
      "batch 579, loss: 0.0033, instance_loss: 0.0006, weighted_loss: 0.0025, label: 1, bag_size: 5256\n",
      "batch 599, loss: 0.0873, instance_loss: 0.0001, weighted_loss: 0.0611, label: 0, bag_size: 3444\n",
      "batch 619, loss: 0.1034, instance_loss: 0.0199, weighted_loss: 0.0784, label: 1, bag_size: 11223\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0010, weighted_loss: 0.0003, label: 1, bag_size: 9610\n",
      "batch 659, loss: 0.0255, instance_loss: 0.0010, weighted_loss: 0.0182, label: 0, bag_size: 10490\n",
      "batch 679, loss: 0.0391, instance_loss: 0.9940, weighted_loss: 0.3256, label: 0, bag_size: 1745\n",
      "batch 699, loss: 0.0004, instance_loss: 0.0025, weighted_loss: 0.0011, label: 1, bag_size: 13365\n",
      "batch 719, loss: 0.0267, instance_loss: 0.0000, weighted_loss: 0.0187, label: 1, bag_size: 12714\n",
      "batch 739, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21218\n",
      "batch 759, loss: 0.0033, instance_loss: 0.0001, weighted_loss: 0.0024, label: 0, bag_size: 2303\n",
      "batch 779, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 6966\n",
      "batch 799, loss: 0.9829, instance_loss: 0.1607, weighted_loss: 0.7362, label: 0, bag_size: 7428\n",
      "batch 819, loss: 0.0201, instance_loss: 0.0239, weighted_loss: 0.0213, label: 1, bag_size: 2695\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9845274390243902: correct 12917/13120\n",
      "class 1 clustering acc 0.9335365853658537: correct 6124/6560\n",
      "Epoch: 59, train_loss: 0.1515, train_clustering_loss:  0.1409, train_error: 0.0646\n",
      "class 0: acc 0.927461139896373, correct 358/386\n",
      "class 1: acc 0.9423963133640553, correct 409/434\n",
      "\n",
      "Val Set, val_loss: 0.1545, val_error: 0.0818, auc: 0.9927\n",
      "class 0 clustering acc 0.9272727272727272: correct 1632/1760\n",
      "class 1 clustering acc 0.8431818181818181: correct 742/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.8793103448275862, correct 51/58\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1056, instance_loss: 0.1816, weighted_loss: 0.1284, label: 1, bag_size: 2937\n",
      "batch 39, loss: 4.3392, instance_loss: 0.5177, weighted_loss: 3.1928, label: 0, bag_size: 3468\n",
      "batch 59, loss: 0.4719, instance_loss: 2.5087, weighted_loss: 1.0830, label: 0, bag_size: 1637\n",
      "batch 79, loss: 0.0084, instance_loss: 0.0003, weighted_loss: 0.0059, label: 1, bag_size: 3082\n",
      "batch 99, loss: 0.0018, instance_loss: 0.0020, weighted_loss: 0.0019, label: 0, bag_size: 2303\n",
      "batch 119, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 10725\n",
      "batch 139, loss: 0.0985, instance_loss: 0.0000, weighted_loss: 0.0690, label: 1, bag_size: 7389\n",
      "batch 159, loss: 0.1358, instance_loss: 0.0332, weighted_loss: 0.1050, label: 1, bag_size: 8026\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 22800\n",
      "batch 199, loss: 0.0207, instance_loss: 0.0040, weighted_loss: 0.0157, label: 1, bag_size: 18603\n",
      "batch 219, loss: 0.0007, instance_loss: 0.4819, weighted_loss: 0.1450, label: 0, bag_size: 1745\n",
      "batch 239, loss: 0.0106, instance_loss: 0.0520, weighted_loss: 0.0230, label: 1, bag_size: 8475\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 10592\n",
      "batch 279, loss: 0.0011, instance_loss: 0.0212, weighted_loss: 0.0071, label: 1, bag_size: 1746\n",
      "batch 299, loss: 0.0031, instance_loss: 0.0398, weighted_loss: 0.0141, label: 0, bag_size: 763\n",
      "batch 319, loss: 0.0263, instance_loss: 0.0000, weighted_loss: 0.0184, label: 1, bag_size: 5629\n",
      "batch 339, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 9673\n",
      "batch 359, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 16087\n",
      "batch 379, loss: 0.0165, instance_loss: 0.0000, weighted_loss: 0.0115, label: 1, bag_size: 5345\n",
      "batch 399, loss: 0.0198, instance_loss: 0.0000, weighted_loss: 0.0139, label: 0, bag_size: 9866\n",
      "batch 419, loss: 0.0024, instance_loss: 0.0958, weighted_loss: 0.0304, label: 1, bag_size: 2146\n",
      "batch 439, loss: 0.4806, instance_loss: 0.0223, weighted_loss: 0.3431, label: 1, bag_size: 1123\n",
      "batch 459, loss: 0.0042, instance_loss: 0.0000, weighted_loss: 0.0029, label: 0, bag_size: 11125\n",
      "batch 479, loss: 0.0033, instance_loss: 0.0000, weighted_loss: 0.0023, label: 0, bag_size: 24911\n",
      "batch 499, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 9234\n",
      "batch 519, loss: 0.0074, instance_loss: 0.3886, weighted_loss: 0.1217, label: 1, bag_size: 9004\n",
      "batch 539, loss: 0.0020, instance_loss: 0.0000, weighted_loss: 0.0014, label: 0, bag_size: 14377\n",
      "batch 559, loss: 0.0126, instance_loss: 0.0084, weighted_loss: 0.0113, label: 1, bag_size: 2495\n",
      "batch 579, loss: 0.0168, instance_loss: 0.0000, weighted_loss: 0.0117, label: 0, bag_size: 10029\n",
      "batch 599, loss: 0.0027, instance_loss: 0.0023, weighted_loss: 0.0026, label: 1, bag_size: 4239\n",
      "batch 619, loss: 0.0862, instance_loss: 0.0013, weighted_loss: 0.0607, label: 1, bag_size: 3652\n",
      "batch 639, loss: 1.1040, instance_loss: 0.3058, weighted_loss: 0.8645, label: 1, bag_size: 1038\n",
      "batch 659, loss: 0.3690, instance_loss: 0.0567, weighted_loss: 0.2753, label: 0, bag_size: 6367\n",
      "batch 679, loss: 0.0879, instance_loss: 0.0058, weighted_loss: 0.0633, label: 1, bag_size: 12626\n",
      "batch 699, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 21682\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 18225\n",
      "batch 739, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 11981\n",
      "batch 759, loss: 0.1040, instance_loss: 0.0605, weighted_loss: 0.0910, label: 0, bag_size: 2063\n",
      "batch 779, loss: 0.0014, instance_loss: 0.0352, weighted_loss: 0.0115, label: 1, bag_size: 20333\n",
      "batch 799, loss: 1.3493, instance_loss: 0.2984, weighted_loss: 1.0340, label: 1, bag_size: 3652\n",
      "batch 819, loss: 1.3376, instance_loss: 0.0705, weighted_loss: 0.9574, label: 0, bag_size: 8420\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9825457317073171: correct 12891/13120\n",
      "class 1 clustering acc 0.9222560975609756: correct 6050/6560\n",
      "Epoch: 60, train_loss: 0.1280, train_clustering_loss:  0.1740, train_error: 0.0537\n",
      "class 0: acc 0.9346246973365617, correct 386/413\n",
      "class 1: acc 0.9582309582309583, correct 390/407\n",
      "\n",
      "Val Set, val_loss: 0.3363, val_error: 0.1545, auc: 0.9924\n",
      "class 0 clustering acc 0.9477272727272728: correct 1668/1760\n",
      "class 1 clustering acc 0.8670454545454546: correct 763/880\n",
      "class 0: acc 0.6730769230769231, correct 35/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0117, instance_loss: 0.0262, weighted_loss: 0.0160, label: 0, bag_size: 3474\n",
      "batch 39, loss: 0.0238, instance_loss: 0.0054, weighted_loss: 0.0183, label: 1, bag_size: 12697\n",
      "batch 59, loss: 0.4337, instance_loss: 0.4063, weighted_loss: 0.4255, label: 0, bag_size: 9616\n",
      "batch 79, loss: 0.0913, instance_loss: 0.0215, weighted_loss: 0.0703, label: 0, bag_size: 1690\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 7191\n",
      "batch 119, loss: 0.0050, instance_loss: 0.0000, weighted_loss: 0.0035, label: 1, bag_size: 1014\n",
      "batch 139, loss: 0.3750, instance_loss: 0.0146, weighted_loss: 0.2669, label: 0, bag_size: 1760\n",
      "batch 159, loss: 0.0263, instance_loss: 0.0683, weighted_loss: 0.0389, label: 0, bag_size: 6281\n",
      "batch 179, loss: 0.0305, instance_loss: 0.0000, weighted_loss: 0.0213, label: 1, bag_size: 12895\n",
      "batch 199, loss: 0.0448, instance_loss: 0.0900, weighted_loss: 0.0584, label: 0, bag_size: 7835\n",
      "batch 219, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 22828\n",
      "batch 239, loss: 0.0165, instance_loss: 0.0045, weighted_loss: 0.0129, label: 1, bag_size: 6665\n",
      "batch 259, loss: 0.0575, instance_loss: 0.0521, weighted_loss: 0.0559, label: 0, bag_size: 18738\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0003, label: 1, bag_size: 6606\n",
      "batch 299, loss: 0.1222, instance_loss: 0.0174, weighted_loss: 0.0907, label: 1, bag_size: 12425\n",
      "batch 319, loss: 0.0106, instance_loss: 0.0234, weighted_loss: 0.0144, label: 1, bag_size: 8448\n",
      "batch 339, loss: 0.0251, instance_loss: 0.0156, weighted_loss: 0.0223, label: 0, bag_size: 17630\n",
      "batch 359, loss: 0.0657, instance_loss: 0.3644, weighted_loss: 0.1553, label: 0, bag_size: 4598\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 1781\n",
      "batch 399, loss: 0.0015, instance_loss: 0.0009, weighted_loss: 0.0013, label: 1, bag_size: 4423\n",
      "batch 419, loss: 0.0033, instance_loss: 0.0242, weighted_loss: 0.0096, label: 0, bag_size: 2873\n",
      "batch 439, loss: 0.1024, instance_loss: 0.0155, weighted_loss: 0.0763, label: 0, bag_size: 9069\n",
      "batch 459, loss: 0.0557, instance_loss: 0.0266, weighted_loss: 0.0469, label: 0, bag_size: 6281\n",
      "batch 479, loss: 0.0066, instance_loss: 0.0000, weighted_loss: 0.0046, label: 1, bag_size: 3651\n",
      "batch 499, loss: 0.0303, instance_loss: 0.0495, weighted_loss: 0.0361, label: 0, bag_size: 7557\n",
      "batch 519, loss: 0.1524, instance_loss: 0.0030, weighted_loss: 0.1076, label: 0, bag_size: 11128\n",
      "batch 539, loss: 0.0028, instance_loss: 0.0054, weighted_loss: 0.0036, label: 0, bag_size: 2534\n",
      "batch 559, loss: 0.1013, instance_loss: 0.0275, weighted_loss: 0.0792, label: 1, bag_size: 7989\n",
      "batch 579, loss: 0.0738, instance_loss: 0.0119, weighted_loss: 0.0552, label: 0, bag_size: 10113\n",
      "batch 599, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 15213\n",
      "batch 619, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 22870\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0295, weighted_loss: 0.0089, label: 1, bag_size: 9732\n",
      "batch 659, loss: 0.0313, instance_loss: 1.0390, weighted_loss: 0.3336, label: 1, bag_size: 1444\n",
      "batch 679, loss: 0.0037, instance_loss: 0.0498, weighted_loss: 0.0175, label: 1, bag_size: 8660\n",
      "batch 699, loss: 0.0565, instance_loss: 0.0227, weighted_loss: 0.0464, label: 0, bag_size: 9387\n",
      "batch 719, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 11477\n",
      "batch 739, loss: 0.0296, instance_loss: 0.0117, weighted_loss: 0.0243, label: 0, bag_size: 19067\n",
      "batch 759, loss: 0.0080, instance_loss: 0.0076, weighted_loss: 0.0079, label: 1, bag_size: 5256\n",
      "batch 779, loss: 0.0001, instance_loss: 0.1892, weighted_loss: 0.0568, label: 1, bag_size: 3295\n",
      "batch 799, loss: 0.0135, instance_loss: 0.0043, weighted_loss: 0.0107, label: 0, bag_size: 1349\n",
      "batch 819, loss: 0.0056, instance_loss: 0.0167, weighted_loss: 0.0089, label: 0, bag_size: 11259\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9860518292682927: correct 12937/13120\n",
      "class 1 clustering acc 0.9350609756097561: correct 6134/6560\n",
      "Epoch: 61, train_loss: 0.1406, train_clustering_loss:  0.1347, train_error: 0.0524\n",
      "class 0: acc 0.9498806682577565, correct 398/419\n",
      "class 1: acc 0.9451371571072319, correct 379/401\n",
      "\n",
      "Val Set, val_loss: 0.1213, val_error: 0.0273, auc: 0.9940\n",
      "class 0 clustering acc 0.9443181818181818: correct 1662/1760\n",
      "class 1 clustering acc 0.8795454545454545: correct 774/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "Validation loss decreased (0.123507 --> 0.121262).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0803, instance_loss: 0.5197, weighted_loss: 0.2122, label: 1, bag_size: 16548\n",
      "batch 39, loss: 0.0094, instance_loss: 0.0644, weighted_loss: 0.0259, label: 0, bag_size: 13892\n",
      "batch 59, loss: 1.5259, instance_loss: 0.4664, weighted_loss: 1.2081, label: 1, bag_size: 1038\n",
      "batch 79, loss: 0.0025, instance_loss: 0.0000, weighted_loss: 0.0018, label: 0, bag_size: 8661\n",
      "batch 99, loss: 2.1366, instance_loss: 0.2786, weighted_loss: 1.5792, label: 0, bag_size: 2694\n",
      "batch 119, loss: 0.0234, instance_loss: 0.0009, weighted_loss: 0.0167, label: 0, bag_size: 2043\n",
      "batch 139, loss: 0.1374, instance_loss: 0.0159, weighted_loss: 0.1009, label: 1, bag_size: 1888\n",
      "batch 159, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 16034\n",
      "batch 179, loss: 0.0225, instance_loss: 0.0006, weighted_loss: 0.0160, label: 1, bag_size: 7246\n",
      "batch 199, loss: 0.0034, instance_loss: 0.0000, weighted_loss: 0.0024, label: 0, bag_size: 14377\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 7650\n",
      "batch 239, loss: 0.0030, instance_loss: 0.0000, weighted_loss: 0.0021, label: 1, bag_size: 16267\n",
      "batch 259, loss: 0.0107, instance_loss: 0.0130, weighted_loss: 0.0114, label: 0, bag_size: 7823\n",
      "batch 279, loss: 0.0025, instance_loss: 0.0002, weighted_loss: 0.0018, label: 0, bag_size: 2652\n",
      "batch 299, loss: 0.0016, instance_loss: 0.0004, weighted_loss: 0.0012, label: 0, bag_size: 15636\n",
      "batch 319, loss: 0.2641, instance_loss: 0.6334, weighted_loss: 0.3749, label: 1, bag_size: 1095\n",
      "batch 339, loss: 0.0598, instance_loss: 0.0000, weighted_loss: 0.0419, label: 1, bag_size: 12895\n",
      "batch 359, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 7515\n",
      "batch 379, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 7515\n",
      "batch 399, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 26271\n",
      "batch 419, loss: 0.0005, instance_loss: 0.0003, weighted_loss: 0.0004, label: 0, bag_size: 2748\n",
      "batch 439, loss: 0.0062, instance_loss: 0.0938, weighted_loss: 0.0325, label: 0, bag_size: 12796\n",
      "batch 459, loss: 0.0107, instance_loss: 0.0832, weighted_loss: 0.0324, label: 1, bag_size: 2682\n",
      "batch 479, loss: 0.0335, instance_loss: 0.0081, weighted_loss: 0.0259, label: 1, bag_size: 1512\n",
      "batch 499, loss: 0.0023, instance_loss: 0.1035, weighted_loss: 0.0327, label: 0, bag_size: 7011\n",
      "batch 519, loss: 0.0639, instance_loss: 0.0254, weighted_loss: 0.0524, label: 1, bag_size: 5907\n",
      "batch 539, loss: 0.0114, instance_loss: 0.1441, weighted_loss: 0.0512, label: 1, bag_size: 2814\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 11642\n",
      "batch 579, loss: 0.0005, instance_loss: 0.0323, weighted_loss: 0.0101, label: 0, bag_size: 10995\n",
      "batch 599, loss: 0.0443, instance_loss: 0.0003, weighted_loss: 0.0311, label: 1, bag_size: 10498\n",
      "batch 619, loss: 0.0821, instance_loss: 0.0068, weighted_loss: 0.0595, label: 1, bag_size: 8012\n",
      "batch 639, loss: 0.0020, instance_loss: 0.0004, weighted_loss: 0.0015, label: 1, bag_size: 645\n",
      "batch 659, loss: 0.0011, instance_loss: 0.0005, weighted_loss: 0.0009, label: 0, bag_size: 10444\n",
      "batch 679, loss: 0.0221, instance_loss: 0.1031, weighted_loss: 0.0464, label: 0, bag_size: 3474\n",
      "batch 699, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11477\n",
      "batch 719, loss: 0.0005, instance_loss: 0.0058, weighted_loss: 0.0021, label: 0, bag_size: 14206\n",
      "batch 739, loss: 0.0066, instance_loss: 0.0010, weighted_loss: 0.0049, label: 1, bag_size: 21827\n",
      "batch 759, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 18649\n",
      "batch 779, loss: 0.0028, instance_loss: 0.0042, weighted_loss: 0.0032, label: 1, bag_size: 3651\n",
      "batch 799, loss: 0.0059, instance_loss: 0.0000, weighted_loss: 0.0041, label: 1, bag_size: 13368\n",
      "batch 819, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 0, bag_size: 2748\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9871189024390243: correct 12951/13120\n",
      "class 1 clustering acc 0.9457317073170731: correct 6204/6560\n",
      "Epoch: 62, train_loss: 0.1456, train_clustering_loss:  0.1199, train_error: 0.0585\n",
      "class 0: acc 0.9361179361179361, correct 381/407\n",
      "class 1: acc 0.9467312348668281, correct 391/413\n",
      "\n",
      "Val Set, val_loss: 0.1396, val_error: 0.0455, auc: 0.9930\n",
      "class 0 clustering acc 0.9556818181818182: correct 1682/1760\n",
      "class 1 clustering acc 0.8613636363636363: correct 758/880\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0459, instance_loss: 0.0117, weighted_loss: 0.0357, label: 0, bag_size: 12910\n",
      "batch 39, loss: 2.4532, instance_loss: 2.3821, weighted_loss: 2.4319, label: 0, bag_size: 7428\n",
      "batch 59, loss: 0.0018, instance_loss: 0.0002, weighted_loss: 0.0013, label: 1, bag_size: 11421\n",
      "batch 79, loss: 0.0054, instance_loss: 0.0025, weighted_loss: 0.0045, label: 0, bag_size: 9387\n",
      "batch 99, loss: 0.0076, instance_loss: 0.0000, weighted_loss: 0.0053, label: 1, bag_size: 6781\n",
      "batch 119, loss: 0.0040, instance_loss: 0.0000, weighted_loss: 0.0028, label: 1, bag_size: 10392\n",
      "batch 139, loss: 0.0002, instance_loss: 0.0002, weighted_loss: 0.0002, label: 0, bag_size: 14377\n",
      "batch 159, loss: 0.0065, instance_loss: 0.0139, weighted_loss: 0.0087, label: 0, bag_size: 5639\n",
      "batch 179, loss: 0.0128, instance_loss: 0.0053, weighted_loss: 0.0105, label: 1, bag_size: 9470\n",
      "batch 199, loss: 0.0859, instance_loss: 0.0244, weighted_loss: 0.0675, label: 0, bag_size: 1920\n",
      "batch 219, loss: 0.1613, instance_loss: 0.0000, weighted_loss: 0.1129, label: 1, bag_size: 20767\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 6851\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 10592\n",
      "batch 279, loss: 0.0016, instance_loss: 0.0098, weighted_loss: 0.0040, label: 0, bag_size: 3970\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 3437\n",
      "batch 319, loss: 0.0235, instance_loss: 0.0000, weighted_loss: 0.0165, label: 1, bag_size: 12575\n",
      "batch 339, loss: 0.0017, instance_loss: 0.0038, weighted_loss: 0.0023, label: 1, bag_size: 3450\n",
      "batch 359, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 5991\n",
      "batch 379, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0011, label: 0, bag_size: 22426\n",
      "batch 399, loss: 0.0053, instance_loss: 0.0046, weighted_loss: 0.0051, label: 1, bag_size: 7981\n",
      "batch 419, loss: 0.0084, instance_loss: 0.0009, weighted_loss: 0.0061, label: 1, bag_size: 2682\n",
      "batch 439, loss: 0.0071, instance_loss: 0.0054, weighted_loss: 0.0066, label: 0, bag_size: 2266\n",
      "batch 459, loss: 0.0001, instance_loss: 1.0151, weighted_loss: 0.3046, label: 0, bag_size: 518\n",
      "batch 479, loss: 0.0734, instance_loss: 0.1357, weighted_loss: 0.0921, label: 0, bag_size: 4598\n",
      "batch 499, loss: 0.0551, instance_loss: 0.0000, weighted_loss: 0.0386, label: 1, bag_size: 7217\n",
      "batch 519, loss: 0.0244, instance_loss: 0.0871, weighted_loss: 0.0433, label: 0, bag_size: 6367\n",
      "batch 539, loss: 0.0730, instance_loss: 0.0350, weighted_loss: 0.0616, label: 1, bag_size: 11316\n",
      "batch 559, loss: 0.1999, instance_loss: 0.0766, weighted_loss: 0.1629, label: 1, bag_size: 2937\n",
      "batch 579, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 1, bag_size: 9689\n",
      "batch 599, loss: 0.0071, instance_loss: 0.0000, weighted_loss: 0.0050, label: 1, bag_size: 29832\n",
      "batch 619, loss: 0.0254, instance_loss: 0.0355, weighted_loss: 0.0285, label: 0, bag_size: 8744\n",
      "batch 639, loss: 0.0004, instance_loss: 0.0272, weighted_loss: 0.0084, label: 0, bag_size: 13591\n",
      "batch 659, loss: 0.0012, instance_loss: 0.0138, weighted_loss: 0.0050, label: 0, bag_size: 22681\n",
      "batch 679, loss: 0.0025, instance_loss: 0.0000, weighted_loss: 0.0017, label: 1, bag_size: 10969\n",
      "batch 699, loss: 0.0251, instance_loss: 0.0000, weighted_loss: 0.0176, label: 1, bag_size: 8592\n",
      "batch 719, loss: 0.0009, instance_loss: 0.0021, weighted_loss: 0.0013, label: 0, bag_size: 16720\n",
      "batch 739, loss: 0.1303, instance_loss: 0.0935, weighted_loss: 0.1193, label: 0, bag_size: 11281\n",
      "batch 759, loss: 0.3102, instance_loss: 0.0241, weighted_loss: 0.2244, label: 0, bag_size: 5211\n",
      "batch 779, loss: 0.0010, instance_loss: 0.0070, weighted_loss: 0.0028, label: 0, bag_size: 1416\n",
      "batch 799, loss: 0.0045, instance_loss: 0.0066, weighted_loss: 0.0052, label: 0, bag_size: 2063\n",
      "batch 819, loss: 0.0009, instance_loss: 0.0552, weighted_loss: 0.0172, label: 0, bag_size: 10128\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9860518292682927: correct 12937/13120\n",
      "class 1 clustering acc 0.9486280487804878: correct 6223/6560\n",
      "Epoch: 63, train_loss: 0.1150, train_clustering_loss:  0.1253, train_error: 0.0500\n",
      "class 0: acc 0.9478672985781991, correct 400/422\n",
      "class 1: acc 0.9522613065326633, correct 379/398\n",
      "\n",
      "Val Set, val_loss: 0.1174, val_error: 0.0273, auc: 0.9934\n",
      "class 0 clustering acc 0.9340909090909091: correct 1644/1760\n",
      "class 1 clustering acc 0.7511363636363636: correct 661/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "Validation loss decreased (0.121262 --> 0.117415).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0035, instance_loss: 0.3573, weighted_loss: 0.1096, label: 1, bag_size: 1755\n",
      "batch 39, loss: 0.0006, instance_loss: 0.0005, weighted_loss: 0.0006, label: 1, bag_size: 4880\n",
      "batch 59, loss: 0.1026, instance_loss: 0.0005, weighted_loss: 0.0720, label: 1, bag_size: 5516\n",
      "batch 79, loss: 0.0071, instance_loss: 0.1758, weighted_loss: 0.0577, label: 1, bag_size: 5366\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 9732\n",
      "batch 119, loss: 1.3486, instance_loss: 0.0312, weighted_loss: 0.9534, label: 1, bag_size: 2565\n",
      "batch 139, loss: 0.0646, instance_loss: 0.0663, weighted_loss: 0.0651, label: 0, bag_size: 24382\n",
      "batch 159, loss: 0.4161, instance_loss: 0.0082, weighted_loss: 0.2937, label: 0, bag_size: 3375\n",
      "batch 179, loss: 0.0503, instance_loss: 0.0000, weighted_loss: 0.0352, label: 1, bag_size: 3674\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 30751\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0002, weighted_loss: 0.0002, label: 1, bag_size: 7935\n",
      "batch 239, loss: 0.0045, instance_loss: 0.0094, weighted_loss: 0.0060, label: 0, bag_size: 15464\n",
      "batch 259, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 11195\n",
      "batch 279, loss: 0.0014, instance_loss: 0.0059, weighted_loss: 0.0027, label: 0, bag_size: 15071\n",
      "batch 299, loss: 0.0243, instance_loss: 0.0008, weighted_loss: 0.0173, label: 0, bag_size: 11654\n",
      "batch 319, loss: 0.0012, instance_loss: 2.8343, weighted_loss: 0.8512, label: 1, bag_size: 2455\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11778\n",
      "batch 359, loss: 0.0121, instance_loss: 0.0000, weighted_loss: 0.0084, label: 1, bag_size: 14887\n",
      "batch 379, loss: 0.0012, instance_loss: 0.0410, weighted_loss: 0.0131, label: 0, bag_size: 10995\n",
      "batch 399, loss: 0.0053, instance_loss: 0.0006, weighted_loss: 0.0038, label: 0, bag_size: 18777\n",
      "batch 419, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 6652\n",
      "batch 439, loss: 0.4180, instance_loss: 2.4667, weighted_loss: 1.0326, label: 0, bag_size: 7835\n",
      "batch 459, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 23714\n",
      "batch 479, loss: 0.3564, instance_loss: 0.0081, weighted_loss: 0.2519, label: 1, bag_size: 5921\n",
      "batch 499, loss: 0.0126, instance_loss: 0.0000, weighted_loss: 0.0088, label: 0, bag_size: 5297\n",
      "batch 519, loss: 1.5853, instance_loss: 0.6335, weighted_loss: 1.2998, label: 0, bag_size: 7428\n",
      "batch 539, loss: 0.0009, instance_loss: 0.0100, weighted_loss: 0.0037, label: 0, bag_size: 1202\n",
      "batch 559, loss: 0.0271, instance_loss: 0.0029, weighted_loss: 0.0199, label: 0, bag_size: 16690\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 10112\n",
      "batch 599, loss: 0.0034, instance_loss: 0.0000, weighted_loss: 0.0024, label: 1, bag_size: 6950\n",
      "batch 619, loss: 0.0001, instance_loss: 0.4222, weighted_loss: 0.1267, label: 0, bag_size: 3101\n",
      "batch 639, loss: 0.0064, instance_loss: 0.0003, weighted_loss: 0.0046, label: 1, bag_size: 11316\n",
      "batch 659, loss: 0.1726, instance_loss: 0.0060, weighted_loss: 0.1226, label: 0, bag_size: 2063\n",
      "batch 679, loss: 0.0073, instance_loss: 0.0162, weighted_loss: 0.0100, label: 0, bag_size: 2195\n",
      "batch 699, loss: 0.0293, instance_loss: 0.1959, weighted_loss: 0.0793, label: 0, bag_size: 8866\n",
      "batch 719, loss: 0.0060, instance_loss: 0.1924, weighted_loss: 0.0619, label: 1, bag_size: 8660\n",
      "batch 739, loss: 0.0007, instance_loss: 0.0001, weighted_loss: 0.0005, label: 1, bag_size: 9673\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15077\n",
      "batch 779, loss: 1.7318, instance_loss: 0.1794, weighted_loss: 1.2661, label: 0, bag_size: 5105\n",
      "batch 799, loss: 0.0829, instance_loss: 0.0015, weighted_loss: 0.0585, label: 1, bag_size: 9519\n",
      "batch 819, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 10671\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9860518292682927: correct 12937/13120\n",
      "class 1 clustering acc 0.9407012195121951: correct 6171/6560\n",
      "Epoch: 64, train_loss: 0.1356, train_clustering_loss:  0.1321, train_error: 0.0500\n",
      "class 0: acc 0.95, correct 399/420\n",
      "class 1: acc 0.95, correct 380/400\n",
      "\n",
      "Val Set, val_loss: 0.1575, val_error: 0.0636, auc: 0.9940\n",
      "class 0 clustering acc 0.9545454545454546: correct 1680/1760\n",
      "class 1 clustering acc 0.8875: correct 781/880\n",
      "class 0: acc 0.8653846153846154, correct 45/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0128, instance_loss: 0.0325, weighted_loss: 0.0187, label: 0, bag_size: 1920\n",
      "batch 39, loss: 0.0617, instance_loss: 0.0055, weighted_loss: 0.0448, label: 0, bag_size: 4418\n",
      "batch 59, loss: 0.0724, instance_loss: 0.0000, weighted_loss: 0.0507, label: 1, bag_size: 6927\n",
      "batch 79, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 8448\n",
      "batch 99, loss: 0.0173, instance_loss: 0.0407, weighted_loss: 0.0243, label: 0, bag_size: 2609\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 11642\n",
      "batch 139, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 10725\n",
      "batch 159, loss: 0.0003, instance_loss: 0.0102, weighted_loss: 0.0032, label: 0, bag_size: 9930\n",
      "batch 179, loss: 0.0010, instance_loss: 0.0180, weighted_loss: 0.0061, label: 0, bag_size: 9786\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 7191\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 11981\n",
      "batch 239, loss: 0.0050, instance_loss: 0.0000, weighted_loss: 0.0035, label: 1, bag_size: 20161\n",
      "batch 259, loss: 0.0059, instance_loss: 0.0000, weighted_loss: 0.0041, label: 1, bag_size: 8475\n",
      "batch 279, loss: 0.0026, instance_loss: 0.0177, weighted_loss: 0.0072, label: 0, bag_size: 2351\n",
      "batch 299, loss: 0.4902, instance_loss: 0.0165, weighted_loss: 0.3481, label: 0, bag_size: 2043\n",
      "batch 319, loss: 0.0002, instance_loss: 0.1299, weighted_loss: 0.0391, label: 0, bag_size: 9930\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 10394\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0045, weighted_loss: 0.0014, label: 1, bag_size: 1412\n",
      "batch 379, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 12865\n",
      "batch 399, loss: 0.0144, instance_loss: 0.2107, weighted_loss: 0.0733, label: 1, bag_size: 1924\n",
      "batch 419, loss: 0.0133, instance_loss: 0.2736, weighted_loss: 0.0914, label: 0, bag_size: 2063\n",
      "batch 439, loss: 0.0227, instance_loss: 0.2528, weighted_loss: 0.0917, label: 0, bag_size: 3893\n",
      "batch 459, loss: 0.0235, instance_loss: 0.0158, weighted_loss: 0.0212, label: 1, bag_size: 8191\n",
      "batch 479, loss: 0.0001, instance_loss: 0.1798, weighted_loss: 0.0540, label: 0, bag_size: 518\n",
      "batch 499, loss: 0.4123, instance_loss: 1.5920, weighted_loss: 0.7662, label: 1, bag_size: 5516\n",
      "batch 519, loss: 0.0907, instance_loss: 0.0944, weighted_loss: 0.0918, label: 0, bag_size: 6367\n",
      "batch 539, loss: 0.0168, instance_loss: 0.0634, weighted_loss: 0.0308, label: 0, bag_size: 1690\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0055, weighted_loss: 0.0017, label: 0, bag_size: 11900\n",
      "batch 579, loss: 0.0003, instance_loss: 0.0142, weighted_loss: 0.0044, label: 0, bag_size: 8981\n",
      "batch 599, loss: 0.0002, instance_loss: 0.0158, weighted_loss: 0.0049, label: 1, bag_size: 4862\n",
      "batch 619, loss: 0.0036, instance_loss: 0.0854, weighted_loss: 0.0282, label: 1, bag_size: 8660\n",
      "batch 639, loss: 0.0130, instance_loss: 0.0290, weighted_loss: 0.0178, label: 0, bag_size: 1234\n",
      "batch 659, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 9571\n",
      "batch 679, loss: 0.0075, instance_loss: 0.0022, weighted_loss: 0.0059, label: 1, bag_size: 10969\n",
      "batch 699, loss: 0.0093, instance_loss: 0.6471, weighted_loss: 0.2007, label: 0, bag_size: 9387\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 14779\n",
      "batch 739, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 8410\n",
      "batch 759, loss: 0.0040, instance_loss: 0.1176, weighted_loss: 0.0381, label: 0, bag_size: 2195\n",
      "batch 779, loss: 0.0110, instance_loss: 0.1778, weighted_loss: 0.0610, label: 1, bag_size: 1755\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 15665\n",
      "batch 819, loss: 0.0153, instance_loss: 0.0180, weighted_loss: 0.0161, label: 0, bag_size: 6884\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9797256097560976: correct 12854/13120\n",
      "class 1 clustering acc 0.9147865853658537: correct 6001/6560\n",
      "Epoch: 65, train_loss: 0.1088, train_clustering_loss:  0.1995, train_error: 0.0402\n",
      "class 0: acc 0.9538834951456311, correct 393/412\n",
      "class 1: acc 0.9656862745098039, correct 394/408\n",
      "\n",
      "Val Set, val_loss: 0.1165, val_error: 0.0455, auc: 0.9934\n",
      "class 0 clustering acc 0.9443181818181818: correct 1662/1760\n",
      "class 1 clustering acc 0.8431818181818181: correct 742/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9482758620689655, correct 55/58\n",
      "Validation loss decreased (0.117415 --> 0.116529).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0010, instance_loss: 0.0217, weighted_loss: 0.0072, label: 1, bag_size: 2662\n",
      "batch 39, loss: 0.7614, instance_loss: 0.0249, weighted_loss: 0.5404, label: 0, bag_size: 5120\n",
      "batch 59, loss: 2.1527, instance_loss: 0.1489, weighted_loss: 1.5516, label: 1, bag_size: 9215\n",
      "batch 79, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 8448\n",
      "batch 99, loss: 0.3018, instance_loss: 0.0114, weighted_loss: 0.2147, label: 0, bag_size: 2219\n",
      "batch 119, loss: 0.0909, instance_loss: 0.0520, weighted_loss: 0.0792, label: 1, bag_size: 4054\n",
      "batch 139, loss: 0.1001, instance_loss: 0.0706, weighted_loss: 0.0913, label: 0, bag_size: 1953\n",
      "batch 159, loss: 0.0014, instance_loss: 0.5714, weighted_loss: 0.1724, label: 0, bag_size: 15003\n",
      "batch 179, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 11642\n",
      "batch 199, loss: 0.0008, instance_loss: 0.0034, weighted_loss: 0.0016, label: 1, bag_size: 11421\n",
      "batch 219, loss: 0.0100, instance_loss: 0.1080, weighted_loss: 0.0394, label: 0, bag_size: 1891\n",
      "batch 239, loss: 0.1273, instance_loss: 0.1928, weighted_loss: 0.1469, label: 1, bag_size: 5516\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0210, weighted_loss: 0.0064, label: 0, bag_size: 9485\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0525, weighted_loss: 0.0157, label: 1, bag_size: 1412\n",
      "batch 299, loss: 0.0599, instance_loss: 0.0938, weighted_loss: 0.0701, label: 0, bag_size: 6884\n",
      "batch 319, loss: 0.0314, instance_loss: 0.2962, weighted_loss: 0.1108, label: 1, bag_size: 6682\n",
      "batch 339, loss: 0.0028, instance_loss: 0.0254, weighted_loss: 0.0096, label: 0, bag_size: 1202\n",
      "batch 359, loss: 0.0003, instance_loss: 0.0001, weighted_loss: 0.0003, label: 1, bag_size: 14202\n",
      "batch 379, loss: 0.0006, instance_loss: 0.0047, weighted_loss: 0.0018, label: 0, bag_size: 11125\n",
      "batch 399, loss: 0.0002, instance_loss: 0.0011, weighted_loss: 0.0005, label: 1, bag_size: 3409\n",
      "batch 419, loss: 0.0184, instance_loss: 1.7162, weighted_loss: 0.5278, label: 1, bag_size: 21252\n",
      "batch 439, loss: 0.3458, instance_loss: 0.1515, weighted_loss: 0.2875, label: 1, bag_size: 1919\n",
      "batch 459, loss: 0.0065, instance_loss: 0.4810, weighted_loss: 0.1489, label: 0, bag_size: 11607\n",
      "batch 479, loss: 0.0027, instance_loss: 0.0332, weighted_loss: 0.0118, label: 0, bag_size: 2242\n",
      "batch 499, loss: 0.0002, instance_loss: 0.0339, weighted_loss: 0.0103, label: 1, bag_size: 2136\n",
      "batch 519, loss: 0.0068, instance_loss: 0.0001, weighted_loss: 0.0048, label: 1, bag_size: 11684\n",
      "batch 539, loss: 0.0009, instance_loss: 0.0138, weighted_loss: 0.0048, label: 1, bag_size: 9478\n",
      "batch 559, loss: 0.0240, instance_loss: 0.4971, weighted_loss: 0.1660, label: 0, bag_size: 1560\n",
      "batch 579, loss: 0.0587, instance_loss: 0.0029, weighted_loss: 0.0420, label: 0, bag_size: 8549\n",
      "batch 599, loss: 0.5738, instance_loss: 0.0027, weighted_loss: 0.4024, label: 1, bag_size: 12714\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 7935\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21218\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 7650\n",
      "batch 679, loss: 0.2824, instance_loss: 0.0337, weighted_loss: 0.2078, label: 1, bag_size: 7351\n",
      "batch 699, loss: 0.0013, instance_loss: 0.0041, weighted_loss: 0.0021, label: 1, bag_size: 7371\n",
      "batch 719, loss: 0.0374, instance_loss: 0.6185, weighted_loss: 0.2118, label: 1, bag_size: 9983\n",
      "batch 739, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 19932\n",
      "batch 759, loss: 0.0264, instance_loss: 0.0338, weighted_loss: 0.0286, label: 0, bag_size: 6898\n",
      "batch 779, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 11113\n",
      "batch 799, loss: 0.0071, instance_loss: 0.0615, weighted_loss: 0.0234, label: 0, bag_size: 2004\n",
      "batch 819, loss: 0.0700, instance_loss: 0.0017, weighted_loss: 0.0495, label: 1, bag_size: 9942\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9823170731707317: correct 12888/13120\n",
      "class 1 clustering acc 0.9314024390243902: correct 6110/6560\n",
      "Epoch: 66, train_loss: 0.1069, train_clustering_loss:  0.1669, train_error: 0.0476\n",
      "class 0: acc 0.9542168674698795, correct 396/415\n",
      "class 1: acc 0.9506172839506173, correct 385/405\n",
      "\n",
      "Val Set, val_loss: 0.1269, val_error: 0.0364, auc: 0.9920\n",
      "class 0 clustering acc 0.9613636363636363: correct 1692/1760\n",
      "class 1 clustering acc 0.9079545454545455: correct 799/880\n",
      "class 0: acc 0.9230769230769231, correct 48/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 4423\n",
      "batch 39, loss: 0.0764, instance_loss: 0.0083, weighted_loss: 0.0560, label: 1, bag_size: 15125\n",
      "batch 59, loss: 0.0157, instance_loss: 0.0048, weighted_loss: 0.0124, label: 1, bag_size: 15609\n",
      "batch 79, loss: 0.0004, instance_loss: 0.0160, weighted_loss: 0.0051, label: 0, bag_size: 9471\n",
      "batch 99, loss: 0.0008, instance_loss: 0.0348, weighted_loss: 0.0110, label: 0, bag_size: 2628\n",
      "batch 119, loss: 0.2089, instance_loss: 0.0176, weighted_loss: 0.1515, label: 0, bag_size: 2219\n",
      "batch 139, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 22426\n",
      "batch 159, loss: 0.0052, instance_loss: 0.0230, weighted_loss: 0.0105, label: 1, bag_size: 10969\n",
      "batch 179, loss: 0.0298, instance_loss: 0.0461, weighted_loss: 0.0347, label: 1, bag_size: 5366\n",
      "batch 199, loss: 0.0010, instance_loss: 0.0308, weighted_loss: 0.0099, label: 0, bag_size: 4902\n",
      "batch 219, loss: 0.5812, instance_loss: 0.0718, weighted_loss: 0.4284, label: 1, bag_size: 2137\n",
      "batch 239, loss: 0.4904, instance_loss: 0.4119, weighted_loss: 0.4668, label: 0, bag_size: 13619\n",
      "batch 259, loss: 0.0024, instance_loss: 0.0255, weighted_loss: 0.0093, label: 0, bag_size: 12083\n",
      "batch 279, loss: 0.0002, instance_loss: 0.0096, weighted_loss: 0.0030, label: 0, bag_size: 12217\n",
      "batch 299, loss: 0.0606, instance_loss: 0.0086, weighted_loss: 0.0450, label: 1, bag_size: 10591\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0099, weighted_loss: 0.0030, label: 0, bag_size: 11527\n",
      "batch 339, loss: 0.0447, instance_loss: 0.0027, weighted_loss: 0.0321, label: 0, bag_size: 15672\n",
      "batch 359, loss: 0.0205, instance_loss: 0.0058, weighted_loss: 0.0161, label: 1, bag_size: 7798\n",
      "batch 379, loss: 0.5605, instance_loss: 0.1854, weighted_loss: 0.4480, label: 0, bag_size: 18516\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 22800\n",
      "batch 419, loss: 0.0032, instance_loss: 0.0005, weighted_loss: 0.0024, label: 1, bag_size: 7119\n",
      "batch 439, loss: 0.0002, instance_loss: 0.0091, weighted_loss: 0.0029, label: 0, bag_size: 8788\n",
      "batch 459, loss: 0.4638, instance_loss: 0.0058, weighted_loss: 0.3264, label: 1, bag_size: 4789\n",
      "batch 479, loss: 0.0095, instance_loss: 0.0082, weighted_loss: 0.0091, label: 1, bag_size: 4786\n",
      "batch 499, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 10482\n",
      "batch 519, loss: 0.0003, instance_loss: 0.0947, weighted_loss: 0.0286, label: 0, bag_size: 2920\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23398\n",
      "batch 559, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 2244\n",
      "batch 579, loss: 0.0143, instance_loss: 0.0097, weighted_loss: 0.0129, label: 1, bag_size: 6781\n",
      "batch 599, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 8522\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11113\n",
      "batch 639, loss: 0.6410, instance_loss: 2.8768, weighted_loss: 1.3118, label: 1, bag_size: 15185\n",
      "batch 659, loss: 0.0439, instance_loss: 0.0749, weighted_loss: 0.0532, label: 0, bag_size: 1234\n",
      "batch 679, loss: 0.0011, instance_loss: 0.0583, weighted_loss: 0.0183, label: 1, bag_size: 1924\n",
      "batch 699, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 5225\n",
      "batch 719, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 14681\n",
      "batch 739, loss: 0.0041, instance_loss: 0.0712, weighted_loss: 0.0242, label: 0, bag_size: 1234\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 15332\n",
      "batch 779, loss: 0.0130, instance_loss: 0.0962, weighted_loss: 0.0380, label: 0, bag_size: 1772\n",
      "batch 799, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 9971\n",
      "batch 819, loss: 0.0025, instance_loss: 0.0550, weighted_loss: 0.0182, label: 0, bag_size: 7989\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9826219512195122: correct 12892/13120\n",
      "class 1 clustering acc 0.9292682926829269: correct 6096/6560\n",
      "Epoch: 67, train_loss: 0.1043, train_clustering_loss:  0.1509, train_error: 0.0439\n",
      "class 0: acc 0.9476439790575916, correct 362/382\n",
      "class 1: acc 0.9634703196347032, correct 422/438\n",
      "\n",
      "Val Set, val_loss: 0.1171, val_error: 0.0273, auc: 0.9930\n",
      "class 0 clustering acc 0.9556818181818182: correct 1682/1760\n",
      "class 1 clustering acc 0.8909090909090909: correct 784/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0230, instance_loss: 0.0038, weighted_loss: 0.0173, label: 0, bag_size: 25814\n",
      "batch 39, loss: 0.0759, instance_loss: 0.0000, weighted_loss: 0.0531, label: 1, bag_size: 13732\n",
      "batch 59, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15077\n",
      "batch 79, loss: 0.0014, instance_loss: 0.8976, weighted_loss: 0.2702, label: 1, bag_size: 22264\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0417, weighted_loss: 0.0126, label: 0, bag_size: 13591\n",
      "batch 119, loss: 0.0106, instance_loss: 0.0463, weighted_loss: 0.0213, label: 0, bag_size: 1797\n",
      "batch 139, loss: 0.0006, instance_loss: 0.0009, weighted_loss: 0.0007, label: 1, bag_size: 9446\n",
      "batch 159, loss: 0.0346, instance_loss: 0.0000, weighted_loss: 0.0242, label: 1, bag_size: 8466\n",
      "batch 179, loss: 0.0286, instance_loss: 0.0004, weighted_loss: 0.0201, label: 0, bag_size: 3444\n",
      "batch 199, loss: 0.0123, instance_loss: 0.0000, weighted_loss: 0.0086, label: 1, bag_size: 9062\n",
      "batch 219, loss: 0.0033, instance_loss: 0.3064, weighted_loss: 0.0942, label: 0, bag_size: 2367\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0024, weighted_loss: 0.0007, label: 0, bag_size: 9851\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 15665\n",
      "batch 279, loss: 0.0082, instance_loss: 0.0002, weighted_loss: 0.0058, label: 1, bag_size: 16379\n",
      "batch 299, loss: 0.0406, instance_loss: 0.4184, weighted_loss: 0.1539, label: 1, bag_size: 5516\n",
      "batch 319, loss: 0.0290, instance_loss: 0.1171, weighted_loss: 0.0555, label: 1, bag_size: 1483\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 14779\n",
      "batch 359, loss: 0.0012, instance_loss: 0.0201, weighted_loss: 0.0069, label: 0, bag_size: 3265\n",
      "batch 379, loss: 0.0066, instance_loss: 0.2655, weighted_loss: 0.0842, label: 1, bag_size: 1064\n",
      "batch 399, loss: 0.0004, instance_loss: 0.0154, weighted_loss: 0.0049, label: 0, bag_size: 9252\n",
      "batch 419, loss: 0.0153, instance_loss: 0.0057, weighted_loss: 0.0124, label: 1, bag_size: 7371\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 10920\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 9851\n",
      "batch 479, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 19043\n",
      "batch 499, loss: 0.0026, instance_loss: 0.0006, weighted_loss: 0.0020, label: 0, bag_size: 7989\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 7935\n",
      "batch 539, loss: 0.0143, instance_loss: 0.0048, weighted_loss: 0.0114, label: 1, bag_size: 14230\n",
      "batch 559, loss: 0.0059, instance_loss: 0.0740, weighted_loss: 0.0263, label: 0, bag_size: 2652\n",
      "batch 579, loss: 0.0007, instance_loss: 0.0031, weighted_loss: 0.0014, label: 0, bag_size: 9471\n",
      "batch 599, loss: 0.0421, instance_loss: 0.1020, weighted_loss: 0.0600, label: 0, bag_size: 1814\n",
      "batch 619, loss: 0.0259, instance_loss: 0.1979, weighted_loss: 0.0775, label: 1, bag_size: 3211\n",
      "batch 639, loss: 0.0035, instance_loss: 0.1901, weighted_loss: 0.0595, label: 0, bag_size: 3474\n",
      "batch 659, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 21076\n",
      "batch 679, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 21404\n",
      "batch 699, loss: 0.0025, instance_loss: 0.0130, weighted_loss: 0.0057, label: 0, bag_size: 2360\n",
      "batch 719, loss: 0.1717, instance_loss: 0.0057, weighted_loss: 0.1219, label: 0, bag_size: 24382\n",
      "batch 739, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 10068\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9644\n",
      "batch 779, loss: 0.0503, instance_loss: 0.0000, weighted_loss: 0.0352, label: 1, bag_size: 11701\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 10898\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 10898\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9835365853658536: correct 12904/13120\n",
      "class 1 clustering acc 0.936890243902439: correct 6146/6560\n",
      "Epoch: 68, train_loss: 0.1184, train_clustering_loss:  0.1307, train_error: 0.0427\n",
      "class 0: acc 0.9532019704433498, correct 387/406\n",
      "class 1: acc 0.961352657004831, correct 398/414\n",
      "\n",
      "Val Set, val_loss: 0.1432, val_error: 0.0545, auc: 0.9927\n",
      "class 0 clustering acc 0.9619318181818182: correct 1693/1760\n",
      "class 1 clustering acc 0.9056818181818181: correct 797/880\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0029, instance_loss: 0.0002, weighted_loss: 0.0021, label: 1, bag_size: 2278\n",
      "batch 39, loss: 1.8580, instance_loss: 0.0134, weighted_loss: 1.3046, label: 1, bag_size: 1703\n",
      "batch 59, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 14618\n",
      "batch 79, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 2904\n",
      "batch 99, loss: 0.0605, instance_loss: 0.0042, weighted_loss: 0.0436, label: 0, bag_size: 1498\n",
      "batch 119, loss: 0.0492, instance_loss: 0.0011, weighted_loss: 0.0348, label: 0, bag_size: 4418\n",
      "batch 139, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 15464\n",
      "batch 159, loss: 0.0085, instance_loss: 0.0006, weighted_loss: 0.0062, label: 1, bag_size: 4054\n",
      "batch 179, loss: 0.0002, instance_loss: 0.0040, weighted_loss: 0.0013, label: 0, bag_size: 11527\n",
      "batch 199, loss: 0.2736, instance_loss: 0.0008, weighted_loss: 0.1917, label: 0, bag_size: 1732\n",
      "batch 219, loss: 0.0434, instance_loss: 0.0000, weighted_loss: 0.0304, label: 0, bag_size: 10898\n",
      "batch 239, loss: 0.0001, instance_loss: 0.2094, weighted_loss: 0.0629, label: 1, bag_size: 1746\n",
      "batch 259, loss: 0.0164, instance_loss: 0.0093, weighted_loss: 0.0143, label: 1, bag_size: 9470\n",
      "batch 279, loss: 0.0015, instance_loss: 0.0006, weighted_loss: 0.0012, label: 0, bag_size: 3541\n",
      "batch 299, loss: 0.0001, instance_loss: 0.2314, weighted_loss: 0.0695, label: 1, bag_size: 2140\n",
      "batch 319, loss: 0.0403, instance_loss: 0.0161, weighted_loss: 0.0330, label: 0, bag_size: 1909\n",
      "batch 339, loss: 0.0059, instance_loss: 0.0000, weighted_loss: 0.0041, label: 0, bag_size: 11194\n",
      "batch 359, loss: 0.0006, instance_loss: 0.0024, weighted_loss: 0.0011, label: 0, bag_size: 2179\n",
      "batch 379, loss: 0.0072, instance_loss: 0.0000, weighted_loss: 0.0051, label: 0, bag_size: 19808\n",
      "batch 399, loss: 0.1983, instance_loss: 2.7416, weighted_loss: 0.9613, label: 0, bag_size: 1714\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 3459\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 19832\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11195\n",
      "batch 479, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 18240\n",
      "batch 499, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 2244\n",
      "batch 519, loss: 0.0153, instance_loss: 0.0147, weighted_loss: 0.0151, label: 0, bag_size: 1690\n",
      "batch 539, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 15464\n",
      "batch 559, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0011, label: 1, bag_size: 13255\n",
      "batch 579, loss: 0.1860, instance_loss: 0.0034, weighted_loss: 0.1312, label: 1, bag_size: 14887\n",
      "batch 599, loss: 0.0370, instance_loss: 0.0445, weighted_loss: 0.0393, label: 1, bag_size: 6682\n",
      "batch 619, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 19832\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 10146\n",
      "batch 659, loss: 0.0423, instance_loss: 0.2172, weighted_loss: 0.0947, label: 0, bag_size: 15898\n",
      "batch 679, loss: 0.0004, instance_loss: 0.0003, weighted_loss: 0.0004, label: 1, bag_size: 2662\n",
      "batch 699, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 12178\n",
      "batch 719, loss: 0.0040, instance_loss: 0.0000, weighted_loss: 0.0028, label: 0, bag_size: 3810\n",
      "batch 739, loss: 0.0012, instance_loss: 2.5791, weighted_loss: 0.7746, label: 1, bag_size: 3856\n",
      "batch 759, loss: 0.0049, instance_loss: 0.0248, weighted_loss: 0.0109, label: 1, bag_size: 645\n",
      "batch 779, loss: 0.0136, instance_loss: 0.4405, weighted_loss: 0.1417, label: 1, bag_size: 7873\n",
      "batch 799, loss: 0.0082, instance_loss: 0.1758, weighted_loss: 0.0585, label: 0, bag_size: 2367\n",
      "batch 819, loss: 0.0009, instance_loss: 0.0113, weighted_loss: 0.0040, label: 1, bag_size: 9478\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9891006097560976: correct 12977/13120\n",
      "class 1 clustering acc 0.9554878048780487: correct 6268/6560\n",
      "Epoch: 69, train_loss: 0.1274, train_clustering_loss:  0.1081, train_error: 0.0476\n",
      "class 0: acc 0.947103274559194, correct 376/397\n",
      "class 1: acc 0.9574468085106383, correct 405/423\n",
      "\n",
      "Val Set, val_loss: 0.1565, val_error: 0.0727, auc: 0.9960\n",
      "class 0 clustering acc 0.9545454545454546: correct 1680/1760\n",
      "class 1 clustering acc 0.8625: correct 759/880\n",
      "class 0: acc 0.8461538461538461, correct 44/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0025, instance_loss: 0.0077, weighted_loss: 0.0041, label: 1, bag_size: 9519\n",
      "batch 39, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 3459\n",
      "batch 59, loss: 0.0010, instance_loss: 0.0258, weighted_loss: 0.0084, label: 0, bag_size: 8330\n",
      "batch 79, loss: 0.0057, instance_loss: 0.0388, weighted_loss: 0.0156, label: 0, bag_size: 2179\n",
      "batch 99, loss: 0.0003, instance_loss: 0.0283, weighted_loss: 0.0087, label: 0, bag_size: 8959\n",
      "batch 119, loss: 0.0073, instance_loss: 0.0019, weighted_loss: 0.0057, label: 1, bag_size: 1459\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 3640\n",
      "batch 159, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 1, bag_size: 20161\n",
      "batch 179, loss: 0.0022, instance_loss: 0.0000, weighted_loss: 0.0015, label: 0, bag_size: 2336\n",
      "batch 199, loss: 0.0184, instance_loss: 0.0000, weighted_loss: 0.0129, label: 1, bag_size: 11701\n",
      "batch 219, loss: 0.0011, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 22681\n",
      "batch 239, loss: 0.0045, instance_loss: 0.0006, weighted_loss: 0.0033, label: 1, bag_size: 2495\n",
      "batch 259, loss: 2.3826, instance_loss: 7.5243, weighted_loss: 3.9251, label: 1, bag_size: 21450\n",
      "batch 279, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 1, bag_size: 4128\n",
      "batch 299, loss: 0.0001, instance_loss: 0.0251, weighted_loss: 0.0076, label: 0, bag_size: 518\n",
      "batch 319, loss: 1.0943, instance_loss: 0.5863, weighted_loss: 0.9419, label: 0, bag_size: 2959\n",
      "batch 339, loss: 0.0149, instance_loss: 0.0065, weighted_loss: 0.0124, label: 0, bag_size: 3908\n",
      "batch 359, loss: 0.2476, instance_loss: 0.0000, weighted_loss: 0.1733, label: 1, bag_size: 9942\n",
      "batch 379, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 5864\n",
      "batch 399, loss: 0.2031, instance_loss: 0.1027, weighted_loss: 0.1730, label: 1, bag_size: 1533\n",
      "batch 419, loss: 0.0294, instance_loss: 0.0170, weighted_loss: 0.0257, label: 0, bag_size: 4959\n",
      "batch 439, loss: 0.0018, instance_loss: 0.0067, weighted_loss: 0.0033, label: 0, bag_size: 5409\n",
      "batch 459, loss: 2.6720, instance_loss: 0.0071, weighted_loss: 1.8725, label: 1, bag_size: 9162\n",
      "batch 479, loss: 0.0006, instance_loss: 0.0972, weighted_loss: 0.0295, label: 0, bag_size: 10068\n",
      "batch 499, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 15636\n",
      "batch 519, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 6343\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 5864\n",
      "batch 559, loss: 0.0027, instance_loss: 0.0000, weighted_loss: 0.0019, label: 1, bag_size: 16379\n",
      "batch 579, loss: 0.0154, instance_loss: 0.4402, weighted_loss: 0.1429, label: 0, bag_size: 2814\n",
      "batch 599, loss: 3.4946, instance_loss: 4.5215, weighted_loss: 3.8027, label: 1, bag_size: 15185\n",
      "batch 619, loss: 0.2445, instance_loss: 0.0871, weighted_loss: 0.1973, label: 0, bag_size: 25420\n",
      "batch 639, loss: 0.0081, instance_loss: 0.0000, weighted_loss: 0.0057, label: 0, bag_size: 11194\n",
      "batch 659, loss: 0.0093, instance_loss: 0.0000, weighted_loss: 0.0065, label: 1, bag_size: 30675\n",
      "batch 679, loss: 0.0189, instance_loss: 0.0458, weighted_loss: 0.0270, label: 1, bag_size: 1483\n",
      "batch 699, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 11389\n",
      "batch 719, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 7515\n",
      "batch 739, loss: 0.0018, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 16211\n",
      "batch 759, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 6792\n",
      "batch 779, loss: 0.1222, instance_loss: 0.0823, weighted_loss: 0.1102, label: 0, bag_size: 6356\n",
      "batch 799, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 23398\n",
      "batch 819, loss: 0.1616, instance_loss: 0.1997, weighted_loss: 0.1730, label: 1, bag_size: 2681\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.988795731707317: correct 12973/13120\n",
      "class 1 clustering acc 0.9513719512195122: correct 6241/6560\n",
      "Epoch: 70, train_loss: 0.1239, train_clustering_loss:  0.1031, train_error: 0.0439\n",
      "class 0: acc 0.9556650246305419, correct 388/406\n",
      "class 1: acc 0.9565217391304348, correct 396/414\n",
      "\n",
      "Val Set, val_loss: 0.1088, val_error: 0.0273, auc: 0.9944\n",
      "class 0 clustering acc 0.959659090909091: correct 1689/1760\n",
      "class 1 clustering acc 0.9034090909090909: correct 795/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "Validation loss decreased (0.116529 --> 0.108824).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0013, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 18045\n",
      "batch 39, loss: 0.1591, instance_loss: 0.0679, weighted_loss: 0.1317, label: 0, bag_size: 2270\n",
      "batch 59, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0014, label: 0, bag_size: 4902\n",
      "batch 79, loss: 0.0157, instance_loss: 0.1706, weighted_loss: 0.0621, label: 0, bag_size: 705\n",
      "batch 99, loss: 0.0017, instance_loss: 0.0001, weighted_loss: 0.0012, label: 0, bag_size: 8582\n",
      "batch 119, loss: 4.5714, instance_loss: 3.1544, weighted_loss: 4.1463, label: 1, bag_size: 15563\n",
      "batch 139, loss: 0.0135, instance_loss: 0.0105, weighted_loss: 0.0126, label: 1, bag_size: 1888\n",
      "batch 159, loss: 0.0054, instance_loss: 0.0000, weighted_loss: 0.0038, label: 0, bag_size: 47866\n",
      "batch 179, loss: 0.0002, instance_loss: 0.0041, weighted_loss: 0.0014, label: 0, bag_size: 16720\n",
      "batch 199, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 18095\n",
      "batch 219, loss: 0.1038, instance_loss: 0.0000, weighted_loss: 0.0727, label: 1, bag_size: 10460\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 6875\n",
      "batch 259, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 19466\n",
      "batch 279, loss: 0.0063, instance_loss: 0.0000, weighted_loss: 0.0044, label: 1, bag_size: 25970\n",
      "batch 299, loss: 0.0026, instance_loss: 0.0834, weighted_loss: 0.0268, label: 0, bag_size: 1745\n",
      "batch 319, loss: 0.0055, instance_loss: 0.0000, weighted_loss: 0.0038, label: 1, bag_size: 30675\n",
      "batch 339, loss: 0.0151, instance_loss: 0.2560, weighted_loss: 0.0874, label: 0, bag_size: 931\n",
      "batch 359, loss: 0.1387, instance_loss: 0.0169, weighted_loss: 0.1022, label: 1, bag_size: 4939\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 10791\n",
      "batch 399, loss: 0.0599, instance_loss: 0.0000, weighted_loss: 0.0419, label: 1, bag_size: 15609\n",
      "batch 419, loss: 0.0213, instance_loss: 0.0284, weighted_loss: 0.0234, label: 1, bag_size: 1051\n",
      "batch 439, loss: 0.0159, instance_loss: 0.0055, weighted_loss: 0.0128, label: 1, bag_size: 1920\n",
      "batch 459, loss: 0.0010, instance_loss: 0.1190, weighted_loss: 0.0364, label: 0, bag_size: 2457\n",
      "batch 479, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 1, bag_size: 14223\n",
      "batch 499, loss: 0.0019, instance_loss: 0.0000, weighted_loss: 0.0013, label: 1, bag_size: 11684\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 518\n",
      "batch 539, loss: 0.0073, instance_loss: 0.0000, weighted_loss: 0.0051, label: 0, bag_size: 21319\n",
      "batch 559, loss: 0.1011, instance_loss: 0.1742, weighted_loss: 0.1230, label: 0, bag_size: 2290\n",
      "batch 579, loss: 1.7487, instance_loss: 1.9784, weighted_loss: 1.8176, label: 1, bag_size: 15563\n",
      "batch 599, loss: 1.8000, instance_loss: 3.1914, weighted_loss: 2.2174, label: 0, bag_size: 7239\n",
      "batch 619, loss: 0.0237, instance_loss: 0.0114, weighted_loss: 0.0200, label: 0, bag_size: 2219\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9078\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 6317\n",
      "batch 679, loss: 0.0011, instance_loss: 0.1310, weighted_loss: 0.0401, label: 0, bag_size: 11146\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 20150\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9610\n",
      "batch 739, loss: 0.0084, instance_loss: 0.0000, weighted_loss: 0.0059, label: 1, bag_size: 8754\n",
      "batch 759, loss: 0.0022, instance_loss: 0.0000, weighted_loss: 0.0016, label: 1, bag_size: 18794\n",
      "batch 779, loss: 0.0387, instance_loss: 0.0000, weighted_loss: 0.0271, label: 1, bag_size: 12714\n",
      "batch 799, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 7371\n",
      "batch 819, loss: 0.0030, instance_loss: 0.0105, weighted_loss: 0.0053, label: 0, bag_size: 2266\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9895579268292682: correct 12983/13120\n",
      "class 1 clustering acc 0.9579268292682926: correct 6284/6560\n",
      "Epoch: 71, train_loss: 0.1118, train_clustering_loss:  0.0965, train_error: 0.0439\n",
      "class 0: acc 0.9535452322738386, correct 390/409\n",
      "class 1: acc 0.9586374695863747, correct 394/411\n",
      "\n",
      "Val Set, val_loss: 0.1345, val_error: 0.0545, auc: 0.9924\n",
      "class 0 clustering acc 0.9477272727272728: correct 1668/1760\n",
      "class 1 clustering acc 0.8875: correct 781/880\n",
      "class 0: acc 0.9807692307692307, correct 51/52\n",
      "class 1: acc 0.9137931034482759, correct 53/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0014, instance_loss: 0.0015, weighted_loss: 0.0015, label: 0, bag_size: 1789\n",
      "batch 39, loss: 0.3982, instance_loss: 0.0653, weighted_loss: 0.2984, label: 1, bag_size: 10591\n",
      "batch 59, loss: 0.0661, instance_loss: 0.0000, weighted_loss: 0.0463, label: 0, bag_size: 15672\n",
      "batch 79, loss: 0.1944, instance_loss: 0.0730, weighted_loss: 0.1580, label: 1, bag_size: 2395\n",
      "batch 99, loss: 0.0006, instance_loss: 0.0001, weighted_loss: 0.0004, label: 1, bag_size: 9571\n",
      "batch 119, loss: 0.0088, instance_loss: 0.0032, weighted_loss: 0.0072, label: 1, bag_size: 12719\n",
      "batch 139, loss: 0.0099, instance_loss: 0.0248, weighted_loss: 0.0143, label: 0, bag_size: 14249\n",
      "batch 159, loss: 0.1482, instance_loss: 0.6877, weighted_loss: 0.3100, label: 1, bag_size: 8438\n",
      "batch 179, loss: 0.0086, instance_loss: 0.0030, weighted_loss: 0.0069, label: 1, bag_size: 1459\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 15716\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0008, weighted_loss: 0.0004, label: 1, bag_size: 1437\n",
      "batch 239, loss: 0.6716, instance_loss: 0.0083, weighted_loss: 0.4726, label: 1, bag_size: 2565\n",
      "batch 259, loss: 0.0007, instance_loss: 0.0033, weighted_loss: 0.0015, label: 1, bag_size: 645\n",
      "batch 279, loss: 0.1348, instance_loss: 0.0018, weighted_loss: 0.0949, label: 1, bag_size: 4956\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9078\n",
      "batch 319, loss: 0.0276, instance_loss: 0.0002, weighted_loss: 0.0194, label: 0, bag_size: 10942\n",
      "batch 339, loss: 0.3443, instance_loss: 0.0000, weighted_loss: 0.2410, label: 1, bag_size: 1764\n",
      "batch 359, loss: 0.0159, instance_loss: 0.0000, weighted_loss: 0.0111, label: 1, bag_size: 10848\n",
      "batch 379, loss: 0.0016, instance_loss: 0.2201, weighted_loss: 0.0672, label: 0, bag_size: 1881\n",
      "batch 399, loss: 0.0017, instance_loss: 0.0091, weighted_loss: 0.0039, label: 0, bag_size: 17268\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9078\n",
      "batch 439, loss: 0.0004, instance_loss: 0.0001, weighted_loss: 0.0003, label: 1, bag_size: 3651\n",
      "batch 459, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 14779\n",
      "batch 479, loss: 0.4316, instance_loss: 0.0132, weighted_loss: 0.3061, label: 1, bag_size: 9561\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 2244\n",
      "batch 519, loss: 0.0185, instance_loss: 0.0000, weighted_loss: 0.0129, label: 1, bag_size: 7217\n",
      "batch 539, loss: 0.0338, instance_loss: 0.0078, weighted_loss: 0.0260, label: 1, bag_size: 1512\n",
      "batch 559, loss: 0.0004, instance_loss: 0.0004, weighted_loss: 0.0004, label: 1, bag_size: 14604\n",
      "batch 579, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 11884\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 6606\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21682\n",
      "batch 639, loss: 0.6968, instance_loss: 0.0011, weighted_loss: 0.4881, label: 1, bag_size: 8592\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 12349\n",
      "batch 679, loss: 0.0105, instance_loss: 0.0001, weighted_loss: 0.0074, label: 1, bag_size: 21701\n",
      "batch 699, loss: 2.9278, instance_loss: 4.4163, weighted_loss: 3.3743, label: 1, bag_size: 2731\n",
      "batch 719, loss: 0.0094, instance_loss: 0.0122, weighted_loss: 0.0103, label: 1, bag_size: 8438\n",
      "batch 739, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 15008\n",
      "batch 759, loss: 0.1012, instance_loss: 0.3071, weighted_loss: 0.1630, label: 0, bag_size: 14333\n",
      "batch 779, loss: 0.0501, instance_loss: 0.0403, weighted_loss: 0.0472, label: 1, bag_size: 6842\n",
      "batch 799, loss: 0.0166, instance_loss: 0.0658, weighted_loss: 0.0313, label: 0, bag_size: 22498\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 31106\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9842987804878048: correct 12914/13120\n",
      "class 1 clustering acc 0.9342987804878049: correct 6129/6560\n",
      "Epoch: 72, train_loss: 0.1065, train_clustering_loss:  0.1395, train_error: 0.0366\n",
      "class 0: acc 0.9588688946015425, correct 373/389\n",
      "class 1: acc 0.9675174013921114, correct 417/431\n",
      "\n",
      "Val Set, val_loss: 0.1068, val_error: 0.0273, auc: 0.9937\n",
      "class 0 clustering acc 0.9539772727272727: correct 1679/1760\n",
      "class 1 clustering acc 0.875: correct 770/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "Validation loss decreased (0.108824 --> 0.106754).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14266\n",
      "batch 39, loss: 0.0027, instance_loss: 0.0012, weighted_loss: 0.0023, label: 0, bag_size: 2036\n",
      "batch 59, loss: 0.0028, instance_loss: 0.0179, weighted_loss: 0.0073, label: 0, bag_size: 23791\n",
      "batch 79, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 12865\n",
      "batch 99, loss: 0.1035, instance_loss: 0.0008, weighted_loss: 0.0727, label: 1, bag_size: 15689\n",
      "batch 119, loss: 0.0914, instance_loss: 0.1480, weighted_loss: 0.1084, label: 1, bag_size: 1284\n",
      "batch 139, loss: 0.0013, instance_loss: 0.0486, weighted_loss: 0.0155, label: 1, bag_size: 2814\n",
      "batch 159, loss: 0.0048, instance_loss: 0.0000, weighted_loss: 0.0034, label: 1, bag_size: 9877\n",
      "batch 179, loss: 0.0061, instance_loss: 0.0000, weighted_loss: 0.0043, label: 1, bag_size: 10396\n",
      "batch 199, loss: 0.0058, instance_loss: 0.3499, weighted_loss: 0.1090, label: 0, bag_size: 803\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 1, bag_size: 4442\n",
      "batch 239, loss: 0.0028, instance_loss: 0.0000, weighted_loss: 0.0020, label: 0, bag_size: 13205\n",
      "batch 259, loss: 0.0028, instance_loss: 0.0013, weighted_loss: 0.0023, label: 0, bag_size: 3657\n",
      "batch 279, loss: 1.0218, instance_loss: 0.0063, weighted_loss: 0.7172, label: 0, bag_size: 5120\n",
      "batch 299, loss: 0.0439, instance_loss: 0.0246, weighted_loss: 0.0381, label: 0, bag_size: 1498\n",
      "batch 319, loss: 0.0034, instance_loss: 0.0021, weighted_loss: 0.0030, label: 1, bag_size: 1759\n",
      "batch 339, loss: 0.0076, instance_loss: 0.0000, weighted_loss: 0.0053, label: 1, bag_size: 20161\n",
      "batch 359, loss: 0.0012, instance_loss: 0.0005, weighted_loss: 0.0010, label: 1, bag_size: 2308\n",
      "batch 379, loss: 0.0138, instance_loss: 0.0081, weighted_loss: 0.0121, label: 0, bag_size: 3670\n",
      "batch 399, loss: 0.3918, instance_loss: 0.0904, weighted_loss: 0.3014, label: 1, bag_size: 1819\n",
      "batch 419, loss: 0.0003, instance_loss: 0.8482, weighted_loss: 0.2547, label: 0, bag_size: 3101\n",
      "batch 439, loss: 0.0016, instance_loss: 0.0780, weighted_loss: 0.0245, label: 1, bag_size: 8191\n",
      "batch 459, loss: 0.0311, instance_loss: 0.0402, weighted_loss: 0.0339, label: 1, bag_size: 8754\n",
      "batch 479, loss: 0.0004, instance_loss: 0.0973, weighted_loss: 0.0295, label: 1, bag_size: 621\n",
      "batch 499, loss: 1.5183, instance_loss: 0.0189, weighted_loss: 1.0685, label: 0, bag_size: 5120\n",
      "batch 519, loss: 0.0010, instance_loss: 0.1795, weighted_loss: 0.0545, label: 0, bag_size: 21093\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 10481\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23368\n",
      "batch 579, loss: 0.2181, instance_loss: 0.0574, weighted_loss: 0.1699, label: 1, bag_size: 4956\n",
      "batch 599, loss: 0.0044, instance_loss: 0.0053, weighted_loss: 0.0047, label: 1, bag_size: 15464\n",
      "batch 619, loss: 0.0003, instance_loss: 0.0085, weighted_loss: 0.0027, label: 0, bag_size: 1824\n",
      "batch 639, loss: 0.0981, instance_loss: 0.1646, weighted_loss: 0.1180, label: 0, bag_size: 1370\n",
      "batch 659, loss: 0.0002, instance_loss: 0.0005, weighted_loss: 0.0003, label: 0, bag_size: 12793\n",
      "batch 679, loss: 0.0004, instance_loss: 0.0001, weighted_loss: 0.0003, label: 0, bag_size: 9485\n",
      "batch 699, loss: 0.0015, instance_loss: 0.0030, weighted_loss: 0.0020, label: 0, bag_size: 5551\n",
      "batch 719, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 18095\n",
      "batch 739, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 5494\n",
      "batch 759, loss: 0.0396, instance_loss: 0.0002, weighted_loss: 0.0278, label: 1, bag_size: 19972\n",
      "batch 779, loss: 0.0008, instance_loss: 0.0043, weighted_loss: 0.0019, label: 0, bag_size: 1213\n",
      "batch 799, loss: 0.0075, instance_loss: 0.0030, weighted_loss: 0.0062, label: 0, bag_size: 13339\n",
      "batch 819, loss: 0.0033, instance_loss: 0.0157, weighted_loss: 0.0070, label: 0, bag_size: 2063\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9880335365853659: correct 12963/13120\n",
      "class 1 clustering acc 0.9515243902439025: correct 6242/6560\n",
      "Epoch: 73, train_loss: 0.1267, train_clustering_loss:  0.1094, train_error: 0.0439\n",
      "class 0: acc 0.9482758620689655, correct 385/406\n",
      "class 1: acc 0.9637681159420289, correct 399/414\n",
      "\n",
      "Val Set, val_loss: 0.1118, val_error: 0.0273, auc: 0.9947\n",
      "class 0 clustering acc 0.946590909090909: correct 1666/1760\n",
      "class 1 clustering acc 0.8522727272727273: correct 750/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0050, instance_loss: 0.1662, weighted_loss: 0.0534, label: 0, bag_size: 5009\n",
      "batch 39, loss: 0.0009, instance_loss: 0.0056, weighted_loss: 0.0023, label: 0, bag_size: 6652\n",
      "batch 59, loss: 0.2150, instance_loss: 0.0078, weighted_loss: 0.1528, label: 1, bag_size: 11220\n",
      "batch 79, loss: 0.0004, instance_loss: 0.0153, weighted_loss: 0.0049, label: 0, bag_size: 14305\n",
      "batch 99, loss: 0.0016, instance_loss: 0.2318, weighted_loss: 0.0707, label: 0, bag_size: 11194\n",
      "batch 119, loss: 0.9181, instance_loss: 3.3710, weighted_loss: 1.6540, label: 1, bag_size: 1533\n",
      "batch 139, loss: 0.0030, instance_loss: 0.0155, weighted_loss: 0.0067, label: 0, bag_size: 15071\n",
      "batch 159, loss: 0.0740, instance_loss: 0.5323, weighted_loss: 0.2115, label: 1, bag_size: 1483\n",
      "batch 179, loss: 0.0103, instance_loss: 0.0004, weighted_loss: 0.0074, label: 1, bag_size: 25695\n",
      "batch 199, loss: 0.0262, instance_loss: 0.0021, weighted_loss: 0.0190, label: 1, bag_size: 10848\n",
      "batch 219, loss: 0.0011, instance_loss: 0.0017, weighted_loss: 0.0013, label: 0, bag_size: 21864\n",
      "batch 239, loss: 0.0005, instance_loss: 1.0132, weighted_loss: 0.3043, label: 1, bag_size: 1609\n",
      "batch 259, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 7078\n",
      "batch 279, loss: 0.0057, instance_loss: 0.5171, weighted_loss: 0.1592, label: 0, bag_size: 1797\n",
      "batch 299, loss: 0.0004, instance_loss: 0.0027, weighted_loss: 0.0011, label: 1, bag_size: 6752\n",
      "batch 319, loss: 0.6288, instance_loss: 0.0000, weighted_loss: 0.4402, label: 0, bag_size: 25420\n",
      "batch 339, loss: 0.0039, instance_loss: 0.0007, weighted_loss: 0.0030, label: 1, bag_size: 5629\n",
      "batch 359, loss: 0.0017, instance_loss: 0.0031, weighted_loss: 0.0021, label: 1, bag_size: 14681\n",
      "batch 379, loss: 0.0021, instance_loss: 0.0143, weighted_loss: 0.0058, label: 0, bag_size: 10304\n",
      "batch 399, loss: 0.0539, instance_loss: 0.0000, weighted_loss: 0.0377, label: 1, bag_size: 6927\n",
      "batch 419, loss: 0.0153, instance_loss: 0.0118, weighted_loss: 0.0142, label: 0, bag_size: 10751\n",
      "batch 439, loss: 0.0130, instance_loss: 0.0084, weighted_loss: 0.0116, label: 0, bag_size: 3893\n",
      "batch 459, loss: 0.0025, instance_loss: 0.0426, weighted_loss: 0.0145, label: 0, bag_size: 11259\n",
      "batch 479, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 10725\n",
      "batch 499, loss: 0.0331, instance_loss: 0.0262, weighted_loss: 0.0310, label: 1, bag_size: 1015\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 7767\n",
      "batch 539, loss: 0.0002, instance_loss: 0.0002, weighted_loss: 0.0002, label: 1, bag_size: 11387\n",
      "batch 559, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 4880\n",
      "batch 579, loss: 0.8789, instance_loss: 3.9563, weighted_loss: 1.8021, label: 0, bag_size: 11128\n",
      "batch 599, loss: 0.0011, instance_loss: 0.0111, weighted_loss: 0.0041, label: 0, bag_size: 3725\n",
      "batch 619, loss: 1.0237, instance_loss: 0.0194, weighted_loss: 0.7224, label: 1, bag_size: 11220\n",
      "batch 639, loss: 0.0201, instance_loss: 0.9091, weighted_loss: 0.2868, label: 0, bag_size: 4598\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0132, weighted_loss: 0.0040, label: 0, bag_size: 10481\n",
      "batch 679, loss: 0.0018, instance_loss: 0.0062, weighted_loss: 0.0031, label: 0, bag_size: 2548\n",
      "batch 699, loss: 0.0018, instance_loss: 0.0043, weighted_loss: 0.0026, label: 1, bag_size: 5494\n",
      "batch 719, loss: 0.0950, instance_loss: 0.2874, weighted_loss: 0.1527, label: 0, bag_size: 6356\n",
      "batch 739, loss: 0.0009, instance_loss: 0.0502, weighted_loss: 0.0157, label: 1, bag_size: 5894\n",
      "batch 759, loss: 0.0037, instance_loss: 0.1667, weighted_loss: 0.0526, label: 0, bag_size: 9866\n",
      "batch 779, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 7513\n",
      "batch 799, loss: 0.0285, instance_loss: 0.0000, weighted_loss: 0.0199, label: 1, bag_size: 25695\n",
      "batch 819, loss: 0.0832, instance_loss: 0.0026, weighted_loss: 0.0590, label: 1, bag_size: 8754\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9782774390243902: correct 12835/13120\n",
      "class 1 clustering acc 0.9140243902439025: correct 5996/6560\n",
      "Epoch: 74, train_loss: 0.1132, train_clustering_loss:  0.2098, train_error: 0.0500\n",
      "class 0: acc 0.9498861047835991, correct 417/439\n",
      "class 1: acc 0.9501312335958005, correct 362/381\n",
      "\n",
      "Val Set, val_loss: 0.1312, val_error: 0.0636, auc: 0.9934\n",
      "class 0 clustering acc 0.9357954545454545: correct 1647/1760\n",
      "class 1 clustering acc 0.825: correct 726/880\n",
      "class 0: acc 0.9807692307692307, correct 51/52\n",
      "class 1: acc 0.896551724137931, correct 52/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0109, weighted_loss: 0.0033, label: 0, bag_size: 8145\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 18649\n",
      "batch 59, loss: 0.0012, instance_loss: 0.0477, weighted_loss: 0.0151, label: 0, bag_size: 9252\n",
      "batch 79, loss: 0.0001, instance_loss: 0.0178, weighted_loss: 0.0054, label: 1, bag_size: 3651\n",
      "batch 99, loss: 0.0105, instance_loss: 0.0332, weighted_loss: 0.0173, label: 0, bag_size: 1508\n",
      "batch 119, loss: 0.0017, instance_loss: 0.0000, weighted_loss: 0.0012, label: 1, bag_size: 17769\n",
      "batch 139, loss: 0.0196, instance_loss: 0.0000, weighted_loss: 0.0137, label: 1, bag_size: 8466\n",
      "batch 159, loss: 0.0037, instance_loss: 0.0271, weighted_loss: 0.0107, label: 0, bag_size: 2266\n",
      "batch 179, loss: 0.0093, instance_loss: 0.0000, weighted_loss: 0.0065, label: 0, bag_size: 21138\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 6851\n",
      "batch 219, loss: 0.0026, instance_loss: 0.0279, weighted_loss: 0.0102, label: 1, bag_size: 6478\n",
      "batch 239, loss: 0.0005, instance_loss: 0.0894, weighted_loss: 0.0271, label: 0, bag_size: 23796\n",
      "batch 259, loss: 0.0023, instance_loss: 0.3917, weighted_loss: 0.1192, label: 1, bag_size: 4308\n",
      "batch 279, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0015, label: 1, bag_size: 14230\n",
      "batch 299, loss: 0.0036, instance_loss: 0.0000, weighted_loss: 0.0025, label: 1, bag_size: 12758\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 5225\n",
      "batch 339, loss: 0.0068, instance_loss: 0.0663, weighted_loss: 0.0247, label: 0, bag_size: 1772\n",
      "batch 359, loss: 1.1172, instance_loss: 0.2972, weighted_loss: 0.8712, label: 1, bag_size: 9215\n",
      "batch 379, loss: 0.0631, instance_loss: 0.0010, weighted_loss: 0.0445, label: 1, bag_size: 2140\n",
      "batch 399, loss: 0.0955, instance_loss: 0.3787, weighted_loss: 0.1805, label: 1, bag_size: 2681\n",
      "batch 419, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 3651\n",
      "batch 439, loss: 0.0213, instance_loss: 0.1411, weighted_loss: 0.0572, label: 0, bag_size: 1684\n",
      "batch 459, loss: 0.0003, instance_loss: 0.0402, weighted_loss: 0.0122, label: 1, bag_size: 22264\n",
      "batch 479, loss: 0.0024, instance_loss: 0.0000, weighted_loss: 0.0016, label: 1, bag_size: 10492\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 12178\n",
      "batch 519, loss: 0.0003, instance_loss: 0.0057, weighted_loss: 0.0019, label: 1, bag_size: 14604\n",
      "batch 539, loss: 2.8449, instance_loss: 3.1629, weighted_loss: 2.9403, label: 0, bag_size: 7428\n",
      "batch 559, loss: 0.0759, instance_loss: 0.0000, weighted_loss: 0.0531, label: 1, bag_size: 21252\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0124, weighted_loss: 0.0038, label: 0, bag_size: 20796\n",
      "batch 599, loss: 0.0056, instance_loss: 0.0080, weighted_loss: 0.0063, label: 0, bag_size: 12687\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0470, weighted_loss: 0.0141, label: 0, bag_size: 1984\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 12212\n",
      "batch 659, loss: 0.2784, instance_loss: 0.0115, weighted_loss: 0.1983, label: 0, bag_size: 2918\n",
      "batch 679, loss: 0.0037, instance_loss: 0.0186, weighted_loss: 0.0082, label: 0, bag_size: 1745\n",
      "batch 699, loss: 0.1316, instance_loss: 2.7252, weighted_loss: 0.9097, label: 1, bag_size: 1444\n",
      "batch 719, loss: 0.0033, instance_loss: 0.0945, weighted_loss: 0.0306, label: 0, bag_size: 18215\n",
      "batch 739, loss: 0.0220, instance_loss: 0.0092, weighted_loss: 0.0182, label: 0, bag_size: 3444\n",
      "batch 759, loss: 0.0005, instance_loss: 0.0008, weighted_loss: 0.0006, label: 1, bag_size: 645\n",
      "batch 779, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8948\n",
      "batch 799, loss: 0.2372, instance_loss: 0.0000, weighted_loss: 0.1660, label: 1, bag_size: 3652\n",
      "batch 819, loss: 0.0159, instance_loss: 0.0000, weighted_loss: 0.0112, label: 1, bag_size: 1838\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9862804878048781: correct 12940/13120\n",
      "class 1 clustering acc 0.9440548780487805: correct 6193/6560\n",
      "Epoch: 75, train_loss: 0.1070, train_clustering_loss:  0.1198, train_error: 0.0451\n",
      "class 0: acc 0.9462915601023018, correct 370/391\n",
      "class 1: acc 0.9627039627039627, correct 413/429\n",
      "\n",
      "Val Set, val_loss: 0.1187, val_error: 0.0273, auc: 0.9937\n",
      "class 0 clustering acc 0.9505681818181818: correct 1673/1760\n",
      "class 1 clustering acc 0.8590909090909091: correct 756/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0690, instance_loss: 0.0308, weighted_loss: 0.0575, label: 1, bag_size: 2681\n",
      "batch 39, loss: 0.0002, instance_loss: 0.0013, weighted_loss: 0.0005, label: 1, bag_size: 1437\n",
      "batch 59, loss: 0.0034, instance_loss: 0.0000, weighted_loss: 0.0024, label: 1, bag_size: 645\n",
      "batch 79, loss: 0.0009, instance_loss: 0.4756, weighted_loss: 0.1433, label: 0, bag_size: 2820\n",
      "batch 99, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 16051\n",
      "batch 119, loss: 0.0005, instance_loss: 0.0145, weighted_loss: 0.0047, label: 0, bag_size: 12732\n",
      "batch 139, loss: 0.0042, instance_loss: 0.0053, weighted_loss: 0.0045, label: 0, bag_size: 1438\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 0, bag_size: 9470\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0239, weighted_loss: 0.0072, label: 1, bag_size: 621\n",
      "batch 199, loss: 0.0004, instance_loss: 0.1993, weighted_loss: 0.0601, label: 0, bag_size: 11199\n",
      "batch 219, loss: 0.0016, instance_loss: 0.0817, weighted_loss: 0.0256, label: 0, bag_size: 8981\n",
      "batch 239, loss: 0.4802, instance_loss: 0.0173, weighted_loss: 0.3413, label: 0, bag_size: 2732\n",
      "batch 259, loss: 0.0064, instance_loss: 0.0804, weighted_loss: 0.0286, label: 0, bag_size: 16211\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0015, weighted_loss: 0.0005, label: 0, bag_size: 13795\n",
      "batch 299, loss: 0.0083, instance_loss: 0.0928, weighted_loss: 0.0337, label: 0, bag_size: 5639\n",
      "batch 319, loss: 0.0006, instance_loss: 0.0986, weighted_loss: 0.0300, label: 1, bag_size: 5256\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0023, weighted_loss: 0.0007, label: 0, bag_size: 2748\n",
      "batch 359, loss: 0.0005, instance_loss: 1.3989, weighted_loss: 0.4200, label: 1, bag_size: 3856\n",
      "batch 379, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 5225\n",
      "batch 399, loss: 0.0001, instance_loss: 0.0043, weighted_loss: 0.0014, label: 1, bag_size: 1255\n",
      "batch 419, loss: 0.0031, instance_loss: 0.0000, weighted_loss: 0.0022, label: 0, bag_size: 21864\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 15464\n",
      "batch 459, loss: 0.0017, instance_loss: 0.0058, weighted_loss: 0.0029, label: 0, bag_size: 6281\n",
      "batch 479, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 9571\n",
      "batch 499, loss: 0.0006, instance_loss: 0.0174, weighted_loss: 0.0056, label: 0, bag_size: 9786\n",
      "batch 519, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 12593\n",
      "batch 539, loss: 1.1885, instance_loss: 0.8484, weighted_loss: 1.0865, label: 0, bag_size: 1714\n",
      "batch 559, loss: 0.1565, instance_loss: 0.7268, weighted_loss: 0.3276, label: 1, bag_size: 2681\n",
      "batch 579, loss: 0.0272, instance_loss: 0.2643, weighted_loss: 0.0984, label: 0, bag_size: 7823\n",
      "batch 599, loss: 0.0021, instance_loss: 0.0047, weighted_loss: 0.0029, label: 1, bag_size: 1823\n",
      "batch 619, loss: 2.2721, instance_loss: 0.0368, weighted_loss: 1.6015, label: 0, bag_size: 7428\n",
      "batch 639, loss: 1.0614, instance_loss: 0.1944, weighted_loss: 0.8013, label: 1, bag_size: 6360\n",
      "batch 659, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 3651\n",
      "batch 679, loss: 0.0343, instance_loss: 0.0156, weighted_loss: 0.0287, label: 1, bag_size: 7468\n",
      "batch 699, loss: 0.0186, instance_loss: 0.0665, weighted_loss: 0.0330, label: 0, bag_size: 1142\n",
      "batch 719, loss: 0.0011, instance_loss: 0.0040, weighted_loss: 0.0020, label: 0, bag_size: 3265\n",
      "batch 739, loss: 0.0365, instance_loss: 0.0000, weighted_loss: 0.0255, label: 1, bag_size: 2140\n",
      "batch 759, loss: 0.0052, instance_loss: 0.0072, weighted_loss: 0.0058, label: 0, bag_size: 19518\n",
      "batch 779, loss: 0.0339, instance_loss: 0.3189, weighted_loss: 0.1194, label: 0, bag_size: 2270\n",
      "batch 799, loss: 0.2353, instance_loss: 0.0009, weighted_loss: 0.1650, label: 0, bag_size: 9597\n",
      "batch 819, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 19808\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9864329268292683: correct 12942/13120\n",
      "class 1 clustering acc 0.9420731707317073: correct 6180/6560\n",
      "Epoch: 76, train_loss: 0.0920, train_clustering_loss:  0.1174, train_error: 0.0366\n",
      "class 0: acc 0.960880195599022, correct 393/409\n",
      "class 1: acc 0.9659367396593674, correct 397/411\n",
      "\n",
      "Val Set, val_loss: 0.1129, val_error: 0.0364, auc: 0.9927\n",
      "class 0 clustering acc 0.9443181818181818: correct 1662/1760\n",
      "class 1 clustering acc 0.8545454545454545: correct 752/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0616, instance_loss: 0.0359, weighted_loss: 0.0539, label: 0, bag_size: 3908\n",
      "batch 39, loss: 2.4895, instance_loss: 4.3205, weighted_loss: 3.0388, label: 1, bag_size: 2731\n",
      "batch 59, loss: 0.0029, instance_loss: 0.0000, weighted_loss: 0.0020, label: 1, bag_size: 25970\n",
      "batch 79, loss: 0.0038, instance_loss: 0.0355, weighted_loss: 0.0133, label: 0, bag_size: 2624\n",
      "batch 99, loss: 0.0015, instance_loss: 0.0648, weighted_loss: 0.0205, label: 1, bag_size: 2344\n",
      "batch 119, loss: 0.0021, instance_loss: 0.0039, weighted_loss: 0.0027, label: 1, bag_size: 14230\n",
      "batch 139, loss: 0.0008, instance_loss: 0.0144, weighted_loss: 0.0049, label: 0, bag_size: 19470\n",
      "batch 159, loss: 0.0002, instance_loss: 0.0317, weighted_loss: 0.0097, label: 0, bag_size: 23796\n",
      "batch 179, loss: 0.3915, instance_loss: 0.5253, weighted_loss: 0.4316, label: 0, bag_size: 8744\n",
      "batch 199, loss: 0.5113, instance_loss: 0.0035, weighted_loss: 0.3589, label: 1, bag_size: 9215\n",
      "batch 219, loss: 0.0164, instance_loss: 0.3667, weighted_loss: 0.1215, label: 0, bag_size: 2814\n",
      "batch 239, loss: 0.0043, instance_loss: 0.0000, weighted_loss: 0.0030, label: 1, bag_size: 12460\n",
      "batch 259, loss: 0.0266, instance_loss: 0.0932, weighted_loss: 0.0466, label: 0, bag_size: 1052\n",
      "batch 279, loss: 0.0048, instance_loss: 0.0000, weighted_loss: 0.0034, label: 1, bag_size: 5340\n",
      "batch 299, loss: 0.2838, instance_loss: 0.0851, weighted_loss: 0.2242, label: 0, bag_size: 13332\n",
      "batch 319, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9065\n",
      "batch 339, loss: 0.0012, instance_loss: 0.0108, weighted_loss: 0.0041, label: 1, bag_size: 4308\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0137, weighted_loss: 0.0041, label: 1, bag_size: 1412\n",
      "batch 379, loss: 0.0013, instance_loss: 0.0654, weighted_loss: 0.0205, label: 0, bag_size: 1881\n",
      "batch 399, loss: 0.0037, instance_loss: 0.0132, weighted_loss: 0.0065, label: 1, bag_size: 2356\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0004, weighted_loss: 0.0002, label: 0, bag_size: 3228\n",
      "batch 439, loss: 0.0041, instance_loss: 0.0000, weighted_loss: 0.0029, label: 1, bag_size: 9955\n",
      "batch 459, loss: 0.0083, instance_loss: 0.0012, weighted_loss: 0.0062, label: 1, bag_size: 7981\n",
      "batch 479, loss: 0.1683, instance_loss: 0.0079, weighted_loss: 0.1202, label: 0, bag_size: 3089\n",
      "batch 499, loss: 0.3834, instance_loss: 0.0303, weighted_loss: 0.2775, label: 0, bag_size: 24382\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11389\n",
      "batch 539, loss: 0.0006, instance_loss: 0.0016, weighted_loss: 0.0009, label: 0, bag_size: 11146\n",
      "batch 559, loss: 0.0730, instance_loss: 0.0880, weighted_loss: 0.0775, label: 0, bag_size: 8744\n",
      "batch 579, loss: 0.0037, instance_loss: 0.0953, weighted_loss: 0.0312, label: 1, bag_size: 1064\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9644\n",
      "batch 619, loss: 0.0882, instance_loss: 0.0103, weighted_loss: 0.0648, label: 1, bag_size: 15125\n",
      "batch 639, loss: 0.0058, instance_loss: 0.4054, weighted_loss: 0.1256, label: 1, bag_size: 1294\n",
      "batch 659, loss: 0.0002, instance_loss: 0.0676, weighted_loss: 0.0204, label: 0, bag_size: 14956\n",
      "batch 679, loss: 0.0006, instance_loss: 0.0077, weighted_loss: 0.0027, label: 1, bag_size: 7246\n",
      "batch 699, loss: 0.0506, instance_loss: 0.2622, weighted_loss: 0.1141, label: 1, bag_size: 2395\n",
      "batch 719, loss: 0.0004, instance_loss: 0.0115, weighted_loss: 0.0038, label: 0, bag_size: 19518\n",
      "batch 739, loss: 2.2040, instance_loss: 0.0640, weighted_loss: 1.5620, label: 1, bag_size: 12494\n",
      "batch 759, loss: 0.1883, instance_loss: 0.0273, weighted_loss: 0.1400, label: 0, bag_size: 14264\n",
      "batch 779, loss: 0.0061, instance_loss: 0.0112, weighted_loss: 0.0076, label: 0, bag_size: 2351\n",
      "batch 799, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 11518\n",
      "batch 819, loss: 0.1974, instance_loss: 0.0553, weighted_loss: 0.1548, label: 0, bag_size: 3783\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9842225609756098: correct 12913/13120\n",
      "class 1 clustering acc 0.9320121951219512: correct 6114/6560\n",
      "Epoch: 77, train_loss: 0.1026, train_clustering_loss:  0.1471, train_error: 0.0390\n",
      "class 0: acc 0.9676616915422885, correct 389/402\n",
      "class 1: acc 0.9545454545454546, correct 399/418\n",
      "\n",
      "Val Set, val_loss: 0.1675, val_error: 0.0636, auc: 0.9934\n",
      "class 0 clustering acc 0.9448863636363637: correct 1663/1760\n",
      "class 1 clustering acc 0.8306818181818182: correct 731/880\n",
      "class 0: acc 0.8653846153846154, correct 45/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0094, instance_loss: 0.0109, weighted_loss: 0.0099, label: 0, bag_size: 3198\n",
      "batch 39, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 4880\n",
      "batch 59, loss: 0.0038, instance_loss: 0.0096, weighted_loss: 0.0055, label: 1, bag_size: 1759\n",
      "batch 79, loss: 0.0118, instance_loss: 0.1293, weighted_loss: 0.0470, label: 0, bag_size: 17268\n",
      "batch 99, loss: 0.0135, instance_loss: 0.0000, weighted_loss: 0.0095, label: 1, bag_size: 7389\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0014, weighted_loss: 0.0004, label: 0, bag_size: 18225\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0161, weighted_loss: 0.0049, label: 0, bag_size: 9485\n",
      "batch 159, loss: 0.0215, instance_loss: 0.0000, weighted_loss: 0.0151, label: 1, bag_size: 10492\n",
      "batch 179, loss: 0.0035, instance_loss: 0.0000, weighted_loss: 0.0024, label: 1, bag_size: 11266\n",
      "batch 199, loss: 0.0007, instance_loss: 0.0061, weighted_loss: 0.0023, label: 0, bag_size: 8755\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 11778\n",
      "batch 239, loss: 0.0020, instance_loss: 0.0043, weighted_loss: 0.0027, label: 0, bag_size: 1438\n",
      "batch 259, loss: 0.0505, instance_loss: 0.0000, weighted_loss: 0.0354, label: 1, bag_size: 12575\n",
      "batch 279, loss: 0.0546, instance_loss: 0.0238, weighted_loss: 0.0453, label: 0, bag_size: 2624\n",
      "batch 299, loss: 0.2305, instance_loss: 0.0000, weighted_loss: 0.1613, label: 1, bag_size: 10432\n",
      "batch 319, loss: 0.0186, instance_loss: 0.0081, weighted_loss: 0.0155, label: 0, bag_size: 1452\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0065, weighted_loss: 0.0020, label: 0, bag_size: 9433\n",
      "batch 359, loss: 0.0000, instance_loss: 0.0754, weighted_loss: 0.0226, label: 1, bag_size: 3295\n",
      "batch 379, loss: 0.0006, instance_loss: 0.0004, weighted_loss: 0.0005, label: 0, bag_size: 19043\n",
      "batch 399, loss: 0.7479, instance_loss: 1.3982, weighted_loss: 0.9430, label: 0, bag_size: 9132\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0024, weighted_loss: 0.0007, label: 0, bag_size: 9433\n",
      "batch 439, loss: 0.0088, instance_loss: 0.0014, weighted_loss: 0.0066, label: 0, bag_size: 4845\n",
      "batch 459, loss: 0.0105, instance_loss: 0.0070, weighted_loss: 0.0094, label: 0, bag_size: 1684\n",
      "batch 479, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 9542\n",
      "batch 499, loss: 0.0350, instance_loss: 0.0395, weighted_loss: 0.0364, label: 1, bag_size: 1867\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9571\n",
      "batch 539, loss: 0.0626, instance_loss: 0.0000, weighted_loss: 0.0438, label: 1, bag_size: 12626\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 15665\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 15716\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0009, weighted_loss: 0.0004, label: 0, bag_size: 2652\n",
      "batch 619, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 14681\n",
      "batch 639, loss: 0.0030, instance_loss: 0.0013, weighted_loss: 0.0025, label: 0, bag_size: 5009\n",
      "batch 659, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 15665\n",
      "batch 679, loss: 0.2059, instance_loss: 0.0430, weighted_loss: 0.1570, label: 1, bag_size: 2935\n",
      "batch 699, loss: 0.4123, instance_loss: 0.0013, weighted_loss: 0.2890, label: 1, bag_size: 1819\n",
      "batch 719, loss: 0.7355, instance_loss: 0.0090, weighted_loss: 0.5175, label: 0, bag_size: 23618\n",
      "batch 739, loss: 0.5003, instance_loss: 0.0156, weighted_loss: 0.3549, label: 0, bag_size: 7428\n",
      "batch 759, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 20537\n",
      "batch 779, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 6343\n",
      "batch 799, loss: 0.0497, instance_loss: 0.0002, weighted_loss: 0.0349, label: 1, bag_size: 8438\n",
      "batch 819, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 23398\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9885670731707317: correct 12970/13120\n",
      "class 1 clustering acc 0.9509146341463415: correct 6238/6560\n",
      "Epoch: 78, train_loss: 0.1195, train_clustering_loss:  0.1030, train_error: 0.0451\n",
      "class 0: acc 0.9437939110070258, correct 403/427\n",
      "class 1: acc 0.9669211195928753, correct 380/393\n",
      "\n",
      "Val Set, val_loss: 0.3525, val_error: 0.1545, auc: 0.9937\n",
      "class 0 clustering acc 0.9272727272727272: correct 1632/1760\n",
      "class 1 clustering acc 0.8465909090909091: correct 745/880\n",
      "class 0: acc 0.6730769230769231, correct 35/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 12217\n",
      "batch 39, loss: 0.0002, instance_loss: 0.0084, weighted_loss: 0.0027, label: 0, bag_size: 23796\n",
      "batch 59, loss: 0.0010, instance_loss: 0.0037, weighted_loss: 0.0018, label: 1, bag_size: 9446\n",
      "batch 79, loss: 0.0003, instance_loss: 0.0015, weighted_loss: 0.0007, label: 0, bag_size: 11546\n",
      "batch 99, loss: 0.0004, instance_loss: 1.3852, weighted_loss: 0.4158, label: 1, bag_size: 3450\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0769, weighted_loss: 0.0231, label: 0, bag_size: 14828\n",
      "batch 139, loss: 0.0148, instance_loss: 0.0000, weighted_loss: 0.0103, label: 0, bag_size: 22498\n",
      "batch 159, loss: 0.0007, instance_loss: 0.0005, weighted_loss: 0.0006, label: 0, bag_size: 19470\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 16417\n",
      "batch 199, loss: 0.0067, instance_loss: 0.0696, weighted_loss: 0.0256, label: 1, bag_size: 1924\n",
      "batch 219, loss: 0.0233, instance_loss: 0.0092, weighted_loss: 0.0190, label: 1, bag_size: 8660\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0031, weighted_loss: 0.0010, label: 0, bag_size: 16992\n",
      "batch 259, loss: 0.0083, instance_loss: 0.0000, weighted_loss: 0.0058, label: 1, bag_size: 12895\n",
      "batch 279, loss: 0.0064, instance_loss: 0.0000, weighted_loss: 0.0045, label: 1, bag_size: 11032\n",
      "batch 299, loss: 0.1924, instance_loss: 0.0533, weighted_loss: 0.1506, label: 1, bag_size: 2522\n",
      "batch 319, loss: 0.0921, instance_loss: 0.4023, weighted_loss: 0.1852, label: 1, bag_size: 1867\n",
      "batch 339, loss: 0.0249, instance_loss: 0.0143, weighted_loss: 0.0217, label: 0, bag_size: 3160\n",
      "batch 359, loss: 0.1923, instance_loss: 0.0017, weighted_loss: 0.1351, label: 1, bag_size: 12719\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 3787\n",
      "batch 399, loss: 0.0003, instance_loss: 0.0203, weighted_loss: 0.0063, label: 0, bag_size: 2036\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21082\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 16782\n",
      "batch 459, loss: 0.0156, instance_loss: 0.0015, weighted_loss: 0.0114, label: 0, bag_size: 2624\n",
      "batch 479, loss: 1.0239, instance_loss: 0.8255, weighted_loss: 0.9643, label: 0, bag_size: 11306\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21076\n",
      "batch 519, loss: 0.0057, instance_loss: 0.0102, weighted_loss: 0.0071, label: 0, bag_size: 1416\n",
      "batch 539, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 22286\n",
      "batch 559, loss: 0.0002, instance_loss: 0.1561, weighted_loss: 0.0470, label: 1, bag_size: 3856\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 8216\n",
      "batch 599, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0008, label: 0, bag_size: 15747\n",
      "batch 619, loss: 0.1150, instance_loss: 4.7449, weighted_loss: 1.5040, label: 0, bag_size: 17279\n",
      "batch 639, loss: 0.0000, instance_loss: 0.0190, weighted_loss: 0.0057, label: 0, bag_size: 9930\n",
      "batch 659, loss: 0.0086, instance_loss: 0.0000, weighted_loss: 0.0061, label: 1, bag_size: 13732\n",
      "batch 679, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0009, label: 0, bag_size: 18415\n",
      "batch 699, loss: 0.0060, instance_loss: 0.0140, weighted_loss: 0.0084, label: 0, bag_size: 11900\n",
      "batch 719, loss: 0.0225, instance_loss: 0.2578, weighted_loss: 0.0931, label: 1, bag_size: 8191\n",
      "batch 739, loss: 0.0901, instance_loss: 0.0104, weighted_loss: 0.0662, label: 1, bag_size: 4786\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0049, weighted_loss: 0.0015, label: 0, bag_size: 31106\n",
      "batch 779, loss: 0.0000, instance_loss: 0.0150, weighted_loss: 0.0045, label: 0, bag_size: 7011\n",
      "batch 799, loss: 0.0012, instance_loss: 0.0295, weighted_loss: 0.0097, label: 0, bag_size: 13205\n",
      "batch 819, loss: 0.0006, instance_loss: 0.0138, weighted_loss: 0.0045, label: 0, bag_size: 22681\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9891768292682926: correct 12978/13120\n",
      "class 1 clustering acc 0.9550304878048781: correct 6265/6560\n",
      "Epoch: 79, train_loss: 0.0971, train_clustering_loss:  0.1188, train_error: 0.0354\n",
      "class 0: acc 0.9584295612009238, correct 415/433\n",
      "class 1: acc 0.9715762273901809, correct 376/387\n",
      "\n",
      "Val Set, val_loss: 0.1550, val_error: 0.0545, auc: 0.9897\n",
      "class 0 clustering acc 0.9511363636363637: correct 1674/1760\n",
      "class 1 clustering acc 0.8602272727272727: correct 757/880\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 7 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 2695\n",
      "batch 39, loss: 0.0018, instance_loss: 0.0177, weighted_loss: 0.0066, label: 0, bag_size: 1639\n",
      "batch 59, loss: 0.0126, instance_loss: 0.0065, weighted_loss: 0.0107, label: 1, bag_size: 16514\n",
      "batch 79, loss: 1.0324, instance_loss: 0.0000, weighted_loss: 0.7227, label: 1, bag_size: 11256\n",
      "batch 99, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 22800\n",
      "batch 119, loss: 0.0009, instance_loss: 0.2492, weighted_loss: 0.0754, label: 1, bag_size: 3651\n",
      "batch 139, loss: 0.0552, instance_loss: 0.0386, weighted_loss: 0.0502, label: 1, bag_size: 3674\n",
      "batch 159, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 9971\n",
      "batch 179, loss: 0.8634, instance_loss: 0.5701, weighted_loss: 0.7754, label: 0, bag_size: 11128\n",
      "batch 199, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 14202\n",
      "batch 219, loss: 0.0493, instance_loss: 0.1459, weighted_loss: 0.0783, label: 1, bag_size: 2681\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19390\n",
      "batch 259, loss: 0.0290, instance_loss: 0.0005, weighted_loss: 0.0204, label: 0, bag_size: 2918\n",
      "batch 279, loss: 0.0048, instance_loss: 0.0000, weighted_loss: 0.0034, label: 1, bag_size: 2356\n",
      "batch 299, loss: 0.3376, instance_loss: 0.0019, weighted_loss: 0.2369, label: 0, bag_size: 12910\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0092, weighted_loss: 0.0028, label: 1, bag_size: 13365\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0026, weighted_loss: 0.0008, label: 0, bag_size: 10898\n",
      "batch 359, loss: 0.0144, instance_loss: 0.2339, weighted_loss: 0.0803, label: 1, bag_size: 6842\n",
      "batch 379, loss: 0.0007, instance_loss: 0.0060, weighted_loss: 0.0023, label: 0, bag_size: 4902\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 31780\n",
      "batch 419, loss: 0.0079, instance_loss: 0.0035, weighted_loss: 0.0066, label: 1, bag_size: 2385\n",
      "batch 439, loss: 0.0007, instance_loss: 0.0075, weighted_loss: 0.0027, label: 0, bag_size: 7637\n",
      "batch 459, loss: 1.7314, instance_loss: 0.1263, weighted_loss: 1.2499, label: 1, bag_size: 21450\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11389\n",
      "batch 499, loss: 0.0010, instance_loss: 0.0004, weighted_loss: 0.0008, label: 0, bag_size: 2296\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0338, weighted_loss: 0.0102, label: 0, bag_size: 11778\n",
      "batch 539, loss: 0.0021, instance_loss: 0.0390, weighted_loss: 0.0132, label: 1, bag_size: 9689\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 7935\n",
      "batch 579, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 11875\n",
      "batch 599, loss: 0.2256, instance_loss: 0.2836, weighted_loss: 0.2430, label: 1, bag_size: 2681\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 31780\n",
      "batch 639, loss: 0.0024, instance_loss: 0.0009, weighted_loss: 0.0019, label: 0, bag_size: 24911\n",
      "batch 659, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 3437\n",
      "batch 679, loss: 0.0002, instance_loss: 0.0007, weighted_loss: 0.0003, label: 0, bag_size: 2063\n",
      "batch 699, loss: 0.0059, instance_loss: 0.0000, weighted_loss: 0.0042, label: 1, bag_size: 5340\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 7191\n",
      "batch 739, loss: 0.0063, instance_loss: 0.0000, weighted_loss: 0.0044, label: 1, bag_size: 10492\n",
      "batch 759, loss: 0.0400, instance_loss: 0.1242, weighted_loss: 0.0653, label: 0, bag_size: 1690\n",
      "batch 779, loss: 0.0051, instance_loss: 0.0004, weighted_loss: 0.0037, label: 0, bag_size: 18215\n",
      "batch 799, loss: 0.0025, instance_loss: 0.0208, weighted_loss: 0.0080, label: 0, bag_size: 9596\n",
      "batch 819, loss: 0.4797, instance_loss: 0.0268, weighted_loss: 0.3438, label: 0, bag_size: 2098\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9881859756097561: correct 12965/13120\n",
      "class 1 clustering acc 0.9541158536585366: correct 6259/6560\n",
      "Epoch: 80, train_loss: 0.1053, train_clustering_loss:  0.1081, train_error: 0.0354\n",
      "class 0: acc 0.9652777777777778, correct 417/432\n",
      "class 1: acc 0.9639175257731959, correct 374/388\n",
      "\n",
      "Val Set, val_loss: 0.3121, val_error: 0.1182, auc: 0.9924\n",
      "class 0 clustering acc 0.9551136363636363: correct 1681/1760\n",
      "class 1 clustering acc 0.9011363636363636: correct 793/880\n",
      "class 0: acc 0.75, correct 39/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 8 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0122, instance_loss: 0.0000, weighted_loss: 0.0085, label: 1, bag_size: 5137\n",
      "batch 39, loss: 0.0163, instance_loss: 0.0022, weighted_loss: 0.0121, label: 1, bag_size: 7424\n",
      "batch 59, loss: 0.0003, instance_loss: 0.0007, weighted_loss: 0.0004, label: 0, bag_size: 3541\n",
      "batch 79, loss: 0.0429, instance_loss: 0.0736, weighted_loss: 0.0521, label: 1, bag_size: 2681\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 6164\n",
      "batch 119, loss: 0.0068, instance_loss: 0.0015, weighted_loss: 0.0052, label: 1, bag_size: 8003\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 15665\n",
      "batch 159, loss: 0.0091, instance_loss: 0.0000, weighted_loss: 0.0064, label: 1, bag_size: 30675\n",
      "batch 179, loss: 0.5450, instance_loss: 0.0378, weighted_loss: 0.3929, label: 0, bag_size: 14264\n",
      "batch 199, loss: 0.2555, instance_loss: 0.0009, weighted_loss: 0.1791, label: 1, bag_size: 5903\n",
      "batch 219, loss: 0.0000, instance_loss: 0.0013, weighted_loss: 0.0004, label: 0, bag_size: 9851\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 16992\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 12217\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 11266\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0044, weighted_loss: 0.0013, label: 0, bag_size: 13225\n",
      "batch 319, loss: 4.1285, instance_loss: 0.0263, weighted_loss: 2.8978, label: 0, bag_size: 2815\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 1639\n",
      "batch 359, loss: 0.0037, instance_loss: 0.0012, weighted_loss: 0.0029, label: 0, bag_size: 2351\n",
      "batch 379, loss: 0.0003, instance_loss: 0.0086, weighted_loss: 0.0028, label: 0, bag_size: 4523\n",
      "batch 399, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 5612\n",
      "batch 419, loss: 0.0012, instance_loss: 0.0059, weighted_loss: 0.0026, label: 0, bag_size: 1831\n",
      "batch 439, loss: 1.2861, instance_loss: 0.0046, weighted_loss: 0.9017, label: 1, bag_size: 3652\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0008, weighted_loss: 0.0002, label: 1, bag_size: 629\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 11546\n",
      "batch 499, loss: 0.0509, instance_loss: 0.0093, weighted_loss: 0.0384, label: 0, bag_size: 7835\n",
      "batch 519, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 6950\n",
      "batch 539, loss: 0.0518, instance_loss: 0.0005, weighted_loss: 0.0364, label: 1, bag_size: 8438\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19390\n",
      "batch 579, loss: 0.0046, instance_loss: 0.0401, weighted_loss: 0.0152, label: 0, bag_size: 1142\n",
      "batch 599, loss: 0.0359, instance_loss: 0.0016, weighted_loss: 0.0256, label: 0, bag_size: 10113\n",
      "batch 619, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 5317\n",
      "batch 639, loss: 0.1843, instance_loss: 0.0190, weighted_loss: 0.1347, label: 0, bag_size: 11151\n",
      "batch 659, loss: 0.0119, instance_loss: 0.0044, weighted_loss: 0.0097, label: 1, bag_size: 3683\n",
      "batch 679, loss: 0.0239, instance_loss: 0.0022, weighted_loss: 0.0174, label: 0, bag_size: 22498\n",
      "batch 699, loss: 0.0093, instance_loss: 0.0000, weighted_loss: 0.0065, label: 1, bag_size: 8466\n",
      "batch 719, loss: 0.0120, instance_loss: 0.1124, weighted_loss: 0.0421, label: 1, bag_size: 1242\n",
      "batch 739, loss: 0.0006, instance_loss: 0.0052, weighted_loss: 0.0020, label: 1, bag_size: 1437\n",
      "batch 759, loss: 0.0206, instance_loss: 0.0061, weighted_loss: 0.0162, label: 0, bag_size: 25814\n",
      "batch 779, loss: 0.0003, instance_loss: 0.0480, weighted_loss: 0.0147, label: 1, bag_size: 1255\n",
      "batch 799, loss: 0.0233, instance_loss: 0.0009, weighted_loss: 0.0166, label: 1, bag_size: 5025\n",
      "batch 819, loss: 0.0124, instance_loss: 0.0083, weighted_loss: 0.0112, label: 1, bag_size: 2682\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.989329268292683: correct 12980/13120\n",
      "class 1 clustering acc 0.9567073170731707: correct 6276/6560\n",
      "Epoch: 81, train_loss: 0.1051, train_clustering_loss:  0.1014, train_error: 0.0390\n",
      "class 0: acc 0.9582366589327146, correct 413/431\n",
      "class 1: acc 0.9640102827763496, correct 375/389\n",
      "\n",
      "Val Set, val_loss: 0.1193, val_error: 0.0273, auc: 0.9930\n",
      "class 0 clustering acc 0.9619318181818182: correct 1693/1760\n",
      "class 1 clustering acc 0.8965909090909091: correct 789/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 9 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0045, instance_loss: 0.0001, weighted_loss: 0.0032, label: 0, bag_size: 14625\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 6851\n",
      "batch 59, loss: 0.0186, instance_loss: 0.0000, weighted_loss: 0.0130, label: 1, bag_size: 12180\n",
      "batch 79, loss: 0.1475, instance_loss: 0.1001, weighted_loss: 0.1333, label: 0, bag_size: 2270\n",
      "batch 99, loss: 0.0675, instance_loss: 0.0002, weighted_loss: 0.0473, label: 1, bag_size: 5903\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 16341\n",
      "batch 139, loss: 0.1716, instance_loss: 0.5802, weighted_loss: 0.2942, label: 1, bag_size: 11220\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8948\n",
      "batch 179, loss: 0.0008, instance_loss: 0.0097, weighted_loss: 0.0035, label: 0, bag_size: 11654\n",
      "batch 199, loss: 1.4531, instance_loss: 0.0747, weighted_loss: 1.0396, label: 1, bag_size: 9162\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0003, weighted_loss: 0.0002, label: 1, bag_size: 645\n",
      "batch 239, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 11600\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15841\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0164, weighted_loss: 0.0049, label: 1, bag_size: 629\n",
      "batch 299, loss: 0.0075, instance_loss: 0.0401, weighted_loss: 0.0173, label: 0, bag_size: 803\n",
      "batch 319, loss: 0.0056, instance_loss: 0.0000, weighted_loss: 0.0039, label: 1, bag_size: 16565\n",
      "batch 339, loss: 0.0251, instance_loss: 0.0101, weighted_loss: 0.0206, label: 0, bag_size: 10898\n",
      "batch 359, loss: 0.0015, instance_loss: 0.1669, weighted_loss: 0.0511, label: 1, bag_size: 6781\n",
      "batch 379, loss: 0.0003, instance_loss: 0.0095, weighted_loss: 0.0031, label: 0, bag_size: 1415\n",
      "batch 399, loss: 0.0246, instance_loss: 0.3646, weighted_loss: 0.1266, label: 0, bag_size: 2179\n",
      "batch 419, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 12593\n",
      "batch 439, loss: 0.0166, instance_loss: 0.5622, weighted_loss: 0.1803, label: 1, bag_size: 25695\n",
      "batch 459, loss: 0.0273, instance_loss: 0.0813, weighted_loss: 0.0435, label: 0, bag_size: 2918\n",
      "batch 479, loss: 0.0032, instance_loss: 0.0000, weighted_loss: 0.0022, label: 1, bag_size: 3640\n",
      "batch 499, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 10146\n",
      "batch 519, loss: 0.0709, instance_loss: 0.5569, weighted_loss: 0.2167, label: 1, bag_size: 1683\n",
      "batch 539, loss: 0.3446, instance_loss: 0.1412, weighted_loss: 0.2836, label: 0, bag_size: 1701\n",
      "batch 559, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 13786\n",
      "batch 579, loss: 0.6649, instance_loss: 0.0000, weighted_loss: 0.4654, label: 1, bag_size: 8592\n",
      "batch 599, loss: 1.3503, instance_loss: 0.2701, weighted_loss: 1.0262, label: 0, bag_size: 7428\n",
      "batch 619, loss: 0.0105, instance_loss: 0.0000, weighted_loss: 0.0073, label: 0, bag_size: 65728\n",
      "batch 639, loss: 0.0001, instance_loss: 0.0002, weighted_loss: 0.0002, label: 0, bag_size: 31780\n",
      "batch 659, loss: 0.0001, instance_loss: 0.0008, weighted_loss: 0.0003, label: 0, bag_size: 11759\n",
      "batch 679, loss: 0.0101, instance_loss: 0.0014, weighted_loss: 0.0075, label: 0, bag_size: 13992\n",
      "batch 699, loss: 0.0464, instance_loss: 1.4469, weighted_loss: 0.4666, label: 0, bag_size: 803\n",
      "batch 719, loss: 0.0247, instance_loss: 0.0000, weighted_loss: 0.0173, label: 1, bag_size: 7389\n",
      "batch 739, loss: 0.0017, instance_loss: 0.0048, weighted_loss: 0.0026, label: 1, bag_size: 2308\n",
      "batch 759, loss: 1.3940, instance_loss: 0.4745, weighted_loss: 1.1182, label: 1, bag_size: 9215\n",
      "batch 779, loss: 0.0018, instance_loss: 0.0002, weighted_loss: 0.0013, label: 0, bag_size: 2511\n",
      "batch 799, loss: 0.0031, instance_loss: 0.0082, weighted_loss: 0.0046, label: 0, bag_size: 3725\n",
      "batch 819, loss: 0.0219, instance_loss: 0.0000, weighted_loss: 0.0153, label: 1, bag_size: 6736\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9880335365853659: correct 12963/13120\n",
      "class 1 clustering acc 0.9515243902439025: correct 6242/6560\n",
      "Epoch: 82, train_loss: 0.1126, train_clustering_loss:  0.1092, train_error: 0.0488\n",
      "class 0: acc 0.9468599033816425, correct 392/414\n",
      "class 1: acc 0.9556650246305419, correct 388/406\n",
      "\n",
      "Val Set, val_loss: 0.2331, val_error: 0.1182, auc: 0.9934\n",
      "class 0 clustering acc 0.9528409090909091: correct 1677/1760\n",
      "class 1 clustering acc 0.8818181818181818: correct 776/880\n",
      "class 0: acc 0.75, correct 39/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 10 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 1.1678, instance_loss: 0.0041, weighted_loss: 0.8187, label: 1, bag_size: 21450\n",
      "batch 39, loss: 0.0275, instance_loss: 0.4768, weighted_loss: 0.1623, label: 0, bag_size: 2367\n",
      "batch 59, loss: 0.0001, instance_loss: 0.0017, weighted_loss: 0.0005, label: 0, bag_size: 1072\n",
      "batch 79, loss: 0.0141, instance_loss: 0.1826, weighted_loss: 0.0647, label: 0, bag_size: 1370\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 645\n",
      "batch 119, loss: 0.0046, instance_loss: 0.3155, weighted_loss: 0.0979, label: 0, bag_size: 9888\n",
      "batch 139, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 1, bag_size: 9065\n",
      "batch 159, loss: 0.0018, instance_loss: 0.0001, weighted_loss: 0.0013, label: 0, bag_size: 2351\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 10068\n",
      "batch 199, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 5345\n",
      "batch 219, loss: 0.0120, instance_loss: 0.0108, weighted_loss: 0.0116, label: 0, bag_size: 1498\n",
      "batch 239, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 26271\n",
      "batch 259, loss: 0.2167, instance_loss: 0.0241, weighted_loss: 0.1589, label: 0, bag_size: 7428\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0216, weighted_loss: 0.0065, label: 0, bag_size: 12687\n",
      "batch 299, loss: 0.0194, instance_loss: 0.0248, weighted_loss: 0.0210, label: 1, bag_size: 8438\n",
      "batch 319, loss: 0.4543, instance_loss: 0.3400, weighted_loss: 0.4200, label: 1, bag_size: 15185\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0040, weighted_loss: 0.0013, label: 0, bag_size: 17155\n",
      "batch 359, loss: 0.1720, instance_loss: 0.0119, weighted_loss: 0.1239, label: 1, bag_size: 11220\n",
      "batch 379, loss: 0.0033, instance_loss: 0.0197, weighted_loss: 0.0082, label: 0, bag_size: 1149\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 16087\n",
      "batch 419, loss: 0.0514, instance_loss: 0.0000, weighted_loss: 0.0360, label: 1, bag_size: 6745\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 23398\n",
      "batch 459, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 3552\n",
      "batch 479, loss: 0.1430, instance_loss: 0.0032, weighted_loss: 0.1010, label: 1, bag_size: 3368\n",
      "batch 499, loss: 0.0000, instance_loss: 0.0011, weighted_loss: 0.0003, label: 0, bag_size: 9433\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23037\n",
      "batch 539, loss: 0.0016, instance_loss: 0.0046, weighted_loss: 0.0025, label: 0, bag_size: 3893\n",
      "batch 559, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 5409\n",
      "batch 579, loss: 0.0010, instance_loss: 0.0051, weighted_loss: 0.0022, label: 0, bag_size: 2548\n",
      "batch 599, loss: 0.0008, instance_loss: 0.0123, weighted_loss: 0.0043, label: 0, bag_size: 9171\n",
      "batch 619, loss: 0.0017, instance_loss: 0.0010, weighted_loss: 0.0015, label: 1, bag_size: 11600\n",
      "batch 639, loss: 0.0004, instance_loss: 0.0004, weighted_loss: 0.0004, label: 1, bag_size: 1101\n",
      "batch 659, loss: 0.0052, instance_loss: 0.0000, weighted_loss: 0.0037, label: 1, bag_size: 6745\n",
      "batch 679, loss: 0.0081, instance_loss: 0.0180, weighted_loss: 0.0110, label: 1, bag_size: 1822\n",
      "batch 699, loss: 0.0056, instance_loss: 0.0002, weighted_loss: 0.0040, label: 1, bag_size: 1759\n",
      "batch 719, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 9471\n",
      "batch 739, loss: 0.0000, instance_loss: 0.0015, weighted_loss: 0.0005, label: 0, bag_size: 12524\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 15008\n",
      "batch 779, loss: 0.0209, instance_loss: 0.0036, weighted_loss: 0.0157, label: 1, bag_size: 11223\n",
      "batch 799, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 10396\n",
      "batch 819, loss: 0.0768, instance_loss: 0.2757, weighted_loss: 0.1365, label: 0, bag_size: 10410\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9888719512195122: correct 12974/13120\n",
      "class 1 clustering acc 0.9513719512195122: correct 6241/6560\n",
      "Epoch: 83, train_loss: 0.1024, train_clustering_loss:  0.1094, train_error: 0.0439\n",
      "class 0: acc 0.9576470588235294, correct 407/425\n",
      "class 1: acc 0.9544303797468354, correct 377/395\n",
      "\n",
      "Val Set, val_loss: 0.2345, val_error: 0.1182, auc: 0.9930\n",
      "class 0 clustering acc 0.9579545454545455: correct 1686/1760\n",
      "class 1 clustering acc 0.8920454545454546: correct 785/880\n",
      "class 0: acc 0.75, correct 39/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 11 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0001, instance_loss: 1.7092, weighted_loss: 0.5128, label: 1, bag_size: 1746\n",
      "batch 39, loss: 0.9763, instance_loss: 0.5626, weighted_loss: 0.8522, label: 1, bag_size: 6360\n",
      "batch 59, loss: 0.0022, instance_loss: 0.0465, weighted_loss: 0.0155, label: 0, bag_size: 1588\n",
      "batch 79, loss: 0.1832, instance_loss: 0.0000, weighted_loss: 0.1282, label: 1, bag_size: 11729\n",
      "batch 99, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 0, bag_size: 15313\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 9930\n",
      "batch 139, loss: 0.0302, instance_loss: 0.0000, weighted_loss: 0.0211, label: 1, bag_size: 13477\n",
      "batch 159, loss: 0.1552, instance_loss: 0.0072, weighted_loss: 0.1108, label: 1, bag_size: 1022\n",
      "batch 179, loss: 0.3186, instance_loss: 0.0028, weighted_loss: 0.2239, label: 1, bag_size: 3652\n",
      "batch 199, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 1244\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 19832\n",
      "batch 239, loss: 0.0004, instance_loss: 0.0007, weighted_loss: 0.0005, label: 0, bag_size: 9455\n",
      "batch 259, loss: 0.0782, instance_loss: 0.0000, weighted_loss: 0.0547, label: 1, bag_size: 16565\n",
      "batch 279, loss: 0.0274, instance_loss: 0.4051, weighted_loss: 0.1407, label: 0, bag_size: 2213\n",
      "batch 299, loss: 0.0028, instance_loss: 0.0000, weighted_loss: 0.0020, label: 1, bag_size: 7513\n",
      "batch 319, loss: 0.0193, instance_loss: 0.0000, weighted_loss: 0.0135, label: 1, bag_size: 29832\n",
      "batch 339, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 9062\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11389\n",
      "batch 379, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 10592\n",
      "batch 399, loss: 0.0374, instance_loss: 0.0002, weighted_loss: 0.0262, label: 1, bag_size: 13440\n",
      "batch 419, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 13368\n",
      "batch 439, loss: 0.0031, instance_loss: 0.0690, weighted_loss: 0.0229, label: 1, bag_size: 1755\n",
      "batch 459, loss: 0.0606, instance_loss: 0.0001, weighted_loss: 0.0425, label: 1, bag_size: 7424\n",
      "batch 479, loss: 0.0022, instance_loss: 0.0000, weighted_loss: 0.0016, label: 1, bag_size: 16154\n",
      "batch 499, loss: 0.0015, instance_loss: 0.0112, weighted_loss: 0.0044, label: 0, bag_size: 2920\n",
      "batch 519, loss: 1.0649, instance_loss: 0.1504, weighted_loss: 0.7906, label: 0, bag_size: 2959\n",
      "batch 539, loss: 0.0004, instance_loss: 0.0014, weighted_loss: 0.0007, label: 0, bag_size: 7989\n",
      "batch 559, loss: 0.1716, instance_loss: 0.0019, weighted_loss: 0.1207, label: 1, bag_size: 7351\n",
      "batch 579, loss: 0.6011, instance_loss: 0.0020, weighted_loss: 0.4214, label: 0, bag_size: 3654\n",
      "batch 599, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 16512\n",
      "batch 619, loss: 0.0126, instance_loss: 2.8160, weighted_loss: 0.8536, label: 0, bag_size: 2104\n",
      "batch 639, loss: 0.0016, instance_loss: 0.0667, weighted_loss: 0.0211, label: 1, bag_size: 9689\n",
      "batch 659, loss: 0.0023, instance_loss: 0.0019, weighted_loss: 0.0022, label: 1, bag_size: 7119\n",
      "batch 679, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 13880\n",
      "batch 699, loss: 0.6557, instance_loss: 0.0000, weighted_loss: 0.4590, label: 0, bag_size: 25420\n",
      "batch 719, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 5225\n",
      "batch 739, loss: 0.0006, instance_loss: 0.0001, weighted_loss: 0.0005, label: 1, bag_size: 22264\n",
      "batch 759, loss: 0.0000, instance_loss: 0.0041, weighted_loss: 0.0012, label: 0, bag_size: 10263\n",
      "batch 779, loss: 0.0034, instance_loss: 0.0015, weighted_loss: 0.0028, label: 0, bag_size: 1789\n",
      "batch 799, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 9455\n",
      "batch 819, loss: 0.0039, instance_loss: 0.0000, weighted_loss: 0.0027, label: 0, bag_size: 4845\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9919969512195121: correct 13015/13120\n",
      "class 1 clustering acc 0.963719512195122: correct 6322/6560\n",
      "Epoch: 84, train_loss: 0.1022, train_clustering_loss:  0.0820, train_error: 0.0402\n",
      "class 0: acc 0.9588377723970944, correct 396/413\n",
      "class 1: acc 0.9606879606879607, correct 391/407\n",
      "\n",
      "Val Set, val_loss: 0.1002, val_error: 0.0364, auc: 0.9937\n",
      "class 0 clustering acc 0.9414772727272728: correct 1657/1760\n",
      "class 1 clustering acc 0.8670454545454546: correct 763/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9655172413793104, correct 56/58\n",
      "Validation loss decreased (0.106754 --> 0.100211).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11546\n",
      "batch 39, loss: 0.0053, instance_loss: 5.9385, weighted_loss: 1.7852, label: 0, bag_size: 20555\n",
      "batch 59, loss: 0.0027, instance_loss: 0.3771, weighted_loss: 0.1151, label: 0, bag_size: 7637\n",
      "batch 79, loss: 0.0255, instance_loss: 0.5035, weighted_loss: 0.1689, label: 1, bag_size: 5155\n",
      "batch 99, loss: 0.0014, instance_loss: 0.5392, weighted_loss: 0.1627, label: 0, bag_size: 931\n",
      "batch 119, loss: 0.0026, instance_loss: 0.0146, weighted_loss: 0.0062, label: 1, bag_size: 2495\n",
      "batch 139, loss: 0.0021, instance_loss: 0.0000, weighted_loss: 0.0015, label: 1, bag_size: 9877\n",
      "batch 159, loss: 0.0015, instance_loss: 0.2686, weighted_loss: 0.0816, label: 1, bag_size: 1755\n",
      "batch 179, loss: 0.0019, instance_loss: 0.0014, weighted_loss: 0.0017, label: 1, bag_size: 1822\n",
      "batch 199, loss: 0.0000, instance_loss: 0.2892, weighted_loss: 0.0868, label: 1, bag_size: 9321\n",
      "batch 219, loss: 0.4617, instance_loss: 0.2422, weighted_loss: 0.3958, label: 0, bag_size: 3897\n",
      "batch 239, loss: 0.0008, instance_loss: 0.0007, weighted_loss: 0.0008, label: 1, bag_size: 11266\n",
      "batch 259, loss: 0.0454, instance_loss: 0.0843, weighted_loss: 0.0571, label: 1, bag_size: 2522\n",
      "batch 279, loss: 0.0979, instance_loss: 0.0724, weighted_loss: 0.0902, label: 0, bag_size: 2160\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23037\n",
      "batch 319, loss: 0.0041, instance_loss: 0.0160, weighted_loss: 0.0077, label: 0, bag_size: 3774\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 16992\n",
      "batch 359, loss: 0.0017, instance_loss: 0.0001, weighted_loss: 0.0012, label: 0, bag_size: 11546\n",
      "batch 379, loss: 0.1155, instance_loss: 0.0158, weighted_loss: 0.0856, label: 0, bag_size: 1415\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 23996\n",
      "batch 419, loss: 0.0008, instance_loss: 0.0088, weighted_loss: 0.0032, label: 0, bag_size: 23796\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0022, weighted_loss: 0.0007, label: 1, bag_size: 4862\n",
      "batch 459, loss: 0.5715, instance_loss: 0.0000, weighted_loss: 0.4001, label: 0, bag_size: 25814\n",
      "batch 479, loss: 0.0117, instance_loss: 0.0087, weighted_loss: 0.0108, label: 1, bag_size: 12603\n",
      "batch 499, loss: 0.0102, instance_loss: 0.1003, weighted_loss: 0.0372, label: 1, bag_size: 9004\n",
      "batch 519, loss: 0.1263, instance_loss: 0.0000, weighted_loss: 0.0884, label: 1, bag_size: 6090\n",
      "batch 539, loss: 0.0049, instance_loss: 0.0000, weighted_loss: 0.0034, label: 1, bag_size: 6745\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 18240\n",
      "batch 579, loss: 0.0000, instance_loss: 0.1084, weighted_loss: 0.0325, label: 1, bag_size: 689\n",
      "batch 599, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 7078\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 16936\n",
      "batch 639, loss: 1.5338, instance_loss: 1.5906, weighted_loss: 1.5509, label: 1, bag_size: 2731\n",
      "batch 659, loss: 0.0128, instance_loss: 0.0425, weighted_loss: 0.0217, label: 1, bag_size: 10912\n",
      "batch 679, loss: 1.6442, instance_loss: 4.0177, weighted_loss: 2.3562, label: 1, bag_size: 8982\n",
      "batch 699, loss: 0.0001, instance_loss: 0.0003, weighted_loss: 0.0002, label: 0, bag_size: 11187\n",
      "batch 719, loss: 0.0000, instance_loss: 0.1149, weighted_loss: 0.0345, label: 1, bag_size: 1609\n",
      "batch 739, loss: 0.0871, instance_loss: 0.0092, weighted_loss: 0.0638, label: 0, bag_size: 2918\n",
      "batch 759, loss: 0.0003, instance_loss: 0.0034, weighted_loss: 0.0012, label: 0, bag_size: 8788\n",
      "batch 779, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15077\n",
      "batch 799, loss: 0.0108, instance_loss: 0.0000, weighted_loss: 0.0075, label: 1, bag_size: 15689\n",
      "batch 819, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 10482\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9823170731707317: correct 12888/13120\n",
      "class 1 clustering acc 0.91875: correct 6027/6560\n",
      "Epoch: 85, train_loss: 0.1057, train_clustering_loss:  0.1785, train_error: 0.0354\n",
      "class 0: acc 0.9663461538461539, correct 402/416\n",
      "class 1: acc 0.9628712871287128, correct 389/404\n",
      "\n",
      "Val Set, val_loss: 0.1590, val_error: 0.0636, auc: 0.9940\n",
      "class 0 clustering acc 0.9545454545454546: correct 1680/1760\n",
      "class 1 clustering acc 0.875: correct 770/880\n",
      "class 0: acc 0.8653846153846154, correct 45/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 1 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 22800\n",
      "batch 39, loss: 0.0069, instance_loss: 0.0000, weighted_loss: 0.0048, label: 0, bag_size: 4345\n",
      "batch 59, loss: 0.0200, instance_loss: 0.0000, weighted_loss: 0.0140, label: 0, bag_size: 10304\n",
      "batch 79, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 14618\n",
      "batch 99, loss: 0.0029, instance_loss: 0.0000, weighted_loss: 0.0020, label: 1, bag_size: 5137\n",
      "batch 119, loss: 0.0001, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 18468\n",
      "batch 139, loss: 0.0008, instance_loss: 0.0007, weighted_loss: 0.0007, label: 1, bag_size: 7389\n",
      "batch 159, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 19043\n",
      "batch 179, loss: 0.0002, instance_loss: 0.0391, weighted_loss: 0.0119, label: 1, bag_size: 689\n",
      "batch 199, loss: 0.0020, instance_loss: 0.0011, weighted_loss: 0.0017, label: 0, bag_size: 13339\n",
      "batch 219, loss: 0.0003, instance_loss: 0.0021, weighted_loss: 0.0009, label: 1, bag_size: 7515\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 0, bag_size: 6851\n",
      "batch 259, loss: 0.0725, instance_loss: 0.0000, weighted_loss: 0.0507, label: 0, bag_size: 10029\n",
      "batch 279, loss: 3.8875, instance_loss: 1.2981, weighted_loss: 3.1107, label: 0, bag_size: 2653\n",
      "batch 299, loss: 0.0005, instance_loss: 0.0437, weighted_loss: 0.0135, label: 0, bag_size: 1234\n",
      "batch 319, loss: 0.0032, instance_loss: 0.0084, weighted_loss: 0.0048, label: 0, bag_size: 2998\n",
      "batch 339, loss: 0.0012, instance_loss: 0.0001, weighted_loss: 0.0009, label: 0, bag_size: 2873\n",
      "batch 359, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 2936\n",
      "batch 379, loss: 0.4231, instance_loss: 0.7237, weighted_loss: 0.5133, label: 1, bag_size: 6360\n",
      "batch 399, loss: 0.0416, instance_loss: 0.0000, weighted_loss: 0.0291, label: 0, bag_size: 10365\n",
      "batch 419, loss: 0.0248, instance_loss: 0.0203, weighted_loss: 0.0235, label: 0, bag_size: 1814\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11195\n",
      "batch 459, loss: 0.0011, instance_loss: 0.0001, weighted_loss: 0.0008, label: 1, bag_size: 1014\n",
      "batch 479, loss: 0.0398, instance_loss: 0.0962, weighted_loss: 0.0567, label: 1, bag_size: 18161\n",
      "batch 499, loss: 0.6681, instance_loss: 0.0214, weighted_loss: 0.4741, label: 1, bag_size: 8103\n",
      "batch 519, loss: 0.0009, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 13205\n",
      "batch 539, loss: 0.0010, instance_loss: 0.1124, weighted_loss: 0.0344, label: 0, bag_size: 10751\n",
      "batch 559, loss: 0.0064, instance_loss: 0.0006, weighted_loss: 0.0046, label: 0, bag_size: 22498\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8948\n",
      "batch 599, loss: 0.0012, instance_loss: 0.0019, weighted_loss: 0.0014, label: 0, bag_size: 2360\n",
      "batch 619, loss: 0.0003, instance_loss: 0.0010, weighted_loss: 0.0005, label: 0, bag_size: 2652\n",
      "batch 639, loss: 0.0634, instance_loss: 0.0217, weighted_loss: 0.0509, label: 1, bag_size: 1822\n",
      "batch 659, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 15313\n",
      "batch 679, loss: 0.0865, instance_loss: 0.0000, weighted_loss: 0.0606, label: 1, bag_size: 6090\n",
      "batch 699, loss: 0.0012, instance_loss: 0.0000, weighted_loss: 0.0009, label: 1, bag_size: 15093\n",
      "batch 719, loss: 0.0200, instance_loss: 0.0001, weighted_loss: 0.0141, label: 1, bag_size: 8026\n",
      "batch 739, loss: 0.0014, instance_loss: 0.6949, weighted_loss: 0.2095, label: 1, bag_size: 5256\n",
      "batch 759, loss: 0.0002, instance_loss: 0.0004, weighted_loss: 0.0003, label: 1, bag_size: 13174\n",
      "batch 779, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 12460\n",
      "batch 799, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 12865\n",
      "batch 819, loss: 0.0375, instance_loss: 0.0002, weighted_loss: 0.0263, label: 1, bag_size: 10432\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9889481707317073: correct 12975/13120\n",
      "class 1 clustering acc 0.948170731707317: correct 6220/6560\n",
      "Epoch: 86, train_loss: 0.0807, train_clustering_loss:  0.1016, train_error: 0.0232\n",
      "class 0: acc 0.9760765550239234, correct 408/418\n",
      "class 1: acc 0.9776119402985075, correct 393/402\n",
      "\n",
      "Val Set, val_loss: 0.1351, val_error: 0.0455, auc: 0.9927\n",
      "class 0 clustering acc 0.9488636363636364: correct 1670/1760\n",
      "class 1 clustering acc 0.85: correct 748/880\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 2 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.1163, instance_loss: 0.0003, weighted_loss: 0.0815, label: 1, bag_size: 15689\n",
      "batch 39, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 8372\n",
      "batch 59, loss: 0.0016, instance_loss: 0.0018, weighted_loss: 0.0016, label: 0, bag_size: 1349\n",
      "batch 79, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11600\n",
      "batch 99, loss: 0.0166, instance_loss: 0.0330, weighted_loss: 0.0215, label: 0, bag_size: 1909\n",
      "batch 119, loss: 0.0044, instance_loss: 0.0000, weighted_loss: 0.0031, label: 1, bag_size: 9877\n",
      "batch 139, loss: 0.0012, instance_loss: 0.0116, weighted_loss: 0.0043, label: 1, bag_size: 1437\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 16720\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0005, weighted_loss: 0.0002, label: 1, bag_size: 9321\n",
      "batch 199, loss: 0.0060, instance_loss: 0.0000, weighted_loss: 0.0042, label: 0, bag_size: 21138\n",
      "batch 219, loss: 0.0117, instance_loss: 0.0001, weighted_loss: 0.0082, label: 1, bag_size: 5454\n",
      "batch 239, loss: 0.0002, instance_loss: 0.0236, weighted_loss: 0.0073, label: 0, bag_size: 3893\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 11865\n",
      "batch 279, loss: 0.0078, instance_loss: 0.0023, weighted_loss: 0.0062, label: 1, bag_size: 7798\n",
      "batch 299, loss: 0.0167, instance_loss: 0.0152, weighted_loss: 0.0163, label: 1, bag_size: 5723\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0006, weighted_loss: 0.0002, label: 0, bag_size: 6898\n",
      "batch 339, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 14206\n",
      "batch 359, loss: 0.0653, instance_loss: 0.2069, weighted_loss: 0.1078, label: 1, bag_size: 12626\n",
      "batch 379, loss: 0.0003, instance_loss: 0.0022, weighted_loss: 0.0009, label: 0, bag_size: 1651\n",
      "batch 399, loss: 1.0789, instance_loss: 0.0102, weighted_loss: 0.7583, label: 0, bag_size: 11212\n",
      "batch 419, loss: 0.0002, instance_loss: 0.0194, weighted_loss: 0.0060, label: 0, bag_size: 19466\n",
      "batch 439, loss: 0.0016, instance_loss: 0.0001, weighted_loss: 0.0012, label: 1, bag_size: 25695\n",
      "batch 459, loss: 0.0226, instance_loss: 0.0050, weighted_loss: 0.0173, label: 1, bag_size: 7768\n",
      "batch 479, loss: 0.0003, instance_loss: 0.0992, weighted_loss: 0.0300, label: 1, bag_size: 9533\n",
      "batch 499, loss: 0.0008, instance_loss: 0.0080, weighted_loss: 0.0030, label: 0, bag_size: 4997\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 22828\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9065\n",
      "batch 559, loss: 0.0290, instance_loss: 0.1110, weighted_loss: 0.0536, label: 0, bag_size: 2043\n",
      "batch 579, loss: 0.0012, instance_loss: 0.0026, weighted_loss: 0.0016, label: 0, bag_size: 10942\n",
      "batch 599, loss: 0.0003, instance_loss: 0.0012, weighted_loss: 0.0006, label: 0, bag_size: 1234\n",
      "batch 619, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 11875\n",
      "batch 639, loss: 0.0047, instance_loss: 0.0000, weighted_loss: 0.0033, label: 1, bag_size: 3683\n",
      "batch 659, loss: 0.0097, instance_loss: 0.0000, weighted_loss: 0.0068, label: 1, bag_size: 9877\n",
      "batch 679, loss: 0.0093, instance_loss: 0.0000, weighted_loss: 0.0065, label: 0, bag_size: 11922\n",
      "batch 699, loss: 0.0020, instance_loss: 0.0042, weighted_loss: 0.0026, label: 1, bag_size: 10072\n",
      "batch 719, loss: 0.0003, instance_loss: 0.0099, weighted_loss: 0.0032, label: 1, bag_size: 14202\n",
      "batch 739, loss: 1.0359, instance_loss: 1.6771, weighted_loss: 1.2282, label: 1, bag_size: 1095\n",
      "batch 759, loss: 0.0229, instance_loss: 0.1428, weighted_loss: 0.0589, label: 1, bag_size: 2522\n",
      "batch 779, loss: 0.0009, instance_loss: 0.2945, weighted_loss: 0.0890, label: 0, bag_size: 2336\n",
      "batch 799, loss: 0.0067, instance_loss: 0.0019, weighted_loss: 0.0053, label: 0, bag_size: 2367\n",
      "batch 819, loss: 0.0000, instance_loss: 0.3536, weighted_loss: 0.1061, label: 1, bag_size: 7935\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9869664634146341: correct 12949/13120\n",
      "class 1 clustering acc 0.9292682926829269: correct 6096/6560\n",
      "Epoch: 87, train_loss: 0.1011, train_clustering_loss:  0.1332, train_error: 0.0451\n",
      "class 0: acc 0.9481481481481482, correct 384/405\n",
      "class 1: acc 0.9614457831325302, correct 399/415\n",
      "\n",
      "Val Set, val_loss: 0.1571, val_error: 0.0545, auc: 0.9934\n",
      "class 0 clustering acc 0.9573863636363636: correct 1685/1760\n",
      "class 1 clustering acc 0.8988636363636363: correct 791/880\n",
      "class 0: acc 0.8846153846153846, correct 46/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 3 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0274, instance_loss: 0.0000, weighted_loss: 0.0192, label: 1, bag_size: 5025\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0033, weighted_loss: 0.0011, label: 0, bag_size: 4523\n",
      "batch 59, loss: 0.0663, instance_loss: 0.1048, weighted_loss: 0.0778, label: 0, bag_size: 2179\n",
      "batch 79, loss: 0.0043, instance_loss: 0.0011, weighted_loss: 0.0033, label: 1, bag_size: 5629\n",
      "batch 99, loss: 0.0028, instance_loss: 0.0737, weighted_loss: 0.0240, label: 1, bag_size: 4239\n",
      "batch 119, loss: 0.0012, instance_loss: 0.0330, weighted_loss: 0.0107, label: 1, bag_size: 1015\n",
      "batch 139, loss: 0.0000, instance_loss: 0.0299, weighted_loss: 0.0090, label: 1, bag_size: 9732\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 5612\n",
      "batch 179, loss: 0.0001, instance_loss: 0.0014, weighted_loss: 0.0005, label: 1, bag_size: 1437\n",
      "batch 199, loss: 0.0746, instance_loss: 0.0067, weighted_loss: 0.0543, label: 1, bag_size: 4939\n",
      "batch 219, loss: 0.0001, instance_loss: 0.0003, weighted_loss: 0.0001, label: 1, bag_size: 8410\n",
      "batch 239, loss: 0.0003, instance_loss: 0.0387, weighted_loss: 0.0118, label: 1, bag_size: 7246\n",
      "batch 259, loss: 3.1651, instance_loss: 4.0520, weighted_loss: 3.4311, label: 1, bag_size: 2731\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0239, weighted_loss: 0.0072, label: 1, bag_size: 4423\n",
      "batch 299, loss: 0.6814, instance_loss: 0.0217, weighted_loss: 0.4835, label: 0, bag_size: 3802\n",
      "batch 319, loss: 0.0827, instance_loss: 0.0570, weighted_loss: 0.0750, label: 1, bag_size: 1123\n",
      "batch 339, loss: 0.0001, instance_loss: 0.0010, weighted_loss: 0.0003, label: 0, bag_size: 2748\n",
      "batch 359, loss: 0.0016, instance_loss: 0.0111, weighted_loss: 0.0045, label: 1, bag_size: 14433\n",
      "batch 379, loss: 0.0005, instance_loss: 0.0006, weighted_loss: 0.0005, label: 1, bag_size: 17769\n",
      "batch 399, loss: 0.0001, instance_loss: 0.0013, weighted_loss: 0.0005, label: 1, bag_size: 1638\n",
      "batch 419, loss: 1.8660, instance_loss: 0.0240, weighted_loss: 1.3134, label: 0, bag_size: 14664\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0004, weighted_loss: 0.0002, label: 1, bag_size: 16417\n",
      "batch 459, loss: 0.1704, instance_loss: 0.2176, weighted_loss: 0.1845, label: 0, bag_size: 10381\n",
      "batch 479, loss: 0.0001, instance_loss: 0.0018, weighted_loss: 0.0006, label: 0, bag_size: 2244\n",
      "batch 499, loss: 0.0211, instance_loss: 0.0331, weighted_loss: 0.0247, label: 0, bag_size: 2270\n",
      "batch 519, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 7078\n",
      "batch 539, loss: 0.1217, instance_loss: 0.0057, weighted_loss: 0.0869, label: 1, bag_size: 2395\n",
      "batch 559, loss: 0.0016, instance_loss: 0.0015, weighted_loss: 0.0015, label: 0, bag_size: 1349\n",
      "batch 579, loss: 0.0009, instance_loss: 0.0049, weighted_loss: 0.0021, label: 0, bag_size: 1483\n",
      "batch 599, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 22870\n",
      "batch 619, loss: 0.0043, instance_loss: 1.5023, weighted_loss: 0.4537, label: 0, bag_size: 803\n",
      "batch 639, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 8788\n",
      "batch 659, loss: 0.2792, instance_loss: 0.0258, weighted_loss: 0.2032, label: 0, bag_size: 1920\n",
      "batch 679, loss: 0.0365, instance_loss: 0.0013, weighted_loss: 0.0259, label: 1, bag_size: 14887\n",
      "batch 699, loss: 0.0046, instance_loss: 0.0075, weighted_loss: 0.0054, label: 0, bag_size: 8755\n",
      "batch 719, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0012, label: 0, bag_size: 16211\n",
      "batch 739, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0006, label: 1, bag_size: 11160\n",
      "batch 759, loss: 0.0194, instance_loss: 0.0000, weighted_loss: 0.0135, label: 1, bag_size: 9004\n",
      "batch 779, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0000, label: 1, bag_size: 9610\n",
      "batch 799, loss: 0.2256, instance_loss: 3.8497, weighted_loss: 1.3128, label: 0, bag_size: 17279\n",
      "batch 819, loss: 0.0000, instance_loss: 0.2853, weighted_loss: 0.0856, label: 1, bag_size: 689\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9886432926829268: correct 12971/13120\n",
      "class 1 clustering acc 0.9545731707317073: correct 6262/6560\n",
      "Epoch: 88, train_loss: 0.0941, train_clustering_loss:  0.1035, train_error: 0.0305\n",
      "class 0: acc 0.9679802955665024, correct 393/406\n",
      "class 1: acc 0.9710144927536232, correct 402/414\n",
      "\n",
      "Val Set, val_loss: 0.1181, val_error: 0.0273, auc: 0.9934\n",
      "class 0 clustering acc 0.9488636363636364: correct 1670/1760\n",
      "class 1 clustering acc 0.8443181818181819: correct 743/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 4 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0156, instance_loss: 0.0305, weighted_loss: 0.0201, label: 0, bag_size: 4845\n",
      "batch 39, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 12460\n",
      "batch 59, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 13051\n",
      "batch 79, loss: 1.0258, instance_loss: 0.2943, weighted_loss: 0.8064, label: 1, bag_size: 1038\n",
      "batch 99, loss: 0.0035, instance_loss: 0.0000, weighted_loss: 0.0024, label: 1, bag_size: 3224\n",
      "batch 119, loss: 0.0322, instance_loss: 0.0033, weighted_loss: 0.0235, label: 0, bag_size: 18954\n",
      "batch 139, loss: 0.0007, instance_loss: 0.0007, weighted_loss: 0.0007, label: 1, bag_size: 16267\n",
      "batch 159, loss: 0.0003, instance_loss: 0.0168, weighted_loss: 0.0052, label: 1, bag_size: 3856\n",
      "batch 179, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 2662\n",
      "batch 199, loss: 0.0148, instance_loss: 0.0024, weighted_loss: 0.0111, label: 0, bag_size: 1508\n",
      "batch 219, loss: 0.0002, instance_loss: 0.0002, weighted_loss: 0.0002, label: 1, bag_size: 14433\n",
      "batch 239, loss: 0.0015, instance_loss: 0.0216, weighted_loss: 0.0075, label: 1, bag_size: 11394\n",
      "batch 259, loss: 0.0004, instance_loss: 0.0002, weighted_loss: 0.0003, label: 1, bag_size: 2904\n",
      "batch 279, loss: 0.0046, instance_loss: 0.0000, weighted_loss: 0.0032, label: 1, bag_size: 17769\n",
      "batch 299, loss: 0.0072, instance_loss: 0.0277, weighted_loss: 0.0134, label: 0, bag_size: 2104\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 5221\n",
      "batch 339, loss: 0.0608, instance_loss: 0.0819, weighted_loss: 0.0671, label: 0, bag_size: 2213\n",
      "batch 359, loss: 0.0359, instance_loss: 0.0150, weighted_loss: 0.0297, label: 1, bag_size: 1123\n",
      "batch 379, loss: 0.1470, instance_loss: 0.0017, weighted_loss: 0.1034, label: 1, bag_size: 1819\n",
      "batch 399, loss: 0.0104, instance_loss: 0.0000, weighted_loss: 0.0073, label: 1, bag_size: 5155\n",
      "batch 419, loss: 0.0032, instance_loss: 0.0000, weighted_loss: 0.0023, label: 0, bag_size: 9415\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 16936\n",
      "batch 459, loss: 0.3415, instance_loss: 0.1249, weighted_loss: 0.2765, label: 0, bag_size: 2959\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 13591\n",
      "batch 499, loss: 0.0055, instance_loss: 0.0008, weighted_loss: 0.0041, label: 1, bag_size: 10622\n",
      "batch 519, loss: 0.2701, instance_loss: 0.0000, weighted_loss: 0.1891, label: 0, bag_size: 14625\n",
      "batch 539, loss: 0.0024, instance_loss: 0.0746, weighted_loss: 0.0241, label: 1, bag_size: 1920\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 7935\n",
      "batch 579, loss: 0.1459, instance_loss: 0.0447, weighted_loss: 0.1155, label: 0, bag_size: 11151\n",
      "batch 599, loss: 0.0003, instance_loss: 0.0048, weighted_loss: 0.0017, label: 1, bag_size: 12758\n",
      "batch 619, loss: 0.0112, instance_loss: 1.3400, weighted_loss: 0.4098, label: 0, bag_size: 10410\n",
      "batch 639, loss: 0.0271, instance_loss: 0.6403, weighted_loss: 0.2111, label: 0, bag_size: 1416\n",
      "batch 659, loss: 0.0022, instance_loss: 0.0098, weighted_loss: 0.0045, label: 1, bag_size: 1493\n",
      "batch 679, loss: 0.0023, instance_loss: 0.0000, weighted_loss: 0.0016, label: 1, bag_size: 12460\n",
      "batch 699, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 0, bag_size: 12593\n",
      "batch 719, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 14377\n",
      "batch 739, loss: 0.0447, instance_loss: 0.2838, weighted_loss: 0.1164, label: 1, bag_size: 1919\n",
      "batch 759, loss: 0.1117, instance_loss: 0.0496, weighted_loss: 0.0931, label: 1, bag_size: 12714\n",
      "batch 779, loss: 0.0041, instance_loss: 0.0015, weighted_loss: 0.0033, label: 0, bag_size: 7557\n",
      "batch 799, loss: 0.0113, instance_loss: 0.0000, weighted_loss: 0.0079, label: 1, bag_size: 22286\n",
      "batch 819, loss: 0.0611, instance_loss: 0.0029, weighted_loss: 0.0437, label: 1, bag_size: 4789\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.988795731707317: correct 12973/13120\n",
      "class 1 clustering acc 0.9521341463414634: correct 6246/6560\n",
      "Epoch: 89, train_loss: 0.0956, train_clustering_loss:  0.1007, train_error: 0.0415\n",
      "class 0: acc 0.9601990049751243, correct 386/402\n",
      "class 1: acc 0.9569377990430622, correct 400/418\n",
      "\n",
      "Val Set, val_loss: 0.1120, val_error: 0.0273, auc: 0.9924\n",
      "class 0 clustering acc 0.9539772727272727: correct 1679/1760\n",
      "class 1 clustering acc 0.8670454545454546: correct 763/880\n",
      "class 0: acc 0.9423076923076923, correct 49/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 5 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0000, instance_loss: 0.0012, weighted_loss: 0.0004, label: 1, bag_size: 11642\n",
      "batch 39, loss: 0.0005, instance_loss: 0.0041, weighted_loss: 0.0016, label: 0, bag_size: 1684\n",
      "batch 59, loss: 0.0865, instance_loss: 0.5032, weighted_loss: 0.2115, label: 0, bag_size: 3375\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 6317\n",
      "batch 99, loss: 0.0083, instance_loss: 0.1961, weighted_loss: 0.0646, label: 0, bag_size: 931\n",
      "batch 119, loss: 0.0022, instance_loss: 0.0000, weighted_loss: 0.0015, label: 0, bag_size: 18415\n",
      "batch 139, loss: 0.0066, instance_loss: 0.0286, weighted_loss: 0.0132, label: 0, bag_size: 19808\n",
      "batch 159, loss: 0.1119, instance_loss: 0.1207, weighted_loss: 0.1145, label: 0, bag_size: 1052\n",
      "batch 179, loss: 0.2192, instance_loss: 0.4093, weighted_loss: 0.2762, label: 1, bag_size: 12719\n",
      "batch 199, loss: 0.0012, instance_loss: 0.0002, weighted_loss: 0.0009, label: 1, bag_size: 11684\n",
      "batch 219, loss: 0.0406, instance_loss: 0.0369, weighted_loss: 0.0395, label: 0, bag_size: 931\n",
      "batch 239, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 0, bag_size: 16992\n",
      "batch 259, loss: 0.0000, instance_loss: 0.1863, weighted_loss: 0.0559, label: 0, bag_size: 1588\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0032, weighted_loss: 0.0010, label: 1, bag_size: 9732\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15077\n",
      "batch 319, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 2303\n",
      "batch 339, loss: 0.0087, instance_loss: 0.0144, weighted_loss: 0.0104, label: 1, bag_size: 1755\n",
      "batch 359, loss: 0.7835, instance_loss: 0.1084, weighted_loss: 0.5810, label: 0, bag_size: 1506\n",
      "batch 379, loss: 1.1862, instance_loss: 0.0584, weighted_loss: 0.8479, label: 0, bag_size: 2653\n",
      "batch 399, loss: 0.0001, instance_loss: 0.0003, weighted_loss: 0.0002, label: 1, bag_size: 5864\n",
      "batch 419, loss: 0.0000, instance_loss: 0.0032, weighted_loss: 0.0010, label: 1, bag_size: 9321\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 13174\n",
      "batch 459, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 0, bag_size: 12201\n",
      "batch 479, loss: 0.0201, instance_loss: 0.0483, weighted_loss: 0.0285, label: 1, bag_size: 2314\n",
      "batch 499, loss: 0.0031, instance_loss: 0.0000, weighted_loss: 0.0022, label: 0, bag_size: 16211\n",
      "batch 519, loss: 0.0068, instance_loss: 0.0000, weighted_loss: 0.0048, label: 1, bag_size: 10848\n",
      "batch 539, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 4465\n",
      "batch 559, loss: 0.0014, instance_loss: 0.0015, weighted_loss: 0.0014, label: 1, bag_size: 549\n",
      "batch 579, loss: 0.0658, instance_loss: 0.9829, weighted_loss: 0.3410, label: 0, bag_size: 24382\n",
      "batch 599, loss: 0.0162, instance_loss: 0.2532, weighted_loss: 0.0873, label: 0, bag_size: 1684\n",
      "batch 619, loss: 0.0094, instance_loss: 0.2093, weighted_loss: 0.0694, label: 1, bag_size: 1755\n",
      "batch 639, loss: 0.1227, instance_loss: 1.5899, weighted_loss: 0.5629, label: 0, bag_size: 13332\n",
      "batch 659, loss: 0.0030, instance_loss: 0.1562, weighted_loss: 0.0490, label: 1, bag_size: 13089\n",
      "batch 679, loss: 0.0007, instance_loss: 0.0045, weighted_loss: 0.0018, label: 1, bag_size: 1014\n",
      "batch 699, loss: 0.3373, instance_loss: 1.6556, weighted_loss: 0.7328, label: 0, bag_size: 17279\n",
      "batch 719, loss: 0.0008, instance_loss: 0.1878, weighted_loss: 0.0569, label: 1, bag_size: 11394\n",
      "batch 739, loss: 0.0142, instance_loss: 0.1358, weighted_loss: 0.0507, label: 0, bag_size: 10128\n",
      "batch 759, loss: 0.2501, instance_loss: 0.0754, weighted_loss: 0.1977, label: 1, bag_size: 2565\n",
      "batch 779, loss: 0.1399, instance_loss: 0.4176, weighted_loss: 0.2232, label: 1, bag_size: 2314\n",
      "batch 799, loss: 0.0027, instance_loss: 0.0000, weighted_loss: 0.0019, label: 1, bag_size: 17769\n",
      "batch 819, loss: 0.0276, instance_loss: 0.0001, weighted_loss: 0.0193, label: 1, bag_size: 12425\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9868140243902439: correct 12947/13120\n",
      "class 1 clustering acc 0.9477134146341464: correct 6217/6560\n",
      "Epoch: 90, train_loss: 0.0947, train_clustering_loss:  0.1225, train_error: 0.0341\n",
      "class 0: acc 0.9610705596107056, correct 395/411\n",
      "class 1: acc 0.9706601466992665, correct 397/409\n",
      "\n",
      "Val Set, val_loss: 0.1564, val_error: 0.0455, auc: 0.9934\n",
      "class 0 clustering acc 0.9517045454545454: correct 1675/1760\n",
      "class 1 clustering acc 0.8590909090909091: correct 756/880\n",
      "class 0: acc 0.9038461538461539, correct 47/52\n",
      "class 1: acc 1.0, correct 58/58\n",
      "EarlyStopping counter: 6 out of 20\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0002, instance_loss: 0.0393, weighted_loss: 0.0119, label: 1, bag_size: 4308\n",
      "batch 39, loss: 0.0416, instance_loss: 0.0000, weighted_loss: 0.0291, label: 1, bag_size: 7873\n",
      "batch 59, loss: 0.0904, instance_loss: 0.0326, weighted_loss: 0.0731, label: 1, bag_size: 3121\n",
      "batch 79, loss: 0.0003, instance_loss: 0.0095, weighted_loss: 0.0031, label: 0, bag_size: 24911\n",
      "batch 99, loss: 0.0144, instance_loss: 0.0000, weighted_loss: 0.0101, label: 1, bag_size: 7389\n",
      "batch 119, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 1781\n",
      "batch 139, loss: 0.0371, instance_loss: 0.1190, weighted_loss: 0.0616, label: 1, bag_size: 5516\n",
      "batch 159, loss: 0.0000, instance_loss: 0.0006, weighted_loss: 0.0002, label: 1, bag_size: 8522\n",
      "batch 179, loss: 0.3469, instance_loss: 0.1155, weighted_loss: 0.2775, label: 1, bag_size: 9162\n",
      "batch 199, loss: 0.0004, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 22681\n",
      "batch 219, loss: 0.0037, instance_loss: 0.0000, weighted_loss: 0.0026, label: 0, bag_size: 12148\n",
      "batch 239, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9673\n",
      "batch 259, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 32227\n",
      "batch 279, loss: 0.0001, instance_loss: 0.0079, weighted_loss: 0.0025, label: 0, bag_size: 3970\n",
      "batch 299, loss: 0.0089, instance_loss: 0.0000, weighted_loss: 0.0062, label: 0, bag_size: 10490\n",
      "batch 319, loss: 0.0006, instance_loss: 0.0000, weighted_loss: 0.0004, label: 1, bag_size: 5864\n",
      "batch 339, loss: 0.0126, instance_loss: 0.0000, weighted_loss: 0.0089, label: 1, bag_size: 7513\n",
      "batch 359, loss: 0.0013, instance_loss: 0.0003, weighted_loss: 0.0010, label: 1, bag_size: 9478\n",
      "batch 379, loss: 0.0002, instance_loss: 0.0001, weighted_loss: 0.0001, label: 0, bag_size: 3265\n",
      "batch 399, loss: 0.0031, instance_loss: 0.0082, weighted_loss: 0.0047, label: 1, bag_size: 11386\n",
      "batch 419, loss: 0.0019, instance_loss: 0.0003, weighted_loss: 0.0015, label: 0, bag_size: 3774\n",
      "batch 439, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 17633\n",
      "batch 459, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 5833\n",
      "batch 479, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 12758\n",
      "batch 499, loss: 0.0010, instance_loss: 0.0000, weighted_loss: 0.0007, label: 1, bag_size: 5864\n",
      "batch 519, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 2748\n",
      "batch 539, loss: 0.0003, instance_loss: 0.0000, weighted_loss: 0.0002, label: 1, bag_size: 11600\n",
      "batch 559, loss: 0.0110, instance_loss: 0.0000, weighted_loss: 0.0077, label: 1, bag_size: 12180\n",
      "batch 579, loss: 0.0001, instance_loss: 0.0010, weighted_loss: 0.0004, label: 1, bag_size: 549\n",
      "batch 599, loss: 0.0029, instance_loss: 0.0042, weighted_loss: 0.0033, label: 0, bag_size: 3657\n",
      "batch 619, loss: 0.0000, instance_loss: 0.0001, weighted_loss: 0.0001, label: 1, bag_size: 8602\n",
      "batch 639, loss: 0.0019, instance_loss: 0.0000, weighted_loss: 0.0013, label: 1, bag_size: 7515\n",
      "batch 659, loss: 0.1289, instance_loss: 0.0224, weighted_loss: 0.0970, label: 0, bag_size: 15898\n",
      "batch 679, loss: 0.0016, instance_loss: 0.0000, weighted_loss: 0.0011, label: 1, bag_size: 10492\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0007, weighted_loss: 0.0002, label: 0, bag_size: 1202\n",
      "batch 719, loss: 0.0000, instance_loss: 0.1124, weighted_loss: 0.0337, label: 1, bag_size: 2136\n",
      "batch 739, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 22828\n",
      "batch 759, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 1, bag_size: 14681\n",
      "batch 779, loss: 0.0155, instance_loss: 0.0010, weighted_loss: 0.0112, label: 1, bag_size: 9561\n",
      "batch 799, loss: 0.0007, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 12148\n",
      "batch 819, loss: 0.0000, instance_loss: 0.0144, weighted_loss: 0.0043, label: 1, bag_size: 1746\n",
      "\n",
      "\n",
      "class 0 clustering acc 0.9923780487804879: correct 13020/13120\n",
      "class 1 clustering acc 0.9708841463414634: correct 6369/6560\n",
      "Epoch: 91, train_loss: 0.0690, train_clustering_loss:  0.0723, train_error: 0.0280\n",
      "class 0: acc 0.9723618090452262, correct 387/398\n",
      "class 1: acc 0.9715639810426541, correct 410/422\n",
      "\n",
      "Val Set, val_loss: 0.0999, val_error: 0.0273, auc: 0.9930\n",
      "class 0 clustering acc 0.9727272727272728: correct 1712/1760\n",
      "class 1 clustering acc 0.8943181818181818: correct 787/880\n",
      "class 0: acc 0.9615384615384616, correct 50/52\n",
      "class 1: acc 0.9827586206896551, correct 57/58\n",
      "Validation loss decreased (0.100211 --> 0.099883).  Saving model ...\n",
      "\n",
      "\n",
      "batch 19, loss: 0.0272, instance_loss: 0.0000, weighted_loss: 0.0190, label: 1, bag_size: 19606\n",
      "batch 39, loss: 0.0001, instance_loss: 0.0002, weighted_loss: 0.0001, label: 0, bag_size: 8252\n",
      "batch 59, loss: 0.0014, instance_loss: 0.0000, weighted_loss: 0.0010, label: 1, bag_size: 6731\n",
      "batch 79, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11600\n",
      "batch 99, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 8252\n",
      "batch 119, loss: 0.0004, instance_loss: 0.0036, weighted_loss: 0.0014, label: 1, bag_size: 22264\n",
      "batch 139, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 22870\n",
      "batch 159, loss: 0.0234, instance_loss: 0.0099, weighted_loss: 0.0194, label: 1, bag_size: 12712\n",
      "batch 179, loss: 0.0007, instance_loss: 0.0019, weighted_loss: 0.0010, label: 1, bag_size: 8191\n",
      "batch 199, loss: 0.0002, instance_loss: 0.0000, weighted_loss: 0.0001, label: 0, bag_size: 11546\n",
      "batch 219, loss: 0.0024, instance_loss: 0.0056, weighted_loss: 0.0033, label: 0, bag_size: 3893\n",
      "batch 239, loss: 0.0408, instance_loss: 0.0278, weighted_loss: 0.0369, label: 0, bag_size: 1800\n",
      "batch 259, loss: 0.0215, instance_loss: 0.0018, weighted_loss: 0.0156, label: 1, bag_size: 1822\n",
      "batch 279, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 13365\n",
      "batch 299, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 4259\n",
      "batch 319, loss: 0.0040, instance_loss: 0.0005, weighted_loss: 0.0030, label: 0, bag_size: 8025\n",
      "batch 339, loss: 0.0986, instance_loss: 0.2304, weighted_loss: 0.1381, label: 1, bag_size: 1444\n",
      "batch 359, loss: 0.0001, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 12178\n",
      "batch 379, loss: 0.0006, instance_loss: 0.0075, weighted_loss: 0.0027, label: 1, bag_size: 2759\n",
      "batch 399, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 19832\n",
      "batch 419, loss: 3.5654, instance_loss: 0.6556, weighted_loss: 2.6925, label: 1, bag_size: 1845\n",
      "batch 439, loss: 0.0000, instance_loss: 0.0911, weighted_loss: 0.0274, label: 1, bag_size: 5160\n",
      "batch 459, loss: 0.0111, instance_loss: 0.0006, weighted_loss: 0.0079, label: 0, bag_size: 5105\n",
      "batch 479, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 11195\n",
      "batch 499, loss: 0.1800, instance_loss: 0.0032, weighted_loss: 0.1270, label: 0, bag_size: 11151\n",
      "batch 519, loss: 0.0008, instance_loss: 0.0000, weighted_loss: 0.0005, label: 0, bag_size: 18415\n",
      "batch 539, loss: 0.0005, instance_loss: 0.0000, weighted_loss: 0.0003, label: 0, bag_size: 9786\n",
      "batch 559, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 10112\n",
      "batch 579, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 15313\n",
      "batch 599, loss: 0.2466, instance_loss: 0.0000, weighted_loss: 0.1726, label: 1, bag_size: 7669\n",
      "batch 619, loss: 0.0025, instance_loss: 0.0000, weighted_loss: 0.0018, label: 1, bag_size: 2682\n",
      "batch 639, loss: 0.8955, instance_loss: 0.0199, weighted_loss: 0.6328, label: 0, bag_size: 3468\n",
      "batch 659, loss: 0.0015, instance_loss: 0.0000, weighted_loss: 0.0011, label: 1, bag_size: 30675\n",
      "batch 679, loss: 0.0214, instance_loss: 0.0018, weighted_loss: 0.0155, label: 0, bag_size: 1614\n",
      "batch 699, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 1, bag_size: 9571\n",
      "batch 719, loss: 0.0000, instance_loss: 0.0000, weighted_loss: 0.0000, label: 0, bag_size: 21093\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python main_myself.py --drop_out --early_stopping --lr 2e-4 --max_lr 2e-3 --k 5 --label_frac 1 --opt adam\\\n",
    "--exp_code cptac_lung_100_level02_mcbat_sb_depth1_adam_FLASH --weighted_sample --bag_loss ce --inst_loss svm \\\n",
    "--task task_2_tumor_subtyping --model_type mcbat_sb --log_data --data_low_dir /home/sci/Disk2/CPTAC-LUNG/FEATURES_level0 \\\n",
    "--data_high_dir /home/sci/Disk2/CPTAC-LUNG/FEATURES_level1 \\\n",
    "--split_dir /home/sci/PycharmProjects/chaofan/projects/CLAM/splits/cptac_lung_100 --subtyping \\\n",
    "--csv_path dataset_csv/cptac_lung_subtyping.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA_VISIBLE_DEVICES=0 python eval.py \\\n",
    "--drop_out \\\n",
    "--k 10 \\\n",
    "--models_exp_code task_1_tumor_vs_normal_CLAM_50_s1 \\\n",
    "--save_exp_code task_1_tumor_vs_normal_CLAM_50_s1_cv \\\n",
    "--task task_1_tumor_vs_normal \\\n",
    "--model_type clam_sb \\\n",
    "--results_dir results \\\n",
    "--data_root_dir /media/yuansh/14THHD/CLAM/FEATURES_DIRECTORY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task': 'task_1_tumor_vs_normal', 'split': 'test', 'save_dir': './eval_results/EVAL_tcga_crc_5_cv', 'models_dir': 'results/tcga_crc_5_s1', 'model_type': 'clam_sb', 'drop_out': True, 'model_size': 'small'}\n",
      "label column: label\n",
      "label dictionary: {'normal_tissue': 0, 'tumor_tissue': 1}\n",
      "number of classes: 2\n",
      "slide-level counts:  \n",
      " 1    1342\n",
      "0      91\n",
      "Name: label, dtype: int64\n",
      "Patient-LVL; Number of samples registered in class 0: 3\n",
      "Slide-LVL; Number of samples registered in class 0: 91\n",
      "Patient-LVL; Number of samples registered in class 1: 497\n",
      "Slide-LVL; Number of samples registered in class 1: 1342\n",
      "Init Model\n",
      "CLAM_SB(\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (instance_loss_fn): CrossEntropyLoss()\n",
      ")\n",
      "Total number of parameters: 790791\n",
      "Total number of trainable parameters: 790791\n",
      "Init Loaders\n",
      "133\n",
      "test_error:  0.022556390977443608\n",
      "auc:  0.43333333333333335\n",
      "Init Model\n",
      "CLAM_SB(\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (instance_loss_fn): CrossEntropyLoss()\n",
      ")\n",
      "Total number of parameters: 790791\n",
      "Total number of trainable parameters: 790791\n",
      "Init Loaders\n",
      "140\n",
      "test_error:  0.04285714285714286\n",
      "auc:  0.5634328358208955\n",
      "Init Model\n",
      "CLAM_SB(\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (instance_loss_fn): CrossEntropyLoss()\n",
      ")\n",
      "Total number of parameters: 790791\n",
      "Total number of trainable parameters: 790791\n",
      "Init Loaders\n",
      "146\n",
      "test_error:  0.0821917808219178\n",
      "auc:  0.34079601990049746\n",
      "Init Model\n",
      "CLAM_SB(\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (instance_loss_fn): CrossEntropyLoss()\n",
      ")\n",
      "Total number of parameters: 790791\n",
      "Total number of trainable parameters: 790791\n",
      "Init Loaders\n",
      "149\n",
      "test_error:  0.08053691275167785\n",
      "auc:  0.3652676399026764\n",
      "Init Model\n",
      "CLAM_SB(\n",
      "  (attention_net): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Attn_Net_Gated(\n",
      "      (attention_a): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_b): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): Sigmoid()\n",
      "        (2): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (attention_c): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifiers): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (instance_classifiers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (instance_loss_fn): CrossEntropyLoss()\n",
      ")\n",
      "Total number of parameters: 790791\n",
      "Total number of trainable parameters: 790791\n",
      "Init Loaders\n",
      "153\n",
      "test_error:  0.08496732026143791\n",
      "auc:  0.4582417582417582\n"
     ]
    }
   ],
   "source": [
    "!python eval.py --drop_out --k 5 --models_exp_code tcga_crc_5_s1 \\\n",
    "    --save_exp_code tcga_crc_5_cv \\\n",
    "    --task task_1_tumor_vs_normal --model_type clam_sb --results_dir results \\\n",
    "    --data_root_dir /home/sci/Disk2/tcga_crc/FEATURES_DIRECTORY_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "exp_arguments\n",
      "n_classes : 2\n",
      "save_exp_code : HEATMAP_OUTPUT\n",
      "raw_save_dir : heatmaps/heatmap_raw_results\n",
      "production_save_dir : heatmaps/heatmap_production_results\n",
      "batch_size : 384\n",
      "\n",
      "data_arguments\n",
      "data_dir : heatmaps/demo/slides/\n",
      "data_dir_key : source\n",
      "process_list : myself.csv\n",
      "preset : presets/bwh_brca.csv\n",
      "slide_ext : .svs\n",
      "label_dict : {'tumor': 1, 'normal': 0}\n",
      "\n",
      "patching_arguments\n",
      "patch_size : 256\n",
      "overlap : 0.5\n",
      "patch_level : 0\n",
      "custom_downsample : 1\n",
      "\n",
      "model_arguments\n",
      "ckpt_path : heatmaps/demo/ckpts/s_4_checkpoint.pt\n",
      "model_type : clam_sb\n",
      "initiate_fn : initiate_model\n",
      "model_size : small\n",
      "drop_out : True\n",
      "\n",
      "heatmap_arguments\n",
      "vis_level : 1\n",
      "alpha : 0.4\n",
      "blank_canvas : False\n",
      "save_orig : True\n",
      "save_ext : jpg\n",
      "use_ref_scores : True\n",
      "blur : False\n",
      "use_center_shift : True\n",
      "use_roi : False\n",
      "calc_heatmap : True\n",
      "binarize : False\n",
      "binary_thresh : -1\n",
      "custom_downsample : 1\n",
      "cmap : jet\n",
      "\n",
      "sample_arguments\n",
      "samples : [{'name': 'topk_high_attention', 'sample': True, 'seed': 1, 'k': 15, 'mode': 'topk'}]\n",
      "Continue? Y/N ^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sci/PycharmProjects/chaofan/projects/CLAM/create_heatmaps.py\", line 94, in <module>\n",
      "    decision = input('Continue? Y/N ')\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python create_heatmaps.py --config config_template.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('lcf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dddf4cc44ccb2cf796823d3e6277e76da85a74a2d70ce7f32ed1ef0f62e61e3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
